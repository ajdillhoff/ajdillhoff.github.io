<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academia 4.3.1">
  <meta name="theme-name" content="academia-hugo"/>

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Alex Dillhoff">

  
  
  
    
  
  <meta name="description" content="Table of Contents Introduction Box Constraints Updating the Lagrangians The Algorithm Implementation Introduction Paper link: https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/
Sequential Minimal Optimization (SMO) is an algorithm to solve the SVM Quadratic Programming (QP) problem efficiently (Platt, n.d.). Developed by John Platt at Microsoft Research, SMO deals with the constraints of the SVM objective by breaking it down into a smaller optimization problem at each step.
The two key components of SMO are">

  
  <link rel="alternate" hreflang="en-us" href="https://ajdillhoff.github.io/notes/sequential_minimal_optimization/">

  


  

  
  
  
  <meta name="theme-color" content="#fc6f5c">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Open+Sans|Roboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academia.min.b246554d075350d61b44c126dfbcbe05.css">

  
    
    
    
    
      
    
    
    
    <link rel="stylesheet" href="/css/academia.a75a9b8a9a725a2157c0c5b929a3d18b.css">
  

  
  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-123456-78', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://ajdillhoff.github.io/notes/sequential_minimal_optimization/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Alex Dillhoff">
  <meta property="og:url" content="https://ajdillhoff.github.io/notes/sequential_minimal_optimization/">
  <meta property="og:title" content="Sequential Minimal Optimization | Alex Dillhoff">
  <meta property="og:description" content="Table of Contents Introduction Box Constraints Updating the Lagrangians The Algorithm Implementation Introduction Paper link: https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/
Sequential Minimal Optimization (SMO) is an algorithm to solve the SVM Quadratic Programming (QP) problem efficiently (Platt, n.d.). Developed by John Platt at Microsoft Research, SMO deals with the constraints of the SVM objective by breaking it down into a smaller optimization problem at each step.
The two key components of SMO are"><meta property="og:image" content="https://ajdillhoff.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://ajdillhoff.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2022-07-04T00:00:00-05:00">
  
  <meta property="article:modified_time" content="2024-02-01T00:00:00&#43;00:00">
  

  


  





  <title>Sequential Minimal Optimization | Alex Dillhoff</title>

  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>



</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Alex Dillhoff</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation"><span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">
      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/notes/"><span>Brain Dump</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

      

        

        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>
    </div>
  </div>
</nav>


  <article class="article py-5" itemscope itemtype="http://schema.org/Article">

  












    

    
    
    
    <div class="article-container py-3">
      <h1 itemprop="name">Sequential Minimal Optimization</h1>

      

      
      



<meta content="2022-07-04 00:00:00 -0500 CDT" itemprop="datePublished">
<meta content="2024-02-01 00:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/alex-dillhoff/">Alex Dillhoff</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    <time>Feb 1, 2024</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    8 min read
  </span>
  

  
  
  
  
  

  
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://ajdillhoff.github.io/notes/sequential_minimal_optimization/&amp;text=Sequential%20Minimal%20Optimization" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://ajdillhoff.github.io/notes/sequential_minimal_optimization/&amp;t=Sequential%20Minimal%20Optimization" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Sequential%20Minimal%20Optimization&amp;body=https://ajdillhoff.github.io/notes/sequential_minimal_optimization/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://ajdillhoff.github.io/notes/sequential_minimal_optimization/&amp;title=Sequential%20Minimal%20Optimization" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Sequential%20Minimal%20Optimization%20https://ajdillhoff.github.io/notes/sequential_minimal_optimization/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://ajdillhoff.github.io/notes/sequential_minimal_optimization/&amp;title=Sequential%20Minimal%20Optimization" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

      














      
      
    </div>
  </div>
</div>

  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      <div class="ox-hugo-toc toc">
<div class="heading">Table of Contents</div>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#box-constraints">Box Constraints</a></li>
<li><a href="#updating-the-lagrangians">Updating the Lagrangians</a></li>
<li><a href="#the-algorithm">The Algorithm</a></li>
<li><a href="#implementation">Implementation</a></li>
</ul>
</div>
<!--endtoc-->
<h2 id="introduction">Introduction</h2>
<p><strong>Paper link:</strong> <a href="https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/" target="_blank" rel="noopener noreferrer">https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/</a></p>
<p>Sequential Minimal Optimization (SMO) is an algorithm to solve the SVM Quadratic Programming (QP) problem efficiently (<a href="#citeproc_bib_item_1">Platt, n.d.</a>). Developed by John Platt at Microsoft Research, SMO deals with the constraints of the SVM objective by breaking it down into a smaller optimization problem at each step.</p>
<p>The two key components of SMO are</p>
<ol>
<li>an analytic method to solving for two Lagrange multipliers at a time</li>
<li>and a heuristic for choosing which multipliers to optimize.</li>
</ol>
<p>The original objective is to maximize the margin between the nearest positive and negative examples.
For the linear case, if the output is given as</p>
<p>\[
u = \mathbf{w}^T \mathbf{x} - b,
\]</p>
<p>where \(\mathbf{w}\) is the normal vector to the hyperplane separating the classes, then the margin is given as</p>
<p>\[
m = \frac{1}{\|w\|_2}.
\]</p>
<p>Maximizing this margin yielded the primal optimization problem</p>
<p>\begin{align*}
\min_{\mathbf{w},b} \frac{1}{2} \|\mathbf{w}\|^2\\
\textrm{s.t.} \quad &amp; y_i(\mathbf{w}^T \mathbf{x} - b) \geq 1, \forall i\\
\end{align*}</p>
<p>The dual form of the objective function for a <a href="/notes/support_vector_machine/">Support Vector Machine</a> is</p>
<p>\[
\min_{\vec\alpha} \Psi(\vec{\alpha}) = \min_{\vec{\alpha}} \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N y_i y_j K(\mathbf{x}_i, \mathbf{x}_j)\alpha_i\alpha_j - \sum_{i=1}^N \alpha_i
\]</p>
<p>with inequality constraints</p>
<p>\[
\alpha_i \geq 0, \forall i,
\]</p>
<p>and a linear equality constraint</p>
<p>\[
\sum_{i=1}^N y_i \alpha_i = 0.
\]</p>
<p>For a linear SVM, the output is dependent on a weight vector \(\mathbf{w}\) and threshold \(b\):</p>
<p>\[
\mathbf{w} = \sum_{i=1}^N y_i \alpha_i \mathbf{x}_i, \quad b = \mathbf{w}^T \mathbf{x}_k - y_k.
\]</p>
<p><strong><strong>The threshold is also dependent on the weight vector?</strong></strong> The weight vector \(\mathbf{w}\) is computed using the training data. The threshold is only dependent on non-zero support vectors, \(\alpha_k &gt; 0\).</p>
<h3 id="overlapping-distributions">Overlapping Distributions</h3>
<p>Slack variables were introduced to allow misclassifications at the cost of a linear penalty.
This is useful for datasets that are not linearly separable.
In practice, this is accomplished with a slight modification of the original objective function:</p>
<p>\begin{align*}
\min_{\mathbf{w},b} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^N \xi_i\\
\textrm{s.t.} \quad &amp; y_i(\mathbf{w}^T \mathbf{x} - b) \geq 1 - \xi_i, \forall i\\
\end{align*}</p>
<p>The convenience of this formulation is that the parameters \(\xi_i\) do not appear in the dual formulation at all.
The only added constraint is</p>
<p>\[
0 \leq \alpha_i \leq C, \forall i.
\]</p>
<p>This is referred to as the box constraint for reasons we shall see shortly.</p>
<h2 id="box-constraints">Box Constraints</h2>
<p>The smallest optimization step that SMO solves is that of two variables.
Given the constraints above, the solution lies on a diagonal line \(\sum_{i=1}^N y_i \alpha_i = 0\) bounded within a box \(0 \leq \alpha_i \leq C, \forall i\).</p>
<p>Isolating for two samples with alphas \(\alpha_1\) and \(\alpha_2\), the constraint \(\sum_{i=1}^n y_i \alpha_i = 0\) suggests that</p>
<p>\[
y_1 \alpha_1 + y_2 \alpha_2 = w.
\]</p>
<p>We first consider the case when \(y_1 \neq y_2\).
Let \(y_1 = 1\) and \(y_2 = -1\), then \(a_1 - a_2 = w\).
As \(\alpha_1\) increases, \(\alpha_2\) must also increase to satisfy the constraint.</p>






<figure>

<img src="/ox-hugo/2022-07-10_22-48-56_screenshot.png" alt="&lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Equality constraint for case 1 (from Platt&#39;s SMO paper)." >



<figcaption data-pre="Figure " data-post=":" >
  
  <p>
    <span class="figure-number">Figure 1: </span>Equality constraint for case 1 (from Platt&rsquo;s SMO paper).
    
    
    
  </p> 
</figcaption>

</figure>

<p>The other case is when \(y_1 = y_2\), then \(\alpha_1 + \alpha_2 = w\).
As \(\alpha_1\) is increased, \(\alpha_2\) is decreased to satisfy the constraint.</p>






<figure>

<img src="/ox-hugo/2022-07-10_22-51-53_screenshot.png" alt="&lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Box constraint for samples of the same class (from Platt&#39;s SMO paper)." >



<figcaption data-pre="Figure " data-post=":" >
  
  <p>
    <span class="figure-number">Figure 2: </span>Box constraint for samples of the same class (from Platt&rsquo;s SMO paper).
    
    
    
  </p> 
</figcaption>

</figure>

<h2 id="updating-the-lagrangians">Updating the Lagrangians</h2>
<p>SMO solves for only two Lagrange multipliers at a time.
Solving for only 1 at a time would be impossible under the constraint \(\sum_{i=1}^N y_i \alpha_i = 0\).
The first step is to compute \(\alpha_2\) and constrain it between the ends of the diagonal line segment from the box constraints.</p>
<p>If \(y_1 \neq y_2\), then the following bounds are applied to \(\alpha_2\):</p>
<p>\begin{equation*}
L = \max(0, \alpha_2 - \alpha_1), \quad H = \min(C, C + \alpha_2 - \alpha_1)
\end{equation*}</p>
<p>otherwise, the bounds are computed as:</p>
<p>\begin{equation*}
L = \max(0, \alpha_2 + \alpha_1 - C), \quad H = \min(C, \alpha_2 + \alpha_1)
\end{equation*}</p>
<p>Updating the actual parameter is done following the update rule of gradient descent:</p>
<p>\[
\alpha_2^{\text{new}} = \alpha_2 + \frac{y_2(E_1 - E_2)}{\eta}.
\]</p>
<p><strong>How do we arrive at this update rule?</strong></p>
<h3 id="second-derivative-of-the-objective-function">Second Derivative of the Objective Function</h3>
<p>Here, \(\eta\) represents the step size and direction. It is computed from the second derivative of the objective function along the diagonal line. To see that this is the case, consider the original objective function</p>
<p>\begin{align*}
\min_{\mathbf{\alpha}} \quad &amp; \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N y_i y_j K(\mathbf{x}_i, \mathbf{x}_j) \mathbf{\alpha}_1 \mathbf{\alpha}_2 - \sum_{i=1}^N \alpha_i\\
\textrm{s.t.} \quad &amp; 0 \leq \alpha_i \leq C, \forall i\\
&amp; \sum_{i=1}^N y_i \alpha_i = 0\\
\end{align*}</p>
<p>Since we are optimizing with respect to only 2 Lagrangian multipliers at a time, we can write the Lagrangian function as</p>
<p>\[
\frac{1}{2} y_1^2 K_{11} \alpha_1^2 + \frac{1}{2} y_2^2 K_{22} \alpha_2^2 + y_1 \alpha_1 \sum_{j=3}^N y_j \alpha_j K_{1j} + y_2 \alpha_2 \sum_{j=3}^N y_j \alpha_j K_{2j} - \alpha_1 - \alpha_2 + \sum_{j=3}^N \alpha_j
\]</p>
<p>We are only optimizing with respect to \(\alpha_1\) and \(\alpha_2\), the next step is to extract those terms from the sum.
This is simplified further by noting that \(\sum_{j=3}^N y_j \alpha_j K_{ij}\) looks very similar to the output of an SVM:</p>
<p>\[
u = \sum_{j=1}^N y_j \alpha_j K(\mathbf{x}_j, \mathbf{x}) - b.
\]</p>
<p>This allows us to introduce a variable \(v_i\) based on \(u_i\), the output of an SVM given sample \(\mathbf{x}_i\):</p>
<p>\[
v_i = \sum_{j=3}^N y_j \alpha_j K_{ij} = u_i + b - y_1 \alpha_1 K_{1i} - y_2 \alpha_2 K_{2i}.
\]</p>
<p>The objective function is then written as</p>
<p>\[
\frac{1}{2} y_1^2 K_{11} \alpha_1^2 + \frac{1}{2} y_2^2 K_{22} \alpha_2^2 + y_1 \alpha_1 v_1 + y_2 \alpha_2 v_2 - \alpha_1 - \alpha_2 + \sum_{j=3}^N \alpha_j.
\]</p>
<p>Note that the trailing sum \(\sum_{j=3}^N \alpha_j\) is treated as a constant since those values are not considered when optimizing for \(\alpha_1\) and \(\alpha_2\).</p>
<p>Given the box constraints from above, we must update \(\alpha_1\) and \(\alpha_2\) such that</p>
<p>\[
\alpha_1 + s \alpha_2 = \alpha_1^* + s \alpha_2^* = w.
\]</p>
<p>This linear relationship allows us to express the objective function in terms of α_2:</p>
<p>\[
\Psi = \frac{1}{2} y_1^2 K_{11} (w - s \alpha_2)^2 + \frac{1}{2} y_2^2 K_{22} \alpha_2^2 + y_1 (w - s \alpha_2) v_1 + y_2 \alpha_2 v_2 - \alpha_1 - \alpha_2 + \sum_{j=3}^N \alpha_j.
\]</p>
<p>The extremum of the function is given by the first derivative with respect to \(\alpha_2\):</p>
<p>\[
\frac{d\Psi}{d\alpha_2} = -sK_{11}(w - s\alpha_2) + K_{22}\alpha_2 - K_{12}\alpha_2 + s K_{12} (w - s \alpha_2) - y_2 v_2 + s + y_2 v_2 - 1 = 0.
\]</p>
<p>In most cases, the second derivative will be positive.
The minimum of \(\alpha_2\) is where</p>
<p>\begin{align*}
\alpha_2 (K_{11} + K_{22} - 2 K_{12}) &amp;= s(K_{11} - K_{12})w + y_2(v_1 - v_2) + 1 - s\\
&amp;= s(K_{11} - K_{12})(s\alpha_2^*+\alpha_1^*)\\
&amp;+ y_2(u_1-u_2+y_1\alpha_1^*(K_{12} - K_{11}) + y_2 \alpha_2^* (K_{22} - K_{21})) + y_2^2 - s\\
&amp;= \alpha_2^*(K_{11}+K_{22} - 2K_{12}) + y_2(u_1 - u_2 + y_2 - y_1).
\end{align*}</p>
<p>If we let \(E_1 = u_1 - y_1\), \(E_2 = u_2 - y_2\), and \(\eta = K_{11} + K_{22} - 2K_{12}\), then</p>
<p>\[
\alpha_2^{\text{new}} = \alpha_2 + \frac{y_2(E_1 - E_2)}{\eta}.
\]</p>
<h2 id="the-algorithm">The Algorithm</h2>
<p>Sequential Minimal Optimization (SMO) solves the SVM problem which usually requires a Quadratic Programming (QP) solution.
It does this by breaking down the larger optimization problem into a small and simple form: solving for two Lagrangians.
Solving for one would not be possible without violating KKT conditions.
There are two components to Sequential Minimal Optimization: the first is how the Lagrangians are selected and the second is the actual optimization step.</p>
<h3 id="choosing-the-first-lagrangian">Choosing the First Lagrangian</h3>
<p>The algorithm first determines which samples in the dataset violate the given KKT conditions. Only those violating the conditions are eligible for optimization. A solution is found when the following are true for all \(i\):</p>
<p>\begin{align*}
\alpha_i = 0 \iff y_i u_i \geq 1,\\
0 &lt; \alpha_i &lt; C \iff y_i u_i = 1,\\
\alpha_i = C \iff y_i u_i \leq 1.
\end{align*}</p>
<p>Additionally, samples that are not on the bounds are selected (those with \(\alpha_i \neq 0\) and \(\alpha_i \neq C\)).
This continues through the dataset until no sample violates the KKT constraints within \(\epsilon\).</p>
<p>As a last step, SMO searches the entire dataset to look for any bound samples that violate KKT conditions. It is possible that updating a non-bound sample would cause a bound sample to violate the KKT conditions.</p>
<h3 id="choosing-the-second-lagrangian">Choosing the Second Lagrangian</h3>
<p>The second Lagrangian is chosen to maximize the size of the step taken during joint optimization.
Noting that the step size is based on</p>
<p>\[
\alpha_2^{\text{new}} = \alpha_2 + \frac{y_2(E_1 - E_2)}{\eta},
\]</p>
<p>it is approximated by computing \(|E_1 - E_2|\).</p>
<p>If positive progress cannot be made given the choice of Lagrangian, SMO will begin iterating through non-bound examples.
If no eligible candidates are found in the non-bound samples, the entire dataset is searched.</p>
<h3 id="updating-the-parameters">Updating the Parameters</h3>
<p>With the second derivative of the objective function, we can take an optimization step along the diagonal line.
To ensure that this step adheres to the box constraints defined above, the new value of \(\alpha_2\) is clipped:</p>
<p>\begin{equation*}
\alpha_2^{\text{new,clipped}} =
\begin{cases}
H &amp;\text{if} \quad \alpha_2^{\text{new}} \geq H;\\
\alpha_2^{\text{new}} &amp;\text{if} \quad L &lt; \alpha_2^{\text{new}} &lt; H;\\
L &amp;\text{if} \quad \alpha_2^{\text{new}} \geq L.\\
\end{cases}
\end{equation*}</p>
<p>With the new value of \(\alpha_2\), \(\alpha_1\) is computed such that the original KKT condition is preserved:</p>
<p>\[
\alpha_1^{\text{new}} = \alpha_1 + s(\alpha_2 - \alpha_2^{\text{new,clipped}}),
\]</p>
<p>where \(s = y_1y_2\).</p>
<p>Points that are beyond the margin are given an alpha of 0: \(\alpha_i = 0\).
Points that are on the margin satisfy \(0 &lt; \alpha_i &lt; C\). These are the support vectors.
Points inside the margin satisfy \(\alpha_i = C\).</p>
<h4 id="linear-svms">Linear SVMs</h4>
<p>In the case of linear SVMs, the parameters can be stored as a single weight vector</p>
<p>\[
\mathbf{w}^{\text{new}} = \mathbf{w} + y_1 (\alpha_1^{\text{new}} - \alpha_1)\mathbf{x}_1 + y_2(\alpha_2^{\text{new,clipped}} - \alpha_2)\mathbf{x}_2.
\]</p>
<p>The output of a linear SVM is computed as</p>
<p>\[
u = \mathbf{w}^T \mathbf{x} - b.
\]</p>
<h4 id="nonlinear-svms">Nonlinear SVMs</h4>
<p>In the nonlinear case, the output of the model is computed as</p>
<p>\[
u = \sum_{i=1}^N y_i \alpha_i K(\mathbf{x}_i, \mathbf{x}) - b.
\]</p>
<h2 id="implementation">Implementation</h2>
<p>An implementation of SMO in Python is available at <a href="https://github.com/ajdillhoff/CSE6363/blob/main/svm/smo.ipynb" target="_blank" rel="noopener noreferrer">https://github.com/ajdillhoff/CSE6363/blob/main/svm/smo.ipynb</a></p>
<h2 id="references">References</h2>
<style>.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}</style><div class="csl-bib-body">
  <div class="csl-entry"><a id="citeproc_bib_item_1"></a>Platt, John C. n.d. “Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines,” 21.</div>
</div>

    </div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/machine-learning/">machine learning</a>
  
</div>



    
      








  
  
    
  
  





  
  
  
    
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="https://ajdillhoff.github.io/">Alex Dillhoff</a></h5>
      <h6 class="card-subtitle">Senior Lecturer</h6>
      <p class="card-text" itemprop="description">&quot;If we understood the world, we would realize that there is a logic of harmony underlying its manifold apparent dissonances.&quot; - Jean Sibelius</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="/#contact" >
              <i class="fas fa-envelope"></i>
            </a>
          </li>
        
          
          
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://scholar.google.co.uk/citations?user=UlLhCtkAAAAJ" target="_blank" rel="noopener">
              <i class="ai ai-google-scholar"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/ajdillhoff" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/notes/object_detection/">Object Detection</a></li>
          
          <li><a href="/notes/boosting/">Boosting</a></li>
          
          <li><a href="/notes/decision_trees/">Decision Trees</a></li>
          
          <li><a href="/notes/camera_models/">Camera Models</a></li>
          
          <li><a href="/notes/hidden_markov_models/">Hidden Markov Models</a></li>
          
        </ul>
      </div>
      
    

    

    

  </div>
</article>

      

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
      

      
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    
    <script id="dsq-count-scr" src="//themefisher-template.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    

    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academia.min.6c2ba2801d406881b3c2277043cedd76.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  <div class="container">
    <div class="row align-items-center">
      <div class="col-md-6 mb-4 mb-md-0">
        
        <p class="mb-0">
          Copyright © 2024 &middot; 
          Powered by
          <a href="https://gethugothemes.com" target="_blank" rel="noopener">Gethugothemes</a>
        </p>
      </div>
      <div class="col-md-6">
        <ul class="list-inline network-icon text-right mb-0">
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="list-inline-item">
            <a href="https://github.com/ajdillhoff" target="_blank" rel="noopener" title="My GitHub"><i class="fab fa-github" aria-hidden="true"></i></a>
          </li>
          
          
          
          
          
          
          
          
          
          
          <li class="list-inline-item">
            <a href="https://github.com/ajdillhoff" target="_blank" rel="noopener" title="My GitHub"><i class="ai ai-google-scholar" aria-hidden="true"></i></a>
          </li>
          
        </ul>
      </div>
    </div>
  </div>
</footer>
  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
