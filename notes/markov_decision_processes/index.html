<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academia 4.3.1">
  <meta name="theme-name" content="academia-hugo"/>

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Alex Dillhoff">

  
  
  
    
  
  <meta name="description" content="Table of Contents Key Terms Defining Goals Policies and Values Bellman Equations Optimality Optimizing the Policy Key Terms Agent: The learner or decision maker. Environment: The world that the agent can interact with. State: A representation of the agent and environment. Action: The agent can take an action in the environment. Reward: Given to the agent based on actions taken. Goal: Maximize rewards earned over time.
At time \(t\), the agent observes the state of the environment \(S_t \in \mathcal{S}\) and can select an action \(A_t \in \mathcal{A}(s)\), where \(\mathcal{A}(s)\) suggests that the available actions are dependent on the current state.">

  
  <link rel="alternate" hreflang="en-us" href="https://ajdillhoff.github.io/notes/markov_decision_processes/">

  


  

  
  
  
  <meta name="theme-color" content="#fc6f5c">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Open+Sans|Roboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academia.min.b246554d075350d61b44c126dfbcbe05.css">

  
    
    
    
    
      
    
    
    
    <link rel="stylesheet" href="/css/academia.a75a9b8a9a725a2157c0c5b929a3d18b.css">
  

  
  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-123456-78', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://ajdillhoff.github.io/notes/markov_decision_processes/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Alex Dillhoff">
  <meta property="og:url" content="https://ajdillhoff.github.io/notes/markov_decision_processes/">
  <meta property="og:title" content="Markov Decision Processes | Alex Dillhoff">
  <meta property="og:description" content="Table of Contents Key Terms Defining Goals Policies and Values Bellman Equations Optimality Optimizing the Policy Key Terms Agent: The learner or decision maker. Environment: The world that the agent can interact with. State: A representation of the agent and environment. Action: The agent can take an action in the environment. Reward: Given to the agent based on actions taken. Goal: Maximize rewards earned over time.
At time \(t\), the agent observes the state of the environment \(S_t \in \mathcal{S}\) and can select an action \(A_t \in \mathcal{A}(s)\), where \(\mathcal{A}(s)\) suggests that the available actions are dependent on the current state."><meta property="og:image" content="https://ajdillhoff.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://ajdillhoff.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2023-07-24T00:00:00-05:00">
  
  <meta property="article:modified_time" content="2023-07-24T00:00:00-05:00">
  

  


  





  <title>Markov Decision Processes | Alex Dillhoff</title>

  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      processEsacpes: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>



</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Alex Dillhoff</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation"><span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">
      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/notes/"><span>Brain Dump</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

      

        

        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>
    </div>
  </div>
</nav>


  <article class="article py-5" itemscope itemtype="http://schema.org/Article">

  












    

    
    
    
    <div class="article-container py-3">
      <h1 itemprop="name">Markov Decision Processes</h1>

      

      
      



<meta content="2023-07-24 00:00:00 -0500 CDT" itemprop="datePublished">
<meta content="2023-07-24 00:00:00 -0500 CDT" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/alex-dillhoff/">Alex Dillhoff</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>Jul 24, 2023</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    7 min read
  </span>
  

  
  
  
  
  

  
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://ajdillhoff.github.io/notes/markov_decision_processes/&amp;text=Markov%20Decision%20Processes" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://ajdillhoff.github.io/notes/markov_decision_processes/&amp;t=Markov%20Decision%20Processes" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Markov%20Decision%20Processes&amp;body=https://ajdillhoff.github.io/notes/markov_decision_processes/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://ajdillhoff.github.io/notes/markov_decision_processes/&amp;title=Markov%20Decision%20Processes" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Markov%20Decision%20Processes%20https://ajdillhoff.github.io/notes/markov_decision_processes/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://ajdillhoff.github.io/notes/markov_decision_processes/&amp;title=Markov%20Decision%20Processes" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

      














      
      
    </div>
  </div>
</div>

  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      <div class="ox-hugo-toc toc">
<div class="heading">Table of Contents</div>
<ul>
<li><a href="#key-terms">Key Terms</a></li>
<li><a href="#defining-goals">Defining Goals</a></li>
<li><a href="#policies-and-values">Policies and Values</a></li>
<li><a href="#bellman-equations">Bellman Equations</a></li>
<li><a href="#optimality">Optimality</a></li>
<li><a href="#optimizing-the-policy">Optimizing the Policy</a></li>
</ul>
</div>
<!--endtoc-->
<h2 id="key-terms">Key Terms</h2>
<ul>
<li><strong>Agent</strong>: The learner or decision maker.</li>
<li><strong>Environment</strong>: The world that the agent can interact with.</li>
<li><strong>State</strong>: A representation of the agent and environment.</li>
<li><strong>Action</strong>: The agent can take an action in the environment.</li>
<li><strong>Reward</strong>: Given to the agent based on actions taken.</li>
</ul>
<p><strong>Goal</strong>: Maximize rewards earned over time.</p>
<p>At time \(t\), the agent observes the state of the environment \(S_t \in \mathcal{S}\) and can select an action \(A_t \in \mathcal{A}(s)\), where \(\mathcal{A}(s)\) suggests that the available actions are dependent on the current state.
At time \(t + 1\), the agent receives a reward \(R_{t+1} \in \mathcal{R}\).</p>






<figure>

<img src="/ox-hugo/2022-11-15_19-01-30_screenshot.png" alt="&lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;The agent-environment in a Markov decision process (Credit: Sutton &amp;amp; Barto)." >



<figcaption data-pre="Figure " data-post=":" >
  
  <p>
    <span class="figure-number">Figure 1: </span>The agent-environment in a Markov decision process (Credit: Sutton &amp; Barto).
    
    
    
  </p> 
</figcaption>

</figure>

<p>To improve its knowledge about an environment or increase its performance on a task, an agent must first be able to interpret or make sense of that environment in some way.
Second, there must be a well defined goal. For an agent playing Super Mario, for example, the goal would be to complete each level while maximizing the score.
Third, the agent must be able to interact with its environment by taking actions.
If the Mario-playing agent could not move Mario around, it would never be able to improve.
If the agent makes a decision which leads to Mario&rsquo;s untimely demise, it would update its knowledge of the world so that it would tend towards a more favorable action.
These three requirements: sensations, actions, and goals, are encapsulated by <strong>Markov Decision Processes</strong>.</p>
<p>A Markov decision process is defined by</p>
<ul>
<li>\(\mathcal{S}\) - a set of states,</li>
<li>\(\mathcal{A}\) - a set of actions,</li>
<li>\(\mathcal{R}\) - a set of rewards,</li>
<li>\(P\) - the transition probability function to determine transition between states,</li>
<li>\(\gamma\) - discount factor for future rewards.</li>
</ul>
<p>At time \(t\), an agent in state \(S_t\) selects an action \(A_t\).
At \(t+1\), it receives a reward \(R_{t+1}\) based on that action.</p>
<p>In a finite MDP, the states, actions, and rewards have a finite number of elements.
Random variables \(R_t\) and \(S_t\) have discrete probability distributions dependent on the preceding state and action.</p>
<p>\[
p(s&rsquo;, r|s, a) = P\{S_t = s&rsquo;, R_t = r|S_{t-1} = s, A_{t-1} = a\}
\]</p>
<p>If we want the state transition probabilities, we can sum over the above distribution:</p>
<p>\[
p(s&rsquo;|s, a) = P\{S_t = s&rsquo;|S_{t-1} = s, A_{t-1}=a\} = \sum_{r\in\mathcal{R}}p(s&rsquo;, r|s, a).
\]</p>
<p>The <strong>reward function</strong> \(r\) gives the expected <em>next</em> reward given some state and action:</p>
<p>\[
r(s, a) = \mathbb{E}[R_{t+1}|S_t=s, A_t=a] = \sum_{r}r \sum_{s&rsquo;}p(s&rsquo;, r|s, a).
\]</p>
<h2 id="defining-goals">Defining Goals</h2>
<p>In reinforcement learning, the goal is encoded in the form of a <strong><strong>reward signal</strong></strong>. The agent sets out to <em>maximize</em> the total amount of reward it receives over an <strong><strong>episode</strong></strong>. An <strong><strong>episode</strong></strong> is defined dependent on the problem context and ends in a <strong><strong>terminal state</strong></strong>. It could be a round of game, a single play, or the result of moving a robot. Typically, the rewards come as a single scalar value at teach time step. This implies that an agent might take an action that results in a negative reward if it is optimal in the long run.</p>
<p>Formally, the <strong>expected return</strong> includes a <strong>discount factor</strong> that allows us to control the trade-off between short-term and long-term rewards:</p>
<p>\[
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1},
\]</p>
<p>where \(0 \leq \gamma \leq 1\). This can be written in terms of the expected return itself as well:</p>
<p>\[
G_t = R_{t+1} + \gamma G_{t+1}.
\]</p>
<h2 id="policies-and-values">Policies and Values</h2>
<p>Two important concepts that help our agent make decisions are the policy and value functions. A <strong>policy</strong>, typically denoted by \(\pi\), maps states to actions. Such a function can be deterministic, \(\pi(s) = a\), or stochastic, \(\pi(a|s)\).</p>
<p>The value of a particular state under a policy \(\pi\) is defined as</p>
<p>\[
v_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s] = \mathbb{E}_{\pi}\Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}\Bigg|S_t=s\Bigg].
\]</p>
<p>We also must define the value of taking an action \(a\) in state \(s\) following policy \(\pi\):</p>
<p>\[
q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t|S_t=s, A_t=a] = \mathbb{E}_{\pi}\Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}\Bigg|S_t=s, A_t=a\Bigg].
\]</p>
<p>This function defines the expected return of following a particular policy and starting in state \(s\).
Both the <strong>state-value</strong> and <strong>action-value</strong> functions can be updated as a result of the agent&rsquo;s experience. How it is updated is method-dependent. Certain methods will also dictate how the policy itself can be updated.</p>
<h2 id="bellman-equations">Bellman Equations</h2>
<p>The recursive relationship between the value of a state and its future states can be represented using <strong><strong>Bellman equations</strong></strong>. In RL, we are interested in the equations for both the state-value and action-value. Given the diagram of an MDP, we can see that they are related to each other. To make the following equations easier to understand, it is important to remember the flow of a Markov decision process:</p>
<ol>
<li>take an action,</li>
<li>arrive at a state and sense the reward,</li>
<li>consult the policy for the next action.</li>
</ol>
<p>With that in mind, let&rsquo;s look at the state-value function first. This function considers the expected value of starting in a state \(s\) and following policy \(\pi\). In other words, we must consider all possible actions and their future rewards.</p>
<p>\begin{align*}
v_{\pi}(s) &amp;= \mathbb{E}_{\pi}[G_t | S_t = s]\\
&amp;= \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]\\
&amp;= \sum_{a} \pi(a|s) \sum_{s&rsquo;}\sum_{r}p(s&rsquo;, r|s, a)\Big[r + \gamma \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]\Big]\\
&amp;= \sum_{a} \pi(a|s) \sum_{s&rsquo;, r}p(s&rsquo;, r|s, a)\Big[r + \gamma v_{\pi}(s&rsquo;)\Big]\\
&amp;= \sum_{a} \pi(a | s)\big[r(s, a) + \gamma \sum_{s&rsquo;}p(s&rsquo;|s, a)v_{\pi}(s&rsquo;)\big]\\
\end{align*}</p>
<p>The first sum over actions considers <em>all possible actions</em>. This is followed by a transition to possible states \(s&rsquo;\) conditioned on taking each action multiplied by the expected value of being at the new state.</p>
<p>The action-value function follows a similar derivation:</p>
<p>\begin{align*}
q_{\pi}(s, a) &amp;= \mathbb{E}_{\pi}[G_t|S_t = s, A_t = a]\\
&amp;= \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a]\\
&amp;= r(s, a) + \sum_{s&rsquo;}p(s&rsquo;|s, a) v_{\pi}(s&rsquo;)
\end{align*}</p>
<p>There is a very similar looking set of terms in the state-value function above, and we should expect that! If we want to evaluate the current state, we need to look ahead at the possible actions and their resulting rewards. Similarly, evaluating the current action requires us to look head at the value of future states.</p>
<p>Let&rsquo;s expand \(q_{\pi}(s,a)\) once more so that it is written in terms of itself.</p>
<p>\begin{align*}
q_{\pi}(s, a) &amp;= r(s, a) + \gamma \sum_{s&rsquo;}p(s&rsquo;|s, a) v_{\pi}(s&rsquo;)\\
&amp;= r(s, a) + \gamma \sum_{s&rsquo;}p(s&rsquo;|s, a) \sum_{a&rsquo;} \pi(s&rsquo;, a&rsquo;) \big[r(s&rsquo;, a&rsquo;) + \gamma \sum_{s&rsquo;&rsquo;} p(s&rsquo;&rsquo;|s&rsquo;, a&rsquo;)v_{\pi}(s&rsquo;&rsquo;)]\\
&amp;= r(s, a) + \gamma \sum_{s&rsquo;}p(s&rsquo;|s, a) \sum_{a&rsquo;} \pi(s&rsquo;, a&rsquo;) q_{\pi}(s&rsquo;, a&rsquo;)
\end{align*}</p>
<h2 id="optimality">Optimality</h2>
<p>To solve a reinforcement learning problem, we are interested in finding the policy \(\pi_{*}\) whose expected return is greater than all other possible policies over all states.
An <strong><strong>optimal policy</strong></strong> will use an <strong>*optimal state-value function</strong> and <strong><strong>optimal action-value function</strong></strong>:</p>
<p>\begin{align*}
v_{*}(s) &amp;= \max_{\pi}v_{\pi}(s)\\
q_{*}(s, a) &amp;= \max_{\pi}q_{\pi}(s, a).
\end{align*}</p>
<p>The optimal state-value function would select the best possible action instead of summing over all possibley actions starting in state \(s\):</p>
<p>\begin{align*}
v_{*}(s) &amp;= \max_{a} q_{\pi_*}(s, a)\\
&amp;= \max_{a}\big[r(s, a) + \gamma \sum_{s&rsquo;} p(s&rsquo;|s, a) v_{*}(s&rsquo;)\big]
\end{align*}</p>
<p>Similarly, the optimal action-value function selects the best possible action from the next state \(s&rsquo;\):</p>
<p>\[
q_{*}(s) = r(s, a) + \gamma \sum_{s&rsquo;} p(s&rsquo;|s, a) \max_{a} q_{*}(s&rsquo;, a&rsquo;).
\]</p>
<h2 id="optimizing-the-policy">Optimizing the Policy</h2>
<p>For smaller problems with reasonably small state and action spaces, we can use Dynamic Programming to compute the optimal policy. These methods quickly become intractable as the complexity of our problem increases. As is common in machine learning, we would resort to approximation methods for complex spaces.</p>
<blockquote>
<p>&ldquo;In fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.&rdquo;</p>
<p>&ndash; Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction</p>
</blockquote>
<p>Imagine if you had a set policy that dictated the actions you would take from work to home. In this example, assume the policy is not an optimal policy. One day, you decide to take a left at a particular intersection rather than going forward. After that, you follow your policy as described. If this decision ultimately resulted in you arriving home sooner, you would probably update your policy to always take that left. This intuition describes a result of the <strong>policy improvement theorem</strong>.</p>
<p>Let \(\pi\) and \(\pi&rsquo;\) be two deterministic policies where</p>
<p>\[
q_{\pi}(s, \pi&rsquo;(s)) \geq v_{\pi}(s),\quad \forall s.
\]</p>

    </div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/reinforcement-learning/">reinforcement learning</a>
  
</div>



    
      








  
  
    
  
  





  
  
  
    
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="https://ajdillhoff.github.io/">Alex Dillhoff</a></h5>
      <h6 class="card-subtitle">Senior Lecturer</h6>
      <p class="card-text" itemprop="description">&quot;If we understood the world, we would realize that there is a logic of harmony underlying its manifold apparent dissonances.&quot; - Jean Sibelius</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="/#contact" >
              <i class="fas fa-envelope"></i>
            </a>
          </li>
        
          
          
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://scholar.google.co.uk/citations?user=UlLhCtkAAAAJ" target="_blank" rel="noopener">
              <i class="ai ai-google-scholar"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/ajdillhoff" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
    

    

    

  </div>
</article>

      

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
      

      
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    
    <script id="dsq-count-scr" src="//themefisher-template.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    

    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academia.min.5328943609f83580d0f13f6d5b5f2587.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  <div class="container">
    <div class="row align-items-center">
      <div class="col-md-6 mb-4 mb-md-0">
        
        <p class="mb-0">
          Copyright Â© 2023 &middot; 
          Powered by
          <a href="https://gethugothemes.com" target="_blank" rel="noopener">Gethugothemes</a>
        </p>
      </div>
      <div class="col-md-6">
        <ul class="list-inline network-icon text-right mb-0">
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="list-inline-item">
            <a href="https://github.com/ajdillhoff" target="_blank" rel="noopener" title="My GitHub"><i class="fab fa-github" aria-hidden="true"></i></a>
          </li>
          
          
          
          
          
          
          
          
          
          
          <li class="list-inline-item">
            <a href="https://github.com/ajdillhoff" target="_blank" rel="noopener" title="My GitHub"><i class="ai ai-google-scholar" aria-hidden="true"></i></a>
          </li>
          
        </ul>
      </div>
    </div>
  </div>
</footer>
  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
