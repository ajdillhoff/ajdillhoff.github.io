<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academia 4.3.1">
  <meta name="theme-name" content="academia-hugo"/>

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Alex Dillhoff">

  
  
  
    
  
  <meta name="description" content="
Table of Contents

Overview of Nsight
Getting Started with Nsight
Case Study: Matrix Multiplication
Tips and Best Practices
OCL Notes



Overview of Nsight
NVIDIA NSight Compute is a profiling tool for CUDA kernels. It features an expert system that can help you identify performance bottlenecks in your code. It is essential for methodically optimizing your code. These notes will cover the basics of using Nsight Compute to profile your CUDA applications.">

  
  <link rel="alternate" hreflang="en-us" href="https://ajdillhoff.github.io/notes/profiling_cuda_applications/">

  


  

  
  
  
  <meta name="theme-color" content="#fc6f5c">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Open+Sans|Roboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academia.min.b246554d075350d61b44c126dfbcbe05.css">

  
    
    
    
    
      
    
    
    
    <link rel="stylesheet" href="/css/academia.a75a9b8a9a725a2157c0c5b929a3d18b.css">
  

  
  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-123456-78', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://ajdillhoff.github.io/notes/profiling_cuda_applications/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Alex Dillhoff">
  <meta property="og:url" content="https://ajdillhoff.github.io/notes/profiling_cuda_applications/">
  <meta property="og:title" content="Profiling CUDA Applications | Alex Dillhoff">
  <meta property="og:description" content="
Table of Contents

Overview of Nsight
Getting Started with Nsight
Case Study: Matrix Multiplication
Tips and Best Practices
OCL Notes



Overview of Nsight
NVIDIA NSight Compute is a profiling tool for CUDA kernels. It features an expert system that can help you identify performance bottlenecks in your code. It is essential for methodically optimizing your code. These notes will cover the basics of using Nsight Compute to profile your CUDA applications."><meta property="og:image" content="https://ajdillhoff.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://ajdillhoff.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2024-01-15T14:48:00-06:00">
  
  <meta property="article:modified_time" content="2024-01-15T14:48:00-06:00">
  

  


  





  <title>Profiling CUDA Applications | Alex Dillhoff</title>

  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>



</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Alex Dillhoff</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation"><span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">
      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/notes/"><span>Brain Dump</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

      

        

        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>
    </div>
  </div>
</nav>


  <article class="article py-5" itemscope itemtype="http://schema.org/Article">

  












    

    
    
    
    <div class="article-container py-3">
      <h1 itemprop="name">Profiling CUDA Applications</h1>

      

      
      



<meta content="2024-01-15 14:48:00 -0600 CST" itemprop="datePublished">
<meta content="2024-01-15 14:48:00 -0600 CST" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/alex-dillhoff/">Alex Dillhoff</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>Jan 15, 2024</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    5 min read
  </span>
  

  
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://ajdillhoff.github.io/notes/profiling_cuda_applications/&amp;text=Profiling%20CUDA%20Applications" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://ajdillhoff.github.io/notes/profiling_cuda_applications/&amp;t=Profiling%20CUDA%20Applications" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Profiling%20CUDA%20Applications&amp;body=https://ajdillhoff.github.io/notes/profiling_cuda_applications/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://ajdillhoff.github.io/notes/profiling_cuda_applications/&amp;title=Profiling%20CUDA%20Applications" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Profiling%20CUDA%20Applications%20https://ajdillhoff.github.io/notes/profiling_cuda_applications/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://ajdillhoff.github.io/notes/profiling_cuda_applications/&amp;title=Profiling%20CUDA%20Applications" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

      














      
      
    </div>
  </div>
</div>

  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      <div class="ox-hugo-toc toc">
<div class="heading">Table of Contents</div>
<ul>
<li><a href="#overview-of-nsight">Overview of Nsight</a></li>
<li><a href="#getting-started-with-nsight">Getting Started with Nsight</a></li>
<li><a href="#case-study-matrix-multiplication">Case Study: Matrix Multiplication</a></li>
<li><a href="#tips-and-best-practices">Tips and Best Practices</a></li>
<li><a href="#ocl-notes">OCL Notes</a></li>
</ul>
</div>
<!--endtoc-->
<h2 id="overview-of-nsight">Overview of Nsight</h2>
<p>NVIDIA NSight Compute is a profiling tool for CUDA kernels. It features an expert system that can help you identify performance bottlenecks in your code. It is essential for methodically optimizing your code. These notes will cover the basics of using Nsight Compute to profile your CUDA applications.</p>
<h2 id="getting-started-with-nsight">Getting Started with Nsight</h2>
<h3 id="profiling-our-first-program">Profiling our first program</h3>
<p>In Lab 0, you implemented a vector addition kernel that is <em>embarrassingly parallel</em>. We will now use Nsight to profile its performance. Realistically, there is not much we can do to increase the performance of this kernel, but it will still help us understand the information that Nsight gives. To profile the application, simply launch <code>ncu</code> with your application.</p>
<p><code>ncu ./build/main</code></p>
<p>Depending on where you are running this program, it may be necessary to launch it with <code>sudo</code>. If everything was successful, it will output something similar to the following:</p>
<p><strong>Nsight Output</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>vec_add(float *, float *, float *, int), 2024-Jan-16 10:42:52, Context 1, Stream 7
</span></span><span style="display:flex;"><span>  Section: GPU Speed Of Light Throughput
</span></span><span style="display:flex;"><span>  ---------------------------------------------------------------------- --------------- ------------------------------
</span></span><span style="display:flex;"><span>  DRAM Frequency                                                           cycle/nsecond                           5.71
</span></span><span style="display:flex;"><span>  SM Frequency                                                             cycle/nsecond                           1.15
</span></span><span style="display:flex;"><span>  Elapsed Cycles                                                                   cycle                          3,279
</span></span><span style="display:flex;"><span>  Memory [%]                                                                           %                           7.54
</span></span><span style="display:flex;"><span>  DRAM Throughput                                                                      %                           7.54
</span></span><span style="display:flex;"><span>  Duration                                                                       usecond                           2.85
</span></span><span style="display:flex;"><span>  L1/TEX Cache Throughput                                                              %                           4.32
</span></span><span style="display:flex;"><span>  L2 Cache Throughput                                                                  %                           4.86
</span></span><span style="display:flex;"><span>  SM Active Cycles                                                                 cycle                         623.58
</span></span><span style="display:flex;"><span>  Compute (SM) [%]                                                                     %                           0.82
</span></span><span style="display:flex;"><span>  ---------------------------------------------------------------------- --------------- ------------------------------
</span></span><span style="display:flex;"><span>  WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full
</span></span><span style="display:flex;"><span>        waves across all SMs. Look at Launch Statistics for more details.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  Section: Launch Statistics
</span></span><span style="display:flex;"><span>  ---------------------------------------------------------------------- --------------- ------------------------------
</span></span><span style="display:flex;"><span>  Block Size                                                                                                        256
</span></span><span style="display:flex;"><span>  Function Cache Configuration                                                                  cudaFuncCachePreferNone
</span></span><span style="display:flex;"><span>  Grid Size                                                                                                          16
</span></span><span style="display:flex;"><span>  Registers Per Thread                                                   register/thread                             16
</span></span><span style="display:flex;"><span>  Shared Memory Configuration Size                                                 Kbyte                           8.19
</span></span><span style="display:flex;"><span>  Driver Shared Memory Per Block                                             Kbyte/block                           1.02
</span></span><span style="display:flex;"><span>  Dynamic Shared Memory Per Block                                             byte/block                              0
</span></span><span style="display:flex;"><span>  Static Shared Memory Per Block                                              byte/block                              0
</span></span><span style="display:flex;"><span>  Threads                                                                         thread                          4,096
</span></span><span style="display:flex;"><span>  Waves Per SM                                                                                                     0.07
</span></span><span style="display:flex;"><span>  ---------------------------------------------------------------------- --------------- ------------------------------
</span></span><span style="display:flex;"><span>  WRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU&#39;s 38
</span></span><span style="display:flex;"><span>        multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
</span></span><span style="display:flex;"><span>        concurrently with other workloads, consider reducing the block size to have at least one block per
</span></span><span style="display:flex;"><span>        multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
</span></span><span style="display:flex;"><span>        Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
</span></span><span style="display:flex;"><span>        description for more details on launch configurations.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  Section: Occupancy
</span></span><span style="display:flex;"><span>  ---------------------------------------------------------------------- --------------- ------------------------------
</span></span><span style="display:flex;"><span>  Block Limit SM                                                                   block                             16
</span></span><span style="display:flex;"><span>  Block Limit Registers                                                            block                             16
</span></span><span style="display:flex;"><span>  Block Limit Shared Mem                                                           block                            100
</span></span><span style="display:flex;"><span>  Block Limit Warps                                                                block                              6
</span></span><span style="display:flex;"><span>  Theoretical Active Warps per SM                                                   warp                             48
</span></span><span style="display:flex;"><span>  Theoretical Occupancy                                                                %                            100
</span></span><span style="display:flex;"><span>  Achieved Occupancy                                                                   %                          15.85
</span></span><span style="display:flex;"><span>  Achieved Active Warps Per SM                                                      warp                           7.61
</span></span><span style="display:flex;"><span>  ---------------------------------------------------------------------- --------------- ------------------------------
</span></span><span style="display:flex;"><span>  WRN   This kernel&#39;s theoretical occupancy is not impacted by any block limit. The difference between calculated
</span></span><span style="display:flex;"><span>        theoretical (100.0%) and measured achieved occupancy (15.9%) can be the result of warp scheduling overheads
</span></span><span style="display:flex;"><span>        or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
</span></span><span style="display:flex;"><span>        as well as across blocks of the same kernel. See the CUDA Best Practices Guide
</span></span><span style="display:flex;"><span>        (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
</span></span><span style="display:flex;"><span>        optimizing occupancy.
</span></span></code></pre></div><h3 id="viewing-results-in-the-gui">Viewing Results in the GUI</h3>
<p>Nsight comes with both CLI and GUI clients. It is recommended to parse the information from the GUI. The GUI can launch programs both locally and remotely. It can also display the result of a previous launch. To generate a profiling report for our vector addition kernel, run the following command:</p>
<p><code>ncu -o vec_add_profile ./build/main</code></p>
<p>The argument after <code>-o</code> is the name of the output file. Open Nsight Compute and load the saved file. It should look something like this.</p>






<figure>

<img src="/ox-hugo/2024-01-16_11-59-20_screenshot.png" alt="&lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Nsight Compute GUI output" >



<figcaption data-pre="Figure " data-post=":" >
  
  <p>
    <span class="figure-number">Figure 1: </span>Nsight Compute GUI output
    
    
    
  </p> 
</figcaption>

</figure>

<p>This basic report only includes three sections: GPU Speed of Light, Launch Statistics, and Occupancy Analysis. We will go over each of these sections in detail.</p>
<h3 id="gpu-speed-of-light">GPU Speed of Light</h3>
<p>This section displays high level aspects of your kernel. The main metrics report what your application is doing relative to peak performance. Sparing the full details of the documentation, the most important metrics are:</p>
<ul>
<li>Duration: The total time spent executing the kernel. This is the most important metric for performance.</li>
<li>SM <code>[%]</code>: The relative throughput of the SMs as compared to the theoretical maximum.</li>
<li>Memory <code>[%]</code>: The relative throughput of the memory as compared to the theoretical maximum.</li>
</ul>
<p><strong>Do not get lost in the numbers!</strong></p>
<p>Remember that this tool is simply reporting facts about your kernel. Take care not to misinterpret the data. In the run from above, the kernel throughput is only 0.85%. There are a number of reasons as to why this number is so low.</p>
<ul>
<li>Latency Issues: The kernel may have to wait for memory operations, resulting in a low throughput.</li>
<li>Workload Characteristics: Your particular kernel may not need to do much work, resulting in a low throughput.</li>
</ul>
<h3 id="launch-statistics">Launch Statistics</h3>
<p>This section shows us the launch configuration that was used for this kernel. In our earlier programs, we may set these manually for testing. Later on, we will want our programs to adapt to changing input sizes, so these statistics will becomes more useful.</p>
<p>More importantly, this shows you the resource usage per block.</p>
<p>If you are profiling an application for which you are not familiar with the code, it is convenient to know the grid and block sizes that were used when launching the kernel.</p>
<h3 id="occupancy-analysis">Occupancy Analysis</h3>
<h3 id="memory-workload-analysis">Memory Workload Analysis</h3>
<h2 id="case-study-matrix-multiplication">Case Study: Matrix Multiplication</h2>
<h2 id="tips-and-best-practices">Tips and Best Practices</h2>
<ul>
<li>Do not confuse high throughput for high performance. Throughput is a measure of how much work is being done, not how fast it is being done.</li>
<li>Using a larger grid size is not always better. More grids introduce more overhead and can lead to lower performance.</li>
</ul>
<h2 id="ocl-notes">OCL Notes</h2>
<ul>
<li>Analysis Driven Optimization</li>
<li>Understanding Performance Limiters</li>
<li>Metrics Review</li>
<li>Memory Bound Analysis</li>
<li>Compute Bound Analysis</li>
</ul>
<p><strong>Goals</strong>
Make efficient use of memory subsystem
Expose enough parallelism to hide latency</p>
<h3 id="analysis-driven-optimization">Analysis Driven Optimization</h3>
<p>Cyclical process</p>
<ol>
<li>Profile</li>
<li>Determine Limiter</li>
<li>Inspect</li>
<li>Optimize</li>
</ol>
<p>Determine if memory or compute bound. If neither, analyze where the latency is coming from.</p>
<h3 id="metrics">Metrics</h3>
<p><strong>Latency</strong></p>
<ul>
<li>sm efficiency</li>
</ul>
<p><strong>Memory</strong></p>
<ul>
<li>dram utilization</li>
<li>L2 utilization</li>
<li>shared utilization</li>
</ul>
<p><strong>Compute</strong></p>
<ul>
<li>DP utilization</li>
<li>SP utilization</li>
<li>HP utilization</li>
<li>TC utilization</li>
<li>Integer</li>
</ul>

    </div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/gpgpu/">gpgpu</a>
  
  <a class="badge badge-light" href="/tags/cuda/">CUDA</a>
  
</div>



    
      








  
  
    
  
  





  
  
  
    
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="https://ajdillhoff.github.io/">Alex Dillhoff</a></h5>
      <h6 class="card-subtitle">Senior Lecturer</h6>
      <p class="card-text" itemprop="description">&quot;If we understood the world, we would realize that there is a logic of harmony underlying its manifold apparent dissonances.&quot; - Jean Sibelius</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="/#contact" >
              <i class="fas fa-envelope"></i>
            </a>
          </li>
        
          
          
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://scholar.google.co.uk/citations?user=UlLhCtkAAAAJ" target="_blank" rel="noopener">
              <i class="ai ai-google-scholar"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/ajdillhoff" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/notes/gpu_performance_basics/">GPU Performance Basics</a></li>
          
          <li><a href="/notes/cuda_memory_architecture/">CUDA Memory Architecture</a></li>
          
          <li><a href="/notes/cuda_architecture/">CUDA Architecture</a></li>
          
          <li><a href="/notes/multidimensional_grids_and_data/">Multidimensional Grids and Data</a></li>
          
          <li><a href="/notes/heterogeneous_data_parallel_computing/">Heterogeneous Data Parallel Computing</a></li>
          
        </ul>
      </div>
      
    

    

    

  </div>
</article>

      

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
      

      
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    

    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academia.min.6c2ba2801d406881b3c2277043cedd76.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  <div class="container">
    <div class="row align-items-center">
      <div class="col-md-6 mb-4 mb-md-0">
        
        <p class="mb-0">
          Copyright © 2024 &middot; 
          Powered by
          <a href="https://gethugothemes.com" target="_blank" rel="noopener">Gethugothemes</a>
        </p>
      </div>
      <div class="col-md-6">
        <ul class="list-inline network-icon text-right mb-0">
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="list-inline-item">
            <a href="https://github.com/ajdillhoff" target="_blank" rel="noopener" title="My GitHub"><i class="fab fa-github" aria-hidden="true"></i></a>
          </li>
          
          
          
          
          
          
          
          
          
          
          <li class="list-inline-item">
            <a href="https://github.com/ajdillhoff" target="_blank" rel="noopener" title="My GitHub"><i class="ai ai-google-scholar" aria-hidden="true"></i></a>
          </li>
          
        </ul>
      </div>
    </div>
  </div>
</footer>
  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
