<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academia 4.3.1">
  <meta name="theme-name" content="academia-hugo"/>

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Alex Dillhoff">

  
  
  
    
  
  <meta name="description" content="
Table of Contents

Introduction
Text Preprocessing
Tasks
Models
Perplexity



Introduction

Text Preprocessing

Character-level tokenization
Word-level tokenization
Subword tokenization
Stopwords
Batching
Padding


Unsupervised Pre-Training

Autoregression
BERT loss


Tasks

Text Classification
Named Entity Recognition
Question Answering
Summarization
Translation
Text Generation



Text Preprocessing
Text preprocessing is an essential step in NLP that involves cleaning and transforming unstructured text data to prepare it for analysis. Some common text preprocessing techniques include:

Expanding contractions (e.g., &ldquo;don&rsquo;t&rdquo; to &ldquo;do not&rdquo;) [7]
Lowercasing text[7]
Removing punctuations[7]
Removing words and digits containing digits[7]
Removing stopwords (common words that do not carry much meaning) [7]
Rephrasing text[7]
Stemming and Lemmatization (reducing words to their root forms) [7]

Common Tokenizers
Tokenization is the process of breaking a stream of textual data into words, terms, sentences, symbols, or other meaningful elements called tokens. Some common tokenizers used in NLP include:">

  
  <link rel="alternate" hreflang="en-us" href="https://ajdillhoff.github.io/notes/natural_language_processing/">

  


  

  
  
  
  <meta name="theme-color" content="#fc6f5c">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Open+Sans|Roboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academia.min.b246554d075350d61b44c126dfbcbe05.css">

  
    
    
    
    
      
    
    
    
    <link rel="stylesheet" href="/css/academia.a75a9b8a9a725a2157c0c5b929a3d18b.css">
  

  
  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-123456-78', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://ajdillhoff.github.io/notes/natural_language_processing/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Alex Dillhoff">
  <meta property="og:url" content="https://ajdillhoff.github.io/notes/natural_language_processing/">
  <meta property="og:title" content="Natural Language Processing | Alex Dillhoff">
  <meta property="og:description" content="
Table of Contents

Introduction
Text Preprocessing
Tasks
Models
Perplexity



Introduction

Text Preprocessing

Character-level tokenization
Word-level tokenization
Subword tokenization
Stopwords
Batching
Padding


Unsupervised Pre-Training

Autoregression
BERT loss


Tasks

Text Classification
Named Entity Recognition
Question Answering
Summarization
Translation
Text Generation



Text Preprocessing
Text preprocessing is an essential step in NLP that involves cleaning and transforming unstructured text data to prepare it for analysis. Some common text preprocessing techniques include:

Expanding contractions (e.g., &ldquo;don&rsquo;t&rdquo; to &ldquo;do not&rdquo;) [7]
Lowercasing text[7]
Removing punctuations[7]
Removing words and digits containing digits[7]
Removing stopwords (common words that do not carry much meaning) [7]
Rephrasing text[7]
Stemming and Lemmatization (reducing words to their root forms) [7]

Common Tokenizers
Tokenization is the process of breaking a stream of textual data into words, terms, sentences, symbols, or other meaningful elements called tokens. Some common tokenizers used in NLP include:"><meta property="og:image" content="https://ajdillhoff.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://ajdillhoff.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2023-04-23T00:00:00-05:00">
  
  <meta property="article:modified_time" content="2023-04-23T00:00:00-05:00">
  

  


  





  <title>Natural Language Processing | Alex Dillhoff</title>

  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>



</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Alex Dillhoff</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation"><span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">
      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/notes/"><span>Brain Dump</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

      

        

        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>
    </div>
  </div>
</nav>


  <article class="article py-5" itemscope itemtype="http://schema.org/Article">

  












    

    
    
    
    <div class="article-container py-3">
      <h1 itemprop="name">Natural Language Processing</h1>

      

      
      



<meta content="2023-04-23 00:00:00 -0500 CDT" itemprop="datePublished">
<meta content="2023-04-23 00:00:00 -0500 CDT" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/alex-dillhoff/">Alex Dillhoff</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>Apr 23, 2023</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    3 min read
  </span>
  

  
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://ajdillhoff.github.io/notes/natural_language_processing/&amp;text=Natural%20Language%20Processing" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://ajdillhoff.github.io/notes/natural_language_processing/&amp;t=Natural%20Language%20Processing" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Natural%20Language%20Processing&amp;body=https://ajdillhoff.github.io/notes/natural_language_processing/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://ajdillhoff.github.io/notes/natural_language_processing/&amp;title=Natural%20Language%20Processing" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Natural%20Language%20Processing%20https://ajdillhoff.github.io/notes/natural_language_processing/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://ajdillhoff.github.io/notes/natural_language_processing/&amp;title=Natural%20Language%20Processing" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

      














      
      
    </div>
  </div>
</div>

  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      <div class="ox-hugo-toc toc">
<div class="heading">Table of Contents</div>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#text-preprocessing">Text Preprocessing</a></li>
<li><a href="#tasks">Tasks</a></li>
<li><a href="#models">Models</a></li>
<li><a href="#perplexity">Perplexity</a></li>
</ul>
</div>
<!--endtoc-->
<h2 id="introduction">Introduction</h2>
<ul>
<li>Text Preprocessing
<ul>
<li>Character-level tokenization</li>
<li>Word-level tokenization</li>
<li>Subword tokenization</li>
<li>Stopwords</li>
<li>Batching</li>
<li>Padding</li>
</ul>
</li>
<li>Unsupervised Pre-Training
<ul>
<li>Autoregression</li>
<li>BERT loss</li>
</ul>
</li>
<li>Tasks
<ul>
<li>Text Classification</li>
<li>Named Entity Recognition</li>
<li>Question Answering</li>
<li>Summarization</li>
<li>Translation</li>
<li>Text Generation</li>
</ul>
</li>
</ul>
<h2 id="text-preprocessing">Text Preprocessing</h2>
<p>Text preprocessing is an essential step in NLP that involves cleaning and transforming unstructured text data to prepare it for analysis. Some common text preprocessing techniques include:</p>
<ul>
<li>Expanding contractions (e.g., &ldquo;don&rsquo;t&rdquo; to &ldquo;do not&rdquo;) [7]</li>
<li>Lowercasing text[7]</li>
<li>Removing punctuations[7]</li>
<li>Removing words and digits containing digits[7]</li>
<li>Removing stopwords (common words that do not carry much meaning) [7]</li>
<li>Rephrasing text[7]</li>
<li>Stemming and Lemmatization (reducing words to their root forms) [7]</li>
</ul>
<h3 id="common-tokenizers">Common Tokenizers</h3>
<p>Tokenization is the process of breaking a stream of textual data into words, terms, sentences, symbols, or other meaningful elements called tokens. Some common tokenizers used in NLP include:</p>
<ol>
<li><strong><strong>Whitespace Tokenizer</strong></strong>: Splits text based on whitespace characters (e.g., spaces, tabs, and newlines) [2].</li>
<li><strong><strong>NLTK Tokenizer</strong></strong>: A popular Python library that provides various tokenization functions, including word and sentence tokenization[1].</li>
<li><strong><strong>SpaCy Tokenizer</strong></strong>: Another popular Python library for NLP that offers a fast and efficient tokenizer, which can handle large documents and is customizable[5].</li>
<li><strong><strong>WordPiece Tokenizer</strong></strong>: A subword tokenizer used in models like BERT, which breaks text into smaller subword units to handle out-of-vocabulary words more effectively[3].</li>
<li><strong><strong>Byte Pair Encoding (BPE) Tokenizer</strong></strong>: A subword tokenizer that iteratively merges the most frequent character pairs in the text, resulting in a vocabulary of subword units[12].</li>
<li><strong><strong>SentencePiece Tokenizer</strong></strong>: A library that provides both BPE and unigram-based subword tokenization, which can handle multiple languages and does not rely on whitespace for tokenization[6].</li>
</ol>
<p>These tokenizers differ in the way they split text into tokens and handle language-specific considerations, such as handling out-of-vocabulary words, dealing with punctuation, and managing whitespace characters. The choice of tokenizer depends on the specific NLP task and the characteristics of the text data being processed.</p>
<p>Citations:
[1] <a href="https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/" target="_blank" rel="noopener noreferrer">https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/</a>
[2] <a href="https://neptune.ai/blog/tokenization-in-nlp" target="_blank" rel="noopener noreferrer">https://neptune.ai/blog/tokenization-in-nlp</a>
[3] <a href="https://towardsdatascience.com/comparing-transformer-tokenizers-686307856955" target="_blank" rel="noopener noreferrer">https://towardsdatascience.com/comparing-transformer-tokenizers-686307856955</a>
[4] <a href="https://www.analyticsvidhya.com/blog/2021/09/essential-text-pre-processing-techniques-for-nlp/" target="_blank" rel="noopener noreferrer">https://www.analyticsvidhya.com/blog/2021/09/essential-text-pre-processing-techniques-for-nlp/</a>
[5] <a href="https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/" target="_blank" rel="noopener noreferrer">https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/</a>
[6] <a href="https://www.reddit.com/r/MachineLearning/comments/rprmq3/d_sentencepiece_wordpiece_bpe_which_tokenizer_is/" target="_blank" rel="noopener noreferrer">https://www.reddit.com/r/MachineLearning/comments/rprmq3/d_sentencepiece_wordpiece_bpe_which_tokenizer_is/</a>
[7] <a href="https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/" target="_blank" rel="noopener noreferrer">https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/</a>
[8] <a href="https://towardsdatascience.com/top-5-word-tokenizers-that-every-nlp-data-scientist-should-know-45cc31f8e8b9" target="_blank" rel="noopener noreferrer">https://towardsdatascience.com/top-5-word-tokenizers-that-every-nlp-data-scientist-should-know-45cc31f8e8b9</a>
[9] <a href="https://www.projectpro.io/recipes/explain-difference-between-word-tokenizer" target="_blank" rel="noopener noreferrer">https://www.projectpro.io/recipes/explain-difference-between-word-tokenizer</a>
[10] <a href="https://www.telusinternational.com/insights/ai-data/article/what-is-text-mining" target="_blank" rel="noopener noreferrer">https://www.telusinternational.com/insights/ai-data/article/what-is-text-mining</a>
[11] <a href="https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4" target="_blank" rel="noopener noreferrer">https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4</a>
[12] <a href="https://towardsdatascience.com/a-comprehensive-guide-to-subword-tokenisers-4bbd3bad9a7c" target="_blank" rel="noopener noreferrer">https://towardsdatascience.com/a-comprehensive-guide-to-subword-tokenisers-4bbd3bad9a7c</a>
[13] <a href="https://towardsdatascience.com/text-preprocessing-in-natural-language-processing-using-python-6113ff5decd8" target="_blank" rel="noopener noreferrer">https://towardsdatascience.com/text-preprocessing-in-natural-language-processing-using-python-6113ff5decd8</a>
[14] <a href="https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/" target="_blank" rel="noopener noreferrer">https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/</a>
[15] <a href="https://docs.tamr.com/new/docs/tokenizers-and-similarity-functions" target="_blank" rel="noopener noreferrer">https://docs.tamr.com/new/docs/tokenizers-and-similarity-functions</a>
[16] <a href="https://pitt.libguides.com/textmining/preprocessing" target="_blank" rel="noopener noreferrer">https://pitt.libguides.com/textmining/preprocessing</a>
[17] <a href="https://medium.com/@ajay_khanna/tokenization-techniques-in-natural-language-processing-67bb22088c75" target="_blank" rel="noopener noreferrer">https://medium.com/@ajay_khanna/tokenization-techniques-in-natural-language-processing-67bb22088c75</a>
[18] <a href="https://datascience.stackexchange.com/questions/75304/bpe-vs-wordpiece-tokenization-when-to-use-which" target="_blank" rel="noopener noreferrer">https://datascience.stackexchange.com/questions/75304/bpe-vs-wordpiece-tokenization-when-to-use-which</a>
[19] <a href="https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing</a>
[20] <a href="https://www.tokenex.com/blog/ab-what-is-nlp-natural-language-processing-tokenization/" target="_blank" rel="noopener noreferrer">https://www.tokenex.com/blog/ab-what-is-nlp-natural-language-processing-tokenization/</a>
[21] <a href="https://hungsblog.de/en/technology/learnings/difference-between-the-tokenizer-and-the-pretrainedtokenizer-class/" target="_blank" rel="noopener noreferrer">https://hungsblog.de/en/technology/learnings/difference-between-the-tokenizer-and-the-pretrainedtokenizer-class/</a>
[22] <a href="https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html" target="_blank" rel="noopener noreferrer">https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html</a>
[23] <a href="https://medium.com/nlplanet/two-minutes-nlp-a-taxonomy-of-tokenization-methods-60e330aacad3" target="_blank" rel="noopener noreferrer">https://medium.com/nlplanet/two-minutes-nlp-a-taxonomy-of-tokenization-methods-60e330aacad3</a>
[24] <a href="https://www.geeksforgeeks.org/text-preprocessing-in-python-set-1/" target="_blank" rel="noopener noreferrer">https://www.geeksforgeeks.org/text-preprocessing-in-python-set-1/</a>
[25] <a href="https://medium.com/@utkarsh.kant/tokenization-a-complete-guide-3f2dd56c0682" target="_blank" rel="noopener noreferrer">https://medium.com/@utkarsh.kant/tokenization-a-complete-guide-3f2dd56c0682</a>
[26] <a href="https://stackoverflow.com/questions/380455/looking-for-a-clear-definition-of-what-a-tokenizer-parser-and-lexers-are" target="_blank" rel="noopener noreferrer">https://stackoverflow.com/questions/380455/looking-for-a-clear-definition-of-what-a-tokenizer-parser-and-lexers-are</a>
[27] <a href="https://blog.floydhub.com/tokenization-nlp/" target="_blank" rel="noopener noreferrer">https://blog.floydhub.com/tokenization-nlp/</a>
[28] <a href="https://medium.com/product-ai/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908" target="_blank" rel="noopener noreferrer">https://medium.com/product-ai/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908</a>
[29] <a href="https://pub.towardsai.net/in-depth-tokenization-methods-of-14-nlp-libraries-with-python-example-297ecdd14c1" target="_blank" rel="noopener noreferrer">https://pub.towardsai.net/in-depth-tokenization-methods-of-14-nlp-libraries-with-python-example-297ecdd14c1</a>
[30] <a href="https://datascience.stackexchange.com/questions/88680/what-is-the-difference-between-countvectorizer-and-tokenizer-or-are-they-the" target="_blank" rel="noopener noreferrer">https://datascience.stackexchange.com/questions/88680/what-is-the-difference-between-countvectorizer-and-tokenizer-or-are-they-the</a>
[31] <a href="https://www.freecodecamp.org/news/train-algorithms-from-scratch-with-hugging-face/" target="_blank" rel="noopener noreferrer">https://www.freecodecamp.org/news/train-algorithms-from-scratch-with-hugging-face/</a>
[32] <a href="https://exchange.scale.com/public/blogs/preprocessing-techniques-in-nlp-a-guide" target="_blank" rel="noopener noreferrer">https://exchange.scale.com/public/blogs/preprocessing-techniques-in-nlp-a-guide</a>
[33] <a href="https://huggingface.co/docs/transformers/tokenizer_summary" target="_blank" rel="noopener noreferrer">https://huggingface.co/docs/transformers/tokenizer_summary</a>
[34] <a href="https://blog.octanove.org/guide-to-subword-tokenization/" target="_blank" rel="noopener noreferrer">https://blog.octanove.org/guide-to-subword-tokenization/</a>
[35] <a href="https://www.enjoyalgorithms.com/blog/text-data-pre-processing-techniques-in-ml/" target="_blank" rel="noopener noreferrer">https://www.enjoyalgorithms.com/blog/text-data-pre-processing-techniques-in-ml/</a>
[36] <a href="https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/" target="_blank" rel="noopener noreferrer">https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/</a></p>
<h2 id="tasks">Tasks</h2>
<h3 id="text-classification">Text Classification</h3>
<p>Commonly seen in the form of sentiment analysis, where the objective is to classify whether some input text is positive or negative. Document classification, in which documents are identified by their content, is also useful.</p>
<h3 id="named-entity-recognition">Named Entity Recognition</h3>
<p>Extract important nouns from a body of text.</p>
<h3 id="question-answering">Question Answering</h3>
<h3 id="summarization">Summarization</h3>
<h3 id="translation">Translation</h3>
<h3 id="text-generation">Text Generation</h3>
<p>Generate text from a prompt. This could be in the form of a simple question or some initial dialog. This is also seen in tools like GitHub Co-Pilot to generate code based on contextual code in the same project.</p>
<h2 id="models">Models</h2>
<p>Discuss GPT2</p>
<h2 id="perplexity">Perplexity</h2>
<p>A measure of confidence of a language model. A naive model may predict a word by randomly selecting any of the \(N\) words in its vocabulary. As the model is optimized and the distribution of possible sequences is acquired, the perplexity decreases.</p>

    </div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/deep-learning/">deep learning</a>
  
  <a class="badge badge-light" href="/tags/nlp/">NLP</a>
  
</div>



    
      








  
  
    
  
  





  
  
  
    
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="https://ajdillhoff.github.io/">Alex Dillhoff</a></h5>
      <h6 class="card-subtitle">Senior Lecturer</h6>
      <p class="card-text" itemprop="description">&quot;If we understood the world, we would realize that there is a logic of harmony underlying its manifold apparent dissonances.&quot; - Jean Sibelius</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="/#contact" >
              <i class="fas fa-envelope"></i>
            </a>
          </li>
        
          
          
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://scholar.google.co.uk/citations?user=UlLhCtkAAAAJ" target="_blank" rel="noopener">
              <i class="ai ai-google-scholar"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/ajdillhoff" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/notes/transformers/">Transformers</a></li>
          
          <li><a href="/notes/long_short_term_memory/">Long Short-Term Memory</a></li>
          
          <li><a href="/notes/recurrent_neural_networks/">Recurrent Neural Networks</a></li>
          
          <li><a href="/notes/optimization_for_deep_learning/">Optimization for Deep Learning</a></li>
          
          <li><a href="/notes/convolutional_neural_networks/">Convolutional Neural Networks</a></li>
          
        </ul>
      </div>
      
    

    

    

  </div>
</article>

      

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
      

      
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    

    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academia.min.6c2ba2801d406881b3c2277043cedd76.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  <div class="container">
    <div class="row align-items-center">
      <div class="col-md-6 mb-4 mb-md-0">
        
        <p class="mb-0">
          Copyright Â© 2024 &middot; 
          Powered by
          <a href="https://gethugothemes.com" target="_blank" rel="noopener">Gethugothemes</a>
        </p>
      </div>
      <div class="col-md-6">
        <ul class="list-inline network-icon text-right mb-0">
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="list-inline-item">
            <a href="https://github.com/ajdillhoff" target="_blank" rel="noopener" title="My GitHub"><i class="fab fa-github" aria-hidden="true"></i></a>
          </li>
          
          
          
          
          
          
          
          
          
          
          <li class="list-inline-item">
            <a href="https://github.com/ajdillhoff" target="_blank" rel="noopener" title="My GitHub"><i class="ai ai-google-scholar" aria-hidden="true"></i></a>
          </li>
          
        </ul>
      </div>
    </div>
  </div>
</footer>
  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
