<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academia 4.3.1">
  <meta name="theme-name" content="academia-hugo"/>

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Alex Dillhoff">

  
  
  
    
  
  <meta name="description" content="Table of Contents Papers Evaluating Object Detection Methods Datasets An Incomplete History of Deep-Learning-based Object Detection Papers https://awesomeopensource.com/projects/object-detection Evaluating Object Detection Methods Object detection algorithms are evaluated using the mean of Average Precision (mAP) across all classes in the dataset.
Precision and recall are computed from the predictions and the ground truth. A sample and the model&rsquo;s prediction can either be positive or negative when it comes to classification. Either it belongs to a class or it does not.">

  
  <link rel="alternate" hreflang="en-us" href="https://ajdillhoff.github.io/notes/object_detection/">

  


  

  
  
  
  <meta name="theme-color" content="#fc6f5c">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Open+Sans|Roboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academia.min.b246554d075350d61b44c126dfbcbe05.css">

  
    
    
    
    
      
    
    
    
    <link rel="stylesheet" href="/css/academia.a75a9b8a9a725a2157c0c5b929a3d18b.css">
  

  
  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-123456-78', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://ajdillhoff.github.io/notes/object_detection/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Alex Dillhoff">
  <meta property="og:url" content="https://ajdillhoff.github.io/notes/object_detection/">
  <meta property="og:title" content="Object Detection | Alex Dillhoff">
  <meta property="og:description" content="Table of Contents Papers Evaluating Object Detection Methods Datasets An Incomplete History of Deep-Learning-based Object Detection Papers https://awesomeopensource.com/projects/object-detection Evaluating Object Detection Methods Object detection algorithms are evaluated using the mean of Average Precision (mAP) across all classes in the dataset.
Precision and recall are computed from the predictions and the ground truth. A sample and the model&rsquo;s prediction can either be positive or negative when it comes to classification. Either it belongs to a class or it does not."><meta property="og:image" content="https://ajdillhoff.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://ajdillhoff.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2022-04-18T00:00:00-05:00">
  
  <meta property="article:modified_time" content="2024-07-27T00:00:00&#43;00:00">
  

  


  





  <title>Object Detection | Alex Dillhoff</title>

  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>



</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Alex Dillhoff</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation"><span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">
      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/notes/"><span>Brain Dump</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

      

        

        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>
    </div>
  </div>
</nav>


  <article class="article py-5" itemscope itemtype="http://schema.org/Article">

  












    

    
    
    
    <div class="article-container py-3">
      <h1 itemprop="name">Object Detection</h1>

      

      
      



<meta content="2022-04-18 00:00:00 -0500 CDT" itemprop="datePublished">
<meta content="2024-07-27 00:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/alex-dillhoff/">Alex Dillhoff</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    <time>Jul 27, 2024</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    11 min read
  </span>
  

  
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://ajdillhoff.github.io/notes/object_detection/&amp;text=Object%20Detection" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://ajdillhoff.github.io/notes/object_detection/&amp;t=Object%20Detection" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Object%20Detection&amp;body=https://ajdillhoff.github.io/notes/object_detection/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://ajdillhoff.github.io/notes/object_detection/&amp;title=Object%20Detection" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Object%20Detection%20https://ajdillhoff.github.io/notes/object_detection/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://ajdillhoff.github.io/notes/object_detection/&amp;title=Object%20Detection" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

      














      
      
    </div>
  </div>
</div>

  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      <div class="ox-hugo-toc toc">
<div class="heading">Table of Contents</div>
<ul>
<li><a href="#papers">Papers</a></li>
<li><a href="#evaluating-object-detection-methods">Evaluating Object Detection Methods</a></li>
<li><a href="#datasets">Datasets</a></li>
<li><a href="#an-incomplete-history-of-deep-learning-based-object-detection">An Incomplete History of Deep-Learning-based Object Detection</a></li>
</ul>
</div>
<!--endtoc-->
<h2 id="papers">Papers</h2>
<ul>
<li><a href="https://awesomeopensource.com/projects/object-detection" target="_blank" rel="noopener noreferrer">https://awesomeopensource.com/projects/object-detection</a></li>
</ul>
<h2 id="evaluating-object-detection-methods">Evaluating Object Detection Methods</h2>
<p>Object detection algorithms are evaluated using the mean of Average Precision (mAP) across all classes in the dataset.</p>
<p>Precision and recall are computed from the predictions and the ground truth. A sample and the model&rsquo;s prediction can either be positive or negative when it comes to classification. Either it belongs to a class or it does not. The table below summarizes the outcomes between the model&rsquo;s prediction and the true underlying class.</p>
<p>
  <img src="/ox-hugo/2022-04-17_18-17-14_screenshot.png" alt="">

Object detection algorithms are evaluated using the mean of Average Precision (mAP) across all classes in the dataset.</p>
<p>Precision and recall are computed from the predictions and the ground truth. A sample and the model&rsquo;s prediction can either be positive or negative when it comes to classification. Either it belongs to a class or it does not. The table below summarizes the outcomes between the model&rsquo;s prediction and the true underlying class.</p>
<p><strong>Precision</strong></p>
<p>\[
\frac{TP}{TP + FP}
\]</p>
<p><strong>Recall</strong></p>
<p>\[
\frac{TP}{TP + FN}
\]</p>
<p>Object detection models predict a bounding box for a given class. A correct bounding box can be identified as one that has an Intersection-over-Union (IoU) score of &gt; 0.5.</p>






<figure>

<img src="/ox-hugo/2024-07-27_16-24-38_screenshot.png" alt="&lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;IoU Visualization (&lt;a href=&#34;#citeproc_bib_item_7&#34;&gt;Szeliski 2021&lt;/a&gt;)" >



<figcaption data-pre="Figure " data-post=":" >
  
  <p>
    <span class="figure-number">Figure 2: </span>IoU Visualization (<a href="#citeproc_bib_item_7">Szeliski 2021</a>)
    
    
    
  </p> 
</figcaption>

</figure>

<p>If you were to plot the precision versus recall of a single class, the area under the curve would be the average precision.</p>






<figure>

<img src="/ox-hugo/2024-07-27_16-32-40_screenshot.png" alt="&lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Average Precision Curves (Girshick, 2020)" >



<figcaption data-pre="Figure " data-post=":" >
  
  <p>
    <span class="figure-number">Figure 3: </span>Average Precision Curves (Girshick, 2020)
    
    
    
  </p> 
</figcaption>

</figure>

<p>The curve implicitly represents varying probability thresholds! As recall increases, precision will generally decrease. This reflects the fact that a model that recalls all input samples as a particular class will sure be misclassifying them. <strong>Keep in mind that recall by itself is not a measure of correctness.</strong> Ideally, the curve will be closer to the top right of the graph, indicating high precision and recall.</p>
<h2 id="datasets">Datasets</h2>
<ul>
<li><a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_blank" rel="noopener noreferrer">Pascal VOC</a></li>
<li><a href="https://cocodataset.org/#home" target="_blank" rel="noopener noreferrer">COCO</a></li>
</ul>
<h2 id="an-incomplete-history-of-deep-learning-based-object-detection">An Incomplete History of Deep-Learning-based Object Detection</h2>
<h3 id="rich-feature-hierarchies-for-accuracy-object-detection-and-semantic-segmentation">Rich Feature Hierarchies for Accuracy Object Detection and Semantic Segmentation (<a href="#citeproc_bib_item_2">Girshick et al. 2014</a>)</h3>
<blockquote>
<p>The last decade of progress on various visual recognition tasks has been based considerably on the use of SIFT and HOG (<a href="#citeproc_bib_item_2">Girshick et al. 2014</a>).</p>
</blockquote>
<p>This is one of the earliest papers to leverage deep learning for object detection. The overall approach is a piecewise one, where the CNN is only used for classification.</p>






<figure>

<img src="/ox-hugo/2024-07-27_17-11-41_screenshot.png" alt="&lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;System overview of the R-CNN approach (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Girshick et al. 2014&lt;/a&gt;)" >



<figcaption data-pre="Figure " data-post=":" >
  
  <p>
    <span class="figure-number">Figure 4: </span>System overview of the R-CNN approach (<a href="#citeproc_bib_item_2">Girshick et al. 2014</a>)
    
    
    
  </p> 
</figcaption>

</figure>

<p><strong>Key Insights</strong></p>
<ul>
<li>Increased mean average precision (mAP) by more than 30% on VOC 2012.</li>
<li>Candidate regions are generated using Selective Search (<a href="#citeproc_bib_item_9">Uijlings et al. 2013</a>).</li>
<li>CNNs are used to perform object classification for each region proposal.</li>
<li>Employs bounding box regression to refine the predicted bounding boxes.</li>
<li>&ldquo;&hellip;when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost.&rdquo;</li>
</ul>
<h4 id="region-proposals">Region Proposals</h4>
<p>The Selective Search algorithm is used to generate region proposals. The algorithm is based on hierarchical grouping of superpixels. Given an input image, approximately 2000 region proposals are generated. Each region proposal is a rectangular bounding box that encloses a region of interest. Since the bounding boxes are not square, the prposals are warped to a fixed size before being fed into the CNN.</p>






<figure>

<img src="/ox-hugo/2024-07-27_17-17-19_screenshot.png" alt="&lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Selective Search region proposals (&lt;a href=&#34;#citeproc_bib_item_9&#34;&gt;Uijlings et al. 2013&lt;/a&gt;)." >



<figcaption data-pre="Figure " data-post=":" >
  
  <p>
    <span class="figure-number">Figure 5: </span>Selective Search region proposals (<a href="#citeproc_bib_item_9">Uijlings et al. 2013</a>).
    
    
    
  </p> 
</figcaption>

</figure>

<h4 id="feature-extraction">Feature Extraction</h4>
<p>Each warped image is passed through a CNN backbone pre-trained on ImageNet. The output is a 4096-dimensional feature vector. In the context of today&rsquo;s methods, this certainly does not seem like a very sophisticated approach. Given the context of the time, this was a significant improvement over the SIFT and HOG features that were previously used. The other benefit is that the CNN can be fine-tuned on the target dataset, if desired.</p>
<h4 id="classification">Classification</h4>
<p>The feature vectors from the CNN are used as input to a linear SVM for classification. The SVM is trained to classify the region proposals into one of the classes in the dataset. As stated before, this really is a piecewise solution. It is also very slow, as each region proposal must be passed through the CNN and SVM.</p>
<h4 id="training">Training</h4>
<p>The CNN and SVM both need to be trained. The CNN is first pre-trained on ImageNet, but it still needs to be fine-tuned on the target dataset. Not only is the target dataset out of domain, but the images themselves are warped. The authors do admit that it is reasonable to simply use the output of the softmax layer for classification in stead of training an SVM. In their experiments, they found that the SVM provided a slight improvement in performance.</p>
<h4 id="inference">Inference</h4>
<p>Given roughly 2000 region proposals, how is a final prediction made? For each class, non-maximum suppression is applied to the region proposals. A region is rejected if it has an IoU overlap with a higher scoring region above a certain threshold.</p>
<h3 id="fast-r-cnn">Fast R-CNN (<a href="#citeproc_bib_item_1">Girshick 2015</a>)</h3>
<p>Published roughly a year after the original R-CNN paper, Fast R-CNN addresses many of the shortcomings and ineffeciencies of the original approach. The main innovation is the introduction of the Region of Interest (RoI) pooling layer. This layer allows the CNN to be applied to the entire image, rather than to each region proposal individually.</p>






<figure>

<img src="/ox-hugo/2024-07-27_17-55-06_screenshot.png" alt="&lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;System overview of Fast R-CNN (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Girshick 2015&lt;/a&gt;)" >



<figcaption data-pre="Figure " data-post=":" >
  
  <p>
    <span class="figure-number">Figure 6: </span>System overview of Fast R-CNN (<a href="#citeproc_bib_item_1">Girshick 2015</a>)
    
    
    
  </p> 
</figcaption>

</figure>

<p><strong>Key Insights</strong></p>
<ul>
<li>Single-stage training joinly optimizes a softmax classifier and bounding box regressor.</li>
<li>RoI pooling layer allows the CNN to be applied to the entire image.</li>
<li>Object proposals are still provided by Selective Search, but only a single forward pass is needed to extract features and make predictions.</li>
</ul>
<h4 id="roi-pooling">RoI Pooling</h4>
<p>Given an input image, a feature extracting CNN produces a feature map. For each region proposal the RoI pooling layer extracts a feature vector directly from the feature map. The feature vector is passed through a fully connected network to produce a class score and bounding box regression.</p>
<p>A <strong>region proposal</strong> is defined by a four-tuple \((r, c, h, w)\) defining the top-left corner \((r, c)\) and the height and width \((h, w)\) of the region. The RoI pooling layer divides the window into a \(H \times W\) grid of sub-windows, where \(H\) and \(W\) are hyperparameters. A max pooling operation is applied to each sub-window to produce a fixed-size output.</p>
<h4 id="training">Training</h4>
<p>Three different CNN backbones were tested. The last max pooling layer is replaced with the RoI pooling layer. From there, the two branches of the network are added: a softmax classifier and a bounding box regressor. The softmax classifier is trained to classify the region proposals into one of the classes in the dataset. The bounding box regressor is trained to refine the predicted bounding boxes.</p>
<p>Another key efficiency improvement comes from hierarchical sampling during training. Since the RoI pooling layer allows the CNN to be applied to the entire image, the same feature map can be used for multiple region proposals. This is in contrast to the original R-CNN approach, where each region proposal was passed through the CNN individually. When training, the authors sample a mini-batch of \(N\) images and \(R\) regions, yielding \(R / N\) RoIs from each image.</p>
<h4 id="results">Results</h4>
<p>This work achieved state-of-the-art results on VOC 2007 (70.0), 2010 (68.8), and 2012 (68.4). Additionally, the model was able to run at about 5 frames per second on a GPU.</p>
<h3 id="faster-r-cnn">Faster R-CNN (<a href="#citeproc_bib_item_6">Ren et al. 2017</a>)</h3>
<p>Fast R-CNN improved on many of the glaring issues presented in the original R-CNN paper. One major bottleneck left was the Selective Search algorithm used to generate region proposals. In the third iteration, Faster R-CNN, the authors introduce the Region Proposal Network (RPN) to replace slower region proposal methods.</p>
<p><strong>Key Insights</strong></p>
<ul>
<li>Region Proposal Network (RPN) generates region proposals.</li>
<li>RPN is a fully convolutional network that shares features with the object detection network.</li>
<li>Anchor boxes of varying aspect ratios are used to predict region proposals.</li>
</ul>
<h4 id="region-proposal-network">Region Proposal Network</h4>
<p>The RPN uses a sliding window approach to generate region proposals. Instead of using a separate CNN, it leverages the feature maps generated by a backbone CNN such as VGG or ResNet.</p>






<figure>

<img src="/ox-hugo/2024-07-28_17-59-11_screenshot.png" alt="&lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Region Proposal Network (RPN) (&lt;a href=&#34;#citeproc_bib_item_6&#34;&gt;Ren et al. 2017&lt;/a&gt;)" >



<figcaption data-pre="Figure " data-post=":" >
  
  <p>
    <span class="figure-number">Figure 7: </span>Region Proposal Network (RPN) (<a href="#citeproc_bib_item_6">Ren et al. 2017</a>)
    
    
    
  </p> 
</figcaption>

</figure>

<p>The RPN produces a feature vector for each sliding window, which is fed into a bounding box regressor and classifier. For each of these sliding windows, \(k\) <strong>anchor</strong> boxes are generated. In their experiments, they generate 9 anchors based on 3 scales and 3 aspect ratios.</p>






<figure>

<img src="/ox-hugo/2024-07-28_18-02-38_screenshot.png" alt="&lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Anchor boxes (&lt;a href=&#34;#citeproc_bib_item_6&#34;&gt;Ren et al. 2017&lt;/a&gt;)" >



<figcaption data-pre="Figure " data-post=":" >
  
  <p>
    <span class="figure-number">Figure 8: </span>Anchor boxes (<a href="#citeproc_bib_item_6">Ren et al. 2017</a>)
    
    
    
  </p> 
</figcaption>

</figure>

<p><strong>Why use anchor boxes instead of directly predicting bounding boxes?</strong> The authors argue that using anchor boxes simplifies the problem. It adds translation invariance: if an object is translated, the proposal should translate accordingly. This approach is also more cost-efficient. Consider an input image for which multiple scales of bounding boxes are generated. To generate multiple scales, the image would need to be resized and passed through the network multiple times. With anchor boxes, the network only needs to be run once.</p>






<figure>

<img src="/ox-hugo/2024-07-28_18-12-19_screenshot.png" alt="&lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Different schemes for addressing multiple scales and sizes (&lt;a href=&#34;#citeproc_bib_item_6&#34;&gt;Ren et al. 2017&lt;/a&gt;)." >



<figcaption data-pre="Figure " data-post=":" >
  
  <p>
    <span class="figure-number">Figure 9: </span>Different schemes for addressing multiple scales and sizes (<a href="#citeproc_bib_item_6">Ren et al. 2017</a>).
    
    
    
  </p> 
</figcaption>

</figure>

<h4 id="results">Results</h4>
<p>Faster R-CNN achieved state-of-the-art results on VOC 2007 (73.2) and 2012 (70.4). These scores increase to 78.8 and 75.9 when using the COCO dataset. Additionally, they evaluate on MS COCO, achieving a mAP@.5 of 42.7, meaning that the model is able to detect 42.7% of the objects in the dataset with an IoU of 0.5 or greater. This score goes down to 21.9% when the IoU threshold is expanded in the range of 0.5 to 0.95.</p>
<h3 id="you-only-look-once">You Only Look Once</h3>
<p>You Only Look Once (YOLO) is a single-stage object detection algorithm that is able to predict multiple bounding boxes and class probabilities in a single forward pass (<a href="#citeproc_bib_item_5">Redmon et al. 2016</a>). Since 2016, it has benefitted from many improvements, some documented by peer-reviewed papers and others by the community. For a recent survey of YOLO and its variants, see (<a href="#citeproc_bib_item_8">Terven, Córdova-Esparza, and Romero-González 2023</a>).</p>
<h4 id="yolov1">YOLOv1 (<a href="#citeproc_bib_item_5">Redmon et al. 2016</a>)</h4>






<figure>

<img src="/ox-hugo/2024-07-28_20-02-46_screenshot.png" alt="&lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;YOLOv1 Overview (&lt;a href=&#34;#citeproc_bib_item_5&#34;&gt;Redmon et al. 2016&lt;/a&gt;)" >



<figcaption data-pre="Figure " data-post=":" >
  
  <p>
    <span class="figure-number">Figure 10: </span>YOLOv1 Overview (<a href="#citeproc_bib_item_5">Redmon et al. 2016</a>)
    
    
    
  </p> 
</figcaption>

</figure>

<p>The original YOLO method works by dividing an input image into a \(S \times S\) grid. Each grid cell predicts \(B\) bounding boxes and confidence scores for each box, along with \(C\) class probabilities.</p>






<figure>

<img src="/ox-hugo/2024-07-28_20-18-23_screenshot.png" alt="&lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;The Model (&lt;a href=&#34;#citeproc_bib_item_5&#34;&gt;Redmon et al. 2016&lt;/a&gt;)." >



<figcaption data-pre="Figure " data-post=":" >
  
  <p>
    <span class="figure-number">Figure 11: </span>The Model (<a href="#citeproc_bib_item_5">Redmon et al. 2016</a>).
    
    
    
  </p> 
</figcaption>

</figure>

<p>During training, only one predictor should be responsible for each object. This is enforced by assigning a predictor to an object based on the highest IoU with the ground truth. For inference, the model outputs bounding boxes with a confidence score greater than a threshold. The entire model is trained using a multi-part loss function that includes terms for objectness, classification, and bounding box regression.</p>






<figure>

<img src="/ox-hugo/2024-07-28_20-21-15_screenshot.png" alt="&lt;span class=&#34;figure-number&#34;&gt;Figure 12: &lt;/span&gt;Annotated description of YOLO loss (&lt;a href=&#34;#citeproc_bib_item_8&#34;&gt;Terven, Córdova-Esparza, and Romero-González 2023&lt;/a&gt;)." >



<figcaption data-pre="Figure " data-post=":" >
  
  <p>
    <span class="figure-number">Figure 12: </span>Annotated description of YOLO loss (<a href="#citeproc_bib_item_8">Terven, Córdova-Esparza, and Romero-González 2023</a>).
    
    
    
  </p> 
</figcaption>

</figure>

<p>YOLOv1 was evaluated on the VOC 2007 dataset, achieving a mAP of 63.4. The model was able to run at 45 frames per second on a GPU.</p>
<h4 id="yolov2-better-faster-and-stronger">YOLOv2: Better, Faster, and Stronger (<a href="#citeproc_bib_item_3">Redmon and Farhadi 2016</a>)</h4>
<p>YOLOv2 was publised at CVPR 2017 and introduced several key improvements over YOLOv1.</p>
<ul>
<li><strong>Batch normalization</strong> acts as a regularizer to reduce overfitting during training.</li>
<li><strong>Fully convolutional</strong>. All dense layers are replaced with convolutional layers.</li>
<li>Following Faster R-CNN, YOLOv2 uses <strong>anchor boxes</strong> to predict bounding boxes.</li>
<li>A predetermined set of anchor box sizes are computed using k-means clustering on the training data.</li>
<li>The model was trained to be robust to varying sizes via <strong>multi-scale training</strong>, where the input image is randomly resized during training.</li>
<li><strong>Multi-task Learning</strong>. The network is pre-trained on ImageNet, where no object detection labels are used. In the event that an input sample does not contain a bounding box annotation, the model is trained to predict the background class.</li>
</ul>
<p>YOLOv2 achieved a 73.4% mAP on VOC 2012, beating Faster R-CNN.</p>
<h4 id="yolov3">YOLOv3 (<a href="#citeproc_bib_item_4">Redmon and Farhadi 2018</a>)</h4>
<p>The third improvement to YOLO was not published at a major conference, but was released on arXiv. The main improvement is a deeper 53-layer backbone network with residual connections. The new method also supports multi-scale predictions by predicting three boxes at three different scales. The same anchor box computation via k-means is done in this work, with the number of priors being expanded to support three different scales.</p>
<h4 id="yolov4-and-beyond">YOLOv4 and beyond</h4>
<p>The remaining iterations were developed by other members of the community and introduce several key improvements while maintaining the spirit and goals of the original paper. For more information, see the informative survey paper by (<a href="#citeproc_bib_item_8">Terven, Córdova-Esparza, and Romero-González 2023</a>).</p>
<h2 id="references">References</h2>
<style>.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}</style><div class="csl-bib-body">
  <div class="csl-entry"><a id="citeproc_bib_item_1"></a>Girshick, Ross. 2015. “Fast R-CNN,” April. <a href="https://doi.org/10.48550/arXiv.1504.08083">https://doi.org/10.48550/arXiv.1504.08083</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_2"></a>Girshick, Ross, Jeff Donahue, Trevor Darrell, and Jitendra Malik. 2014. “Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.” <i>Arxiv:1311.2524 [Cs]</i>, October. <a href="http://arxiv.org/abs/1311.2524">http://arxiv.org/abs/1311.2524</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_3"></a>Redmon, Joseph, and Ali Farhadi. 2016. “YOLO9000: Better, Faster, Stronger.” <i>Arxiv:1612.08242 [Cs]</i>, December. <a href="http://arxiv.org/abs/1612.08242">http://arxiv.org/abs/1612.08242</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_4"></a>———. 2018. “YOLOv3: An Incremental Improvement,” 6.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_5"></a>Redmon, Joseph, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. “You Only Look Once: Unified, Real-Time Object Detection.” <i>Arxiv:1506.02640 [Cs]</i>, May. <a href="http://arxiv.org/abs/1506.02640">http://arxiv.org/abs/1506.02640</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_6"></a>Ren, Shaoqing, Kaiming He, Ross Girshick, and Jian Sun. 2017. “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.” <i>Ieee Transactions on Pattern Analysis and Machine Intelligence</i> 39 (6): 1137–49. <a href="https://doi.org/10.1109/TPAMI.2016.2577031">https://doi.org/10.1109/TPAMI.2016.2577031</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_7"></a>Szeliski, Richard. 2021. <i>Computer Vision: Algorithms and Applications</i>. 2nd ed. <a href="http://szeliski.org/Book/2ndEdition.htm">http://szeliski.org/Book/2ndEdition.htm</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_8"></a>Terven, Juan, Diana-Margarita Córdova-Esparza, and Julio-Alejandro Romero-González. 2023. “A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS.” <i>Machine Learning and Knowledge Extraction</i> 5 (4): 1680–1716. <a href="https://doi.org/10.3390/make5040083">https://doi.org/10.3390/make5040083</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_9"></a>Uijlings, J. R. R., K. E. A. van de Sande, T. Gevers, and A. W. M. Smeulders. 2013. “Selective Search for Object Recognition.” <i>International Journal of Computer Vision</i> 104 (2): 154–71. <a href="https://doi.org/10.1007/s11263-013-0620-5">https://doi.org/10.1007/s11263-013-0620-5</a>.</div>
</div>

    </div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/computer-vision/">computer vision</a>
  
  <a class="badge badge-light" href="/tags/machine-learning/">machine learning</a>
  
</div>



    
      








  
  
    
  
  





  
  
  
    
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="https://ajdillhoff.github.io/">Alex Dillhoff</a></h5>
      <h6 class="card-subtitle">Senior Lecturer</h6>
      <p class="card-text" itemprop="description">&quot;If we understood the world, we would realize that there is a logic of harmony underlying its manifold apparent dissonances.&quot; - Jean Sibelius</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="/#contact" >
              <i class="fas fa-envelope"></i>
            </a>
          </li>
        
          
          
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://scholar.google.co.uk/citations?user=UlLhCtkAAAAJ" target="_blank" rel="noopener">
              <i class="ai ai-google-scholar"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/ajdillhoff" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/notes/instance_segmentation/">Instance Segmentation</a></li>
          
          <li><a href="/notes/convolutional_neural_networks/">Convolutional Neural Networks</a></li>
          
          <li><a href="/notes/boosting/">Boosting</a></li>
          
          <li><a href="/notes/decision_trees/">Decision Trees</a></li>
          
          <li><a href="/notes/camera_models/">Camera Models</a></li>
          
        </ul>
      </div>
      
    

    

    

  </div>
</article>

      

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
      

      
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    

    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academia.min.6c2ba2801d406881b3c2277043cedd76.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  <div class="container">
    <div class="row align-items-center">
      <div class="col-md-6 mb-4 mb-md-0">
        
        <p class="mb-0">
          Copyright © 2024 &middot; 
          Powered by
          <a href="https://gethugothemes.com" target="_blank" rel="noopener">Gethugothemes</a>
        </p>
      </div>
      <div class="col-md-6">
        <ul class="list-inline network-icon text-right mb-0">
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="list-inline-item">
            <a href="https://github.com/ajdillhoff" target="_blank" rel="noopener" title="My GitHub"><i class="fab fa-github" aria-hidden="true"></i></a>
          </li>
          
          
          
          
          
          
          
          
          
          
          <li class="list-inline-item">
            <a href="https://github.com/ajdillhoff" target="_blank" rel="noopener" title="My GitHub"><i class="ai ai-google-scholar" aria-hidden="true"></i></a>
          </li>
          
        </ul>
      </div>
    </div>
  </div>
</footer>
  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
