<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>My Notes on Alex Dillhoff</title>
    <link>https://ajdillhoff.github.io/notes/</link>
    <description>Recent content in My Notes on Alex Dillhoff</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; {year}</copyright>
    <lastBuildDate>Wed, 30 Aug 2023 00:00:00 -0500</lastBuildDate>
    
	    <atom:link href="https://ajdillhoff.github.io/notes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Python File I/O</title>
      <link>https://ajdillhoff.github.io/notes/file_io/</link>
      <pubDate>Wed, 30 Aug 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/file_io/</guid>
      <description>&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/ajdillhoff/python-examples/blob/main/basics/file_io.ipynb&#34;&gt;
  &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;/&gt;
&lt;/a&gt;
&lt;h1 id=&#34;file-io-in-python&#34;&gt;File I/O in Python&lt;/h1&gt;
&lt;p&gt;This notebook will cover the following topics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Opening and closing files&lt;/li&gt;
&lt;li&gt;Reading and writing text files&lt;/li&gt;
&lt;li&gt;Reading and writing binary files&lt;/li&gt;
&lt;li&gt;Using the &lt;code&gt;with&lt;/code&gt; statement&lt;/li&gt;
&lt;li&gt;Using the &lt;code&gt;pickle&lt;/code&gt; module&lt;/li&gt;
&lt;li&gt;Serializing objects with &lt;code&gt;pickle&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Reading and writing JSON files&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;opening-and-closing-files&#34;&gt;Opening and closing files&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with the basics. To open a file, we use the built-in &lt;code&gt;open()&lt;/code&gt; function. The &lt;code&gt;open()&lt;/code&gt; function takes two arguments: the name of the file to open and the mode in which to open it. The mode can be one of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&#39;r&#39;&lt;/code&gt; - open for reading (default)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;w&#39;&lt;/code&gt; - open for writing, truncating the file first&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;x&#39;&lt;/code&gt; - open for exclusive creation, failing if the file already exists&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;a&#39;&lt;/code&gt; - open for writing, appending to the end of the file if it exists&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;b&#39;&lt;/code&gt; - binary mode&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;t&#39;&lt;/code&gt; - text mode (default)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;+&#39;&lt;/code&gt; - open a disk file for updating (reading and writing)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To make this tutorial more interesting, let&amp;rsquo;s create a purpose for our code. Our goal is to create a logging system for players and their rolls. Every time a player rolls, we will add it to our log. We will also add a timestamp to each roll. We will store the log in a file called &lt;code&gt;log.txt&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; random
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; datetime
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# First, we&amp;#39;ll open a file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;fp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;log.txt&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;w&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Let&amp;#39;s create a list of players&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;players &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Naomi&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;James&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Amos&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bobbie&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Let&amp;#39;s roll a d20 for each player and write it in the log, including a timestamp&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; player &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; players:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    roll &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    timestamp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; datetime&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;datetime&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;now()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    fp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;write(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;timestamp&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; - &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;player&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; rolled a &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;roll&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Finally, let&amp;#39;s close the file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;fp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;close()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It isn&amp;rsquo;t recommended to open and close the file manually like this. Instead, we will use the &lt;code&gt;with&lt;/code&gt; statement. This will ensure that the file is closed properly even if an exception is raised.&lt;/p&gt;
&lt;p&gt;Additionally, it is possible that calling &lt;code&gt;write&lt;/code&gt; without using &lt;code&gt;with&lt;/code&gt; will not write to the file immediately. Instead, it will be buffered and written later. Using &lt;code&gt;with&lt;/code&gt; will ensure that the file is written to immediately.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; datetime
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;log.txt&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;a&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; f:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    f&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;write(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;datetime&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;datetime&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;now()&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;player&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; rolled &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;roll&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;reading-and-writing-binary-files&#34;&gt;Reading and writing binary files&lt;/h2&gt;
&lt;p&gt;For the next example, we will write the user&amp;rsquo;s rolls as binary. Each user will get their own file so that we can easily read their rolls later.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; player &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; players:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    rolls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rolls/&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; player &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;.bin&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;wb&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; fp:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        fp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;write(bytes(rolls))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# To verify, let&amp;#39;s open the files and print the contents&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; player &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; players:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rolls/&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; player &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;.bin&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rb&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; fp:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(player, list(fp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read()))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Naomi [2, 13, 18, 8, 16]
James [7, 10, 17, 15, 15]
Amos [19, 9, 11, 6, 3]
Bobbie [16, 5, 18, 13, 1]
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;reading-from-csv-files&#34;&gt;Reading from CSV files&lt;/h1&gt;
&lt;p&gt;Python has a built-in module for reading and writing CSV files. CSV stands for comma-separated values. It is a common format for storing tabular data.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Load and parse CSV&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; csv
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Store the data in a list of dictionaries&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;../data/musicnet_metadata.csv&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;r&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; fp:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    reader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; csv&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reader(fp)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    keys &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; next(reader)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; reader:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(dict(zip(keys, row)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Let&amp;#39;s print the first 5 rows&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; data[:&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(row)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Count the number of Violin pieces are in the dataset&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;count &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; data:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Violin&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ensemble&amp;#34;&lt;/span&gt;]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        count &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;There are &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;count&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; Violin pieces in the dataset&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;{&#39;id&#39;: &#39;1727&#39;, &#39;composer&#39;: &#39;Schubert&#39;, &#39;composition&#39;: &#39;Piano Quintet in A major&#39;, &#39;movement&#39;: &#39;2. Andante&#39;, &#39;ensemble&#39;: &#39;Piano Quintet&#39;, &#39;source&#39;: &#39;European Archive&#39;, &#39;transcriber&#39;: &#39;http://tirolmusic.blogspot.com/&#39;, &#39;catalog_name&#39;: &#39;OP114&#39;, &#39;seconds&#39;: &#39;447&#39;}
{&#39;id&#39;: &#39;1728&#39;, &#39;composer&#39;: &#39;Schubert&#39;, &#39;composition&#39;: &#39;Piano Quintet in A major&#39;, &#39;movement&#39;: &#39;3. Scherzo: Presto&#39;, &#39;ensemble&#39;: &#39;Piano Quintet&#39;, &#39;source&#39;: &#39;European Archive&#39;, &#39;transcriber&#39;: &#39;http://tirolmusic.blogspot.com/&#39;, &#39;catalog_name&#39;: &#39;OP114&#39;, &#39;seconds&#39;: &#39;251&#39;}
{&#39;id&#39;: &#39;1729&#39;, &#39;composer&#39;: &#39;Schubert&#39;, &#39;composition&#39;: &#39;Piano Quintet in A major&#39;, &#39;movement&#39;: &#39;4. Andantino - Allegretto&#39;, &#39;ensemble&#39;: &#39;Piano Quintet&#39;, &#39;source&#39;: &#39;European Archive&#39;, &#39;transcriber&#39;: &#39;http://tirolmusic.blogspot.com/&#39;, &#39;catalog_name&#39;: &#39;OP114&#39;, &#39;seconds&#39;: &#39;444&#39;}
{&#39;id&#39;: &#39;1730&#39;, &#39;composer&#39;: &#39;Schubert&#39;, &#39;composition&#39;: &#39;Piano Quintet in A major&#39;, &#39;movement&#39;: &#39;5. Allegro giusto&#39;, &#39;ensemble&#39;: &#39;Piano Quintet&#39;, &#39;source&#39;: &#39;European Archive&#39;, &#39;transcriber&#39;: &#39;http://tirolmusic.blogspot.com/&#39;, &#39;catalog_name&#39;: &#39;OP114&#39;, &#39;seconds&#39;: &#39;368&#39;}
{&#39;id&#39;: &#39;1733&#39;, &#39;composer&#39;: &#39;Schubert&#39;, &#39;composition&#39;: &#39;Piano Sonata in A major&#39;, &#39;movement&#39;: &#39;2. Andantino&#39;, &#39;ensemble&#39;: &#39;Solo Piano&#39;, &#39;source&#39;: &#39;Museopen&#39;, &#39;transcriber&#39;: &#39;Segundo G. Yogore&#39;, &#39;catalog_name&#39;: &#39;D959&#39;, &#39;seconds&#39;: &#39;546&#39;}
There are 35 Violin pieces in the dataset
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;application-list-all-works-in-the-dataset-by-bach-for-solo-violin&#34;&gt;Application: List all works in the dataset by Bach for Solo Violin&lt;/h1&gt;
&lt;p&gt;Now that we have our data loaded. Let&amp;rsquo;s use list comprehensions to print out all the works by Bach for solo violin. Our formatted table should show the following columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;id&lt;/li&gt;
&lt;li&gt;composition name&lt;/li&gt;
&lt;li&gt;seconds&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Filter all lines that are written by Bach&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;bach &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [row &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; data &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bach&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composer&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Solo Violin&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ensemble&amp;#34;&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print the number of Bach pieces&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;There are &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;len(bach)&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; Bach pieces in the dataset&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print them all out&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; bach:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(row)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;There are 9 Bach pieces in the dataset
{&#39;id&#39;: &#39;2186&#39;, &#39;composer&#39;: &#39;Bach&#39;, &#39;composition&#39;: &#39;Violin Partita No 3 in E major&#39;, &#39;movement&#39;: &#39;1. Preludio&#39;, &#39;ensemble&#39;: &#39;Solo Violin&#39;, &#39;source&#39;: &#39;Oliver Colbentston&#39;, &#39;transcriber&#39;: &#39;suzumidi&#39;, &#39;catalog_name&#39;: &#39;BWV1006&#39;, &#39;seconds&#39;: &#39;214&#39;}
{&#39;id&#39;: &#39;2191&#39;, &#39;composer&#39;: &#39;Bach&#39;, &#39;composition&#39;: &#39;Violin Partita No 3 in E major&#39;, &#39;movement&#39;: &#39;6. Bourree&#39;, &#39;ensemble&#39;: &#39;Solo Violin&#39;, &#39;source&#39;: &#39;Oliver Colbentston&#39;, &#39;transcriber&#39;: &#39;suzumidi&#39;, &#39;catalog_name&#39;: &#39;BWV1006&#39;, &#39;seconds&#39;: &#39;102&#39;}
{&#39;id&#39;: &#39;2241&#39;, &#39;composer&#39;: &#39;Bach&#39;, &#39;composition&#39;: &#39;Violin Sonata No 1 in G minor&#39;, &#39;movement&#39;: &#39;1. Adagio&#39;, &#39;ensemble&#39;: &#39;Solo Violin&#39;, &#39;source&#39;: &#39;European Archive&#39;, &#39;transcriber&#39;: &#39;suzumidi&#39;, &#39;catalog_name&#39;: &#39;BWV1001&#39;, &#39;seconds&#39;: &#39;242&#39;}
{&#39;id&#39;: &#39;2242&#39;, &#39;composer&#39;: &#39;Bach&#39;, &#39;composition&#39;: &#39;Violin Sonata No 1 in G minor&#39;, &#39;movement&#39;: &#39;2. Fuga&#39;, &#39;ensemble&#39;: &#39;Solo Violin&#39;, &#39;source&#39;: &#39;European Archive&#39;, &#39;transcriber&#39;: &#39;suzumidi&#39;, &#39;catalog_name&#39;: &#39;BWV1001&#39;, &#39;seconds&#39;: &#39;312&#39;}
{&#39;id&#39;: &#39;2243&#39;, &#39;composer&#39;: &#39;Bach&#39;, &#39;composition&#39;: &#39;Violin Sonata No 1 in G minor&#39;, &#39;movement&#39;: &#39;3. Siciliana&#39;, &#39;ensemble&#39;: &#39;Solo Violin&#39;, &#39;source&#39;: &#39;European Archive&#39;, &#39;transcriber&#39;: &#39;suzumidi&#39;, &#39;catalog_name&#39;: &#39;BWV1001&#39;, &#39;seconds&#39;: &#39;193&#39;}
{&#39;id&#39;: &#39;2244&#39;, &#39;composer&#39;: &#39;Bach&#39;, &#39;composition&#39;: &#39;Violin Sonata No 1 in G minor&#39;, &#39;movement&#39;: &#39;4. Presto&#39;, &#39;ensemble&#39;: &#39;Solo Violin&#39;, &#39;source&#39;: &#39;European Archive&#39;, &#39;transcriber&#39;: &#39;suzumidi&#39;, &#39;catalog_name&#39;: &#39;BWV1001&#39;, &#39;seconds&#39;: &#39;214&#39;}
{&#39;id&#39;: &#39;2288&#39;, &#39;composer&#39;: &#39;Bach&#39;, &#39;composition&#39;: &#39;Violin Partita No 1 in B minor&#39;, &#39;movement&#39;: &#39;2. Corrente&#39;, &#39;ensemble&#39;: &#39;Solo Violin&#39;, &#39;source&#39;: &#39;John Garner&#39;, &#39;transcriber&#39;: &#39;suzumidi&#39;, &#39;catalog_name&#39;: &#39;BWV1002&#39;, &#39;seconds&#39;: &#39;191&#39;}
{&#39;id&#39;: &#39;2289&#39;, &#39;composer&#39;: &#39;Bach&#39;, &#39;composition&#39;: &#39;Violin Partita No 1 in B minor&#39;, &#39;movement&#39;: &#39;3. Sarabande&#39;, &#39;ensemble&#39;: &#39;Solo Violin&#39;, &#39;source&#39;: &#39;John Garner&#39;, &#39;transcriber&#39;: &#39;suzumidi&#39;, &#39;catalog_name&#39;: &#39;BWV1002&#39;, &#39;seconds&#39;: &#39;203&#39;}
{&#39;id&#39;: &#39;2659&#39;, &#39;composer&#39;: &#39;Bach&#39;, &#39;composition&#39;: &#39;Violin Partita No 1 in B minor&#39;, &#39;movement&#39;: &#39;6. Double&#39;, &#39;ensemble&#39;: &#39;Solo Violin&#39;, &#39;source&#39;: &#39;John Garner&#39;, &#39;transcriber&#39;: &#39;suzumidi&#39;, &#39;catalog_name&#39;: &#39;BWV1002&#39;, &#39;seconds&#39;: &#39;108&#39;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like this dataset is missing quite a few pieces. Bach wrote 6 sonatas and partitas for solo violin. We only have 3 of them in this dataset. Of the 3 that are included, partitas 1 and 3 are incomplete as well.&lt;/p&gt;
&lt;p&gt;Based on the order of the ids, it looks like this data should be present. Let&amp;rsquo;s see if we can fill some of this in.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# The times are based on Hilary Hahn&amp;#39;s recordings&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;missing_pieces &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2679&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composer&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bach&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composition&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Violin Partita No 3 in E major&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;movement&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2. Loure&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ensemble&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Solo Violin&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;source&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;DASC 5300 Fall 2023&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;transcriber&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;none&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;catalog_name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;BWV 1006&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;seconds&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;287&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2680&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composer&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bach&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composition&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Violin Partita No 3 in E major&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;movement&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;3. Gavotte en Rondeau&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ensemble&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Solo Violin&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;source&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;DASC 5300 Fall 2023&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;transcriber&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;none&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;catalog_name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;BWV 1006&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;seconds&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;196&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2681&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composer&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bach&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composition&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Violin Partita No 3 in E major&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;movement&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;4. Menuet I&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ensemble&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Solo Violin&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;source&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;DASC 5300 Fall 2023&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;transcriber&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;none&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;catalog_name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;BWV 1006&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;seconds&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;113&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2682&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composer&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bach&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composition&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Violin Partita No 3 in E major&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;movement&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;5. Menuet II&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ensemble&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Solo Violin&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;source&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;DASC 5300 Fall 2023&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;transcriber&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;none&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;catalog_name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;BWV 1006&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;seconds&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;183&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;formatting-our-output&#34;&gt;Formatting Our Output&lt;/h1&gt;
&lt;p&gt;Viewing raw lines of a dictionary or CSV file is less than ideal. Let&amp;rsquo;s format our output to make it easier to read. We will format the filtered Bach data. Since we know that every piece was written by him, we don&amp;rsquo;t need to show that column.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Let&amp;#39;s add the missing pieces to our dataset&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extend(missing_pieces)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Refilter the Bach pieces&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;bach &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [row &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; data &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bach&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composer&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Solo Violin&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ensemble&amp;#34;&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;There are &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;len(bach)&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; Bach pieces in the dataset&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print them all out, sorted by composition then movement&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;composition_width &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(len(row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composition&amp;#34;&lt;/span&gt;]) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; bach)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;movement_width &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(len(row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;movement&amp;#34;&lt;/span&gt;]) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; bach)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;id_width &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(len(row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;]) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; bach)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;{:&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;} | {:&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;} | {:&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Composition&amp;#34;&lt;/span&gt;, composition_width, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Movement&amp;#34;&lt;/span&gt;, movement_width, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ID&amp;#34;&lt;/span&gt;, id_width))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (composition_width &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; movement_width &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; id_width &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sorted(bach, key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: (x[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composition&amp;#34;&lt;/span&gt;], x[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;movement&amp;#34;&lt;/span&gt;])):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;{:&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;} | {:&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;} | {:&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composition&amp;#34;&lt;/span&gt;], composition_width, row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;movement&amp;#34;&lt;/span&gt;], movement_width, row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;], id_width))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;There are 13 Bach pieces in the dataset
Composition                    | Movement              | ID  
-------------------------------------------------------------
Violin Partita No 1 in B minor | 2. Corrente           | 2288
Violin Partita No 1 in B minor | 3. Sarabande          | 2289
Violin Partita No 1 in B minor | 6. Double             | 2659
Violin Partita No 3 in E major | 1. Preludio           | 2186
Violin Partita No 3 in E major | 2. Loure              | 2679
Violin Partita No 3 in E major | 3. Gavotte en Rondeau | 2680
Violin Partita No 3 in E major | 4. Menuet I           | 2681
Violin Partita No 3 in E major | 5. Menuet II          | 2682
Violin Partita No 3 in E major | 6. Bourree            | 2191
Violin Sonata No 1 in G minor  | 1. Adagio             | 2241
Violin Sonata No 1 in G minor  | 2. Fuga               | 2242
Violin Sonata No 1 in G minor  | 3. Siciliana          | 2243
Violin Sonata No 1 in G minor  | 4. Presto             | 2244
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Now that it looks good, let&amp;#39;s write the updated dataset to a new file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;../data/musicnet_metadata_updated.csv&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;w&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; fp:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    writer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; csv&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DictWriter(fp, fieldnames&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;keys)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    writer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;writeheader()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    writer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;writerows(data)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;reading-and-writing-json&#34;&gt;Reading and Writing JSON&lt;/h1&gt;
&lt;p&gt;JSON stands for JavaScript Object Notation. It is a common format for storing and transmitting data. It is often used for web APIs. Python has a built-in module for reading and writing JSON files.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s open the CSV file from the previous example and write it to JSON.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Load and parse CSV&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; csv
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; json
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Store the data in a list of dictionaries&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;../data/musicnet_metadata.csv&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;r&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; fp:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    reader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; csv&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reader(fp)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    keys &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; next(reader)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; reader:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(dict(zip(keys, row)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Write the data to a JSON file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;musicnet_metadata.json&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;w&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; fp:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    json&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dump(data, fp)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;reading-and-writing-pickle-files&#34;&gt;Reading and Writing &lt;code&gt;pickle&lt;/code&gt; Files&lt;/h1&gt;
&lt;p&gt;Python has a built-in module for reading and writing &lt;code&gt;pickle&lt;/code&gt; files. &lt;code&gt;pickle&lt;/code&gt; is a binary format for serializing Python objects. It is not human-readable, but it is very useful for storing and transmitting data. Note that &lt;code&gt;pickle&lt;/code&gt; is not secure. It is possible to create malicious &lt;code&gt;pickle&lt;/code&gt; files that can execute arbitrary code when loaded. Also, other languages cannot natively read &lt;code&gt;pickle&lt;/code&gt; files. However, there are usually libraries available for reading &lt;code&gt;pickle&lt;/code&gt; files in other languages.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start by writing the list from the previous example to a &lt;code&gt;pickle&lt;/code&gt; file.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Write the data to a pickle file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pickle
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;musicnet_metadata.pkl&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;wb&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; fp:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    pickle&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dump(data, fp)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Get the file size of the pickle file we just wrote&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; os
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;getsize(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;musicnet_metadata.pkl&amp;#34;&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;52353
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;serializing-class-objects&#34;&gt;Serializing Class Objects&lt;/h1&gt;
&lt;p&gt;Serializing objects is simple with Python. Let&amp;rsquo;s create a simple class to represent the attributes of our dataset. We can then convert our previous list of dictionary data to a list of objects. Finally, we can serialize the list of objects to a &lt;code&gt;pickle&lt;/code&gt; file.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create a class for our data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Piece&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, data):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;composer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composer&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;composition &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composition&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;movement &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;movement&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ensemble &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ensemble&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;source &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;source&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transcriber &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;transcriber&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;catalog_name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;catalog_name&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;seconds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;seconds&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __repr__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;lt;Piece &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;id&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;composer&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; - &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;composition&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __str__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;composer&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; - &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;composition&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Convert our data into a list of objects and write it to a pickle file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pieces &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [Piece(d) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; d &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; data]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;musicnet_metadata.pkl&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;wb&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; fp:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    pickle&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dump(pieces, fp)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print the size of this new file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;getsize(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;musicnet_metadata.pkl&amp;#34;&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;54352
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;bonus-write-the-class-objects-to-json&#34;&gt;Bonus: Write the class objects to JSON&lt;/h1&gt;
&lt;p&gt;We can&amp;rsquo;t write the class objects to JSON directly. We need to convert them to a dictionary first. We can do this by implementing the &lt;code&gt;__dict__&lt;/code&gt; method. Without explicitly defining &lt;code&gt;__dict__&lt;/code&gt;, it will return the default dictionary for the class. However, we can override it to return a custom dictionary. The default version will include all of the class attributes. We can use this to our advantage to convert the class to a dictionary.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Bonus: Save the class data as JSON&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; json
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Write the list of class objects to a JSON file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;musicnet_metadata.json&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;w&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; fp:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    json&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dump(pieces, fp, default&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; o: o&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__dict__)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Python Functions</title>
      <link>https://ajdillhoff.github.io/notes/functions/</link>
      <pubDate>Wed, 30 Aug 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/functions/</guid>
      <description>&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/ajdillhoff/python-examples/blob/main/basics/functions.ipynb&#34;&gt;
  &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;/&gt;
&lt;/a&gt;
&lt;h1 id=&#34;functions&#34;&gt;Functions&lt;/h1&gt;
&lt;p&gt;We learned the importance of functions early on in mathematics. It is a compact way of represented a complex process dependent on a set of variables. In programming, functions are used to encapsulate a set of instructions that are used repeatedly. Functions are also used to make code more readable and easier to debug.&lt;/p&gt;
&lt;p&gt;In this notebook, we will look at a few important built-in functions in Python as well as how to define our own functions.&lt;/p&gt;
&lt;h2 id=&#34;built-in-functions&#34;&gt;Built-in Functions&lt;/h2&gt;
&lt;p&gt;Python has a good number of built-in functions that cover a general range of tasks.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Find the max of a list of numbers&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;max_val &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The max value is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;max_val&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Find the min of a list of numbers&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;min_val &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The min value is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;min_val&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Get the length of a string&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; input(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Enter some text: &amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The length of &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;text&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;len(text)&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;The max value is 3
The min value is 1
The length of &amp;quot;test&amp;quot; is 4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;len&lt;/code&gt; function returns the length of a string, list, or other iterable object. Since these functions are built-in, we should treat them as keywords. However, we can still overwrite them if we want to.&lt;/p&gt;
&lt;h2 id=&#34;type-conversion-functions&#34;&gt;Type Conversion Functions&lt;/h2&gt;
&lt;p&gt;Python has built-in functions to convert between data types. These are &lt;code&gt;int&lt;/code&gt;, &lt;code&gt;float&lt;/code&gt;, &lt;code&gt;str&lt;/code&gt;, &lt;code&gt;bool&lt;/code&gt;, and &lt;code&gt;list&lt;/code&gt;. Converting between incompatible types will result in an error.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Try entering a number and text&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;val &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; input(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Enter a number: &amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;val_int &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int(val)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The value of &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;val&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;val_int&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Converting an integer type to float will truncate the decimal value&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;float_val &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3.14&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;int_val &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int(float_val)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The value of &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;float_val&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;int_val&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Converting a number to a string also has its uses&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;big_number &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;184759372934&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;big_number_str &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; str(big_number)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;There are &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;len(big_number_str)&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; digits in &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;big_number_str&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;The value of 10 is 10
The value of 3.14 is 3
There are 12 digits in 184759372934
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;math-functions&#34;&gt;Math Functions&lt;/h2&gt;
&lt;p&gt;Python has many more functions that are available through named modules. For example, the &lt;code&gt;math&lt;/code&gt; module contains many useful functions for mathematical operations. To use these functions, we need to import the module first.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; math
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;signal_power &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;noise_power &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ratio &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; signal_power &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; noise_power
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;decibels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;log10(ratio)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The decibel value is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;decibels&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# The math module also has a function to convert from radians to degrees&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;radians &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.7&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;degrees &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;degrees(radians)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;radians&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; radians is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;degrees&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; degrees&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# The math module also has a function to convert from degrees to radians&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;degrees &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;45&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;radians &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;radians(degrees)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;degrees&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; degrees is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;radians&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; radians&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;The decibel value is 3.010299956639812
0.7 radians is 40.10704565915762 degrees
45 degrees is 0.7853981633974483 radians
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;example-getting-the-number-of-digits-without-len&#34;&gt;Example: Getting the number of digits without &lt;code&gt;len&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;We can use the &lt;code&gt;math.log10&lt;/code&gt; function to get the number of digits in a number.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;big_number &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;184759372934&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;big_number_log &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;log10(big_number)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The log of &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;big_number&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;big_number_log&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can use this to get the number of digits in a number&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;big_number_log_int &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int(big_number_log)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;There are &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;big_number_log_int &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; digits in &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;big_number&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;The log of 184759372934 is 11.266606479598726
There are 12 digits in 184759372934
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;random-numbers&#34;&gt;Random Numbers&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;random&lt;/code&gt; module contains functions for generating random numbers. The &lt;code&gt;random&lt;/code&gt; function returns a random number between 0 and 1. The &lt;code&gt;randint&lt;/code&gt; function returns a random integer between two numbers. Other functions in the &lt;code&gt;random&lt;/code&gt; module include &lt;code&gt;randrange&lt;/code&gt;, &lt;code&gt;choice&lt;/code&gt;, &lt;code&gt;choices&lt;/code&gt;, &lt;code&gt;shuffle&lt;/code&gt;, and &lt;code&gt;sample&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; random
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Generate 10 random numbers between 0 and 1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Generate 10 random integers between 4 and 10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Randomly select a value from a list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;outcomes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rock&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;paper&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;scissors&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;choice(outcomes))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;0.5791115192498043
0.25376108559783617
0.3024994224981705
0.09072083021401978
0.42037740159252324
0.6617409553852582
0.4379309412534801
0.2475132325137145
0.8452657960508129
0.9268148237541722
4
5
7
5
7
10
8
6
7
4
scissors
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Third-party modules such as &lt;code&gt;numpy&lt;/code&gt; and &lt;code&gt;scipy&lt;/code&gt; contain many more functions for generating random numbers.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Generate 10 random numbers between 0 and 1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rand(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(numbers)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Sample 10 random numbers from a normal distribution&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(numbers)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[0.47426298 0.22698408 0.42633457 0.97584666 0.68835279 0.57067918
 0.56958053 0.86689698 0.54039587 0.59397872]
[ 0.97783258 -0.4043045   0.05416324 -2.21162364 -0.60327265 -0.39077797
 -0.83294774  0.56285811 -0.28047169 -0.60044315]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;defining-functions&#34;&gt;Defining Functions&lt;/h2&gt;
&lt;p&gt;We can define our own functions using the &lt;code&gt;def&lt;/code&gt; keyword. The syntax for defining a function is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function_name&lt;/span&gt;(parameters):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# function body&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; value
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Python functions can have multiple parameters and return multiple values. The &lt;code&gt;return&lt;/code&gt; keyword is optional. If it is not used, the function will return &lt;code&gt;None&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; math
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;calculate_stats&lt;/span&gt;(numbers):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Calculate the mean and standard deviation of a list of numbers&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    mean &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sum(numbers) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; len(numbers)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    variance &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sum((x &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; mean) &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; numbers) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; len(numbers)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    std_dev &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt(variance)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; mean, std_dev
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the mean and standard deviation of a list of numbers&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mean, std_dev &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; calculate_stats(numbers)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The mean is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;mean&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; and the standard deviation is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;std_dev&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;The mean is 3.0 and the standard deviation is 1.4142135623730951
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;default-argument-values&#34;&gt;Default Argument Values&lt;/h1&gt;
&lt;p&gt;Python functions can have default values for their arguments. This allows us to call the function without specifying the value for that argument. If we do not specify a value for an argument, the default value will be used.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# To demonstrate default argument values, let&amp;#39;s make a function that&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# allows the user to select the axis along which to calculate the mean&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;calculate_mean&lt;/span&gt;(numbers, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Calculates the mean of a 2D array along a given axis&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Check that the input is a 2D python list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; isinstance(numbers, list) &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; isinstance(numbers[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], list):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ValueError&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Input must be a 2D list&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; axis &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Here, zip(*numbers) will return a list of tuples,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# where each tuple is a column of our 2D list.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; [sum(col) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; len(col) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; col &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; zip(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;numbers)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; axis &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; [sum(row) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; len(row) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; numbers]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ValueError&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Invalid axis value&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the mean along the rows&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mean &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; calculate_mean(numbers, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Mean of each row:&amp;#34;&lt;/span&gt;, mean)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the mean along the columns&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mean &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; calculate_mean(numbers, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Mean of each column&amp;#34;&lt;/span&gt;, mean)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Mean of each row: [2.0, 5.0]
Mean of each column [2.5, 3.5, 4.5]
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;keyword-arguments&#34;&gt;Keyword Arguments&lt;/h1&gt;
&lt;p&gt;Python functions can also have keyword arguments. This allows us to specify the name of the argument when calling the function. This is useful when a function has many arguments and we only want to specify a few of them.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;matrix_max&lt;/span&gt;(matrix, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, return_indices&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Calculates the maximum of a 2D array along a given axis&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Check that the input is a 2D python list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; isinstance(matrix, list) &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; isinstance(matrix[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], list):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ValueError&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Input must be a 2D list&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; axis &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Here, zip(*matrix) will return a list of tuples,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# where each tuple is a column of our 2D list.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        max_vals &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [max(col) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; col &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; zip(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;matrix)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; return_indices:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            max_indices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [col&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;index(max(col)) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; col &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; zip(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;matrix)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; max_vals, max_indices
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; max_vals
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; axis &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        max_vals &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [max(row) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; matrix]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; return_indices:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            max_indices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [row&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;index(max(row)) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; matrix]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; max_vals, max_indices
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; max_vals
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ValueError&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Invalid axis value&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the max along the rows&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;max_vals &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; matrix_max(numbers, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Max of each row:&amp;#34;&lt;/span&gt;, max_vals)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the max along the columns&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;max_vals &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; matrix_max(numbers, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Max of each column&amp;#34;&lt;/span&gt;, max_vals)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the max along the rows and return the indices&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;max_vals, max_indices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; matrix_max(numbers, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, return_indices&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Max of each row:&amp;#34;&lt;/span&gt;, max_vals)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Indices of max values:&amp;#34;&lt;/span&gt;, max_indices)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Max of each row: [3, 6]
Max of each column [4, 5, 6]
Max of each row: [3, 6]
Indices of max values: [2, 2]
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;list-unpacking-and-variable-arguments&#34;&gt;List Unpacking and Variable Arguments&lt;/h1&gt;
&lt;p&gt;Python functions can have variable arguments. This allows us to pass in a variable number of arguments to a function. The &lt;code&gt;*&lt;/code&gt; operator is used to unpack a list or tuple into separate arguments.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# The range function expects up to 3 arguments: start, stop, and step&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can use list unpacking to put our arguments in a list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;args &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;args):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(i)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;1
3
5
7
9
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can create a function that support multiple arguments as well as keyword arguments&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;print_args&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;args, &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;kwargs):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Positional arguments:&amp;#34;&lt;/span&gt;, args)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Keyword arguments:&amp;#34;&lt;/span&gt;, kwargs)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print_args(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, a&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, b&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Positional arguments: (1, 2, 3)
Keyword arguments: {&#39;a&#39;: 4, &#39;b&#39;: 5}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Git</title>
      <link>https://ajdillhoff.github.io/notes/introduction_to_git/</link>
      <pubDate>Sat, 26 Aug 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/introduction_to_git/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-version-control&#34;&gt;What is Version Control?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-git&#34;&gt;What is Git?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-a-repository&#34;&gt;What is a Repository?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#configuring-git&#34;&gt;Configuring Git&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#creating-a-repository&#34;&gt;Creating a Repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#staging-files&#34;&gt;Staging Files&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#committing-changes&#34;&gt;Committing Changes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ignoring-files&#34;&gt;Ignoring Files&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#branching&#34;&gt;Branching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#merging&#34;&gt;Merging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#remotes&#34;&gt;Remotes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cloning-an-existing-repository&#34;&gt;Cloning an Existing Repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;what-is-version-control&#34;&gt;What is Version Control?&lt;/h2&gt;
&lt;p&gt;Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later. This is useful not just for team projects, for for individual projects as well.&lt;/p&gt;
&lt;p&gt;With version control, you can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;revert files back to a previous state&lt;/li&gt;
&lt;li&gt;revert the entire project back to a previous state&lt;/li&gt;
&lt;li&gt;compare changes over time&lt;/li&gt;
&lt;li&gt;see who last modified something that might be causing a problem&lt;/li&gt;
&lt;li&gt;who introduced an issue and when&lt;/li&gt;
&lt;li&gt;and much more&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When tracking changes to a project over time, the simplest approach is one that you might recognize if you&amp;rsquo;ve ever worked on an essay for class. Imagine you&amp;rsquo;ve just finished the first draft of an assignment. You decide to save this document as &lt;code&gt;essay_first_draft.docx&lt;/code&gt;. After working on it a bit more, you choose to save the updated copy to a new file so that you can compare the initial and final draft. This one is then named &lt;code&gt;essay_first_draft_COMPLETE.docx&lt;/code&gt;. You end up reading some new information and realize you missed a key requirement of the assignment. After adding in the new information you save it as &lt;code&gt;essay_first_draft_COMPLETE_v2.docx&lt;/code&gt;. After many such iterations you end up with a collection of ill-named files.&lt;/p&gt;
&lt;p&gt;Maybe you&amp;rsquo;ve never done this yourself, but this example actually depicts a version control system. Luckily for us, there have been many improvements to this naive method. A more ideal choice, especially in a team environment, would be a Centralized VCS. Project files would be hosted on a server that keeps track of the different changes. Team members can download the latest versions, modify them, and update the server once they are done.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_18-53-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Centralized VCS ([source](&amp;lt;https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control&amp;gt;))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Centralized VCS (&lt;a href=&#34;https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;source&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This is a welcome improvement over the naive version, but it still has its downsides. What if the server or connection goes down? There are many scenarios that would lead to a catastrophic loss of data. For important projects, you would not want to keep all of your eggs in once basket. A more ideal solution would be a Distributed VCS, this is what Git is.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_18-58-25_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Distributed VCS ([source](&amp;lt;https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control&amp;gt;))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Distributed VCS (&lt;a href=&#34;https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;source&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In a DVCS, every user has a complete copy of the project. If the server goes down, or a connection is lost, you can still work on the project. Since every user has a complete copy of the project, there is no single point of failure. Another huge advantage is speed. The operations are performed locally. It is only when you want to share your changes that you need to connect to the server. This means that you can commit changes, create branches, and perform other operations without an internet connection.&lt;/p&gt;
&lt;h2 id=&#34;what-is-git&#34;&gt;What is Git?&lt;/h2&gt;
&lt;p&gt;There are two primary ways of thinking about versioning in general: snapshots and differences. The first starts with your original files and records each change as a delta between the latest version and the previous.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_19-03-32_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Difference-based Version Control ([source](&amp;lt;https://git-scm.com/book/en/v2/Getting-Started-What-is-Git%3F&amp;gt;))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Difference-based Version Control (&lt;a href=&#34;https://git-scm.com/book/en/v2/Getting-Started-What-is-Git%3F&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;source&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The second starts with your original files and records each change as a snapshot of the entire project. Files that have not changed will not be duplicated. Instead, Git will create a reference to the previous version of the file. This is the approach that Git uses, and it comes with a great benefit that we will see when we get to branching.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_19-05-52_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Snapshot-based Version Control ([source](&amp;lt;https://git-scm.com/book/en/v2/Getting-Started-What-is-Git%3F&amp;gt;))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Snapshot-based Version Control (&lt;a href=&#34;https://git-scm.com/book/en/v2/Getting-Started-What-is-Git%3F&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;source&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;what-is-a-repository&#34;&gt;What is a Repository?&lt;/h2&gt;
&lt;p&gt;A repository is a collection of files and folders that are tracked by Git. It is the project folder that you will be working in. You can create a repository from scratch, or you can clone an existing repository. We will cover examples of both during class. Cloning is the process of copying an existing repository to your local machine. We will start with the first approach: creating a repository from scratch.&lt;/p&gt;
&lt;p&gt;Before starting, it is important to at least know the three major states of Git. Files can be &lt;code&gt;modified&lt;/code&gt;, &lt;code&gt;staged&lt;/code&gt;, or &lt;code&gt;committed&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;code&gt;modified&lt;/code&gt; file has been changed locally, but has not been committed to the repository.&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;staged&lt;/code&gt; file is a modified file that has been marked to be included in the next commit.&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;committed&lt;/code&gt; file is a staged file that has been saved to the repository.&lt;/li&gt;
&lt;/ul&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_19-16-09_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;The main sections of Git. ([source](&amp;lt;https://git-scm.com/book/en/v2/Getting-Started-What-is-Git%3F&amp;gt;))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;The main sections of Git. (&lt;a href=&#34;https://git-scm.com/book/en/v2/Getting-Started-What-is-Git%3F&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;source&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The figure above depicts the three major sections of working with a Git repository. Each repository has a &lt;code&gt;.git&lt;/code&gt; directory that contains all of the information about the project. The &lt;code&gt;working directory&lt;/code&gt; is the root directory where the latest versions of the files exist. Once modifications are made, these changes are sent to the &lt;code&gt;staging area&lt;/code&gt;. This is where you can choose which changes to include in the next commit. Once you are happy with the changes, you can commit them to the repository. This will save the changes to the &lt;code&gt;.git&lt;/code&gt; directory.&lt;/p&gt;
&lt;h2 id=&#34;configuring-git&#34;&gt;Configuring Git&lt;/h2&gt;
&lt;p&gt;Once you have installed Git, there are a few important configuration options to get started. If you have already been using Git, you can skip this section. If you are using Git for the first time, you will need to set your name and email address. This information will be used to identify you as the author of the commits that you make.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git config --global user.name &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Naomi Nagata&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git config --global user.email &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;naomi@rocinante.exp&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you have already used a service like GitHub, note that this name and email does not need to match the one you used to log into that service.&lt;/p&gt;
&lt;p&gt;You can view your current configuration at any time by running the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git config --list --show-origin
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;creating-a-repository&#34;&gt;Creating a Repository&lt;/h2&gt;
&lt;p&gt;For this example, our first project will be a Python program that resizes images to a specified width. This is to ensure that the aspect ratio is maintained.&lt;/p&gt;
&lt;p&gt;Now that we have Git installed and configured, we can create our first repository. First, create a new directory for the project. I will use &lt;code&gt;pyresize&lt;/code&gt; in this document. Then, navigate to that directory and run the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mkdir pyresize &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; cd pyresize
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git init
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You may see the following warning when creating a new repository:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint: Using &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;master&amp;#39;&lt;/span&gt; as the name &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; the initial branch. This default branch name
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint: is subject to change. To configure the initial branch name to use in all
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint: of your new repositories, which will suppress this warning, call:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint: 	git config --global init.defaultBranch &amp;lt;name&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint: Names commonly chosen instead of &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;master&amp;#39;&lt;/span&gt; are &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;main&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;trunk&amp;#39;&lt;/span&gt; and
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;development&amp;#39;&lt;/span&gt;. The just-created branch can be renamed via this command:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint: 	git branch -m &amp;lt;name&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Initialized empty Git repository in /home/alex/dev/pyresize/.git/
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s first set the default branch name to &lt;code&gt;main&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git config --global init.defaultBranch main
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next, we will change the name of the current branch to &lt;code&gt;main&lt;/code&gt;. We could also delete the &lt;code&gt;.git&lt;/code&gt; directory and start over, but this is a good opportunity to learn how to rename a branch.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git branch -m main
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can view the status of your repository at any time by using the &lt;code&gt;git status&lt;/code&gt; command. This will show you the current branch, the files that have been modified, and the files that have been staged. Our newly created repository looks like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ git status
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;On branch main
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;No commits yet
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;nothing to commit &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;create/copy files and use &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git add&amp;#34;&lt;/span&gt; to track&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;staging-files&#34;&gt;Staging Files&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s create our first file and add it to the repository. We will create a file called &lt;code&gt;pyresize.py&lt;/code&gt; that contains the following code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sys
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; PIL &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Image
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;resize_image&lt;/span&gt;(image_path, width):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(image_path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    wpercent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (width &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; float(image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;size[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    hsize &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int((float(image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;size[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; float(wpercent)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;resize((width, hsize), Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;LANCZOS)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;save(image_path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; __name__ &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;__main__&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    resize_image(sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argv[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], int(sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argv[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;At this point, we have a local change that our repository is not aware of. We can see this by running the &lt;code&gt;git status&lt;/code&gt; command again.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ git status
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;On branch main
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;No commits yet
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Untracked files:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;use &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git add &amp;lt;file&amp;gt;...&amp;#34;&lt;/span&gt; to include in what will be committed&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    pyresize.py
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;nothing added to commit but untracked files present &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;use &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git add&amp;#34;&lt;/span&gt; to track&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s add our file with &lt;code&gt;git add pyresize.py&lt;/code&gt; and check the status again.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ git add pyresize.py
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ git status
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;On branch main
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;No commits yet
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Changes to be committed:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;use &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git rm --cached &amp;lt;file&amp;gt;...&amp;#34;&lt;/span&gt; to unstage&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    new file:   pyresize.py
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;committing-changes&#34;&gt;Committing Changes&lt;/h2&gt;
&lt;p&gt;Finally, we will &lt;code&gt;commit&lt;/code&gt; this change via &lt;code&gt;git commit&lt;/code&gt;. There are a few things to note about this command. If you haven&amp;rsquo;t configured your default editor, it might be set to something like &lt;code&gt;nano&lt;/code&gt; or &lt;code&gt;vim&lt;/code&gt; by default. If you are not familiar with these editors, you can set your default editor to something else by running the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git config --global core.editor &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;code --wait&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will set your default editor to Visual Studio Code. Obviously, this should be installed on your system if you are using it. If you are using a different editor, you can replace &lt;code&gt;code&lt;/code&gt; with the command that you would use to open a file in that editor. For example, if you are using &lt;code&gt;vim&lt;/code&gt;, you would use &lt;code&gt;vim&lt;/code&gt;. The &lt;code&gt;--wait&lt;/code&gt; flag above will wait for the editor to close before continuing. This is important for Git to know when you are done writing your commit message. &lt;strong&gt;Note that not every application supports this flag.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once you have set your default editor, you can run &lt;code&gt;git commit&lt;/code&gt; to open the editor and write your commit message. The first line should be a short description of the change. The following lines should be a more detailed description of the change. You can see an example below:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-git&#34; data-lang=&#34;git&#34;&gt;Added our first file.
# Please enter the commit message for your changes. Lines starting
# with &amp;#39;#&amp;#39; will be ignored, and an empty message aborts the commit.
#
# On branch main
#
# Initial commit
#
# Changes to be committed:
#	new file:   pyresize.py
#
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once we save this message, the commit will be complete. You can view the commit history by running &lt;code&gt;git log&lt;/code&gt;. This will show you the commit hash, the author, the date, and the commit message. You can also commit changes and add a message in one command.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git commit -m &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Added our first file.&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We do not yet have a remote repository to &lt;code&gt;push&lt;/code&gt; to, so we will save that for later. For now, we will continue to work locally. Let&amp;rsquo;s add an image to our repository so that we can test it. I am going to use the &lt;a href=&#34;![]%28https://resources.uta.edu/mme/identity/_images/new-logos/new-initials-logo.jpg%29&#34;&gt;UTA Logo&lt;/a&gt; for this example. You can download this and variations from the &lt;a href=&#34;https://resources.uta.edu/mme/identity/brand/index.php&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;UTA Branding Resources&lt;/a&gt; page.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_20-04-35_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;UTA Logo&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;UTA Logo
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Create a new &lt;code&gt;imgs&lt;/code&gt; folder and add your image(s) to it. We can then add the directory along with all of its contents using &lt;code&gt;git add imgs&lt;/code&gt;. You can see the status of your repository by running &lt;code&gt;git status&lt;/code&gt; again. Let&amp;rsquo;s go ahead and &lt;code&gt;commit&lt;/code&gt; these changes.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git commit -am &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Added the UTA logo.&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;making-a-change&#34;&gt;Making a Change&lt;/h3&gt;
&lt;p&gt;For this project, we don&amp;rsquo;t really need to have a bunch of test images. It is sufficient to have one or two. The name of our image folder should probably change to reflect its purpose. Let&amp;rsquo;s start by renaming &lt;code&gt;imgs&lt;/code&gt; to &lt;code&gt;test_imgs&lt;/code&gt;. We can do this with the &lt;code&gt;mv&lt;/code&gt; command in bash. Our repository will now look like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ git status
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;On branch main
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Changes not staged &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; commit:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;use &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git add/rm &amp;lt;file&amp;gt;...&amp;#34;&lt;/span&gt; to update what will be committed&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;use &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git restore &amp;lt;file&amp;gt;...&amp;#34;&lt;/span&gt; to discard changes in working directory&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	deleted:    imgs/uta.png
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Untracked files:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;use &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git add &amp;lt;file&amp;gt;...&amp;#34;&lt;/span&gt; to include in what will be committed&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	test_imgs/
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;no changes added to commit &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;use &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git add&amp;#34;&lt;/span&gt; and/or &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git commit -a&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This might be a tad unexpected. We renamed the folder, but Git is telling us that we deleted a file. This is because Git is tracking the file &lt;code&gt;imgs/uta.png&lt;/code&gt;. When we renamed the folder, Git no longer knew where to find the file. We can fix this by running &lt;code&gt;git rm imgs/uta.png&lt;/code&gt;. This will remove the file from the repository. We can then add the new folder with &lt;code&gt;git add test_imgs&lt;/code&gt;. However, if we simply use &lt;code&gt;git add test_imgs&lt;/code&gt;, Git will not know that we renamed the folder. We can fix this by using the &lt;code&gt;-A&lt;/code&gt; flag. This will tell Git to add all changes, including renames. Our repository will now look like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ git status
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;On branch main
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Changes to be committed:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;use &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git restore --staged &amp;lt;file&amp;gt;...&amp;#34;&lt;/span&gt; to unstage&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    renamed:    imgs/uta.png -&amp;gt; test_imgs/uta.png
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Go ahead and &lt;code&gt;commit&lt;/code&gt; these changes.&lt;/p&gt;
&lt;h2 id=&#34;ignoring-files&#34;&gt;Ignoring Files&lt;/h2&gt;
&lt;p&gt;There are certain files and directories that will end up in our project folder that we do not want to track. For example, we may want to resize images and save them in a local &lt;code&gt;output&lt;/code&gt; directory. However, we do not want to track any of these images. We can have Git remember what we want or do &lt;em&gt;not&lt;/em&gt; want using an ignore file. This file will contain a list of files and directories that we want to ignore. Let&amp;rsquo;s create a &lt;code&gt;.gitignore&lt;/code&gt; file and add the following line:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;output/
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Make sure you add and commit the &lt;code&gt;.gitignore&lt;/code&gt; file.&lt;/p&gt;
&lt;h2 id=&#34;branching&#34;&gt;Branching&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s go ahead and test our program by resizing one of our images. I&amp;rsquo;m going to to resize &lt;code&gt;uta.png&lt;/code&gt; to have a width of 500 pixels using the following command.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;python pyresize.py test_imgs/uta.png &lt;span style=&#34;color:#ae81ff&#34;&gt;500&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Our program doesn&amp;rsquo;t support creating a new file when resizing. Instead, it resizes the file and overwrites the original. We should add this feature and modify it without messing up our current code base. This is where branching comes in. We can create a new branch that will contain our new feature. We can then test it and merge it back into the main branch once we are happy with it.&lt;/p&gt;
&lt;p&gt;Every &lt;code&gt;commit&lt;/code&gt; that we make is a snapshot of the entire project up to that point. There is a unique identifier attached to each commit. If we want to work on a specific bug or new feature without affecting the current code base, we can create a branch to track those changes independently of the other branches. The main benefits are that we can potentially break the code base without affecting the production-ready code. We can also work on multiple features at the same time without affecting each other.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s create a new branch called &lt;code&gt;output_write&lt;/code&gt;. We can do this with the &lt;code&gt;git branch&lt;/code&gt; command.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git branch output_write
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;





&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_20-38-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;The result of creating a new branch named `testing`. ([source](&amp;lt;https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell&amp;gt;))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;The result of creating a new branch named &lt;code&gt;testing&lt;/code&gt;. (&lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;source&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This only creates a new branch, but we are still on the &lt;code&gt;main&lt;/code&gt; branch. We can see this by running &lt;code&gt;git branch&lt;/code&gt;. The current branch will be highlighted with an asterisk. The current branch is pointed to by the &lt;code&gt;HEAD&lt;/code&gt; pointer. We can switch to the new branch using &lt;code&gt;git checkout&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git checkout output_write
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;





&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_20-40-25_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;The `HEAD` pointer after switching to a new branch. ([source](&amp;lt;https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell&amp;gt;))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;The &lt;code&gt;HEAD&lt;/code&gt; pointer after switching to a new branch. (&lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;source&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;modifying-the-code&#34;&gt;Modifying the Code&lt;/h3&gt;
&lt;p&gt;Now that we are on the &lt;code&gt;output_write&lt;/code&gt; branch, we can modify the code without affecting the &lt;code&gt;main&lt;/code&gt; branch. Let&amp;rsquo;s modify our original function to take in an additional argument: the output path. We can then use this path to save the resized image to a new file. Our new code will look like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sys
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; PIL &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Image
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;resize_image&lt;/span&gt;(image_path, width, output_path):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(image_path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    wpercent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (width &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; float(image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;size[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    hsize &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int((float(image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;size[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; float(wpercent)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;resize((width, hsize), Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;LANCZOS)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;save(output_path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; __name__ &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;__main__&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    resize_image(sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argv[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], int(sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argv[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]), sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argv[&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Go ahead and commit these changes. Since we have already moved the &lt;code&gt;HEAD&lt;/code&gt; pointer to the new branch, this change will not affect our &lt;code&gt;main&lt;/code&gt; branch. The figure below is analagous to this scenario.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_20-45-17_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;The result of committing changes to a new branch. ([source](&amp;lt;https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell&amp;gt;))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;The result of committing changes to a new branch. (&lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;source&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Let&amp;rsquo;s test our program once more by resizing an image and saving it to the &lt;code&gt;output&lt;/code&gt; directory. We can do this with the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;python pyresize.py test_imgs/uta.png &lt;span style=&#34;color:#ae81ff&#34;&gt;500&lt;/span&gt; output/uta.png
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Notice that when you run &lt;code&gt;git status&lt;/code&gt; after resizing the image and saving to the &lt;code&gt;output&lt;/code&gt; directory, it does not report any changes. Our ignore file is working as intended!&lt;/p&gt;
&lt;h2 id=&#34;merging&#34;&gt;Merging&lt;/h2&gt;
&lt;p&gt;Now that we have completed our new feature and tested it, we should merge these changes back to the &lt;code&gt;main&lt;/code&gt; branch. We can do this with the &lt;code&gt;git merge&lt;/code&gt; command. First, we need to switch back to the &lt;code&gt;main&lt;/code&gt; branch.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git checkout main
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can then merge the &lt;code&gt;output_write&lt;/code&gt; branch into the &lt;code&gt;main&lt;/code&gt; branch.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git merge output_write
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will merge the changes from the &lt;code&gt;output_write&lt;/code&gt; branch into the &lt;code&gt;main&lt;/code&gt; branch. If there are any conflicts, Git will let you know and you can resolve them manually. Once the merge is complete, you can delete the &lt;code&gt;output_write&lt;/code&gt; branch.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git branch -d output_write
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;





&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_20-51-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;The result of merging a branch into the `master` branch. ([source](&amp;lt;https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell&amp;gt;))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;The result of merging a branch into the &lt;code&gt;master&lt;/code&gt; branch. (&lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;source&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The figure above shows the history of a repository in which a branch named &lt;code&gt;iss53&lt;/code&gt; was created, modified with new commits, and eventually merged back into the &lt;code&gt;master&lt;/code&gt; branch.&lt;/p&gt;
&lt;h2 id=&#34;remotes&#34;&gt;Remotes&lt;/h2&gt;
&lt;p&gt;We have now covered the basics of using Git locally. Eventually, we will want our changes to be backed up on a remote server. This will allow us to collaborate with others and work on our projects from multiple machines. There are many services that provide this functionality. We will use GitHub for this example, but the process is similar for other services.&lt;/p&gt;
&lt;h3 id=&#34;creating-a-repository&#34;&gt;Creating a Repository&lt;/h3&gt;
&lt;p&gt;First, we need to create a new repository on GitHub. You can do this by clicking the &lt;code&gt;New&lt;/code&gt; button on the &lt;a href=&#34;https://github.com&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;GitHub homepage&lt;/a&gt;. I am only going to add a short description of this program. Go ahead and click &lt;code&gt;Create repository&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;GitHub supports both SSH and HTTPS. I already have an SSH key set up. If you haven&amp;rsquo;t configured one yet, check out &lt;a href=&#34;https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Adding a new SSH key to your GitHub account&lt;/a&gt; for instructions on how to do so. You can also use HTTPS, this requires a personal access token. More information can be found at &lt;a href=&#34;https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Creating a personal access token&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Once created, we will have the option to use either our HTTPS or SSH URL. Mine is &lt;code&gt;git@github.com:ajdillhoff/pyresize.git&lt;/code&gt;. We can add this as a remote repository using the &lt;code&gt;git remote add&lt;/code&gt; command.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git remote add origin git@github.com:ajdillhoff/pyresize.git
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;pushing-to-a-remote&#34;&gt;Pushing to a Remote&lt;/h3&gt;
&lt;p&gt;Now that we have a remote repository, we can push our changes to it. We can do this with the &lt;code&gt;git push&lt;/code&gt; command. However, we need to specify the remote repository and the branch that we want to push. We can do this with the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git push -u origin main
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;That&amp;rsquo;s it! Our changes are now backed up on GitHub. We can view our repository by navigating to the URL in our browser. We can also view the commit history by clicking the &lt;code&gt;Commits&lt;/code&gt; link.&lt;/p&gt;
&lt;h2 id=&#34;cloning-an-existing-repository&#34;&gt;Cloning an Existing Repository&lt;/h2&gt;
&lt;p&gt;If our repository already exists on GitHub, we can clone it to our local machine. This will create a new directory with the same name as the repository. We can do this with the &lt;code&gt;git clone&lt;/code&gt; command. Let&amp;rsquo;s clone the &lt;code&gt;pyresize&lt;/code&gt; repository that we just created.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git clone git@github.com:ajdillhoff/pyresize.git
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can clone using either the HTTPS or SSH URLs. Make sure you have the appropriate key or access token to do so.&lt;/p&gt;
&lt;h3 id=&#34;pulling-from-a-remote&#34;&gt;Pulling from a Remote&lt;/h3&gt;
&lt;p&gt;Now that we have cloned the repository, we can make changes and push them to the remote. However, if someone else makes changes to the remote repository, we will need to pull those changes to our local repository. We can do this with the &lt;code&gt;git pull&lt;/code&gt; command.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git pull origin main
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Git will always require that we are up-to-date with the remote before we can push our changes. If someone else has made changes to the remote, we will need to pull those changes before we can push our own. This is to prevent conflicts.&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;We have covered the basics of using Git. We have created a repository, staged and committed changes, created branches, merged branches, and pushed our changes to a remote repository. There are many more features that we have not covered, but this should be enough to get you started. If you are interested in learning more, check out the &lt;a href=&#34;https://git-scm.com/book/en/v2&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Git Book&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Command Reference&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Command&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git init&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Create a new repository&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git config&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Configure Git&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git status&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;View the status of your repository&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git add&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Add files to the staging area&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git commit&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Commit changes to the repository&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git branch&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Create, list, or delete branches&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git checkout&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Switch branches or restore working tree files&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git merge&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Join two or more development histories together&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git remote&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Manage set of tracked repositories&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git push&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Update remote refs along with associated objects&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git clone&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Clone a repository into a new directory&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git pull&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fetch from and integrate with another repository or a local branch&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git log&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Show commit logs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git rm&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Remove files from the working tree and from the index&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git mv&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Move or rename a file, a directory, or a symlink&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git branch -d&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Delete a branch&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git remote add&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Add a remote repository&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Python List Comprehensions</title>
      <link>https://ajdillhoff.github.io/notes/list_comprehensions/</link>
      <pubDate>Sat, 26 Aug 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/list_comprehensions/</guid>
      <description>&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/ajdillhoff/python-examples/blob/main/basics/list_comprehensions.ipynb&#34;&gt;
  &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;/&gt;
&lt;/a&gt;
&lt;h1 id=&#34;lists-and-list-comprehensions&#34;&gt;Lists and List Comprehensions&lt;/h1&gt;
&lt;p&gt;List comprehensions provide a concise way to create lists, and are often faster than using a for-loop. They are inspired by set-builder notation in mathematics.&lt;/p&gt;
&lt;p&gt;This notebook demonstrates common list functions as well as the syntax and basic usage of list comprehensions.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Naomi&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;James&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Amos&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bobbie&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Append &amp;#34;Miller&amp;#34; to the end of the list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Miller&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# This can also be accomplished using the + operator or the extend() method&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# These are similar, where `+=` is shorthand for `extend`&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extend([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Chrisjen&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Alex&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# `extend()` works on any iterable&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extend([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Fred&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Dawes&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Ashford&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# This can also be done using slicing&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# names[len(names):] = [&amp;#34;Fred&amp;#34;, &amp;#34;Dawes&amp;#34;, &amp;#34;Ashford&amp;#34;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Insert &amp;#34;Holden&amp;#34; at the beginning of the list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;insert(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Holden&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can easily remove items&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;remove(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Alex&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can also remove items by value&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;remove(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Ashford&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can also remove items by index. This will return the removed item&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;eros_passenger &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pop(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(eros_passenger &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; is on his way to Venus&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# If your list has duplicates, you can count the number of times a value appears&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Naomi&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; appears &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; str(names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Naomi&amp;#34;&lt;/span&gt;)) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; time(s)&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Reversing a list is easy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reverse()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can also create a shallow copy of a list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names_copy &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Miller is on his way to Venus
Naomi appears 1 time(s)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sorting&#34;&gt;Sorting&lt;/h2&gt;
&lt;p&gt;A list can be sorted by calling the &lt;code&gt;sort&lt;/code&gt; function. The list is sorted in-place, meaning that the original list is modified. Since a list can contain any type of object, the objects must be comparable to each other. For example, a list of strings can be sorted alphabetically, but a list of strings and integers cannot be sorted.&lt;/p&gt;
&lt;p&gt;In cases with mixed types, a custom &lt;code&gt;key&lt;/code&gt; function can be passed to the &lt;code&gt;sort&lt;/code&gt; function. The &lt;code&gt;key&lt;/code&gt; function is called on each element of the list, and the return value is used to sort the list. For example, to sort a list of strings and integers by the length of the string, the &lt;code&gt;key&lt;/code&gt; function would transform the integers into strings.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; l &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;abc&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ab&amp;#39;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; l&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sort(key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;str)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; l
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;abc&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ab&amp;#39;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Sorting is a very common operation, and Python gives us some level of control over how the items are sorted.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# The default is to sort in ascending order&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Sort the list in ascending order&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sort()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(names)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Sort the list in descending order&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sort(reverse&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(names)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can also sort by a key function, such as the length of each name&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sort(key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;len)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(names)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[&#39;Amos&#39;, &#39;Bobbie&#39;, &#39;Chrisjen&#39;, &#39;Dawes&#39;, &#39;Fred&#39;, &#39;Holden&#39;, &#39;James&#39;, &#39;Naomi&#39;]
[&#39;Naomi&#39;, &#39;James&#39;, &#39;Holden&#39;, &#39;Fred&#39;, &#39;Dawes&#39;, &#39;Chrisjen&#39;, &#39;Bobbie&#39;, &#39;Amos&#39;]
[&#39;Fred&#39;, &#39;Amos&#39;, &#39;Naomi&#39;, &#39;James&#39;, &#39;Dawes&#39;, &#39;Holden&#39;, &#39;Bobbie&#39;, &#39;Chrisjen&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s introduce a slightly more complex scenario in which each person in the list rolls a 20-sided die. We&amp;rsquo;ll use the random module to generate a random number between 1 and 20 for each person. Using those values, we can then sort the list by the roll of the die, in descending order.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; random
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Roll a 20-sided die for each person using a for loop&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# rolls = []&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# for name in names:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#     rolls.append(random.randint(1, 20))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# The above code is commented out by default because it can be written more succinctly using a list comprehension&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rolls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; names]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(names)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;index(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Fred&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Sort the names by the roll of the die, in descending order&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# First, create a function that returns the roll based on the name&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_roll&lt;/span&gt;(name):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# print(names) # reveals an empty list!&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; rolls[names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;index(name)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# names.sort(key=get_roll, reverse=True)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# The above will not work because `names` does not exist within the scope of the function.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can instead combine the rolls with the names using the zip() function&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names_and_rolls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(zip(names, rolls))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names_and_rolls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sort(key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; roll: roll[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], reverse&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(names_and_rolls)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[&#39;Fred&#39;, &#39;Amos&#39;, &#39;Naomi&#39;, &#39;James&#39;, &#39;Dawes&#39;, &#39;Holden&#39;, &#39;Bobbie&#39;, &#39;Chrisjen&#39;]
[(&#39;Dawes&#39;, 19), (&#39;Fred&#39;, 18), (&#39;Amos&#39;, 12), (&#39;Naomi&#39;, 11), (&#39;Bobbie&#39;, 10), (&#39;Holden&#39;, 8), (&#39;Chrisjen&#39;, 7), (&#39;James&#39;, 5)]
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;list-comprehensions&#34;&gt;List Comprehensions&lt;/h1&gt;
&lt;p&gt;We can also create nested list comprehensions, which is equivalent to nested for loops. For example, let&amp;rsquo;s create a 3x4 matrix using a nested list comprehension.&lt;/p&gt;
&lt;h2 id=&#34;nested-list-comprehension&#34;&gt;Nested list comprehension&lt;/h2&gt;
&lt;h2 id=&#34;advanced-list-comprehension&#34;&gt;Advanced list comprehension&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Nested list comprehension&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create a 2D list where each row represents the top 5 rolls for each person&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;top_rolls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; names]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(top_rolls)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can create a similar 2D list where each row is a tuple of the name and the top 5 rolls&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;top_rolls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [(name, [random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)]) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; name &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; names]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(top_rolls)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# If we wrote this with loops, it would look like this:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;top_rolls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; name &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; names:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    rolls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        rolls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    top_rolls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append((name, rolls))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[[7, 1, 11, 15, 14], [3, 14, 15, 10, 9], [7, 16, 17, 15, 19], [12, 6, 10, 17, 19], [10, 4, 7, 17, 11], [16, 1, 14, 2, 12], [3, 10, 2, 12, 13], [7, 17, 16, 3, 6]]
[(&#39;Fred&#39;, [15, 7, 1, 3, 16]), (&#39;Amos&#39;, [7, 15, 9, 4, 6]), (&#39;Naomi&#39;, [19, 7, 12, 5, 6]), (&#39;James&#39;, [6, 11, 4, 10, 17]), (&#39;Dawes&#39;, [13, 5, 15, 19, 8]), (&#39;Holden&#39;, [14, 18, 16, 5, 15]), (&#39;Bobbie&#39;, [12, 15, 8, 18, 20]), (&#39;Chrisjen&#39;, [18, 16, 19, 10, 11])]
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;performance-of-list-comprehensions-versus-for-loops&#34;&gt;Performance of list comprehensions versus &lt;code&gt;for&lt;/code&gt; loops&lt;/h1&gt;
&lt;p&gt;A strong argument for list comprehensions is that they are more elegant and easier to read. However, they are also faster than for loops. Let&amp;rsquo;s compare the performance of list comprehensions and for loops.&lt;/p&gt;
&lt;p&gt;Two common benchmarks to test their performance are to append numbers to a list and to square numbers. Let&amp;rsquo;s compare the performance of list comprehensions and for loops for these two tasks.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; timeit
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Benchmark #1: Append vs. List Comprehension&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Append&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;for_append&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    names &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1000000&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(i)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# print the average out of 10 runs (in milliseconds)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Append: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; str(timeit&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;timeit(for_append, number&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# List Comprehension&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;list_comprehension&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    names &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [i &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1000000&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# print the average out of 10 runs (in milliseconds)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Append: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; str(timeit&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;timeit(for_append, number&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Append: 371.44755799999984
Append: 351.0218179999356
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Benchmark 2: Squaring Numbers&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# For loop&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;for_loop&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    squares &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1000000&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        squares&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(i&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# print the average out of 10 runs (in milliseconds)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;For Loop: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; str(timeit&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;timeit(for_loop, number&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# List Comprehension&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;list_comprehension&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    squares &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [i&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1000000&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# print the average out of 10 runs (in milliseconds)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;List Comprehension: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; str(timeit&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;timeit(list_comprehension, number&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;For Loop: 593.4785219997138
List Comprehension: 572.2285600004398
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Control Flow in Python</title>
      <link>https://ajdillhoff.github.io/notes/control_flow_in_python/</link>
      <pubDate>Tue, 22 Aug 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/control_flow_in_python/</guid>
      <description>&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/ajdillhoff/python-examples/blob/main/basics/control_flow.ipynb&#34;&gt;
  &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;/&gt;
&lt;/a&gt;
&lt;h1 id=&#34;control-flow&#34;&gt;Control Flow&lt;/h1&gt;
&lt;p&gt;Control flow allows us to build programs that react to some pre-determined condition. For example, what happens when a user logs in with the correct credentials? What if they don&amp;rsquo;t give valid credentials?&lt;/p&gt;
&lt;p&gt;This notebook covers the basic tools to writing conditional statements in Python. It follows Chapter 3 in &lt;a href=&#34;https://www.py4e.com/html3/03-conditional&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Python for Everyone&lt;/a&gt; by Charles Severance along with my own examples.&lt;/p&gt;
&lt;h2 id=&#34;boolean-expressions&#34;&gt;Boolean Expressions&lt;/h2&gt;
&lt;p&gt;A boolean expression evaluates to either &lt;code&gt;True&lt;/code&gt; or &lt;code&gt;False&lt;/code&gt;. This type of expression would be used to check status codes or to check if a user has entered the correct password, for example.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1==1 is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1==2 is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;1==1 is True
1==2 is False
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Operators like &lt;code&gt;==&lt;/code&gt; are called &lt;em&gt;relational operators&lt;/em&gt; and they compare two operands and return either &lt;code&gt;True&lt;/code&gt; or &lt;code&gt;False&lt;/code&gt;. Other relational operators include:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# x is not equal to y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# x is greater than y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# x is less than y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# x is greater than or equal to y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# x is less than or equal to y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# x is the same as y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# x is not the same as y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The last two operators, &lt;code&gt;is&lt;/code&gt; and &lt;code&gt;is not&lt;/code&gt;, are used to check if two variables are referencing the same object. The fact that the operands must be &lt;em&gt;objects&lt;/em&gt; is important here. You should avoid comparing a value with a variable. Python will let you do this, but it will also output a warning.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# True, but not recommended&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# False, but not recommended&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;gt;:3: SyntaxWarning: &amp;quot;is&amp;quot; with a literal. Did you mean &amp;quot;==&amp;quot;?
&amp;lt;&amp;gt;:3: SyntaxWarning: &amp;quot;is&amp;quot; with a literal. Did you mean &amp;quot;==&amp;quot;?
/var/folders/vd/wbzsx0g538nfr96xq81fp7k40000gn/T/ipykernel_24914/759655086.py:3: SyntaxWarning: &amp;quot;is&amp;quot; with a literal. Did you mean &amp;quot;==&amp;quot;?
  x is 5





True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Python includes three logical operators that are verbose compared to other languages.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# True if both x and y are True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# True if either x or y are True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; x &lt;span style=&#34;color:#75715e&#34;&gt;# True if x is False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Example: FizzBuzz&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Consider two possible solutions to the FizzBuzz problem&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Solution 1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;FizzBuzz&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Solution 2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Fizz&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Buzz&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;FizzBuzz
Fizz
Buzz
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the second solution, the output was separated to two separate lines since the &lt;code&gt;print&lt;/code&gt; function automatically adds a newline. We can change this behavior by adding a second argument to the &lt;code&gt;print&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Fizz&amp;#34;&lt;/span&gt;, end&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Buzz&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;FizzBuzz
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;conditional-execution&#34;&gt;Conditional Execution&lt;/h2&gt;
&lt;p&gt;We have already used a key conditional execution tool: the &lt;code&gt;if&lt;/code&gt; statement. The &lt;code&gt;if&lt;/code&gt; statement allows us to execute a block of code if a condition is met. The general syntax is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; condition:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if condition is True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Also note that Python is particular about indentation. The code that is executed if the condition is met must be indented. The standard is to use four spaces for each level of indentation.&lt;/p&gt;
&lt;p&gt;We can also chain conditional statements together using &lt;code&gt;elif&lt;/code&gt; and &lt;code&gt;else&lt;/code&gt;. The &lt;code&gt;elif&lt;/code&gt; statement is short for &amp;ldquo;else if&amp;rdquo; and allows us to check another condition if the previous condition was not met. The &lt;code&gt;else&lt;/code&gt; statement is used to execute code if none of the previous conditions were met. The general syntax is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; condition:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if condition is True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; condition:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if the first condition is False and this condition is True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if all other conditions are False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;switch-statements&#34;&gt;Switch Statements&lt;/h3&gt;
&lt;p&gt;Until version 3.10, Python did not have a &lt;code&gt;switch&lt;/code&gt; statement. This is a conditional statement that allows us to check a variable against a series of values.&lt;/p&gt;
&lt;p&gt;With version 3.10 comes the &lt;code&gt;match&lt;/code&gt; statement. This statement is similar to the &lt;code&gt;switch&lt;/code&gt; statement in other languages. The general syntax is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;match&lt;/span&gt; variable:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; value1:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if variable == value1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; value2:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if variable == value2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; value3:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if variable == value3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; _:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if none of the previous conditions were met&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;language &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; input(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;What is your favorite programming language? &amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;match&lt;/span&gt; language:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Python&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;You&amp;#39;re in the right place.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Java&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Do you despise C++ as much as the creator of Java?&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;C++&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;You probably like game development.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;C&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Speed is your thing.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; _:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;You like something else!&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;You like something else!
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unlike other languages that implement a &lt;code&gt;switch&lt;/code&gt; statement, Python&amp;rsquo;s &lt;code&gt;match&lt;/code&gt; statement does not have a &lt;code&gt;break&lt;/code&gt; statement. We can still utilize fall-through behavior by including multiple values in a single case separated by &lt;code&gt;|&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;match&lt;/span&gt; variable:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; value1 &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; value2:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if variable == value1 or variable == value2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; value3:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if variable == value3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; _:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if none of the previous conditions were met&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;language &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; input(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;What is your favorite programming language? &amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;match&lt;/span&gt; language:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Python&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;python&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;You&amp;#39;re in the right place.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Java&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Do you despise C++ as much as the creator of Java?&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;C++&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;You probably like game development.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;C&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Speed is your thing.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; _:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;You like something else!&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;You like something else!
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;iterations&#34;&gt;Iterations&lt;/h2&gt;
&lt;p&gt;Iterations allow us to execute a block of code multiple times. This is useful for iterating over a list of items or for executing a block of code until a condition is met.&lt;/p&gt;
&lt;p&gt;Python supports both a &lt;code&gt;while&lt;/code&gt; loop and a &lt;code&gt;for&lt;/code&gt; loop. The &lt;code&gt;while&lt;/code&gt; loop will execute a block of code until a condition is met. The &lt;code&gt;for&lt;/code&gt; loop will iterate over a sequence of items.&lt;/p&gt;
&lt;h3 id=&#34;for-loops&#34;&gt;For Loops&lt;/h3&gt;
&lt;p&gt;As opposed to something like C, Python&amp;rsquo;s &lt;code&gt;for&lt;/code&gt; loop is more like a &lt;code&gt;foreach&lt;/code&gt; loop. The &lt;code&gt;for&lt;/code&gt; loop will iterate over a sequence of items. The general syntax is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; item &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sequence:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute for each item in the sequence&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It is commonly used with the &lt;code&gt;range&lt;/code&gt; function to iterate over a sequence of numbers. The &lt;code&gt;range&lt;/code&gt; function takes three arguments: &lt;code&gt;start&lt;/code&gt;, &lt;code&gt;stop&lt;/code&gt;, and &lt;code&gt;step&lt;/code&gt;. The &lt;code&gt;start&lt;/code&gt; argument is the first number in the sequence. The &lt;code&gt;stop&lt;/code&gt; argument is the last number in the sequence. The &lt;code&gt;step&lt;/code&gt; argument is the amount to increment the sequence by. The &lt;code&gt;step&lt;/code&gt; argument is optional and defaults to &lt;code&gt;1&lt;/code&gt;. The &lt;code&gt;stop&lt;/code&gt; argument is required. The &lt;code&gt;start&lt;/code&gt; argument is optional and defaults to &lt;code&gt;0&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(i)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;while-loops&#34;&gt;While Loops&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;while&lt;/code&gt; loop will execute a block of code until a condition is met. The general syntax is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; condition:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute while condition is True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;lists&#34;&gt;Lists&lt;/h2&gt;
&lt;p&gt;Lists are a sequence of values. They are similar to arrays in other languages. The values in a list are called &lt;em&gt;elements&lt;/em&gt; or &lt;em&gt;items&lt;/em&gt;. Lists are mutable, meaning that we can change the values in a list. Lists are also ordered, meaning that the order of the elements in a list is important.&lt;/p&gt;
&lt;p&gt;We can create a list by separating the elements with commas and surrounding the list with square brackets.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Naomi&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bobbie&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;James&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Amos&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Chrisjen&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Alex&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Clarissa&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names_and_numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Naomi&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bobbie&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;James&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Amos&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;11&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Chrisjen&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;13&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Alex&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Clarissa&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;17&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can even include lists in our lists&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;nested_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;iterating-over-lists&#34;&gt;Iterating Over Lists&lt;/h3&gt;
&lt;p&gt;We can iterate over a list using a &lt;code&gt;for&lt;/code&gt; loop. The &lt;code&gt;for&lt;/code&gt; loop will iterate over each element in the list. The general syntax is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; item &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; list:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute for each item in the list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If the list contains tuples, we can use tuple unpacking to assign the values in the tuple to multiple variables.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;), (&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;), (&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; x, y &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; numbers:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(x, y)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;combining-lists-with-zip&#34;&gt;Combining Lists with &lt;code&gt;zip&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;zip&lt;/code&gt; function allows us to combine two lists into a single list of tuples. The first element in the first list will be paired with the first element in the second list, the second element in the first list will be paired with the second element in the second list, and so on. The general syntax is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;user_ids &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;usernames &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;alice&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;bob&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;charlie&amp;#39;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;users &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; zip(user_ids, usernames)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;user_ids &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;user_names &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Naomi&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bobbie&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;James&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Amos&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Chrisjen&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can combine these lists into a single list of tuples&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;user_ids_and_names &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; zip(user_ids, user_names)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can also convert the zip object into a list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;user_ids_and_names &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(user_ids_and_names)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; users &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; user_ids_and_names:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(users)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;(1, &#39;Naomi&#39;)
(2, &#39;Bobbie&#39;)
(3, &#39;James&#39;)
(4, &#39;Amos&#39;)
(5, &#39;Chrisjen&#39;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Python</title>
      <link>https://ajdillhoff.github.io/notes/introduction_to_python/</link>
      <pubDate>Sun, 20 Aug 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/introduction_to_python/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#programming-with-python&#34;&gt;Programming with Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variables-values-and-data-types&#34;&gt;Variables, Values, and Data Types&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#basic-operators&#34;&gt;Basic Operators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#statements-and-expressions&#34;&gt;Statements and Expressions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#basic-i-o&#34;&gt;Basic I/O&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#commenting-code&#34;&gt;Commenting Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;These notes are focused on introducing programming with Python for those without a technical background.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://docs.python.org/3/faq/general.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;official website&lt;/a&gt; provides the following description of Python.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Python is an interpreted, interactive, object-oriented programming language. It incorporates modules, exceptions, dynamic typing, very high level dynamic data types, and classes. It supports multiple programming paradigms beyond object-oriented programming, such as procedural and functional programming. Python combines remarkable power with very clear syntax. It has interfaces to many system calls and libraries, as well as to various window systems, and is extensible in C or C++. It is also usable as an extension language for applications that need a programmable interface. Finally, Python is portable: it runs on many Unix variants including Linux and macOS, and on Windows.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you have never studied any programming languages before, much of this description will be useless to you. There is no context for the types of programming (object-oriented, functional, etc.), and the fact that is extensible in C or C++ may mean absolutely nothing.&lt;/p&gt;
&lt;h3 id=&#34;why-python&#34;&gt;Why Python?&lt;/h3&gt;
&lt;p&gt;Given that this course is for prospective data science practitioners, and the fact that we have less than a month to cover a programming language, Python is a natural choice. It is widely used in the field of Machine Learning and is gaining more and more ground over R (or so I think) for statistics. There are many third-party libraries for data analysis, visualization, and just about any other data science application we can think of.&lt;/p&gt;
&lt;h3 id=&#34;how-to-use-these-notes&#34;&gt;How to use these notes&lt;/h3&gt;
&lt;p&gt;These notes are organized to follow each major topic in Python. They will also follow the free online book &lt;a href=&#34;https://www.py4e.com/book&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Python for Everybody&lt;/a&gt;. These particular lecture notes will start with Chapter 2: Variables, expressions, and statements. It is highly recommended that you run the examples on your own machine, and you are encouraged to make changes. Try to break the code and fix it again. Have it output something different and change the program&amp;rsquo;s purpose entirely.&lt;/p&gt;
&lt;p&gt;A Python notebook will accompany each lecture and will be accessible on &lt;a href=&#34;https://github.com/ajdillhoff/python-examples&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;my GitHub page&lt;/a&gt;. I will also include code snippets directly in this article to highlight a particular example or point.&lt;/p&gt;
&lt;h3 id=&#34;programming-is-hard&#34;&gt;Programming is Hard&lt;/h3&gt;
&lt;p&gt;Before we dive into the language itself, there are a few things that are important to keep in mind. First, &lt;strong&gt;programming is hard&lt;/strong&gt;. It is a skill that requires practice. The tools that you use to program are constantly evolving to keep up with the use-cases of the day. A processor is a complex calculator which means we have to be extremely explicit about the instructions we provide. If you are picking this up for the first time, remember to be patient and be kind to yourself. You will be able to work on big projects that are important to you, but we all have to start somewhere.&lt;/p&gt;
&lt;h3 id=&#34;resources-are-finite&#34;&gt;Resources are Finite&lt;/h3&gt;
&lt;p&gt;Another important thing to remember is that we are working with limited resources. There is only so much memory and storage space that we can access. These notes are not meant to accompany a full course on hardware architectures, so we will use the following diagram to visualize this point.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-20_13-52-14_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Figure 1.3 from Python for Everybody.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Figure 1.3 from Python for Everybody.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The closer the memory is to the CPU, the quicker it can be accessed. Most of the algorithms and data structures we will study in this course will be used with data in &lt;strong&gt;main memory&lt;/strong&gt;. The trade off is that memory that is closer to the CPU is more expensive and has reduced capacity when compared with secondary memory. When we start working with larger sources of data, we will need to adapt our solutions to work with memory that is not directly accessible through a local machine. For now, keep this picture in mind as we dive into Python.&lt;/p&gt;
&lt;h2 id=&#34;programming-with-python&#34;&gt;Programming with Python&lt;/h2&gt;
&lt;p&gt;Programming languages provide the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a way to write instructions (&lt;strong&gt;syntax, statements, expressions&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;a way to execute a complex series of instructions (&lt;strong&gt;functions&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;a way to store the results of computations and represent data (&lt;strong&gt;variables, data structures&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They mostly differ in how the language is written, the &lt;strong&gt;syntax&lt;/strong&gt;. Consider the following snippet of Python code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;username &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;test&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;password &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;securePass1&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;user_logged_in &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; login(username, password) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    user_logged_in &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Even if you have never seen any Python code before, you could probably figure out what this block of code is doing. First, 3 &lt;strong&gt;variables&lt;/strong&gt; are defined which store the username, password, and a &lt;em&gt;flag&lt;/em&gt; that represents whether or not the user is logged into the system.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;control statement&lt;/strong&gt; &lt;code&gt;if&lt;/code&gt; is declared to evaluate if an &lt;strong&gt;expression&lt;/strong&gt; is true. This expression calls a function named &lt;code&gt;login&lt;/code&gt; and passes the user&amp;rsquo;s credentials as its arguments. We can assume that the call to &lt;code&gt;login&lt;/code&gt; is doing something like validating the user&amp;rsquo;s information and registering their request with a server. If the user was successfully logged in, the function will &lt;code&gt;return True&lt;/code&gt;. In this case, we can updated our variable &lt;code&gt;user_logged_in&lt;/code&gt; to reflect this.&lt;/p&gt;
&lt;h2 id=&#34;variables-values-and-data-types&#34;&gt;Variables, Values, and Data Types&lt;/h2&gt;
&lt;p&gt;The first 3 lines in the example above are variable initializations. The first line is &lt;code&gt;username = &amp;quot;test&amp;quot;&lt;/code&gt; which instructs our machines to create a new &lt;strong&gt;variable&lt;/strong&gt; named &lt;code&gt;username&lt;/code&gt; and assign it the value &lt;code&gt;&amp;quot;test&amp;quot;&lt;/code&gt;. All variables require memory to store their &lt;strong&gt;values&lt;/strong&gt;. When a variable is created, our machine will assign it an address so that it knows where to access that variable&amp;rsquo;s value. This concept is rather simple: &lt;strong&gt;in order for something to exist, there must be space for it.&lt;/strong&gt; As Python developers, we will rarely think about where and how these values are being stored.&lt;/p&gt;
&lt;p&gt;Most languages have rules about what names we can give to variables, and Python is no exception. A variable can use any combination of letters, numbers, and underscores, as long as it does not start with a number and is not the same as a &lt;strong&gt;reserved word&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reserved words in Python&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;and     continue    finally     is          raise
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;as      def         for         lambda      return
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;assert  del         from        None        True
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;async   elif        global      nonlocal    try
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;await   else        if          not         while
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;break   except      import      or          with
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;class   False       in          pass        yield
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;data-types&#34;&gt;Data Types&lt;/h3&gt;
&lt;p&gt;Different &lt;strong&gt;values&lt;/strong&gt; are represented differently depending on their &lt;strong&gt;type&lt;/strong&gt;. An integer can be represented in binary in a very straightforward manner. &lt;code&gt;10&lt;/code&gt; in base 10 is represented as &lt;code&gt;1010&lt;/code&gt; in binary, for example. Characters in a &lt;code&gt;string&lt;/code&gt; like &lt;code&gt;&amp;quot;securePass1&amp;quot;&lt;/code&gt; are represented using an encoding such as &lt;a href=&#34;https://en.wikipedia.org/wiki/ASCII&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;ASCII&lt;/a&gt; or &lt;a href=&#34;https://en.wikipedia.org/wiki/Unicode&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Unicode&lt;/a&gt;. Real numbers are typically represented as floating-point types using the &lt;a href=&#34;https://en.wikipedia.org/wiki/IEEE_754&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;IEEE 754 Standard for Floating-Point Arithmetic&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When we create a variable in Python, we do not need to explicitly declare what type that variable it is. That is what makes Python &lt;strong&gt;dynamically typed&lt;/strong&gt;. Instead, it will infer the type based on the value. We can always ask Python how it is representing each variable, as seen in the following code.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt; type&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;securePass1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;lt;class &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;str&amp;#39;&lt;/span&gt;&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt; type&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;10&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;lt;class &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;int&amp;#39;&lt;/span&gt;&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt; type&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;3.14&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;lt;class &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;float&amp;#39;&lt;/span&gt;&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;basic-operators&#34;&gt;Basic Operators&lt;/h2&gt;
&lt;p&gt;A programming language would be pretty useless if it did not offer some way to do basic arithmetic. Python supports the following arithmetic operators: &lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt;, &lt;code&gt;//&lt;/code&gt;, &lt;code&gt;**&lt;/code&gt; and &lt;code&gt;%&lt;/code&gt;. You may instantly recognize the first 4, but the last 3 may not be so familiar. Let us start with &lt;code&gt;//&lt;/code&gt;, integer division.&lt;/p&gt;
&lt;p&gt;An integer data type cannot represent decimal values. So what happens if you try to execute something like &lt;code&gt;1 / 2&lt;/code&gt;? We recognize this to be &lt;code&gt;0.5&lt;/code&gt;, but that is not the case with every programming language. In C, for example, &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;2&lt;/code&gt; are treated as integer types by default. If you attempt to evaluate &lt;code&gt;1 / 2&lt;/code&gt;, the result is &lt;code&gt;0&lt;/code&gt; since there is no way to store the decimal information. Essentially, the decimal portion of the result is &lt;strong&gt;truncated&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Python is a little more forgiving. &lt;code&gt;1 / 2&lt;/code&gt; evaluates to &lt;code&gt;0.5&lt;/code&gt;, as we may expect. However, if you want to perform division between these operands as if they were both integers, you can use &lt;code&gt;//&lt;/code&gt;. Indeed, &lt;code&gt;1 // 2&lt;/code&gt; evalutes to &lt;code&gt;0&lt;/code&gt; in Python.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;**&lt;/code&gt; operator is more straightforward: it raises the left-hand operand to whatever value is provided on the right side. For example, &lt;code&gt;2**4&lt;/code&gt; evaluates to &lt;code&gt;16&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally, the modulus operator &lt;code&gt;%&lt;/code&gt; provides the remainder of integer division. So something like &lt;code&gt;5 % 2&lt;/code&gt; would return &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;statements-and-expressions&#34;&gt;Statements and Expressions&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;statement&lt;/strong&gt; is anything that can be executed. This could be a simple variable assignment or a function call.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;username &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;user1&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(username &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;user1&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;An &lt;strong&gt;expression&lt;/strong&gt; is a statement that evaluates into some result. This could be the result of a function call, an assignment, or a complex computation.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;get_user_by_id(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;basic-i-o&#34;&gt;Basic I/O&lt;/h2&gt;
&lt;p&gt;Python provides functions to read input from the user&amp;rsquo;s keyboard as well as print information back to the terminal using &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;print&lt;/code&gt;. When &lt;code&gt;input&lt;/code&gt; is evaluated, it will wait for the user to press &lt;code&gt;Enter&lt;/code&gt; before processing the input. If you would like to provide a text prompt to the user before entering, you can pass the prompt as a string.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; input(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Enter some text: &amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(text)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;An example run of this program may look like the following.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Enter some text: OK here it is
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;OK here it is
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Notice that the result of the &lt;code&gt;input&lt;/code&gt; function is the actual data entered by the user. This is immediately assigned to the &lt;code&gt;text&lt;/code&gt; variable. We can easily print out the value of &lt;code&gt;text&lt;/code&gt; by passing it as an argument to the &lt;code&gt;print&lt;/code&gt; function.&lt;/p&gt;
&lt;h3 id=&#34;formatted-output&#34;&gt;Formatted Output&lt;/h3&gt;
&lt;p&gt;We can work with more detailed output using formatted strings, as demonstrated in the following example.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3.14159265359&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The value of pi is approximately &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;pi&lt;span style=&#34;color:#e6db74&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;.3f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;The value of pi is approximately 3.141.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;More details about the different ways of using formatting strings is documented &lt;a href=&#34;https://docs.python.org/3/tutorial/inputoutput.html?highlight=formatted%20print#formatted-string-literals&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;commenting-code&#34;&gt;Commenting Code&lt;/h2&gt;
&lt;p&gt;The last topic of this introduction is about commenting. Communicating the purpose of your program is not only important when working with others, but you will find it to be extremely helpful as you build larger and larger projects. It is a common trap to dive into an idea with absolute focus, quickly hacking away as your program takes shape. This sort of approach is like a house of cards. As soon as your attention is diverted, it takes time to build that model up in your head again.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-20_21-52-05_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Daily experiences with programming.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Daily experiences with programming.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Writing down your program&amp;rsquo;s purpose and design while documenting its function is paramount for a product that is both robust and maintainable. The simplest way to communicate ideas is to leave &lt;strong&gt;comments&lt;/strong&gt; in the code itself. There are two ways to leave basic comments in Python: single-line and multi-line. The code below demonstrates both.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;This small code example shows how to comment in Python.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;By the way, this is a multi-line comment.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;user1&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# stores the user&amp;#39;s name&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As see above, multi-line comments are wrapped in &lt;code&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;. These are typically reserved for things like function documentation (more on that later). Single-line comments start with &lt;code&gt;#&lt;/code&gt; and can be placed on the same line as a statement.&lt;/p&gt;
&lt;p&gt;There is a third type of commenting called &lt;strong&gt;self-commenting&lt;/strong&gt;. The same example above will motivate this type of commenting. There is nothing invalid about the statement &lt;code&gt;a = &amp;quot;user1&amp;quot;&lt;/code&gt;. It defines a variable named &lt;code&gt;a&lt;/code&gt; whose value is the string &lt;code&gt;&amp;quot;user1&amp;quot;&lt;/code&gt;. However, if there wasn&amp;rsquo;t a comment on the same line describing its purpose, it might not be so clear. There is an easier way to communicate this without commenting at all. We could instead write something like &lt;code&gt;username = &amp;quot;user1&amp;quot;&lt;/code&gt;. The variable name itself resolves any ambiguity about its purpose and obviates the need for an additional comment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Markov Decision Processes</title>
      <link>https://ajdillhoff.github.io/notes/markov_decision_processes/</link>
      <pubDate>Mon, 24 Jul 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/markov_decision_processes/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#key-terms&#34;&gt;Key Terms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#defining-goals&#34;&gt;Defining Goals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#policies-and-values&#34;&gt;Policies and Values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bellman-equations&#34;&gt;Bellman Equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimality&#34;&gt;Optimality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimizing-the-policy&#34;&gt;Optimizing the Policy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;key-terms&#34;&gt;Key Terms&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Agent&lt;/strong&gt;: The learner or decision maker.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt;: The world that the agent can interact with.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;State&lt;/strong&gt;: A representation of the agent and environment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: The agent can take an action in the environment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reward&lt;/strong&gt;: Given to the agent based on actions taken.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Maximize rewards earned over time.&lt;/p&gt;
&lt;p&gt;At time \(t\), the agent observes the state of the environment \(S_t \in \mathcal{S}\) and can select an action \(A_t \in \mathcal{A}(s)\), where \(\mathcal{A}(s)\) suggests that the available actions are dependent on the current state.
At time \(t + 1\), the agent receives a reward \(R_{t+1} \in \mathcal{R}\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-11-15_19-01-30_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;The agent-environment in a Markov decision process (Credit: Sutton &amp;amp;amp; Barto).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;The agent-environment in a Markov decision process (Credit: Sutton &amp;amp; Barto).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To improve its knowledge about an environment or increase its performance on a task, an agent must first be able to interpret or make sense of that environment in some way.
Second, there must be a well defined goal. For an agent playing Super Mario, for example, the goal would be to complete each level while maximizing the score.
Third, the agent must be able to interact with its environment by taking actions.
If the Mario-playing agent could not move Mario around, it would never be able to improve.
If the agent makes a decision which leads to Mario&amp;rsquo;s untimely demise, it would update its knowledge of the world so that it would tend towards a more favorable action.
These three requirements: sensations, actions, and goals, are encapsulated by &lt;strong&gt;Markov Decision Processes&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;A Markov decision process is defined by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(\mathcal{S}\) - a set of states,&lt;/li&gt;
&lt;li&gt;\(\mathcal{A}\) - a set of actions,&lt;/li&gt;
&lt;li&gt;\(\mathcal{R}\) - a set of rewards,&lt;/li&gt;
&lt;li&gt;\(P\) - the transition probability function to determine transition between states,&lt;/li&gt;
&lt;li&gt;\(\gamma\) - discount factor for future rewards.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At time \(t\), an agent in state \(S_t\) selects an action \(A_t\).
At \(t+1\), it receives a reward \(R_{t+1}\) based on that action.&lt;/p&gt;
&lt;p&gt;In a finite MDP, the states, actions, and rewards have a finite number of elements.
Random variables \(R_t\) and \(S_t\) have discrete probability distributions dependent on the preceding state and action.&lt;/p&gt;
&lt;p&gt;\[
p(s&amp;rsquo;, r|s, a) = P\{S_t = s&amp;rsquo;, R_t = r|S_{t-1} = s, A_{t-1} = a\}
\]&lt;/p&gt;
&lt;p&gt;If we want the state transition probabilities, we can sum over the above distribution:&lt;/p&gt;
&lt;p&gt;\[
p(s&amp;rsquo;|s, a) = P\{S_t = s&amp;rsquo;|S_{t-1} = s, A_{t-1}=a\} = \sum_{r\in\mathcal{R}}p(s&amp;rsquo;, r|s, a).
\]&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;reward function&lt;/strong&gt; \(r\) gives the expected &lt;em&gt;next&lt;/em&gt; reward given some state and action:&lt;/p&gt;
&lt;p&gt;\[
r(s, a) = \mathbb{E}[R_{t+1}|S_t=s, A_t=a] = \sum_{r}r \sum_{s&amp;rsquo;}p(s&amp;rsquo;, r|s, a).
\]&lt;/p&gt;
&lt;h2 id=&#34;defining-goals&#34;&gt;Defining Goals&lt;/h2&gt;
&lt;p&gt;In reinforcement learning, the goal is encoded in the form of a &lt;strong&gt;&lt;strong&gt;reward signal&lt;/strong&gt;&lt;/strong&gt;. The agent sets out to &lt;em&gt;maximize&lt;/em&gt; the total amount of reward it receives over an &lt;strong&gt;&lt;strong&gt;episode&lt;/strong&gt;&lt;/strong&gt;. An &lt;strong&gt;&lt;strong&gt;episode&lt;/strong&gt;&lt;/strong&gt; is defined dependent on the problem context and ends in a &lt;strong&gt;&lt;strong&gt;terminal state&lt;/strong&gt;&lt;/strong&gt;. It could be a round of game, a single play, or the result of moving a robot. Typically, the rewards come as a single scalar value at teach time step. This implies that an agent might take an action that results in a negative reward if it is optimal in the long run.&lt;/p&gt;
&lt;p&gt;Formally, the &lt;strong&gt;expected return&lt;/strong&gt; includes a &lt;strong&gt;discount factor&lt;/strong&gt; that allows us to control the trade-off between short-term and long-term rewards:&lt;/p&gt;
&lt;p&gt;\[
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1},
\]&lt;/p&gt;
&lt;p&gt;where \(0 \leq \gamma \leq 1\). This can be written in terms of the expected return itself as well:&lt;/p&gt;
&lt;p&gt;\[
G_t = R_{t+1} + \gamma G_{t+1}.
\]&lt;/p&gt;
&lt;h2 id=&#34;policies-and-values&#34;&gt;Policies and Values&lt;/h2&gt;
&lt;p&gt;Two important concepts that help our agent make decisions are the policy and value functions. A &lt;strong&gt;policy&lt;/strong&gt;, typically denoted by \(\pi\), maps states to actions. Such a function can be deterministic, \(\pi(s) = a\), or stochastic, \(\pi(a|s)\).&lt;/p&gt;
&lt;p&gt;The value of a particular state under a policy \(\pi\) is defined as&lt;/p&gt;
&lt;p&gt;\[
v_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s] = \mathbb{E}_{\pi}\Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}\Bigg|S_t=s\Bigg].
\]&lt;/p&gt;
&lt;p&gt;We also must define the value of taking an action \(a\) in state \(s\) following policy \(\pi\):&lt;/p&gt;
&lt;p&gt;\[
q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t|S_t=s, A_t=a] = \mathbb{E}_{\pi}\Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}\Bigg|S_t=s, A_t=a\Bigg].
\]&lt;/p&gt;
&lt;p&gt;This function defines the expected return of following a particular policy and starting in state \(s\).
Both the &lt;strong&gt;state-value&lt;/strong&gt; and &lt;strong&gt;action-value&lt;/strong&gt; functions can be updated as a result of the agent&amp;rsquo;s experience. How it is updated is method-dependent. Certain methods will also dictate how the policy itself can be updated.&lt;/p&gt;
&lt;h2 id=&#34;bellman-equations&#34;&gt;Bellman Equations&lt;/h2&gt;
&lt;p&gt;The recursive relationship between the value of a state and its future states can be represented using &lt;strong&gt;&lt;strong&gt;Bellman equations&lt;/strong&gt;&lt;/strong&gt;. In RL, we are interested in the equations for both the state-value and action-value. Given the diagram of an MDP, we can see that they are related to each other. To make the following equations easier to understand, it is important to remember the flow of a Markov decision process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;take an action,&lt;/li&gt;
&lt;li&gt;arrive at a state and sense the reward,&lt;/li&gt;
&lt;li&gt;consult the policy for the next action.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With that in mind, let&amp;rsquo;s look at the state-value function first. This function considers the expected value of starting in a state \(s\) and following policy \(\pi\). In other words, we must consider all possible actions and their future rewards.&lt;/p&gt;
&lt;p&gt;\begin{align*}
v_{\pi}(s) &amp;amp;= \mathbb{E}_{\pi}[G_t | S_t = s]\\
&amp;amp;= \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]\\
&amp;amp;= \sum_{a} \pi(a|s) \sum_{s&amp;rsquo;}\sum_{r}p(s&amp;rsquo;, r|s, a)\Big[r + \gamma \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]\Big]\\
&amp;amp;= \sum_{a} \pi(a|s) \sum_{s&amp;rsquo;, r}p(s&amp;rsquo;, r|s, a)\Big[r + \gamma v_{\pi}(s&amp;rsquo;)\Big]\\
&amp;amp;= \sum_{a} \pi(a | s)\big[r(s, a) + \gamma \sum_{s&amp;rsquo;}p(s&amp;rsquo;|s, a)v_{\pi}(s&amp;rsquo;)\big]\\
\end{align*}&lt;/p&gt;
&lt;p&gt;The first sum over actions considers &lt;em&gt;all possible actions&lt;/em&gt;. This is followed by a transition to possible states \(s&amp;rsquo;\) conditioned on taking each action multiplied by the expected value of being at the new state.&lt;/p&gt;
&lt;p&gt;The action-value function follows a similar derivation:&lt;/p&gt;
&lt;p&gt;\begin{align*}
q_{\pi}(s, a) &amp;amp;= \mathbb{E}_{\pi}[G_t|S_t = s, A_t = a]\\
&amp;amp;= \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a]\\
&amp;amp;= r(s, a) + \sum_{s&amp;rsquo;}p(s&amp;rsquo;|s, a) v_{\pi}(s&amp;rsquo;)
\end{align*}&lt;/p&gt;
&lt;p&gt;There is a very similar looking set of terms in the state-value function above, and we should expect that! If we want to evaluate the current state, we need to look ahead at the possible actions and their resulting rewards. Similarly, evaluating the current action requires us to look head at the value of future states.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s expand \(q_{\pi}(s,a)\) once more so that it is written in terms of itself.&lt;/p&gt;
&lt;p&gt;\begin{align*}
q_{\pi}(s, a) &amp;amp;= r(s, a) + \gamma \sum_{s&amp;rsquo;}p(s&amp;rsquo;|s, a) v_{\pi}(s&amp;rsquo;)\\
&amp;amp;= r(s, a) + \gamma \sum_{s&amp;rsquo;}p(s&amp;rsquo;|s, a) \sum_{a&amp;rsquo;} \pi(s&amp;rsquo;, a&amp;rsquo;) \big[r(s&amp;rsquo;, a&amp;rsquo;) + \gamma \sum_{s&amp;rsquo;&amp;rsquo;} p(s&amp;rsquo;&amp;rsquo;|s&amp;rsquo;, a&amp;rsquo;)v_{\pi}(s&amp;rsquo;&amp;rsquo;)]\\
&amp;amp;= r(s, a) + \gamma \sum_{s&amp;rsquo;}p(s&amp;rsquo;|s, a) \sum_{a&amp;rsquo;} \pi(s&amp;rsquo;, a&amp;rsquo;) q_{\pi}(s&amp;rsquo;, a&amp;rsquo;)
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;optimality&#34;&gt;Optimality&lt;/h2&gt;
&lt;p&gt;To solve a reinforcement learning problem, we are interested in finding the policy \(\pi_{*}\) whose expected return is greater than all other possible policies over all states.
An &lt;strong&gt;&lt;strong&gt;optimal policy&lt;/strong&gt;&lt;/strong&gt; will use an &lt;strong&gt;*optimal state-value function&lt;/strong&gt; and &lt;strong&gt;&lt;strong&gt;optimal action-value function&lt;/strong&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;\begin{align*}
v_{*}(s) &amp;amp;= \max_{\pi}v_{\pi}(s)\\
q_{*}(s, a) &amp;amp;= \max_{\pi}q_{\pi}(s, a).
\end{align*}&lt;/p&gt;
&lt;p&gt;The optimal state-value function would select the best possible action instead of summing over all possibley actions starting in state \(s\):&lt;/p&gt;
&lt;p&gt;\begin{align*}
v_{*}(s) &amp;amp;= \max_{a} q_{\pi_*}(s, a)\\
&amp;amp;= \max_{a}\big[r(s, a) + \gamma \sum_{s&amp;rsquo;} p(s&amp;rsquo;|s, a) v_{*}(s&amp;rsquo;)\big]
\end{align*}&lt;/p&gt;
&lt;p&gt;Similarly, the optimal action-value function selects the best possible action from the next state \(s&amp;rsquo;\):&lt;/p&gt;
&lt;p&gt;\[
q_{*}(s) = r(s, a) + \gamma \sum_{s&amp;rsquo;} p(s&amp;rsquo;|s, a) \max_{a} q_{*}(s&amp;rsquo;, a&amp;rsquo;).
\]&lt;/p&gt;
&lt;h2 id=&#34;optimizing-the-policy&#34;&gt;Optimizing the Policy&lt;/h2&gt;
&lt;p&gt;For smaller problems with reasonably small state and action spaces, we can use Dynamic Programming to compute the optimal policy. These methods quickly become intractable as the complexity of our problem increases. As is common in machine learning, we would resort to approximation methods for complex spaces.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;In fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ndash; Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Imagine if you had a set policy that dictated the actions you would take from work to home. In this example, assume the policy is not an optimal policy. One day, you decide to take a left at a particular intersection rather than going forward. After that, you follow your policy as described. If this decision ultimately resulted in you arriving home sooner, you would probably update your policy to always take that left. This intuition describes a result of the &lt;strong&gt;policy improvement theorem&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let \(\pi\) and \(\pi&amp;rsquo;\) be two deterministic policies where&lt;/p&gt;
&lt;p&gt;\[
q_{\pi}(s, \pi&amp;rsquo;(s)) \geq v_{\pi}(s),\quad \forall s.
\]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gradient Boosting</title>
      <link>https://ajdillhoff.github.io/notes/gradient_boosting/</link>
      <pubDate>Mon, 17 Jul 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/gradient_boosting/</guid>
      <description>&lt;h2 id=&#34;notes-from&#34;&gt;Notes from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Friedman 2001&lt;/a&gt;)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Many machine learning methods are parameterized functions that are optimized using some numerical optimization techniques, notably steepest-descent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Initial learner is a stump, subsequent learners are trees with depth as some power of 2 (commonly).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Numerical optimization in function space&lt;/strong&gt;
\[
g_m(\mathbf{x}) = E_y\Big[\frac{\partial L(y, F(\mathbf{x}))}{\partial F(\mathbf{x})}|\mathbf{x}\Big]_{F(\mathbf{x})=F_{m-1}(\mathbf{x})}
\]
The optimal step size found by solving&lt;/p&gt;
&lt;p&gt;\[
\rho_m = \mathop{\arg \min}_{\rho} E_{y,\mathbf{x}}L(y,F_{m-1}(\mathbf{x})-\rho g_m(\mathbf{x}))
\]
Then the function \(m\) is updated:
\[
f_m(\mathbf{x}) = -\rho_m g_m(\mathbf{x})
\]&lt;/p&gt;
&lt;p&gt;Walking through it&amp;hellip;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Make an initial guess with \(f_0(\mathbf{x})\)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Evaluate \(L(y, f_0(\mathbf{x}))\)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Improve model by boosting \(f_1(\mathbf{x}) = -\rho_1 g_1(\mathbf{x})\), where \[ g_1(\mathbf{x}) = \frac{\partial L(y, f_0(\mathbf{x}))}{\partial f_0(\mathbf{x})}. \]
This implies that \(f_1\) is predicting the gradient of the previous function.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the model is nonparametric, the expected value of the function conditioned on the input cannot be estimated accurately because we cannot sample the entire distribution of \(\mathbf{x}\). The author&amp;rsquo;s note that &amp;ldquo;&amp;hellip;even if it could, one would like to estimate \(F^*(\mathbf{x})\) at \(\mathbf{x}\) values other than the training sample points.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Smoothness is imposed by approximating the function with a parametric model. I think this means that the distribution is approximated as well.&lt;/p&gt;
&lt;p&gt;\begin{equation}
(\beta_m, \mathbf{a}_m) = \mathop{\arg \min}_{\beta, \mathbf{a}}\sum_{i=1}^N L(y_i, F_{m-1}(\mathbf{x}_i) + \beta h(\mathbf{x}_i; \mathbf{a}))
\end{equation}&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What if a solution to the above equation is difficult to obtain? Instead, view \(\beta_m h(\mathbf{x};\mathbf{a}_m)\) as the best greedy step toward \(F^*(\mathbf{x})\), under the constraint that the step direction, in this case \(h(\mathbf{x};\mathbf{a}_m)\), is a member of the class of functions \(h(\mathbf{x};\mathbf{a})\). The negative gradient can be evaluated at each data point:
\[
-g_m(\mathbf{x}_i) = -\frac{\partial L(y_i, F_{m-1}(\mathbf{x}_i))}{\partial F_{m-1}(\mathbf{x}_i)}.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This gradient is evaluated at every data point. However, we cannot generalize to new values not in our dataset. The proposed solution comes via \(\mathbf{h}_m = \{h(\mathbf{x}_i;\mathbf{a}_m)\}_{1}^N\) &amp;ldquo;most parallel to&amp;rdquo; \(-\mathbf{g}_m \in \mathbb{R}^N\).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As long as we can compute a derivative for the original loss function, our subsequent boosting problems are solved via least-squared error:
\[
\mathbf{a}_m = \mathop{\arg \min}_{\mathbf{a}, \beta} \sum_{i=1}^N \Big(-g_m(\mathbf{x}_i)-\beta h(\mathbf{x}_i;\mathbf{a})\Big)^2
\]&lt;/p&gt;

        
        
        
        
        
        &lt;figure&gt;
        
        &lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-07-18_19-43-31_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Original generic algorithm from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Friedman 2001&amp;lt;/a&amp;gt;).&#34; &gt;
        
        
        
        &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
          
          &lt;p&gt;
            &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Original generic algorithm from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Friedman 2001&lt;/a&gt;).
            
            
            
          &lt;/p&gt; 
        &lt;/figcaption&gt;
        
        &lt;/figure&gt;

&lt;p&gt;Check out a basic implementation in Python &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/blob/main/boosting/intro_to_gradient_boosting.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Friedman, Jerome H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” &lt;i&gt;The Annals of Statistics&lt;/i&gt; 29 (5): 1189–1232. &lt;a href=&#34;https://www.jstor.org/stable/2699986&#34;&gt;https://www.jstor.org/stable/2699986&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bias and Variance</title>
      <link>https://ajdillhoff.github.io/notes/bias_and_variance/</link>
      <pubDate>Tue, 04 Jul 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/bias_and_variance/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#generalization&#34;&gt;Generalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bias&#34;&gt;Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variance&#34;&gt;Variance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bias-variance-tradeoff&#34;&gt;Bias-Variance Tradeoff&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;generalization&#34;&gt;Generalization&lt;/h2&gt;
&lt;p&gt;When fitting machine learning models to data, we want them to &lt;strong&gt;generalize&lt;/strong&gt; well to the distribution that we have sampled from. We can measure a model&amp;rsquo;s ability to generalize by evaluating it on previously unseen data that is sampled from the same distribution as the training set. However, we often do not know the true underlying distribution. So we must fit the models to empirical distributions derived from observed data.&lt;/p&gt;
&lt;p&gt;Measuring bias and variance is crucial for determining the quality of a model. &lt;strong&gt;Bias&lt;/strong&gt; refers to the difference between the average prediction of a model and the correct value we are trying to predict. A model with high bias oversimplifies the problem and leads to high error on both training and test data. &lt;strong&gt;Variance&lt;/strong&gt; refers to the sensitivity of a model to fluctuations in the training set. High variance suggests that the model&amp;rsquo;s performance changes significantly when it is fit on different samplings of the training data, which can lead to overfitting.&lt;/p&gt;
&lt;p&gt;To achieve good generalization, it is essential to find a balance between bias and variance, minimizing the total error. This can be done by selecting appropriate model complexity and using regularization techniques to prevent overfitting or underfitting. Additionally, model validation techniques, such as hold-out validation and cross-validation, can be employed to assess a model&amp;rsquo;s ability to generalize to unseen data.&lt;/p&gt;
&lt;h2 id=&#34;bias&#34;&gt;Bias&lt;/h2&gt;
&lt;p&gt;Consider fitting a simple linear model to nonlinear data. The model will not be able to generalize well, regardless of the size of the training set. In fact, it would also exhibit poor performance when evaluated on the training set as well. When a model has not learned the patterns in the training data and is likewise unable to generalize to new data, it is known as &lt;strong&gt;underfitting&lt;/strong&gt;. In this case, such a model has &lt;strong&gt;high bias&lt;/strong&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-07-04_22-51-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Regardless of the dataset sampled, a linear model exhibits high bias.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Regardless of the dataset sampled, a linear model exhibits high bias.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;variance&#34;&gt;Variance&lt;/h2&gt;
&lt;p&gt;Variance is described in terms of the model fitting procedure and the training data. In terms of data, variance measures dispersion. It could also be interpreted as a measure of diversity. Sets with low variance contain samples that are close to the mean, and sampling from such a set would produce rather consistent data points.&lt;/p&gt;
&lt;p&gt;In terms of model fitting, a model that fits the training data well but not the test data describes &lt;strong&gt;overfitting&lt;/strong&gt;. This is because the training data is only an empirical sample of the true underlying distribution. A different sampling of the distribution may yield a set that more closely resembles the test set. Due to the &lt;strong&gt;variance&lt;/strong&gt; of the underlying distribution, our model overfits the patterns that exist in the training set.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-07-04_17-54-36_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A 5th degree polynomial trained on 3 different samplings of the distribution.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A 5th degree polynomial trained on 3 different samplings of the distribution.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;bias-variance-tradeoff&#34;&gt;Bias-Variance Tradeoff&lt;/h2&gt;
&lt;p&gt;If a model is not complex enough to capture the underlying distribution, it will perform poorly on both the training and test sets. Indeed, the model has low bias. If the model is too complex, it will exhibit low bias and high variance, overfitting the training set while failing to generalize well to unseen data. The solution then is to find a tradeoff between bias and variance with respect to the model complexity.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Transformers</title>
      <link>https://ajdillhoff.github.io/notes/transformers/</link>
      <pubDate>Sun, 06 Nov 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/transformers/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#attention&#34;&gt;Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#key-value-store&#34;&gt;Key-value Store&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scaled-dot-product-attention&#34;&gt;Scaled Dot Product Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multi-head-attention&#34;&gt;Multi-Head Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#encoder-decoder-architecture&#34;&gt;Encoder-Decoder Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#encoder&#34;&gt;Encoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#decoder&#34;&gt;Decoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The story of Transformers begins with &amp;ldquo;Attention Is All You Need&amp;rdquo; (Vaswani et al., n.d.). In this seminal work, the authors describe the current landscape of sequential models, their shortcomings, and the novel ideas that result in their successful application.&lt;/p&gt;
&lt;p&gt;Their first point highlights a fundamental flaw in how &lt;a href=&#34;https://ajdillhoff.github.io/notes/recurrent_neural_networks/&#34;&gt;Recurrent Neural Networks&lt;/a&gt; process sequential data: their output is a function of the previous time step. Given the hindsight of 2022, where large language models are crossing the &lt;a href=&#34;https://arxiv.org/pdf/2101.03961.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;trillion parameter milestone&lt;/a&gt;, a model requiring recurrent computation dependent on previous time steps without the possibility of parallelization would be virtually intractable.&lt;/p&gt;
&lt;p&gt;The second observation refers to attention mechanisms, a useful addition to sequential models that enable long-range dependencies focused on specific contextual information. When added to translation models, attention allows the model to focus on particular words (Bahdanau, Cho, and Bengio 2016).&lt;/p&gt;
&lt;p&gt;The Transformer architecture considers the entire sequence using only attention mechanisms.
There are no recurrence computations in the model, allowing for higher efficiency through parallelization.&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;The original architecture consists of an encoder and decoder, each containing one or more attention mechanisms.
Not every type of model uses both encoders and decoders. This is discussed later [TODO: discuss model types].
Before diving into the architecture itself, it is important to understand what an attention mechanism is and how it functions.&lt;/p&gt;
&lt;h2 id=&#34;attention&#34;&gt;Attention&lt;/h2&gt;
&lt;p&gt;Attention mechanisms produce relationships between sequences. When we look at an image of a dog running in a field with the intent of figuring out what the dog is doing in the picture, we pay greater attention to the dog and look at contextual cues in the image that might inform us of their task. This is an automatic process which allows us to efficiently process information.&lt;/p&gt;
&lt;p&gt;Attention mechanisms follow the same concept. Consider a machine translation task in which a sentence in English is translated to French. Certain words between the input and output will have stronger correlations than others.&lt;/p&gt;
&lt;h3 id=&#34;soft-attention&#34;&gt;Soft Attention&lt;/h3&gt;
&lt;p&gt;Use of context vector that is dependent on a sequence of annotations. These contain information about the input sequence with a focus on the parts surrounding the $i$-th word.&lt;/p&gt;
&lt;p&gt;\[
c_i = \sum_{j=1}^{T_x}\alpha_{ij}h_j
\]&lt;/p&gt;
&lt;p&gt;What is \(\alpha_{ij}\) and how is it computed? This comes from an alignment model which assigns a score reflecting how well the inputs around position \(j\) and output at position \(i\) match, given by&lt;/p&gt;
&lt;p&gt;\[
e_{ij} = a(s_{i-1}, h_j),
\]&lt;/p&gt;
&lt;p&gt;where \(a\) is a feed-forward neural network and \(h_j\) is an annotation produced by the hidden layer of a BRNN.
These scores are passed to the softmax function so that \(\alpha_{ij}\) represents the weight of annotation \(h_j\):&lt;/p&gt;
&lt;p&gt;\[
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp (e_{ik})}.
\]&lt;/p&gt;
&lt;p&gt;This weight reflects how important \(h_j\) is at deciding the next state \(s_i\) and generating \(y_i\).&lt;/p&gt;
&lt;h3 id=&#34;soft-vs-dot-hard-attention&#34;&gt;Soft vs. Hard Attention&lt;/h3&gt;
&lt;p&gt;This mechanism was also described in the context of visual attention as &amp;ldquo;soft&amp;rdquo; attention (Xu et al. 2016).
The authors also describe an alternative version they call &amp;ldquo;hard&amp;rdquo; attention.
Instead of providing a probability of where the model should look, hard attention provides a single location that is sampled from a multinoulli distribution parameterized by \(\alpha_i\).&lt;/p&gt;
&lt;p&gt;\[
p(s_{t,i} = 1 | s_{j&amp;lt;t}, \mathbf{a}) = \alpha_{t,i}
\]&lt;/p&gt;
&lt;p&gt;Here, \(s_{t,i}\) represents the location \(i\) at time \(t\), \(s_{j&amp;lt;t}\) are the location variables prior to \(t\), and \(\mathbf{a}\) is an image feature vector.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-11-10_12-07-42_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Hard attention for &amp;#34;A man and a woman playing frisbee in a field.&amp;#34; (Xu et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Hard attention for &amp;ldquo;A man and a woman playing frisbee in a field.&amp;rdquo; (Xu et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-11-10_12-08-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Soft attention for &amp;#34;A woman is throwing a frisbee in a park.&amp;#34; (Xu et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Soft attention for &amp;ldquo;A woman is throwing a frisbee in a park.&amp;rdquo; (Xu et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The two figures above show the difference between soft and hard attention.
Hard attention, while faster at inference time, is non-differentiable and requires more complex methods to train (TODO: cite Luong).&lt;/p&gt;
&lt;h3 id=&#34;self-attention&#34;&gt;Self-Attention&lt;/h3&gt;
&lt;p&gt;Self attention is particularly useful for determining the relationship between different parts of an input sequence. The figure below demonstrates self-attention given an input sentence (Cheng, Dong, and Lapata 2016).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-11-10_13-11-31_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Line thickness indicates stronger self-attention (Cheng et al.).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Line thickness indicates stronger self-attention (Cheng et al.).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;How aligned the two vectors are.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cross-attention&#34;&gt;Cross Attention&lt;/h3&gt;
&lt;p&gt;TODO&lt;/p&gt;
&lt;h2 id=&#34;key-value-store&#34;&gt;Key-value Store&lt;/h2&gt;
&lt;p&gt;Query, key, and value come from the same input (self-attention).&lt;/p&gt;
&lt;p&gt;Check query against all possible keys in the dictionary. They have the same size.
The value is the result stored there, not necessarily the same size.
Each item in the sequence will generate a query, key, and value.&lt;/p&gt;
&lt;p&gt;The attention vector is a function of they keys and the query.&lt;/p&gt;
&lt;p&gt;Hidden representation is a function of the values and the attention vector.&lt;/p&gt;
&lt;p&gt;The Transformer paper talks about queries, keys, and values. This idea comes from retrieval systems.
If you are searching for something (a video, book, song, etc.), you present a system your query. That system will compare your query against the keys in its database. If there is a key that matches your query, the value is returned.&lt;/p&gt;
&lt;p&gt;\[
att(q, \mathbf{k}, \mathbf{v}) = \sum_i v_i f(q, k_i),
\]
where \(f\) is a similarity function.&lt;/p&gt;
&lt;p&gt;This is an interesting and convenient representation of attention.
To implement this idea, we need some measure of similarity.
Why not orthogonality? Two vectors that are orthogonal produce a scalar value of 0.
The maximum value two vectors will produce as a result of the dot product occurs when the two vectors have the exact same direction.
This is convenient because the dot product is simple and efficient and we are already performing these calculations in our deep networks in the form of matrix multiplication.&lt;/p&gt;
&lt;h2 id=&#34;scaled-dot-product-attention&#34;&gt;Scaled Dot Product Attention&lt;/h2&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-11-21_18-39-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Scaled dot-product attention ((Vaswani et al., n.d.))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Scaled dot-product attention ((Vaswani et al., n.d.))
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Each &lt;strong&gt;query&lt;/strong&gt; vector is multiplied with each &lt;strong&gt;key&lt;/strong&gt; using the dot product.
This is implemented more efficiently via matrix multiplication.
A few other things are added here to control the output.
The first is &lt;strong&gt;scaling&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;multi-head-attention&#34;&gt;Multi-Head Attention&lt;/h2&gt;
&lt;p&gt;A single attention head can transform the input into a single representation. Is this analagous to using a single convolutional filter? The benefit of having multiple filters is to create multiple possible representations from the same input.&lt;/p&gt;
&lt;h2 id=&#34;encoder-decoder-architecture&#34;&gt;Encoder-Decoder Architecture&lt;/h2&gt;
&lt;p&gt;The original architecture of a transformer was defined in the context of sequence transduction tasks, where both the input and output are sequences. The most common task of this type is machine translation.&lt;/p&gt;
&lt;h2 id=&#34;encoder&#34;&gt;Encoder&lt;/h2&gt;
&lt;p&gt;The encoder layer takes an input sequence \(\{\mathbf{x}_t\}_{t=0}^T\) and transforms it into another sequence \(\{\mathbf{z}_t\}_{t=0}^T\).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;What is \(\mathbf{z}_t\)?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How is it used?
Input as key and value into second multi-head attention layer of the &lt;strong&gt;decoder&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Could you create an encoder only model?
Yes. Suitable for classification tasks &amp;ndash; classify the representation produced by the encoder.
&lt;strong&gt;How does this representation relate to understanding?&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It&amp;rsquo;s a transformation to another representation.&lt;/p&gt;
&lt;p&gt;Generated representation also considers the context of other parts of the same sequence (bi-directional).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;decoder&#34;&gt;Decoder&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Generates an output sequence.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Decoder-only models?
Suitable for text generation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What does the input represent?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What does the output represent?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What if we don&amp;rsquo;t use an encoder, what information is added in lieu of the encoder output?&lt;/p&gt;
&lt;!-- This HTML table template is generated by emacs/table.el --&gt;
&lt;table border=&#34;1&#34;&gt;
  &lt;tr&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Model&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Examples&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Tasks&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Encoder&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;ALBERT,&amp;nbsp;BERT,&amp;nbsp;DistilBERT,&lt;br /&gt;
      ELECTRA,&amp;nbsp;RoBERTa&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Sentence&amp;nbsp;classification,&amp;nbsp;named&amp;nbsp;entity&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      recognition,&amp;nbsp;extractive&amp;nbsp;question&amp;nbsp;answering&amp;nbsp;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Decoder&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;CTRL,&amp;nbsp;GPT,&amp;nbsp;GPT-2,&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      Transformer&amp;nbsp;XL&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Text&amp;nbsp;generation&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Encoder-decoder&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;BART,&amp;nbsp;T5,&amp;nbsp;Marian,&amp;nbsp;mBART&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Summarization,&amp;nbsp;translation,&amp;nbsp;generative&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      question&amp;nbsp;answering&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;p&gt;TODO&lt;/p&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/labmlai/status/1543159412940242945?s=20&amp;amp;t=EDu5FzDWl92EqnJlWvfAxA&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://twitter.com/labmlai/status/1543159412940242945?s=20&amp;t=EDu5FzDWl92EqnJlWvfAxA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Transduction_%28machine_learning%29&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://en.wikipedia.org/wiki/Transduction_(machine_learning)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.apronus.com/math/transformer-language-model-definition&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.apronus.com/math/transformer-language-model-definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2018-06-24-attention/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://lilianweng.github.io/posts/2018-06-24-attention/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://nlp.seas.harvard.edu/annotated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;http://nlp.seas.harvard.edu/annotated-transformer/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Submitting Assignments using GitHub</title>
      <link>https://ajdillhoff.github.io/notes/submitting_assignments_using_github/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/submitting_assignments_using_github/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cloning-a-repository&#34;&gt;Cloning a Repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-a-new-file&#34;&gt;Adding a new file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#committing-changes&#34;&gt;Committing Changes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pushing-your-local-changes-to-the-remote-repository&#34;&gt;Pushing your local changes to the remote repository&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This article walks through the steps needed to complete Assignment 0.
For this course, we only need to use 5 commands.
Although it is not required for this course, it is highly recommended that you learn the basics of &lt;code&gt;git&lt;/code&gt;.
The &lt;a href=&#34;https://git-scm.com/doc&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;documentation page&lt;/a&gt; provided by &lt;code&gt;git-scm&lt;/code&gt; is extremely helpful.
It includes links to a free book on &lt;code&gt;git&lt;/code&gt; as well as a cheat sheet with the most common &lt;code&gt;git&lt;/code&gt; commands.&lt;/p&gt;
&lt;h2 id=&#34;cloning-a-repository&#34;&gt;Cloning a Repository&lt;/h2&gt;
&lt;p&gt;After accepting the assignment, you are provided with a link to your own private repository.
To work with it on your local machine, you will first need to clone it.
To clone it, you will need to authenticate that you are allowed to work with that repository.
This is done by either SSH or HTTPS.&lt;/p&gt;
&lt;p&gt;If you want to add and use an SSH key to authenticate, follow the instructions listed &lt;a href=&#34;https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.
This article will walk through authentication via HTTPS.
To clone your repository, you will need the HTTPS link.
This is the same as the link to your actual repository.
You can also view it by clicking on the green &lt;code&gt;Code&lt;/code&gt; button and selecting &lt;code&gt;HTTPS&lt;/code&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-09-03_14-28-43_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Viewing the repository link.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Viewing the repository link.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In a terminal window, run the command &lt;code&gt;git clone [link]&lt;/code&gt;, where &lt;code&gt;[link]&lt;/code&gt; is the URL you copied for your respository.
You should be prompted to enter your GitHub username and password.
If you use your regular account password, you will see something similar to the output below&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-09-03_14-43-41_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Attempting to authenticate using your GitHub password.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Attempting to authenticate using your GitHub password.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;As stated in the error message&lt;/strong&gt;, GitHub does not support using your password to authenticate via HTTPS.
&lt;strong&gt;The link that is provided in the error message&lt;/strong&gt; directs your to an article explaining this decision.
The article links to another article on how to set up a &lt;strong&gt;personal access token&lt;/strong&gt; (&lt;a href=&#34;https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;direct link&lt;/a&gt;).
Create a personal access token as described in the direct link.
&lt;strong&gt;This will be used in place of your regular password, so make sure you keep it somewhere safe.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When creating the access token, be sure to at least select the &lt;code&gt;repo&lt;/code&gt; scope.
This will ensure that your access token is authorized to clone your repository.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-09-03_14-49-53_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Selecting scopes.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Selecting scopes.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;With the generated access token, you can successfully clone your repository via HTTPS.
&lt;strong&gt;As a reminder, entering text into a password prompt in terminal will not show the characters you are typing.&lt;/strong&gt;
Do not worry! It is still reading the input.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-09-03_14-51-43_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Successfully cloning the repository.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Successfully cloning the repository.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;adding-a-new-file&#34;&gt;Adding a new file&lt;/h2&gt;
&lt;p&gt;Now that the repository is cloned, we can begin working with it locally.
The assignment requires us to create a program which will print the lines of CSV to the terminal window.
This requires opening &lt;code&gt;data.csv&lt;/code&gt;, looping through its contents, and printing each line as our program reads it.&lt;/p&gt;
&lt;p&gt;Start by creating a new file named &lt;code&gt;read_csv.c&lt;/code&gt; using your favorite code editor.
It does not matter which editor you use.
It only matters that the file you create is in the repository folder.
This assignment is really about making sure you can use &lt;code&gt;git&lt;/code&gt; properly, so a solution has been given below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;stdio.h&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#define BUF_SIZE 128
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;() {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;char&lt;/span&gt; buffer[BUF_SIZE] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; { &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; };
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    FILE &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;fp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;fopen&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;data.csv&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;r&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (fp &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; NULL) &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; (&lt;span style=&#34;color:#a6e22e&#34;&gt;fgets&lt;/span&gt;(buffer, BUF_SIZE, fp)) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a6e22e&#34;&gt;printf&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;%s&amp;#34;&lt;/span&gt;, buffer);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once we have finished editing the code, we can add it to our local repository.
To check the current status of changes in our repo, use &lt;code&gt;git status&lt;/code&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-09-03_15-03-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Checking the status.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Checking the status.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;We have created a file named &lt;code&gt;read_csv.c&lt;/code&gt;, but it is not currently tracked by our repository.
To track this file, we need to add it via &lt;code&gt;git add read_csv.c&lt;/code&gt;.
After adding the file, we can see that our local repo&amp;rsquo;s status has changed.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-09-03_15-04-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Status after adding a file.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Status after adding a file.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Now that the file is being tracked, any modifications we make to it will be recorded.
Git keeps snapshots of each state the file is in.
We can always review a history of our file&amp;rsquo;s changes and see how the code has developed over the lifetime of a project.
In that status output above, our repository detects changes to &lt;code&gt;read_csv.c&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;committing-changes&#34;&gt;Committing Changes&lt;/h2&gt;
&lt;p&gt;If we are happy with the changes, we can &lt;code&gt;commit&lt;/code&gt; them to the repository using &lt;code&gt;git commit&lt;/code&gt;.
A commit should be accompanied by a message explaining what was changed.
This will be very useful later on when you need to review what changes were made and why.
Since this code fulfills the requirements of the assignment, let&amp;rsquo;s form our message that way.
We can commit with a message with the following command.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git commit -m &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Completed assignment.&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;If you have other tracked files with changes that needed to be committed, you can use the flag &lt;code&gt;-a&lt;/code&gt; to add them with your commit.&lt;/strong&gt;&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-09-03_15-14-13_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Committing the changes.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Committing the changes.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;pushing-your-local-changes-to-the-remote-repository&#34;&gt;Pushing your local changes to the remote repository&lt;/h2&gt;
&lt;p&gt;When you clone a repository, you get a full copy of that repository including all of its data.
If the server that is hosting your project crashes, you will still have a full copy of the repository on your local machine.
To synchronize your changes with the local repository, you can &lt;code&gt;push&lt;/code&gt; the local files with &lt;code&gt;git push&lt;/code&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-09-03_15-16-04_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Pushing local changes to the remote repo.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Pushing local changes to the remote repo.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;If we view the website for our repository, it shows our changes.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-09-03_15-16-56_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Repository website after pushing local changes.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Repository website after pushing local changes.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;That is all that is needed for submitting your assignments.
If the code is on your remote repository, than it will be considered as your submission.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sequential Minimal Optimization</title>
      <link>https://ajdillhoff.github.io/notes/sequential_minimal_optimization/</link>
      <pubDate>Mon, 04 Jul 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/sequential_minimal_optimization/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#box-constraints&#34;&gt;Box Constraints&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#updating-the-lagrangians&#34;&gt;Updating the Lagrangians&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-algorithm&#34;&gt;The Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementation&#34;&gt;Implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Paper link:&lt;/strong&gt; &lt;a href=&#34;https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Sequential Minimal Optimization (SMO) is an algorithm to solve the SVM Quadratic Programming (QP) problem efficiently. Developed by John Platt at Microsoft Research, SMO deals with the constraints of the SVM objective by breaking it down into a smaller optimization problem at each step.&lt;/p&gt;
&lt;p&gt;The two key components of SMO are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;an analytic method to solving for two Lagrange multipliers at a time&lt;/li&gt;
&lt;li&gt;and a heuristic for choosing which multipliers to optimize.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The original objective is to maximize the margin between the nearest positive and negative examples.
For the linear case, if the output is given as&lt;/p&gt;
&lt;p&gt;\[
u = \mathbf{w}^T \mathbf{x} - b,
\]&lt;/p&gt;
&lt;p&gt;where \(\mathbf{w}\) is the normal vector to the hyperplane separating the classes, then the margin is given as&lt;/p&gt;
&lt;p&gt;\[
m = \frac{1}{\|w\|_2}.
\]&lt;/p&gt;
&lt;p&gt;Maximizing this margin yielded the primal optimization problem&lt;/p&gt;
&lt;p&gt;\begin{align*}
\min_{\mathbf{w},b} \frac{1}{2} \|\mathbf{w}\|^2\\
\textrm{s.t.} \quad &amp;amp; y_i(\mathbf{w}^T \mathbf{x} - b) \geq 1, \forall i\\
\end{align*}&lt;/p&gt;
&lt;p&gt;The dual form of the objective function for a &lt;a href=&#34;https://ajdillhoff.github.io/notes/support_vector_machine/&#34;&gt;Support Vector Machine&lt;/a&gt; is&lt;/p&gt;
&lt;p&gt;\[
\min_{\vec\alpha} \Psi(\vec{\alpha}) = \min_{\vec{\alpha}} \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N y_i y_j K(\mathbf{x}_i, \mathbf{x}_j)\alpha_i\alpha_j - \sum_{i=1}^N \alpha_i
\]&lt;/p&gt;
&lt;p&gt;with inequality constraints&lt;/p&gt;
&lt;p&gt;\[
\alpha_i \geq 0, \forall i,
\]&lt;/p&gt;
&lt;p&gt;and a linear equality constraint&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^N y_i \alpha_i = 0.
\]&lt;/p&gt;
&lt;p&gt;For a linear SVM, the output is dependent on a weight vector \(\mathbf{w}\) and threshold \(b\):&lt;/p&gt;
&lt;p&gt;\[
\mathbf{w} = \sum_{i=1}^N y_i \alpha_i \mathbf{x}_i, \quad b = \mathbf{w}^T \mathbf{x}_k - y_k.
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;The threshold is also dependent on the weight vector?&lt;/strong&gt;&lt;/strong&gt; The weight vector \(\mathbf{w}\) is computed using the training data. The threshold is only dependent on non-zero support vectors, \(\alpha_k &amp;gt; 0\).&lt;/p&gt;
&lt;h3 id=&#34;overlapping-distributions&#34;&gt;Overlapping Distributions&lt;/h3&gt;
&lt;p&gt;Slack variables were introduced to allow misclassifications at the cost of a linear penalty.
This is useful for datasets that are not linearly separable.
In practice, this is accomplished with a slight modification of the original objective function:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\min_{\mathbf{w},b} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^N \xi_i\\
\textrm{s.t.} \quad &amp;amp; y_i(\mathbf{w}^T \mathbf{x} - b) \geq 1 - \xi_i, \forall i\\
\end{align*}&lt;/p&gt;
&lt;p&gt;The convenience of this formulation is that the parameters \(\xi_i\) do not appear in the dual formulation at all.
The only added constraint is&lt;/p&gt;
&lt;p&gt;\[
0 \leq \alpha_i \leq C, \forall i.
\]&lt;/p&gt;
&lt;p&gt;This is referred to as the box constraint for reasons we shall see shortly.&lt;/p&gt;
&lt;h2 id=&#34;box-constraints&#34;&gt;Box Constraints&lt;/h2&gt;
&lt;p&gt;The smallest optimization step that SMO solves is that of two variables.
Given the constraints above, the solution lies on a diagonal line \(\sum_{i=1}^N y_i \alpha_i = 0\) bounded within a box \(0 \leq \alpha_i \leq C, \forall i\).&lt;/p&gt;
&lt;p&gt;Isolating for two samples with alphas \(\alpha_1\) and \(\alpha_2\), the constraint \(\sum_{i=1}^n y_i \alpha_i = 0\) suggests that&lt;/p&gt;
&lt;p&gt;\[
y_1 \alpha_1 + y_2 \alpha_2 = w.
\]&lt;/p&gt;
&lt;p&gt;We first consider the case when \(y_1 \neq y_2\).
Let \(y_1 = 1\) and \(y_2 = -1\), then \(a_1 - a_2 = w\).
As \(\alpha_1\) increases, \(\alpha_2\) must also increase to satisfy the constraint.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-07-10_22-48-56_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Equality constraint for case 1 (from Platt&amp;#39;s SMO paper).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Equality constraint for case 1 (from Platt&amp;rsquo;s SMO paper).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The other case is when \(y_1 = y_2\), then \(\alpha_1 + \alpha_2 = w\).
As \(\alpha_1\) is increased, \(\alpha_2\) is decreased to satisfy the constraint.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-07-10_22-51-53_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Box constraint for samples of the same class (from Platt&amp;#39;s SMO paper).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Box constraint for samples of the same class (from Platt&amp;rsquo;s SMO paper).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;updating-the-lagrangians&#34;&gt;Updating the Lagrangians&lt;/h2&gt;
&lt;p&gt;SMO solves for only two Lagrange multipliers at a time.
Solving for only 1 at a time would be impossible under the constraint \(\sum_{i=1}^N y_i \alpha_i = 0\).
The first step is to compute \(\alpha_2\) and constrain it between the ends of the diagonal line segment from the box constraints.&lt;/p&gt;
&lt;p&gt;If \(y_1 \neq y_2\), then the following bounds are applied to \(\alpha_2\):&lt;/p&gt;
&lt;p&gt;\begin{equation*}
L = \max(0, \alpha_2 - \alpha_1), \quad H = \min(C, C + \alpha_2 - \alpha_1)
\end{equation*}&lt;/p&gt;
&lt;p&gt;otherwise, the bounds are computed as:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
L = \max(0, \alpha_2 + \alpha_1 - C), \quad H = \min(C, \alpha_2 + \alpha_1)
\end{equation*}&lt;/p&gt;
&lt;p&gt;Updating the actual parameter is done following the update rule of gradient descent:&lt;/p&gt;
&lt;p&gt;\[
\alpha_2^{\text{new}} = \alpha_2 + \frac{y_2(E_1 - E_2)}{\eta}.
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do we arrive at this update rule?&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;second-derivative-of-the-objective-function&#34;&gt;Second Derivative of the Objective Function&lt;/h3&gt;
&lt;p&gt;Here, \(\eta\) represents the step size and direction. It is computed from the second derivative of the objective function along the diagonal line. To see that this is the case, consider the original objective function&lt;/p&gt;
&lt;p&gt;\begin{align*}
\min_{\mathbf{\alpha}} \quad &amp;amp; \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N y_i y_j K(\mathbf{x}_i, \mathbf{x}_j) \mathbf{\alpha}_1 \mathbf{\alpha}_2 - \sum_{i=1}^N \alpha_i\\
\textrm{s.t.} \quad &amp;amp; 0 \leq \alpha_i \leq C, \forall i\\
&amp;amp; \sum_{i=1}^N y_i \alpha_i = 0\\
\end{align*}&lt;/p&gt;
&lt;p&gt;Since we are optimizing with respect to only 2 Lagrangian multipliers at a time, we can write the Lagrangian function as&lt;/p&gt;
&lt;p&gt;\[
\frac{1}{2} y_1^2 K_{11} \alpha_1^2 + \frac{1}{2} y_2^2 K_{22} \alpha_2^2 + y_1 \alpha_1 \sum_{j=3}^N y_j \alpha_j K_{1j} + y_2 \alpha_2 \sum_{j=3}^N y_j \alpha_j K_{2j} - \alpha_1 - \alpha_2 + \sum_{j=3}^N \alpha_j
\]&lt;/p&gt;
&lt;p&gt;We are only optimizing with respect to \(\alpha_1\) and \(\alpha_2\), the next step is to extract those terms from the sum.
This is simplified further by noting that \(\sum_{j=3}^N y_j \alpha_j K_{ij}\) looks very similar to the output of an SVM:&lt;/p&gt;
&lt;p&gt;\[
u = \sum_{j=1}^N y_j \alpha_j K(\mathbf{x}_j, \mathbf{x}) - b.
\]&lt;/p&gt;
&lt;p&gt;This allows us to introduce a variable \(v_i\) based on \(u_i\), the output of an SVM given sample \(\mathbf{x}_i\):&lt;/p&gt;
&lt;p&gt;\[
v_i = \sum_{j=3}^N y_j \alpha_j K_{ij} = u_i + b - y_1 \alpha_1 K_{1i} - y_2 \alpha_2 K_{2i}.
\]&lt;/p&gt;
&lt;p&gt;The objective function is then written as&lt;/p&gt;
&lt;p&gt;\[
\frac{1}{2} y_1^2 K_{11} \alpha_1^2 + \frac{1}{2} y_2^2 K_{22} \alpha_2^2 + y_1 \alpha_1 v_1 + y_2 \alpha_2 v_2 - \alpha_1 - \alpha_2 + \sum_{j=3}^N \alpha_j.
\]&lt;/p&gt;
&lt;p&gt;Note that the trailing sum \(\sum_{j=3}^N \alpha_j\) is treated as a constant since those values are not considered when optimizing for \(\alpha_1\) and \(\alpha_2\).&lt;/p&gt;
&lt;p&gt;Given the box constraints from above, we must update \(\alpha_1\) and \(\alpha_2\) such that&lt;/p&gt;
&lt;p&gt;\[
\alpha_1 + s \alpha_2 = \alpha_1^* + s \alpha_2^* = w.
\]&lt;/p&gt;
&lt;p&gt;This linear relationship allows us to express the objective function in terms of α_2:&lt;/p&gt;
&lt;p&gt;\[
\Psi = \frac{1}{2} y_1^2 K_{11} (w - s \alpha_2)^2 + \frac{1}{2} y_2^2 K_{22} \alpha_2^2 + y_1 (w - s \alpha_2) v_1 + y_2 \alpha_2 v_2 - \alpha_1 - \alpha_2 + \sum_{j=3}^N \alpha_j.
\]&lt;/p&gt;
&lt;p&gt;The extremum of the function is given by the first derivative with respect to \(\alpha_2\):&lt;/p&gt;
&lt;p&gt;\[
\frac{d\Psi}{d\alpha_2} = -sK_{11}(w - s\alpha_2) + K_{22}\alpha_2 - K_{12}\alpha_2 + s K_{12} (w - s \alpha_2) - y_2 v_2 + s + y_2 v_2 - 1 = 0.
\]&lt;/p&gt;
&lt;p&gt;In most cases, the second derivative will be positive.
The minimum of \(\alpha_2\) is where&lt;/p&gt;
&lt;p&gt;\begin{align*}
\alpha_2 (K_{11} + K_{22} - 2 K_{12}) &amp;amp;= s(K_{11} - K_{12})w + y_2(v_1 - v_2) + 1 - s\\
&amp;amp;= s(K_{11} - K_{12})(s\alpha_2^*+\alpha_1^*)\\
&amp;amp;+ y_2(u_1-u_2+y_1\alpha_1^*(K_{12} - K_{11}) + y_2 \alpha_2^* (K_{22} - K_{21})) + y_2^2 - s\\
&amp;amp;= \alpha_2^*(K_{11}+K_{22} - 2K_{12}) + y_2(u_1 - u_2 + y_2 - y_1).
\end{align*}&lt;/p&gt;
&lt;p&gt;If we let \(E_1 = u_1 - y_1\), \(E_2 = u_2 - y_2\), and \(\eta = K_{11} + K_{22} - 2K_{12}\), then&lt;/p&gt;
&lt;p&gt;\[
\alpha_2^{\text{new}} = \alpha_2 + \frac{y_2(E_1 - E_2)}{\eta}.
\]&lt;/p&gt;
&lt;h2 id=&#34;the-algorithm&#34;&gt;The Algorithm&lt;/h2&gt;
&lt;p&gt;Sequential Minimal Optimization (SMO) solves the SVM problem which usually requires a Quadratic Programming (QP) solution.
It does this by breaking down the larger optimization problem into a small and simple form: solving for two Lagrangians.
Solving for one would not be possible without violating KKT conditions.
There are two components to Sequential Minimal Optimization: the first is how the Lagrangians are selected and the second is the actual optimization step.&lt;/p&gt;
&lt;h3 id=&#34;choosing-the-first-lagrangian&#34;&gt;Choosing the First Lagrangian&lt;/h3&gt;
&lt;p&gt;The algorithm first determines which samples in the dataset violate the given KKT conditions.
Only those violating the conditions are eligible for optimization.
Additionally, samples that are not on the bounds are selected (those with \(\alpha_i \neq 0\) and \(\alpha_i \neq C\)).
This continues through the dataset until no sample violates the KKT constraints within \(\epsilon\).&lt;/p&gt;
&lt;p&gt;As a last step, SMO searches the entire dataset to look for any bound samples that violate KKT conditions.&lt;/p&gt;
&lt;h3 id=&#34;choosing-the-second-lagrangian&#34;&gt;Choosing the Second Lagrangian&lt;/h3&gt;
&lt;p&gt;The second Lagrangian is chosen to maximize the size of the step taken during joint optimization.
Noting that the step size is based on&lt;/p&gt;
&lt;p&gt;\[
\alpha_2^{\text{new}} = \alpha_2 + \frac{y_2(E_1 - E_2)}{\eta},
\]&lt;/p&gt;
&lt;p&gt;it is approximated by computing \(|E_1 - E_2|\).&lt;/p&gt;
&lt;p&gt;If positive progress cannot be made given the choice of Lagrangian, SMO will begin iterating through non-bound examples.
If no eligible candidates are found in the non-bound samples, the entire dataset is searched.&lt;/p&gt;
&lt;h3 id=&#34;updating-the-parameters&#34;&gt;Updating the Parameters&lt;/h3&gt;
&lt;p&gt;With the second derivative of the objective function, we can take an optimization step along the diagonal line.
To ensure that this step adheres to the box constraints defined above, the new value of \(\alpha_2\) is clipped:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\alpha_2^{\text{new,clipped}} =
\begin{cases}
H &amp;amp;\text{if} \quad \alpha_2^{\text{new}} \geq H;\\
\alpha_2^{\text{new}} &amp;amp;\text{if} \quad L &amp;lt; \alpha_2^{\text{new}} &amp;lt; H;\\
L &amp;amp;\text{if} \quad \alpha_2^{\text{new}} \geq L.\\
\end{cases}
\end{equation*}&lt;/p&gt;
&lt;p&gt;With the new value of \(\alpha_2\), \(\alpha_1\) is computed such that the original KKT condition is preserved:&lt;/p&gt;
&lt;p&gt;\[
\alpha_1^{\text{new}} = \alpha_1 + s(\alpha_2 - \alpha_2^{\text{new,clipped}}),
\]&lt;/p&gt;
&lt;p&gt;where \(s = y_1y_2\).&lt;/p&gt;
&lt;p&gt;Points that are beyond the margin are given an alpha of 0: \(\alpha_i = 0\).
Points that are on the margin satisfy \(0 &amp;lt; \alpha_i &amp;lt; C\). These are the support vectors.
Points inside the margin satisfy \(\alpha_i = C\).&lt;/p&gt;
&lt;h4 id=&#34;linear-svms&#34;&gt;Linear SVMs&lt;/h4&gt;
&lt;p&gt;In the case of linear SVMs, the parameters can be stored as a single weight vector&lt;/p&gt;
&lt;p&gt;\[
\mathbf{w}^{\text{new}} = \mathbf{w} + y_1 (\alpha_1^{\text{new}} - \alpha_1)\mathbf{x}_1 + y_2(\alpha_2^{\text{new,clipped}} - \alpha_2)\mathbf{x}_2.
\]&lt;/p&gt;
&lt;p&gt;The output of a linear SVM is computed as&lt;/p&gt;
&lt;p&gt;\[
u = \mathbf{w}^T \mathbf{x} - b.
\]&lt;/p&gt;
&lt;h4 id=&#34;nonlinear-svms&#34;&gt;Nonlinear SVMs&lt;/h4&gt;
&lt;p&gt;In the nonlinear case, the output of the model is computed as&lt;/p&gt;
&lt;p&gt;\[
u = \sum_{i=1}^N y_i \alpha_i K(\mathbf{x}_i, \mathbf{x}) - b.
\]&lt;/p&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;An implementation of SMO in Python is available at &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/blob/main/svm/smo.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://github.com/ajdillhoff/CSE6363/blob/main/svm/smo.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Discriminant Functions</title>
      <link>https://ajdillhoff.github.io/notes/discriminant_functions/</link>
      <pubDate>Tue, 07 Jun 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/discriminant_functions/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#binary-classification&#34;&gt;Binary Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plotting-a-decision-boundary&#34;&gt;Plotting a Decision Boundary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiple-classes&#34;&gt;Multiple Classes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sensitivity-to-outliers&#34;&gt;Sensitivity to Outliers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The notebook for this lesson can be found &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/blob/main/logistic_regression/least_squares_classification.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_regression/&#34;&gt;linear regression&lt;/a&gt;, we were able to fit a model to our data in order to make inferences on unseen data points. In the examples, both the input features and observation were continuous. With discriminant functions, we will use similar models to classify the data points based on their input features. We start out with the simplest approach: we assume that the data is linearly separable and can be assigned one of \(K\) discrete classes.&lt;/p&gt;
&lt;p&gt;For classification with linear discriminant functions, we will use a \(K\) dimensional vector that has a 1 corresponding to the class encoding for that input and a 0 for all other positions. For example, if our possible target classes were \(\{\text{car, truck, person}\}\), then a target vector for \(\text{person}\) would be \(\mathbf{y} = [0, 0, 1]^T\).&lt;/p&gt;
&lt;p&gt;This article will stick to a discriminative approach to classification. That is, we define a discriminant function which assigns each data input \(\mathbf{x}\) to a class. For a probabilistic perspective, see &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_discriminant_analysis/&#34;&gt;Linear Discriminant Analysis&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We will again start with a linear model \(y = f(\mathbf{x}; \mathbf{w})\). Unlike the model used with &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_regression/&#34;&gt;linear regression&lt;/a&gt;, ours will need to predict a discrete class label. In other words, we need to predict a vector with a 1 corresponding to the class encoding.&lt;/p&gt;
&lt;h2 id=&#34;binary-classification&#34;&gt;Binary Classification&lt;/h2&gt;
&lt;p&gt;Consider a simple dataset with 2 features per data sample. Our goal is to classify the data as being one of two possible classes.
This only requires a single function which classifies the sample as being in class 0 if \(f(\mathbf{x};\mathbf{w}) \geq 0\) and class 1 otherwise.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-23_18-10-03_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Two groups of data that are very clearly linearly separable.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Two groups of data that are very clearly linearly separable.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The model output is such that \(f(\mathbf{x};\mathbf{w}) = [1, 0]\) when \(\mathbf{x}\) is predicted as class 1. If \(f(\mathbf{x};\mathbf{w}) = [0, 1]\) then \(\mathbf{x}\) is assigned to class 2.
In practice, the actual output will not be a one-hot vector.
There will be some values in all positions of the vector.&lt;/p&gt;
&lt;p&gt;For example, a model trained on a binary classification task outputs the following vector given a randomly selected input sample:&lt;/p&gt;
&lt;p&gt;\[
[0.1224, 0.8776]
\]&lt;/p&gt;
&lt;p&gt;A class would be assigned by taking the argmax of this output vector.
That is, the model predicts that this sample belongs to class 1.&lt;/p&gt;
&lt;h3 id=&#34;measuring-classifier-performance&#34;&gt;Measuring Classifier Performance&lt;/h3&gt;
&lt;p&gt;L1 loss can be used to measure classifier performance for linear discriminant function models.&lt;/p&gt;
&lt;p&gt;\[
E = \sum_{i=1}^N \sum_{j=1}^M |\hat{y}_{ij} - y_{ij}|
\]&lt;/p&gt;
&lt;h2 id=&#34;plotting-a-decision-boundary&#34;&gt;Plotting a Decision Boundary&lt;/h2&gt;
&lt;p&gt;In the case of binary classification, a sample is predicted as class 1 if the output vector has the highest value at index 0.
Otherwise, it is classified as class 2.
If we were to plot the decision regions, we would see that the boundary is at the point when the output for both classes is equal.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-10_19-03-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Binary classification with decision regions shown.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Binary classification with decision regions shown.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;multiple-classes&#34;&gt;Multiple Classes&lt;/h2&gt;
&lt;p&gt;Extending this to multiple classes is as easy as encoding the classes in a one-hot vector whose length equals the number of classes.
The parameters of the model can be obtained using gradient descent, the normal equations, or any other method that optimizes the least squares criterion.&lt;/p&gt;
&lt;p&gt;The figure below shows an example of a linear discriminant function model fit to a dataset with 3 classes.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-10_19-08-29_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Multiclass classification using linear discriminant functions.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Multiclass classification using linear discriminant functions.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;sensitivity-to-outliers&#34;&gt;Sensitivity to Outliers&lt;/h2&gt;
&lt;p&gt;One major flaw with least squares models is their sensitivity to outliers in the data.
Consider the dataset shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-11_11-28-25_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Linearly separable dataset&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Linearly separable dataset
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This dataset is clearly linearly separable. This will be no problem for our linear classifier, as seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-11_11-29-36_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Linear classifier fit to data using least squares.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Linear classifier fit to data using least squares.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This dataset has a convenient property that the samples from each class are tightly clustered.
What happens if our data is slightly more diverse?&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-11_11-32-10_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;2-class dataset in which one class is not as tightly clustered as the other.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;2-class dataset in which one class is not as tightly clustered as the other.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the dataset above, we can still clearly see that it should be linearly separable.
Unfortunately, our least squares model will be very sensitive to the 20 points at the top left of the plot.
Training a linear discriminant function using least squares results in the following decision boundary.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-11_11-33-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;The model misclassifies samples that should be linearly separable.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;The model misclassifies samples that should be linearly separable.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;If we determine that a linear classifier is adequate for a given dataset, we may wish to use a slightly more robust model such as &lt;a href=&#34;https://ajdillhoff.github.io/notes/logistic_regression/&#34;&gt;Logistic Regression&lt;/a&gt; instead of linear discriminant functions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Long Short-Term Memory</title>
      <link>https://ajdillhoff.github.io/notes/long_short_term_memory/</link>
      <pubDate>Tue, 12 Apr 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/long_short_term_memory/</guid>
      <description>&lt;p&gt;The recurrent nature of RNNs means that gradients get smaller and smaller as the timesteps increase.
This is known as the &lt;strong&gt;vanishing gradient problem&lt;/strong&gt;.
One of the first popular solutions to this problem is called &lt;strong&gt;Long Short-Term Memory&lt;/strong&gt;, a recurrent network architecture by Hochreiter and Schmidhuber.&lt;/p&gt;
&lt;p&gt;An LSTM is made up of memory blocks as opposed to simple hidden units.
Each block is differentiable and contains a memory cell along with 3 gates: the input, output, and forget gates.
These components allow the blocks to maintain some history of information over longer range dependencies.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_13-36-14_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;LSTM memory block with a single cell (adapted from Andrew Ng&amp;#39;s diagram).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;LSTM memory block with a single cell (adapted from Andrew Ng&amp;rsquo;s diagram).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The original LSTM only had an input and output gate.
Forget gates were added in 2000 by Gers et al. to control the amount of context that could be reset, if the task called for it.
Peephole connections were proposed by Gers et al. in 2002.
These are weights that combine the previous cell state to the gates in order to learn tasks that require precise timing.&lt;/p&gt;
&lt;p&gt;By controlling when information can either enter or leave the memory cell, LSTM blocks are able to maintain more historical context than RNNs.&lt;/p&gt;
&lt;h2 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h2&gt;
&lt;p&gt;The equations listed here follow notation and description from Alex Graves&amp;rsquo; thesis.&lt;/p&gt;
&lt;p&gt;The weight from unit \(i\) to \(j\) is given as \(w_{ij}\).
The input to unit \(j\) at time \(t\) is \(a_j^t\) and the result of its activation function is \(b_j^t\).
Let \(\psi\), \(\phi\), and \(\omega\) be the input, forget, and output gates.
A memory cell is denoted by \(c \in C\), where \(C\) is the set of cells in the network.
The activation, or state, of a given cell \(c\) at time \(t\) is \(s_c^t\).
The output of each gate passes through an activation function \(f\), while the input and output activation functions of a memory block are given by \(g\) and \(h\).&lt;/p&gt;
&lt;p&gt;The forward pass for the input gates is&lt;/p&gt;
&lt;p&gt;\[
a_{\psi}^t = \sum_{i=1}^I w_{i\psi}x_i^t + \sum_{h=1}^H w_{h\psi}b_h^{t-1} + \sum_{c=1}^C w_{c\psi}s_c^{t-1}.
\]&lt;/p&gt;
&lt;p&gt;The output of the forget gates is&lt;/p&gt;
&lt;p&gt;\[
a_{\phi}^t = \sum_{i=1}^I w_{i\phi}x_i^t + \sum_{h=1}^H w_{h\phi}b_h^{t-1} + \sum_{c=1}^C w_{c\phi}s_c^{t-1}.
\]&lt;/p&gt;
&lt;p&gt;The output of the output gates is&lt;/p&gt;
&lt;p&gt;\[
a_{\omega}^t = \sum_{i=1}^I w_{i\omega}x_i^t + \sum_{h=1}^H w_{h\omega}b_h^{t-1} + \sum_{c=1}^C w_{c\omega}s_c^{t-1}.
\]&lt;/p&gt;
&lt;p&gt;Each of the outputs above is passed through an activation function \(f\).&lt;/p&gt;
&lt;p&gt;The output of each cell is computed as&lt;/p&gt;
&lt;p&gt;\[
a_c^t = \sum_{i=1}^I w_{ic}x_i^t + \sum_{i=1}^H w_{hc}b_h^{t-1}
\]&lt;/p&gt;
&lt;p&gt;and the internal state is updated via&lt;/p&gt;
&lt;p&gt;\[
s_c^t = b_{\phi}^t s_c^{t-1} + b_{\psi}^t g(a_c^t).
\]&lt;/p&gt;
&lt;p&gt;The state update considers the state at the previous timestep multiplied by the output of the forget gate.
That is, it controls how much of the current memory to keep.&lt;/p&gt;
&lt;p&gt;The final cell output is given as&lt;/p&gt;
&lt;p&gt;\[
b_c^t = b_{\omega}^t h(s_c^t).
\]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recurrent Neural Networks</title>
      <link>https://ajdillhoff.github.io/notes/recurrent_neural_networks/</link>
      <pubDate>Sun, 10 Apr 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/recurrent_neural_networks/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bidirectional-recurrent-neural-networks&#34;&gt;Bidirectional Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Neural networks are an effective tool for regression and classification tasks, but they do not consider the dependencies of information over time.
Many tasks have implicit information that is dependent on input that may have already been processed or may not be seen until the future.&lt;/p&gt;
&lt;p&gt;Recurrent Neural Networks (RNN) consider the historical context of time-series data.
Bi-directional Recurrent Neural Networks (BRNN) consider both historical and future context. This is necessary for tasks like language tanslation.&lt;/p&gt;
&lt;p&gt;Parameter sharing across different parts of the model is key for sequence models.
Different instances of a particular feature may appear at different time steps.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I see Naomi there.&amp;rdquo; and &amp;ldquo;Naomi is right there&amp;rdquo; both convey that Naomi is present, but we would not require the model to have separate parameters just because the word position is different between the two.&lt;/p&gt;
&lt;p&gt;Recurrent connections provide a memory of sorts.
This enables important contextual information to be &amp;ldquo;remembered&amp;rdquo; throughout time.
These models are not without their limitations.
When trained with gradient descent, the gradient information passed throughout multiple time steps can become insignificant.
There are several ways to address the &lt;strong&gt;vanishing gradient&lt;/strong&gt; problem which are explored in alternative models such as &lt;a href=&#34;https://ajdillhoff.github.io/notes/long_short_term_memory/&#34;&gt;Long Short-Term Memory&lt;/a&gt; and &lt;a href=&#34;https://ajdillhoff.github.io/notes/transformers/&#34;&gt;Transformers&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;The definition of RNNs start with that of &lt;a href=&#34;https://ajdillhoff.github.io/notes/neural_networks/&#34;&gt;Neural Networks&lt;/a&gt;.
One layer of an RNN has some number of hidden units that transforms the input into an intermediate representation.
In addition to transforming the input, another set of parameters is used to transform the hidden context over time.
The difference is that the hidden layer is shared over time, as seen in the equation below.&lt;/p&gt;
&lt;p&gt;\[
\mathbf{h}^{(t)} = f(\mathbf{h}^{(t-1)}, \mathbf{x}^{(t)}; \mathbf{\theta})
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-09_16-36-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Computation graph of an RNN (By fdeloche - Own work, CC BY-SA 4.0, &amp;lt;https://commons.wikimedia.org/w/index.php?curid=60109157&amp;gt;)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Computation graph of an RNN (By fdeloche - Own work, CC BY-SA 4.0, &lt;a href=&#34;https://commons.wikimedia.org/w/index.php?curid=60109157&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://commons.wikimedia.org/w/index.php?curid=60109157&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the computation graph above, a recurrent network has three weight matrices associated with its forward pass.
An input weight matrix \(U \in \mathbb{R}^{H \times D}\) processes the features for each &lt;em&gt;frame&lt;/em&gt; of the input sequence.
The hidden layer has a weight matrix \(V \in \mathbb{R}^{H \times H}\), where \(H\) is the number of hidden nodes.
The output layer will have a weight matrix \(W \in \mathbb{R}^{O \times H}\).&lt;/p&gt;
&lt;h3 id=&#34;forwards-pass&#34;&gt;Forwards Pass&lt;/h3&gt;
&lt;p&gt;To understand the computation graph of an RNN, consider an input of length \(T\) with \(D\) features. That is, each input sample is a sequence of features. This could be represented as encoded video data, text data, or any other sequence signals.
To compute the output of a hidden layer \(\mathbf{h}\) at time \(t\), take a linear combination of all input feature \(x_i^t\) at time \(t\) in addition to the output of the previous hidden layer and then add the linear combination of output activations for each node in the hidden layer:&lt;/p&gt;
&lt;p&gt;\[
a_h^t = \sum_{d=1}^D w_{dh} x_d^t + \sum_{h&amp;rsquo;=1}^H w_{h&amp;rsquo;h} b_{h&amp;rsquo;}^{t-1},
\]&lt;/p&gt;
&lt;p&gt;where \(b_h^t = \theta_h(a_h^t)\) and we assume the bias term is concatenated with the weights.&lt;/p&gt;
&lt;p&gt;Weights in the hidden layer are crucial for RNNs to adapt to contextual features based on their occurrence relative to time.
For example, a character-based language model based on a traditioinal network would produce similar output for consecutive letters that are the same.
In an RNN, the hidden weights would produce a different output for each consecutive character even if it were the same.&lt;/p&gt;
&lt;p&gt;The hidden layer outputs are used in both the subsequent computations through time as well as the output node for each instance \(t\). The inputs to the output node are computed from the hidden node at the same time as the output to the hidden activation:&lt;/p&gt;
&lt;p&gt;\[
a_k^t = \sum_{h=1}^H w_{hk}b_h^t.
\]&lt;/p&gt;
&lt;h3 id=&#34;backwards-pass&#34;&gt;Backwards Pass&lt;/h3&gt;
&lt;p&gt;The gradients of a recurrent network are computed using backpropagation, similar to neural networks.
Since the forward pass is over \(t\) time step, the backward pass must consider them as well.
This variant of backpropagation for recurrent models is calling backpropagation through time (BPTT).&lt;/p&gt;
&lt;p&gt;Like a feed forward network, the output is dependent on the activation of the hidden layer.
For a recurrent model, its dependence is through the output of the hidden layer as well as the pass to the next hidden time step.&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathcal{L}}{\partial a_h^t} = \frac{\partial \mathcal{L}}{\partial b_h^t} \frac{\partial b_h^t}{\partial a_h^t}
\]&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathcal{L}}{\partial b_h^t} = \sum_{k=1}^K \frac{\partial \mathcal{L}}{\partial a_k^t} \frac{\partial a_k^t}{\partial b_h^t} + \sum_{h&amp;rsquo;=1}^H \frac{\partial \mathcal{L}}{\partial a_{h&amp;rsquo;}^{t+1}} \frac{\partial a_{h&amp;rsquo;}^{t+1}}{\partial a_{h}^t}
\]&lt;/p&gt;
&lt;p&gt;The derivatives with respect to the weights are given as&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathcal{L}}{\partial w_{ij}} = \sum_{t=1}^T \frac{\partial \mathcal{L}}{\partial a_j^t} \frac{\partial a_j^t}{\partial w_{ij}}.
\]&lt;/p&gt;
&lt;h2 id=&#34;bidirectional-recurrent-neural-networks&#34;&gt;Bidirectional Recurrent Neural Networks&lt;/h2&gt;
&lt;p&gt;Standard RNNs work for many problems with sequential input.
Training such a model would consider the full input through time \(T\), but inference may only be able to consider the data up to time \(t &amp;lt; T\).
There are sequential tasks which could leverage from both past and future context, such as language translation.
For this case, BRNNs were proposed &amp;lt;&amp;amp;schusterBidirectionalRecurrentNeural1997&amp;gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-11-06_15-18-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Diagram of BRNN from Graves et al.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Diagram of BRNN from Graves et al.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://karpathy.github.io/2015/05/21/rnn-effectiveness/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;The Unreasonable Effectiveness of RNNs&lt;/a&gt; by Andrej Karpathy&lt;/li&gt;
&lt;li&gt;&amp;lt;&amp;amp;gravesSupervisedSequenceLabelling2012&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Understanding LSTM Networks&lt;/a&gt; by Christopher Colah&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Convolutional Neural Networks</title>
      <link>https://ajdillhoff.github.io/notes/convolutional_neural_networks/</link>
      <pubDate>Sat, 02 Apr 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/convolutional_neural_networks/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#useful-resources&#34;&gt;Useful Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#convolution-operator&#34;&gt;Convolution Operator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sizes-and-types&#34;&gt;Sizes and Types&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parameter-sharing&#34;&gt;Parameter Sharing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pooling&#34;&gt;Pooling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backwards-pass&#34;&gt;Backwards Pass&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example&#34;&gt;Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#neural-networks-for-image-classification&#34;&gt;Neural Networks for Image Classification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;useful-resources&#34;&gt;Useful Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.songho.ca/dsp/convolution/convolution.html#convolution_2d&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.songho.ca/dsp/convolution/convolution.html#convolution_2d&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vdumoulin/conv_arithmetic&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://github.com/vdumoulin/conv_arithmetic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cs231n.github.io/convolutional-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://cs231n.github.io/convolutional-networks/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://grzegorzgwardys.wordpress.com/2016/04/22/8/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://grzegorzgwardys.wordpress.com/2016/04/22/8/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Early success seen in (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;LeCun et al. 1989&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;convolution-operator&#34;&gt;Convolution Operator&lt;/h2&gt;
&lt;p&gt;\[
(f * g)(t) = \int x(a)g(t - a)da
\]&lt;/p&gt;
&lt;p&gt;Commonly written in DL papers as&lt;/p&gt;
&lt;p&gt;\[
(x * w)(t) = \int x(a)w(t - a)da,
\]&lt;/p&gt;
&lt;p&gt;where \(x\) is the &lt;strong&gt;input&lt;/strong&gt; and \(w\) is the &lt;strong&gt;kernel&lt;/strong&gt;. The result the &lt;strong&gt;feature map&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In the case of discrete values, the operator is written as&lt;/p&gt;
&lt;p&gt;\[
(x * w)(t) = \sum_{a}x(a)w(t-a).
\]&lt;/p&gt;
&lt;p&gt;In machine learning, the kernel \(w\) is usually represented by some set of parameters that is optimized.&lt;/p&gt;
&lt;p&gt;CNNs for images use a 2D convolution defined as&lt;/p&gt;
&lt;p&gt;\[
(I * K)(i, j) = \sum_m \sum_n I(m, n)K(i-m, j-n).
\]&lt;/p&gt;
&lt;p&gt;In this formulation, the kernel is effectively flipped across the vertical and horizontal axis.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-05_19-09-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;2D Convolution (Image Credit: Song Ho Ahn (linked above)).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;2D Convolution (Image Credit: Song Ho Ahn (linked above)).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Convolution is commutative:&lt;/p&gt;
&lt;p&gt;\[
(K * I)(i, j) = \sum_m \sum_n I(i-m, j-n)K(m, n).
\]&lt;/p&gt;
&lt;p&gt;In practice, most deep learning APIs implement &lt;strong&gt;cross-correlation&lt;/strong&gt;.
Whether the function is implemented as true convolution makes no difference when it comes to optimizing a deep model since filter weights that are produced with cross-correlation would be produced, albeit flipped, with convolution.&lt;/p&gt;
&lt;p&gt;\[
(K * I)(i, j) = \sum_m \sum_n I(i+m, j+n)K(m, n).
\]&lt;/p&gt;
&lt;h2 id=&#34;sizes-and-types&#34;&gt;Sizes and Types&lt;/h2&gt;






&lt;figure&gt;

&lt;img src=&#34;Introduction/2022-04-05_17-32-27_full_padding_no_strides.gif&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Convolution with full padding and a stride of 1 (Dumoulin &amp;amp;amp; Visin).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Convolution with full padding and a stride of 1 (Dumoulin &amp;amp; Visin).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;parameter-sharing&#34;&gt;Parameter Sharing&lt;/h2&gt;
&lt;p&gt;In a densely connected layer, each input has a corresponding weight attached to it.
For example, we ran a few &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/tree/main/deep_learning&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;introductory experiments&lt;/a&gt; on the CIFAR10 dataset using a deep, densely connected network.
To reduce the amount of parameters in the first layer, we converted each image to grayscale.
The input also had to be vectorized in order to be processed.
With a processed image of size \(32 \times 32\), this resulted in a \(1024\) dimensional vector for each input.
Our first layer had \(512\) nodes resulting in a parameter matrix of size \(1024 \times 512\).&lt;/p&gt;
&lt;p&gt;Convolution layers have &lt;strong&gt;shared parameters&lt;/strong&gt;, meaning the same parameters are used for each region on the input.
A single channel 2D filter of size \(n \times n\) only requires \(n \times n\) parameters.
Each kernel is applied to every location in the original input using the same parameters.&lt;/p&gt;
&lt;p&gt;Kernels are &lt;strong&gt;equivariant&lt;/strong&gt; to translation because of their shared parameters.
That is, as the input changes, the output will change in the same way.
Formally, two functions \(f\) and \(g\) are equivarient if&lt;/p&gt;
&lt;p&gt;\[
f(g(x)) = g(f(x)).
\]&lt;/p&gt;
&lt;p&gt;In the context of image features, a kernel applied across an image will produce strong responses in regions that exhibit the same local features.
For example, a kernel that detects horizontal lines will produce strong responses across all parts of the image that show a large contrast between vertical pixels.&lt;/p&gt;
&lt;h2 id=&#34;pooling&#34;&gt;Pooling&lt;/h2&gt;
&lt;p&gt;When a convolution is applied to some input image, the resulting output feature map represents the responses of the kernel applied to each location in the image.
If this original image were to be shifted by a few pixels, the reponses would also be shifted.
In order to increase the robustness of a model to small perturbations such as translation, a pooling layer was historically employed after each non-linear activation following a convolutional layer.&lt;/p&gt;
&lt;p&gt;They effectively provide a summary statistic of a local region by selecting the average or maximum responses in a small window. This provides translation invariance since the maximum response will be the same for a region even if it is translated by a small amount.
It also acts as a quick way to downsample the image, leading to fewer parameters in the model.&lt;/p&gt;
&lt;p&gt;Modern works do not employ pooling operations as often. For example (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;He et al. 2016&lt;/a&gt;) perform dimensionality reduction with \(1 \times 1\) convolutions.
(&lt;a href=&#34;#citeproc_bib_item_5&#34;&gt;Springenberg et al. 2015&lt;/a&gt;) argue that fully convolutional networks can achieve the same performance without max pooling.&lt;/p&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster.&amp;rdquo; - Geoffrey Hinton&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;h2 id=&#34;backwards-pass&#34;&gt;Backwards Pass&lt;/h2&gt;
&lt;p&gt;The parameters of a convolutional layer are updated via backpropagation like any other layer with trainable parameters.
Given a kernel \(w\), it is necessary to compute \(\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}}\), where \(w_{m&amp;rsquo;, n&amp;rsquo;}\) is the \((m&amp;rsquo;, n&amp;rsquo;)th\) entry of the kernel.
This entry affects all entries in the feature map, so \(\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}}\) will sum over all such entries.&lt;/p&gt;
&lt;p&gt;To show the gradient calculation, we will assume a convolutional layer with zero padding and unit stride with a square \(2 \times 2\) kernel applied to a square \(3 \times 3\) input.
The output map is then \((3 - 2 + 1) \times (3 - 2 + 1) = 2 \times 2\).&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}} = \sum_{i=0}^2 \sum_{j=0}^2 \frac{\partial \mathcal{L}}{\partial x_{i, j}} \frac{\partial x_{i,j}}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}}
\]&lt;/p&gt;
&lt;p&gt;If \(\mathbf{z}^{(l-1)}\) is the output from the previous layer, then&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{\partial x_{i, j}}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}} &amp;amp;= \frac{\partial}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}} \sum_{m} \sum_{n} w_{m, n} z_{i+m, j+n}^{(l-1)} + b\\
&amp;amp;= \frac{\partial}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}} w_{m&amp;rsquo;, n&amp;rsquo;}z_{i+m&amp;rsquo;, j+n&amp;rsquo;}^{(l-1)}\\
&amp;amp;= z_{i+m&amp;rsquo;, j+n&amp;rsquo;}^{(l-1)}
\end{align*}&lt;/p&gt;
&lt;p&gt;Then \(\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}}\) becomes&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}} &amp;amp;= \sum_{i=0}^2 \sum_{j=0}^2 \frac{\partial \mathcal{L}}{\partial x_{i, j}} z_{i+m&amp;rsquo;, j+n&amp;rsquo;}^{(l-1)}\\
&amp;amp;= \frac{\partial \mathcal{L}}{\partial x_{i, j}} * z_{m&amp;rsquo;, n&amp;rsquo;}^{(l-1)}.
\end{align*}&lt;/p&gt;
&lt;p&gt;\(\frac{\partial \mathcal{L}}{\partial x_{i, j}}\) represent the gradients with respect to the feature maps. To match the flipped kernel used in the forward pass, they are flipped in an opposite manner.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s train and evaluate a convolutional neural network on the OG network: LeNet5 (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;LeCun et al. 1989&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;neural-networks-for-image-classification&#34;&gt;Neural Networks for Image Classification&lt;/h2&gt;
&lt;h3 id=&#34;ilsvrc&#34;&gt;ILSVRC&lt;/h3&gt;
&lt;p&gt;The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is the most popular image classification and object detection challenge starting in 2010. It now exists as the ILSVRC 2012-2017 challenge on &lt;a href=&#34;https://www.kaggle.com/c/imagenet-object-localization-challenge/overview/description&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Kaggle&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;alexnet&#34;&gt;AlexNet&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://code.google.com/archive/p/cuda-convnet/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://code.google.com/archive/p/cuda-convnet/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The network that arguably popuarlized deep learning by achieving a 37.5% top-1 and 17% top-5 error rate on the ILSVRC-2010 test set. This model performed significantly better than leading competitors (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Krizhevsky, Sutskever, and Hinton 2017&lt;/a&gt;).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-12_18-25-11_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;ILSVRC-2010 results reported by Krizhevsky et al.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;ILSVRC-2010 results reported by Krizhevsky et al.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This performance was based on many different insights and techniques including ReLU activations and dropout.
The authors stated in their original publication that the large capacity of the model is necessary to fully describe the diversity of objects in ImageNet.&lt;/p&gt;
&lt;h4 id=&#34;architecture&#34;&gt;Architecture&lt;/h4&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-12_18-35-38_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;AlexNet architecture (from Krizhevsky et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;AlexNet architecture (from Krizhevsky et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;AlexNet is made up of 5 convolutional layers followed by 3 fully-connected layers.
The outputs of the last layer are used as input to the softmax function.&lt;/p&gt;
&lt;p&gt;Each layer uses the ReLU activation function.&lt;/p&gt;
&lt;p&gt;\[
f(x) = \max(0, x)
\]&lt;/p&gt;
&lt;p&gt;The justification for switching to ReLU as opposed to sigmoid or tanh is the faster training times.
Experiments on smaller CNNs show that networks with ReLU reach 25% training error on CIFAR-10 six times faster than those with tanh activations.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-12_18-49-42_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Training loss over time using ReLU (solid) versus tanh (dotted) (from Krizhevsky et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Training loss over time using ReLU (solid) versus tanh (dotted) (from Krizhevsky et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Another benefit of ReLU activations is that they are less reliant on input normalization.
In a saturating activation function like tanh, large absolute values in the inputs will be clamped to either -1 or 1.
ReLU is unbounded above 0.
Networks can still train as long as some input is positive.
Local response normalization (LRN) is used after the first and second convolutional layers.&lt;/p&gt;
&lt;p&gt;The motivation behind LRN is taken from &lt;em&gt;lateral inhibition&lt;/em&gt; in neurobiology.
An overly excited neuron (one with a high response) can subdue or dampen the responses from local neighbors.
If all responses in a local region are uniformly large, which can happend since ReLU is unbounded, it will dampen them all.&lt;/p&gt;
&lt;p&gt;In practice, they showed that applying LRNs to their model reduced the top-1 and top-5 error rates by 1.4% and 1.2%, respectively.&lt;/p&gt;
&lt;h4 id=&#34;regularization&#34;&gt;Regularization&lt;/h4&gt;
&lt;p&gt;The entire network has 60 million parameters.
Even with so many parameters and training on a dataset with over 8 million images, their model overfits the training data quickly without the aid of regularization.
They employ both image translations and horizontal reflections.&lt;/p&gt;
&lt;p&gt;The use of translations is where the popular \(224 \times 224\) training size originated.
The original size of the images in the dataset is \(256 \times 256\).
To work with random translations without worrying about padding, they crop the final output to \(224 \times 224\).
The final output of the network extracts 5 \(224 \times 224\) patches from the test input and averages the network prediction made on each patch.&lt;/p&gt;
&lt;p&gt;Additionally, they alter the RGB intensities so that the network is less reliant on specific intensities and illumination for each object.
The intuition is that the identity of an object is invariant to lighting conditions.&lt;/p&gt;
&lt;p&gt;As a last form of regularization, they employ dropout in the first two fully-connected layers.&lt;/p&gt;
&lt;h4 id=&#34;training&#34;&gt;Training&lt;/h4&gt;
&lt;p&gt;They trained their model on a training set of 1.2 million images using two NVIDIA GTX 580 3GB GPUs.
They had to write their own optimized CUDA code for this since deep learning frameworks such as Tensorflow and PyTorch did not exist yet.
The training took ~6 days to pass 90 epochs.&lt;/p&gt;
&lt;h3 id=&#34;vgg&#34;&gt;VGG&lt;/h3&gt;
&lt;p&gt;Published in 2015, (&lt;a href=&#34;#citeproc_bib_item_4&#34;&gt;Simonyan and Zisserman 2015&lt;/a&gt;) explore how depth plays a role in convolutional neural networks.
They systematically increase the depth of the network while keep other hyperparameters fixed.
The filter sizes are also kept at \(3 \times 3\).&lt;/p&gt;
&lt;p&gt;Similar to (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Krizhevsky, Sutskever, and Hinton 2017&lt;/a&gt;), they use ReLU activations and in only one of their models to they employ Local Response Normalization.
They found that adding LRN to their model did not increase performance.
Instead, it only increased computation time and memory consumption.
Their models are summarized in the table below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-13_07-48-12_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Model configurations used (Simonyan and Zisserman).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Model configurations used (Simonyan and Zisserman).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The number of parameters for each network is 133 million, 133 million, 134 million, 138 million, and 144 million starting from A to E.&lt;/p&gt;
&lt;h3 id=&#34;googlenet&#34;&gt;GoogLeNet&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_14-18-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;The network-in-network architecture pairs perfectly with the Inception meme.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;The network-in-network architecture pairs perfectly with the Inception meme.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Proposed a 22-layer network architecture that has \(12 \times\) fewer parameters than (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Krizhevsky, Sutskever, and Hinton 2017&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The authors were already thinking about applications in mobile computing, where hardware limitations would require smaller networks that still perform well.&lt;/p&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;In this paper, we will focus on an efficient deep neural network architecture for computer vision, codenamed Inception, which derives its name from the Network in network paper by Lin et al. in conjunction with the famous “we need to go deeper” internet meme.&amp;rdquo; - Szegedy et al.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;It was apparent at the time that building larger networks would generally lead to better performance.
Adding more parameters leads to easier overfitting.
Bigger networks also mean more computation. If the goal is to adapt high quality networks into mobile computing, solutions would have to include more sophistication than simply adding more components.&lt;/p&gt;
&lt;h4 id=&#34;hebbian-learning&#34;&gt;Hebbian Learning&lt;/h4&gt;
&lt;p&gt;A linear increase in filters leads to a quadratic increase in computation.
If most filter parameters end up being close to 0, then this increase in model capacity is wasted.
One solution is to include sparsity in the network instead of having dense connections.
Szegedy et al. were motivated by the work of Arora et al., which they summarized as follows.&lt;/p&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Their main result states that if the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs.&amp;rdquo; - Szegedy et al.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;This result relates with &lt;a href=&#34;https://en.wikipedia.org/wiki/Hebbian_theory&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Hebbian theory&lt;/a&gt; on synaptic plasticity which is summarized as &amp;ldquo;neurons that fire together, wire together.&amp;rdquo;&lt;/p&gt;
&lt;h4 id=&#34;from-theory-to-architecture&#34;&gt;From Theory to Architecture&lt;/h4&gt;
&lt;p&gt;Motivated by sparse connections, the architecture is designed to approximate sparsity given current dense components like convolutional layers.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_14-52-12_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Naive version of the Inception module (Szegedy et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Naive version of the Inception module (Szegedy et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The Inception module design as seen above is motivated as follows.
In layers closer to the raw input, filters would be grouped into local regions.
In this case, a \(1 \times 1\) convolution would summarize these groups.&lt;/p&gt;
&lt;p&gt;For clusters that are spread out, a larger filter would be needed to cover the larger regions.
This motivates the use of \(3 \times 3\) and \(5 \times 5\) filters.&lt;/p&gt;
&lt;p&gt;The choice to include a max pooling function in each module is based on previous successes of using max pooling.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-01-46_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Description of layers from Szegedy et al.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Description of layers from Szegedy et al.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;vanishing-gradients&#34;&gt;Vanishing Gradients&lt;/h4&gt;
&lt;p&gt;Creating a deeper network means that training is more susceptible to the vanishing gradient problem.
They noted that shallower networks that perform well on image classification would surely provide strong disciminative features.
They leverage this idea by computing 2 additional intermediate outputs: one in the middle of the network and an additional output 3 layers beyond that one.
This permits the gradients to be strengthened by intermediate losses when combined with the original gradients.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-07-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;GoogLeNet model (Szegedy et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;GoogLeNet model (Szegedy et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;results&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;GoogLeNet took 1st place in the 2014 ILSVRC with a 6.67% top-5 error rate.&lt;/p&gt;
&lt;h3 id=&#34;resnet&#34;&gt;ResNet&lt;/h3&gt;
&lt;p&gt;By 2016, it was clear that deeper models could build a richer hierarchy of features leading to better performance on a wide range of computer vision tasks.
However, with deeper networks comes the vanishing gradient problem.
Training them remained difficult for a time, but initialization and other normalization techniques found ways to resolve this issue.&lt;/p&gt;
&lt;p&gt;With deeper networks, a new problem appeared.
Adding more layers generally results in higher accuracy.
At a certain point, adding additional layers leads to a decrease in accuracy.
Many experiments ruled out the possibility of overfitting by observing that the training error was increasing as well.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-19-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 11: &amp;lt;/span&amp;gt;Result of experiments showing that decreased accuracy was not a result of overfitting.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;Result of experiments showing that decreased accuracy was not a result of overfitting.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;identity-mappings&#34;&gt;Identity Mappings&lt;/h4&gt;
&lt;p&gt;Consider a shallow network with some measure performance on a task.
If we were to add additional layers to make this network deeper, but those layers were simply identity mappings, then we should expect an error no greater than the original shallow network.
However, current solvers are unable to find such a solution in a reasonable amount of time on an equally deep network optimized from a random initialization.&lt;/p&gt;
&lt;h4 id=&#34;residual-functions&#34;&gt;Residual Functions&lt;/h4&gt;
&lt;p&gt;The main idea of this paper is to attempt to learn a residual function \(\mathcal{F}(\mathbf{x}) := \mathcal{H}(\mathbf{x}) - \mathbf{x}\) of the desired mapping \(\mathcal{H}(\mathbf{x})\) rather than attempting to learn the mapping directly.
The desired mapping then given by \(\mathcal{H}(\mathbf{x}) = \mathcal{F}(\mathbf{x}) + \mathbf{x}\).
If it were optimal to learn an identity mapping, the idea is that it would be simpler to learn by moving towards a 0 residual.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-45-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 12: &amp;lt;/span&amp;gt;Residual unit (He et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 12: &lt;/span&gt;Residual unit (He et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The function can be implemented into neural networks by using skip connections, as seen in the figure above.
Adding these identity mappings does not require any additional parameters, as the input is simply passed to the end of the stack.&lt;/p&gt;
&lt;h4 id=&#34;architecture-complexity&#34;&gt;Architecture Complexity&lt;/h4&gt;
&lt;p&gt;They compare a 34-layer plain network based on the VGG-19 architecture with a 34-layer residual network.
They note that VGG-19 has more filters and higher complexity than their residual network.
Specifically, VGG-19 requires 19.6 billion FLOPs compared to only 3.6 billion for their 34-layer residual network.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-49-39_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 13: &amp;lt;/span&amp;gt;Comparison of architectures and their complexity (He et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 13: &lt;/span&gt;Comparison of architectures and their complexity (He et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;results&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;They evaluate how well the residual networks generalize when adding more layers.
As mentioned in the introduction, typical models would see an increase in training error as the number of layers were increased.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-53-18_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 14: &amp;lt;/span&amp;gt;Training comparisons between plain and residual networks (He et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 14: &lt;/span&gt;Training comparisons between plain and residual networks (He et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Their ensemble of models achieved 3.57% top-5 error on the ImageNet test set, achieving 1st place in the ILSVRC 2015 classification challenge.
It additionally was adapted to other challenges and won first place on ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in both the ILSVRC and COCO 2015 competitions.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “Deep Residual Learning for Image Recognition.” In &lt;i&gt;2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/i&gt;, 770–78. Las Vegas, NV, USA: IEEE. &lt;a href=&#34;https://doi.org/10.1109/CVPR.2016.90&#34;&gt;https://doi.org/10.1109/CVPR.2016.90&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. “ImageNet Classification with Deep Convolutional Neural Networks.” &lt;i&gt;Communications of the Acm&lt;/i&gt; 60 (6): 84–90. &lt;a href=&#34;https://doi.org/10.1145/3065386&#34;&gt;https://doi.org/10.1145/3065386&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_3&#34;&gt;&lt;/a&gt;LeCun, Yann, Bernhard Boser, John Denker, Donnie Henderson, R. Howard, Wayne Hubbard, and Lawrence Jackel. 1989. “Handwritten Digit Recognition with a Back-Propagation Network.” In &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;. Vol. 2. Morgan-Kaufmann.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_4&#34;&gt;&lt;/a&gt;Simonyan, Karen, and Andrew Zisserman. 2015. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” &lt;i&gt;Arxiv:1409.1556 [Cs]&lt;/i&gt;, April. &lt;a href=&#34;https://arxiv.org/abs/1409.1556&#34;&gt;https://arxiv.org/abs/1409.1556&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_5&#34;&gt;&lt;/a&gt;Springenberg, Jost Tobias, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. 2015. “Striving for Simplicity: The All Convolutional Net.” &lt;i&gt;Arxiv:1412.6806 [Cs]&lt;/i&gt;, April. &lt;a href=&#34;https://arxiv.org/abs/1412.6806&#34;&gt;https://arxiv.org/abs/1412.6806&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning</title>
      <link>https://ajdillhoff.github.io/notes/deep_learning/</link>
      <pubDate>Tue, 29 Mar 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/deep_learning/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#syllabi&#34;&gt;Syllabi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#questions&#34;&gt;Questions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-makes-a-model-deep&#34;&gt;What makes a model deep?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deep-networks&#34;&gt;Deep Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#activation-functions&#34;&gt;Activation Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#loss-functions&#34;&gt;Loss Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-typical-training-pipeline&#34;&gt;A Typical Training Pipeline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bias-variance-tradeoff&#34;&gt;Bias/Variance Tradeoff&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regularization&#34;&gt;Regularization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimization-algorithms&#34;&gt;Optimization Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;syllabi&#34;&gt;Syllabi&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cs230.stanford.edu/syllabus/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://cs230.stanford.edu/syllabus/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;questions&#34;&gt;Questions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Does backprop give the same result when combining the weights and bias versus separating?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;What is deep learning?&lt;/li&gt;
&lt;li&gt;What problems is it used for?&lt;/li&gt;
&lt;li&gt;Basic neural nets vs &amp;ldquo;deep&amp;rdquo; nets&lt;/li&gt;
&lt;li&gt;Structuring a project&lt;/li&gt;
&lt;li&gt;CNNs&lt;/li&gt;
&lt;li&gt;Sequence models&lt;/li&gt;
&lt;li&gt;Geometric Deep Learning&lt;/li&gt;
&lt;li&gt;Representation of a model in code (vectorization)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Deep learning has been employed in many fields from visual recognition to medical diagnosis.
Its continued success is impossible to ignore.
As more and more companies pivot to utilize deep learning in some capacity, it is paramount that any machine learning practitioner or research be well versed in moden practices and frameworks.
This section will provide a broad overview of deep learning with examples using &lt;a href=&#34;https://pytorch.org&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;PyTorch&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are not yet familiar with &lt;a href=&#34;https://ajdillhoff.github.io/notes/neural_networks/&#34;&gt;neural networks&lt;/a&gt;, follow the link to learn about their basics as they are the foundation of deep learning systems.&lt;/p&gt;
&lt;p&gt;We will cover how to implement an array of deep learning models for different tasks.
Different layers and activation functions will be explored as well as the effect of regularization.
There will also be a focus on best practices for organizing a machine learning project.&lt;/p&gt;
&lt;h2 id=&#34;what-makes-a-model-deep&#34;&gt;What makes a model deep?&lt;/h2&gt;
&lt;p&gt;We begin by comparing &lt;em&gt;shallow&lt;/em&gt; networks with &lt;em&gt;deep&lt;/em&gt; networks.
What defines a deep network? Is it as simple as crossing a threshold into \(n\) layers?
As evidenced by &amp;lt;&amp;amp;zeilerVisualizingUnderstandingConvolutional2013&amp;gt;, deeper networks allow for a more robust hierarchy of image features.&lt;/p&gt;
&lt;p&gt;There is work by &amp;lt;&amp;amp;montufarNumberLinearRegions2014&amp;gt; which suggests that shallow networks require an exponential amount of nodes as compared to deeper networks.
Additionally, there are many individual results which suggest that deeper networks provide better task generalization.&lt;/p&gt;
&lt;p&gt;As we will later see when studying Convolutional Neural Networks, the optimization of such deep networks produces features that maximize the performance of a task.
That is, our network is not only optimizing the performance of task, but it produces features from the data.
This is particularly useful for transfer learning, where large pre-trained models can be used as starting points for novel tasks.
The benefit being that a complete retraining of the model is not necessary.&lt;/p&gt;
&lt;h2 id=&#34;deep-networks&#34;&gt;Deep Networks&lt;/h2&gt;
&lt;p&gt;Like &lt;a href=&#34;https://ajdillhoff.github.io/notes/neural_networks/&#34;&gt;neural networks&lt;/a&gt;, deep networks are defined by the number of layers, nodes per layer, activation functions, and loss functions.
We now review the forward and backward pass, providing more insight into the structure and usage of deep networks along the way.&lt;/p&gt;
&lt;p&gt;Consider a deep network with \(L\) layers. Layer \(l\) has \(n_{l-1}\) input connections and \(n_l\) output nodes and activation function \(g^{(l)}\).
The final output is evaluated with some ground truth using a loss function \(\mathcal{L}\).&lt;/p&gt;
&lt;h3 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h3&gt;
&lt;p&gt;\begin{align*}
\mathbf{a}^{(l)} &amp;amp;= W^{(l)}\mathbf{z}^{(l-1)} + \mathbf{b}^{(l)}\\
\mathbf{z}^{(l)} &amp;amp;= g^{(l)}(\mathbf{a}^{(l)})\\
\end{align*}&lt;/p&gt;
&lt;p&gt;This is repeated from the input to the last layer.
For the first layer \(l=1\), the input \(\mathbf{z}^{(0)} = \mathbf{x}\).
In practice, the output \(\mathbf{a}^{(l)}\) is cached since it is required for the backward pass.
This prevents the values from needing to be computed twice.&lt;/p&gt;
&lt;p&gt;It is also worth it to study the sizes of the matrices while performing a forward pass.
For a layer \(l\), \(W^{(l)} \in \mathbb{R}^{n_l \times n_{l-1}}\) and the input \(\mathbf{z}^{(l-1)} \in \mathbb{R}^{n_{l-1} \times 1}\).
When training, it is common to perform batch gradient descent with batches of input of size \(B\).
Then, \(\mathbf{z}^{(l-1)} \in \mathbb{R}^{n_{l-1}\times B}\) and \(\mathbf{a}^{(l)}, \mathbf{b}^{(l)} \in \mathbb{R}^{n_l \times B}\).&lt;/p&gt;
&lt;h3 id=&#34;backward-pass&#34;&gt;Backward Pass&lt;/h3&gt;
&lt;p&gt;During the backward pass, the gradient is propagated from the last layer to the first.
Each layer that contains trainable parameters must also compute the gradient of the network output with respect to the weights and biases.
This can be done in a modular way, as shown next.&lt;/p&gt;
&lt;p&gt;Consider the last layer. The gradients with respect to the weights and biases are&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{d\mathcal{L}}{dW^{(L)}} &amp;amp;= \frac{d\mathcal{L}}{d\mathbf{z}^{(L)}} \frac{d\mathbf{z}^{(L)}}{d\mathbf{a}^{(L)}} \frac{d\mathbf{a}^{(L)}}{dW^{(L)}}\\
\frac{d\mathcal{L}}{d\mathbf{b}^{(L)}} &amp;amp;= \frac{d\mathcal{L}}{d\mathbf{z}^{(L)}} \frac{d\mathbf{z}^{(L)}}{d\mathbf{a}^{(L)}} \frac{d\mathbf{a}^{(L)}}{d\mathbf{b}^{(L)}}.
\end{align*}&lt;/p&gt;
&lt;p&gt;To see how the gradient continues to be propagated backward, compute the same thing for layer \(L-1\)&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{d\mathcal{L}}{dW^{(L-1)}} &amp;amp;= \frac{d\mathcal{L}}{d\mathbf{z}^{(L)}} \frac{d\mathbf{z}^{(L)}}{d\mathbf{a}^{(L)}} \frac{d\mathbf{a}^{(L)}}{d\mathbf{z}^{(L-1)}} \frac{d\mathbf{z}^{(L-1)}}{d\mathbf{a}^{(L-1)}} \frac{d\mathbf{a}^{(L-1)}}{dW^{(L-1)}}\\
\frac{d\mathcal{L}}{d\mathbf{b}^{(L-1)}} &amp;amp;= \frac{d\mathcal{L}}{d\mathbf{z}^{(L)}} \frac{d\mathbf{z}^{(L)}}{d\mathbf{a}^{(L)}} \frac{d\mathbf{a}^{(L)}}{d\mathbf{z}^{(L-1)}} \frac{d\mathbf{z}^{(L-1)}}{d\mathbf{a}^{(L-1)}} \frac{d\mathbf{a}^{(L-1)}}{d\mathbf{b}^{(L-1)}}.
\end{align*}&lt;/p&gt;
&lt;p&gt;As seen above, to continue propagating the gradient backward, each layer \(l\) must also compute&lt;/p&gt;
&lt;p&gt;\[
\frac{d\mathbf{a}^{(l)}}{d\mathbf{z}^{(l-1)}}.
\]&lt;/p&gt;
&lt;p&gt;To summarize, every layer with trainable parameters will compute&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{d\mathcal{L}}{dW^{(l)}} = \frac{d\mathbf{a}^{(l+1)}}{d\mathbf{z}^{(l)}} \frac{d\mathbf{z}^{(l)}}{d\mathbf{a}^{(l)}} \frac{d\mathbf{a}^{(l)}}{dW^{(l)}}\\
\frac{d\mathcal{L}}{d\mathbf{b}^{(l)}} = \frac{d\mathbf{a}^{(l+1)}}{d\mathbf{z}^{(l)}} \frac{d\mathbf{z}^{(l)}}{d\mathbf{a}^{(l)}} \frac{d\mathbf{a}^{(l)}}{d\mathbf{b}^{(l)}}.
\end{align*}&lt;/p&gt;
&lt;p&gt;The term \(\frac{d\mathbf{a}^{(l+1)}}{d\mathbf{z}^{(l)}}\) is the gradient that is propagated from layer \(l+1\).&lt;/p&gt;
&lt;h2 id=&#34;activation-functions&#34;&gt;Activation Functions&lt;/h2&gt;
&lt;h3 id=&#34;sigmoid&#34;&gt;Sigmoid&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\[
\sigma(\mathbf{x}) = \frac{1}{1 + \exp(-\mathbf{x})}
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Derivative&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\[
\sigma(\mathbf{x})(1 - \sigma(\mathbf{x}))
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-31_10-04-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Sigmoid non-linearity (Wikipedia)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Sigmoid non-linearity (Wikipedia)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;loss-functions&#34;&gt;Loss Functions&lt;/h2&gt;
&lt;h2 id=&#34;a-typical-training-pipeline&#34;&gt;A Typical Training Pipeline&lt;/h2&gt;
&lt;p&gt;When training and evaluating models, especially on benchmark datasets, it is important to properly test their generalization performance.
This test is crucial when comparing the efficacy of your ideas versus baseline evaluations or competing methods.&lt;/p&gt;
&lt;p&gt;To ensure that your model is evaluated in a fair way, it is common to set aside a set of test data that is only used during the final comparison.
This data is typically annotated so that some metric can be used.&lt;/p&gt;
&lt;p&gt;It is true that the training data drives the parameter tuning during optimization.
This is most commonly done with gradient descent.
However, we will also change the hyperparamers such as learning rate, batch size, and data augmentation.
In this case, we want to evaluate the relative performance of each change.&lt;/p&gt;
&lt;p&gt;If we use the test set to do this, then we are necessarily using the test set for training.
Our biases and intuitions about the model&amp;rsquo;s performance would be implicitly influenced by that set.
To track our relative changes without using the test set, we can take a portion of the original training set and label it as our &lt;strong&gt;validation set&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The split between training, validation, and test data is relatively small.
Most modern datasets are large, with millions of samples.
Consider &lt;a href=&#34;https://www.image-net.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;ImageNet&lt;/a&gt;, an image classification dataset with over 14 million samples.
Taking 10,000 samples to serve as a validation set is only \(~.07\%\) of the dataset.&lt;/p&gt;
&lt;p&gt;Most modern machine learning frameworks have an easy way to split the dataset.
We can do this in PyTorch using &lt;code&gt;torch.utils.data.random_split&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;train_dataset, val_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;utils&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random_split(dataset, [train_size, val_size])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;bias-variance-tradeoff&#34;&gt;Bias/Variance Tradeoff&lt;/h2&gt;
&lt;p&gt;Ideally, we want a model that generalizes well to unseen data.
Being able to evaluate how well our model performs is, in many ways, more important than the model itself.
A model that shows good generalization performance will have low bias and low variance.
Bonus points are awarded if the model remains simple.&lt;/p&gt;
&lt;p&gt;During training, we can detect a model that is overfitting the data by also monitoring its performance on a separate validation set.
If the validation loss diverges from the training loss, the model is beginning to overfit.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-31_22-38-54_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Higher complexity on the dataset leads to higher variance and lower bias.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Higher complexity on the dataset leads to higher variance and lower bias.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;regularization&#34;&gt;Regularization&lt;/h2&gt;
&lt;h2 id=&#34;optimization-algorithms&#34;&gt;Optimization Algorithms&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Boosting</title>
      <link>https://ajdillhoff.github.io/notes/boosting/</link>
      <pubDate>Wed, 23 Mar 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/boosting/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#todo&#34;&gt;TODO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adaboost&#34;&gt;AdaBoost&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;todo&#34;&gt;TODO&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Gradient Boosting&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Combining predictions from multiple sources is usually preferred to a single source.
For example, a medical diagnosis would carry much more weight if it was the result of a consensus of several experts.
This idea of prediction by consensus is a powerful way to improve classification and regression models.
In fact, good performance of a committee of models can be achieved even if each individual model is conceptually very simple.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Boosting&lt;/strong&gt; is one such way of building a committee of models for classification or regression and is popularly implemented by an algorithm called &lt;strong&gt;AdaBoost&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;adaboost&#34;&gt;AdaBoost&lt;/h2&gt;
&lt;p&gt;Given a dataset \(\{\mathbf{x}_i\}\) and target variables \(\{\mathbf{y}_i\}\), AdaBoost first initializes a set of weights corresponding to each data sample as \(w_i = \frac{1}{N}\).
At each step of the algorithm, a simple classifier, called a &lt;strong&gt;weak learner&lt;/strong&gt; is fit to the data.
The weights for each sample are adjusted based on the individual classifier&amp;rsquo;s performance.
If the sample was misclassified, the relative weight for that sample is increased.
After all classifiers have been fit, they are combined to form an ensemble model.&lt;/p&gt;
&lt;h3 id=&#34;the-algorithm&#34;&gt;The Algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Initialize data weights \({w_i}\) as \(w_i^{(1)} = \frac{1}{n}\) for \(i = 1, \dots, n\).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fit each weak learner \(j\) to the training data by minimizing the misclassification cost:&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^n w_i^{(j)} \mathbb{1}(f_j(\mathbf{x}_i) \neq \mathbf{y}_i)
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute a weighted error rate&lt;/p&gt;
&lt;p&gt;\[
\epsilon_j = \frac{\sum_{i=1}^n w_i^{(j)} \mathbb{1}(f_j(\mathbf{x}_i) \neq \mathbf{y}_i)}{\sum_{i=1}^n w_i^{(j)}}
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the weighted error rate to compute a weight for each classifier such that misclassified samples are given higher weight:&lt;/p&gt;
&lt;p&gt;\[
\alpha_j = \ln \bigg\{\frac{1 - \epsilon_j}{\epsilon_j}\bigg\}.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update the data weights for the next model in the sequence:&lt;/p&gt;
&lt;p&gt;\[
w_i^{j+1} = w_i^{j} \exp\{\alpha_j \mathbb{1}(f_j(\mathbf{x}_i \neq \mathbf{y}_i)\}.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once all weak learners are trained, the final model predictions are given by&lt;/p&gt;
&lt;p&gt;\[
Y_M(\mathbf{x}) = \text{sign} \Bigg(\sum_{j=1}^M \alpha_j f_j(\mathbf{x})\Bigg).
\]&lt;/p&gt;
&lt;h3 id=&#34;weak-learners&#34;&gt;Weak Learners&lt;/h3&gt;
&lt;p&gt;The weak learners can be any classification or regression model.
However, they are typically chosen to be very simple to account for training time.
For example, a complex deep learning model would be a poor choice for a weak learner.&lt;/p&gt;
&lt;p&gt;One example of a weak learner is a simple linear model like a &lt;a href=&#34;https://ajdillhoff.github.io/notes/perceptron/&#34;&gt;Perceptron&lt;/a&gt; or decision stump.
A standard implementation of AdaBoost uses a decision tree with depth 1, as observed in &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html?highlight=boost#sklearn.ensemble.AdaBoostClassifier&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;sklearn&amp;rsquo;s implementation.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s put this together and walk through the first few steps of training an AdaBoost model using a decision stump as the weak learner. We will use a very simple dataset to keep the values easy to compute by hand.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Initial Data&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Weak Learner 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first learner is trained on the initial data and picks \(x_1 = 2.5\) as the split threshold.
Input where \(x_1 \leq 2.5\) is assigned to class 0 and all other samples are assigned class 1.
The data with this learner&amp;rsquo;s predictions are shown below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;th&gt;prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Error and weight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The error is simple enough to compute as all samples are currently weighted equally. Since two of the samples were misclassified, the error is the sum of their weights.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Total error&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(e_1 = 0.2 + 0.2 = 0.4\).&lt;/p&gt;
&lt;p&gt;The weight of the classifier can then be computed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classifier weight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(\alpha_1 = \frac{1}{2} \ln \big(\frac{1 - e_1}{e_1}\big) = 0.2027\).&lt;/p&gt;
&lt;p&gt;The weights of our data can now be updated using this value of \(\alpha_1\).
The weight of each example is updated by multiplying each correctly classifed sample by \(\exp\{-\alpha_1\}\) and each incorrectly classified sample by \(\exp\{\alpha\}\):&lt;/p&gt;
&lt;p&gt;\[
w_i^{j+1} = w_i^{j} \exp\{\alpha_j \mathbb{1}(f_j(\mathbf{x}_i \neq \mathbf{y}_i)\}.
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; You will notice that the equation above is different from the actual update rule that was applied to the weights in this example. In the original publication &lt;strong&gt;(TODO: reference Fruend)&lt;/strong&gt;, the weights are renormalized at the end of the loop. In this example, the normalization is combined with the update. In either case, the updated weights are shown below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Weak Learner 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The algorithm now moves to the next weak learner, which classifies the data given a threshold of \(x_1 = 3.5\). Its predictions are shown below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;th&gt;prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.250&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.250&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Only a single sample is misclassified, and the error is computed as before.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Total error&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(e_2 = 0.250\)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classifier weight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(\alpha_2 = \frac{1}{2} \ln \big(\frac{1 - e_2}{e_2}\big) = 0.5493\)&lt;/p&gt;
&lt;p&gt;The weights are updated for each sample, yielding the following data:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.500&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The second sample has been misclassified twice at this point, leading to a relatively high weight. This will hopefully be addressed by the third learner.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Weak Learner 3&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The final weak learner splits the data on \(x_2 = 6.5\), yielding the following output for each sample.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;th&gt;prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.500&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Unfortunately, sample 2 is too tricky for any of our weak learners. The total error is shown below. Since this is a binary classification problem, the error suggests that our weak learner performs worse than random guessing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Total error&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(e_3 = 0.667\)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classifier weight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(\alpha_3 = \frac{1}{2} \ln \big(\frac{1 - e_3}{e_3}\big) = -0.3473\)&lt;/p&gt;
&lt;p&gt;The negative value of the classifier weight suggests that its predictions will be reversed when evaluated. The updated weights of each data sample are given below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.375&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.125&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Final Classifier&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The final classifier is a weighted vote of the three weak learners, with the weights being the classifier weights we calculated (0.2027, 0.5493, and -0.3473). The negative weight means that the third learner&amp;rsquo;s predictions are reversed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Decision Trees</title>
      <link>https://ajdillhoff.github.io/notes/decision_trees/</link>
      <pubDate>Fri, 18 Mar 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/decision_trees/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-iris-dataset&#34;&gt;Example: Iris Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#growing-a-tree&#34;&gt;Growing a Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#examining-the-iris-classification-tree&#34;&gt;Examining the Iris Classification Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pruning-a-tree&#34;&gt;Pruning a Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-algorithm&#34;&gt;The Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/dmilla/introduction-to-decision-trees-titanic-dataset&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.kaggle.com/dmilla/introduction-to-decision-trees-titanic-dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;decision tree&lt;/strong&gt;, or Classification and Regression Trees (CART), is a model that recursively partitions the input space based on a collection of features.
The partitions are split based on very simple binary choices.
If yes, branch to the left; if no, branch to the right.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;Introduction/2022-03-18_13-03-12_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Regression tree (left) and its piecewise constant surface (right) (Source: _Machine Learning: A Probabilistic Perspective_ by Kevin P. Murphy).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Regression tree (left) and its piecewise constant surface (right) (Source: &lt;em&gt;Machine Learning: A Probabilistic Perspective&lt;/em&gt; by Kevin P. Murphy).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To compute the response, we represent each individual decision as a function \(\phi\) and sum the responses:&lt;/p&gt;
&lt;p&gt;\[
f(\mathbf{x}) = \mathbb{E}[y | \mathbf{x}] = \sum_{m=1}^M w_m \mathbb{1} (\mathbf{x} \in R_m) = \sum_{m=1}^M w_m \phi(\mathbf{x};\mathbf{v}_m),
\]&lt;/p&gt;
&lt;p&gt;where \(R_m\) is the \(m^{\text{th}}\) region, \(w_m\) is the mean response, and \(\mathbf{v}_m\) is the choice of variable to split on along with its threshold value.&lt;/p&gt;
&lt;h2 id=&#34;example-iris-dataset&#34;&gt;Example: Iris Dataset&lt;/h2&gt;
&lt;p&gt;To see this on real data, consider the Iris flower dataset.
For example, we will look at a decision tree model that classifies each flower into either &lt;strong&gt;setosa&lt;/strong&gt;, &lt;strong&gt;versicolor&lt;/strong&gt;, or &lt;strong&gt;virginica&lt;/strong&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-20_21-04-19_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Our initial Iris classifier.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Our initial Iris classifier.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;We are given data about a new iris and want to classify it using this tree.
Our sample has a sepal length of 6.1cm, a sepal width of 2.8cm, a petal length of 4.7cm, and a petal width of 1.2cm.
The first decision considers the petal width.
Our sample has a width of 1.2, so it continues down the right branch.&lt;/p&gt;
&lt;p&gt;The second decision consideres petal width again.
Since our sample does not have a width greater than 1.75, we continue down the left branch.&lt;/p&gt;
&lt;p&gt;At this point, the model was optimized to now consider the petal length.
Our length comes in at 4.7, just shy of going down the right path.&lt;/p&gt;
&lt;p&gt;We now arrive at the last decision.
Since our petal length is not greater than 1.65, the model classifies this sample as &lt;strong&gt;versicolor&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;growing-a-tree&#34;&gt;Growing a Tree&lt;/h2&gt;
&lt;p&gt;To grow a tree, a decision needs to be made as to whether or not the current set of data can be split based on some feature.
As such, there should be a reliable way of determining if a feature provided a good split.
This is evaluated using a cost function and selecting the feature and value corresponding to the minimum cost:&lt;/p&gt;
&lt;p&gt;\[
(j^*, t^*) = \text{arg} \min_{j\in\{1, \dots, D\}} \min_{t \in \mathcal{T}_j} \text{cost}(\{\mathbf{x}_i, y_i : x_{ij} \leq t\}) + \text{cost}(\{\mathbf{x}_i, y_i : x_{ij} &amp;gt; t\}).
\]&lt;/p&gt;
&lt;p&gt;In words, this function finds a value \(t\) such that groups the data with the lowest cost.
For a regression task, the cost function is typically defined as&lt;/p&gt;
&lt;p&gt;\[
\text{cost}(\mathcal{D}) = \sum_{i \in \mathcal{D}}(y_i - \bar{y})^2,
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\bar{y} = \frac{1}{|\mathcal{D}|}\sum_{i \in \mathcal{D}} y_i.
\]&lt;/p&gt;
&lt;p&gt;Splits that result in clusters with high variance may still see a higher cost, even though they are the minimum.&lt;/p&gt;
&lt;p&gt;As their alternative name implies, decision trees can also be used for classification.
The splits are still based on features and threshold values at each branch.
When a split is considered, a class-conditional probability is estimated for that data.
Given data satisfying \(X_j &amp;lt; t\), the class-conditional probability is&lt;/p&gt;
&lt;p&gt;\[
\hat{\pi}_c = \frac{1}{|\mathcal{D}|}\sum_{i \in \mathcal{D}} \mathbb{1}(y_i = c).
\]&lt;/p&gt;
&lt;p&gt;The common error functions used for classification are &lt;strong&gt;misclassification rate&lt;/strong&gt;, &lt;strong&gt;entropy&lt;/strong&gt;, and &lt;strong&gt;Gini index&lt;/strong&gt;.
Misclassification rate is computed by summing the number of misclassifications:&lt;/p&gt;
&lt;p&gt;\[
\frac{1}{|\mathcal{D}|} \sum_{i \in \mathcal{D}} \mathbb{1}(y_i \neq \hat{y}) = 1 - \hat{\pi}_{\hat{y}}.
\]&lt;/p&gt;
&lt;p&gt;Entropy is computed as&lt;/p&gt;
&lt;p&gt;\[
\mathbb{H}(\mathbb{\hat{\pi}}) = -\sum_{c=1}^C \hat{\pi}_c \log \hat{\pi}_c.
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gini index&lt;/strong&gt; computes the expected error rate.&lt;/p&gt;
&lt;p&gt;\[
G = \sum_{c=1}^C \hat{\pi}_c (1 - \hat{\pi}_c) = 1 - \sum_{c=1}^C \hat{\pi}_c^2
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-19_17-32-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Impurity measured for binary classification (Source: _Machine Learning: A Probabilistic Perspective_ by Kevin P. Murphy)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Impurity measured for binary classification (Source: &lt;em&gt;Machine Learning: A Probabilistic Perspective&lt;/em&gt; by Kevin P. Murphy)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Like entropy, it promotes an equal number of observations across all classes in a node.
For small values of \(\hat{\pi}\), the error is smaller than that of entropy.
If the dataset is imbalanced, entropy is typically favored as it penalizes imbalanced datasets more than Gini will.
Both will favor splits that result in one node being pure.&lt;/p&gt;
&lt;h3 id=&#34;stopping-growth&#34;&gt;Stopping Growth&lt;/h3&gt;
&lt;p&gt;If left unchecked, the algorithm to grow a tree will continue until the data can no longer be split.
In the trivial case, this will be when every data point represents a leaf node.
In order to prevent overfitting, there are several criteria that are considered.&lt;/p&gt;
&lt;h4 id=&#34;does-the-split-reduce-the-cost-enough&#34;&gt;Does the split reduce the cost enough?&lt;/h4&gt;
&lt;p&gt;It may be ideal to only split the data if the cost is reduced by some acceptable value.
The reduction can be computed by&lt;/p&gt;
&lt;p&gt;\[
\Delta = \text{cost}(\mathcal{D}) - \bigg(\frac{|\mathcal{D}_L|}{|\mathcal{D}|}\text{cost}(\mathcal{D}_L) + \frac{|\mathcal{D}_R|}{|\mathcal{D}|} \text{cost}(\mathcal{D}_R)\bigg).
\]&lt;/p&gt;
&lt;h4 id=&#34;has-the-tree-reached-some-maximum-depth&#34;&gt;Has the tree reached some maximum depth?&lt;/h4&gt;
&lt;p&gt;The depth of a tree is set as a hyperparameter.
Later, when we look at an example, we will use cross validation to select the best depth parameter for our model.&lt;/p&gt;
&lt;h4 id=&#34;is-the-distribution-of-the-split-pure&#34;&gt;Is the distribution of the split &lt;strong&gt;pure&lt;/strong&gt;?&lt;/h4&gt;
&lt;p&gt;If either of the splits is fully made up of data with the same label, there is no need to split it any further.&lt;/p&gt;
&lt;h4 id=&#34;is-the-split-too-small&#34;&gt;Is the split too small?&lt;/h4&gt;
&lt;p&gt;A split that is too small may lead to overfitting.&lt;/p&gt;
&lt;h2 id=&#34;examining-the-iris-classification-tree&#34;&gt;Examining the Iris Classification Tree&lt;/h2&gt;
&lt;p&gt;How exactly does the earlier example model make its decision at each node?
The full tree is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-20_21-11-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;A detailed view of our Iris classifier.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;A detailed view of our Iris classifier.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The first split is visually very simple to intuit.
Using petal width can perfectly split all setosa samples into a single leaf node.&lt;/p&gt;
&lt;h2 id=&#34;pruning-a-tree&#34;&gt;Pruning a Tree&lt;/h2&gt;
&lt;p&gt;Depending on the data, stopping growth based on measuring the relative decrease in error may not result in a model that performs well.
Image a dataset that requires multiple features to provide a sufficient classification.
If only one of the features is considered in isolation, it may provide no decrease in error.
A practical example of this is the XOR problem.
Splitting on \(x_1\) or \(x_2\) in isolation does not provide any indication about the true output.
It is only when \(x_1 \neq x_2\) does the output equal to 1.&lt;/p&gt;
&lt;p&gt;To rectify this, a tree can be grown until it is completely full before &lt;strong&gt;pruning&lt;/strong&gt; the branches
that result in the smallest increase in error.&lt;/p&gt;
&lt;h2 id=&#34;the-algorithm&#34;&gt;The Algorithm&lt;/h2&gt;
&lt;p&gt;The general algorithm is shown in MATLAB below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;% node = fitTree(node, D, depth)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;% Recursive function to learn a decision tree&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;% Returns the index of the current node.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;%&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;%   node  - The node index in obj.Nodes.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;%   D     - Indices to the current data.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;%   depth - Current depth of the tree.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; node = &lt;span style=&#34;color:#a6e22e&#34;&gt;fitTree&lt;/span&gt;(obj, node, D, depth)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;% Determine best split for the data and return the split&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    [j, t, dSplit, classDist] = obj.split(D, obj.Nodes(node).features);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    obj.Nodes(node).prediction = classDist;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    disp(classDist);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;% Use heuristic to determine if node is worth splitting&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; obj.splitNode(depth, classDist) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; true
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;% set the node test, the function that determines the branch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        obj.Nodes(node .test = {j, t};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        newFeatures = obj.Nodes(node).features(obj.Nodes(node).features &lt;span style=&#34;color:#f92672&#34;&gt;~=&lt;/span&gt; j);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;% set the child nodes to the left and right splits&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        obj.Nodes(node).children = zeros(size(dSplit, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        numNewNodes = size(dSplit, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i = &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; : numNewNodes
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            obj.Nodes(&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) = struct(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;prediction&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;test&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#75715e&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;features&amp;#39;&lt;/span&gt;, newFeatures, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;children&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;parent&amp;#39;&lt;/span&gt;, node);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            obj.Nodes(node).children(i)  = obj.fitTree(length(obj.Nodes), dSplit{i}, depth &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Hidden Markov Models</title>
      <link>https://ajdillhoff.github.io/notes/hidden_markov_models/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/hidden_markov_models/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-markov-assumption&#34;&gt;The Markov Assumption&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-viterbi-algorithm&#34;&gt;The Viterbi Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#estimating-parameters&#34;&gt;Estimating Parameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#expectation-maximization&#34;&gt;Expectation Maximization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This article is essentially a grok of a tutorial on HMMs by (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;RABINER 1989&lt;/a&gt;). It will be useful for the reader to reference the &lt;a href=&#34;https://courses.physics.illinois.edu/ece417/fa2017/rabiner89.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;original paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Up to this point, we have only explored &amp;ldquo;atomic&amp;rdquo; data points.
That is, all of the information about a particular sample is encapsulated into one vector.
Sequential data is easily represented by graphical models.
This article introduces Hidden Markov Models, a powerful probabilistic graphical model used in many applications from gesture recognition to natural language processing.&lt;/p&gt;
&lt;p&gt;There are many tasks for which we do not know the underlying process.
However, we can observe samples that are produced from such processes.
Music, gesture recognition, speech, text, etc.
All of these have some underlying process which forms their outputs together into a hopefully coherent sequence.
If we wish to make predictions about future samples given these sequences, we will need to make some guess
about the underlying processes defining their output.&lt;/p&gt;
&lt;h2 id=&#34;the-markov-assumption&#34;&gt;The Markov Assumption&lt;/h2&gt;
&lt;p&gt;Markov models make a convenient assumption about sequential data.
That is, all relevant information required for predicting future samples is captured in the current time step \(t\).
Given a joint distribution over an input of \(T\) frames, \(p(\mathbf{x}_{1:T})\), the Markov assumption allows us to represent it as&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{x}_{1:T}) = p(\mathbf{x}_1)\prod_{t=2}^T p(\mathbf{x}_t|\mathbf{x}_{t-1})
\]&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;A more complicated case is when we are attempting to model some unknown process that is responsible for the observations.
In this case, an ordinary Markov chain is not sufficient.
A &lt;strong&gt;hidden Markov model (HMM)&lt;/strong&gt; is defined by a set \(z_t \in \{1, \dots, K\}\) of discrete hidden states and an &lt;strong&gt;observation&lt;/strong&gt; model \(p(\mathbf{x}_i|z_t)\).
The joint probability distribution of this model is given by&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{z}, \mathbf{x}) = p(\mathbf{z})p(\mathbf{x}|\mathbf{z}) = \Big(p(z_1)\prod_{t=2}^Tp(z_t|z_{t-1})\Big)\Big(\prod_{t=1}^Tp(\mathbf{x}_t|z_t)\Big).
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-24_20-41-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;The observations y are generated by the latent states x. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;The observations y are generated by the latent states x. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Although the states themselves are discrete, the observations may be continuous: \(p(\mathbf{x}|z_t, \mathbf{\theta})\).
If they are discrete, they can be modeled by an observation matrix \(B\).
Continuous observations are typically modeled using a conditional Gaussian:&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{x}_t|z_t=k, \theta) = \mathcal{N}(\mathbf{x}_t|\mathbf{\mu}_k,\mathbf{\Sigma}_k).
\]&lt;/p&gt;
&lt;p&gt;Following &lt;a href=&#34;https://courses.physics.illinois.edu/ece417/fa2017/rabiner89.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Rabiner&lt;/a&gt;, an HMM can be characterized by&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The number of states in the model \(N\).&lt;/li&gt;
&lt;li&gt;The number of distinct observation symbols per state \(M\).&lt;/li&gt;
&lt;li&gt;The state probability distribution \(A = \{a_{ij}\}\), \(a_{ij} = p(z_t=j | z_{t-1} = i)\).&lt;/li&gt;
&lt;li&gt;The observation symbol probability distribution \(B = \{b_j(k)\} = p(\mathbf{x}_t = k|z_t = j)\).&lt;/li&gt;
&lt;li&gt;An initial state distribution \(\mathbf{\pi}_i = p(z_t = i)\).&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-24_20-42-34_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;HMM with observation probabilities and state transition probabilities. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;HMM with observation probabilities and state transition probabilities. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The observation probability distribution is commonly modeled as a Gaussian, Mixture of Gaussians, or Multinomial distribution. Thus, the parameter estimates for those distributions follow the likelihood estimates for each respective distribution.&lt;/p&gt;
&lt;p&gt;In his famous tutorial on HMMs, Rabiner addressed the three fundamental problems of HMMs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Given an observation sequence and model parameters, how do we compute the probability of the observation sequence given the parameters (likelihood)?&lt;/li&gt;
&lt;li&gt;Given an observation sequence and model parameters, how do we choose a state sequence which is optimal (decoding)?&lt;/li&gt;
&lt;li&gt;How do we adjust the model parameters (learning)?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;HMMs are able to solve several different inference problems.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Filtering&lt;/strong&gt; computes \(p(z_t | \mathbf{x}_{1:t})\). That is, we are computing this probability as new samples come in up to time \(t\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smoothing&lt;/strong&gt; is accomplished when we have all the data in the sequence.
This is expressed as \(p(z_t|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fixed lag smoothing&lt;/strong&gt; allows for a trade off between accuracy and delay. It is useful in cases where we might not have the full sequence, but we wish to compute \(p(z_{t-l}|\mathbf{x}_{1:t})\) for some \(l &amp;gt; 0\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Predictions&lt;/strong&gt; are represented as \(p(z_{t+h}|\mathbf{x}_{1:t})\), where \(h &amp;gt; 0\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MAP estimation&lt;/strong&gt; yields the most probably state sequence \(\text{arg}\max_{\mathbf{z}_{1:T}}p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;We can sample the &lt;strong&gt;posterior&lt;/strong&gt; \(p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;We can also compute \(p(\mathbf{x}_{1:T})\) by summing up over all hidden paths. This is useful for classification tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;We start by solving the first problem posited by (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;RABINER 1989&lt;/a&gt;).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Given an observation sequence and model parameters, how do we compute the probability of the observation sequence given the parameters?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That is, given some model parameters \(\lambda = (A, B, \pi)\), compute \(p(z_t|\mathbf{x}_{1:t})\).&lt;/p&gt;
&lt;h3 id=&#34;forwards-pass&#34;&gt;Forwards Pass&lt;/h3&gt;
&lt;p&gt;The forwards algorithm solves two problems of interest.
First, we want to know how well our current parameters explain the observation sequence.
That is, \(p(\mathbf{x}_{1:T}|\lambda)\).&lt;/p&gt;
&lt;p&gt;Second, we want to compute \(p(z_t | \mathbf{x}_{1:t})\).
To compute these in an efficient way, a recursive strategy is adopted.
Let the forward variable \(\alpha_t(i)\) be defined as&lt;/p&gt;
&lt;p&gt;\[
\alpha_t(i) = p(\mathbf{x}_{1:t}, z_t = i | \lambda).
\]&lt;/p&gt;
&lt;p&gt;The forwards algorithm is defined as 3 steps.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Initialization:&lt;/p&gt;
&lt;p&gt;\[
\alpha_1(i) = \pi_i b_i(\mathbf{x}_1),\quad 1 \leq i \leq N.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recursion:&lt;/p&gt;
&lt;p&gt;\[
\alpha_{t+1}(j) = \Big(\sum_{i=1}^N \alpha_t(i)a_{ij}\Big)b_j(\mathbf{x}_{t+1}),\quad 1 \leq t \leq T - 1,\quad 1 \leq j \leq N
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Termination:&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{x}_{1:T})  = \sum_{i=1}^N \alpha_T(i).
\]&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The recursive step is visualized as a lattice structure &lt;a href=&#34;#figure--lattice&#34;&gt;as seen below.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;figure--lattice&#34;&gt;&lt;/a&gt;&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-24_20-12-13_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;From Rabiner 1989.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;From Rabiner 1989.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;With this step, we have a solution for the first problem.
We can now calculate more efficiently the probability of our observations given the current model parameters.
This along with the following backwards pass will be essential for updating our model parameters.&lt;/p&gt;
&lt;p&gt;The forwards algorithm is also used to solve the &lt;strong&gt;filtering&lt;/strong&gt; problem.
To see how, consider \(p(z_t | \mathbf{x}_{1:t-1})\) right before time \(t\).&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(z_t=j|\mathbf{x}_{1:t-1}) = \sum_i p(z_t=j|z_{t-1}=i)p(z_{t-1}=i|\mathbf{x}_{1:t-1})
\end{equation*}&lt;/p&gt;
&lt;p&gt;When we update for time \(t\), we have that&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(z_t=j|\mathbf{x}_{1:t}) &amp;amp;= p(z_t=j|\mathbf{x}_t, \mathbf{x}_{1:t})\\
&amp;amp;=\frac{p(\mathbf{x}_t|z_t=j, \mathbf{x}_{1:t-1})p(z_t=j|\mathbf{x}_{1:t-1})}{p(\mathbf{x}_t|\mathbf{x}_{t-1})}
\end{align*}&lt;/p&gt;
&lt;p&gt;However, \(\mathbf{x}_{1:t-1}\) is conditionally independent given \(z_t\), so it becomes&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(z_t=j|\mathbf{x}_{1:t})=\frac{p(\mathbf{x}_t|z_t=j)p(z_t=j|\mathbf{x}_{1:t-1})}{p(\mathbf{x}_t|\mathbf{x}_{t-1})}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Writing out \(p(z_t=j|\mathbf{x}_{1:t-1})\) fully yields&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(z_t=j|\mathbf{x}_{1:t}) \propto p(\mathbf{x}_t|z_t=j)\sum_i p(z_t=j|z_{t-1}=i)p(z_{t-1}=i|\mathbf{x}_{1:t-1}).
\end{equation*}&lt;/p&gt;
&lt;p&gt;This is the recursion step from above!&lt;/p&gt;
&lt;p&gt;This can also be represented in terms of the \(\alpha\) variables from above. To compute \(p(z_t=i|\mathbf{x}_{1:t})\), we can use the definition of a conditional probability distribution:&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(z_t=i|\mathbf{x}_{1:t}) &amp;amp;= \frac{p(z_t=i, \mathbf{x}_{1:t})}{p(\mathbf{x}_{1:t})}\\
&amp;amp;= \frac{\alpha_t(i)}{\sum_{j=1}^N \alpha_t(j)}
\end{align*}&lt;/p&gt;
&lt;p&gt;Compared to the complexity of the explicit representation, the forwards pass needs only \(N^2T\) calculations.
As pointed out in (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;RABINER 1989&lt;/a&gt;), with 5 hidden states and an observation sequence of length 100, the forwards pass only needs around 3000 computations.
A direct calculation would require \(10^{72}\).&lt;/p&gt;
&lt;h3 id=&#34;backwards-pass&#34;&gt;Backwards Pass&lt;/h3&gt;
&lt;p&gt;When updating the parameters of our model, we will need to consider the entire observation sequence.
The forward pass did not require the entire sequence.
Instead, we can compute the probability of the observation up to some time \(t\).
The backwards pass begins by defining the variable&lt;/p&gt;
&lt;p&gt;\[
\beta_t(i) = p(\mathbf{x}_{t+1:T} | z_t = i).
\]&lt;/p&gt;
&lt;p&gt;We can utilize a recursive process similar to the forwards algorithm with the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Initialization:&lt;/p&gt;
&lt;p&gt;\[
\beta_T(i) = 1,\quad 1 \leq i \leq N
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recursion:&lt;/p&gt;
&lt;p&gt;\[
\beta_t(i) = \sum_{j=1}^N a_{ij}b_j(\mathbf{x}_{t+1})\beta_{t+1}(j),\quad t = T-1,\dots,1,\quad 1 \leq i \leq N.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Termination:&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{x}_{1:T}) = \sum_{j=1}^N \pi_j b_j(x_1) \beta_1(j)
\]&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The complexity of the backwards algorithm is similar to that of the forwards: \(N^2T\).&lt;/p&gt;
&lt;p&gt;With both the forward and backwards passes defined, we can compute the &lt;strong&gt;smoothing&lt;/strong&gt; problem:&lt;/p&gt;
&lt;p&gt;\[
p(z_t=i|\mathbf{x}_{1:T}) = \frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N \alpha_t(j)\beta_t(j)}
\]&lt;/p&gt;
&lt;h2 id=&#34;the-viterbi-algorithm&#34;&gt;The Viterbi Algorithm&lt;/h2&gt;
&lt;p&gt;With problem 1 out of the way, we turn our attention to problem 2.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Given an observation sequence and model parameters, how do we choose a state sequence which is optimal?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
\mathbf{z}^* = \text{arg}\max_{\mathbf{z}_{1:T}}p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})
\]&lt;/p&gt;
&lt;p&gt;With respect to the &lt;a href=&#34;#figure--lattice&#34;&gt;lattice diagram&lt;/a&gt;, this is equivalent to computing the shortest path.
This is accomplished via the &lt;strong&gt;Viterbi&lt;/strong&gt; algorithm, sometimes referred to as the max-sum algorithm.
As with the forwards-backwards algorithm, the Viterbi algorithm takes on a recursive approach.
It starts by defining an intermediate variable&lt;/p&gt;
&lt;p&gt;\[
\gamma_t(i) = p(z_t=i|\mathbf{x}_{1:T}).
\]&lt;/p&gt;
&lt;p&gt;Using the variables defined in the forwards-backwards algorithm, this can be expressed as&lt;/p&gt;
&lt;p&gt;\[
\gamma_t(i) = \frac{\alpha_t(i) \beta_t(i)}{\sum_{i=1}^N \alpha_t(i) \beta_t(i)}.
\]&lt;/p&gt;
&lt;p&gt;This \(\gamma_t(i)\), we can compute the most likely state at time \(t\):&lt;/p&gt;
&lt;p&gt;\[
z_t^* = \text{arg}\max_{1\leq i \leq N} \gamma_t(i), \quad 1 \leq t \leq T.
\]&lt;/p&gt;
&lt;p&gt;One problem with this approach alone is that the most likely state at a particular time \(t\) may not lead us to the most probable sequence of states.
As stated above, we need to maximize \(p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).
In order to tackle this efficiently, Viterbi employs a dynamic programming approach.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Initialization&lt;/p&gt;
&lt;p&gt;Start with the best initial state out of all states given the observation at \(t=1\).
Additionally, we want to record the index of each state through time so that the best path can be retraced.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\delta_1(i) &amp;amp;= \pi_i b_i(\mathbf{x}_1),\quad 1 \leq i \leq N\\
\psi_1(i) &amp;amp;= 0
\end{align*}&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recursion:&lt;/p&gt;
&lt;p&gt;The quantity \(\delta_t(i)\) represents the joint probability of state sequences and observations up to time \(t\) ending with state \(z_t=i\).
Thus, the recursive step is to maximize the probability of the intermediate output for \(t-1\):&lt;/p&gt;
&lt;p&gt;\[
\delta_t(j) = \max_{1 \leq i \leq N} (\delta_{t-1}(i) a_{ij})b_j(\mathbf{x}_t), \quad 2 \leq t \leq T,\quad 1 \leq j \leq N.
\]&lt;/p&gt;
&lt;p&gt;The corresponding index for this step is recorded in the path matrix:&lt;/p&gt;
&lt;p&gt;\[
\psi_t(j) = \text{arg}\max_{1 \leq i \leq N} \delta_{t-1}(i)a_{ij},\quad 2 \leq t \leq T,\quad 1 \leq j \leq N.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Termination&lt;/p&gt;
&lt;p&gt;The last step of the Viterbi algorithm completes the calcuation of the joint probability of state sequences and observations.&lt;/p&gt;
&lt;p&gt;\[
p^* = \max_{1 \leq i \leq N} \delta_T(i)
\]&lt;/p&gt;
&lt;p&gt;\[
\mathbf{z}_T^* = \text{arg}\max_{1 \leq i \leq N} \delta_T(i)
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Path Backtrace&lt;/p&gt;
&lt;p&gt;With the state sequence matrix recorded along the way, we can retrace it to get the most probable sequence:&lt;/p&gt;
&lt;p&gt;\[
z_t^* = \psi_{t+1}(z_{t+1}^*),\quad t = T-1, \cdots, 1.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;estimating-parameters&#34;&gt;Estimating Parameters&lt;/h2&gt;
&lt;p&gt;If the hidden states were fully observable, then updating our model parameters would be as straightforward as computing the maximum likelihood estimates for the model parameters \(\lambda = (A, B, \pi)\).
For \(A\) and \(\pi\), we first tally up the following counts:&lt;/p&gt;
&lt;p&gt;\[
\hat{a}_{ij} = \frac{N_{ij}}{\sum_j N_{ij}},
\]&lt;/p&gt;
&lt;p&gt;the number of times we expect to transition from \(i\) to \(j\) divided by the number of times we transition from \(i\) to any other state.&lt;/p&gt;
&lt;p&gt;For \(\pi\), we have&lt;/p&gt;
&lt;p&gt;\[
\hat{\pi_i} = \frac{N_i}{\sum_i N_i},
\]&lt;/p&gt;
&lt;p&gt;The number of times we expect to start in state \(i\) divided by the number of times we start in any other state.&lt;/p&gt;
&lt;p&gt;Estimating the parameters for \(B\) depends on which distribution we are using for our observation probabilities.
For a multinomial distribution, we would compute the number of times we are in state \(j\) and observe a symbol \(k\) divided by the number of times we are in state \(j\):&lt;/p&gt;
&lt;p&gt;\[
\hat{B}_{jk} = \frac{N_{jk}}{N_k},
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
N_{jk} = \sum_{i=1}^N \sum_{t=1}^T \mathbb{1} (z_{i, t}=j, x_{i, t}=k).
\]&lt;/p&gt;
&lt;p&gt;If the observation probability follows a Gaussian distribution, the MLEs for \(\mu\) and \(\mathbf{\Sigma}\) are&lt;/p&gt;
&lt;p&gt;\[
\hat{\mathbf{\mu}}_k = \frac{\bar{\mathbf{x}}_k}{N_k},\quad \hat{\mathbf{\Sigma}}_k = \frac{(\bar{\mathbf{x}}\bar{\mathbf{x}})_k^T - N_k \hat{\mathbf{\mu}}_k\hat{\mathbf{\mu}}_k^T}{N_k},
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\bar{\mathbf{x}}_k = \sum_{i=1}^N \sum_{t=1}^T \mathbb{1}(z_{i, t}=k)\mathbf{x}_{i, t}
\]&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;\[
(\bar{\mathbf{x}}\bar{\mathbf{x}})_k^T) = \sum_{i=1}^N \sum_{t=1}^T \mathbb{1} (z_{i, t}=k)\mathbf{x}_{i,k}\mathbf{x}_{i,k}^T.
\]&lt;/p&gt;
&lt;h2 id=&#34;expectation-maximization&#34;&gt;Expectation Maximization&lt;/h2&gt;
&lt;p&gt;Of course, HMMs have hidden states which are not fully observable.
Thus, we need to come up with another strategy for updating our parameters based on the observable data.
The intuition behind this approach is as follows.
We first start out by using our current parameters to estimate the missing data, making it complete.
Initially, we may randomize our estimates if we have no good heuristic or guess as to what they should be.&lt;/p&gt;
&lt;p&gt;With the completed data, we can update our current parameters.
In other words, the expected values of the sufficient statistics can be derived now that the data has been filled in.
A new set of parameters is found such that it maximizes the likelihood function with respect to the estimated data.&lt;/p&gt;
&lt;h3 id=&#34;e-step&#34;&gt;E Step&lt;/h3&gt;
&lt;p&gt;Following (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;RABINER 1989&lt;/a&gt;), we start with the joint probability of being in state \(i\) at time \(t\) and state \(j\) at time \(t+1\):&lt;/p&gt;
&lt;p&gt;\[
\xi_t(i, j) = p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T}).
\]&lt;/p&gt;
&lt;p&gt;This can be computed using the forwards-backwards algorithm:&lt;/p&gt;
&lt;p&gt;\[
\xi_t(i, j) = \frac{\alpha_t(i)a_{ij}b_j(\mathbf{x}_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^N \sum_{j=1}^N \alpha_t(i)a_{ij}b_j(\mathbf{x}_{t+1})\beta_{t+1}(j)}.
\]&lt;/p&gt;
&lt;p&gt;This can be related back to \(\gamma_t(i)\) by summing over over \(j\):&lt;/p&gt;
&lt;p&gt;\[
\gamma_t(i) = \sum_{j=1}^N \xi_t(i, j).
\]&lt;/p&gt;
&lt;p&gt;Here, \(\gamma_t(i)\) is the expected number of times we transition from \(z = i\).
Summing over all \(t\) yields the expected transitions from \(z_i\) over all time steps:&lt;/p&gt;
&lt;p&gt;\[
\sum_{t=1}^{T-1} \gamma_t(i).
\]&lt;/p&gt;
&lt;p&gt;Since \(\xi_t(i, j)\) is the expected transition from \(i\) to \(j\) at time \(t\), we can compute the total number of transitions from \(i\) to \(j\) via&lt;/p&gt;
&lt;p&gt;\[
\sum_{t=1}^{T-1} \xi_t(i, j).
\]&lt;/p&gt;
&lt;h3 id=&#34;m-step&#34;&gt;M Step&lt;/h3&gt;
&lt;p&gt;The previous &lt;strong&gt;E Step&lt;/strong&gt; computed the expected values given the current parameter estimates.
Now that the data is complete, we can update our parameter estimates.
Starting with the transition probabilities, we must add the expected number of transitions from \(i\) to \(j\) and divide by the expected number of times we transition from \(i\).
Using the parameters from the E Step, this can be written&lt;/p&gt;
&lt;p&gt;\[
\hat{a}_{ij} = \frac{\sum_{t=1}^{T-1}\xi_t(i, j)}{\sum_{t=1}^{T-1}\gamma_t(i)}.
\]&lt;/p&gt;
&lt;p&gt;The initial state probability at \(t=1\) is the number of times we expect to be in state \(z=i\) at \(t=1\):&lt;/p&gt;
&lt;p&gt;\[
\gamma_1(i).
\]&lt;/p&gt;
&lt;p&gt;Finally, the observation probability parameters are updated by considering the number of times we are in state \(z=j\) and observing \(x=k\) divided by the number of times we are in state \(z=j\). Note that this is for a multinomial probabiliy distribution:&lt;/p&gt;
&lt;p&gt;\[
\hat{b}_j(k) = \frac{\sum_{t=1, x_t = k}^T \gamma_t(j)}{\sum_{t=1}^T \gamma_t(j)}.
\]&lt;/p&gt;
&lt;p&gt;These formulas are derived from maximizing Baum&amp;rsquo;s auxiliary function&lt;/p&gt;
&lt;p&gt;\[
Q(\lambda, \hat{\lambda}) = \sum_{Q} p(\mathbf{z}|\mathbf{x}, \lambda) \log p(\mathbf{x}, \mathbf{z}|\hat{\lambda})
\]&lt;/p&gt;
&lt;p&gt;over \(\hat{\lambda}\). It has further been shown that maximizing this function leads to increased likelihood:&lt;/p&gt;
&lt;p&gt;\[
\max_{\hat{\lambda}} Q(\lambda, \hat{\lambda}) \implies p(\mathbf{x}|\hat{\lambda}) \geq p(\mathbf{x}|\lambda).
\]&lt;/p&gt;
&lt;p&gt;If we have a Gaussian observation model, the values for \(\hat{b}_j(k)\) are computed to accommodate the parameters of the distribution.
These parameter estimates assume a Gaussian mixture model.
Starting with \(\hat{\mu}_{jk}\), it can be estimated by dividing the expected value of observations belonging to Gaussian density \(k\) by the expected number of times we are in state \(j\) using the \(k^{\text{th}}\) mixture component:&lt;/p&gt;
&lt;p&gt;\[
\hat{\mathbf{\mu}}_{jk} = \frac{\sum_{t=1}^T \gamma_t(j, k)\mathbf{x}_t}{\sum_{t=1}^T \gamma_t(j, k)}.
\]&lt;/p&gt;
&lt;p&gt;Here, \(\gamma_t(j, k)\) is the probability of being in state \(j\) at time \(t\) with the \(k^{\text{th}}\) mixture component accounting for \(\mathbf{x}_t\):&lt;/p&gt;
&lt;p&gt;\[
\gamma_t(j, k) = \frac{\alpha_t(j)\beta_t(j)}{\sum_{j=1}^N \alpha_t(j) \beta_t(j)} \frac{c_{jk}\mathcal{N}(\mathbf{x}_t, \mu_{jk}, \mathbf{\Sigma}_{jk})}{\sum_{m=1}^M c_{jm}\mathcal{N}(\mathbf{x}_t, \mu_{jm}, \mathbf{\Sigma}_{jm})}.
\]&lt;/p&gt;
&lt;p&gt;This method is proven to improve the parameters.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Each iteration is guaranteed to improve the log-likelihood function.&lt;/li&gt;
&lt;li&gt;The process is guaranteed to converge.&lt;/li&gt;
&lt;li&gt;The convergence point is a fixed point of the likelihood function.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These guarantees are similar to gradient ascent.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;RABINER, LAWRENCE R. 1989. “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.” &lt;i&gt;Proceedings of the Ieee&lt;/i&gt; 77 (2): 30.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Lagrangian Multipliers</title>
      <link>https://ajdillhoff.github.io/notes/lagrangian_multipliers/</link>
      <pubDate>Sat, 05 Feb 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/lagrangian_multipliers/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s take a simple constrained problem (from Nocedal and Wright).&lt;/p&gt;
&lt;p&gt;\begin{align*}
\min \quad &amp;amp; x_1 + x_2\\
\textrm{s.t.} \quad &amp;amp; x_1^2 + x_2^2 - 2 = 0
\end{align*}&lt;/p&gt;
&lt;p&gt;The set of possible solutions to this problem lie on the boundary of the circle defined by the constraint:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2021-12-01_18-06-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Source: Nocedal and Wright&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Source: Nocedal and Wright
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;If we let \(g(\mathbf{x}) = x_1^2 + x_2^2 - 2\), then the gradient vector is \((2x_1, 2x_2)\)&lt;/p&gt;
&lt;p&gt;Our original function \(f(\mathbf{x}) = x_1 + x_2\) has a gradient vector of \((1, 1)\).&lt;/p&gt;
&lt;p&gt;The figure above visualizes these vectors at different points on the constraint boundary.&lt;/p&gt;
&lt;p&gt;Notice that the optimal solution \(\mathbf{x}^* = (-1, -1)\) is at a point where \(\nabla g(\mathbf{x}^*)\) is parallel to \(\nabla f(\mathbf{x}^*)\). However, the gradients of the vectors are not equal. So there must be some scalar \(\lambda\) such that \(\nabla f(\mathbf{x}^*) = \lambda \nabla g(\mathbf{x}^*)\).&lt;/p&gt;
&lt;p&gt;This scalar \(\lambda\) is called a Lagrangian multiplier. We use this and introduce the Lagrangian function:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathcal{L}(\mathbf{x}, \lambda) = f(\mathbf{x}) - \lambda g(\mathbf{x})
\end{equation*}&lt;/p&gt;
&lt;p&gt;This yields a form for which we can analytically calculate the stationary points. That is,&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\nabla_{\mathbf{x}} \mathcal{L}(\mathbf{x}^*, \lambda^*) = 0.
\end{equation*}&lt;/p&gt;
&lt;h2 id=&#34;lagrangian-duality&#34;&gt;Lagrangian Duality&lt;/h2&gt;
&lt;p&gt;In general, the primal optimization problem is formulated as&lt;/p&gt;
&lt;p&gt;\begin{align*}
\min_{w} \quad &amp;amp; f(w)\\
\textrm{s.t.} \quad &amp;amp; g_i(w) \leq 0, \quad i = 1, \dots, k\\
&amp;amp; h_i(w) = 0, \quad i = 1, \dots, l.
\end{align*}&lt;/p&gt;
&lt;p&gt;The Lagrangian function is then&lt;/p&gt;
&lt;p&gt;\[
L(w, \alpha, \beta) = f(w) + \sum_{i=1}^k\alpha_i g_i(w) + \sum_{i=1}^l \beta_i h_i(w).
\]&lt;/p&gt;
&lt;h2 id=&#34;additional-resources&#34;&gt;Additional Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cs229.stanford.edu/notes2021fall/cs229-notes3.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://cs229.stanford.edu/notes2021fall/cs229-notes3.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Kernels</title>
      <link>https://ajdillhoff.github.io/notes/kernels/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/kernels/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dual-representation&#34;&gt;Dual Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#relating-back-to-the-original-formulation&#34;&gt;Relating Back to the Original Formulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#types-of-kernels&#34;&gt;Types of Kernels&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Notebook link: &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/blob/main/svm/kernels.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://github.com/ajdillhoff/CSE6363/blob/main/svm/kernels.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Parametric models use training data to estimate a set of parameters that can then be used to perform inference on new data.
An alternative approach uses &lt;strong&gt;nonparametric methods&lt;/strong&gt;, meaning the function is estimated directly from the data instead of optimizing a set of parameters.&lt;/p&gt;
&lt;p&gt;One possible downside to such an approach is that it becomes less efficient as the amount of training data increases.
Additionally, the transformation into a feature space such that the data becomes linearly separable may be intractable.
Consider sequential data such as text or audio.
If each sample has a variable number of features, how do we account for this using standard linear models with a fixed number of parameters?&lt;/p&gt;
&lt;p&gt;The situations described above can be overcome through the use of the &lt;strong&gt;kernel trick&lt;/strong&gt;.
We will see that, by computing a measure of similarity between samples in the feature space, we do not need to directly transform each individual sample to that space.&lt;/p&gt;
&lt;p&gt;A kernel function is defined as&lt;/p&gt;
&lt;p&gt;\[
k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = \phi(\mathbf{x})^T \phi(\mathbf{x}&amp;rsquo;),
\]&lt;/p&gt;
&lt;p&gt;where \(\phi\) is some function which transforms the input to a feature space.&lt;/p&gt;
&lt;p&gt;Methods that require part or all of the training data to make prediction will benefit from using kernel representations, especially when using high dimensional data. Instead of transforming the data into a high dimensional space which may be computationally intractable, a measure of similarity via the &lt;em&gt;inner product&lt;/em&gt; is used. The inner product is not the projection into some space. Instead, it represents the outcome of that projection.&lt;/p&gt;
&lt;h2 id=&#34;dual-representation&#34;&gt;Dual Representation&lt;/h2&gt;
&lt;p&gt;The key to taking advantage of the kernel trick relies on reformulating our linear model into a dual representation.
In this form, we will establish a dependence on the kernel function.&lt;/p&gt;
&lt;p&gt;The following derivation of the dual representation for linear regression follows (Bishop). Consider the least squares loss with \(L2\) regularization, as we discussed with &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_regression/&#34;&gt;Linear Regression&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;\[
J(\mathbf{w}) = \frac{1}{2}\sum_{i=1}^n(\mathbf{w}^T\phi(\mathbf{x}_i) - y_i)^2 + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}
\]&lt;/p&gt;
&lt;p&gt;Here, \(\phi\) is a basis function that transforms the input. This could also be a simple identity function in which \(\phi(\mathbf{x}) = \mathbf{x}\). To solve for \(\mathbf{w}\), we take the gradient of \(J(\mathbf{w})\) with respect to \(\mathbf{w}\) and set it to 0.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\nabla_{\mathbf{w}}J(\mathbf{w}) &amp;amp;= \sum_{i=1}^n(\mathbf{w}^T\phi(\mathbf{x}_i) - y_i)\phi(\mathbf{x}_i) + \lambda \mathbf{w}\\
\implies \mathbf{w} &amp;amp;= -\frac{1}{\lambda}\sum_{i=1}^n(\mathbf{w}^T\phi(\mathbf{x}_i) - y_i)\phi(\mathbf{x}_i)
\end{align*}&lt;/p&gt;
&lt;p&gt;We can formulate this as a matrix-vector product by letting&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{\Phi} =
\begin{bmatrix}
\phi(\mathbf{x}_1)^T\\
\vdots \\
\phi(\mathbf{x}_n)^T\\
\end{bmatrix}
\text{ and }
a_{i} = -\frac{1}{\lambda}(\mathbf{w}^T\phi(\mathbf{x}_i) - y_i).
\end{equation*}&lt;/p&gt;
&lt;p&gt;Then, \(\mathbf{w} = \mathbf{\Phi}^T\mathbf{a}\), where \(\mathbf{a} = [a_1, \dots, a_n]^T\).&lt;/p&gt;
&lt;p&gt;The dual representation is derived by reformulating \(J(\mathbf{w})\) in terms of \(\mathbf{a}\).&lt;/p&gt;
&lt;p&gt;\begin{equation*}
J(\mathbf{a}) = \frac{1}{2}\mathbf{a}^T\mathbf{\Phi}\mathbf{\Phi}^T\mathbf{\Phi}\mathbf{\Phi}^T\mathbf{a} - \mathbf{a}^T\mathbf{\Phi}\mathbf{\Phi}^T\mathbf{y} + \frac{1}{2}\mathbf{y}^T\mathbf{y} + \frac{\lambda}{2} \mathbf{a}^T\mathbf{\Phi}\mathbf{\Phi}^T\mathbf{a},
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(\mathbf{y} = [y_1, \dots, y_n]\).&lt;/p&gt;
&lt;p&gt;Looking at the products \(\mathbf{\Phi}\mathbf{\Phi}^T\), we see that these relate to our original kernel form: \(\phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j)\). This product defines a &lt;a href=&#34;https://en.wikipedia.org/wiki/Gram_matrix&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Gram matrix&lt;/a&gt; \(\mathbf{K} = \mathbf{\Phi}\mathbf{\Phi}^T\) whose elements are \(k(\mathbf{x}_i, \mathbf{x}_j)\). Thus, we can rewrite \(J(\mathbf{a})\) as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
J(\mathbf{a}) = \frac{1}{2}\mathbf{a}^T\mathbf{K}\mathbf{K}\mathbf{a} - \mathbf{a}^T\mathbf{K}\mathbf{y} + \frac{1}{2}\mathbf{y}^T\mathbf{y} + \frac{\lambda}{2}\mathbf{a}^T\mathbf{K}\mathbf{a}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Solving for \(\mathbf{a}\) can be done by computing the gradient of \(J(\mathbf{a})\) with respect to \(\mathbf{a}\) and setting the result to 0.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\nabla_\mathbf{a}J(\mathbf{a}) = \mathbf{K}\mathbf{K}\mathbf{a} - \mathbf{K}\mathbf{y} + \lambda \mathbf{K}\mathbf{a} &amp;amp;= 0\\
\mathbf{K}\mathbf{a} + \lambda I\mathbf{a} - \mathbf{y} &amp;amp;= 0\\
(\mathbf{K} + \lambda I)\mathbf{a} &amp;amp;= \mathbf{y}\\
\mathbf{a} &amp;amp;= (\mathbf{K} + \lambda I)^{-1} \mathbf{y}.
\end{align*}&lt;/p&gt;
&lt;p&gt;With \(\mathbf{a}\) solved, we can complete the dual representation of our original linear regression model. Recall that&lt;/p&gt;
&lt;p&gt;\begin{equation*}
h(\mathbf{x}; \mathbf{w}) = \mathbf{w}^T\phi(\mathbf{x}).
\end{equation*}&lt;/p&gt;
&lt;p&gt;If we substitute \(\mathbf{w} = \mathbf{\Phi}^T\mathbf{a}\), we get&lt;/p&gt;
&lt;p&gt;\begin{align*}
f(\mathbf{x};\mathbf{a}) &amp;amp;= \mathbf{a}^T\mathbf{\Phi}\phi(\mathbf{x})\\
&amp;amp;= \Big[(\mathbf{K} + \lambda I)^{-1}\mathbf{y})\Big]^T\mathbf{\Phi}\phi(\mathbf{x}).
\end{align*}&lt;/p&gt;
&lt;p&gt;Again, the kernel form is apparent in the product \(\mathbf{\Phi}\phi(\mathbf{x})\). If we let \(k_i(\mathbf{x}) = k(\mathbf{x}_i,\mathbf{x})\) and&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{k}(\mathbf{x}) =
\begin{bmatrix}
k_1(\mathbf{x})\\
\vdots \\
k_n(\mathbf{x})
\end{bmatrix},
\end{equation*}&lt;/p&gt;
&lt;p&gt;we can write the dual representation of our linear regression model as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f(\mathbf{x}) = \mathbf{k}(\mathbf{x})^T(\mathbf{K} + \lambda \mathbf{I})^{-1}\mathbf{y}.
\end{equation*}&lt;/p&gt;
&lt;h2 id=&#34;relating-back-to-the-original-formulation&#34;&gt;Relating Back to the Original Formulation&lt;/h2&gt;
&lt;p&gt;In this dual formulation, the solution for \(\mathbf{a}\) can be expressed as a linear combination of elements \(\phi(\mathbf{x})\).
From above, we see that&lt;/p&gt;
&lt;p&gt;\[
a_i = -\frac{1}{\lambda}\big(\mathbf{w}^T\phi(\mathbf{x}_i) - y_i\big).
\]&lt;/p&gt;
&lt;p&gt;Expanding this into individual coefficients yields&lt;/p&gt;
&lt;p&gt;\begin{align*}
a_i &amp;amp;= -\frac{1}{\lambda}\big(w_1\phi_1(\mathbf{x}_i) + \cdots + w_m \phi_m(\mathbf{x}_i) - y_i\big)\\
&amp;amp;= -\frac{w_1}{\lambda}\phi_1(\mathbf{x}_i) - \cdots - \frac{w_m}{\lambda} \phi_m(\mathbf{x}_i) + \frac{y_i}{\lambda}.
\end{align*}&lt;/p&gt;
&lt;p&gt;We are close, but we still need to do something about the term \(\frac{y_i}{\lambda}\). For this, we can multiply both sides of our equation by a convenient 1. That is, we multiply by&lt;/p&gt;
&lt;p&gt;\[
\frac{\phi_1(\mathbf{x}_i) + \cdots + \phi_m(\mathbf{x}_i)}{\phi_1(\mathbf{x}_i) + \cdots + \phi_m(\mathbf{x}_i)}.
\]&lt;/p&gt;
&lt;p&gt;By doing this and grouping the \(\phi_j\) terms, we get&lt;/p&gt;
&lt;p&gt;\begin{align*}
&amp;amp;\Big(\frac{y_i}{\lambda}\cdot \frac{1}{\phi_1(\mathbf{x}_i) + \cdots + \phi_m(\mathbf{x}_i)} - \frac{w_1}{\lambda}\Big)\phi_1(\mathbf{x}_i) + \cdots\\
&amp;amp;+ \Big(\frac{y_i}{\lambda}\cdot \frac{1}{\phi_1(\mathbf{x}_i) + \cdots + \phi_m(\mathbf{x}_i)} - \frac{w_m}{\lambda}\Big)\phi_m(\mathbf{x}_i).
\end{align*}&lt;/p&gt;
&lt;p&gt;We can simplify this by introducing a term&lt;/p&gt;
&lt;p&gt;\[
c_i = \frac{y_i}{\lambda}\cdot \frac{1}{\phi_1(\mathbf{x}_i) + \cdots + \phi_m(\mathbf{x}_i)}.
\]&lt;/p&gt;
&lt;p&gt;Then the solution can be rewritten as&lt;/p&gt;
&lt;p&gt;\[
\Big(c_i - \frac{w_1}{\lambda}\Big)\phi_1(\mathbf{x}_i) + \cdots + \Big(c_i - \frac{w_m}{\lambda}\Big)\phi_m(\mathbf{x}_i).
\]&lt;/p&gt;
&lt;p&gt;With this, we can step backwards using intermediate results in the previous section to get back to the original formulation of our linear regression model.&lt;/p&gt;
&lt;h2 id=&#34;types-of-kernels&#34;&gt;Types of Kernels&lt;/h2&gt;
&lt;p&gt;There are several types of kernels that can be used to transform the input data depending on the problem. The simplest kernel is the &lt;strong&gt;identity kernel:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\[
k(\mathbf{x}, \mathbf{x&amp;rsquo;}) = \mathbf{x}^T \mathbf{x&amp;rsquo;}.
\]&lt;/p&gt;
&lt;h3 id=&#34;polynomial-kernel&#34;&gt;Polynomial Kernel&lt;/h3&gt;
&lt;p&gt;A polynomial kernel is defined as&lt;/p&gt;
&lt;p&gt;\[
k(\mathbf{x}, \mathbf{x&amp;rsquo;}) = (\mathbf{x}^T\mathbf{x&amp;rsquo;}+c)^d.
\]&lt;/p&gt;
&lt;p&gt;This is a common choice for solving problems akin to polynomial regression.
We can use this kernel to present a visual explanation of kernel functions.
Consider the following dataset.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-19_22-06-55_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Binary classification dataset that is not linearly separable.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Binary classification dataset that is not linearly separable.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;It is easy enough to see that this dataset could not be separated using a hyperplane in 2D.
We could separate the two using some nonlinear decision boundary like a circle.
If we could transform this into 3D space, we could come up with some features such that it is linearly separable in 3D.
For example, let \(\phi(\mathbf{x}) = (x_1^2, x_2^2, \sqrt{2}x_1x_2)\).&lt;/p&gt;
&lt;p&gt;Transforming all points and visualizing yields the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-19_22-11-36_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Binary classification dataset transformed into a 3D feature space.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Binary classification dataset transformed into a 3D feature space.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;From this perspective, we can clearly see that the data is linearly separable.
The question remains: if we only have the original 2D features, how do we compare points in this 3D features space without explicitly transforming each point?
The kernel function corresponding to the feature transform above is&lt;/p&gt;
&lt;p&gt;\begin{align*}
k(\mathbf{x}, \mathbf{x}&amp;rsquo;) &amp;amp;= (\mathbf{x}^T\mathbf{x}&amp;rsquo;)^2\\
&amp;amp;= (x_1x&amp;rsquo;_1 + x_2x&amp;rsquo;_2)^2\\
&amp;amp;= 2x_1x&amp;rsquo;_1x_2x&amp;rsquo;_2 + (x_1x&amp;rsquo;_1)^2 + (x_2x&amp;rsquo;_2)^2\\
&amp;amp;= \phi(\mathbf{x})^T \phi(\mathbf{x}&amp;rsquo;)
\end{align*}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\phi(\mathbf{x}) =
\begin{bmatrix}
\sqrt{2}x_1x_2\\
x_1^2\\
x_2^2
\end{bmatrix}.
\]&lt;/p&gt;
&lt;h3 id=&#34;radial-basis-function-kernel&#34;&gt;Radial Basis Function Kernel&lt;/h3&gt;
&lt;p&gt;This kernel follows a Gaussian term and is commonly used with &lt;a href=&#34;https://ajdillhoff.github.io/notes/support_vector_machine/&#34;&gt;SVMs&lt;/a&gt;. It is defined as&lt;/p&gt;
&lt;p&gt;\[
k(\mathbf{x}, \mathbf{x&amp;rsquo;}) = \exp\Big(-\frac{\|\mathbf{x}-\mathbf{x&amp;rsquo;}\|^2}{2\sigma^2}\Big).
\]&lt;/p&gt;
&lt;h3 id=&#34;cosine-similarity&#34;&gt;Cosine Similarity&lt;/h3&gt;
&lt;p&gt;Consider the problem of comparing text sequences for a document classification task.
One approach is to compare the number of occurrences of each word.
The idea is that documents that are similar will have a similar number of words that occur.&lt;/p&gt;
&lt;p&gt;\[
k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = \frac{\mathbf{x}^T \mathbf{x}&amp;rsquo;}{\|\mathbf{x}\|_2 \|\mathbf{x}&amp;rsquo;\|_2}
\]&lt;/p&gt;
&lt;p&gt;Documents that are &lt;strong&gt;orthogonal&lt;/strong&gt;, in the sense that the resulting cosine similarity is 0, are dissimilar.
The similarity increases as the score approaches 1.
There are several issues with this approach which are addressed by using the term frequence-inverse document frequency (TF-IDF) score.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linear Discriminant Analysis</title>
      <link>https://ajdillhoff.github.io/notes/linear_discriminant_analysis/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/linear_discriminant_analysis/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gaussian-class-conditional-densities&#34;&gt;Gaussian Class Conditional Densities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quadratic-descriminant-analysis&#34;&gt;Quadratic Descriminant Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example&#34;&gt;Example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This section covers classification from a probabilistic perspective.
The &lt;a href=&#34;https://ajdillhoff.github.io/notes/discriminant_functions/&#34;&gt;discriminative approach&lt;/a&gt; involves a parameterized function which assigns each input vector \(\mathbf{x}\) to a specific class.
We will see that modeling the conditional probability distribution \(p(C_k|\mathbf{x})\) grants us additional benefits while still fulfilling our original classification task.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s begin with a 2 class problem. To classify this with a generative model, we use the class-conditional densities \(p(\mathbf{x}|C_i)\) and class priors \(p(C_i)\).
The posterior probability for \(C_1\) can be written in the form of a sigmoid function:&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(C_1|\mathbf{x}) &amp;amp;= \frac{p(\mathbf{x}|C_1)p(C_1)}{p(\mathbf{x}|C_1)p(C_1) + p(\mathbf{x}|C_2)p(C_2)}
\end{align*}&lt;/p&gt;
&lt;p&gt;Then multiply the numerator and denominator by&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{(p(\mathbf{x}|C_1))^{-1}}{(p(\mathbf{x}|C_1))^{-1}},
\end{equation*}&lt;/p&gt;
&lt;p&gt;which yields&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{1}{1 + \frac{p(\mathbf{x}|C_2)p(C_2)}{p(\mathbf{x}|C_1)p(C_1)}}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Noting that \(a = \exp(\ln(a))\), we can rewrite further&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{1}{1 + \exp(-a)},
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(a = \ln \frac{p(\mathbf{x}|C_1)p(C_1)}{p(\mathbf{x}|C_2)p(C_2)}\).&lt;/p&gt;
&lt;p&gt;Writing this distribution in the form of the sigmoid function is convenient as it is a natural choice for many other classification models. It also has a very simple derivative which is convenient for models optimized using gradient descent.&lt;/p&gt;
&lt;p&gt;Given certain choices for the class conditional densities, the posterior probabilty distribution will be a linear function of the input features:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\ln p(C_k|\mathbf{x};\theta) = \mathbf{w}^T \mathbf{x} + c,
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(\mathbf{w}\) is a parameter vector based on the parameters of the chosen probability distribution, and \(c\) is a constant term that is not dependent on the parameters. As we will see, the resulting model will take an equivalent form to the discriminative approach.&lt;/p&gt;
&lt;h2 id=&#34;gaussian-class-conditional-densities&#34;&gt;Gaussian Class Conditional Densities&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s assume that our class conditional densities \(p(\mathbf{x}|C_k)\) are Gaussian. We will additionally assume that the covariance matrices between classes are shared. This will result in linear decision boundaries. Since the conditional densities are chosen to be Gaussian, the posterior is given by&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(C_k|\mathbf{x};\theta) \propto \pi_k\mathcal{N}(\mathbf{x}|\mathbf{\mu}_c,\Sigma),
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(\pi_k\) is the prior probability of class \(k\). We choose to ignore the normalizing constant since it is not dependent on the class.&lt;/p&gt;
&lt;p&gt;The class conditional density function for class \(k\) is given by&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(\mathbf{x}|C_k;\theta) = \frac{1}{2\pi^{D/2}}\frac{1}{|\Sigma|^{1/2}}\exp\Big(-\frac{1}{2}(\mathbf{x} - \mathbf{\mu}_k)^T \Sigma^{-1} (\mathbf{x} - \mathbf{\mu}_k)\Big).
\end{equation*}&lt;/p&gt;
&lt;p&gt;Now that we have a concrete function to work with, let&amp;rsquo;s go back to the simple case of two classes and define \(a = \ln \frac{p(\mathbf{x}|C_1)p(C_1)}{p(\mathbf{x}|C_2)p(C_2)}\). First, we rewrite \(a\):&lt;/p&gt;
&lt;p&gt;\begin{equation*}
a = \ln p(\mathbf{x}|C_1) - \ln p(\mathbf{x}|C_2) + \ln \frac{p(C_1)}{p(C_2)}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;The log of the class conditional density for a Gaussian is&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\ln p(\mathbf{x}|C_k;\mathbf{\mu}_k,\Sigma) =
-\frac{D}{2}\ln(2\pi) - \frac{1}{2}\ln|\Sigma|-\frac{1}{2}(\mathbf{x}-\mathbf{\mu}_k)^T \Sigma^{-1} (\mathbf{x}-\mathbf{\mu}_k).
\end{equation*}&lt;/p&gt;
&lt;p&gt;To simplify the above result, we will group the terms that are not dependent on the class parameters since they are consant:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\ln p(\mathbf{x}|C_k;\mathbf{\mu}_k,\Sigma) =
-\frac{1}{2}(\mathbf{x}-\mathbf{\mu}_k)^T \Sigma^{-1} (\mathbf{x}-\mathbf{\mu}_k) + c.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Observing that this quantity takes on a quadratic form, we can rewrite the above as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\ln p(\mathbf{x}|C_k;\mathbf{\mu}_k,\Sigma) =
-\frac{1}{2}\mathbf{\mu}_k\Sigma^{-1}\mathbf{\mu}_k + \mathbf{x}^T \Sigma^{-1} \mathbf{\mu}_k
-\frac{1}{2}\mathbf{x}^T \Sigma^{-1}\mathbf{x} + c.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Using this, we complete the definition of \(a\):&lt;/p&gt;
&lt;p&gt;\begin{align*}
a &amp;amp;= \ln p(\mathbf{x}|C_1) - \ln p(\mathbf{x}|C_2) + \ln \frac{p(C_1)}{p(C_2)}\\
&amp;amp;= -\frac{1}{2}\mathbf{\mu}_1\Sigma^{-1}\mathbf{\mu}_1 + \mathbf{x}^T \Sigma^{-1} \mathbf{\mu}_1 + \frac{1}{2}\mathbf{\mu}_2\Sigma^{-1}\mathbf{\mu}_2 - \mathbf{x}^T \Sigma^{-1} \mathbf{\mu}_2 + \ln \frac{p(C_1)}{p(C_2)}\\
&amp;amp;= \mathbf{x}^T(\Sigma^{-1}(\mathbf{\mu}_1 - \mathbf{\mu}_2)) - \frac{1}{2}\mathbf{\mu}_1\Sigma^{-1}\mathbf{\mu}_1 + \frac{1}{2}\mathbf{\mu}_2\Sigma^{-1}\mathbf{\mu}_2 + \ln \frac{p(C_1)}{p(C_2)}\\
&amp;amp;= (\Sigma^{-1}(\mathbf{\mu}_1 - \mathbf{\mu}_2))^T \mathbf{x} - \frac{1}{2}\mathbf{\mu}_1\Sigma^{-1}\mathbf{\mu}_1 + \frac{1}{2}\mathbf{\mu}_2\Sigma^{-1}\mathbf{\mu}_2 + \ln \frac{p(C_1)}{p(C_2)}.
\end{align*}&lt;/p&gt;
&lt;p&gt;Finally, we define&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{w} = \Sigma^{-1}(\mathbf{\mu}_1 - \mathbf{\mu}_2)
\end{equation*}&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;\begin{equation*}
w_0 = - \frac{1}{2}\mathbf{\mu}_1\Sigma^{-1}\mathbf{\mu}_1 - \frac{1}{2}\mathbf{\mu}_2\Sigma^{-1}\mathbf{\mu}_2 + \ln \frac{p(C_1)}{p(C_2)}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Thus, our posterior takes on the form&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(C_1|\mathbf{x};\theta) = \sigma(\mathbf{w}^T \mathbf{x} + w_0).
\end{equation*}&lt;/p&gt;
&lt;h3 id=&#34;multiple-classes&#34;&gt;Multiple Classes&lt;/h3&gt;
&lt;p&gt;What if we have more than 2 classes?
Recall that a &lt;strong&gt;generative classifier&lt;/strong&gt; is modeled as&lt;/p&gt;
&lt;p&gt;\[
p(C_k|\mathbf{x};\mathbf{\theta}) = \frac{p(C_k|\mathbf{\theta})p(\mathbf{x}|C_k, \mathbf{\theta})}{\sum_{k&amp;rsquo;}p(C_{k&amp;rsquo;}|\mathbf{\theta})p(\mathbf{x}|C_{k&amp;rsquo;}, \mathbf{\theta})}.
\]&lt;/p&gt;
&lt;p&gt;As stated above, \(\mathbf{\pi}_k = p(C_k|\mathbf{\theta})\) and \(p(\mathbf{x}|C_k,\mathbf{\theta}) = \mathcal{N}(\mathbf{x}|\mathbf{\mu}_c,\Sigma)\).&lt;/p&gt;
&lt;p&gt;For LDA, the covariance matrices are shared across all classes.
This permits a simplification of the class posterior distribution \(p(C_k|\mathbf{x};\mathbf{\theta})\):&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(C_k|\mathbf{x};\mathbf{\theta}) &amp;amp;\propto \mathbf{\pi}_k \exp\big(\mathbf{\mu}_k^T \mathbf{\Sigma}^{-1}\mathbf{x} - \frac{1}{2}\mathbf{x}^T\mathbf{\Sigma}^{-1}\mathbf{x} - \frac{1}{2}\mathbf{\mu}_k\mathbf{\Sigma}^{-1}\mathbf{\mu}_k\big)\\
&amp;amp;= \exp\big(\mathbf{\mu}_k^T \mathbf{\Sigma}^{-1}\mathbf{x}  - \frac{1}{2}\mathbf{\mu}_k\mathbf{\Sigma}^{-1}\mathbf{\mu}_k + \log \mathbf{\pi}_k \big) \exp\big(- \frac{1}{2}\mathbf{x}^T\mathbf{\Sigma}^{-1}\mathbf{x}\big).
\end{align*}&lt;/p&gt;
&lt;p&gt;The term \(\exp\big(- \frac{1}{2}\mathbf{x}^T\mathbf{\Sigma}^{-1}\mathbf{x}\big)\) is placed aside since it is not dependent on the class \(k\).
When divided by the sum per the definition of \(p(C_k|\mathbf{x};\mathbf{\theta})\), it will equal to 1.&lt;/p&gt;
&lt;p&gt;Under this formulation, we let&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{w}_k &amp;amp;= \mathbf{\Sigma}^{-1}\mathbf{\mu}_k\\
\mathbf{b}_k &amp;amp;= -\frac{1}{2}\mathbf{\mu}_k^T \mathbf{\Sigma}^{-1}\mathbf{\mu}_k + \log \mathbf{\pi}_k.
\end{align*}&lt;/p&gt;
&lt;p&gt;This lets us express \(p(C_k|\mathbf{x};\mathbf{\theta})\) as the &lt;strong&gt;softmax&lt;/strong&gt; function:&lt;/p&gt;
&lt;p&gt;\(p(C_k|\mathbf{x};\mathbf{\theta}) = \frac{\exp(\mathbf{w}_k^T\mathbf{x}+\mathbf{b}_k)}{\sum_{k&amp;rsquo;}\exp(\mathbf{w}_{k&amp;rsquo;}^T\mathbf{x}+\mathbf{b}_{k&amp;rsquo;})}\).&lt;/p&gt;
&lt;h2 id=&#34;maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/h2&gt;
&lt;p&gt;Given our formulation in the previous section, we can estimate the parameters of the model via &lt;strong&gt;maximum likelihood estimation&lt;/strong&gt;. Assuming \(K\) classes with Gaussian class conditional densities, the likelihood function is&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(\mathbf{X}|\mathbf{\theta}) = \prod_{i=1}^n \mathcal{M}(y_i|\mathbf{\pi})\prod_{k=1}^K \mathcal{N}(\mathbf{x}_i|\mathbf{\mu}_k, \mathbf{\Sigma}_k)^{\mathbb{1}(y_i=k)}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Taking the log of this function yields&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\ln p(\mathbf{X}|\mathbf{\theta}) = \Big[\sum_{i=1}^n \sum_{k=1}^K \mathbb{1}(y_i=k)\ln \pi_k\Big] + \sum_{k=1}^K\Big[\sum_{i:y_i=c} \ln \mathcal{N}(\mathbf{x}_n|\mathbf{\mu}_k, \mathbf{\Sigma}_k)\Big].
\end{equation*}&lt;/p&gt;
&lt;p&gt;Given that this is a sum of two different components, we can optimize the multinomial parameter \(\mathbf{\pi}\) and the class Gaussian parameters \((\mathbf{\mu}_k, \mathbf{\Sigma}_k)\) separately.&lt;/p&gt;
&lt;h3 id=&#34;class-prior&#34;&gt;Class Prior&lt;/h3&gt;
&lt;p&gt;For multinomial distributions, the class prior parameter estimation \(\hat{\pi}_k\) is easily calculated by counting the number of samples belonging to class \(k\) and dividing it by the total number of samples.&lt;/p&gt;
&lt;p&gt;\[
\hat{\pi}_k = \frac{n_k}{n}
\]&lt;/p&gt;
&lt;h3 id=&#34;class-gaussians&#34;&gt;Class Gaussians&lt;/h3&gt;
&lt;p&gt;The Gaussian parameters can be calculated as discussed during the probability review. The parameter estimates are&lt;/p&gt;
&lt;p&gt;\begin{align*}
\hat{\mathbf{u}}_k &amp;amp;= \frac{1}{n_k}\sum_{i:y_i=k}\mathbf{x}_i\\
\hat{\Sigma}_k &amp;amp;= \frac{1}{n_k}\sum_{i:y_i=k}(\mathbf{x}_i - \hat{\mathbf{\mu}}_k)(\mathbf{x}_i - \hat{\mathbf{\mu}}_k)^T
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;the-decision-boundary&#34;&gt;The Decision Boundary&lt;/h3&gt;
&lt;p&gt;The decision boundary between two classes can be visualized at the point when \(p(C_k|\mathbf{x};\theta) = 0.5\).&lt;/p&gt;
&lt;h2 id=&#34;quadratic-descriminant-analysis&#34;&gt;Quadratic Descriminant Analysis&lt;/h2&gt;
&lt;p&gt;Linear Discriminant Analysis is a special case of Quadratic Discriminant Analysis (QDA) where the covariance matrices are shared across all classes. Assuming each class conditional density is Gaussian, the posterior probability is given by&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(C_k|\mathbf{x};\theta) \propto \pi_k\mathcal{N}(\mathbf{x}|\mathbf{\mu}_k,\Sigma_k).
\end{equation*}&lt;/p&gt;
&lt;p&gt;Taking the log of this function yields&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\ln p(C_k|\mathbf{x};\theta) = \ln \pi_k - \frac{1}{2}\ln |\Sigma_k| - \frac{1}{2}(\mathbf{x} - \mathbf{\mu}_k)^T \Sigma_k^{-1}(\mathbf{x} - \mathbf{\mu}_k) + c.
\end{equation*}&lt;/p&gt;
&lt;p&gt;With LDA, the term \(\frac{1}{2}\ln |\Sigma_k|\) is constant across all classes, so we treat it as another constant. Since QDA considers a different covariance matrix for each class, we must keep this term in the equation.&lt;/p&gt;
&lt;h3 id=&#34;quadratic-decision-boundary&#34;&gt;Quadratic Decision Boundary&lt;/h3&gt;
&lt;p&gt;In the more general case of QDA, the decision boundary is quadratic, leading to a quadratic discriminant function. As shown above, the posterior probability function for LDA is linear in \(\mathbf{x}\), which leads to a linear discriminant function.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;See &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/blob/main/logistic_regression/lda.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt; for an example using scikit-learn.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>https://ajdillhoff.github.io/notes/logistic_regression/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/logistic_regression/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#picking-a-model&#34;&gt;Picking a Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#binary-classification&#34;&gt;Binary Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiple-classes&#34;&gt;Multiple Classes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;With &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_regression/&#34;&gt;Linear Regression&lt;/a&gt; we were able to fit a model to our data in order to make inferences on unseen data points. In the examples, both the input features and observation were continuous. With logistic regression, we will use similar models to classify the data points based on their input features. We start out with the simplest approach: we assume that the data is linearly separable and can be assigned one of \(K\) discrete classes.&lt;/p&gt;
&lt;p&gt;In the binary case, the target variable will takes on either a 0 or 1. For \(K &amp;gt; 2\), we will use a \(K\) dimensional vector that has a 1 corresponding to the class encoding for that input and a 0 for all other positions. For example, if our possible target classes were \(\{\text{car, truck, person}\}\), then a target vector for \(\text{person}\) would be \(\mathbf{y} = [0, 0, 1]^T\).&lt;/p&gt;
&lt;p&gt;This article will stick to a discriminative approach to logistic regression. That is, we define a discriminant function which assigns each data input \(\mathbf{x}\) to a class. For a probabilistic perspective, see &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_discriminant_analysis/&#34;&gt;Linear Discriminant Analysis&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;picking-a-model&#34;&gt;Picking a Model&lt;/h2&gt;
&lt;p&gt;We will again start with a linear model \(y = f(\mathbf{x}; \mathbf{w})\). Unlike the model used with &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_regression/&#34;&gt;Linear Regression&lt;/a&gt;, ours will need to predict a discrete class label. The logistic model is often approached by introducing the &lt;strong&gt;odds&lt;/strong&gt; of an event occurring:&lt;/p&gt;
&lt;p&gt;\[
\frac{p}{1-p},
\]&lt;/p&gt;
&lt;p&gt;where \(p\) is the probability of the event happening.
As \(p\) increases, the odds of it happening increase exponentially.&lt;/p&gt;
&lt;p&gt;Our input \(p\) represents the probability in range \((0, 1)\) which we want to map to the real number space.
To approximate this, we apply the natural logarithm to the odds.&lt;/p&gt;
&lt;p&gt;The logistic model assumes a linear relationship between the linear model \(\mathbf{w}^T\mathbf{x}\) and the logit function&lt;/p&gt;
&lt;p&gt;\[
\text{logit}(p) = \ln \frac{p}{1-p}.
\]&lt;/p&gt;
&lt;p&gt;This function maps a value in range \((0, 1)\) to the space of real numbers.
Under this assumption, we can write&lt;/p&gt;
&lt;p&gt;\[
\text{logit}(p) = \mathbf{w}^T\mathbf{x}.
\]&lt;/p&gt;
&lt;p&gt;This assumption is reasonable because we ultimately want to predict the &lt;strong&gt;probability&lt;/strong&gt; that an event occurs.
The output should then be in the range of \((0, 1)\).
If the logit function produces output in the range of real numbers, as does our linear model \(\mathbf{w}^T\mathbf{x}\), then we ultimately want a function that maps &lt;strong&gt;from&lt;/strong&gt; the range of real numbers to &lt;strong&gt;to&lt;/strong&gt; \((0, 1)\).&lt;/p&gt;
&lt;p&gt;We can achieve this using the &lt;strong&gt;inverse&lt;/strong&gt; of the logit function, the logistic sigmoid function.
It is defined as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\sigma(z) = \frac{1}{1 + \exp(-z)},
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(z = \mathbf{w}^T\mathbf{x}\).&lt;/p&gt;
&lt;p&gt;The reason for this choice becomes more clear when plotting the function, as seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-23_17-43-13_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;The logistic sigmoid function. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;The logistic sigmoid function. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The inputs on the \(x\) axis are clamped to values between 0 and 1. It is also called a squashing function because of this property. This form is also convenient and arises naturally in many probabilistic settings. With this nonlinear activation function, the form of our model becomes&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f(\mathbf{x};\mathbf{w}) = h(\mathbf{w}^T\mathbf{x}),
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(h\) is our choice of activation function.&lt;/p&gt;
&lt;p&gt;The logistic sigmoid function also has a convenient derivative, which is useful when solving for the model parameters via gradient descent.&lt;/p&gt;
&lt;p&gt;\[
\frac{d}{dx} = \sigma(x)(1 - \sigma(x))
\]&lt;/p&gt;
&lt;h2 id=&#34;binary-classification&#34;&gt;Binary Classification&lt;/h2&gt;
&lt;p&gt;Consider a simple dataset with 2 features per data sample. Our goal is to classify the data as being one of two possible classes. For now, we&amp;rsquo;ll drop the activation function so that our model represents a line that separates both groups of data.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-23_18-10-03_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Two groups of data that are very clearly linearly separable.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Two groups of data that are very clearly linearly separable.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the binary case, we are approximating \(p(C_1|\mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{x})\).
Then \(p(C_2|\mathbf{x}) = 1 - p(C_1| \mathbf{x})\).&lt;/p&gt;
&lt;p&gt;The parameter vector \(\mathbf{w}\) is orthogonal to the decision boundary that separates the two classes. The model output is such that \(f(\mathbf{x};\mathbf{w}) = 0\) when \(\mathbf{x}\) lies on the decision boundary. If \(f(\mathbf{x};\mathbf{w}) \geq 0\) then \(\mathbf{x}\) is assigned to class 1. It is assigned to class 2 otherwise. Since we originally stated that the model should predict either a 0 or 1, we can use the model result as input to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Heaviside_step_function&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Heaviside step function&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;fitting-the-model-via-maximum-likelihood&#34;&gt;Fitting the Model via Maximum Likelihood&lt;/h3&gt;
&lt;p&gt;Let \(y_i \in \{0, 1\}\) be the target for binary classification and \(\hat{y}_i \in (0, 1)\) be the output of a logistic regression model.
The likelihood function is&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{y}|\mathbf{w}) = \prod_{i=1}^n \hat{y}_i^{y_i}(1 - \hat{y}_i)^{1 - y_i}.
\]&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s briefly take a look at \(\hat{y}_i^{y_i}(1 - \hat{y}_i)^{1 - y_i}\) to understand the output when the model correctly predicts the \(i^{\text{th}}\) sample or not.
Since the output is restricted within the range \((0, 1)\), the model will never produce 0 or 1.&lt;/p&gt;
&lt;p&gt;If the target \(y_i = 0\), then we can evaluate the subexpression \(1 - \hat{y}_i\).
In this case, the likelihood increases as \(\hat{y}_i\) decreases.&lt;/p&gt;
&lt;p&gt;If the target \(y_i = 1\), then we evaluate the subexpression \(\hat{y}_i\).&lt;/p&gt;
&lt;p&gt;When fitting this model, we want to define an error measure based on the above function.
This is done by taking the negative logarithm of \(p(\mathbf{y}|\mathbf{w})\).&lt;/p&gt;
&lt;p&gt;\[
E(\mathbf{w}) = -\ln p(\mathbf{y}|\mathbf{w}) = -\sum_{i=1}^n y_i \ln \hat{y}_i + (1 - y_i) \ln (1 - \hat{y}_i)
\]&lt;/p&gt;
&lt;p&gt;This function is commonly referred to as the &lt;strong&gt;cross-entropy&lt;/strong&gt; function.&lt;/p&gt;
&lt;p&gt;If we use this as an objective function for gradient descent with the understanding that \(\hat{y}_i = \sigma(\mathbf{w}^T \mathbf{x})\), then the gradient of the error function is&lt;/p&gt;
&lt;p&gt;\[
\nabla E(\mathbf{w}) = \sum_{i=1}^n (\hat{y}_i - y_i)\mathbf{x}_i.
\]&lt;/p&gt;
&lt;p&gt;This results in a similar update rule as linear regression, even though the problem itself is different.&lt;/p&gt;
&lt;h3 id=&#34;measuring-classifier-performance&#34;&gt;Measuring Classifier Performance&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;How do we determine how well our model is performing?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will use L1 loss because it works well with discrete outputs. L1 loss is defined as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
L_1 = \sum_{i}|\hat{y}_i - y_i|,
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(\hat{y}_i\) is the ground truth corresponding to \(\mathbf{x}_i\) and \(y_i\) is the output of our model. We can further normalize this loss to bound it between 0 and 1. Either way, a loss of 0 will indicate 100% classification accuracy.&lt;/p&gt;
&lt;h2 id=&#34;multiple-classes&#34;&gt;Multiple Classes&lt;/h2&gt;
&lt;p&gt;In multiclass logistic regression, we are dealing with target values that can take on one of \(k\) values \(y \in \{1, 2, \dots, k\}\).
If our goal is to model the distribution over \(K\) classes, a multinomial distribution is the obvious choice.
Let \(p(y|\mathbf{x};\theta)\) be a distribution over \(K\) numbers \(w_1, \dots, w_K\) that sum to 1.
Our parameterized model cannot be represented exactly by a multinomial distribution, so we will derive it so that it satisfies the same constraints.&lt;/p&gt;
&lt;p&gt;We can start by introducing \(K\) parameter vectors \(\mathbf{w}_1, \dots, \mathbf{w}_K \in \mathbb{R}^{d}\), where \(d\) is the number of input features.
Then each vector \(\mathbf{w}_k^T \mathbf{x}\) represents \(p(C_k | \mathbf{x};\mathbf{w}_k)\).
We need to &lt;em&gt;squash&lt;/em&gt; each \(\mathbf{w}_k^T \mathbf{x}\) so that the output sums to 1.&lt;/p&gt;
&lt;p&gt;This is accomplished via the &lt;strong&gt;softmax function&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;\[
p(C_k|\mathbf{x}) = \frac{\exp(\mathbf{w}_k^T \mathbf{x})}{\sum_{j} \exp(\mathbf{w}_j^T \mathbf{x})}.
\]&lt;/p&gt;
&lt;h3 id=&#34;maximum-likelihood&#34;&gt;Maximum Likelihood&lt;/h3&gt;
&lt;p&gt;The target vector for each sample is \(\mathbf{y}_i \in \mathbb{R}^{k}\).
Likewise, the output vector \(\hat{\mathbf{y}}_i\) also has \(k\) elements.&lt;/p&gt;
&lt;p&gt;The maximum likelihood function for the multiclass setting is given by&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{Y}|\mathbf{W}) = \prod_{i=1}^n \prod_{k=1}^K p(C_k|\mathbf{x}_i)^{y_{ik}} = \prod_{i=1}^n \prod_{k=1}^K \hat{y}_{ik}^{y_{ik}}.
\]&lt;/p&gt;
&lt;p&gt;\(\mathbf{Y} \in \mathbb{R}^{n \times K}\) is a matrix of all target vectors in the data set.
As with the binary case, we can take the negative logarithm of this function to produce an error function.&lt;/p&gt;
&lt;p&gt;\[
E(\mathbf{W}) = -\ln p(\mathbf{Y}|\mathbf{W}) = -\sum_{i=1}^n \sum_{k=1}^K y_{ik} \ln \hat{y}_{ik}
\]&lt;/p&gt;
&lt;p&gt;This is the &lt;strong&gt;cross-entropy&lt;/strong&gt; function for multiclass classification.&lt;/p&gt;
&lt;p&gt;The gradient of this function is given as&lt;/p&gt;
&lt;p&gt;\[
\nabla_{\mathbf{w}_j}E(\mathbf{W}) = \sum_{i=1}^n (\hat{y}_{ij} - y_{ij}) \mathbf{x}_i.
\]&lt;/p&gt;
&lt;p&gt;Part of your first assignment will be to work through the derivation of this function.
It is standard practice at this point, but it is highly valuable to understand how the result was produced.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Naive Bayes</title>
      <link>https://ajdillhoff.github.io/notes/naive_bayes/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/naive_bayes/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#making-a-decision&#34;&gt;Making a Decision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#relation-to-multinomial-logistic-regression&#34;&gt;Relation to Multinomial Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mnist-example&#34;&gt;MNIST Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gaussian-formulation&#34;&gt;Gaussian Formulation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;To motivate naive Bayes classifiers, let&amp;rsquo;s look at slightly more complex data. The MNIST dataset was one of the standard benchmarks for computer vision classification algorithms for a long time. It remains useful for educational purposes. The dataset consists of 60,000 training images and 10,000 testing images of size \(28 \times 28\). These images depict handwritten digits. For the purposes of this section, we will work with binary version of the images. This implies that each data sample has 784 binary features.&lt;/p&gt;
&lt;p&gt;We will use the naive Bayes classifier to make an image classification model which predicts the class of digit given a new image. Each image will be represented by a vector \(\mathbf{x} \in \mathbb{R}^{784}\). Modeling \(p(\mathbf{x}|C_k)\) with a multinomial distribution would require \(10^{784} - 1\) parameters since there are 10 classes and 784 features.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;Introduction/2022-02-01_18-47-49_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Samples of the MNIST training dataset.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Samples of the MNIST training dataset.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;With the naive assumption that the features are independent conditioned on the class, the number model parameters becomes \(10 \times 784\).&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;A naive Bayes classifier makes the assumption that the features of the data are independent. That is,
\[
p(\mathbf{x}|C_k, \mathbf{\theta}) = \prod_{d=1}^D p(x_i|C_k, \theta_{dk}),
\]
where \(\mathbf{\theta}_{dk}\) are the parameters for the class conditional density for class \(k\) and feature \(d\). Using the MNIST dataset, \(\mathbf{\theta}_{dk} \in \mathbb{R}^{784}\). The posterior distribution is then&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(C_k|\mathbf{x},\mathbf{\theta}) = \frac{p(C_k|\mathbf{\pi})\prod_{i=1}^Dp(x_i|C_k, \mathbf{\theta}_{dk})}{\sum_{k&amp;rsquo;}p(C_{k&amp;rsquo;}|\mathbf{\pi})\prod_{i=1}^Dp(x_i|C_{k&amp;rsquo;},\mathbf{\theta}_{dk&amp;rsquo;})}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;If we convert the input images to binary, the class conditional density \(p(\mathbf{x}|C_k, \mathbf{\theta})\) takes on the Bernoulli pdf. That is,&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(\mathbf{x}|C_k, \mathbf{\theta}) = \prod_{i=1}^D\text{Ber}(x_i|\mathbf{\theta}_{dk}).
\end{equation*}&lt;/p&gt;
&lt;p&gt;The parameter \(\theta_{dk}\) is the probability that the feature \(x_i=1\) given class \(C_k\).&lt;/p&gt;
&lt;h2 id=&#34;maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/h2&gt;
&lt;p&gt;Fitting a naive Bayes classifier is relatively simple using MLE. The likelihood is given by&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(\mathbf{X}, \mathbf{y}|\mathbf{\theta}) = \prod_{n=1}^N \mathcal{M}(y_n|\mathbf{\pi})\prod_{d=1}^D\prod_{k=1}^{K}p(x_{nd}|\mathbf{\theta}_{dk})^{\mathbb{1}(y_n=k)}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;To derive the estimators, we first take the log of the likelihood:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\ln p(\mathbf{X}, \mathbf{y}|\mathbf{\theta}) = \Bigg[\sum_{n=1}^N\sum_{k=1}^K \mathbb{1}(y_n = k)\ln \pi_k\Bigg] + \sum_{k=1}^K\sum_{d=1}^D\Bigg[\sum_{n:y_n=k}\ln p(x_{nd}|\theta_{dk})\Bigg].
\end{equation*}&lt;/p&gt;
&lt;p&gt;Thus, we have a term for the the multinomial and terms for the class-feature parameters. As with previous models that use a multinomial form, the parameter estimate for the first term is computed as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\hat{\pi}_k = \frac{N_k}{N}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;The features used in our data are binary, so the parameter estimate for each \(\hat{\theta}_{dk}\) follows the Bernoulli distribution:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\hat{\theta}_{dk} = \frac{N_{dk}}{N_{k}}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;That is, the number of times that feature \(d\) is in an example of class \(k\) divided by the total number of samples for class \(k\).&lt;/p&gt;
&lt;h2 id=&#34;making-a-decision&#34;&gt;Making a Decision&lt;/h2&gt;
&lt;p&gt;Given parameters \(\mathbf{\theta}\), how can we classify a given data sample?&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\text{arg}\max_{k}p(y=k)\prod_{i}p(x_i|y=k)
\end{equation*}&lt;/p&gt;
&lt;h2 id=&#34;relation-to-multinomial-logistic-regression&#34;&gt;Relation to Multinomial Logistic Regression&lt;/h2&gt;
&lt;p&gt;Consider some data with discrete features having one of \(K\) states, then \(x_{dk} = \mathbb{1}(x_d=k)\). The class conditional density, in this case, follows a multinomial distribution:&lt;/p&gt;
&lt;p&gt;\[
p(y=c|\mathbf{x}, \mathbf{\theta}) = \prod_{d=1}^D \prod_{k=1}^K \theta_{dck}^{x_{dk}}.
\]&lt;/p&gt;
&lt;p&gt;We can see a connection between naive Bayes and logistic regression when we evaluate the posterior over classes:&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(y=c|\mathbf{x}, \mathbf{\theta}) &amp;amp;= \frac{p(y)p(\mathbf{x}|y, \mathbf{\theta})}{p(\mathbf{x})}\\
&amp;amp;= \frac{\pi_c \prod_{d} \prod_{k} \theta_{dck}^{x_{dk}}}{\sum_{c&amp;rsquo;}\pi_{c&amp;rsquo;}\prod_{d}\prod_{k}\theta_{dc&amp;rsquo;k}^{x_{dk}}} \\
&amp;amp;= \frac{\exp[\log \pi_c + \sum_d \sum_k x_{dk}\log \theta_{dck}]}{\sum_{c&amp;rsquo;} \exp[\log \pi_{c&amp;rsquo;} + \sum_d \sum_k x_{dk} \log \theta_{dc&amp;rsquo;k}]}.
\end{align*}&lt;/p&gt;
&lt;p&gt;This has the same form as the softmax function:&lt;/p&gt;
&lt;p&gt;\[
p(y=c|\mathbf{x}, \mathbf{\theta}) = \frac{e^{\beta^{T}_c \mathbf{x} + \gamma_c}}{\sum_{c&amp;rsquo;=1}^C e^{\beta^{T}_{c&amp;rsquo;}\mathbf{x} + \gamma_{c&amp;rsquo;}}}
\]&lt;/p&gt;
&lt;h2 id=&#34;mnist-example&#34;&gt;MNIST Example&lt;/h2&gt;
&lt;p&gt;With the model definition and parameter estimates defined, we can fit and evaluate the model. Using &lt;code&gt;scikit-learn&lt;/code&gt;, we fit a Bernoulli naive Bayes classifier on the MNIST training set: &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/blob/main/logistic_regression/naive_bayes_mnist.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Naive Bayes&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;gaussian-formulation&#34;&gt;Gaussian Formulation&lt;/h2&gt;
&lt;p&gt;If our features are continuous, we would model them with univariate Gaussians.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Neural Networks</title>
      <link>https://ajdillhoff.github.io/notes/neural_networks/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/neural_networks/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#forward-pass&#34;&gt;Forward Pass&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#activation-functions&#34;&gt;Activation Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multi-class-classification&#34;&gt;Multi-Class Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backpropagation&#34;&gt;Backpropagation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#non-convex-optimization&#34;&gt;Non-Convex Optimization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://playground.tensorflow.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://playground.tensorflow.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Previously, we studied the &lt;a href=&#34;https://ajdillhoff.github.io/notes/perceptron/&#34;&gt;Perceptron&lt;/a&gt; and saw that while it made for a simple linear classifier, it is severely limited to problems that are already linearly separable.
This limitation was resolved by introduding a hidden layer with multiple perceptron units, aptly named Multi-Layer Perceptrons.&lt;/p&gt;
&lt;p&gt;In this series, we will explore the more general method of neural networks.
We will see that even a network of only two layers can approximate any continuous functional mapping to arbitrary accuracy.
Through a discussion about network architectures, activation functions, and backpropagation, we will understand and use neural networks to resolve a large number of both classification and regression tasks.&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;We will take an abstract view of neural networks in which any formulation of a neural network defines a nonlinear mapping from an input space to some output space.
This implies that our choice of activation function &lt;strong&gt;must&lt;/strong&gt; be nonlinear.
The function we create will be parameterized by some weight matrix \(W\).
Thus, any neural network can be simply formulated as&lt;/p&gt;
&lt;p&gt;\[
f(\mathbf{x};W).
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;Introduction/2022-02-12_18-08-25_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;General neural network diagram.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;General neural network diagram.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;A neural network is in part defined by its &lt;strong&gt;layers&lt;/strong&gt;, the number of &lt;strong&gt;nodes&lt;/strong&gt; in each layer, the choice of &lt;strong&gt;activation function&lt;/strong&gt;, and the choice of &lt;strong&gt;loss function&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Each layer has a number of weights equal to the number of input nodes times the number of output nodes.
This is commonly represented as a weight matrix \(W\).&lt;/p&gt;
&lt;p&gt;The network produces output through the &lt;strong&gt;forward pass&lt;/strong&gt; and computes the gradients with respect to that output in the &lt;strong&gt;backwards pass&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h2&gt;
&lt;p&gt;Computing the output is done in what is called the &lt;strong&gt;forward pass&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Our neural network function takes in an input \(\mathbf{x} \in \mathbb{R}^D\), where \(D\) is the number of features in our input space.
Each output node \(a_j\) in a hidden layer \(h_l\) has a corresponding weight vector \(\mathbf{w}_j^{(l)}\).
The intermediate output of a hidden layer \(h_l\) is a linear combination of the weights and the input followed by some nonlinear function. Node \(a_j\) of a hidden layer is computed as&lt;/p&gt;
&lt;p&gt;\[
a_j = \sum_{i=1}^d w_{ji}^{(l)} x_{i} + w_{j0}^{(l)}.
\]&lt;/p&gt;
&lt;p&gt;As with &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_regression/&#34;&gt;Linear Regression&lt;/a&gt;, we will prepend a constant 1 to our input so that the computation is simply&lt;/p&gt;
&lt;p&gt;\[
a_{j} = \sum_{i=0}^d w_{ji}^{(i)} x_i = \mathbf{w}_j^T \mathbf{x}.
\]&lt;/p&gt;
&lt;p&gt;The final output of the hidden layer is \(a_j\) transformed by a nonlinear function \(g\) such that&lt;/p&gt;
&lt;p&gt;\[
z_j = g(a_j).
\]&lt;/p&gt;
&lt;p&gt;We can combine all weight vectors for each hidden layer node into a weight matrix \(W \in \mathbb{R}^{n \times d}\), where \(n\) is the number of nodes in the layer and \(d\) is the number of input features such that&lt;/p&gt;
&lt;p&gt;\begin{equation*}
W =
\begin{bmatrix}
\mathbf{w}_1^T\\
\vdots\\
\mathbf{w}_n^T\\
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Then the output of the hidden layer can be computed as&lt;/p&gt;
&lt;p&gt;\[
\mathbf{a} = W\mathbf{x}.
\]&lt;/p&gt;
&lt;p&gt;If you instead wanted to separate the bias term, this would be&lt;/p&gt;
&lt;p&gt;\[
\mathbf{a} = W\mathbf{x} + \mathbf{b}.
\]&lt;/p&gt;
&lt;p&gt;Using the notation to specify the individual layer, we can write the output of a full network.
Let \(W^{(l)} \in \mathbb{R}^{n_{l} \times n_{l-1}}\) be the weights for layer \(l\) which have \(n_{l-1}\) input connections and \(n_{l}\) output nodes.
The activation function for layer \(l\) is given by \(g^{(l)}\).&lt;/p&gt;
&lt;p&gt;The complete forward pass of the network is computed by repeating the following step for all layers:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{z}^{(l)} = g^{(l)}(\mathbf{a}^{(l-1)}),
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\mathbf{a}^{(l-1)} = W^{(l-1)}\mathbf{z}^{(l-1)} + \mathbf{b}^{(l-1)}.
\]&lt;/p&gt;
&lt;p&gt;Once all layers have been computed, then the output of the last layer, \(\hat{\mathbf{y}}^{(L)}\) is used as the final output of the model.
For training, this is compared with some ground truth label \(\mathbf{y}\) using a loss function \(\mathcal{L}\):&lt;/p&gt;
&lt;p&gt;\[
\mathcal{L}(\hat{\mathbf{y}}, \mathbf{y}).
\]&lt;/p&gt;
&lt;h3 id=&#34;xor-example&#34;&gt;XOR Example&lt;/h3&gt;
&lt;p&gt;Consider the XOR problem. A single &lt;a href=&#34;https://ajdillhoff.github.io/notes/perceptron/&#34;&gt;Perceptron&lt;/a&gt; was unable to solve that problem.
However, adding a hidden layer and forming a multi-layer perceptron network allowed for a more complex decision boundary.
Consider the network below and produce the output given all combinations of binary input:
\(\{(0, 0), (0, 1), (1, 0), (1, 1)\}\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-13_22-36-49_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A network with 1 hidden layer that computes XOR. Source: &amp;lt;https://athitsos.utasites.cloud/courses/cse4309_fall2021/lectures/09a_neural_networks.pdf&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A network with 1 hidden layer that computes XOR. Source: &lt;a href=&#34;https://athitsos.utasites.cloud/courses/cse4309_fall2021/lectures/09a_neural_networks.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://athitsos.utasites.cloud/courses/cse4309_fall2021/lectures/09a_neural_networks.pdf&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;activation-functions&#34;&gt;Activation Functions&lt;/h2&gt;
&lt;h3 id=&#34;sigmoid-function&#34;&gt;Sigmoid Function&lt;/h3&gt;
&lt;p&gt;\[
g(x) = \frac{1}{1 + e^{-x}}
\]&lt;/p&gt;
&lt;p&gt;The logistic sigmoid function serves two purposes.
First, it allows the output of the neuron to be interpreted as a posterior probability.
Note that this is not actually a probability.
Second, it is a continuous function for which the derivative can be computed:&lt;/p&gt;
&lt;p&gt;\[
g&amp;rsquo;(x) = g(x)(1 - g(x)).
\]&lt;/p&gt;
&lt;h3 id=&#34;hyperbolic-tangent-function&#34;&gt;Hyperbolic Tangent Function&lt;/h3&gt;
&lt;p&gt;\[
\tanh x = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
\]&lt;/p&gt;
&lt;p&gt;The hyperbolic tangent function maps input to a range of \((-1, 1)\).&lt;/p&gt;
&lt;p&gt;The derivative is calculated as&lt;/p&gt;
&lt;p&gt;\[
\frac{d}{dx} \tanh x = 1 - \tanh^2 x.
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-13_23-00-27_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Hyperbolic Tangent Function. Source: Wolfram&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Hyperbolic Tangent Function. Source: Wolfram
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Key Terms&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;bias&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;activation function&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Neurons fire after input reaches some threshold.&lt;/li&gt;
&lt;li&gt;Differential activation functions necessary for backpropagation.&lt;/li&gt;
&lt;li&gt;Multi-class learning&lt;/li&gt;
&lt;li&gt;How long to train?&lt;/li&gt;
&lt;li&gt;Weight decay&lt;/li&gt;
&lt;li&gt;How many layers versus how many nodes per layer?&lt;/li&gt;
&lt;li&gt;Training&lt;/li&gt;
&lt;li&gt;Data split (train/test/val)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;multi-class-classification&#34;&gt;Multi-Class Classification&lt;/h2&gt;
&lt;p&gt;Consider an output layer of a network with \(k\) nodes.
Each of these nodes represents a decision node for a one-versus-all classifier.
For a classification task, we have to think about whether or not the sum of squares loss function works.&lt;/p&gt;
&lt;p&gt;As far as activation functions go, the logistic sigmoid function is a good way to produce some interpretation of probability.
If we treat every output node as its own one versus all classifier, then a logistic sigmoid at the end of each one would
indicate the &amp;ldquo;probability&amp;rdquo; that node \(k\) assigns class \(k\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do we formulate this in a neural network?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The number of nodes in the output layer will be \(K\), the number of classes.
Since the output of each node produces a value in range \((0, 1)\), we want to construct a target value that works with this.
Instead of assigning an integer to each class label (e.g. 1 for class 2, 2 for class 3, etc.), we will encode the target label as a \(K\) dimensional vector.
For example, if our class label is for the class 1, then the corresponding target vector will be&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{t} =
\begin{bmatrix}
1\\
0\\
\vdots\\
0
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Since the output of our final layer is also a \(K\) dimensional vector, we can compare the two using some loss function.&lt;/p&gt;
&lt;h2 id=&#34;backpropagation&#34;&gt;Backpropagation&lt;/h2&gt;
&lt;p&gt;Given a series of linear layers with nonlinear activation functions,
how can we update the weights across the entire network?&lt;/p&gt;
&lt;p&gt;The short answer is through the chain rule of differentiation.
Let&amp;rsquo;s explore this through an example.&lt;/p&gt;
&lt;p&gt;After constructing some series of hidden layers with an arbitrary number of nodes,
we will pick an error function that provides a metric of how our network performs
on a given regression or classification task.
This loss is given by \(\mathcal{L}\).&lt;/p&gt;
&lt;p&gt;Neural networks are traditionally trained using &lt;strong&gt;gradient descent&lt;/strong&gt;.
The goal is to optimize the weights such that they result in the lowest loss, or error.
This is also why our choice of loss function is important.&lt;/p&gt;
&lt;p&gt;\[
\mathbf{W}^* = \text{argmin}\frac{1}{n}\sum_{i=1}^n \mathcal{L}(f(\mathbf{x}^{(i)}; \mathbf{W}), \mathbf{y}^{(i)})
\]&lt;/p&gt;
&lt;p&gt;We first compute the gradients of the network with respect to the weights and biases.
Then, we use those gradients to update our previous values for the weights and biases.&lt;/p&gt;
&lt;h3 id=&#34;a-simple-example&#34;&gt;A Simple Example&lt;/h3&gt;
&lt;p&gt;We will first look at computing these gradients on a smaller network for binary classification with 1 hidden layer and 1 output layer.
The loss function is defined using the binary cross-entropy function:&lt;/p&gt;
&lt;p&gt;\[
\mathcal{L}(\hat{\mathbf{y}}, \mathbf{y}) = -\mathbf{y}\log \hat{\mathbf{y}} - (1 - \mathbf{y}) \log (1 - \hat{\mathbf{y}})
\]&lt;/p&gt;
&lt;p&gt;The network&amp;rsquo;s output is computed in sequence following&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{a}^{(1)} &amp;amp;= W^{(1)}\mathbf{x} + \mathbf{b}^{(1)}\\
\mathbf{z}^{(1)} &amp;amp;= g^{(1)}(\mathbf{a}^{(1)})\\
\mathbf{a}^{(2)} &amp;amp;= W^{(2)}\mathbf{z}^{(1)} + \mathbf{b}^{(2)}\\
\mathbf{z}^{(2)} &amp;amp;= g^{(2)}(\mathbf{a}^{(2)})\\
\end{align*}&lt;/p&gt;
&lt;p&gt;The goal is to compute the gradients for all weights and biases:&lt;/p&gt;
&lt;p&gt;\[
\frac{d\mathcal{L}}{dW^{(1)}},\quad \frac{d\mathcal{L}}{d\mathbf{b}^{(1)}},\quad \frac{d\mathcal{L}}{dW^{(2)}},\quad \frac{d\mathcal{L}}{d\mathbf{b}^{(2)}}.
\]&lt;/p&gt;
&lt;p&gt;Starting with the weights of the output layer:&lt;/p&gt;
&lt;p&gt;\[
\frac{d\mathcal{L}}{dW^{(2)}} = \frac{d\mathcal{L}}{d\mathbf{z}^{(2)}} \frac{d\mathbf{z}^{(2)}}{d\mathbf{a}^{(2)}} \frac{d\mathbf{a}^{(2)}}{dW^{(2)}}.
\]&lt;/p&gt;
&lt;p&gt;The first step is to compute the partial gradient of the loss function with respect to its input \(\hat{\mathbf{y}} = \mathbf{z}^{(2)}\):&lt;/p&gt;
&lt;p&gt;\[
\frac{d\mathcal{L}}{d\mathbf{z}^{(2)}} = \frac{\mathbf{z}^{(2)} - \mathbf{y}}{\mathbf{z}^{(2)}(1 - \mathbf{z}^{(2)})}.
\]&lt;/p&gt;
&lt;p&gt;Next, compute the gradient of the last layer&amp;rsquo;s activation function with respect to its input \(\mathbf{a}^{(2)}\):&lt;/p&gt;
&lt;p&gt;\[
\frac{d\mathbf{z}^{(2)}}{d\mathbf{a}^{(2)}} = \mathbf{z}^{(2)}(1 - \mathbf{z}^{(2)}).
\]&lt;/p&gt;
&lt;p&gt;Finally, we compute \(\frac{d\mathbf{a}^{(2)}}{dW^{(2)}}\):
\[
\frac{d\mathbf{a}^{(2)}}{dW^{(2)}} = \mathbf{z}^{(1)}.
\]&lt;/p&gt;
&lt;p&gt;Putting all of this together yields&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{d\mathcal{L}}{dW^{(2)}} &amp;amp;= \frac{\mathbf{z}^{(2)} - \mathbf{y}}{\mathbf{z}^{(2)}(1 - \mathbf{z}^{(2)})} * \mathbf{z}^{(2)}(1 - \mathbf{z}^{(2)}) * \mathbf{z}^{(1)}\\
&amp;amp;= \mathbf{z}^{(1)} (\mathbf{z}^{(2)} - \mathbf{y}).
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;non-convex-optimization&#34;&gt;Non-Convex Optimization&lt;/h2&gt;
&lt;p&gt;Optimizing networks with non-linearities produces a non-convex landscape.
Depending on our choice of optimization algorithm and initial starting point, the algorithm will most likely get &amp;ldquo;stuck&amp;rdquo; in some local minimum.
Consider the figure below produced by (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Li et al. 2017&lt;/a&gt;).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-31_09-48-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Loss surface of ResNet-56 (Li et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Loss surface of ResNet-56 (Li et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Li, Hao, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2017. “Visualizing the Loss Landscape of Neural Nets,” 11.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Perceptron</title>
      <link>https://ajdillhoff.github.io/notes/perceptron/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/perceptron/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-perceptron-learning-algorithm&#34;&gt;The Perceptron Learning Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#limitations-of-single-layer-perceptrons&#34;&gt;Limitations of Single-Layer Perceptrons&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;A popular example of a &lt;a href=&#34;https://ajdillhoff.github.io/notes/logistic_regression/&#34;&gt;Logistic Regression&lt;/a&gt; model is the &lt;strong&gt;perceptron&lt;/strong&gt;. Proposed by Frank Rosenblatt in 1962, the perceptron is defined as a generalized linear model:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f(\mathbf{w}^T\mathbf{\phi}(\mathbf{x})),
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(\phi\) is a basis function and \(f\) is a stepwise function with the form&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f(a) =
\begin{cases}
1, a \geq 0\\
-1, a &amp;lt; 0
\end{cases}
\end{equation*}&lt;/p&gt;
&lt;p&gt;To match this, the targets will take on a value of either 1 or -1.&lt;/p&gt;
&lt;h2 id=&#34;the-perceptron-learning-algorithm&#34;&gt;The Perceptron Learning Algorithm&lt;/h2&gt;
&lt;p&gt;Based on the stepwise function, the parameters \(\mathbf{w}\) should lead to outputs above 0 for one class and outputs below 0 for the other.
There is 0 error with a correct classification.&lt;/p&gt;
&lt;p&gt;The original formulation does not work well with gradient based optimization methods due to the fact that the derivative of the stepwise function is 0 almost everyone. To get around this, the perceptron criterion is used:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
E(\mathbf{w}) = -\sum_i \mathbf{w}^T\phi(\mathbf{x}_i)\hat{y}_i,
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(\hat{y}_i\) is the target class (either 1 or -1).&lt;/p&gt;
&lt;p&gt;An incorrect classification will minimize \(\mathbf{w}^T\phi_i y_i\). We can consider this loss only for misclassified patterns.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For each input, evaluate \(f(\mathbf{w}^T\phi(\mathbf{x}_i))\).&lt;/li&gt;
&lt;li&gt;For incorrect classifications
&lt;ul&gt;
&lt;li&gt;Add \(\phi(\mathbf{x}_i)\) to \(\mathbf{w}\) estimate for class 1&lt;/li&gt;
&lt;li&gt;Subtract \(\phi(\mathbf{x}_i)\) from \(\mathbf{w}\) for class 2.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Does not necessarily get better each step, but guaranteed to converge.&lt;/p&gt;
&lt;h2 id=&#34;limitations-of-single-layer-perceptrons&#34;&gt;Limitations of Single-Layer Perceptrons&lt;/h2&gt;
&lt;p&gt;Single layer perceptrons are limited to solving linearly separable patterns. As we have seen with a few datasets now, expecting our data to be linearly separable is wishful thinking. Minsky and Papert exposed this limitation in their book &lt;a href=&#34;https://en.wikipedia.org/wiki/Perceptrons_%28book%29&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Perceptrons: an introduction to computational geometry&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Consider the example XOR problem. It is a binary classification problem consisting of 4 data points. It is &lt;strong&gt;not&lt;/strong&gt; linearly separable as seen in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-06-27_21-22-04_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;XOR cannot be solved with a linear classifier.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;XOR cannot be solved with a linear classifier.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This is the result of using only a single Perceptron. What if we added another perceptron? A single perceptron computes \(\mathbf{w}^T + b\). It is important to transform the first perceptron&amp;rsquo;s output using a non-linear activation function, otherwise the output would be similar to that of a logistic regression model. The updated &amp;ldquo;network&amp;rdquo; is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-06-27_21-54-23_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A 2 layer perceptron for which each layer has a single node.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A 2 layer perceptron for which each layer has a single node.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The result is the same! The original input in 2D is transformed to a single dimensional output. This is then used as input to the second perceptron. The result is a linear decision boundary followed by another linear decision boundary. What if we used 2 perceptrons in the first layer? The idea is that using two linear decision boundaries in a single space would allow our model to create a more complex boundary. The updated network is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-06-27_21-58-30_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;A 2 layer perceptron for which the first layer has 2 nodes.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;A 2 layer perceptron for which the first layer has 2 nodes.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This effectively solves the XOR problem! Since each node computes a linear combination of the input, we can visualize two decision boundaries with respect to the input space.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-06-27_22-04-07_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Visualization of input space.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Visualization of input space.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Similarly, we can visualize how the data points are transformed by visualizing the space of the output layer.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-06-27_22-05-05_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Output space&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Output space
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Principal Component Analysis</title>
      <link>https://ajdillhoff.github.io/notes/principal_component_analysis/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/principal_component_analysis/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maximum-variance-formulation&#34;&gt;Maximum Variance Formulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#motivating-example&#34;&gt;Motivating Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#noise-and-redundancy&#34;&gt;Noise and Redundancy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#covariance-matrix&#34;&gt;Covariance Matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;If we have some measurements of data, but do not know the underlying dynamics, PCA can resolve this by producing a change of basis such that the dynamics are reflected upon the eigenvectors.&lt;/p&gt;
&lt;h2 id=&#34;maximum-variance-formulation&#34;&gt;Maximum Variance Formulation&lt;/h2&gt;
&lt;p&gt;Although there are several derivations of PCA. I really like the approach of projecting the data onto a lower dimensional space in order to maximize the variance of the projected data.&lt;/p&gt;
&lt;p&gt;Let \(\mathbf{X}\) be a dataset of \(N\) samples, each with \(D\) features. The goal of PCA is to project this data onto an $M$-dimensional space such that \(M &amp;lt; D\).&lt;/p&gt;
&lt;p&gt;Remember that the goal here is to maximize the variance of the &lt;strong&gt;projected data&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do we project the data?&lt;/strong&gt;
Let&amp;rsquo;s say that we want to go from $D$-dimensional space to $M$-dimensional space where \(M = 1\). Let the vector \(\mathbf{u}\) define this 1D space. If \(\mathbf{u}\) is a unit vector, then the scalar projection of a data point \(\mathbf{x}\) onto \(\mathbf{u}\) is simply \(\mathbf{u} \cdot \mathbf{x}\).&lt;/p&gt;
&lt;p&gt;Since we are maximizing variance, we need to subtract the mean sample from our data&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{\bar{x}} = \frac{1}{N}\sum_{n=1}^{N}\mathbf{x}_n
\end{equation*}&lt;/p&gt;
&lt;p&gt;Then, the mean of the projected data is \(\mathbf{u} \cdot \mathbf{\bar{x}}\).&lt;/p&gt;
&lt;p&gt;With the mean of the projected data, we can calculate the variance:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{1}{N}\sum_{n=1}^{N}\{\mathbf{u}^T\mathbf{x}_n - \mathbf{u}^T\mathbf{\bar{x}}\}^2 = \mathbf{u}^T\mathbf{S}\mathbf{u}
\end{equation*}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{S} = \frac{1}{N}\sum_{n=1}^{N}(\mathbf{x}_n - \mathbf{\bar{x}})(\mathbf{x}_n - \mathbf{\bar{x}})^T
\end{equation*}&lt;/p&gt;
&lt;p&gt;Thus, if we are maximizing the variance of the projected data, then we are maximizing \(\mathbf{u}^T\mathbf{S}\mathbf{u}\)!&lt;/p&gt;
&lt;p&gt;So this is an optimization problem, but there is one minor issue to deal with: if \(\mathbf{u}\) is not constrained, then we scale it to infinity while maximizing the function.&lt;/p&gt;
&lt;p&gt;Before, we stated that \(\mathbf{u}\) is a unit vector. Thus, the constraint is that \(\mathbf{u} \cdot \mathbf{u} = 1\).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;After reviewing Lagrangian multipliers&amp;hellip;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To enforce this constraint, we can use a lagrangian multiplier:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathcal{L}(\mathbf{u}, \lambda) = \mathbf{u}^T\mathbf{S}\mathbf{u} + \lambda(1 - \mathbf{u}^T\mathbf{u}).
\end{equation*}&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see what happens when we compute the stationary points (critical points) of the given Lagrangian function.&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\nabla_{\mathbf{u}}\mathcal{L}(\mathbf{u}, \lambda) = \mathbf{S}\mathbf{u} - \lambda \mathbf{u} = 0
\end{equation*}&lt;/p&gt;
&lt;p&gt;This implies that&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{S}\mathbf{u} = \lambda \mathbf{u}
\end{equation*}&lt;/p&gt;
&lt;p&gt;That particular equation means that \(\mathbf{u}\) is an eigenvector of \(\mathbf{S}\) with \(\lambda\) being the corresponding eigenvalue. Since \(\mathbf{u}\) is a unit vector, we can conveniently left-multiply both sides of that equation by \(\mathbf{u}^T\), resulting in:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{u}^T\mathbf{S}\mathbf{u} = \lambda
\end{equation*}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What does this mean?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;That means that the variance is maximized when \(\mathbf{u}\) is the eigenvector corresponding to the largest eigenvalue \(\lambda\).&lt;/p&gt;
&lt;p&gt;We can repeat this process to find the direction (eigenvector) corresponding to the second largest variance by considering eigenvectors that are orthogonal to the first one. This is where an orthonormal eigenbasis comes in handy.&lt;/p&gt;
&lt;h2 id=&#34;motivating-example&#34;&gt;Motivating Example&lt;/h2&gt;
&lt;p&gt;Consider a frictionless, massless spring that produces dynamics in a single direction.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2021-11-24_12-23-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Toy model of spring with ball observed from 3 perspectives.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Toy model of spring with ball observed from 3 perspectives.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;We can clearly understand that the spring will only move in a single direction. That movement reflects the underlying dynamics of this data. To understand how PCA can be useful in this situation, let&amp;rsquo;s pretend that we do not know the underlying dynamics. Instead, we observe that the data seems to go back and forth along a single axis. We observe the data over time from 3 different perspectives given by the cameras in the above figure.&lt;/p&gt;
&lt;p&gt;From the perspective of the observer, we are recording some observations in an effort to understand which dimensions are the most salient at representing the underlying mechanics. From the above figure, we know that the most important dimension in this system is that of the labeled x-axis.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How would we figure this out if we did not already know that?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each camera has its own coordinate system (basis). If each camera gives us a 2D location of the ball relative to that camera&amp;rsquo;s basis, then each sample in time gives us a 6D vector of locations.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Equivalently, every time sample is a vector that lies in an $m$-dimensional vector space spanned by an orthonormal basis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Is it possible to find another basis that best expresses the data?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Mathemtically, is there some matrix \(P\) that changes our original data \(X\) into a new representation \(Y\)?&lt;/p&gt;
&lt;p&gt;\(PX = Y\)&lt;/p&gt;
&lt;h2 id=&#34;noise-and-redundancy&#34;&gt;Noise and Redundancy&lt;/h2&gt;
&lt;p&gt;When observing real data, we will have to account for noisy measurements. Noise can come from a wide variety of sources. Being able to reduce it or filter it out is vital to understanding the underlying system.&lt;/p&gt;
&lt;p&gt;Noise is an arbitrary measurement and means nothing without some measurement of a signal. Thus, we typically measure the amount of noise in our system using a Signal-to-Noise Ratio (SNR). This assumes we have some idea of what our signal is. This is usually given based on the nature of whatever problem we are investigating. &lt;strong&gt;In the toy example, we know that the spring largely moves in a single dimension. That is the signal we expect to observe.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For arguments sake, imagine we that the recordings over time from a single camera plot the following data:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2021-11-24_14-56-35_screenshot.png&#34; &gt;


&lt;/figure&gt;

&lt;p&gt;From our advantageous position of knowing the true nature of the problem, we understand there really should be no noise. However, let&amp;rsquo;s say that our camera has some noise in interpreting the precise location of the ball at any given time. In this case, our SNR is quite high, which is good! Ideally, it would be a straight line.&lt;/p&gt;
&lt;p&gt;There is a second factor to consider: the fact that we are taking measurements from multiple sensors means that there may be some redundancy among the data collected from them. If we were to discover features that have high redundancy, we could be confident in concluding that they are highly correlated.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2021-11-24_15-01-05_screenshot.png&#34; &gt;


&lt;/figure&gt;

&lt;h2 id=&#34;covariance-matrix&#34;&gt;Covariance Matrix&lt;/h2&gt;
&lt;p&gt;Let \(X\) be a an \(m \times n\) matrix of \(n\) observations with \(m\) features per observation.&lt;/p&gt;
&lt;p&gt;We can produce a covariance matrix of the features via \(S_{X} = \frac{1}{n-1}XX^{T}\).&lt;/p&gt;
&lt;p&gt;This gives us a measurement of the correlations between all pairs of measurements.&lt;/p&gt;
&lt;p&gt;If we want to reduce redunancy between separate measurements (those in the off-diagonal of the matrix), we would then want to diagonalize this matrix. In terms of the equation \(PX=Y\), this has the effective of finding a new covariance matrix \(S_{Y}\) that is diagonal. This means that each value in the off-diagonal of \(S_{Y}\) is 0.&lt;/p&gt;
&lt;p&gt;PCA has a convenient assumption: the change of basis matrix \(P\) is orthonormal.
&lt;strong&gt;Why is this convenient?&lt;/strong&gt;
PCA can then select the normalized direction in the feature space for which the variance in the data is maximized. This is called the first &lt;em&gt;principal component&lt;/em&gt;. Because we assume \(P\) is orthonormal, the subsequent principal components must be orthogonal to the previously discovered components.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;One more thing&lt;/strong&gt;
If \(P\) is not orthonormal, then we can simply scale our eigenvectors to maximize variance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Probability Theory</title>
      <link>https://ajdillhoff.github.io/notes/probability_theory/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/probability_theory/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-simple-example&#34;&gt;A Simple Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#probability-distributions&#34;&gt;Probability Distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conditional-probability&#34;&gt;Conditional Probability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rules-of-probability&#34;&gt;Rules of Probability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-variables&#34;&gt;Random Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#continuous-variables&#34;&gt;Continuous Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#moments-of-a-distribution&#34;&gt;Moments of a Distribution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Probability theory provides a consistent framework for the quantification and manipulation of uncertainty.
It allows us to make the best decisions given the limited information we may have.
Many tasks, models, and evaluation metrics that we will explore in this course are either based on, or are inspired by, probability theory.&lt;/p&gt;
&lt;h2 id=&#34;a-simple-example&#34;&gt;A Simple Example&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Scenario:&lt;/strong&gt; There are two cookie jars, a blue one for cookies with oatmeal raisin cookies and a red one for chocolate chip cookies. The jar with oatmeal raisin cookies has 8 cookies in it. The chocolate chip jar has 10 cookies. Some monster took 2 of the chocolate chip cookies and placed them in the oatmeal raisin jar and placed 1 of the oatmeal raisin cookies in the chocolate chip jar. Thus, the oatmeal raisin jar has 2 chocolate chip and 7 oatmeal raisin. The chocolate chip jar has 8 chocolate chip and 1 oatmeal raisin.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say that we pick the chocolate chip jar 80% of the time and the oatmeal raisin jar 20% of the time. For a given jar, the cookies inside are all equally likely to be picked. We can assign this probability to &lt;strong&gt;random variables&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(J\) - The type of jar, either blue \(b\) or red \(r\).&lt;/li&gt;
&lt;li&gt;\(C\) - The type of cookie, either oatmeal \(o\) or chocolate chip \(c\).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can define the probability of picking a particular jar:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(p(J=b) = 0.2\)&lt;/li&gt;
&lt;li&gt;\(p(J = r) = 0.8\)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notice that their sum is 1.0. These probabilities can be estimated empirically given an observer recording the events. We may also define the probabilities of picking a particular type of cookie.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(p(C = o)\)&lt;/li&gt;
&lt;li&gt;\(p(C = c)\)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For each jar, the probabilities of picking the cookies must sum to 1. The tables below show the individual probabilities of picking each type of cookie from each jar. Since we can observe the actual quantities, we can define the probabilities empirically.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Chocolate Chip&lt;/th&gt;
&lt;th&gt;Oatmeal Raisin&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Blue Jar&lt;/td&gt;
&lt;td&gt;2 / 9 = 0.222&lt;/td&gt;
&lt;td&gt;7 / 9 = 0.778&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Chocolate Chip&lt;/th&gt;
&lt;th&gt;Oatmeal Raisin&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Red Jar&lt;/td&gt;
&lt;td&gt;8 / 9 = 0.889&lt;/td&gt;
&lt;td&gt;1 / 9 = 0.111&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Given these quantities, we can ask slightly more complicated questions such as &amp;ldquo;what is the probability that I will select the red jar AND take a chocolate chip cookie?&amp;rdquo; This is expressed as a &lt;strong&gt;joint probability distribution&lt;/strong&gt;, written as \(p(J = r, C = c)\). It is defined based on two events:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the prior probability of picking the red jar,&lt;/li&gt;
&lt;li&gt;the conditional probability of picking a chocolate chip cookie conditioned on the event that the red jar was picked.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\begin{equation*}
p(J=r, C=c) = p(C=c | J=r) p(J = r)
\end{equation*}&lt;/p&gt;
&lt;p&gt;This is also referred to as the &lt;strong&gt;product rule&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We already know \(p(J=r) = 0.8\). From the table above, we can see that \(p(C=c|J=r) = 0.889\). Thus, \(p(C=c,J=r) = 0.8 * 0.889 = 0.711\).&lt;/p&gt;
&lt;p&gt;If we knew nothing about the contents of the jar or the prior probabilities of selecting a jar, we could measure the joint probability empirically. This would simply be the number of times we select the red jar AND a chocolate chip cookie divided by total number of trials. For best results, perform an infinite number of trials.&lt;/p&gt;
&lt;p&gt;If instead we wanted to measure the &lt;strong&gt;conditional probability&lt;/strong&gt; \(p(C=c|J=r)\), we would simply take the number of times a chocolate chip cookie is taken from the red jar and divide by the total number of times the red jar was selected.&lt;/p&gt;
&lt;p&gt;We can construct a joint probability table given the joint probabilities of all the events listed.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Chocolate Chip&lt;/th&gt;
&lt;th&gt;Oatmeal Raisin&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Red Jar&lt;/td&gt;
&lt;td&gt;0.711&lt;/td&gt;
&lt;td&gt;0.089&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Blue Jar&lt;/td&gt;
&lt;td&gt;0.044&lt;/td&gt;
&lt;td&gt;0.156&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If you summed each row and further took the sum of the sum of rows, you would get 1. Likewise, the sum of the sum of columns would equal 1.&lt;/p&gt;
&lt;p&gt;Summing the columns for each row yields the prior probability of selecting each type of jar. Similarly, summing the rows for each column gives the prior probability of selecting that type of cookie. This is referred to as the &lt;strong&gt;marginal probability&lt;/strong&gt; or &lt;strong&gt;sum rule&lt;/strong&gt;, which is computed by summing out the other variables in the joint distribution. For example,&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(x_i) = \sum_j p(x_i, y_j)
\end{equation*}&lt;/p&gt;
&lt;p&gt;Empirically, this is computed as the number of times event \(x_i\) occurs out of ALL trials.&lt;/p&gt;
&lt;p&gt;Although the joint probabilities \(p(X, Y)\) and \(p(Y, X)\) would be written slightly differently, they are equal. With this in mind, we can set them equal to each other to derive &lt;strong&gt;Bayes&amp;rsquo; rule&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(X, Y) &amp;amp;= p(Y, X)\\
p(X|Y)p(Y) &amp;amp;= p(Y|X)p(X)\\
p(X|Y) &amp;amp;= \frac{p(Y|X)p(X)}{p(Y)}
\end{align*}&lt;/p&gt;
&lt;p&gt;In this context, \(p(X|Y)\) is referred to as the &lt;strong&gt;posterior probability&lt;/strong&gt; of event \(X\) conditioned on the fact that we know event \(Y\) has occurred. On the right, \(p(X)\) is the &lt;strong&gt;prior probability&lt;/strong&gt; of event \(X\) in the absence of any additional evidence.&lt;/p&gt;
&lt;p&gt;Two variables are &lt;strong&gt;independent&lt;/strong&gt;, then&lt;/p&gt;
&lt;p&gt;\[
p(X, Y) = p(X)p(Y)
\]&lt;/p&gt;
&lt;p&gt;If two variables are conditionally independent given a third event, then&lt;/p&gt;
&lt;p&gt;\[
p(X, Y|Z) = P(X|Z)P(Y|Z)
\]&lt;/p&gt;
&lt;h2 id=&#34;probability-distributions&#34;&gt;Probability Distributions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Events&lt;/strong&gt; come from a &lt;strong&gt;space&lt;/strong&gt; of possible outcomes.&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\Omega = {1, 2, 3, 4, 5, 6}
\end{equation*}&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;measureable event&lt;/strong&gt; is one for which we can assign a probability.&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;event space&lt;/strong&gt; must satisfy the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It contains the empty event \(\emptyset\) and trivial event \(\Omega\)&lt;/li&gt;
&lt;li&gt;It is closed under union&lt;/li&gt;
&lt;li&gt;It is closed under complementation: if \(\alpha \in S\), so is \(\Omega - \alpha\)&lt;/li&gt;
&lt;li&gt;Statement 2 implies difference and intersection&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A &lt;strong&gt;probability distribution&lt;/strong&gt; \(P\) over \((\Omega, S)\) maps events \(S\) to real values and satisfies:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;\(P(\alpha) \geq 0\) for all \(\alpha \in S\)&lt;/li&gt;
&lt;li&gt;\(P(\Omega) = 1\)&lt;/li&gt;
&lt;li&gt;If \(\alpha,\beta \in S\) and \(\alpha \cap \beta = \emptyset\), then \(P(\alpha \cup \beta) = P(\alpha)+P(\beta)\)&lt;/li&gt;
&lt;li&gt;\(P(\emptyset) = 0\)&lt;/li&gt;
&lt;li&gt;\(P(\alpha \cup \beta) = P(\alpha) + P(\beta) - P(\alpha \cap \beta)\)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conditional-probability&#34;&gt;Conditional Probability&lt;/h2&gt;
&lt;p&gt;Defined as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
P(\beta | \alpha) = \frac{P(\alpha \cap \beta)}{P(\alpha)}
\end{equation*}&lt;/p&gt;
&lt;p&gt;The more that \(\alpha\) and \(\beta\) relate, the higher the probability.&lt;/p&gt;
&lt;h3 id=&#34;the-chain-rule-of-probability&#34;&gt;The Chain Rule of Probability&lt;/h3&gt;
&lt;p&gt;\begin{equation*}
P(\alpha \cap \beta) = P(\alpha) P(\beta | \alpha)
\end{equation*}&lt;/p&gt;
&lt;p&gt;Generally&amp;hellip;&lt;/p&gt;
&lt;p&gt;\begin{equation*}
P(\alpha_1 \cap \dotsb \cap \alpha_k) = P(\alpha_1)P(\alpha_2 | \alpha_1) \dotsm P(\alpha_k | \alpha_1 \cap \dotsb \cap \alpha_{k-1})
\end{equation*}&lt;/p&gt;
&lt;h3 id=&#34;bayes-rule&#34;&gt;Bayes&amp;rsquo; Rule&lt;/h3&gt;
&lt;p&gt;\begin{equation*}
P(\alpha | \beta) = \frac{P(\beta | \alpha)P(\alpha)}{P(\beta)}
\end{equation*}&lt;/p&gt;
&lt;p&gt;Computes the inverse conditional probability.&lt;/p&gt;
&lt;p&gt;A general conditional version of Baye&amp;rsquo;s rule:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
P(\alpha | \beta \cap \gamma) = \frac{P(\beta | \alpha \cap \gamma)P(\alpha | \gamma)}{P(\beta | \gamma)}
\end{equation*}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example: TB Tests&lt;/strong&gt;
A common example for introduction Bayes&amp;rsquo; rule is that of the test that gives 95% accuracy.
The naive assumption here is that if you receive a positive result with no prior information, then
there is a 95% chance you have the infection. This is wrong because that value is conditioned on
&lt;strong&gt;already being infected&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;rules-of-probability&#34;&gt;Rules of Probability&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Sum Rule:&lt;/strong&gt;&lt;/strong&gt; \(p(X) = \sum_{Y}p(X, Y)\)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Product Rule:&lt;/strong&gt;&lt;/strong&gt; \(p(X, Y) = p(Y|X)p(X)\)&lt;/p&gt;
&lt;h2 id=&#34;random-variables&#34;&gt;Random Variables&lt;/h2&gt;
&lt;p&gt;Allows for compact notation when talking about an event. It can also be represented as a function:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f_{\text{Grade}}
\end{equation*}&lt;/p&gt;
&lt;p&gt;maps each person in \(\Omega\) to a grade value.&lt;/p&gt;
&lt;p&gt;Random variables are commonly either &lt;strong&gt;categorical&lt;/strong&gt; or &lt;strong&gt;real numbers&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;multinoulli distribution&lt;/strong&gt; is one over \(k &amp;gt; 2\) categorical random variables.
If \(k = 2\), the distribution is called the &lt;strong&gt;Bernoulli&lt;/strong&gt; or &lt;strong&gt;binomial&lt;/strong&gt; distribution.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;marginal distribution&lt;/strong&gt; is one over a single random variable \(X\).&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;joint distribution&lt;/strong&gt; is one over a set of random variables.&lt;/p&gt;
&lt;p&gt;The marginal can be computed from a joint distribution.&lt;/p&gt;
&lt;p&gt;\begin{equation*}
P(x) = \sum_{y}P(x, y)
\end{equation*}&lt;/p&gt;
&lt;h2 id=&#34;continuous-variables&#34;&gt;Continuous Variables&lt;/h2&gt;
&lt;p&gt;The introductory example looked at events that take on discrete values. That is, we either selected a cookie or did not. Most of the problems we will deal with in this course involve continuous values. In this case, we are concerned with intervals that the values may take on. If we consider a small differential of our random variable \(x\) as \(\delta x\), we can compute the probability density \(p(x)\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-23_13-11-32_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;PDF (p(x)) and CDF (P(x)). Source: Bishop&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;PDF (p(x)) and CDF (P(x)). Source: Bishop
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;With this differential \(\delta x\), we can compute the probability that \(x\) lies on some interval \((a, b)\):&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(a \leq x \leq b) = \int_{a}^{b} p(x) dx
\end{equation*}&lt;/p&gt;
&lt;p&gt;As with discrete probability distributions, the probability density must sum to 1 and cannot take a negative value. That is&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(x) &amp;amp;\geq 0\\
\int_{-\infty}^{\infty}p(x)dx &amp;amp;= 1
\end{align*}&lt;/p&gt;
&lt;p&gt;In the plot above, \(p(x)\) is the probability density function (pdf) and \(P(x)\) is the cumulative distribution function (cdf). It is possible for a pdf to have a value greater than 1, as long as integrals over any interval are less than or equal to 1.&lt;/p&gt;
&lt;p&gt;The cumulative distribution function \(P(x)\) is the probability that \(x\) lies in the interval \((-\infty, z)\), given by&lt;/p&gt;
&lt;p&gt;\begin{equation*}
P(z) = \int_{\infty}^{z} p(x)dx.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Note that the derivative of the cdf is equal to the pdf.&lt;/p&gt;
&lt;p&gt;The product rule for continuous probability distributions takes on the same form as that of discrete distributions. The sum rule is written in terms of integration:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(x) = \int p(x, y)dy.
\end{equation*}&lt;/p&gt;
&lt;h2 id=&#34;moments-of-a-distribution&#34;&gt;Moments of a Distribution&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;moment&lt;/strong&gt; of a function describes a quantitative measurement related to its graph. With respect to probability densities, the $k$th moment of \(p(x)\) is defined as \(\mathbb{E}[x^k]\). The first moment is the &lt;strong&gt;mean&lt;/strong&gt; of the distribution, the second moment is the &lt;strong&gt;variance&lt;/strong&gt;, and the third moment is the &lt;strong&gt;skewness&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Three extremely important statistics for any probability distribution are the average, variance, and covariance.&lt;/p&gt;
&lt;h3 id=&#34;expectation&#34;&gt;Expectation&lt;/h3&gt;
&lt;p&gt;The average of a function \(f(x)\) under a probability distribution \(p(x)\) is referred to as the &lt;strong&gt;expectation&lt;/strong&gt; of \(f(x)\), written as \(\mathbb{E}[f]\). The expectation for discrete and continuous distributions are&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbb{E}[f] &amp;amp;= \sum_x p(x)f(x) \text{ and}\\
\mathbb{E}[f] &amp;amp;= \int p(x)f(x)dx,
\end{align*}&lt;/p&gt;
&lt;p&gt;respectively.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_17-56-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Expectation of rolling a d6 over ~1800 trials converges to 3.5. Source: [Seeing Theory](https://seeing-theory.brown.edu/)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Expectation of rolling a d6 over ~1800 trials converges to 3.5. Source: &lt;a href=&#34;https://seeing-theory.brown.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Seeing Theory&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The &lt;strong&gt;mean value&lt;/strong&gt; for a discrete and continuous probability distribution is define as&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbb{E}[f] &amp;amp;= \sum_x p(x)x \text{ and}\\
\mathbb{E}[f] &amp;amp;= \int_{-\infty}^{\infty} p(x)xdx,
\end{align*}&lt;/p&gt;
&lt;p&gt;respectively.&lt;/p&gt;
&lt;p&gt;Empirically, we can approximate this quantity given \(N\) samples as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbb{E}[f] \approx \frac{1}{N}\sum_{i=1}^{N}f(x_i).
\end{equation*}&lt;/p&gt;
&lt;h3 id=&#34;variance&#34;&gt;Variance&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;variance&lt;/strong&gt; of a function \(f(x)\) under a probability distribution \(p(x)\) measures how much variability is in \(f(x)\) around the expected value \(\mathbb{E}[f(x)]\) and is defined by&lt;/p&gt;
&lt;p&gt;\begin{align*}
\text{var}[f] &amp;amp;= \mathbb{E}[(f(x) - \mathbb{E}[f(x)])^2]\\
&amp;amp;= \mathbb{E}[f(x)^2] - \mathbb{E}[f(x)]^2.
\end{align*}&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_18-02-03_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Variance of drawing cars with values 1-10 100 trials converges to 8.79. True variance is 8.25. Source: [Seeing Theory](https://seeing-theory.brown.edu/)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Variance of drawing cars with values 1-10 100 trials converges to 8.79. True variance is 8.25. Source: &lt;a href=&#34;https://seeing-theory.brown.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Seeing Theory&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;covariance&#34;&gt;Covariance&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;covariance&lt;/strong&gt; of two random variables \(x\) and \(y\) provides a measure of dependence between the two variables. This implies that the covariance between two independent variables is 0.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\text{cov}[\mathbf{x},\mathbf{y}] &amp;amp;= \mathbf{E}_{\mathbf{x},\mathbf{y}}[\{\mathbf{x} - \mathbb{E}[\mathbf{x}]\}\{\mathbf{y}^T - \mathbb{E}[\mathbf{y}^T]\}]\\
&amp;amp;= \mathbb{E}_{\mathbf{x},\mathbf{y}}[\mathbf{x}\mathbf{y}^T] - \mathbb{E}[\mathbf{x}]\mathbb{E}[\mathbf{y}^T].
\end{align*}&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_18-13-52_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Plot of 2D data with negative covariance. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Plot of 2D data with negative covariance. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_18-14-22_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Plot of 2D data with approximately 0 covariance. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Plot of 2D data with approximately 0 covariance. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_18-14-45_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Plot of data with positive covariance. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Plot of data with positive covariance. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;correlation&#34;&gt;Correlation&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;correlation&lt;/strong&gt; between two random variables \(x\) and \(y\) relates to their covariance, but it is normalized to lie between -1 and 1.&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\text{corr}[\mathbf{x},\mathbf{y}] = \frac{\text{cov}[\mathbf{x},\mathbf{y}]}{\sqrt{\text{var}[\mathbf{x}]\text{var}[\mathbf{y}]}}
\end{equation*}&lt;/p&gt;
&lt;p&gt;The correlation between two variables will equal 1 if there is a linear relationship between them. We can then view the correlation as providing a measurement of linearity.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-29_23-01-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Sets of points with their correlation coefficients. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Sets of points with their correlation coefficients. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;limitations-of-moments&#34;&gt;Limitations of Moments&lt;/h3&gt;
&lt;p&gt;Summary statistics can be useful but do not tell the whole story of your data. When possible, it is always better to visualize the data. An example of this is the &lt;strong&gt;Anscombosaurus&lt;/strong&gt;, derived from the Anscombe&amp;rsquo;s quartet. The quartet consists of four datasets that have nearly identical summary statistics but are visually distinct. A modern version, called the Datasaurus Dozen, consists of 12 datasets that have the same summary statistics but are visually distinct.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-29_21-15-04_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Datasaurus Dozen (source: [Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing](https://www.autodeskresearch.com/publications/samestats))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Datasaurus Dozen (source: &lt;a href=&#34;https://www.autodeskresearch.com/publications/samestats&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Support Vector Machine</title>
      <link>https://ajdillhoff.github.io/notes/support_vector_machine/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/support_vector_machine/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maximum-margin-classifier&#34;&gt;Maximum Margin Classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#formulation&#34;&gt;Formulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#overlapping-class-distributions&#34;&gt;Overlapping Class Distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiclass-svm&#34;&gt;Multiclass SVM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#additional-resources&#34;&gt;Additional Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Support Vector Machines are a class of supervised learning methods primarily used for classification. Although they can be formulated for regression and outlier detection as well. Instead of optimizing a set of parameters which compress or summarize the training set, they use a small subset of the training data to compute the decision function.&lt;/p&gt;
&lt;p&gt;They rely on the data being linearly separable, so feature transformations are critical for problems in which the original representation of the data is not linearly separable.&lt;/p&gt;
&lt;h2 id=&#34;maximum-margin-classifier&#34;&gt;Maximum Margin Classifier&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with a simple classification model as we studied with &lt;a href=&#34;https://ajdillhoff.github.io/notes/logistic_regression/&#34;&gt;Logistic Regression&lt;/a&gt;. That is, we have&lt;/p&gt;
&lt;p&gt;\[
f(\mathbf{x}) = \mathbf{w}^T\phi(\mathbf{x}),
\]&lt;/p&gt;
&lt;p&gt;where \(\phi(\mathbf{x})\) is a function which transforms our original input into some new feature space. The transformed input is assumed to be linearly separable so that a decision boundary can be computed. In the original logistic regression problem, a decision boundary was found through optimization. For linearly separable data, there are an infinite number of decision boundaries that satisfy the problem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What about the quality of the decision boundary?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Is one decision boundary better than the other?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TODO:&lt;/strong&gt; Add a few plots comparing decision boundaries&lt;/p&gt;
&lt;h2 id=&#34;formulation&#34;&gt;Formulation&lt;/h2&gt;
&lt;p&gt;Given a training set \(\{\mathbf{x}_1, \dots, \mathbf{x}_n\}\) with labels \(\{y_1, \dots, y_n\}\), where \(y_i \in \{-1, 1\}\), we construct a linear model which classifies an input sample depending on the sign of the output.&lt;/p&gt;
&lt;p&gt;Our decision rule for classification, given some input \(\mathbf{x}\), is&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f(\mathbf{x}) =
\begin{cases}
1\text{ if }\mathbf{w}^T\mathbf{x} + b \geq 0\\
-1\text{ if }\mathbf{w}^T\mathbf{x} + b &amp;lt; 0
\end{cases}
\end{equation*}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How large should the margin be?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the original formulation of &lt;a href=&#34;https://ajdillhoff.github.io/notes/logistic_regression/&#34;&gt;Logistic Regression&lt;/a&gt;, we saw that the parameter vector \(\mathbf{w}\) described the &lt;strong&gt;normal&lt;/strong&gt; to the decision boundary. The distance between a given point \(\mathbf{x}\) and the decision boundary is given by&lt;/p&gt;
&lt;p&gt;\[
\frac{y_if(\mathbf{x})}{||\mathbf{w}||}.
\]&lt;/p&gt;
&lt;p&gt;We can frame this as an optimization problem: come up with a value for \(\mathbf{w}\) that maximizes the margin.&lt;/p&gt;
&lt;p&gt;\[
\text{arg max}_{\mathbf{w}, b} \frac{1}{\|\mathbf{w}\|}\min_{i} y_i (\mathbf{w}^T\phi(\mathbf{x}_i) + b)
\]&lt;/p&gt;
&lt;p&gt;We can arbitrarily scale the parameters, so we add an additional constraint that any point that lies on the boundary of the margin satisfies&lt;/p&gt;
&lt;p&gt;\[
y_i(\mathbf{w}^T\mathbf{x} + b) = 1.
\]&lt;/p&gt;
&lt;p&gt;Under this constraint, we have that all samples satisfy&lt;/p&gt;
&lt;p&gt;\[
y_i(\mathbf{w}^T\mathbf{x} + b) \geq 1.
\]&lt;/p&gt;
&lt;p&gt;That is, all positive samples with target \(1\) will produce at least a \(1\), yielding a value greater than or equal to 1. All negative samples with target \(-1\) will produce at most a \(-1\), yielding a value greater than or equal to 1.&lt;/p&gt;
&lt;p&gt;Another way of writing this is&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f(\mathbf{x}) =
\begin{cases}
1\text{ if }\mathbf{w}^T\mathbf{x}_{+} + b \geq 1\\
-1\text{ if }\mathbf{w}^T\mathbf{x}_{-} + b \leq -1,
\end{cases}
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(\mathbf{x}_+\) is a positive sample and \(\mathbf{x}_-\) is a negative sample. The decision rule can then be written as&lt;/p&gt;
&lt;p&gt;\[
y_i(\mathbf{w}^T\mathbf{x} + b) - 1 \geq 0.
\]&lt;/p&gt;
&lt;p&gt;This implies that the only samples that would yield an output of 0 are those that lie directly on the margins of the decision boundary.&lt;/p&gt;
&lt;p&gt;Given this constraint of \(y_i(\mathbf{w}^T\mathbf{x} + b) - 1 = 0\), we can derive our optimization objective.&lt;/p&gt;
&lt;p&gt;The margin can be computed via the training data. To do this, consider two data points which lie on their respective boundaries, one positive and one negative, and compute the distance between them: \(\mathbf{x}_+ - \mathbf{x}_-\). This distance with respect to our decision boundary, defined by \(\mathbf{w}\), is given by&lt;/p&gt;
&lt;p&gt;\[
(\mathbf{x}_+ - \mathbf{x}_-) \cdot \frac{\mathbf{w}}{||\mathbf{w}||}.
\]&lt;/p&gt;
&lt;p&gt;For clarity, we can rewrite this as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{1}{||\mathbf{w}||}(\mathbf{x}_{+} \cdot \mathbf{w} - \mathbf{x}_{-} \cdot \mathbf{w}).
\end{equation*}&lt;/p&gt;
&lt;p&gt;If we substitute the sample values into the equality constraint above, we can simplify this form. For the positive sample, we have \(\mathbf{w}^T\mathbf{x} = 1 - b\). For the negative sample, we get \(\mathbf{w}^T\mathbf{x} = -1 - b\). The equation above then becomes&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{1}{||\mathbf{w}||}(1 - b - (-1 - b)) = \frac{2}{||\mathbf{w}||}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Thus, our objective is to maximize \(\frac{2}{||\mathbf{w}||}\) which is equivalent to minimizing \(\frac{1}{2}||\mathbf{w}||^2\) subject to the constraints \(y_i(\mathbf{w}^T\mathbf{x}+b)\geq 1\). This is a constrainted optimization problem. As discussed previously, we can simplify such problems by introducing &lt;a href=&#34;https://ajdillhoff.github.io/notes/lagrangian_multipliers/&#34;&gt;Lagrangian Multipliers&lt;/a&gt;. Doing this produces the dual representation of our optimization objection:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
L = \frac{1}{2}||\mathbf{w}||^2 - \sum_{i=1}^n \alpha_i \big(y_i(\mathbf{w}^T\mathbf{x}_i + b) - 1\big).
\end{equation*}&lt;/p&gt;
&lt;p&gt;To solve for \(\mathbf{w}\) we compute \(\frac{\partial}{\partial \mathbf{w}}L\).&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{\partial}{\partial \mathbf{w}}L = \mathbf{w} - \sum_{i=1}^n \alpha_i y_i \mathbf{x}_i.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Setting this to 0 yields&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{w} = \sum_{i=1}^n \alpha_i y_i \mathbf{x}_i.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Doing the same for the other parameter \(b\) yields&lt;/p&gt;
&lt;p&gt;\[
0 = \sum_{i=1}^n \alpha_i y_i.
\]&lt;/p&gt;
&lt;p&gt;We can now simplify our objective function by substituting these results into it:&lt;/p&gt;
&lt;p&gt;\begin{align*}
L &amp;amp;= \frac{1}{2}\Big(\sum_{i=1}^n \alpha_i y_i \mathbf{x}_i\Big)^2 - \sum_{i=1}^n \alpha_i\Big(y_i\big((\sum_{i=1}^n\alpha_i y_i \mathbf{x}_i)^T\mathbf{x}_i + b \big) - 1 \Big)\\
&amp;amp;= \frac{1}{2}\Big(\sum_{i=1}^n \alpha_i y_i \mathbf{x}_i\Big)^2 - \Big(\sum_{i=1}^n \alpha_i y_i \mathbf{x}_i \Big)^2 - \sum_{i=1}^n \alpha_i y_i b + \sum_{i=1}^n \alpha_i\\
&amp;amp;= -\frac{1}{2} \Big(\sum_{i=1}^n \alpha_i y_i \mathbf{x}_i \Big)^2 + \sum_{i=1}^n \alpha_i\\
&amp;amp;= \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^m \alpha_i \alpha_j y_i y_j \mathbf{x}_i \cdot \mathbf{x}_j
\end{align*}&lt;/p&gt;
&lt;p&gt;Thus, the objective is dependent on the inner product of samples \(\mathbf{x}_i\) and \(\mathbf{x}_j\). If these were representations in some complex feature space, our problem would remain computationally inefficient. However, we can take advantage of &lt;a href=&#34;https://ajdillhoff.github.io/notes/kernels/&#34;&gt;Kernels&lt;/a&gt; for this.&lt;/p&gt;
&lt;p&gt;Note that, in most cases, \(\alpha_i\) will be 0 since we only consider &lt;strong&gt;support vectors&lt;/strong&gt;. That is, the points that lie on the margins of the decision boundary.&lt;/p&gt;
&lt;h2 id=&#34;overlapping-class-distributions&#34;&gt;Overlapping Class Distributions&lt;/h2&gt;
&lt;p&gt;The above formulation is fine and works with datasets that have no overlap in feature space.
That is, they are completely linearly separable.
However, it is not always the case that they will be.&lt;/p&gt;
&lt;p&gt;To account for misclassifications while still maximizing a the margin between datasets, we introduce a penalty value for points that are misclassified.
As long as there aren&amp;rsquo;t too many misclassifications, this penalty will stay relatively low while still allowing us to come up with an optimal solution.&lt;/p&gt;
&lt;p&gt;This penalty comes in the form of a &lt;strong&gt;slack variable&lt;/strong&gt; \(\xi_i \geq 0\) for each sample that is \(0\) for points that are on or inside the correct margin and \(\xi_i = |y_i - f(\mathbf{x})|\) for others.
If the point is misclassified, its slack variable will be \(\xi_i &amp;gt; 1\).&lt;/p&gt;
&lt;h2 id=&#34;multiclass-svm&#34;&gt;Multiclass SVM&lt;/h2&gt;
&lt;p&gt;Similar to our simple &lt;a href=&#34;https://ajdillhoff.github.io/notes/logistic_regression/&#34;&gt;Logistic Regression&lt;/a&gt; method, SVMs are binary classifiers by default. We can take a similar approach to extending them to multiple classes, but there are downsides to each approach.&lt;/p&gt;
&lt;p&gt;The &amp;ldquo;one-vs-all&amp;rdquo; approach entails building \(|K|\) classifiers and choose the classifier which predicts the input with the greatest margin.&lt;/p&gt;
&lt;p&gt;The &amp;ldquo;one-vs-one&amp;rdquo; approach involves building \(|K|\cdot\frac{|K| - 1}{2}\) classifiers. In this case, training each classifer will be more tractable since the amount of data required for each one is less. For example, you would have a model for class 1 vs 2, class 1 vs 3, &amp;hellip;, class 1 vs \(K\). Then repeat for class 2: 2 vs 3, 2 vs 4, &amp;hellip;, 2 vs \(|K|\), and so on.&lt;/p&gt;
&lt;p&gt;A third approach is to construct several models using a feature vector dependent on both the data and class label. When given a new input, the model computes&lt;/p&gt;
&lt;p&gt;\[
y = \text{arg}\max_{y&amp;rsquo;}\mathbf{w}^T\phi(\mathbf{x},y&amp;rsquo;).
\]&lt;/p&gt;
&lt;p&gt;The margin for this classifier is the distance between the correct class and the closest data point of any other class.&lt;/p&gt;
&lt;h2 id=&#34;additional-resources&#34;&gt;Additional Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://web.mit.edu/6.034/wwwbob/svm.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://web.mit.edu/6.034/wwwbob/svm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://ajdillhoff.github.io/notes/linear_regression/</link>
      <pubDate>Wed, 12 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/linear_regression/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#probabilistic-interpretation&#34;&gt;Probabilistic Interpretation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solving-with-normal-equations&#34;&gt;Solving with Normal Equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#another-approach-to-normal-equations&#34;&gt;Another Approach to Normal Equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fitting-polynomials&#34;&gt;Fitting Polynomials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear-basis-functions&#34;&gt;Linear Basis Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Given a dataset of observations \(\mathbf{X} \in \mathbb{R}^{n \times d}\), where \(n\) is the number of samples and \(d\) represents the number of features per sample, and corresponding target values \(\mathbf{Y} \in \mathbb{R}^n\), create a simple prediction model which predicts the target value \(\mathbf{y}\) given a new observation \(\mathbf{x}\). The classic example in this case is a linear model, a function that is a linear combination of the input features and some weights \(\mathbf{w}\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-15_13-35-19_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Plot of univariate data where the (x) values are features and (y) are observations.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Plot of univariate data where the (x) values are features and (y) are observations.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The generated data is plotted above along with the underlying true function that was used to generate it. If we already know what the true function is, our job is done. Suppose that we only have the data points (in blue). How do we go about modelling it? It is reasonable to first visualize the data and observe that it does follow a linear pattern. Thus, a linear model would be a decent model to choose.&lt;/p&gt;
&lt;p&gt;If the data followed a curve, we may decide to fit a polynomial. We will look at an example of that later on. For now, let&amp;rsquo;s formalize all of the information that we have.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\((\mathbf{x}, \mathbf{y})\) - Data points from the original dataset. Generally, \(\mathbf{x}\) is a vector of features and \(\mathbf{y}\) is the target vector. In our simple dataset above, these are both scalar values.&lt;/li&gt;
&lt;li&gt;\(\mathbf{w} = (w_0, w_1)\) - Our model parameters. Comparing to the equation \(y = mx + b\), \(w_0\) is our bias term and \(w_1\) is our slope parameter.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;making-predictions&#34;&gt;Making Predictions&lt;/h3&gt;
&lt;p&gt;Given \(\mathbf{w}\), we can make a prediction for a new data sample \(\mathbf{x} = x_1\).&lt;/p&gt;
&lt;p&gt;\[
h(\mathbf{x}; \mathbf{w}) = w_0 + w_1 x_1
\]&lt;/p&gt;
&lt;p&gt;Note that the bias term is always added to the result. We can simplify this into a more general form by appending a constant 1 (s.t. \(x_0 = 1\)) to each of our samples such that \(\mathbf{x} = (1, x_1, &amp;hellip;, x_d)\). Then, the general linear model becomes&lt;/p&gt;
&lt;p&gt;\[
h(\mathbf{x}; \mathbf{w}) = \sum_{i=0}^{d} w_i x_i = \mathbf{w}^T \mathbf{x}.
\]&lt;/p&gt;
&lt;p&gt;If our data happened to have more than 1 feature, it would be easy enough to model it appropriately using this notation.&lt;/p&gt;
&lt;h3 id=&#34;determining-fitness&#34;&gt;Determining Fitness&lt;/h3&gt;
&lt;p&gt;If we really wanted to, we could fit our model by plotting it and manually adjusting the weights until our model looked acceptable by some qualitative standard. Fortunately we won&amp;rsquo;t be doing that. Instead, we will use a quantitative measurement that provides a metric of how well our current parameters fit the data.&lt;/p&gt;
&lt;p&gt;For this, we use a &lt;strong&gt;&lt;strong&gt;cost function&lt;/strong&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;strong&gt;loss function&lt;/strong&gt;&lt;/strong&gt;. The most common one to use for this type of model is the least-squares function:&lt;/p&gt;
&lt;p&gt;\[
J(\mathbf{w}) = \frac{1}{2}\sum_{i=1}^{n}(h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i})^2.
\]&lt;/p&gt;
&lt;h3 id=&#34;stochastic-gradient-descent&#34;&gt;Stochastic Gradient Descent&lt;/h3&gt;
&lt;p&gt;Depending on the random initialization of parameters, our error varies greatly. We can observe that no matter what the chose parameters are, there is no possible way we can achieve an error of 0. The best we can do is minimize this error:&lt;/p&gt;
&lt;p&gt;\[
\min_{\mathbf{w}} J(\mathbf{w}).
\]&lt;/p&gt;
&lt;p&gt;For this, we rely on stochastic gradient descent. The basic idea is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Begin with an initial guess for \(\mathbf{w}\).&lt;/li&gt;
&lt;li&gt;Compare the prediction for sample \(\mathbf{x}^{(i)}\) with its target \(\mathbf{y}^{(i)}\).&lt;/li&gt;
&lt;li&gt;Update \(\mathbf{w}\) based on the comparison in part 2.&lt;/li&gt;
&lt;li&gt;Repeat steps 2 and 3 on the dataset until the loss has converged.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Steps 1, 3, and 4 are easy enough. What about step 2? How can we possibly know how to modify \(\mathbf{w}\) such that \(J(\mathbf{w})\) will decrease? By computing the gradient \(\frac{d}{d\mathbf{w}}J(\mathbf{w})\)! How will we know when we have arrived at a minima? When \(\nabla J(\mathbf{w}) = 0\).&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{d}{d\mathbf{w}}J(\mathbf{w}) &amp;amp;= \frac{d}{d\mathbf{w}}\frac{1}{2}(h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i})^2\\
&amp;amp;= 2 \cdot \frac{1}{2}(h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i}) \cdot \frac{d}{d\mathbf{w}} (h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i})\\
&amp;amp;= (h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i}) \cdot \frac{d}{d\mathbf{w}} (\mathbf{w}^T \mathbf{x}_{i} - \mathbf{y}_{i})\\
&amp;amp;= (h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i}) \mathbf{x}_{i}
\end{align*}&lt;/p&gt;
&lt;p&gt;The gradient represents the direction of greatest change for a function evaluated With this gradient, we can use an update rule to modify the previous parameter vector \(\mathbf{w}\):&lt;/p&gt;
&lt;p&gt;\[
\mathbf{w}_{t+1} = \mathbf{w}_{t} - \alpha \sum_{i=1}^{n} (h(\mathbf{x}_{i};\mathbf{w}_{t}) - \mathbf{y}_{i}) \mathbf{x}_{i}.
\]&lt;/p&gt;
&lt;p&gt;Here, \(\alpha\) is an update hyperparameter that allows us to control how big or small of a step our weights can take with each update. In general, a smaller value will be more likely to get stuck in local minima. However, too large of a value may never converge to any minima.&lt;/p&gt;
&lt;p&gt;Another convenience of this approach is that it is possible to update the weights based on a single sample, batch of samples, or the entire dataset. This sequential process makes optimization using very large dataset feasible.&lt;/p&gt;
&lt;h2 id=&#34;probabilistic-interpretation&#34;&gt;Probabilistic Interpretation&lt;/h2&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Probability theory is nothing but common sense reduced to calculation.&amp;rdquo; - Pierre-Simon Laplace&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;Recall Bayes&amp;rsquo; theorem:&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{w}|\mathbf{X}) = \frac{p(\mathbf{X}|\mathbf{w})p(\mathbf{w})}{p(\mathbf{X})}.
\]&lt;/p&gt;
&lt;p&gt;That is, the &lt;em&gt;posterior&lt;/em&gt; probability of the weights conditioned on the observered data \(\mathbf{X}\) is equal to the &lt;em&gt;likelihood&lt;/em&gt; of the observed data given the times the &lt;em&gt;prior&lt;/em&gt; distribution. This base notation doesn&amp;rsquo;t line up well with our problem. For our problem, we have observations \(\mathbf{Y}\) which are dependent on the input features \(\mathbf{X}\):&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{w}|\mathbf{X}, \mathbf{Y}) = \frac{p(\mathbf{Y}|\mathbf{X}, \mathbf{w}) p(\mathbf{w}|\mathbf{X})}{p(\mathbf{Y}|\mathbf{X})},
\]&lt;/p&gt;
&lt;p&gt;where \(\mathbf{X} \in \mathbb{R}^{n \times d}\) and \(\mathbf{Y} \in \mathbb{R}^n\).&lt;/p&gt;
&lt;p&gt;The choice of least squares also has statistical motivations. As discussed previously, we are making a reasonable assumption that there is some relationship between the features of the data and the observed output. This is typically modeled assume&lt;/p&gt;
&lt;p&gt;\[
\hat{\mathbf{Y}} = f(\mathbf{X}) + \epsilon.
\]&lt;/p&gt;
&lt;p&gt;Here, \(\epsilon\) is a random error term that is independent of \(\mathbf{X}\) and has 0 mean. This term represents any random noise that occurs either naturally or from sampling. It also includes any effects that are not properly captured by \(f\). Rearranging the terms of this equation to solve for \(\epsilon\) allows us to define the discrepencies in the model:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{\epsilon}_i = h(\mathbf{x}_{i}; \mathbf{w}) - \mathbf{y}_{i}.
\]&lt;/p&gt;
&lt;p&gt;If we assume that these discrepancies are independent and identically distributed with variance \(\sigma^2\) and Gaussian PDF \(f\), the likelihood of observations \(\mathbf{y}^{(i)}\) given parameters \(\mathbf{w}\) is&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) = \prod_{i=1}^{n} f(\epsilon_i; \sigma),
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
f(\epsilon_i; \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{\epsilon^2}{2\sigma^2}\Big).
\]&lt;/p&gt;
&lt;p&gt;This new parameter changes our original distribution function to&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{w}|\mathbf{X}, \mathbf{Y}, \sigma) = \frac{p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) p(\mathbf{w}|\mathbf{X}, \sigma)}{p(\mathbf{Y}|\mathbf{X}, \sigma)}.
\]&lt;/p&gt;
&lt;p&gt;Two things to note before moving on. First, the prior \(p(\mathbf{Y}|\mathbf{X}, \sigma)\) is a normalizing constant to ensure that the posterior is a valid probability distribution. Second, if we assume that all value for \(\mathbf{w}\) are equally likely, then \(p(\mathbf{w}|\mathbf{x}, \sigma)\) also becomes constant. This is a convenient assumption which implies that maximizing the posterior is equivalent to maximizing the likelihood function.&lt;/p&gt;
&lt;p&gt;With that out of the way, we can focus solely on the likelihood function. Expanding out the gaussian PDF \(f\) yields&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) = -\frac{n}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i})^2\Big).
\]&lt;/p&gt;
&lt;p&gt;We can see that maximizing \(p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma)\) is the same as minimizing the sum of squares. In practice, we use the negative log of the likelihood function since the negative logarithm is monotonically decreasing.&lt;/p&gt;
&lt;h2 id=&#34;solving-with-normal-equations&#34;&gt;Solving with Normal Equations&lt;/h2&gt;
&lt;p&gt;You may have studied the normal equations when you took Linear Algebra. The normal equations are motivated by finding approximate solutions to \(A\mathbf{x} = \mathbf{b}\). Most of the earlier part of linear algebra courses focus on finding exact solutions by solving systems of equations using Gaussian elimination (row reduction). Approximate solutions can be found by projecting the observed data points \(\mathbf{b}\) onto the column space of \(A\) and solving \(A \mathbf{x} = \hat{\mathbf{b}}\), where \(\hat{\mathbf{b}} = \text{proj}_{\text{Col} A}\mathbf{b}\). Then, \(\mathbf{b} - \hat{\mathbf{b}}\) represents a vector orthogonal to \(\text{Col}A\).&lt;/p&gt;
&lt;p&gt;Since each column vector of \(A\) is orthogonal to \(\mathbf{b} - \hat{\mathbf{b}}\), the dot product between them should be 0. Rewriting this, we get&lt;/p&gt;
&lt;p&gt;\begin{aligned}
A^T(\mathbf{b} - A\mathbf{x}) &amp;amp;= \mathbf{0}\\
A^T \mathbf{b} - A^T A \mathbf{x} &amp;amp;= \mathbf{0}.
\end{aligned}&lt;/p&gt;
&lt;p&gt;This means that each least-squares solution of \(A\mathbf{x} = \mathbf{b}\) satisfies&lt;/p&gt;
&lt;p&gt;\[
A^T A \mathbf{x} = A^T \mathbf{b}.
\]&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s take our univariate problem of \((\mathbf{x}, \mathbf{y})\) pairs. To use the normal equations to solve the least squares problem, we first change the notation just a bit as not confuse our data points and our parameters:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{X}^T \mathbf{X} \beta = \mathbf{X}^T \mathbf{y}
\]&lt;/p&gt;
&lt;p&gt;Create the design matrix \(\mathbf{X}\) where each row represents the the \(\mathbf{x}\) values. Recall that even though we only have 1 feature for \(\mathbf{x}\), we append the bias constant as \(x_0 = 1\) to account for the bias parameter. \(\mathbf{X}\) is then&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{X} =
\begin{bmatrix}
x_0^{(0)} &amp;amp; x_1^{(0)}\\
x_0^{(1)} &amp;amp; x_1^{(1)}\\
\vdots &amp;amp; \vdots \\
x_0^{(n)} &amp;amp; x_1^{(n)}
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;The parameter vector is&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\beta =
\begin{bmatrix}
\beta_0\\
\beta_1
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;The observed values are packed into \(\mathbf{y}\). We can then solve for \(\beta\) using any standard solver:&lt;/p&gt;
&lt;p&gt;\[
\beta = (\mathbf{X}^T \mathbf{X})^{-1}X^T \mathbf{y}.
\]&lt;/p&gt;
&lt;h3 id=&#34;rank-deficient-matrices&#34;&gt;Rank-Deficient matrices&lt;/h3&gt;
&lt;p&gt;In the event that the matrix \(\mathbf{X}^T \mathbf{X}\) is singular, then its inverse cannot be computed.
This implies that one or more of the features is a linear combination of the others.&lt;/p&gt;
&lt;p&gt;This can be detected by checking the rank of \(\mathbf{X}^T \mathbf{X}\) before attempting to compute the inverse.
You can also determine which features are redundant via Gaussian elimination.
The columns in the reduced matrix that do not have a pivot entry are redundant.&lt;/p&gt;
&lt;h2 id=&#34;another-approach-to-normal-equations&#34;&gt;Another Approach to Normal Equations&lt;/h2&gt;
&lt;p&gt;We can arrive at the normal equations by starting at the probabilistic perspective. Recall the likelihood function&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) = -\frac{n}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i})^2\Big).
\]&lt;/p&gt;
&lt;p&gt;Taking the natural log of this function yields&lt;/p&gt;
&lt;p&gt;\[
\ln p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) = - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(h(\mathbf{x}_{i}; \mathbf{w}) - \mathbf{y}_{i})^2 - \frac{n}{2}\ln(\sigma^2) - \frac{n}{2}\ln(2\pi).
\]&lt;/p&gt;
&lt;p&gt;As mentioned before, maximizing the likelihood function is equivalent to minimizing the sum-of-squares function. Thus, we must find the critical point of the likelihood function by computing the gradient (w.r.t. \(\mathbf{w}\)) and solving for 0:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\nabla \ln p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) &amp;amp;= \sum_{i=1}^{n}(\mathbf{w}^T\mathbf{x}_{i} - \mathbf{y}_{i})\mathbf{x}_{i}^{T}\\
&amp;amp;= \mathbf{w}^T \sum_{i=1}^{n}\mathbf{x}_i\mathbf{x}_i^T - \sum_{i=1}^{n}\mathbf{y}_{i}\mathbf{x}_{i}^{T}\\
\end{align*}&lt;/p&gt;
&lt;p&gt;Noting that \(\sum_{i=1}^{n}\mathbf{x}_i \mathbf{x}_i^T\) is simply matrix multiplication, we can use&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{X} =
\begin{bmatrix}
\mathbf{x}_1^T\\
\vdots\\
\mathbf{x}_n^T\\
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Then, \(\sum_{i=1}^{n}\mathbf{x}_i \mathbf{x}_i^T = \mathbf{X}^T \mathbf{X}\), \(\sum_{i=1}^{n}\mathbf{y}_i \mathbf{x}_i^T = \mathbf{Y}^T \mathbf{X}\), and&lt;/p&gt;
&lt;p&gt;\[
\nabla \ln p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) = \mathbf{w}^T \mathbf{X}^T \mathbf{X} - \mathbf{Y}^T \mathbf{X}.
\]&lt;/p&gt;
&lt;p&gt;Since we are finding the maximum likelihood, we set \(\nabla \ln p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) = 0\) and solve for \(\mathbf{w}\):&lt;/p&gt;
&lt;p&gt;\[
\mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}.
\]&lt;/p&gt;
&lt;p&gt;Thus, we arrive again at the normal equations and can solve this using a linear solver.&lt;/p&gt;
&lt;h2 id=&#34;fitting-polynomials&#34;&gt;Fitting Polynomials&lt;/h2&gt;
&lt;p&gt;Not every dataset can be modeled using a simple line.
Data can be exponential or logarithmic in nature.
We may also look to use &lt;a href=&#34;https://en.wikipedia.org/wiki/Spline_%28mathematics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;splines&lt;/a&gt; to model more complex data.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-01_17-08-27_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Data generated from a nonlinear function with added noise.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Data generated from a nonlinear function with added noise.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The dataset above was generated from the function as seen in red.
Using a simple linear model (blue) does not fit the data well.
For cases such as this, we can fit a polynomial to the data by changing our input data.&lt;/p&gt;
&lt;p&gt;The simple dataset above has 100 paired samples \((x_i, y_i)\).
There is only a single feature \(x_i\) for each sample.
It is trivial to determine that the shape of the data follows a cubic function.
One solution would be to raise each input to the power of 3.
This results in the function (blue) below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-01_17-30-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Solution from raising each input to the power of 3.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Solution from raising each input to the power of 3.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To fit this data, we need to add more features to our input.
Along with the original \(x_i\) features, we will also add \(x_i^2\) and \(x_i^3\).
Our data is then 3 dimensional.
The figure below shows the least squares fit using the modified data (blue).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-01_17-38-57_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Least squares fit using a polynomial model (blue).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Least squares fit using a polynomial model (blue).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;A demo of this can be found &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/blob/main/linear_regression/Linear%20Regression.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;linear-basis-functions&#34;&gt;Linear Basis Functions&lt;/h2&gt;
&lt;p&gt;Linear models are linear in their inputs.
This formulation is simple, producing models with limited representation.
Linear models can be extended as a linear combination of fixed nonlinear functions of the original features.
In the previous section, was saw that they could easily be extended to fit polynomial functions.&lt;/p&gt;
&lt;p&gt;We now consider creating a model that transforms the original input using one or more nonlinear functions.
This type of model is called a &lt;strong&gt;&lt;strong&gt;linear basis function model&lt;/strong&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;\[
h(\mathbf{x};\mathbf{w}) = \sum_{j=1}^{m} w_j\phi_j(\mathbf{x})
\]&lt;/p&gt;
&lt;p&gt;Common basis functions are the sigmoid, Gaussian, or exponential function.
If we choose the \(\sin\) function as a basis function, we can more closely fit our dataset using the least squares approach.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-01_18-46-08_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;A linear basis function model using the sin function as the choice of basis.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;A linear basis function model using the sin function as the choice of basis.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
  </channel>
</rss>
