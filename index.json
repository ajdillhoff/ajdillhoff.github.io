[{"authors":["admin"],"categories":null,"content":"I love building things and helping others build things they are passionate about. I also love distilling complex topics, clearing the path towards knowledge.\n","date":1719032400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1722038400,"objectID":"05ebe7d8c837c5d6c5fe1186ef4f04cb","permalink":"https://ajdillhoff.github.io/authors/authors/","publishdate":"2024-06-22T00:00:00-05:00","relpermalink":"/authors/authors/","section":"authors","summary":"I love building things and helping others build things they are passionate about. I also love distilling complex topics, clearing the path towards knowledge.","tags":null,"title":"Alex Dillhoff","type":"authors"},{"authors":["admin"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nIt is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum. The point of using Lorem Ipsum. distracted by the readable content of a page.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://ajdillhoff.github.io/authors/authors/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/authors/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nIt is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum.","tags":null,"title":"Alex Dillhoff","type":"authors"},{"authors":["admin"],"categories":null,"content":"I love building things and helping others build things they are passionate about. I also love distilling complex topics, clearing the path towards knowledge.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4036ab9376e924db20b9e42fb1307811","permalink":"https://ajdillhoff.github.io/authors/authors/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/authors/","section":"authors","summary":"I love building things and helping others build things they are passionate about. I also love distilling complex topics, clearing the path towards knowledge.","tags":null,"title":"Alex Dillhoff","type":"authors"},{"authors":null,"categories":null,"content":"This is the course page for CSE 3380 - Linear Algebra in Spring 2024. Here you\u0026rsquo;ll find class notes and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Section 001: Xulin Chen (xxc6289 (at) mavs (dot) uta (dot) edu) Section 002: Rishabh Mediratta (rxm5684 (at) mavs (dot) uta (dot) edu) Office Hours Days Time Location MoWeFri 12PM - 1PM ERB 651 TuTh 1PM - 2PM ERB 125 Rishabh Mediratta Days Time Tu 5:30PM - 8:30PM We 4PM - 5PM ","date":1692057600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1692057600,"objectID":"43dab0026194729643560902831198d5","permalink":"https://ajdillhoff.github.io/courses/archive/spring2024/cse3380/","publishdate":"2023-08-15T00:00:00Z","relpermalink":"/courses/archive/spring2024/cse3380/","section":"courses","summary":"Linear Algebra theory combined with applications in Engineering and Computer Science.","tags":null,"title":"CSE 3380 - Linear Algebra for CSE","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for CSE 1325 - Object-Oriented Programming in Summer 2023. Here you’ll find slides and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Xuan Wang (xuan (dot) wang2 (at) mavs (dot) uta (dot) edu) TAs across all sections are eligible to help you with assignments and other coding questions. Click here to view the TA lab schedule.\nOffice Hours Office hours are facilitated via Teams. If these times do not work for you, e-mail me to schedule an appointment.\nMonday, Wednesday, and Friday from 2PM - 3PM Tuesday and Thursday from 4:30PM - 5:30PM ","date":1689811200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1689811200,"objectID":"14223e1c38e3bdf11fdefa5e5bcfedc9","permalink":"https://ajdillhoff.github.io/courses/archive/summer2023/cse1325/","publishdate":"2023-07-20T00:00:00Z","relpermalink":"/courses/archive/summer2023/cse1325/","section":"courses","summary":"Introduction to object oriented programming using Java.","tags":null,"title":"CSE 1325 - Object Oriented Programming","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for CSE1310 - Introduction to Computers \u0026amp; Programming in Summer 2023. Here you\u0026rsquo;ll find slides and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Sachit Satyal (sxs3757 (at) mavs (dot) uta (dot) edu) Office Hours MoWeFri 2PM - 3PM and TuTh 4:30PM - 5:30PM via Teams (or by appointment).\n","date":1689552000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1689552000,"objectID":"7f035aaf21d8190ebddfaa8902ca6ab9","permalink":"https://ajdillhoff.github.io/courses/archive/summer2023/cse1310/","publishdate":"2023-07-17T00:00:00Z","relpermalink":"/courses/archive/summer2023/cse1310/","section":"courses","summary":"Introductory programming course using C.","tags":null,"title":"CSE 1310 - Introduction to Computers \u0026 Programming","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for CSE1310 - Introduction to Computers \u0026amp; Programming in Summer 2024. Here you\u0026rsquo;ll find slides and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Yan Wang (yan (dot) wang3 (at) mavs (dot) uta (dot) edu) Office Hours MoWeFri 1PM - 2PM and TuTh 4PM - 5PM via Teams (or by appointment).\n","date":1689552000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1689552000,"objectID":"ca3d2589b58f8c12d98f55a7b636f491","permalink":"https://ajdillhoff.github.io/courses/cse1310/","publishdate":"2023-07-17T00:00:00Z","relpermalink":"/courses/cse1310/","section":"courses","summary":"Introductory programming course using C.","tags":null,"title":"CSE 1310 - Introduction to Computers \u0026 Programming","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for CSE 4310 - Fundamentals of Computer Vision. Here you’ll find slides and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Section 001: Roman Strukov (rxs6055 (at) mavs (dot) uta (dot) edu) Office Hours Days Time Location MoWeFri 12PM - 1PM ERB 651 TuTh 1PM - 2PM ERB 125 ","date":1703203200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1703203200,"objectID":"f36395739fc2c06831422a4f8cec774c","permalink":"https://ajdillhoff.github.io/courses/archive/spring2024/cse4310/","publishdate":"2023-12-22T00:00:00Z","relpermalink":"/courses/archive/spring2024/cse4310/","section":"courses","summary":"Covers basic concepts in computer vision, including image formation, image filtering, feature extraction, stereo vision, and object recognition.","tags":null,"title":"CSE 4310 - Fundamentals of Computer Vision","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for CSE 4310 - Fundamentals of Computer Vision. Here you’ll find slides and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Samyam Thapa (sxt4279 (at) mavs (dot) uta (dot) edu) Office Hours Days Time Location MoWeFri 1PM - 2PM Teams TuTh 4PM - 5PM Team ","date":1703203200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1703203200,"objectID":"344cf6e05cf83fc65a6b1fb4b02c51b8","permalink":"https://ajdillhoff.github.io/courses/cse4310/","publishdate":"2023-12-22T00:00:00Z","relpermalink":"/courses/cse4310/","section":"courses","summary":"Covers basic concepts in computer vision, including image formation, image filtering, feature extraction, stereo vision, and object recognition.","tags":null,"title":"CSE 4310 - Fundamentals of Computer Vision","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for DASC 5300 - Foundations of Computing in Fall 2023. Here you\u0026rsquo;ll find class notes and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Keshav Bansal (kxb5527 (at) mavs (dot) uta (dot) edu) Office Hours All office hours are conducted via Teams by default. Please contact me for an appointment if you wish to meet in person.\nAlex Dillhoff\nMoWeFri 12PM - 1PM TuTh 11AM - 12PM Keshav Bansal\nTuTh 4:30PM - 6:30PM (via Teams) ","date":1692230400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1692230400,"objectID":"55699c11d75f687ce80ee2f270bd67e5","permalink":"https://ajdillhoff.github.io/courses/archive/fall2023/dasc5300/","publishdate":"2023-08-17T00:00:00Z","relpermalink":"/courses/archive/fall2023/dasc5300/","section":"courses","summary":"Basics of programming, data structures, and algorithms. Introduction to databases and operating systems.","tags":null,"title":"DASC 5300 - Foundations of Computing","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for CSE 1320 - Intermediate Programming in Fall 2023. Here you\u0026rsquo;ll find class notes and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Section 001: Sama Nikanfar (sxn8789 (at) mavs (dot) uta (dot) edu) Section 006: Rishabh Mediratta (rxm5684 (at) mavs (dot) uta (dot) edu) Office Hours All office hours are conducted via Teams by default. Please contact me for an appointment if you wish to meet in person.\nMoWeFri 12PM - 1PM TuTh 11AM - 12PM ","date":1692057600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1692057600,"objectID":"ea943b4d4e2e7256a4fc3cc0fbb6caff","permalink":"https://ajdillhoff.github.io/courses/archive/fall2023/cse1320/","publishdate":"2023-08-15T00:00:00Z","relpermalink":"/courses/archive/fall2023/cse1320/","section":"courses","summary":"Advanced programming concepts in C paired with introductory data structures and algorithms.","tags":null,"title":"CSE 1320 - Intermediate Programming","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for CSE 3380 - Linear Algebra in Fall 2023. Here you\u0026rsquo;ll find class notes and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Section 001: Yanjun Lyu (yxl9168 (at) mavs (dot) uta (dot) edu) Section 002: Roman Strukov (rxs6055 (at) mavs (dot) uta (dot) edu) Office Hours All office hours are conducted via Teams by default. Please contact me for an appointment if you wish to meet in person.\nMoWeFri 12PM - 1PM TuTh 11AM - 12PM ","date":1692057600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1692057600,"objectID":"849bae13a97920ec09092b8e9c95e3d2","permalink":"https://ajdillhoff.github.io/courses/archive/fall2023/cse3380/","publishdate":"2023-08-15T00:00:00Z","relpermalink":"/courses/archive/fall2023/cse3380/","section":"courses","summary":"Linear Algebra theory combined with applications in Engineering and Computer Science.","tags":null,"title":"CSE 3380 - Linear Algebra for CSE","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for CSE 6363 - Machine Learning in Fall 2023. Here you\u0026rsquo;ll find class notes and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Greeshma Jayanth (gxj4507 (at) mavs (dot) uta (dot) edu) Office Hours All office hours are conducted via Teams by default. Please contact me for an appointment if you wish to meet in person.\nMoWeFri 12PM - 1PM TuTh 11AM - 12PM ","date":1692057600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1692057600,"objectID":"f76e8dac075b3141d17b37eef1d8950a","permalink":"https://ajdillhoff.github.io/courses/archive/fall2023/cse6363/","publishdate":"2023-08-15T00:00:00Z","relpermalink":"/courses/archive/fall2023/cse6363/","section":"courses","summary":"Graduate-level machine learning topics covering the foundations up to modern publications.","tags":null,"title":"CSE 6363 - Machine Learning","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for CSE 6363 - Machine Learning in Summer 2023. Here you\u0026rsquo;ll find class notes and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Yanjun Lyu (yxl9168 (at) mavs (dot) uta (dot) edu) Xiao Shi (xiao (dot) shi (at) mavs (dot) uta (dot) edu) Office Hours All office hours are conducted via Teams by default. Please contact me for an appointment if you wish to meet in person.\nMoWeFri 2PM - 3PM TuTh 4:30PM - 5:30PM ","date":1689724800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1689724800,"objectID":"68c00b7cac3dc2a82e18e821e2a58551","permalink":"https://ajdillhoff.github.io/courses/archive/summer2023/cse6363/","publishdate":"2023-07-19T00:00:00Z","relpermalink":"/courses/archive/summer2023/cse6363/","section":"courses","summary":"Graduate-level machine learning topics covering the foundations up to modern publications.","tags":null,"title":"CSE 6363 - Machine Learning (Summer 2023)","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for CSE 4373/5373 - General Purpose GPU Programming in Spring 2024. Here you\u0026rsquo;ll find class notes and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Keshav Bansal (kxb5527 (at) mavs (dot) uta (dot) edu) Office Hours Days Time Location MoWeFri 12PM - 1PM ERB 651 TuTh 1PM - 2PM ERB 125 ","date":1692054000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1692054000,"objectID":"0bed8153b4bdf4fba56ff11b997ae2b4","permalink":"https://ajdillhoff.github.io/courses/archive/spring2024/cse5373/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/archive/spring2024/cse5373/","section":"courses","summary":"Study of general purpose computation on a GPU. Topics include GPU architecture, CUDA programming, and performance optimization.","tags":null,"title":"CSE 4373/5373 - General Purpose GPU Programming","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for CSE 5311 - Design and Analysis of Algorithms in Spring 2024. Here you\u0026rsquo;ll find class notes and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants TBD\nOffice Hours Days Time Location MoWeFri 12PM - 1PM ERB 651 TuTh 1PM - 2PM ERB 125 ","date":1710201600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1710201600,"objectID":"47318d023b89f03790b3e9f680d77bbd","permalink":"https://ajdillhoff.github.io/courses/archive/spring2024/cse5311/","publishdate":"2024-03-12T00:00:00Z","relpermalink":"/courses/archive/spring2024/cse5311/","section":"courses","summary":"Covers sorting and searching algorithms, complexity analysis, and algorithm design techniques.","tags":null,"title":"CSE 5311 - Design and Analysis of Algorithms","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for CSE 6363 - Machine Learning in Spring 2024. Here you\u0026rsquo;ll find class notes and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Section 006: Greeshma Jayanth (gxj4507 (at) mavs (dot) uta (dot) edu) Section 101: Priyank Gupta (pxg9364 (at) mavs (dot) uta (dot) edu) Office Hours Days Time Location MoWeFri 12PM - 1PM ERB 651 TuTh 1PM - 2PM ERB 125 ","date":1692057600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1692057600,"objectID":"73a7fcbf022b3b97f046062989121bf5","permalink":"https://ajdillhoff.github.io/courses/archive/spring2024/cse6363/","publishdate":"2023-08-15T00:00:00Z","relpermalink":"/courses/archive/spring2024/cse6363/","section":"courses","summary":"Graduate-level machine learning topics covering the foundations up to modern publications.","tags":null,"title":"CSE 6363 - Machine Learning","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for CSE 6363 - Machine Learning in Summer 2024. Here you\u0026rsquo;ll find class notes and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Qifeng Zhou (qxz8706 (at) mavs (dot) uta (dot) edu) Office Hours Days Time Location MoWeFri 1PM - 2PM Teams TuTh 4PM - 5PM Teams ","date":1692057600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1692057600,"objectID":"8d9b4ecfa5ebcb8f21c39196f962b138","permalink":"https://ajdillhoff.github.io/courses/cse6363/","publishdate":"2023-08-15T00:00:00Z","relpermalink":"/courses/cse6363/","section":"courses","summary":"Graduate-level machine learning topics covering the foundations up to modern publications.","tags":null,"title":"CSE 6363 - Machine Learning","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials June 4 Course Introduction, Python Review June 6 Color, Sampling and Aliasing Light and Color Sampling and Aliasing June 11 Linear Filters and Interest Points Linear Filters Interest Points June 13 Histogram of Oriented Features, Scale-Invariant Feature Transform HOG SIFT June 18 Bag of Words, Feature Matching, RANSAC Bag of Visual Words RANSAC June 20 Hough Transform, Image Segmentation Hough Transform Image Segmentation Segmentation via Clustering June 25 Segmentation by Clustering, Optical Flow Segmentation via Clustering Optical Flow June 27 CLASS CANCELLED July 2 Tracking Tracking July 4 Independence Day Holiday July 9 Camera Calibration, Stereo Vision Camera Models Stereo Vision July 11 Machine Learning Intro. Linear Regression Logistic Regression July 16 Neural Networks Neural Networks July 18 Intro. to Deep Learning Deep Learning Convolutional Neural Networks July 23 PyTorch Tutorial Neural Networks July 25 Object Detection ","date":1717714800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717714800,"objectID":"f5302f35ac4495521b6565507868b99f","permalink":"https://ajdillhoff.github.io/courses/cse4310/schedule/","publishdate":"2024-06-07T00:00:00+01:00","relpermalink":"/courses/cse4310/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials June 4 Course Introduction, Python Review June 6 Color, Sampling and Aliasing Light and Color Sampling and Aliasing June 11 Linear Filters and Interest Points Linear Filters Interest Points June 13 Histogram of Oriented Features, Scale-Invariant Feature Transform HOG SIFT June 18 Bag of Words, Feature Matching, RANSAC Bag of Visual Words RANSAC June 20 Hough Transform, Image Segmentation Hough Transform Image Segmentation Segmentation via Clustering June 25 Segmentation by Clustering, Optical Flow Segmentation via Clustering Optical Flow June 27 CLASS CANCELLED July 2 Tracking Tracking July 4 Independence Day Holiday July 9 Camera Calibration, Stereo Vision Camera Models Stereo Vision July 11 Machine Learning Intro.","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials June 5 Course Introcuction, Supervised Learning Linear Regression June 10 Supervised Learning, Logistic Regression, HW1 Out Linear Regression Logistic Regression June 12 Review of Probability Theory, Linear Discriminant Analysis Probability Theory LDA June 17 Naive Bayes, Kernels, HW2 Out Naive Bayes Kernels June 19 Juneteenth Holiday June 24 Support Vector Machine Support Vector Machine June 26 Neural Networks Neural Networks July 1 Backpropagation, Bias-Variance Tradeoff, HW3 Out Neural Networks Bias-Variance Tradeoff July 3 Decision Trees Decision Trees July 8 Deep Learning Deep Learning Convolutional Neural Networks July 10 Convolutional Neural Networks Convolutional Neural Networks July 15 Boosting Boosting Gradient Boosting July 17 PyTorch Tutorial, HW4 Out July 22 Optimization for Deep learning Optimization for Deep learning July 24 Markov Decision Processes and Reinforcement Learning Markov Decision Processes Reinforcement Learning Policy Gradient Methods July 29 Unsupervised Learning Principal Component Analysis July 31 Attention Mechanism, Transformers Recurrent Neural Networks Transformers August 5 Natural Language Processing Natural Language Processing August 7 Project Presentations ","date":1717714800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717714800,"objectID":"2a3448e80b3ddf3d6fc98a1c6bc12913","permalink":"https://ajdillhoff.github.io/courses/cse6363/schedule/","publishdate":"2024-06-07T00:00:00+01:00","relpermalink":"/courses/cse6363/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials June 5 Course Introcuction, Supervised Learning Linear Regression June 10 Supervised Learning, Logistic Regression, HW1 Out Linear Regression Logistic Regression June 12 Review of Probability Theory, Linear Discriminant Analysis Probability Theory LDA June 17 Naive Bayes, Kernels, HW2 Out Naive Bayes Kernels June 19 Juneteenth Holiday June 24 Support Vector Machine Support Vector Machine June 26 Neural Networks Neural Networks July 1 Backpropagation, Bias-Variance Tradeoff, HW3 Out Neural Networks Bias-Variance Tradeoff July 3 Decision Trees Decision Trees July 8 Deep Learning Deep Learning Convolutional Neural Networks July 10 Convolutional Neural Networks Convolutional Neural Networks July 15 Boosting Boosting Gradient Boosting July 17 PyTorch Tutorial, HW4 Out July 22 Optimization for Deep learning Optimization for Deep learning July 24 Markov Decision Processes and Reinforcement Learning Markov Decision Processes Reinforcement Learning Policy Gradient Methods July 29 Unsupervised Learning Principal Component Analysis July 31 Attention Mechanism, Transformers Recurrent Neural Networks Transformers August 5 Natural Language Processing Natural Language Processing August 7 Project Presentations ","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials March 18 Course Introduction, Algorithms, Growth Functions, and Computational Complexity Introduction to Algorithms Complexity Analysis March 20 Divide and Conquer Algorithms Divide and Conquer Substitution Method Recursion Tree Method March 25 Master Theorem and Heapsort Master Theorem Heapsort March 27 Quicksort, Sorting in Linear Time, Quiz 1 Quicksort Sorting in Linear Time April 1 Medians and Order Statistics, Hash Tables Medians and Order Statistics Hash Maps April 3 Binary Search Trees and Red-Black Trees, Checkpoint 1 Binary Search Trees Red-Black Trees April 8 Dynamic Programming Dynamic Programming April 10 Greedy Algorithms, Quiz 2 Greedy Algorithms April 15 Greedy Algorithms, Exam 1 Greedy Algorithms April 17 BFS, DFS, and Topological Sort Priority Queues Intro. to Graph Theory Topological Sort April 22 Minimum Spanning Trees, Shortest Path Algorithms Minimum Spanning Trees Single-Source Shortest Paths April 24 All-Pairs Shortest Path, Quiz 3 All-Pairs Shortest Paths April 29 Maximum Flow, Parallel Algorithms Maximum Flow Parallel Sorting Algorithms May 1 Parallel Algorithms, Intractability, Quiz 4 Parallel Merge NP-Completeness May 6 Machine Learning Algorithms, Exam 2 ","date":1710198000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710198000,"objectID":"ce2d4c7b97d99fba38fe21bc5c89063e","permalink":"https://ajdillhoff.github.io/courses/archive/spring2024/cse5311/schedule/","publishdate":"2024-03-12T00:00:00+01:00","relpermalink":"/courses/archive/spring2024/cse5311/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials March 18 Course Introduction, Algorithms, Growth Functions, and Computational Complexity Introduction to Algorithms Complexity Analysis March 20 Divide and Conquer Algorithms Divide and Conquer Substitution Method Recursion Tree Method March 25 Master Theorem and Heapsort Master Theorem Heapsort March 27 Quicksort, Sorting in Linear Time, Quiz 1 Quicksort Sorting in Linear Time April 1 Medians and Order Statistics, Hash Tables Medians and Order Statistics Hash Maps April 3 Binary Search Trees and Red-Black Trees, Checkpoint 1 Binary Search Trees Red-Black Trees April 8 Dynamic Programming Dynamic Programming April 10 Greedy Algorithms, Quiz 2 Greedy Algorithms April 15 Greedy Algorithms, Exam 1 Greedy Algorithms April 17 BFS, DFS, and Topological Sort Priority Queues Intro.","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials August 21 Course Introduction, Intro. to Python Introduction Introduction to Python August 23 Control Flow, Quiz 1 Control Flow August 28 Git Introduction to Git August 30 Comprehensions, Functions, File I/O Comprehensions September 6 OOP in Python, Quiz 2 Object-Oriented Programming in Python Example: Data Loaders in Python September 11 Additional Packages NumPy: Basics NumPy: Shape Manipulation NumPy: Copies and Views September 13 Exam 1 Review Python Review Questions September 18 Exam 1 September 20 CANCELLED September 25 Introduction to Algorithms Introduction to Algorithms September 27 Complexity Analysis Complexity Analysis October 2 Introduction to Data Structures Introduction to Data Structures October 4 Stacks, Queues, and Linked Lists Stacks and Queues October 9 Hash Maps, Quiz 3 Hash Maps October 11 Binary Search Trees Binary Search Trees October 16 Red-Black Trees Red-Black Trees October 18 Introduction to Graphs Intro. to Graphs October 23 Minimum Spanning Trees, Quiz 4 Minimum Spanning Trees October 25 Single-Source Shortest Paths Single-Source Shortest Paths October 30 Introduction to Databases Introduction to Databases November 1 Structured Query Language Structured Query Language November 6 Exam 2 Review November 8 Exam 2 November 13 Advanced SQL Structured Query Language November 15 Distributed Databases Distributed Databases November 20 NOSQL, Quiz 5 NOSQL ","date":1692226800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692226800,"objectID":"5bc8b4006d9c8088c30461b22a0cb4d6","permalink":"https://ajdillhoff.github.io/courses/archive/fall2023/dasc5300/schedule/","publishdate":"2023-08-17T00:00:00+01:00","relpermalink":"/courses/archive/fall2023/dasc5300/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials August 21 Course Introduction, Intro. to Python Introduction Introduction to Python August 23 Control Flow, Quiz 1 Control Flow August 28 Git Introduction to Git August 30 Comprehensions, Functions, File I/O Comprehensions September 6 OOP in Python, Quiz 2 Object-Oriented Programming in Python Example: Data Loaders in Python September 11 Additional Packages NumPy: Basics NumPy: Shape Manipulation NumPy: Copies and Views September 13 Exam 1 Review Python Review Questions September 18 Exam 1 September 20 CANCELLED September 25 Introduction to Algorithms Introduction to Algorithms September 27 Complexity Analysis Complexity Analysis October 2 Introduction to Data Structures Introduction to Data Structures October 4 Stacks, Queues, and Linked Lists Stacks and Queues October 9 Hash Maps, Quiz 3 Hash Maps October 11 Binary Search Trees Binary Search Trees October 16 Red-Black Trees Red-Black Trees October 18 Introduction to Graphs Intro.","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials Week 1 Course Introduction, CSE 1310 Review Introduction Introduction to C Number Systems Week 2 CSE 1310 Review Arrays Loops File I/O Week 3 Pointers and Structs Structs Pointers Week 4 Function Pointers and Dynamic Memory Allocation Pointers Dynamic Memory Allocation Week 5 Introduction to Data Structures Stacks \u0026amp; Queues Linked Lists Week 6 Exam 1 Week 7 Data Structures, Continued Binary Search Trees Hash Maps Week 8 Makefiles and Macros Makefiles Macros ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"76614a05dbe4e70575507030a08fe418","permalink":"https://ajdillhoff.github.io/courses/archive/fall2023/cse1320/schedule/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/archive/fall2023/cse1320/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials Week 1 Course Introduction, CSE 1310 Review Introduction Introduction to C Number Systems Week 2 CSE 1310 Review Arrays Loops File I/O Week 3 Pointers and Structs Structs Pointers Week 4 Function Pointers and Dynamic Memory Allocation Pointers Dynamic Memory Allocation Week 5 Introduction to Data Structures Stacks \u0026amp; Queues Linked Lists Week 6 Exam 1 Week 7 Data Structures, Continued Binary Search Trees Hash Maps Week 8 Makefiles and Macros Makefiles Macros ","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials Week 1 Course Introduction, Systems of Linear Equations, Solution Sets 1.1, 1.2, and 1.5 Week 2 Vectors, Linear Combinations, Matrix Operations 1.3, 1.4, 2.1, 2.2 Week 3 Matrix Inverse, Block Matrices, and Determinants 2.4, 3.1, 3.2 Week 4 Determinants, Vector Spaces, and Linear Independence 3.2, 4.1, 4.2, 4.3 Week 5 Exam 1 Week 6 Linear Independence, Bases, and Linear Transformations 1.8, 4.3 Week 7 Computer Graphics, Python Crash Course 2.7 Week 8 Determinants and Volume, Coordinate Systems, and Dimensionality 3.3, 4.4, 4.5 ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"e1fc4f6256de95a8c685731109c20c91","permalink":"https://ajdillhoff.github.io/courses/archive/fall2023/cse3380/schedule/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/archive/fall2023/cse3380/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials Week 1 Course Introduction, Systems of Linear Equations, Solution Sets 1.1, 1.2, and 1.5 Week 2 Vectors, Linear Combinations, Matrix Operations 1.3, 1.4, 2.1, 2.2 Week 3 Matrix Inverse, Block Matrices, and Determinants 2.4, 3.1, 3.2 Week 4 Determinants, Vector Spaces, and Linear Independence 3.2, 4.1, 4.2, 4.3 Week 5 Exam 1 Week 6 Linear Independence, Bases, and Linear Transformations 1.","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials August 21 Course Introduction August 23 Supervised Learning Linear Regression August 25 Supervised Learning Linear Regression August 28 Supervised Learning Logistic Regression August 30 Python Review, Probability Theory Probability Theory September 1 Linear Discriminant Analysis, Regularization Linear Discriminant Analysis Regularization September 6 Naive Bayes Classifiers Naive Bayes Classifiers September 8 Kernels Kernels September 11 Support Vector Machines Support Vector Machines September 13 Support Vector Machines Support Vector Machines September 15 Sequential Minimal Optimization Sequential Minimal Optimization September 18 Perceptrons Perceptron September 20 Artificial Neural Networks Artificial Neural Networks September 22 Backpropagation Artificial Neural Networks September 25 Bias-Variance Tradeoff, Model Selection, and Cross-Validation Bias-Variance Tradeoff September 27 Hidden Markov Models Hidden Markov Models September 29 Hidden Markov Models Hidden Markov Models October 4 Hidden Markov Models Hidden Markov Models October 6 Decision Trees Decision Trees October 9 Decision Trees Decision Trees October 11 Clustering October 13 Principal Component Analysis Principal Component Analysis October 16 Principal Component Analysis Principal Component Analysis October 18 Boosting Boosting October 20 Boosting Boosting October 23 Introduction to Deep Learning Deep Learning October 25 Convolutional Neural Networks Convolutional Neural Networks October 27 Convolutional Neural Networks Convolutional Neural Networks October 30 Optimization for Deep Learning Optimization for Deep Learning November 1 Recurrent Neural Networks Recurrent Neural Networks November 3 Transformers Transformers November 6 Markov Decision Processes Markov Decision Processes November 8 Markov Decision Processes Markov Decision Processes November 10 Reinforcement Learning Reinforcement Learning November 13 Policy Gradient Methods Policy Gradient Methods November 15 Natural Language Processing Natural Language Processing November 17 Recent History of LLMs November 20 CANCELLED ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"7950ebadf34f3f5a019aaff9d0faee98","permalink":"https://ajdillhoff.github.io/courses/archive/fall2023/cse6363/schedule/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/archive/fall2023/cse6363/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials August 21 Course Introduction August 23 Supervised Learning Linear Regression August 25 Supervised Learning Linear Regression August 28 Supervised Learning Logistic Regression August 30 Python Review, Probability Theory Probability Theory September 1 Linear Discriminant Analysis, Regularization Linear Discriminant Analysis Regularization September 6 Naive Bayes Classifiers Naive Bayes Classifiers September 8 Kernels Kernels September 11 Support Vector Machines Support Vector Machines September 13 Support Vector Machines Support Vector Machines September 15 Sequential Minimal Optimization Sequential Minimal Optimization September 18 Perceptrons Perceptron September 20 Artificial Neural Networks Artificial Neural Networks September 22 Backpropagation Artificial Neural Networks September 25 Bias-Variance Tradeoff, Model Selection, and Cross-Validation Bias-Variance Tradeoff September 27 Hidden Markov Models Hidden Markov Models September 29 Hidden Markov Models Hidden Markov Models October 4 Hidden Markov Models Hidden Markov Models October 6 Decision Trees Decision Trees October 9 Decision Trees Decision Trees October 11 Clustering October 13 Principal Component Analysis Principal Component Analysis October 16 Principal Component Analysis Principal Component Analysis October 18 Boosting Boosting October 20 Boosting Boosting October 23 Introduction to Deep Learning Deep Learning October 25 Convolutional Neural Networks Convolutional Neural Networks October 27 Convolutional Neural Networks Convolutional Neural Networks October 30 Optimization for Deep Learning Optimization for Deep Learning November 1 Recurrent Neural Networks Recurrent Neural Networks November 3 Transformers Transformers November 6 Markov Decision Processes Markov Decision Processes November 8 Markov Decision Processes Markov Decision Processes November 10 Reinforcement Learning Reinforcement Learning November 13 Policy Gradient Methods Policy Gradient Methods November 15 Natural Language Processing Natural Language Processing November 17 Recent History of LLMs November 20 CANCELLED ","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials Week 1 Course Introduction and Systems of Linear Equations 1.1 and 1.2 Week 2 Solution Sets, Vectors, and Linear Combinations 1.3, 1.4, and 1.5 Week 3 Matrix Operations, Block Matrices, and Determinants 2.1, 2.2, 2.4, 3.1, 3.2 Week 4 Vector Spaces and Subspaces 1.8, 2.8, 4.1, and 4.2 Week 5 Exam Week (Review and Exam) Week 6 Linear Independence, Bases, and Linear Transformations 1.8 and 4.3 Week 7 Computer Graphics, Determinants and Volumes 3.3 and 4.4 Week 8 Change of Basis 4.7 Week 9 Spring Break Week 10 Orthogonality, Gram-Schmidt, and QR Factorization 6.1, 6.2, and 6.3 Week 11 Least-Squares Solutions, Eigenvectors and Eigenvalues 5.1, 6.5, and 6.6 Week 12 Exam 2 Week 13 Diagonalization 5.3 and 7.1 Week 14 Singular Value Decomposition and Applications 7.4 Week 15 Principal Component Analysis Week 16 Exam 3 Review ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"91ed5e766e07d1668a84ae57a217c6dd","permalink":"https://ajdillhoff.github.io/courses/archive/spring2024/cse3380/schedule/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/archive/spring2024/cse3380/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials Week 1 Course Introduction and Systems of Linear Equations 1.1 and 1.2 Week 2 Solution Sets, Vectors, and Linear Combinations 1.3, 1.4, and 1.5 Week 3 Matrix Operations, Block Matrices, and Determinants 2.1, 2.2, 2.4, 3.1, 3.2 Week 4 Vector Spaces and Subspaces 1.8, 2.8, 4.1, and 4.2 Week 5 Exam Week (Review and Exam) Week 6 Linear Independence, Bases, and Linear Transformations 1.","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials January 17 Course Introduction January 19 Human Vision and Color Color January 22 Sampling and Aliasing Sampling and Aliasing January 24 Linear Filters Linear Filters January 26 Python Crash Course January 29 Interest Points Interest Points January 31 Image Features Image Features February 2 HOG and SIFT Histogram of Oriented Gradients SIFT Features February 5 Bag of Visual Words and Feature Matching Bag of Visual Words February 7 RANSAC RANSAC February 9 CANCELLED February 12 Hough Transforms Hough Transforms February 14 K-Means Clustering Slides (Athitsos, External) February 16 Image Segmentation and Active Contours Image Segmentation Active Contours February 19 Image Segmentation via Clustering Segmentation via Clustering February 21 Optical Flow Optical Flow February 23 Tracking Tracking February 26 Kalman Filters Tracking February 28 Camera Models Camera Models February 30 Supervised Learning Linear Regression Logistic Regression March 4 Camera Calibration Camera Models March 6 Stereo Vision Camera Models March 8 CANCELLED March 18 Neural Networks Neural Networks March 20 Deep Learning Deep Learning March 22 Convolutional Neural Networks Convolutional Neural Networks March 25 PyTorch Tutorial March 27 Pytorch Lightning Tutorial March 29 CANCELLED April 1 Optimization for Deep Learning Optimization for Deep Learning April 3 Object Detection April 5 Recurrent Neural Networks Recurrent Neural Networks April 8 CANCELLED: Solar Eclipse April 10 Transformers Transformers April 12 Transformers for Computer Vision April 15 Instance Segmentation April 17 Generate Adversarial Networks April 19 Diffusion Models Diffusion Models April 22 Pose Estimation April 24 Project Presentations April 26 Project Presentations April 29 Project Presentations ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"2562a13fd7b017397dd84a9d9e6c5ba2","permalink":"https://ajdillhoff.github.io/courses/archive/spring2024/cse4310/schedule/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/archive/spring2024/cse4310/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials January 17 Course Introduction January 19 Human Vision and Color Color January 22 Sampling and Aliasing Sampling and Aliasing January 24 Linear Filters Linear Filters January 26 Python Crash Course January 29 Interest Points Interest Points January 31 Image Features Image Features February 2 HOG and SIFT Histogram of Oriented Gradients SIFT Features February 5 Bag of Visual Words and Feature Matching Bag of Visual Words February 7 RANSAC RANSAC February 9 CANCELLED February 12 Hough Transforms Hough Transforms February 14 K-Means Clustering Slides (Athitsos, External) February 16 Image Segmentation and Active Contours Image Segmentation Active Contours February 19 Image Segmentation via Clustering Segmentation via Clustering February 21 Optical Flow Optical Flow February 23 Tracking Tracking February 26 Kalman Filters Tracking February 28 Camera Models Camera Models February 30 Supervised Learning Linear Regression Logistic Regression March 4 Camera Calibration Camera Models March 6 Stereo Vision Camera Models March 8 CANCELLED March 18 Neural Networks Neural Networks March 20 Deep Learning Deep Learning March 22 Convolutional Neural Networks Convolutional Neural Networks March 25 PyTorch Tutorial March 27 Pytorch Lightning Tutorial March 29 CANCELLED April 1 Optimization for Deep Learning Optimization for Deep Learning April 3 Object Detection April 5 Recurrent Neural Networks Recurrent Neural Networks April 8 CANCELLED: Solar Eclipse April 10 Transformers Transformers April 12 Transformers for Computer Vision April 15 Instance Segmentation April 17 Generate Adversarial Networks April 19 Diffusion Models Diffusion Models April 22 Pose Estimation April 24 Project Presentations April 26 Project Presentations April 29 Project Presentations ","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials January 17 Course Introduction Intro. to GPGPU Programming January 19 Heterogeneous Data Parallel Computing Heterogeneous Data Parallel Computing January 22 Environment Setup Day January 24 Multidimensional Grids and Data Multidimensional Grids and Data January 26 Multidimensional Grids and Data Multidimensional Grids and Data January 29 CUDA Compute Architecture CUDA Architecture January 31 Memory Architecture Memory Architecture February 2 Tiling and Shared Memory Memory Architecture February 5 Recitation Day February 7 GPU Performance Basics GPU Performance Basics February 9 Recitation February 12 Convolutional Pattern GPU Pattern: Convolution February 14 Profiling and Benchmarking Profiling CUDA Applications February 16 Recitation February 19 Convolutional Pattern GPU Pattern: Convolution February 21 Quiz 1 February 23 Recitation February 26 Stencil Pattern GPU Pattern: Stencils February 28 Stencil Pattern GPU Pattern: Stencils March 1 Parallel Histogram GPU Pattern: Parallel Histogram March 4 Reductions GPU Pattern: Reduction March 6 NVIDIA Visual Profiler March 8 Recitation March 18 Parallel Scan GPU Pattern: Parallel Scan March 20 Parallel Scan GPU Pattern: Parallel Scan March 22 Quiz 2 March 25 Merge GPU Pattern: Merge March 27 Merge GPU Pattern: Merge March 29 CANCELLED April 1 Parallel Sorting Parallel Sorting Algorithms April 3 Sparse Matrix Multiplication Sparse Matrix Multiplication April 5 Sparse Matrix Multiplication Sparse Matrix Multiplication April 8 CANCELLED: Solar Eclipse April 10 Parallel Graph Traversal Parallel Graph Traversal April 12 Parallel Graph Traversal Parallel Graph Traversal April 15 Deep Learning Convolutional Neural Networks April 17 Deep Learning Using cuDNN April 19 Deep Learning Using cuDNN April 22 Dynamic Parallelism Dynamic Parallelism April 24 Project Presentations April 26 Project Presentations April 29 Project Presentations ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"a35daafda98e3dc639b1246a14840fd1","permalink":"https://ajdillhoff.github.io/courses/archive/spring2024/cse5373/schedule/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/archive/spring2024/cse5373/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials January 17 Course Introduction Intro. to GPGPU Programming January 19 Heterogeneous Data Parallel Computing Heterogeneous Data Parallel Computing January 22 Environment Setup Day January 24 Multidimensional Grids and Data Multidimensional Grids and Data January 26 Multidimensional Grids and Data Multidimensional Grids and Data January 29 CUDA Compute Architecture CUDA Architecture January 31 Memory Architecture Memory Architecture February 2 Tiling and Shared Memory Memory Architecture February 5 Recitation Day February 7 GPU Performance Basics GPU Performance Basics February 9 Recitation February 12 Convolutional Pattern GPU Pattern: Convolution February 14 Profiling and Benchmarking Profiling CUDA Applications February 16 Recitation February 19 Convolutional Pattern GPU Pattern: Convolution February 21 Quiz 1 February 23 Recitation February 26 Stencil Pattern GPU Pattern: Stencils February 28 Stencil Pattern GPU Pattern: Stencils March 1 Parallel Histogram GPU Pattern: Parallel Histogram March 4 Reductions GPU Pattern: Reduction March 6 NVIDIA Visual Profiler March 8 Recitation March 18 Parallel Scan GPU Pattern: Parallel Scan March 20 Parallel Scan GPU Pattern: Parallel Scan March 22 Quiz 2 March 25 Merge GPU Pattern: Merge March 27 Merge GPU Pattern: Merge March 29 CANCELLED April 1 Parallel Sorting Parallel Sorting Algorithms April 3 Sparse Matrix Multiplication Sparse Matrix Multiplication April 5 Sparse Matrix Multiplication Sparse Matrix Multiplication April 8 CANCELLED: Solar Eclipse April 10 Parallel Graph Traversal Parallel Graph Traversal April 12 Parallel Graph Traversal Parallel Graph Traversal April 15 Deep Learning Convolutional Neural Networks April 17 Deep Learning Using cuDNN April 19 Deep Learning Using cuDNN April 22 Dynamic Parallelism Dynamic Parallelism April 24 Project Presentations April 26 Project Presentations April 29 Project Presentations ","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials January 16 Course Introduction January 18 Supervised Learning Linear Regression January 23 Logistic Regression Logistic Regression January 25 Regularization and Probability Theory Regularization Probability Theory January 30 Linear Discriminant Analysis Linear Discriminant Analysis February 1 Naive Bayes Classifier, Kernels Naive Bayes Kernels February 6 Support Vector Machine Support Vector Machine February 8 Support Vector Machine Support Vector Machine February 13 Perceptrons Perceptrons February 15 Artificial Neural Networks Neural Networks February 20 ANNs, Bias-Variance Tradeoff, Model Selection, and Cross-Validation Neural Networks Bias-Variance Tradeoff February 22 Decision Trees Decision Trees February 27 Random Forests and Boosting Decision Trees Boosting February 29 CANCELLED March 5 Gradient Boosting Gradient Boosting March 7 Intro. to Deep Learning Deep Learning March 19 Convolutional Neural Networks Convolutional Neural Networks March 21 Optimization for Deep Learning Optimization for Deep Learning March 26 Recurrent Neural Networks Recurrent Neural Networks March 28 Transformers Transformers April 2 Clustering April 4 Principal Component Analysis Principal Component Analysis April 9 Markov Decision Processes Markov Decision Processes April 11 Reinforcement Learning Reinforcement Learning April 16 CANCELLED April 18 Policy Gradient Methods, NLP Policy Gradient Methods Natural Language Processing April 23 Large Language Models Natural Language Processing April 25 Project Presentations April 30 Project Presentations ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"41d96bac0903dfb887275bd5fb94a4b9","permalink":"https://ajdillhoff.github.io/courses/archive/spring2024/cse6363/schedule/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/archive/spring2024/cse6363/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials January 16 Course Introduction January 18 Supervised Learning Linear Regression January 23 Logistic Regression Logistic Regression January 25 Regularization and Probability Theory Regularization Probability Theory January 30 Linear Discriminant Analysis Linear Discriminant Analysis February 1 Naive Bayes Classifier, Kernels Naive Bayes Kernels February 6 Support Vector Machine Support Vector Machine February 8 Support Vector Machine Support Vector Machine February 13 Perceptrons Perceptrons February 15 Artificial Neural Networks Neural Networks February 20 ANNs, Bias-Variance Tradeoff, Model Selection, and Cross-Validation Neural Networks Bias-Variance Tradeoff February 22 Decision Trees Decision Trees February 27 Random Forests and Boosting Decision Trees Boosting February 29 CANCELLED March 5 Gradient Boosting Gradient Boosting March 7 Intro.","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials June 4 Course Introduction, Introduction to C Introduction Introduction to C June 6 Introduction to C (Continued) Introduction to C June 11 UNIX, Code Formatting, Input/Output Examples Introduction to UNIX June 13 Control Structures Basic Control June 18 Control Structures Loops June 20 Data Types and Number Systems Number Systems June 25 Arrays Arrays June 27 Strings, Midterm Review Arrays July 2 Midterm Exam July 4 Independence Day Holiday July 9 Functions Functions July 11 File I/O File I/O July 16 File I/O File I/O July 18 Cancelled July 23 File I/O File I/O July 25 Debugging ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"7de37af4d9cf2e385b491cba140d8897","permalink":"https://ajdillhoff.github.io/courses/cse1310/schedule/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/cse1310/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials June 4 Course Introduction, Introduction to C Introduction Introduction to C June 6 Introduction to C (Continued) Introduction to C June 11 UNIX, Code Formatting, Input/Output Examples Introduction to UNIX June 13 Control Structures Basic Control June 18 Control Structures Loops June 20 Data Types and Number Systems Number Systems June 25 Arrays Arrays June 27 Strings, Midterm Review Arrays July 2 Midterm Exam July 4 Independence Day Holiday July 9 Functions Functions July 11 File I/O File I/O July 16 File I/O File I/O July 18 Cancelled July 23 File I/O File I/O July 25 Debugging ","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials June 6 Course Introduction Introduction June 8 Introduction to Java, Classes and Objects Intro. to Java Classes and Objects June 13 Classes and Objects, Inheritance Classes and Objects Inheritance June 15 CANCELLED, Quiz 1 June 20 Inheritance, UML \u0026amp; Documentation Inheritance UML \u0026amp; Documentation June 22 Interfaces, Quiz 2 Interfaces June 27 Interfaces, Midterm Exam Review Interfaces June 29 Midterm Exam July 4 US Independence Day Holiday July 6 Exception Handling, ArrayList, Quiz 3 Exception Handling ArrayList July 11 Generic Programming Generics July 13 Collections Collections July 18 GUI Programming GUI Programming July 20 GUI Programming GUI Programming July 25 Multithreading, Lambda Functions Multithreading Lambdas July 27 Introduction to C\u0026#43;\u0026#43;, Classes in C\u0026#43;\u0026#43; C\u0026#43;\u0026#43; Basics C\u0026#43;\u0026#43; Classes ","date":1689811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689811200,"objectID":"8d290555026c6d1afd0214382e8849da","permalink":"https://ajdillhoff.github.io/courses/archive/summer2023/cse1325/schedule/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/archive/summer2023/cse1325/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials June 6 Course Introduction Introduction June 8 Introduction to Java, Classes and Objects Intro. to Java Classes and Objects June 13 Classes and Objects, Inheritance Classes and Objects Inheritance June 15 CANCELLED, Quiz 1 June 20 Inheritance, UML \u0026amp; Documentation Inheritance UML \u0026amp; Documentation June 22 Interfaces, Quiz 2 Interfaces June 27 Interfaces, Midterm Exam Review Interfaces June 29 Midterm Exam July 4 US Independence Day Holiday July 6 Exception Handling, ArrayList, Quiz 3 Exception Handling ArrayList July 11 Generic Programming Generics July 13 Collections Collections July 18 GUI Programming GUI Programming July 20 GUI Programming GUI Programming July 25 Multithreading, Lambda Functions Multithreading Lambdas July 27 Introduction to C\u0026#43;\u0026#43;, Classes in C\u0026#43;\u0026#43; C\u0026#43;\u0026#43; Basics C\u0026#43;\u0026#43; Classes ","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials June 5 Course Introduction, Supervised Learning Linear Regression June 7 Supervised Learning, Python Review Logistic Regression June 12 Probability Theory, Linear Discriminant Analysis Probability Theory, LDA June 14 Naive Bayes, Overfitting \u0026amp; Regularization in Linear Models, Kernels Naive Bayes, Regularization, Kernels June 19 Juneteenth Holiday June 21 Support Vector Machines Support Vector Machines June 26 Support Vector Machines, Sequential Minimal Optimization SVM, SMO June 28 Neural Networks Neural Networks July 3 US Independence Day Extra July 5 Introduction to PyTorch, Bias-Variance Tradeoff, Model Selection, Regularization Bias and Variance July 10 Hidden Markov Models Hidden Markov Models July 12 Decision Trees Decision Trees July 17 HMM Example, DT Example, Boosting Boosting July 19 Gradient Boosting, Principal Component Analysis Gradient Boosting, Principal Component Analysis July 24 Deep Learning, Convolutional Neural Networks, Sequential Models Deep Learning, CNNs July 26 Transformers, Markov Decision Processes Transformers, Markov Decision Processes ","date":1689721200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689721200,"objectID":"4bd30469545bd8dfeb55aca56843be33","permalink":"https://ajdillhoff.github.io/courses/archive/summer2023/cse6363/schedule/","publishdate":"2023-07-19T00:00:00+01:00","relpermalink":"/courses/archive/summer2023/cse6363/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials June 5 Course Introduction, Supervised Learning Linear Regression June 7 Supervised Learning, Python Review Logistic Regression June 12 Probability Theory, Linear Discriminant Analysis Probability Theory, LDA June 14 Naive Bayes, Overfitting \u0026amp; Regularization in Linear Models, Kernels Naive Bayes, Regularization, Kernels June 19 Juneteenth Holiday June 21 Support Vector Machines Support Vector Machines June 26 Support Vector Machines, Sequential Minimal Optimization SVM, SMO June 28 Neural Networks Neural Networks July 3 US Independence Day Extra July 5 Introduction to PyTorch, Bias-Variance Tradeoff, Model Selection, Regularization Bias and Variance July 10 Hidden Markov Models Hidden Markov Models July 12 Decision Trees Decision Trees July 17 HMM Example, DT Example, Boosting Boosting July 19 Gradient Boosting, Principal Component Analysis Gradient Boosting, Principal Component Analysis July 24 Deep Learning, Convolutional Neural Networks, Sequential Models Deep Learning, CNNs July 26 Transformers, Markov Decision Processes Transformers, Markov Decision Processes ","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials June 6 Course Introduction Introduction June 8 Introduction to C Introduction to C June 13 UNIX, Code Formatting, Input/Output Examples UNIX June 15 CANCELLED June 20 Control Structures Basic Control June 22 Loops, Data Types \u0026amp; Conversions Loops June 27 Arrays, Midterm Exam Review Arrays June 29 Midterm Exam July 4 US Independence Day Holiday July 6 Arrays Arrays July 11 Number Systems Number Systems July 13 Functions Functions July 18 Benchmark Review, Debugging July 20 File I/O File I/O July 25 Binary Files, Software Development Lifecycle File I/O, SDLC July 27 Software Development Lifecycle SDLC ","date":1689548400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689548400,"objectID":"7e2fb4d0c832da5f8dc0c9e32aba5abc","permalink":"https://ajdillhoff.github.io/courses/archive/summer2023/cse1310/schedule/","publishdate":"2023-07-17T00:00:00+01:00","relpermalink":"/courses/archive/summer2023/cse1310/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials June 6 Course Introduction Introduction June 8 Introduction to C Introduction to C June 13 UNIX, Code Formatting, Input/Output Examples UNIX June 15 CANCELLED June 20 Control Structures Basic Control June 22 Loops, Data Types \u0026amp; Conversions Loops June 27 Arrays, Midterm Exam Review Arrays June 29 Midterm Exam July 4 US Independence Day Holiday July 6 Arrays Arrays July 11 Number Systems Number Systems July 13 Functions Functions July 18 Benchmark Review, Debugging July 20 File I/O File I/O July 25 Binary Files, Software Development Lifecycle File I/O, SDLC July 27 Software Development Lifecycle SDLC ","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"Books Cormen, T. H., Leiserson, C. E., Rivest, R. L., \u0026amp; Stein, C. (2022). Introduction to Algorithms, MIT press. Resources Python Programming Language ","date":1710198000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710198000,"objectID":"5798ba0482a8c7b4dede2b2526ae2552","permalink":"https://ajdillhoff.github.io/courses/archive/spring2024/cse5311/materials/","publishdate":"2024-03-12T00:00:00+01:00","relpermalink":"/courses/archive/spring2024/cse5311/materials/","section":"courses","summary":"Books Cormen, T. H., Leiserson, C. E., Rivest, R. L., \u0026amp; Stein, C. (2022). Introduction to Algorithms, MIT press. Resources Python Programming Language ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books W. Foster and L. Foster, C By Discovery (4th ed.), Scott/Jones, 2004. Stephen Prata, C Primer Plus, Addison-Wesley, 2014. Resources Submitting Assignments using GitHub Using GCC with MinGW CSE 13xx SharePoint C/C++ Reference The Linux Command Line Git Cheat Sheet Programming Practice C Etudes Project Euler C Exercises (W3 Schools) edabit Challenges exercism ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"726cd40e9b862a537aa933940e1084cd","permalink":"https://ajdillhoff.github.io/courses/archive/fall2023/cse1320/materials/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/archive/fall2023/cse1320/materials/","section":"courses","summary":"Books W. Foster and L. Foster, C By Discovery (4th ed.), Scott/Jones, 2004. Stephen Prata, C Primer Plus, Addison-Wesley, 2014. Resources Submitting Assignments using GitHub Using GCC with MinGW CSE 13xx SharePoint C/C++ Reference The Linux Command Line Git Cheat Sheet Programming Practice C Etudes Project Euler C Exercises (W3 Schools) edabit Challenges exercism ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books David Lay, Stephen Lay, and Judi McDonald, Linear Algebra and Its Applications (5th ed.), Pearson, 2016. Sheldon Axler, Linear Algebra Done Right, Springer International Publishing, 2014. Resources 3blue1brown - Essence of Linear Algebra Python Programming Language ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"9715f3bd7c2d4f9837b222d2fb5137af","permalink":"https://ajdillhoff.github.io/courses/archive/fall2023/cse3380/materials/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/archive/fall2023/cse3380/materials/","section":"courses","summary":"Books David Lay, Stephen Lay, and Judi McDonald, Linear Algebra and Its Applications (5th ed.), Pearson, 2016. Sheldon Axler, Linear Algebra Done Right, Springer International Publishing, 2014. Resources 3blue1brown - Essence of Linear Algebra Python Programming Language ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006. Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer, 2009. Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, An Introduction to Statistical Learning, Springer, 2023. Kevin Murphy, Probabilistic Machine Learning: An Introduction, MIT Press, 2022. Resources Python Programming Language PyTorch Anaconda HuggingFace ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"4bc980e7f9a26006729eab232217581d","permalink":"https://ajdillhoff.github.io/courses/archive/fall2023/cse6363/materials/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/archive/fall2023/cse6363/materials/","section":"courses","summary":"Books Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006. Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer, 2009. Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, An Introduction to Statistical Learning, Springer, 2023. Kevin Murphy, Probabilistic Machine Learning: An Introduction, MIT Press, 2022. Resources Python Programming Language PyTorch Anaconda HuggingFace ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books Charles, Severance, Python for Everybody. Al Sweigart, Automate the Boring Stuff with Python. Michael Sipser, Introduction to the Theory of Computation, Cengage Learning, 2012. Ramez Elmasri, Sham Navathe, Fundamentals of Database Systems, Pearson, 2016. Cormen, Leiserson, Rivest, Stein, Introduction to Algorithms, MIT Press, 2009. Resources Python Programming Language Anaconda SQL Programming Practice exercism.io leetcode.com ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"7d645b0caffef8da0793d2565addd7f7","permalink":"https://ajdillhoff.github.io/courses/archive/fall2023/dasc5300/materials/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/archive/fall2023/dasc5300/materials/","section":"courses","summary":"Books Charles, Severance, Python for Everybody. Al Sweigart, Automate the Boring Stuff with Python. Michael Sipser, Introduction to the Theory of Computation, Cengage Learning, 2012. Ramez Elmasri, Sham Navathe, Fundamentals of Database Systems, Pearson, 2016. Cormen, Leiserson, Rivest, Stein, Introduction to Algorithms, MIT Press, 2009. Resources Python Programming Language Anaconda SQL Programming Practice exercism.io leetcode.com ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books David Lay, Stephen Lay, and Judi McDonald, Linear Algebra and Its Applications (5th ed.), Pearson, 2016. Sheldon Axler, Linear Algebra Done Right, Springer International Publishing, 2014. Resources 3blue1brown - Essence of Linear Algebra Python Programming Language ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"5882d16998989393e0b7b5225dfa9b01","permalink":"https://ajdillhoff.github.io/courses/archive/spring2024/cse3380/materials/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/archive/spring2024/cse3380/materials/","section":"courses","summary":"Books David Lay, Stephen Lay, and Judi McDonald, Linear Algebra and Its Applications (5th ed.), Pearson, 2016. Sheldon Axler, Linear Algebra Done Right, Springer International Publishing, 2014. Resources 3blue1brown - Essence of Linear Algebra Python Programming Language ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books Richard Szeliski, Computer Vision: Algorithms and Applications, Springer, 2020. David Forsyth and Jean Ponce, Computer Vision: A Modern Approach, Prentice Hall, 2011. Resources Python Programming Language OpenCV PyTorch Anaconda HuggingFace ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"9c795004ca7f6134384563aba6b68641","permalink":"https://ajdillhoff.github.io/courses/archive/spring2024/cse4310/materials/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/archive/spring2024/cse4310/materials/","section":"courses","summary":"Books Richard Szeliski, Computer Vision: Algorithms and Applications, Springer, 2020. David Forsyth and Jean Ponce, Computer Vision: A Modern Approach, Prentice Hall, 2011. Resources Python Programming Language OpenCV PyTorch Anaconda HuggingFace ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books Wen-mei W. Hwu, David B. Kirk, Izzat El Hajj. Programming Massively Parallel Processors: A Hands-on Approach, 4th Edition, 2022. Jason Sanders and Edward Kandrot, CUDA by Example: An Introduction to General-Purpose GPU Programming, Addison-Wesley Professional, 2010. Resources Python Programming Language CUDA ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"e40c0c13bf4f35bae51363c8e725d045","permalink":"https://ajdillhoff.github.io/courses/archive/spring2024/cse5373/materials/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/archive/spring2024/cse5373/materials/","section":"courses","summary":"Books Wen-mei W. Hwu, David B. Kirk, Izzat El Hajj. Programming Massively Parallel Processors: A Hands-on Approach, 4th Edition, 2022. Jason Sanders and Edward Kandrot, CUDA by Example: An Introduction to General-Purpose GPU Programming, Addison-Wesley Professional, 2010. Resources Python Programming Language CUDA ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006. Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer, 2009. Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, An Introduction to Statistical Learning, Springer, 2023. Kevin Murphy, Probabilistic Machine Learning: An Introduction, MIT Press, 2022. Resources Python Programming Language PyTorch Anaconda HuggingFace ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"b431ab5112d19ca5c139af4828e6ae47","permalink":"https://ajdillhoff.github.io/courses/archive/spring2024/cse6363/materials/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/archive/spring2024/cse6363/materials/","section":"courses","summary":"Books Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006. Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer, 2009. Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, An Introduction to Statistical Learning, Springer, 2023. Kevin Murphy, Probabilistic Machine Learning: An Introduction, MIT Press, 2022. Resources Python Programming Language PyTorch Anaconda HuggingFace ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books Richard Szeliski, Computer Vision: Algorithms and Applications, Springer, 2020. David Forsyth and Jean Ponce, Computer Vision: A Modern Approach, Prentice Hall, 2011. Resources Python Programming Language OpenCV PyTorch Anaconda HuggingFace ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"9a16754a5c42b5aa41efbfc512a71c4a","permalink":"https://ajdillhoff.github.io/courses/cse4310/materials/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/cse4310/materials/","section":"courses","summary":"Books Richard Szeliski, Computer Vision: Algorithms and Applications, Springer, 2020. David Forsyth and Jean Ponce, Computer Vision: A Modern Approach, Prentice Hall, 2011. Resources Python Programming Language OpenCV PyTorch Anaconda HuggingFace ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006. Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer, 2009. Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, An Introduction to Statistical Learning, Springer, 2023. Kevin Murphy, Probabilistic Machine Learning: An Introduction, MIT Press, 2022. Resources Python Programming Language PyTorch Anaconda HuggingFace ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"32a4e7f841ca201e054e6091dbff81c5","permalink":"https://ajdillhoff.github.io/courses/cse6363/materials/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/cse6363/materials/","section":"courses","summary":"Books Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006. Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer, 2009. Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, An Introduction to Statistical Learning, Springer, 2023. Kevin Murphy, Probabilistic Machine Learning: An Introduction, MIT Press, 2022. Resources Python Programming Language PyTorch Anaconda HuggingFace ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books Horstmann, Cay S. Core Java Volume 1\u0026ndash;Fundamentals (11th ed.), Pearson, 2018. Morelli, Ralph and Walde, Ralph, Java, Java, Java Resources Submitting Assignments using GitHub CSE 13xx SharePoint Java Documentation Programming Practice Project Euler Java (W3 Schools) edabit Challenges exercism - Coding Exercises ","date":1689811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689811200,"objectID":"27374616a4ab70fbfa9e08144168d429","permalink":"https://ajdillhoff.github.io/courses/archive/summer2023/cse1325/materials/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/archive/summer2023/cse1325/materials/","section":"courses","summary":"Books Horstmann, Cay S. Core Java Volume 1\u0026ndash;Fundamentals (11th ed.), Pearson, 2018. Morelli, Ralph and Walde, Ralph, Java, Java, Java Resources Submitting Assignments using GitHub CSE 13xx SharePoint Java Documentation Programming Practice Project Euler Java (W3 Schools) edabit Challenges exercism - Coding Exercises ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006. Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer, 2009. Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, An Introduction to Statistical Learning, Springer, 2023. Kevin Murphy, Probabilistic Machine Learning: An Introduction, MIT Press, 2022. Resources Python Programming Language PyTorch Anaconda HuggingFace ","date":1689721200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689721200,"objectID":"20c52c4e821b648e67d9cf8f23b1cd94","permalink":"https://ajdillhoff.github.io/courses/archive/summer2023/cse6363/materials/","publishdate":"2023-07-19T00:00:00+01:00","relpermalink":"/courses/archive/summer2023/cse6363/materials/","section":"courses","summary":"Books Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006. Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer, 2009. Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, An Introduction to Statistical Learning, Springer, 2023. Kevin Murphy, Probabilistic Machine Learning: An Introduction, MIT Press, 2022. Resources Python Programming Language PyTorch Anaconda HuggingFace ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books L. Foster and W. Foster, C By Discovery (4th ed.), Pearson, 2004. Stephen Prata, C Primer Plus, Pearson, 2013. Bjarne Stroustrup, The C++ Programming Language (4th ed.), Addison-Wesley, 2013. Resources Submitting Assignments using GitHub CSE 13xx SharePoint C/C++ Reference The Linux Command Line Git Cheat Sheet Using GCC with MinGW Using WSL with VS Code GDB Cheat Sheet Programming Practice C Etudes Project Euler C Exercises (W3 Schools) C++ Exercises (W3 Schools) edabit Challenges exercism - Coding Exercises ","date":1689548400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689548400,"objectID":"ef694aacb53f50f69a7fffa6b72ec821","permalink":"https://ajdillhoff.github.io/courses/archive/summer2023/cse1310/materials/","publishdate":"2023-07-17T00:00:00+01:00","relpermalink":"/courses/archive/summer2023/cse1310/materials/","section":"courses","summary":"Books L. Foster and W. Foster, C By Discovery (4th ed.), Pearson, 2004. Stephen Prata, C Primer Plus, Pearson, 2013. Bjarne Stroustrup, The C++ Programming Language (4th ed.), Addison-Wesley, 2013. Resources Submitting Assignments using GitHub CSE 13xx SharePoint C/C++ Reference The Linux Command Line Git Cheat Sheet Using GCC with MinGW Using WSL with VS Code GDB Cheat Sheet Programming Practice C Etudes Project Euler C Exercises (W3 Schools) C++ Exercises (W3 Schools) edabit Challenges exercism - Coding Exercises ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books L. Foster and W. Foster, C By Discovery (4th ed.), Pearson, 2004. Stephen Prata, C Primer Plus, Pearson, 2013. Bjarne Stroustrup, The C++ Programming Language (4th ed.), Addison-Wesley, 2013. Resources Submitting Assignments using GitHub CSE 13xx SharePoint C/C++ Reference The Linux Command Line Git Cheat Sheet Using GCC with MinGW Using WSL with VS Code GDB Cheat Sheet Programming Practice C Etudes Project Euler C Exercises (W3 Schools) C++ Exercises (W3 Schools) edabit Challenges exercism - Coding Exercises ","date":1689548400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689548400,"objectID":"a47a98ad4c6ee3bb6f707044d6fe3970","permalink":"https://ajdillhoff.github.io/courses/cse1310/materials/","publishdate":"2023-07-17T00:00:00+01:00","relpermalink":"/courses/cse1310/materials/","section":"courses","summary":"Books L. Foster and W. Foster, C By Discovery (4th ed.), Pearson, 2004. Stephen Prata, C Primer Plus, Pearson, 2013. Bjarne Stroustrup, The C++ Programming Language (4th ed.), Addison-Wesley, 2013. Resources Submitting Assignments using GitHub CSE 13xx SharePoint C/C++ Reference The Linux Command Line Git Cheat Sheet Using GCC with MinGW Using WSL with VS Code GDB Cheat Sheet Programming Practice C Etudes Project Euler C Exercises (W3 Schools) C++ Exercises (W3 Schools) edabit Challenges exercism - Coding Exercises ","tags":null,"title":"Course Materials","type":"docs"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Types of Errors Debugging Techniques Examples With great power comes great responsibility. -Ben Parker\nC is a powerful language, but it is also considered unsafe. Mistakes stemming from pointer arithmetic, memory allocation, and other low-level operations can lead to segmentation faults, memory leaks, and other undefined behavior. This is one of the reasons why advocates are pushing for the use of safer languages like Rust.\nThese notes will cover some of the tools and techniques that can be used to debug C programs, from simple printf statements to more advanced tools like gdb.\nTypes of Errors Classifying the different types of errors is the first step towards becoming an expert debugger. If you can quickly identify what type of problem you have, you will be able to move towards a solution faster.\nSyntax Errors A syntax error is one in which a minor syntactical mistake was made in the code. For example, C requires that all statements end with a semicolon. If you forget to add one, the compiler will fail to compile your code and produce a relevent error message. These are the simplest types of errors to fix because they are typically accompanied by a helpful compiler message.\nConsider the following example:\n#include \u0026lt;stdio.h\u0026gt; int main(void) { printf(\u0026#34;Hello, World!\\n\u0026#34;) return 0; } This code will produce the following error message:\nhello.c: In function ‘main’: hello.c:4:30: error: expected ‘;’ before ‘return’ 4 | printf(\u0026#34;Hello, World!\\n\u0026#34;) | ^ | ; 5 | return 0; | ~~~~~~ Your compiler may vary slightly, but the message is clear: you forgot to add a semicolon at the end of the printf statement. You may read that message and find it a tad misleading. It says that the semicolon was expected before return. You might expect it mention that it is missing at the end of the printf statement. This is because the compiler would not detect the error until it reaches the return statement. It is important to read the error message carefully and look at the line numbers to determine where the error actually is.\nSemantic Errors Semantic errors do not produce a compiler error, so the program will still compile and run. These errors range from easy to extremely difficult depending on the size of the program and how often the error occurs. Consider the following working example:\n#include \u0026lt;stdio.h\u0026gt; int main(void) { double subtotal = 52.75; int num_purchases = 10; if (num_purchases \u0026gt;= 10) subtotal = subtotal - (subtotal * 0.10); // 10% discount printf(\u0026#34;Total: $%.2f\\n\u0026#34;, subtotal); return 0; } This program checks if a customer loyalty discount should be applied. If they have purchased 10 or more items, they receive a 10% discount. The program compiles and runs without any errors and works correctly. Let\u0026rsquo;s look at a slightly modified version of this code.\n#include \u0026lt;stdio.h\u0026gt; int main(void) { double subtotal = 52.75; int num_purchases = 10; if (num_purchases \u0026gt;= 10) printf(\u0026#34;Congratulations! You\u0026#39;ve unlocked a 10% discount.\\n\u0026#34;); subtotal = subtotal - (subtotal * 0.10); // 10% discount printf(\u0026#34;Total: $%.2f\\n\u0026#34;, subtotal); return 0; } This code compiles and runs but no longer matches our original intent. Can you spot the error?\nThe if statement only applies to the first line of code after it. This is a common mistake that can be difficult to spot in larger programs. In this case, a discount is applied regardless of the number of purchases because only the printf statement following the if statement is considered within the scope of the if, even though the next line is indented. The fix is simple: add curly braces to the if statement.\nLogical Errors Errors in logic can be hard to detect, especially as the scope of our problem grows. These errors are not necessarily syntax errors, and they may not even produce a compiler error. They are usually detected by testing the program and verifying that the output matches the expected output.\nConsider the Collatz conjecture problem, as seen on Project Euler:\nThe following iterative sequence is defined for the set of positive integers:\nn → n/2 (n is even) n → 3n + 1 (n is odd)\nUsing the rule above and starting with 13, we generate the following sequence:\n13 → 40 → 20 → 10 → 5 → 16 → 8 → 4 → 2 → 1\nIt can be seen that this sequence (starting at 13 and finishing at 1) contains 10 terms. Although it has not been proved yet (Collatz Problem), it is thought that all starting numbers finish at 1.\nWhich starting number, under one million, produces the longest chain?\nAn incorrect solution to the problem is shown below.\n#include \u0026lt;stdio.h\u0026gt; int collatz_steps(long long n) { int steps = 0; while (n != 1) { if (!n % 2 == 0) { n = n / 2; } else { n = 3 * n + 1; } printf(\u0026#34;%lld\u0026#34;, n); if (n != 1) { printf(\u0026#34; -\u0026gt; \u0026#34;); } steps++; } printf(\u0026#34;\\n\u0026#34;); return steps; } int main() { long long n = 12; int result = collatz_steps(n); printf(\u0026#34;Number of steps for %lld to reach 1: %d\\n\u0026#34;, n, result); return 0; } This solution is almost correct. The issue lies in the if statement in the collatz_steps function. The condition !n % 2 == 0 is incorrect. The correct condition should be n % 2 == 0. If you organize your code such that your functions are relatively small and modular, you can easily test each function individually to ensure that it works as expected.\nRuntime Errors Debugging Techniques Simple Debugging The simplest way to debug a C program is to litter it with printf statements until you narrow down exactly where the problem is. It works, sure, but it\u0026rsquo;s not very efficient. Depending on the size of your program, it can be very time consuming to add and remove printf statements. To this point, I would always recommend adding a debug target to your Makefile that compiles your program with debugging symbols. When used with a macro specifically for debugging, you can easily add and remove debugging statements without needing to manually remove them.\nDebugging with gdb GDB stands for GNU Debugger. It is a command-line debugger that can be used to step through a program line-by-line, set breakpoints, and inspect memory. It is a very powerful tool, but it can be a bit intimidating to use at first. This section will cover some of the basic commands that can be used to debug a program. Another variant of this for ARM-based processors like the new Macbooks is called lldb. It is very similar to gdb and can be used in the same way. I will include both versions of the commands as we go along.\nGDB typically comes pre-installed on most Linux distributions. If you are using a Mac, you can install it with brew. It is available for Windows via MSYS2. I trust you know how to install it on your own system. Once you have it installed, you can run it by typing gdb in your terminal. You will be greeted with a prompt that looks like this:\nGNU gdb (Ubuntu 12.1-0ubuntu1~22.04) 12.1 Copyright (C) 2022 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later \u0026lt;http://gnu.org/licenses/gpl.html\u0026gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \u0026#34;show copying\u0026#34; and \u0026#34;show warranty\u0026#34; for details. This GDB was configured as \u0026#34;x86_64-linux-gnu\u0026#34;. Type \u0026#34;show configuration\u0026#34; for configuration details. For bug reporting instructions, please see: \u0026lt;https://www.gnu.org/software/gdb/bugs/\u0026gt;. Find the GDB manual and other documentation resources online at: \u0026lt;http://www.gnu.org/software/gdb/documentation/\u0026gt;. For help, type \u0026#34;help\u0026#34;. Type \u0026#34;apropos word\u0026#34; to search for commands related to \u0026#34;word\u0026#34;. (gdb) In order for gdb to be useful, you need to compile your program with debugging symbols. This can be done by adding the -g flag to your gcc command. It is also recommended to disable compiler optimizations with the -O0 flag. This will ensure that the code you are debugging is as close to the source code as possible. For example, if you have a program called hello.c, you can compile it with debugging symbols like this:\ngcc -g -O0 hello.c -o hello Once you have compiled your program with debugging symbols, you can run it with gdb like this:\ngdb ./hello This will not actually begin running the program itself. Instead, it loads gdb which initializes the debugger. You can run the program with run to start execution. Let\u0026rsquo;s make a simple program to debug a memory issue.\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int main(void) { int *ptr; *ptr = 42; printf(\u0026#34;%d\\n\u0026#34;, *ptr); return 0; } Running this on my machine produces the following output:\n(gdb) run Starting program: /home/alex/dev/teaching/cse1320/CSE1320-Examples/debugging/a.out [Thread debugging using libthread_db enabled] Using host libthread_db library \u0026#34;/lib/x86_64-linux-gnu/libthread_db.so.1\u0026#34;. Program received signal SIGSEGV, Segmentation fault. 0x0000555555555159 in main () at forgot_alloc.c:6 6\t*ptr = 42; We already get a lot of information that would have taken us much longer to acquire through the simple printf method. Without adding any debugging statements, we know the exact line which caused the segmentation fault. We also know that it was caused by a SIGSEGV signal. This is a signal that is sent to a process when it tries to access memory that it does not have access to. In this case, we are trying to write to a memory address that we have not allocated. This is a very common mistake in C programs.\nUsing breakpoints We can use breakpoints to stop execution at a specific line. This is useful if we want to inspect the state of the program at a specific point in time. We can set a breakpoint at a specific line with the break command. For example, if we want to stop execution at line 6, we can do this:\nGDB\n(gdb) break 6 Breakpoint 1 at 0x555555555159: file forgot_alloc.c, line 6. LLDB\n(gdb) break 6 Breakpoint 1 at 0x555555555159: file forgot_alloc.c, line 6. Running the program again from the beginning will stop execution at line 6. We can then inspect the value of ptr with the print command.\n(gdb) run The program being debugged has been started already. Start it from the beginning? (y or n) y Starting program: /home/alex/dev/teaching/cse1320/CSE1320-Examples/debugging/a.out [Thread debugging using libthread_db enabled] Using host libthread_db library \u0026#34;/lib/x86_64-linux-gnu/libthread_db.so.1\u0026#34;. Breakpoint 1, main () at forgot_alloc.c:6 6\t*ptr = 42; (gdb) print ptr $1 = (int *) 0x555555555060 \u0026lt;_start\u0026gt; I wanted to start with this example, because you should never let this scenario happen. I\u0026rsquo;m not saying that you should always write bugless code, but this example highlights a very common mistake: forgetting to initialize your variables. In C, every single pointer variable should always be initialized to NULL. Let\u0026rsquo;s make that change, recompile the program, and debug again. Again, set a breakpoint at line 6 so we can see the value of ptr.\n(gdb) print ptr $1 = (int *) 0x0 Ensuring that our pointers are always initialized to NULL saves us a LOT of headaches.\nInspecting Memory We can inspect the memory of our program with the x command. This command takes two arguments: the number of units to print and the format. For example, if we want to print the first 10 bytes of memory, we can do this:\n(gdb) x/10b ptr 0x0:\t0\t0\t0\t0\t0\t0\t0\t0 0x8:\t0\t0 Examples Allocate memory in a function that will inevitably lead to a segmentation fault. Discover the original source of the allocation. ","date":1719032400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719032400,"objectID":"ca18a3fd208d976fced839fcefccb6bb","permalink":"https://ajdillhoff.github.io/notes/debugging_in_c/","publishdate":"2024-06-22T00:00:00-05:00","relpermalink":"/notes/debugging_in_c/","section":"notes","summary":"Table of Contents Types of Errors Debugging Techniques Examples With great power comes great responsibility. -Ben Parker\nC is a powerful language, but it is also considered unsafe. Mistakes stemming from pointer arithmetic, memory allocation, and other low-level operations can lead to segmentation faults, memory leaks, and other undefined behavior. This is one of the reasons why advocates are pushing for the use of safer languages like Rust.\nThese notes will cover some of the tools and techniques that can be used to debug C programs, from simple printf statements to more advanced tools like gdb.","tags":["programming"],"title":"Debugging in C","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Key Concepts Key Concepts Traditional Fine-Tuning Fine-tuning a model for a specific task can be expensive if the entire weight matrix is updated. LLMs range from billions to trillions of parameters, making fine-tuning infeasible for many applications.\nLow Rank Decomposition Low Rank Adaptation (LoRA) is a method of decomposing the weight update matrix \\(\\Delta W\\) into smaller matrices \\(A\\) and \\(B\\) such that \\(\\Delta W \\approx AB\\). The rank \\(r\\) of the decomposition is a hyperparameter that can be tuned to balance performance and computational cost (Hu et al. 2021).\nEfficiency LoRA is more efficient than fine-tuning because the rank \\(r\\) is much smaller than the number of parameters in the model. This allows for faster training and inference times. If \\(W \\in \\mathbb{R}^{d \\times d}\\), then \\(A \\in \\mathbb{R}^{d \\times r}\\) and \\(B \\in \\mathbb{R}^{r \\times d}\\), so the number of parameters in the decomposition is \\(2dr\\). Compare this to the \\(d^2\\) parameters in the original weight matrix.\nImplementation When fine-tuning with LoRA, the original weights are frozen. This has a significant impact on the performance of the model since the gradients do not need to be stored for the original weights. Only the gradients for the decomposition matrices \\(A\\) and \\(B\\) need to be stored.\nReferences Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. “LoRA: Low-Rank Adaptation of Large Language Models.” arXiv. https://doi.org/10.48550/arXiv.2106.09685. ","date":1717869780,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717869780,"objectID":"da981cf5d7e914c5f141235cded30e23","permalink":"https://ajdillhoff.github.io/notes/low_rank_adaptation/","publishdate":"2024-06-08T13:03:00-05:00","relpermalink":"/notes/low_rank_adaptation/","section":"notes","summary":"Table of Contents Key Concepts Key Concepts Traditional Fine-Tuning Fine-tuning a model for a specific task can be expensive if the entire weight matrix is updated. LLMs range from billions to trillions of parameters, making fine-tuning infeasible for many applications.\nLow Rank Decomposition Low Rank Adaptation (LoRA) is a method of decomposing the weight update matrix \\(\\Delta W\\) into smaller matrices \\(A\\) and \\(B\\) such that \\(\\Delta W \\approx AB\\).","tags":["llms","deep learning"],"title":"Low Rank Adaptation","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Native Patch Extraction Changing Perspective The Mechanics of as_strided What about RGB? This post is a recreation of Misha Laskin\u0026rsquo;s Twitter post about patch extraction in numpy. I wanted to provide a version of it that can be accessed without requiring a Twitter account.\nPatch extraction is a common image preprocessing technique that splits an input image into a regular grid of sub images. It is commonly used to prepare an image for input into a Vision Transformer (Dosovitskiy et al. 2021). As Misha points out in their original post, it is also used for convolutions, min and max pooling, and splicing audio and text.\nFigure 1: Patch extraction used for ViT (Dosovitskiy et al. 2021). Native Patch Extraction A general algorithm for patch extraction is easy enough to come up with. If the goal is to extract a grid of patches with equal size, simply loop over a grid specified by the image size and patch size.\ndef patch_extraction(img, size): H, W = img.shape rows, cols = H // size, W // size # rows and cols should be integers # initialize patches array patches = np.zeros((rows, cols, size, size)) # patches(0, 0) yields the first patch # fill in patches for i in range(rows): for j in range(cols): patches = img[size*i:size*(i+1), size*j:size*(j+1)] return patches This naive approach essentially takes a bunch of crops of the original image. The optimization proposed by Misha takes advantage of how the array is stored in memory. The result is this: instead of extracting the patches, we can change how the array itself is accessed in memory. By default, numpy stores array elements contiguously. This is actually pretty common.\nFigure 2: Array elements are contiguous (Misha\u0026rsquo;s Thread) Changing Perspective How can we possibly change how the array is accessed? Fortunately, NumPy provides the tools do to this. A numpy array is three important properties that we will use:\nshape: A list whose length is the number of dimensions of the array. Each entry of the list tells us how many elements belong to that dimension. For example, an array of shape (3, 3) has 2 dimensions, both of size 3. This would be the shape of the array seen above. strides: Represents the number of bytes needed to traverse the contiguous memory block to read the next element along a given dimension. Using the \\(3 \\times 3\\) matrix from above, its stride would be (12, 4) since an int is 4 bytes, the stride is simply 4 bytes to move to the immediate next element. To traverse to the next row, you would need to move 12 bytes. This is visualized below. itemsize: The size of each element in the array. Figure 3: Visualization of strides (Misha\u0026rsquo;s Thread) Since we already know the output shape of the patch tensor, we can specify the strides needed to get the desired patches. NumPy provides a module with a function that does this for us: stride_tricks.as_strided.\nfrom numpy.lib import stride_tricks # non-overlapping patches of size 8 size = 8 H, W = img.shape shape = [H // size, W // size] + [size, size] # (row, col, patch_row, patch_col) strides = [size * s for s in img.strides] + list(img.strides) # extract patches patches = stride_tricks.as_strided(img, shape=shape, strides=strides) The Mechanics of as_strided How does as_strided work? Consider the example below, where we break a \\(4 \\times 4\\) matrix into a \\(2 \\times 2\\) array of patches, where each patch is \\(2 \\times 2\\).\nFigure 4: Breaking a matrix into patches (Misha\u0026rsquo;s Thread) We need to provide strides for the following:\nthe patch in the adjacent row the patch in the adjacent column the adjacent row within a patch the adjacent column within a patch Let\u0026rsquo;s start with the patch itself. Each value in the patch is a single byte (itemsize = 1). The stride needed to get to the adjacent column is 1 and the stride needed to get to the next row is 2.\nMoving between patches is a bit trickier. Since each patch only has 2 columns, moving from the first to the second patch in the same column only requires moving 2 bytes. Moving from the first row of patches to the second requires 8 bytes as seen below.\nFigure 5: Strides required for patches (Misha\u0026rsquo;s Thread) What about RGB? The process given above works great for grayscale images, but what if we want to patchify a color image? Let\u0026rsquo;s take the same \\(4 \\times 4\\) example from above, but assume it is a 3-channel image. We can create this example quickly with numpy.\n# It is common to represent images as height, width, channels. mat = np.zeros((4, 4, 3), dtype=np.uint8) mat.strides # (12, 3, 1) As an RGB image, each element in the grayscale version now spans 3 values. If each element is a single byte, the stride required to access the adjacent element in the next row is 12. What does this mean for our RGB version? It means we do not have to change much. The stride across the channels is built in. As long as the output shape and strides array are configured, we are in the clear. This requires two modifications.\nEach patch is now an RGB image, so [size, size] should be changed to [size, size, 3]. Since the original image has 3 values for strides, only the first two should be taken when setting up the new strides list: strides = [size * s for s in img.strides[:2]] + list(img.strides). Figure 6: Computing strides for RGB patch extraction (Misha\u0026rsquo;s Thread) That\u0026rsquo;s it! Happy coding.\nReferences Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” Arxiv:2010.11929 [Cs], June. http://arxiv.org/abs/2010.11929. ","date":1717714620,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717714620,"objectID":"f3bf24406f73410b9cf847ce67f3a791","permalink":"https://ajdillhoff.github.io/notes/patch_extraction/","publishdate":"2024-06-06T17:57:00-05:00","relpermalink":"/notes/patch_extraction/","section":"notes","summary":"Table of Contents Native Patch Extraction Changing Perspective The Mechanics of as_strided What about RGB? This post is a recreation of Misha Laskin\u0026rsquo;s Twitter post about patch extraction in numpy. I wanted to provide a version of it that can be accessed without requiring a Twitter account.\nPatch extraction is a common image preprocessing technique that splits an input image into a regular grid of sub images. It is commonly used to prepare an image for input into a Vision Transformer (Dosovitskiy et al.","tags":["deep learning","data augmentation"],"title":"Patch Extraction","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Formal Languages Reductions Clique Problem Vertex Cover Problem Introduction Most of the algorithms discussed in a typical algorithms course run in polynomial time. This focus is reasonable since algorithms that run worse than polynomial time have little practical use. To simplify this notion: a problem for which a polynomial-time algorithm exists is \u0026ldquo;easy\u0026rdquo; and a problem for which no polynomial-time algorithm exists is \u0026ldquo;hard\u0026rdquo;. Knowing how to determine whether a problem is easy or hard is extremely useful. If one can identify a hard problem, then an approximate solution may be the best that can be achieved.\nOne of the most fundamental problems in computer science is the classification of problems into these two categories. These notes provide an introduction to this classification.\nP, NP, and NP-Complete There are three classes of algorithms:\nPolynomial-time NP (nondeterministic polynomial time) NP-complete Problems in P are those solvable in polynomial time. This means any constant \\(k\\) such that the running time is \\(O(n^k)\\).\nThe class NP is a superset of P. These are problems that can be verified in polynomial time. This means that if someone gives you a solution to the problem, you can verify that it is correct in polynomial time. This is different from solving the problem in polynomial time. Problems in NP can be solved in nondeterministic polynomial time. However, such a model of computation does not exist in the real world.\nNP-Complete problems are problems in NP that are as hard as any other problem in NP. This means that if you can solve an NP-Complete problem in polynomial time, you can solve any problem in NP in polynomial time. This is why NP-Complete problems are so important.\nVerifying a Solution As long as we can come up with a verification algorithm for a problem in polynomial time, we can say that the problem is in \\(NP\\). This is true even if we later find a polynomial-time algorithm for the problem.\nProving that a problem is NP-Complete Proving that a problem belongs to either NP or NPC is difficult the first time you do it. Luckily, now that problems have been proven to be NP-Complete, you can use these problems to prove that other problems are NP-Complete. First, let\u0026rsquo;s introduce one more class: NP-Hard. Informally, a problem \\(X\\) is NP-Hard if it is at least as hard as any problem in NP. If we can reduce every problem \\(Y \\in NP\\) to \\(X\\) in polynomial time, then \\(X\\) is NP-Hard. If \\(X\\) is also in NP, then \\(X\\) is NP-Complete.\nOptimization versus decision problems Many problems are framed as optimization problems. Given some criteria, the goal is to find the best solution according to that criteria. For the shortest path problem, the algorithm finds a path between two vertices in the fewest number of edges. One can intuit that this is a slightly harder problem than that of determining if a path exists using only \\(k\\) edges. This latter problem is a decision problem.\nThe reason this is worth talking about is that decision problems are often easier to come up with than optimization problems. If one can provide that a decision problem is hard, then its optimization problem is also hard.\nReducing one problem to another A common strategy for relating two problems is to reduce one to the other. For example, if problem \\(B\\) runs in polynomial time, and we can reduce problem \\(A\\) to problem \\(B\\) in polynomial time, then problem \\(A\\) is also in P. This is because we can solve \\(A\\) by reducing it to \\(B\\) and then solving \\(B\\) in polynomial time.\nFormal Languages Reductions The main idea behind reductions is to first show that a problem is NP-Complete. This first proof was done by Cook in 1971. With a problem proven to be NP-Complete, one can then show that other problems are NP-Complete by reducing them to the first problem. This method is far simpler than the original proof and provides a convenient process for proving that a problem is NP-Complete. The process is based on the following lemma.\nLemma 34.8\nIf \\(L\\) is a language such that \\(L\u0026rsquo; \\leq_p L\\) for some \\(L\u0026rsquo; \\in \\text{NPC}\\), then \\(L\\) is NP-hard. If, in addition, we have \\(L \\in \\text{NP}\\), then \\(L\\) is NP-Complete.\nCircuit Satisfiability What is this problem? Why is it important? How is it related to NP-Completeness? Given a boolean combinatorial circuit, is it satisfiable. That is, is there an assignment of values to the inputs that makes the output true?\nThis problem is in the class NP. To prove this, we only need to show that given an assignment of values to the inputs, we can verify that the output is true in polynomial time. For a given circuit, we can verify that the output is true by following the circuit from the inputs to the outputs.\nProving that it is NP-Complete is much more difficult, so a brief overview is provided here. Let \\(A\\) be any problem in \\(NP\\). Since any problem in \\(NP\\) has a polynomial-time verification algorithm, we can construct a boolean circuit that simulates this algorithm. This circuit will have a single output that is true if and only if the input is a valid solution to the problem. This circuit is satisfiable if and only if the input is a valid solution to the problem. Therefore, the circuit satisfiability problem is NP-Complete.\nExample: Exercise 34.3-1 Verify that the given circuit is unsatisfiable.\nFigure 1: Figure 34.8 from (Cormen et al. 2022). For reference, here are definitions for each of the gates listed in the figure.\nFigure 2: Definitions for the gates in the circuit from Figure 34.7 (Cormen et al. 2022). How can we prove this circuit is unsatisfiable?\nThe easiest way to do this is to code it up and brute force it.\nFormula Satisfiability Satisfiability (SAT) is NP-Hard. Can show that CIRCUIT-SAT \\(\\leq_p\\) SAT. Then show that SAT \\(\\leq_p\\) 3SAT. An instance of the formula satisfiability (SAT) problem is a boolean formula \\(\\phi\\) with\n\\(n\\) variables \\(x_1, x_2, \\ldots, x_n\\) \\(m\\) clauses \\(C_1, C_2, \\ldots, C_m\\) For example, the formula\n\\[ \\phi = (x_1 \\lor x_2) \\land (\\lnot x_1 \\lor x_3) \\land (x_2 \\lor x_3) \\]\nhas the satisfying assignment \\(x_1 = 1, x_2 = 0, x_3 = 1\\).\nSAT belongs to NP Showing that SAT is in NP is straightforward. Given a boolean formula \\(\\phi\\) and an assignment of values to the variables, one can verify that the formula is satisfied in polynomial time. This is enough to show that SAT is in NP.\nCIRCUIT-SAT \\(\\leq_p\\) SAT If we can reduce an instance of CIRCUIT-SAT, which is known to be NP-Complete, to SAT, then SAT is also NP-Complete. This proof is by contradiction: assume that SAT is not NP-Complete. Then, CIRCUIT-SAT is not NP-Complete. But we know that CIRCUIT-SAT is NP-Complete, so this is a contradiction.\nThe reduction starts by introducing a variable for each wire and a clause for each gate, as seen below.\nThe reduction algorithm produces a formula for each gate in terms of an \u0026ldquo;if and only if\u0026rdquo; statement.\n\\begin{align*} \\phi = x_{10} \u0026amp;\\land (x_4 \\leftrightarrow \\lnot x_3)\\\\ \u0026amp;\\land (x_5 \\leftrightarrow (x_1 \\lor x_2))\\\\ \u0026amp;\\land (x_6 \\leftrightarrow \\lnot x_4)\\\\ \u0026amp;\\land (x_7 \\leftrightarrow (x_1 \\land x_2 \\land x_4))\\\\ \u0026amp;\\land (x_8 \\leftrightarrow (x_5 \\lor x_6))\\\\ \u0026amp;\\land (x_9 \\leftrightarrow (x_6 \\lor x_7))\\\\ \u0026amp;\\land (x_{10} \\leftrightarrow (x_6 \\land x_8 \\land x_9))\\\\ \\end{align*}\nA simpler explanation for this reduction is that a circuit can be represented as a boolean formula. This formula can be solved by the SAT algorithm.\n3SAT The 3SAT problem is a special case of SAT where each clause has exactly three literals. This problem is also NP-Complete. Many problems can be reduced to 3SAT, which is why it is so important.\nDefinition An instance of 3SAT is a boolean formula \\(\\phi\\) with\n\\(n\\) literals \\(x_1, x_2, \\ldots, x_n\\) \\(m\\) clauses \\(C_1, C_2, \\ldots, C_m\\) Each clause has exactly three literals and is in conjunctive normal form (CNF), which means it is expressed as an AND of clauses. For example, the formula\n\\[ \\phi = (x_1 \\lor x_2 \\lor x_3) \\land (\\lnot x_1 \\lor x_2 \\lor x_3) \\land (x_1 \\lor \\lnot x_2 \\lor x_3) \\]\nis a 3SAT formula.\n3SAT is NP-Complete The 3SAT problem is NP-Complete. This can be shown by reducing SAT to 3SAT. A thorough proof is provided in the textbook (Cormen et al. 2022).\nClique Problem A clique is a complete subgraph of an undirected graph \\(G\\). That is, a clique is a set of vertices such that every pair of vertices is connected by an edge. The clique problem is to find the largest clique in a graph.\n\\[ \\text{CLIQUE} = \\{ \\langle G, k \\rangle \\mid G \\text{ has a clique of size } k \\} \\]\nClique is in NP Let\u0026rsquo;s say you have access to a clique of size \\(k\\). You can verify that this is a clique in polynomial time by checking that every pair of vertices is connected by an edge. That is, for each pair \\(u, v \\in V\u0026rsquo;\\), the edge \\((u, v)\\) is in \\(E\\), where \\(V\u0026rsquo;\\) is the set of vertices in the clique.\nThus, we have a polynomial-time verification algorithm for the clique problem, so it is in NP.\nClique is NP-Complete Knowing that 3SAT is NP-Complete, we can reduce 3SAT to the clique problem. The reduction may not be intuitive as a boolean formula seems to have no relation to a graph.\nLet \\(\\phi = C_1 \\land C_2 \\land \\ldots \\land C_m\\) be a 3SAT formula with \\(n\\) variables. We construct a graph \\(G\\) with \\(3n\\) vertices. Each \\(C_r\\) has three literals, \\(l_1^r, l_2^r, l_3^r\\).\nTo construct the graph, we create a triplet of vertices \\(v_1^r, v_2^r, v_3^r\\) for each clause \\(C_r\\) such that there is no edge connecting any two vertices in the same triplet. There is an edge \\((v_i^r, v_j^s) \\in E\\) if\n\\(v_i^r\\) and \\(v_j^s\\) are in different triplets \\(l_i^r\\) and \\(l_j^s\\) are not negations of each other One such formula that we can convert is\n\\[ \\phi = (x_1 \\lor \\lnot x_2 \\lor \\lnot x_3) \\land (\\lnot x_1 \\lor x_2 \\lor x_3) \\land (x_1 \\lor x_2 \\lor x_3). \\]\nThe resulting graph is shown below.\nFigure 3: Graph constructed from the 3SAT formula (phi) (Cormen et al. 2022). If a satisfying assignment exists for \\(\\phi\\), then each \\(C_r\\) has at least one literal that is true. Consider the corresponding vertices in the graph for a satisfying assignment. Since there is at least one true literal in each clause, there is at least one edge between the corresponding vertices. Thus, a reduction from 3SAT to the clique problem is possible.\nHow does this show that Clique is NP-Complete?\nIt is true that this example shows a very specialized graph. However, this is enough to show that the problem is NPC. If there were a polynomial time solution for Clique on a general graph \\(G\\), then surely it would work for a specialized graph as well.\nIf it could solve this one, then the corresponding 3SAT formula would be solvable as well. This is a contradiction, so the Clique problem is NP-Complete.\nVertex Cover Problem Figure 4: Vertex cover examples (Wikipedia). The vertex cover problem is to find the smallest set of vertices such that every edge in the graph is incident to at least one vertex in the set. More formally, a vertex cover of a graph \\(G\\) is a set \\(V\u0026rsquo; \\subseteq V\\) such that for every edge \\((u, v) \\in E\\), either \\(u \\in V\u0026rsquo;\\) or \\(v \\in V\u0026rsquo;\\).\nVertex Cover is in NP Given a set of vertices \\(V\u0026rsquo;\\), one can verify that it is a vertex cover in polynomial time by checking that every edge is incident to at least one vertex in the set. This is a polynomial-time verification algorithm, so the vertex cover problem is in NP.\nVertex Cover is NP-Complete We can show that the vertex cover problem is NP-Complete by reducing it to an instance of the clique problem. For this, we need to introduce the definition of a graph complement. Given a graph \\(G = (V, E)\\), the complement of \\(G\\) is the graph \\(\\overline{G} = (V, E\u0026rsquo;)\\) where \\(E\u0026rsquo; = \\{ (u, v) \\mid u, v \\in V \\text{ and } (u, v) \\notin E \\}\\). Basically, \\(\\overline{G}\\) has all the edges that \\(G\\) does not have.\nLet \\(G\\) contain a clique \\(V\u0026rsquo; \\subseteq V\\), where \\(|V\u0026rsquo;| = k\\). Then \\(V - V\u0026rsquo;\\) is a vertex cover of \\(\\overline{G}\\). If \\((u, v) \\in \\overline{E}\\), but is not in \\(E\\), then at least one of \\(u\\) or \\(v\\) is not in \\(V\u0026rsquo;\\).\nFigure 5: Graph (G) and its complement (overline{G}) (Cormen et al. 2022). In the example above, any edge in \\(\\overline{G}\\) has at least one vertex that is not in \\(G\\). On the same edge, at least one is in \\(V - V\u0026rsquo;\\), implying that \\((u, v)\\) is covered by \\(V - V\u0026rsquo;\\). Thus, a reduction from the clique problem to the vertex cover problem is possible.\nReferences Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. Introduction to Algorithms. 4th ed. MIT Press. http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/. ","date":1714060980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714060980,"objectID":"961f922158d3bf9b72ac70e94a67ffbf","permalink":"https://ajdillhoff.github.io/notes/np_completeness/","publishdate":"2024-04-25T11:03:00-05:00","relpermalink":"/notes/np_completeness/","section":"notes","summary":"Table of Contents Introduction Formal Languages Reductions Clique Problem Vertex Cover Problem Introduction Most of the algorithms discussed in a typical algorithms course run in polynomial time. This focus is reasonable since algorithms that run worse than polynomial time have little practical use. To simplify this notion: a problem for which a polynomial-time algorithm exists is \u0026ldquo;easy\u0026rdquo; and a problem for which no polynomial-time algorithm exists is \u0026ldquo;hard\u0026rdquo;. Knowing how to determine whether a problem is easy or hard is extremely useful.","tags":["algorithms","computer science"],"title":"NP-Completeness","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Problem Representation A Naive Solution The Floyd-Warshall Algorithm TychoLink is a telecommunications company looking to optimize its network for the fastest and most efficient data transfer possible. The network consists of multiple routers, each connected by various types of links that differ in latency and bandwidth. The company wants to ensure that data packets can travel from any router to any other router in the network using the path that offers the best balance between low latency and high bandwidth. There are three objectives in total:\nDetermine the all-pairs shortest paths across the network, taking into account both latency and bandwidth. Minimize the overall latency for data packet transmission across the network. Maximize the effective bandwidth along the chosen paths to ensure high data transfer rates. Given the solutions discussed in Single-Source Shortest Paths, we can simply run the Bellman-Ford algorithm for each router in the network to find the shortest paths to all other routers. This results in a time complexity of \\(O(V^2 E)\\). If the network is dense, then the number of edges \\(E = \\Theta(V^2)\\), which results in a time complexity of \\(O(V^4)\\).\nThese notes discuss another solution, the Floyd-Warshall algorithm, which can find the shortest paths between all pairs of routers in the network in \\(O(V^3)\\) time. The algorithm is particularly useful when the network is dense, as it is more efficient than running the Bellman-Ford algorithm for each router.\nProblem Representation Given a network \\(G = (V, E)\\) and a set of weights \\(W = (w_{ij})\\) for each edge \\((i, j) \\in E\\), the goal is to find the shortest path between all pairs of vertices in \\(V\\). The graph and weights will be represented as an adjacency matrix with entries \\(w_{ij}\\) for each edge \\((i, j) \\in E\\).\n\\[ w_{ij} = \\begin{cases} 0 \u0026amp; \\text{if } i = j \\\\ \\text{weight of edge } (i, j) \u0026amp; \\text{if } i \\neq j, (i, j) \\in E \\\\ \\infty \u0026amp; \\text{if } i \\neq j, (i, j) \\notin E. \\end{cases} \\]\nThe \\((i, j)\\) entry of the output matrix is \\(\\delta(i, j)\\), the shortest-path weight from \\(i\\) to \\(j\\).\nA Naive Solution To construct a dynamic programming solution, we need to establish that the problem has optimal substructure. The shortest path structure was first discussed in Single-Source Shortest Paths.\nRecursive Solution Step 2 is to state the recursive solution. Let \\(l_{ij}^{( r)}\\) be the minimum weight of any path \\(i \\leadsto j\\) that contains at most \\(r\\) edges. For \\(r = 0\\), the cost is either 0 if \\(i = j\\) or \\(\\infty\\) otherwise.\nFor \\(r = 1\\), try all possible predecessors \\(k\\) of \\(j\\).\n\\begin{align*} l_{ij}^{( r)} \u0026amp;= \\min \\Big\\{l_{ij}^{(r-1)}, \\min \\{l_{ik}^{(r-1)} + w_{kj} : 1 \\leq k \\leq n\\}\\Big\\}\\\\ \u0026amp;= \\min \\{l_{ik}^{(r-1)} + w_{kj} : 1 \\leq k \\leq n\\}. \\end{align*}\nEither the solution comes from a path of length \\(r-1\\) or from a path of length \\(r-1\\) with an additional edge \\((k, j)\\). The shortest path weights \\(\\delta(i, j)\\) contain at most \\(n-1\\) edges since the shortest path cannot contain a cycle. This implies that \\(\\delta(i, j) = l_{ij}^{(n-1)} = l_{ij}^{(n)} = l_{ij}^{(n+1)} = \\ldots\\)\nBottom-Up Approach Starting with a matrix \\(W = (w_{ij})\\), where \\(w_{ij}\\) are the edge weights, the following approach computes a series of matrices \\(L^{(0)}, L^{(1)}, \\ldots, L^{(n-1)}\\), where \\(L^{( r)} = (l_{ij}^{( r)})\\). The final matrix \\(L^{(n-1)}\\) contains the shortest path weights.\ndef extend_shortest_paths(L, W): n = len(L) L_prime = [[float(\u0026#39;inf\u0026#39;) for _ in range(n)] for _ in range(n)] for i in range(n): for j in range(n): for k in range(n): L_prime[i][j] = min(L_prime[i][j], L[i][k] + W[k][j]) return L_prime This function has the structure of matrix multiplication and has a running time of \\(O(n^3)\\). Since this must be repeated \\(n\\) times, the total running time is \\(O(n^4)\\).\nSince they are so similar, we can actually reframe this as matrix multiplication. This will lead to a more efficient algorithm. Start with the statement being computed inside the loop:\n\\[ l_{ij}^{( r)} = \\min \\{l_{ij}^{( r)}, l_{ik}^{(r-1)} + w_{kj}\\}. \\]\nIf we swap \\(\\min\\) with \\(+\\) and \\(\\cdot\\) with \\(\\min\\), we can rewrite this as a matrix multiplication:\n\\[ l_{ij}^{( r)} = l_{ij}^{( r)} + l_{ik}^{(r-1)} \\cdot w_{kj}. \\]\nFor this to yield the correct result, we also need to swap the identity of \\(\\min\\), which is \\(\\infty\\), with the identity of \\(+\\), which is 0.\nFaster APSP What\u0026rsquo;s the point?\nThe purpose of this reframing is to benefit from the associative property of matrix multiplication. As it turns out, our modified matrix multiplication using \\(\\min\\) is also associative. This allows us to compute the shortest paths in \\(O(n^3 \\lg n)\\) time. Consider that we only really care about \\(L^{(n-1)}\\). If we are not using negative weights, then computing anything above \\(n-1\\) will yield the same result.\nWe can get to this result in fewer steps by using repeated squaring.\n\\begin{align*} L^{(1)} \u0026amp;= \u0026amp;W, \u0026amp;\\quad\\\\ L^{(2)} \u0026amp;= \u0026amp;W^2 \u0026amp;= W \\cdot W,\\\\ L^{(4)} \u0026amp;= \u0026amp;W^4 \u0026amp;= W^2 \\cdot W^2,\\\\ \u0026amp; \u0026amp;\\vdots \u0026amp;\\\\ L^{(2^{\\lceil \\lg (n-1) \\rceil})} \u0026amp;= \u0026amp;W^{2^{\\lceil \\lg (n-1) \\rceil}} \u0026amp;= W^{2^{\\lceil \\lg (n-1) \\rceil - 1}} \\cdot W^{2^{\\lceil \\lg (n-1) \\rceil - 1}}. \\end{align*}\nIn total, we compute \\(O(\\lg n)\\) matrices, each of which takes \\(O(n^3)\\) time to compute. The total running time is \\(O(n^3 \\lg n)\\).\ndef faster_all_pairs_shortest_paths(W): n = len(W) L = W m = 1 while m \u0026lt; n - 1: L = extend_shortest_paths(L, L) m *= 2 return L Since we know that \\(L^{( r)} = L^{(n-1)}\\) for all \\(r \\geq n-1\\), we can stop the loop when \\(m \\geq n-1\\).\nExample Exercise 23.1-1: Run APSP on the following graph and show the resulting matrices at each step.\nFigure 1: Figure 23.2 from (Cormen et al. 2022). The Floyd-Warshall Algorithm The next solution to the APSP problem is the Floyd-Warshall algorithm. This algorithm can handle negative weight edges, but it will fail to produce a result if a negative weight cycle exists. It is a dynamic programming approach that reconsiders the structure of a shortest path.\nGiven a path \\(p = \\langle v_1, v_2, \\dots, v_l\\rangle\\), an intermediate vertex is an vertex of \\(p\\) other than \\(v_1\\) and \\(v_l\\). Then define \\(d_{ij}^{(k)}\\) as the weight of the shortest path from \\(i\\) to \\(j\\) that uses only the vertices \\(\\{1, 2, \\dots, k\\}\\) as intermediate vertices. Note that the vertices are arbitrarily numbered from 1 to \\(n\\).\nConsider a shortest path \\(i \\leadsto j\\) that uses intermediate vertices in the set \\(\\{1, 2, \\dots, k\\}\\):\nDrop \\(k\\): If \\(k\\) is not an intermediate vertex, then the path is a shortest path from \\(i\\) to \\(j\\) that uses only the vertices \\(\\{1, 2, \\dots, k-1\\}\\) as intermediate vertices. Split \\(k\\): If \\(k\\) is an intermediate vertex, split the path \\(p\\) into \\(i \\overset{p_1}{\\leadsto} k \\overset{p_2}{\\leadsto} j\\). In this case, \\(p_1\\) is a shortest path from \\(i\\) to \\(k\\) that uses only the vertices \\(\\{1, 2, \\dots, k-1\\}\\) as intermediate vertices, and \\(p_2\\) is a shortest path from \\(k\\) to \\(j\\) that uses only the vertices \\(\\{1, 2, \\dots, k-1\\}\\) as intermediate vertices. Notice that, in either case, the set of intermediate vertices is reduced.\nRecursive Solution Let \\(d_{ij}^{(k)}\\) be the weight of a shortest path from \\(i\\) to \\(j\\) that uses only the vertices \\(\\{1, 2, \\dots, k\\}\\) as intermediate vertices. The base case is \\(d_{ij}^{(0)} = w_{ij}\\).\n\\[ d_{ij}^{(k)} = \\begin{cases} w_{ij} \u0026amp; \\text{if } k = 0,\\\\ \\min \\{d_{ij}^{(k-1)}, d_{ik}^{(k-1)} + d_{kj}^{(k-1)}\\} \u0026amp; \\text{if } k \\geq 1. \\end{cases} \\]\nThe goal is to compute \\(D^{(n)} = (d_{ij}^{(n)})\\), since all intermediate vertices belong to the set \\(\\{1, 2, \\dots, n\\}\\).\nBottom-Up Approach With a recurrent solution in hand, a bottom-up approach can be used to compute the shortest path. The Floyd-Warshall algorithm takes as input a weighted adjacency matrix \\(W = (w_{ij})\\) and returns a matrix \\(D = (d_{ij})\\).\ndef floyd_warshall(W): n = len(W) D = W for k in range(n): D_prime = [[float(\u0026#39;inf\u0026#39;) for _ in range(n)] for _ in range(n)] for i in range(n): for j in range(n): D_prime[i][j] = min(D[i][j], D[i][k] + D[k][j]) D = D_prime return D This provides a running time of \\(\\Theta(n^3)\\).\nConstructing the Shortest Paths The Floyd-Warshall algorithm can be modified to construct the shortest paths themselves. This can be done by maintaining a matrix \\(P\\) that stores the predecessor of each vertex along the shortest path. Let \\(P^{(k)} = (p_{ij}^{(k)})\\) for \\(k = 0, 1, \\dots, n\\) be the matrix of predecessors. Each entry \\(p_{ij}^{(k)}\\) is defined recursively. The base case is\n\\[ p_{ij}^{(0)} = \\begin{cases} i \u0026amp; \\text{if } i \\neq j \\text{ and } w_{ij} \u0026lt; \\infty,\\\\ \\text{NIL} \u0026amp; \\text{otherwise}. \\end{cases} \\]\nThe recursive case is\n\\[ p_{ij}^{(k)} = \\begin{cases} p_{kj}^{(k-1)} \u0026amp; \\text{if } d_{ij}^{(k-1)} \u0026gt; d_{ik}^{(k-1)} + d_{kj}^{(k-1)},\\\\ p_{ij}^{(k-1)} \u0026amp; \\text{otherwise}. \\end{cases} \\]\nIn words, the recursive case is split into two parts. If the shortest path from \\(i\\) to \\(j\\) has \\(k\\) as an intermediate vertex, then it is \\(i \\leadsto k \\leadsto j\\) where \\(k \\neq j\\). In this case, choose \\(j\\)\u0026rsquo;s predecessor to be the predecessor of \\(j\\) on a shortest path from \\(k\\) to \\(j\\) with all intermediate vertices less than \\(k\\): \\(p_{ij}^{(k)} = p_{kj}^{(k-1)}\\).\nThe second subcase is when \\(k\\) is not an intermediate vertex. Keep the same predecessor as the shortest path from \\(i\\) to \\(j\\) with all intermediate vertices less than \\(k\\): \\(p_{ij}^{(k)} = p_{ij}^{(k-1)}\\).\nExample Walk through the Floyd-Warshall algorithm on the graph from the previous example. A Python notebook of this example is available in the repository.\nTransitive Closure of a Graph The algorithms presented above successfully solve the all-pairs shortest paths problem. What if we simply wanted to determine if a path exists between for all pairs of vertices? The answer to this question lies in the transitive closure of a graph.\nLet \\(G = (V, E)\\) be a directed graph with a vertex set \\(V = \\{1, 2, \\dots, n\\}\\). The transitive closure of \\(G\\) is a graph \\(G^* = (V, E^*)\\) such that \\((i, j) \\in E^*\\) if there is a path from \\(i\\) to \\(j\\) in \\(G\\).\nOne solution to this problem is to assign a weight of 1 to each edge of \\(E\\) and run the Floyd-Warshall algorithm. If \\(d_{ij} \u0026lt; n\\), then there is a path from \\(i\\) to \\(j\\) in \\(G\\). If \\(d_{ij} = \\infty\\), then there is no path from \\(i\\) to \\(j\\) in \\(G\\). First, substitute the \\(\\min\\) and \\(+\\) operations with \\(\\lor\\) (OR) and \\(\\land\\) (AND), respectively. This will allow us to determine if a path exists between two vertices.\nJust like Floyd-Warshall, we will maintain a series of matrices \\(T^{(0)}, T^{(1)}, \\ldots, T^{(n-1)}\\), where \\(T^{( r)} = (t_{ij}^{( r)})\\). The final matrix \\(T^{(n)}\\) contains the transitive closure of the graph. The values are defined as\n\\[ t_{ij}^{( r)} = \\begin{cases} 1 \u0026amp; \\text{if } r = 0 \\text{ and } (i, j) \\in E,\\\\ 1 \u0026amp; \\text{if } r \u0026gt; 0 \\text{ and } (t_{ij}^{(r-1)} = 1 \\lor (t_{ik}^{(r-1)} \\land t_{kj}^{(r-1)})),\\\\ 0 \u0026amp; \\text{otherwise}. \\end{cases} \\]\ndef transitive_closure(W): n = len(W) T = W for k in range(n): T_prime = [[0 for _ in range(n)] for _ in range(n)] for i in range(n): for j in range(n): T_prime[i][j] = T[i][j] or (T[i][k] and T[k][j]) T = T_prime return T This algorithm has a running time of \\(\\Theta(n^3)\\) while using simpler operations compared to the Floyd-Warshall algorithm.\nReferences Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. Introduction to Algorithms. 4th ed. MIT Press. http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/. ","date":1713630720,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713630720,"objectID":"70f6050732900794dd3bf075e29bd5b9","permalink":"https://ajdillhoff.github.io/notes/all_pairs_shortest_paths/","publishdate":"2024-04-20T11:32:00-05:00","relpermalink":"/notes/all_pairs_shortest_paths/","section":"notes","summary":"Table of Contents Problem Representation A Naive Solution The Floyd-Warshall Algorithm TychoLink is a telecommunications company looking to optimize its network for the fastest and most efficient data transfer possible. The network consists of multiple routers, each connected by various types of links that differ in latency and bandwidth. The company wants to ensure that data packets can travel from any router to any other router in the network using the path that offers the best balance between low latency and high bandwidth.","tags":["algorithms","computer science"],"title":"All-Pairs Shortest Paths","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":"Dynamic Parallelism is an extension to CUDA that enables kernels to directly call other kernels. Earlier versions of CUDA only allowed kernels to be launched from the host code. When we studied , the segmented approach required multiple kernel calls.\n","date":1713563520,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713563520,"objectID":"a5ed132699c65e5850e574003448c759","permalink":"https://ajdillhoff.github.io/notes/dynamic_parallelism/","publishdate":"2024-04-19T16:52:00-05:00","relpermalink":"/notes/dynamic_parallelism/","section":"notes","summary":"Dynamic Parallelism is an extension to CUDA that enables kernels to directly call other kernels. Earlier versions of CUDA only allowed kernels to be launched from the host code. When we studied , the segmented approach required multiple kernel calls.","tags":["gpgpu","cuda"],"title":"Dynamic Parallelism","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents What is cuDNN? Setting up cuDNN Handling Errors Representing Data Dense Layers Activation Functions Loss Functions Convolutions Pooling What is cuDNN? NVIDIA cuDNN provides optimized implementations of core operations used in deep learning. It is designed to be integrated into higher-level machine learning frameworks, such as TensorFlow, PyTorch, and Caffe.\nSetting up cuDNN To use cuDNN in your applications, each program needs to establish a handle to the cuDNN library. This is done by creating a cudnnHandle_t object and initializing it with cudnnCreate.\n#include \u0026lt;cudnn.h\u0026gt; int main() { cudnnHandle_t handle; cudnnCreate(\u0026amp;handle); // Use the handle cudnnDestroy(handle); return 0; } Handling Errors cuDNN functions return a cudnnStatus_t value, which indicates whether the function call was successful. As with previous CUDA code that we have reviewed, it is best to check the return value of each call. Not only does this help with debugging, but it also allows you to handle errors gracefully.\nRepresenting Data All data in cuDNN is represented as a tensor. A tensor is a multi-dimensional array of data. In cuDNN, tensors have the following parameters:\n# of dimensions data type an array of integers indicating the size of each dimension an array of integers indicating the stride of each dimension There are a few tensor descriptors for commonly used tensor types:\n3D Tensors (BMN): Batch, Height, Width 4D Tensors (NCHW): Batch, Channel, Height, Width 5D Tensors (NCDHW): Batch, Channel, Depth, Height, Width When creating a tensor to use with cuDNN operations, we need to create a tensor descriptor as well as the data itself. The tensor descriptor is a struct that contains the parameters of the tensor.\n// Create descriptor cudnnDataType_t data_type = CUDNN_DATA_FLOAT; cudnnTensorFormat_t format = CUDNN_TENSOR_NCHW; int n = 1, c = 3, h = 224, w = 224; cudnnTensorDescriptor_t tensor_desc; cudnnCreateTensorDescriptor(\u0026amp;tensor_desc); cudnnSetTensor4dDescriptor(tensor_desc, format, data_type, n, c, h, w); The descriptor is then used to allocate memory for the tensor data.\nfloat *data; cudaMalloc(\u0026amp;data, n * c * h * w * sizeof(float)); Retrieving Tensor Information To retrieve the properties of a tensor that already exists, we can use the cudnnGetTensor4dDescriptor function.\ncudnnStatus_t cudnnGetTensor4dDescriptor( const cudnnTensorDescriptor_t tensorDesc, cudnnDataType_t *dataType, int *n, int *c, int *h, int *w, int *nStride, int *cStride, int *hStride, int *wStride) The parameters are as follows:\ntensorDesc: the tensor descriptor dataType: the data type of the tensor n: the number of batches c: the number of channels h: the height of the tensor w: the width of the tensor nStride: the stride of the batch dimension cStride: the stride of the channel dimension hStride: the stride of the height dimension wStride: the stride of the width dimension Dense Layers A dense layer refers to a fully connected layer in a neural network. Each neuron in the layer is connected to every neuron in the previous layer. The weights of the connections are stored in a matrix, and the biases are stored in a vector. Implementing the forward and backward pass of a dense layer involves matrix multiplication and addition for which cuBLAS has optimized routines.\nForward Pass The forward pass of a dense layer is computed as follows:\n\\[ \\mathbf{a} = W \\mathbf{x} + \\mathbf{b} \\]\nwhere \\(\\mathbf{W}\\) is the weight matrix, \\(\\mathbf{x}\\) is the input tensor, \\(\\mathbf{b}\\) is the bias vector, and \\(\\mathbf{a}\\) is the output tensor.\nThis can be implemented in CUDA with a matrix multiply followed by a vector addition. The first operation we will use is cublasSgemm. The function declaration is\ncublasStatus_t cublasSgemm(cublasHandle_t handle, cublasOperation_t transa, cublasOperation_t transb, int m, int n, int k, const float *alpha, const float *A, int lda, const float *B, int ldb, const float *beta, float *C, int ldc); This computes\n\\[ C = \\alpha \\text{op}(A) \\text{op}(B) + \\beta C. \\]\nThe parameters are as follows:\nhandle: the cuBLAS handle transa: the operation to perform on matrix A (transpose or not) transb: the operation to perform on matrix B (transpose or not) m: the number of rows in matrix A and C n: the number of columns in matrix B and C k: the number of columns in matrix A and rows in matrix B alpha: scalar multiplier for the product of A and B A: matrix A lda: leading dimension of matrix A B: matrix B ldb: leading dimension of matrix B beta: scalar multiplier for matrix C C: matrix C ldc: leading dimension of matrix C This function is called twice in the forward pass of a dense layer: once for the matrix multiplication and once for the vector addition.\nBackward Pass The backward pass of a dense layer computes the gradients of the weights and biases with respect to the loss.\n\\[ \\frac{\\partial \\mathbf{a}}{\\partial W} = \\frac{\\partial}{\\partial W} (W \\mathbf{x} + \\mathbf{b}) = \\mathbf{x} \\]\n\\[ \\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{b}} = \\frac{\\partial}{\\partial \\mathbf{b}} (W \\mathbf{x} + \\mathbf{b}) = 1 \\]\nAdditionally, the layer should propagate the gradients of the loss with respect to the input tensor.\n\\[ \\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{x}} = \\frac{\\partial}{\\partial \\mathbf{x}} (W \\mathbf{x} + \\mathbf{b}) = W \\]\nThese gradients are only the local gradients of the layer. During backpropagation, the gradients are multiplied by the gradients propagated from the subsequent layer, as shown below:\n\\[ \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial \\mathbf{a}} \\frac{\\partial \\mathbf{a}}{\\partial W} \\]\n\\[ \\frac{\\partial L}{\\partial \\mathbf{b}} = \\frac{\\partial L}{\\partial \\mathbf{a}} \\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{b}} \\]\n\\[ \\frac{\\partial L}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{a}} \\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{x}} \\]\nThe first two gradients are used to update the weights and biases of the current layer. The last gradient is propagated to the previous layer.\nThese can be implemented in CUDA with matrix multiplication and vector addition, similar to the forward pass.\nActivation Functions cuDNN supports a variety of activation functions, including:\nSigmoid Hyperbolic Tangent Rectified Linear Unit (ReLU) Clipped Rectified Linear Unit (CLReLU) Exponential Linear Unit (ELU) To use an activation function, we need to create an activation descriptor and set the activation function type.\ncudnnActivationDescriptor_t activation_desc; cudnnCreateActivationDescriptor(\u0026amp;activation_desc); cudnnSetActivationDescriptor(activation_desc, CUDNN_ACTIVATION_RELU, CUDNN_NOT_PROPAGATE_NAN, 0.0); The third enum CUDNN_NOT_PROPAGATE_NAN indicates that NaN values should not be propagated through the activation function. The last parameter is a coefficient value, which is used by clipped ReLU and ELU.\nWe can also query the activation descriptor to extract the properties.\ncudnnGetActivationDescriptor(activation_desc, \u0026amp;mode, \u0026amp;reluNanOpt, \u0026amp;coef); Forward Pass To process the forward pass of an activation function, we use the cudnnActivationForward function.\ncudnnActivationForward( cudnnHandle_t handle, cudnnActivationDescriptor_t activationDesc, void *alpha, // scalar multiplier cudnnTensorDescriptor_t xDesc, void *x, void *beta, // scalar modifier cudnnTensorDescriptor_t zDesc, void *z); This computes the following operation:\n\\[ \\mathbf{z} = \\alpha \\cdot g(\\mathbf{x}) + \\beta \\cdot \\mathbf{z} \\]\nwhere \\(\\mathbf{x}\\) is the input tensor, \\(\\mathbf{z}\\) is the output tensor, \\(\\alpha\\) is a scalar multiplier, and \\(\\beta\\) is a scalar modifier.\nBackward Pass Likewise, the backward pass is done with cudnnActivationBackward.\ncudnnActivationBackward( cudnnHandle_t handle, cudnnActivationDescriptor_t activationDesc, void *alpha, // gradient modifier cudnnTensorDescriptor_t zDesc, void *z, cudnnTensorDescriptor_t dzDesc, void *dz, void *beta, cudnnTensorDescriptor_t xDesc, void *x, cudnnTensorDescriptor_t dxDesc, void *dx ); This computes the following operation:\n\\[ d\\mathbf{x} = \\alpha \\cdot \\nabla_{\\mathbf{x}} g(\\mathbf{x}) d\\mathbf{z} + \\beta \\cdot d\\mathbf{x} \\]\nwhere \\(d\\mathbf{z}\\) is the input tensor to the backward function. Under this same notation, \\(\\mathbf{z}\\) was the output of the activation function. The input to the activation function \\(d\\mathbf{x}\\) is the output tensor of the backward pass, since it is being propagated in the backwards direction.\nLoss Functions cuDNN also provides optimized implementations of loss functions such as cross-entropy. Since the related lab focuses on classification, we will limit our discussion to the cross-entropy loss combined with the softmax function.\nSoftmax The softmax function is used to convert the output of a neural network into a probability distribution. It is defined as\n\\[ \\text{softmax}(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}} \\]\nwhere \\(\\mathbf{x}\\) is the input tensor and \\(i\\) is the index of the output tensor.\nForward Pass Implementing the forward pass of the softmax function is straightforward. We use the cudnnSoftmaxForward function.\ncudnnStatus_t cudnnSoftmaxForward( cudnnHandle_t handle, cudnnSoftmaxAlgorithm_t algorithm, cudnnSoftmaxMode_t mode, const void *alpha, const cudnnTensorDescriptor_t xDesc, const void *x, const void *beta, const cudnnTensorDescriptor_t yDesc, void *y) Most of the parameters are similar to other cuDNN functions. The algorithm parameter specifies the algorithm to use for the softmax function, and the mode parameter specifies the mode of the softmax function.\nalgorithm: CUDNN_SOFTMAX_FAST, CUDNN_SOFTMAX_ACCURATE, or CUDNN_SOFTMAX_LOG. The most numerically stable is CUDNN_SOFTMAX_ACCURATE. mode: CUDNN_SOFTMAX_MODE_INSTANCE or CUDNN_SOFTMAX_MODE_CHANNEL. The former computes the softmax function for each instance in the batch, while the latter computes the softmax function for each channel in the tensor. Backward Pass The backward pass of the softmax function is implemented with cudnnSoftmaxBackward.\ncudnnStatus_t cudnnSoftmaxBackward( cudnnHandle_t handle, cudnnSoftmaxAlgorithm_t algorithm, cudnnSoftmaxMode_t mode, const void *alpha, const cudnnTensorDescriptor_t yDesc, const void *y, const cudnnTensorDescriptor_t dyDesc, const void *dy, const void *beta, const cudnnTensorDescriptor_t dxDesc, void *dx) Note that the softmax function is used in the forward pass of the loss function, so the gradients are propagated from the loss function to the softmax function. In practice, the two are combined into a much simpler gradient calculation. If the softmax function is followed by the cross-entropy loss, the gradients are computed as\n\\[ \\frac{\\partial L}{\\partial \\mathbf{x}} = \\mathbf{x} - \\mathbf{y} \\]\nwhere \\(\\mathbf{y}\\) is the target tensor.\nConvolutions For a background on convolutions, see these notes. The notes in this article refer to the cuDNN implementation of convolutions.\nWhen using convolution operations in cuDNN, we need to create a convolution descriptor cudnnConvolutionDescriptor_t as well as a filter descriptor cudnnFilterDescriptor_t.\nCreating a filter To create a filter descriptor, we use the cudnnCreateFilterDescriptor function.\ncudnnFilterDescriptor_t filter_desc; cudnnCreateFilterDescriptor(\u0026amp;filter_desc); We then set the filter descriptor with the cudnnSetFilter4dDescriptor function.\ncudnnStatus_t cudnnSetFilter4dDescriptor( cudnnFilterDescriptor_t filterDesc, cudnnDataType_t dataType, cudnnTensorFormat_t format, int k, int c, int h, int w) The parameters are as follows:\nfilterDesc: the filter descriptor dataType: the data type of the filter format: the format of the filter (NCHW or NHWC). Use CUDNN_TENSOR_NCHW for most cases. k: the number of output feature maps c: the number of input feature maps h: the height of the filter w: the width of the filter We can also query the filter descriptor to extract the properties.\ncudnnDataType_t data_type; cudnnTensorFormat_t format; int k, c, h, w; cudnnGetFilter4dDescriptor(filter_desc, \u0026amp;data_type, \u0026amp;format, \u0026amp;k, \u0026amp;c, \u0026amp;h, \u0026amp;w); Creating a convolution To create a convolution descriptor, we use the cudnnCreateConvolutionDescriptor function. Once we are done with it, we should destroy it with cudnnDestroyConvolutionDescriptor. Since our convolution is 2D, we use the cudnnSetConvolution2dDescriptor function.\ncudnnStatus_t cudnnSetConvolution2dDescriptor( cudnnConvolutionDescriptor_t convDesc, int pad_h, int pad_w, int u, int v, int dilation_h, int dilation_w, cudnnConvolutionMode_t mode, cudnnDataType_t computeType) The parameters are as follows:\nconvDesc: the convolution descriptor pad_h: the height padding pad_w: the width padding u: the vertical stride v: the horizontal stride dilation_h: the height dilation dilation_w: the width dilation mode: the convolution mode (CUDNN_CONVOLUTION or CUDNN_CROSS_CORRELATION) computeType: the data type used for the convolution Although the library supports both convolution and cross-correlation, the difference is only in the order of the operands. In practice, the two are equivalent. Most deep learning frameworks use cross-correlation.\nTo query the convolution descriptor, we can use the cudnnGetConvolution2dDescriptor function.\ncudnnStatus_t cudnnGetConvolution2dDescriptor( cudnnConvolutionDescriptor_t convDesc, int *pad_h, int *pad_w, int *u, int *v, int *dilation_h, int *dilation_w, cudnnConvolutionMode_t *mode, cudnnDataType_t *computeType) If we know all the parameters of the convolution, we can use the cudnnGetConvolution2dForwardOutputDim function to calculate the output dimensions.\ncudnnStatus_t cudnnGetConvolution2dForwardOutputDim( const cudnnConvolutionDescriptor_t convDesc, const cudnnTensorDescriptor_t inputTensorDesc, const cudnnFilterDescriptor_t filterDesc, int *n, int *c, int *h, int *w) Forward Pass cuDNN supports several methods for performing a convolution operation. An evaluation of the available algorithms can be found here. The algorithms provide tradeoffs in terms of speed and memory usage. Diving into these details is beyond the scope of this article, but it is important to be aware of the options.\nThe forward pass of a convolution is implemented with the cudnnConvolutionForward function.\ncudnnStatus_t cudnnConvolutionForward( cudnnHandle_t handle, const void *alpha, const cudnnTensorDescriptor_t xDesc, const void *x, const cudnnFilterDescriptor_t wDesc, const void *w, const cudnnConvolutionDescriptor_t convDesc, cudnnConvolutionFwdAlgo_t algo, void *workSpace, size_t workSpaceSizeInBytes, const void *beta, const cudnnTensorDescriptor_t yDesc, void *y) The parameters are as follows:\nhandle: the cuDNN handle alpha: scalar multiplier for the input tensor xDesc: the input tensor descriptor x: the input tensor wDesc: the filter descriptor w: the filter tensor convDesc: the convolution descriptor algo: the algorithm to use for the convolution workSpace: the workspace for the convolution workSpaceSizeInBytes: the size of the workspace beta: scalar modifier for the output tensor yDesc: the output tensor descriptor y: the output tensor Backward Pass There are three different backward passes for a convolutional layer: one for the weights, one for the input tensor, and one for the bias.\nWeights The backward pass for the weights is implemented with the cudnnConvolutionBackwardFilter function.\ncudnnStatus_t cudnnConvolutionBackwardFilter( cudnnHandle_t handle, const void *alpha, const cudnnTensorDescriptor_t xDesc, const void *x, const cudnnTensorDescriptor_t dyDesc, const void *dy, const cudnnConvolutionDescriptor_t convDesc, cudnnConvolutionBwdFilterAlgo_t algo, void *workSpace, size_t workSpaceSizeInBytes, const void *beta, const cudnnFilterDescriptor_t dwDesc, void *dw) A detailed description of the parameters can be found here.\nBias The backward pass for the bias is implemented with the cudnnConvolutionBackwardBias function.\ncudnnStatus_t cudnnConvolutionBackwardBias( cudnnHandle_t handle, const void *alpha, const cudnnTensorDescriptor_t dyDesc, const void *dy, const void *beta, const cudnnTensorDescriptor_t dbDesc, void *db) A detailed description of the parameters can be found here.\nInput The backward pass for the input tensor is implemented with the cudnnConvolutionBackwardData function.\ncudnnStatus_t cudnnConvolutionBackwardData( cudnnHandle_t handle, const void *alpha, const cudnnFilterDescriptor_t wDesc, const void *w, const cudnnTensorDescriptor_t dyDesc, const void *dy, const cudnnConvolutionDescriptor_t convDesc, cudnnConvolutionBwdDataAlgo_t algo, void *workSpace, size_t workSpaceSizeInBytes, const void *beta, const cudnnTensorDescriptor_t dxDesc, void *dx) A detailed description of the parameters can be found here.\nPooling Pooling is a common operation in convolutional neural networks. It reduces the spatial dimensions of the input tensor, which helps to reduce the number of parameters and computation in the network. Using pooling in cuDNN requires creating a descriptor. Make sure to destroy it when you\u0026rsquo;re done.\nCreating a pooling descriptor To create a pooling descriptor, we use the cudnnCreatePoolingDescriptor function.\ncudnnPoolingDescriptor_t pooling_desc; cudnnCreatePoolingDescriptor(\u0026amp;pooling_desc); We then set the pooling descriptor with the cudnnSetPooling2dDescriptor function.\ncudnnStatus_t cudnnSetPooling2dDescriptor( cudnnPoolingDescriptor_t poolingDesc, cudnnPoolingMode_t mode, cudnnNanPropagation_t maxpoolingNanOpt, int windowHeight, int windowWidth, int verticalPadding, int horizontalPadding, int verticalStride, int horizontalStride) The parameters are as follows:\npoolingDesc: the pooling descriptor mode: the pooling mode (CUDNN_POOLING_MAX or CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING) maxpoolingNanOpt: the NaN propagation option for max pooling windowHeight: the height of the pooling window windowWidth: the width of the pooling window verticalPadding: the vertical padding horizontalPadding: the horizontal padding verticalStride: the vertical stride horizontalStride: the horizontal stride We can also query the pooling descriptor to extract the properties.\ncudnnPoolingMode_t mode; cudnnNanPropagation_t nan_opt; int window_h, window_w, pad_h, pad_w, stride_h, stride_w; cudnnGetPooling2dDescriptor(pooling_desc, \u0026amp;mode, \u0026amp;nan_opt, \u0026amp;window_h, \u0026amp;window_w, \u0026amp;pad_h, \u0026amp;pad_w, \u0026amp;stride_h, \u0026amp;stride_w); Forward Pass The forward pass of a pooling operation is implemented with the cudnnPoolingForward function.\ncudnnStatus_t cudnnPoolingForward( cudnnHandle_t handle, const cudnnPoolingDescriptor_t poolingDesc, const void *alpha, const cudnnTensorDescriptor_t xDesc, const void *x, const void *beta, const cudnnTensorDescriptor_t yDesc, void *y) The parameters are as follows:\nhandle: the cuDNN handle poolingDesc: the pooling descriptor alpha: scalar multiplier for the input tensor xDesc: the input tensor descriptor x: the input tensor beta: scalar modifier for the output tensor yDesc: the output tensor descriptor y: the output tensor Backward Pass The backward pass of a pooling operation is implemented with the cudnnPoolingBackward function.\ncudnnStatus_t cudnnPoolingBackward( cudnnHandle_t handle, const cudnnPoolingDescriptor_t poolingDesc, const void *alpha, const cudnnTensorDescriptor_t yDesc, const void *y, const cudnnTensorDescriptor_t dyDesc, const void *dy, const cudnnTensorDescriptor_t xDesc, const void *x, const void *beta, const cudnnTensorDescriptor_t dxDesc, void *dx) The parameters are as follows:\nhandle: the cuDNN handle poolingDesc: the pooling descriptor alpha: scalar multiplier for the input tensor yDesc: the output tensor descriptor y: the output tensor dyDesc: the input tensor descriptor dy: the input tensor xDesc: the input tensor descriptor x: the input tensor beta: scalar modifier for the output tensor dxDesc: the output tensor descriptor dx: the output tensor ","date":1713230040,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713230040,"objectID":"c7bd2e66fb6e77b989a797ba1a6782a5","permalink":"https://ajdillhoff.github.io/notes/using_the_cudnn_library/","publishdate":"2024-04-15T20:14:00-05:00","relpermalink":"/notes/using_the_cudnn_library/","section":"notes","summary":"Table of Contents What is cuDNN? Setting up cuDNN Handling Errors Representing Data Dense Layers Activation Functions Loss Functions Convolutions Pooling What is cuDNN? NVIDIA cuDNN provides optimized implementations of core operations used in deep learning. It is designed to be integrated into higher-level machine learning frameworks, such as TensorFlow, PyTorch, and Caffe.\nSetting up cuDNN To use cuDNN in your applications, each program needs to establish a handle to the cuDNN library.","tags":["gpgpu","deep learning"],"title":"Using the cuDNN Library","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Objective Questions Maximum Flow A polynomial time solution A flow network is a directed graph in which the edges begin at a node that produces the flow and the adjacent nodes are the ones that receive it. Flow in this context could take on many meanings, such as the amount of water that can flow through a pipe, the amount of data that can be sent through a network, or the amount of traffic that can be sent through a road network. The goal of a flow network is to maximize the flow from the source to the sink.\nThe problem may have intermediate constraints. For example, a network graph may have a node with limited bandwidth, so the flow through that node must be less than or equal to the bandwidth. These notes review the formal definition of the problem followed by a solution using the Ford-Fulkerson algorithm as well as one related to bipartite matching.\nObjective Questions What is the maximum flow problem? How can we solve it? Maximum Flow A flow network \\(G = (V, E)\\) is a directed graph in which each edge \\((u, v) \\in E\\) has a nonnegative capacity \\(c(u, v) \\geq 0\\). The graph does not contain reverse edges between two vertices. If an edge does not exist in the set, then its capacity is 0. Each graph has a source and a sink, which will be the main edges of note when analyzing the graph. The goal is to maximize the flow going from the source to the sink. This implies that the source has no incoming edges.\nFigure 1: A flow network. Each edge depicts (f(u,v)/c(u,v)), the flow and capacity (Cormen et al. 2022). A flow in a graph \\(G\\) is a function \\(f : V \\times V \\rightarrow \\mathbb{R}\\) that satisfies two properties:\nCapacity constraint: For all \\(u, v \\in V\\), \\[ 0 \\leq f(u,v) \\leq c(u,v). \\] Flow conservation: For all \\(u \\in V - \\{s, t\\}\\), \\[ \\sum_{v \\in V} f(v, u) = \\sum_{v \\in V} f(u, v). \\] There are usually many different possibly paths of flow in a flow network. The maximum flow problem asks: what is the path that yields the maximum flow?\nAntiparallel Edges The restriction that no two nodes may have more than one edge seems to be unrealistic. For example, modeling the flow of a network graph with this restriction means that network traffic can only move in one direction between two datacenters. If there were two edges between adjacent nodes \\(v_1\\) and \\(v_2\\) such that \\((v_1, v_2) \\in E\\) and \\((v_1, v_2) \\in E\\), we would call these edges antiparallel. In such cases, the graph is modified with a new node \\(v\u0026rsquo;\\) such that \\((v_1, v\u0026rsquo;) \\in E\\) and \\((v\u0026rsquo;, v_2) \\in E\\). An example is shown below.\nFigure 2: Addressing antiparallel edges in a flow network (Cormen et al. 2022). Multiple Sources and Sinks Another restriction that is unrealistic in many real-world scenarios is that maximum flow graphs can only have a single source and sink. It is easy to imagine a scenario where multiple sources and sinks within a network. Again, the graph can be modified to accommodate this scenario by defining a supersource and supersink whose outgoing and incoming flows are infinite.\nFigure 3: Assing a supersource and supersink to a graph with multiple sources and sinks (Cormen et al. 2022). A polynomial time solution Ford-Fulkerson relies on three foundational concepts:\nResidual networks Augmenting paths Cuts The solution presented by Ford and Fulkerson is not a single algorithm but rather a set of general instructions.\nInitialize \\(f(u, v) = 0\\) for all \\(u,v \\in V\\), giving an initial flow of 0. Increase the flow by finding an augmenting path in a residual network. The edges of the augmented path indicate where to increase the flow. This is repeated until the residual network has no more augmenting paths.\nResidual Networks Consider a pair of vertices \\(u, v \\in V\\), the residual capacity \\(c_f(u, v)\\) is amount of additional flow that can be pushed from \\(u\\) to \\(v\\).\n\\[ c_f(u, v)= \\begin{cases} c(u,v) - f(u, v)\\quad \\text{if } (u,v) \\in E,\\\\ f(v, u)\\quad \\text{if } (v, u) \\in E,\\\\ 0\\quad \\text{otherwise (i.e., } (u, v), (v, u) \\notin E). \\end{cases} \\]\nThe residual network is \\(G_f = (V, E_f)\\), where\n\\[ E_f= \\{(u, v) \\in V \\times V : c_f(u, v) \u0026gt; 0\\}. \\]\nThe edges of \\(G_f\\) represent those edges in \\(G\\) with the capacity to change the flow. There is also no requirement for all edges in \\(G\\) to be present in \\(G_f\\). As the algorithm works out the solution, we are only considered with edges that permit more flow.\nAn edge \\((u, v) \\in E\\) means that the reverse edge \\((v, u) \\notin E\\). However, the residual network can have edges that are not in \\(G\\). These are used to represent paths in which flow is sent in the reverse direction. This can happen if reducing flow from one edge results in a net increase across some other.\nIn \\(G_f\\), the reverse edges \\((v, u)\\) represent the flow on \\((u, v) \\in G\\) that could be sent back.\nFigure 4: A flow network and its residual network (Cormen et al. 2022). Augmentation Function Given flows \\(f\\) in \\(G\\) and \\(f\u0026rsquo;\\) in \\(G_f\\), define the augmentation of \\(f\\) by \\(f\u0026rsquo;\\), as a function \\(V \\times V \\rightarrow \\mathbb{R}\\):\n\\[ (f \\uparrow f\u0026rsquo;)(u, v) = \\begin{cases} f(u, v) + f\u0026rsquo;(u, v) - f\u0026rsquo;(u, v)\\quad \\text{if } (u, v) \\in E,\\\\ 0 \\quad \\text{otherwise} \\end{cases} \\]\nfor all \\((u, v) \\in V\\).\nThis augmentation function represents an increase of flow on \\((u, v)\\) by \\(f\u0026rsquo;(u, v)\\) with a decrease by \\(f\u0026rsquo;(v, u)\\) since pushing flow on the reverse edge in \\(G_f\\) represents a decrease in \\(G\\). This is known as cancellation.\nLemma\nGiven a flow network \\(G\\), a flow \\(f\\) in \\(G\\), and the residual network \\(G_f\\), let \\(f\u0026rsquo;\\) be a flow in \\(G_f\\). Then \\((f \\uparrow f\u0026rsquo;)\\) is a flow in \\(G\\) with value \\(|f \\uparrow f\u0026rsquo;| = |f| + |f\u0026rsquo;|\\).\nThis lemma defines the idea of net flow. If there are 10 units of flow in one direction and 4 in the other, the edge effectively has 6 units of flow.\nAugmenting Paths An augmenting path is a simple path from the source to the sink in the residual network.\nFigure 5: An augmenting path in a flow network (Cormen et al. 2022). The purpose of an augmenting path is to increase the flow from the source to the sink. The flow is increased by the minimum capacity of the edges in the path.\nLemma 24.2\nLet \\(G = (V, E)\\) be a flow network, let \\(f\\) be a flow in \\(G\\), and let \\(p\\) be an augmenting path in \\(G_f\\). Define \\(f_p : V \\times V \\rightarrow \\mathbb{R}\\) by\n\\[ f_p(u, v) = \\begin{cases} c_f(p) \u0026amp; \\text{if } (u, v) \\text{ is in } p,\\\\ 0 \u0026amp; \\text{otherwise}. \\end{cases} \\]\nThen \\(f_p\\) is a flow in \\(G_f\\) with value \\(|f_p| \u0026gt; 0\\).\nThe maximum amount that an augmenting path can be increased is the minimum capacity of the edges in the path. This is known as the residual capacity of the path.\n\\[ c_f(p) = \\min \\{c_f(u, v) : (u, v) \\text{ is in } p\\}. \\]\nPut simply, if there is a path in the residual network, the flow can be increased by the minimum capacity of the edges in the path. If there is no path, the flow is at its maximum.\nCuts A cut \\((S, T)\\) of a flow network \\(G = (V, E)\\) is a partition of \\(V\\) into two sets \\(S\\) and \\(T = V - S\\) such that \\(s \\in S\\) and \\(t \\in T\\). The capacity of the cut is the sum of the capacities of the edges from \\(S\\) to \\(T\\). Any cut is a valid cut as long as the source is in \\(S\\) and the sink is in \\(T\\).\nIf \\(f\\) is a flow in \\(G\\) and \\((S, T)\\) is a cut of \\(G\\), then the net flow across the cut is\n\\[ f(S, T) = \\sum_{u \\in S} \\sum_{v \\in T} f(u, v) - \\sum_{u \\in S} \\sum_{v \\in T} f(v, u). \\]\nThe capacity of the cut is\n\\[ c(S, T) = \\sum_{u \\in S} \\sum_{v \\in T} c(u, v). \\]\nA minimum cut is a cut whose capacity is the smallest among all cuts.\nLemma\nFor any flow \\(f\\) and any cut \\((S, T)\\) of \\(G\\), we have that $|f| = f(S, T).$\nThis lemma states that the flow across a cut is equal to the value of the flow.\nProof\n\\begin{align*} f(S, T) \u0026amp;= f(S, V) - f(S, S)\\\\ \u0026amp;= f(S, V)\\\\ \u0026amp;= f(s, V) + f(S - s, V)\\\\ \u0026amp;= f(s, V) = |f|. \\end{align*}\nKey concept: We can determine the flow by making cuts in the graph. The minimum cut leads to the maximum flow.\nCorollary\nThe value of any flow \\(f\\) in a flow network \\(G\\) is bounded from above by the capacity of any cut of \\(G\\).\nMax-flow Min-cut Theorem The following statements are logically equivalent.\nThe flow \\(f\\) is a maximum flow in \\(G\\). The residual network \\(G_f\\) contains no augmenting paths. The value of the flow \\(f\\) is equal to the capacity of the cut \\((S, T)\\) for some cut of \\(G\\). Ford-Fulkerson Algorithm The Ford-Fulkerson algorithm is a general method for solving the maximum flow problem. The algorithm is not a single algorithm but rather a set of instructions that can be implemented in different ways. The algorithm is as follows:\ndef ford_fulkerson(G, s, t): f = {u: {v: 0 for v in G} for u in G} while True: # Find an augmenting path path = bfs(G, s, t, f) if not path: break cf = min(G[u][v] - f[u][v] for u, v in path) for u, v in path: f[u][v] += cf f[v][u] -= cf return f Example Run the Ford-Fulkerson algorithm on the following graph.\nAnalysis The running time of Ford-Fulkerson hinges on how the augmenting path is found. If implemented with a breadth-first search, the algorithm runs in \\(O(VE^2)\\) time.\nEdmonds-Karp Algorithm The Edmonds-Karp algorithm is a specific implementation of Ford-Fulkerson that uses breadth-first search to find the augmenting path. The algorithm presented above is actually the Edmonds-Karp algorithm.\nReferences Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. Introduction to Algorithms. 4th ed. MIT Press. http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/. ","date":1712965860,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712965860,"objectID":"f6271dcd6ae2e59a149acb6d466b3be1","permalink":"https://ajdillhoff.github.io/notes/maximum_flow/","publishdate":"2024-04-12T18:51:00-05:00","relpermalink":"/notes/maximum_flow/","section":"notes","summary":"Table of Contents Objective Questions Maximum Flow A polynomial time solution A flow network is a directed graph in which the edges begin at a node that produces the flow and the adjacent nodes are the ones that receive it. Flow in this context could take on many meanings, such as the amount of water that can flow through a pipe, the amount of data that can be sent through a network, or the amount of traffic that can be sent through a road network.","tags":["algorithms","computer science"],"title":"Maximum Flow","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":"The accompanying Colab notebook is available here.\nLarge Language Models like ChatGPT have rapidly become ubiquitous tools that enhance productivity, creativity, and even decision-making processes across various domains. Their ability to generate human-like text, comprehend complex instructions, and provide informative responses has captivated the imagination of users worldwide. This paragraph was generated by an LLM (and edited by me).\nThis workshop is for those that are curious as to how these models interpret the input. By the end of this hour, you will hopefully be able to answer the following questions, among others:\nHow do Large Language Models read and process text? Why are LLMs good at complex tasks, but seem to perform poorly on seemingly simple tasks like spelling or arithmetic? How does an LLM understand what it is processing? Agenda Tokenization Unicode byte encodings Byte Pair Encoding (BPE) Embeddings Tokenization Tokenization is the process of transforming a sequence of characters into a sequence of tokens. A token is a unit of text that we treat as a single entity. For example, in English, a token could be a word, a sentence, or a paragraph. In programming languages, a token could be a variable name, a keyword, or a string.\nBefore we get started, let\u0026rsquo;s check out a live demonstration of tokenization.\nConsider the input prompt below. It isn\u0026rsquo;t likely that you would have mixed emoji and code in a single text file, but it serves as a good example for tokenization.\nWhy does my code 💥 with a segmentation fault? int main() { int *arr = NULL; scanf(\u0026#34;%d\u0026#34;, arr); return 0; } At the most basic level, how is this text represented in a computer?\nThese characters are represented as encodings such as ASCII or Unicode. For the purposes of the rest of this article, we will assume the input is represented using Unicode.\nUnicode Byte Encodings If we were to print out the unicode values of the prompt above, we would get the following:\n[10, 87, 104, 121, 32, 100, 111, 101, 115, 32, 109, 121, 32, 99, 111, 100, 101, 32, 128165, ...] Most of the values displayed in the previous cell are the same for ASCII. The emoji value has a very large number and can easily be spotted in the list.\nIs that it? Is this how the input is fed into the model?\nThis encoding is done at the character-level. What other types of encodings are there?\nCharacter encoding Word encoding Sub-word encoding What is the difference between them? Why would we pick one over another?\nCharacter Encoding Character encoding converts each character into a unique integer. This is by far the simplest form of tokenization and has the benefit of a compact vocabulary. However, it is not able to effectively compress any common subsequences in the input. This leads to much larger sequences and longer training times.\nThe biggest downside to this approach is that the individual characters are not very informative on a semantic level. For example, the word \u0026ldquo;cat\u0026rdquo; would be represented as three separate tokens, \u0026lsquo;c\u0026rsquo;, \u0026lsquo;a\u0026rsquo;, and \u0026rsquo;t\u0026rsquo;. If someone were to present you a single letter without context, you probably would likely not be able to understand the point of the message.\nWord Encoding Word encoding is a step up from character encoding. This encoding directly captures the semantic meaning of words and is a fine choice for text classification and sentiment analysis. The sequences formed are much shorter since every word can be converted into a unique token.\nThe vocabulary size is very large since individual tokens cannot be broken down or recombined in new contexts. It also struggles with out-of-vocabulary words, since there are no base tokens to build upon.\nSub-word Encoding Sub-word encoding is a compromise between character and word encoding. It is able to capture the semantic meaning of words and can be broken down into smaller tokens. This allows for the model to generalize better to unseen words and phrases. Most large language models use sub-word encoding.\nByte Pair Encoding (BPE) Byte Pair Encoding is a sub-word encoding technique that was originally designed for data compression. It is a simple algorithm that iteratively merges the most frequent pair of bytes in a sequence. This process is repeated until a predefined vocabulary size is reached.\nThe algorithm is as follows:\nInitialize the vocabulary with all the characters in the input. Count the frequency of all pairs of characters in the vocabulary. Merge the most frequent pair of characters. Update the vocabulary with the merged pair. Repeat steps 2-4 until the vocabulary size reaches a predefined limit. Embeddings A word embedding is a learned representation of text in which semantically similar words are mapped to nearby points in the embedding space. Since they are represented as vectors, all vector operations can be applied to them. This allows for the model to learn relationships between words and phrases, quantify their similarities and differences, and encode higher-level context information.\nEmbeddings can be learned independently or jointly with the model. For example, the Word2Vec model learns embeddings using an unsupervised approach. It predicts the context of a word given its surrounding words. The embeddings are then used as input to a downstream task (Mikolov et al. 2013).\nLLMs typically train embeddings jointly with the model. This allows them to learn embeddings for sentences, paragraphs, or even whole documents.\nCreating an embedding layer We can use libraries such as PyTorch to create a learnable embedding layer. The code below creates an embedding layer that converts each individual token into a `1024` dimensional embedded layer.\nimport torch.nn as nn import torch token_embedding = nn.Embedding(vocab_size, 1024) prompt_embedded = token_embedding(torch.LongTensor(encode(prompt))) print(prompt_embedded.shape) Training the embeddings Our corpus of a single C file is far too small to learn anything meaningful. Learning an embedding space requires a lot data and compute power. We can instead look at pre-trained embeddings. Huggingface has a large collection of pre-trained models that can be used for a variety of tasks. The accompanying notebook uses embeddings from SentenceTransformer to demonstrate how embeddings can be used in practice.\nSentence Embeddings To demonstrate the power of embeddings, we will close out the workshop by reviewing sentence embeddings. BERT (Devlin et al. 2019) and RoBERTa (Liu et al. 2019) are LLMs that perform tasks such as semantic textual similarity. They both require that whole sentences be input, resulting in a very expensive computation.\nSentence-BERT proposed an architecture that would embed these into meaningful embeddings that could be easily compared with vector operations (Reimers and Gurevych 2019).\nIn the cells below, we will use Huggingface (huggingface.co) to download and use a pre-trained sentence transformer. This particular one was trained on 1,170,060,424 sentence pairs.\nReferences Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv. https://doi.org/10.48550/arXiv.1810.04805. Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” arXiv. https://doi.org/10.48550/arXiv.1907.11692. Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” arXiv. http://arxiv.org/abs/1301.3781. Reimers, Nils, and Iryna Gurevych. 2019. “Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks.” arXiv. https://doi.org/10.48550/arXiv.1908.10084. ","date":1712811600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712811600,"objectID":"846c28ea863e434a0fc5f44bc423e31d","permalink":"https://ajdillhoff.github.io/articles/the-language-of-llms/","publishdate":"2024-04-11T00:00:00-05:00","relpermalink":"/articles/the-language-of-llms/","section":"articles","summary":"How do LLMs read and process the high dimensional landscape of text efficiently? Presented as a workshop at UTA's Datathon on April 13, 2024.","tags":["article","machine learning","llms"],"title":"The Language of LLMs","type":"articles"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Parallelization over vertices Parallelization over edges Improving work efficiency Privatization to reduce contention Additional Optimizations Introduction For an introduction on basic graph theory and traversal algorithms, see these notes.\nParallelization over vertices When it comes to parallel algorithms, the first thing that may come to your mind is that they must operate on either the edges or the vertices. In either case, it doesn\u0026rsquo;t take much to imagine that such algorithms will require communication between threads due to dependencies in the graph or traversal algorithm.\nIn a breadth-first search, all vertices in one level must be explored before moving into the next. The first approach we look at will require multiple calls to the kernel, one for each level.\nTop-down Approach __global__ void bfs_kernel(CSRGraph csrGraph, uint *level, uint *newVertexVisited, uint *currLevel) { int vertex = blockIdx.x * blockDim.x + threadIdx.x; if (vertex \u0026lt; csrGraph.numVertices) { if (level[vertex] == currLevel - 1) { for (uint edge = csrGraph.srcPtrs[vertex]; edge \u0026lt; csrGraph.srcPtrs[vertex + 1]; edge++) { int neighbor = csrGraph.edges[edge]; if (level[neighbor] == UINT_MAX) { // Neighbor not visited level[neighbor] = currLevel; *newVertexVisited = 1; } } } } } The first kernel above labels the vertices that belong on the given level. For each vertex, it iterates over the outgoing edges. These can be accessed as the nonzero elements in the adjacency matrix for each row. The CSR format is ideal for this case. The figure below shows the result of the first kernel. Only 2 of the threads are active for the first level based on the input graph. This particular version of the algorithm is called the push version.\nFigure 1: Vertex-centric push BFS traversal from level 1 to level 2 (Hwu, Kirk, and El Hajj 2022). The boundary check ensures that the kernel only processes vertices that belong to the current level. Each thread has access to a global newVertexVisited and will set this to 1 if it finds a new vertex to visit. This is used to determine if the traversal should continue to the next level.\nBottom-up Approach The second kernel is also a vertex-centric approach, except it considers incoming edges rather than outgoing ones. This is called the pull version.\n__global__ void bfs_kernel(CSCGraph cscGraph, uint *level, uint *newVertexVisited, uint *currLevel) { int vertex = blockIdx.x * blockDim.x + threadIdx.x; if (vertex \u0026lt; cscGraph.numVertices) { if (level[vertex] == UINT_MAX) { for (uint edge = cscGraph.dstPtrs[vertex]; edge \u0026lt; cscGraph.dstPtrs[vertex + 1]; edge++) { int neighbor = cscGraph.edges[edge]; if (level[neighbor] == currLevel - 1) { // Neighbor visited level[vertex] = currLevel; *newVertexVisited = 1; break; } } } } } Note the use of the Compressed Sparse Column (CSC) format for the graph. The kernel requires that each thread be able to access the incoming edges, which would be determined by the nonzero elements of a give column of the adjacency matrix.\nFigure 2: Vertex-centric pull BFS traversal from level 1 to level 2 (Hwu, Kirk, and El Hajj 2022). From each thread\u0026rsquo;s point of view, if there is an incoming edge at the previous level then it will be visited at the current level. In that case its job is done and it can break out of the loop. This approach is more efficient for graphs with a high average degree and variance.\nIn early levels, the push approach is more efficient because they have a relatively smaller number of vertices per level. As more vertices are visited, the pull approach is more efficient because there is a higher chance of finding an incoming edge and exiting early. Since each level is a separate kernel call, both of these can be combined. An additional piece of overhead would be that one would require both CSR and CSC representations of the graph.\nParallelization over edges As the name suggests, the edge-centric approach processes the edges in parallel. If the source vertex belongs to a previous level and the destination vertex is unvisited, then the destination vertex is labeled with the current level.\n__global__ void bfs_kernel(COOGraph cooGraph, uint *level, uint *newVertexVisited, uint *currLevel) { int edge = blockIdx.x * blockDim.x + threadIdx.x; if (edge \u0026lt; csrGraph.numEdges) { uint vertex = cooGraph.src[edge]; if (level[vertex] == currLevel - 1) { uint neighbor = cooGraph.dst[edge]; if (level[neighbor] == UINT_MAX) { level[neighbor] = currLevel; *newVertexVisited = 1; } } } } The edge-centric approach has more parallelism than the vertex-centric approaches. Graphs typically have more edges than vertices, so the largest benefit is on smaller graphs. Another advantage related to load imbalance. In the vertex-centric approach, imbalance comes from the fact that some vertices have more edges than others.\nThe tradeoff of this that every edge is considered. There may be many edges that are not relevant for a particular level. The vertex-centric approach could skip these entirely. Since every edge needs to be indexed, the COO format is used. This requires more space than the CSR or CSC formats.\nSince these sparse representations are already used, we could perform these operations using sparse matrix multiplications. Libraries such as cugraph provide implementations of these algorithms.\nImproving work efficiency The previous two approaches have a common problem: it is likely that many threads will perform no useful work. Take the vertex-centric approach, for example. The threads that are launched only to find out that their vertex is not in the current level will not do anything. This is a waste of resources. Ideally, those threads would not be launched in the first place. A simple solution to this is to have each thread build a frontier of vertices that they visit, so that only the vertices in the frontier are processed.\n__global__ void bfs_kernel(CSRGraph csrGraph, uint *level, uint *prevFrontier, uint *currFrontier, uint numPrevFrontier, uint *numCurrFrontier, uint currLevel) { uint i = blockIdx.x * blockDim.x + threadIdx.x; if (i \u0026lt; numPrevFrontier) { uint vertex = prevFrontier[i]; for (uint edge = csrGraph.srcPtrs[vertex]; edge \u0026lt; csrGraph.srcPtrs[vertex + 1]; edge++) { uint neighbor = csrGraph.dst[edge]; if (atomicCAS(\u0026amp;level[neighbor], UINT_MAX, currLevel) == UINT_MAX) { uint currFrontierIdx = atomicAdd(numCurrFrontier, 1); currFrontier[currFrontierIdx] = neighbor; } } } } When launched, only the threads corresponding to a frontier will be active. They start by loading the elements from the previous frontier. As before, they iterate over the outgoing edges. If the neighbor has not been visited, then it is labeled with the current level and added to the current frontier. The atomic operation ensures that the size of the frontier is updated correctly.\nThe call to atomicCAS prevents multiple threads from adding the same vertex to the frontier. It checks whether the current vertex is unvisited. Not every thread is going to visit the same neighbor, so the contention should be low for this call.\nPrivatization to reduce contention The use of atomic operations in the previous example introduces contention between threads. As we have previously studied, privatization can be applied in these cases to reduce that contention. In this case, each block will have its own private frontier. The contention is then reduced to atomic operations within a block. An added benefit is that the local frontier is in shared memory, resulting in lower latency atomic operations.\n__global__ void bfs_kernel(CSRGraph csrGraph, uint *level, uint *prevFrontier, uint *currFrontier, uint numPrevFrontier, uint *numCurrFrontier, uint currLevel) { // Initialize privatized frontier __shared__ uint currFrontier_s[LOCAL_FRONTIER_CAPACITY]; __shared__ uint numCurrFrontier_s; if (threadIdx.x == 0) { numCurrFrontier_s = 0; } __syncthreads(); // Perform BFS uint i = blockIdx.x * blockDim.x + threadIdx.x; if (i \u0026lt; numPrevFrontier) { uint vertex = prevFrontier[i]; for (uint edge = csrGraph.srcPtrs[vertex]; edge \u0026lt; csrGraph.srcPtrs[vertex + 1]; edge++) { uint neighbor = csrGraph.dst[edge]; if (atomicCAS(\u0026amp;level[neighbor], UINT_MAX, currLevel) == UINT_MAX) { uint currFrontierIdx_s = atomicAdd(\u0026amp;numCurrFrontier_s, 1); if (currFrontierIdx_s \u0026lt; LOCAL_FRONTIER_CAPACITY) { currFrontier_s[currFrontierIdx_s] = neighbor; } else { numCurrFrontier_s = LOCAL_FRONTIER_CAPACITY; uint currFrontierIdx = atomicAdd(numCurrFrontier, 1); currFrontier[currFrontierIdx] = neighbor; } } } } __syncthreads(); // Allocate in global frontier __shared__ uint currFrontierStartIdx; if (threadIdx.x == 0) { currFrontierStartIdx = atomicAdd(numCurrFrontier, numCurrFrontier_s); } __syncthreads(); // Commit to global frontier for (uint currFrontierIdx_s = threadIdx.x; currFrontierIdx_s \u0026lt; numCurrFrontier_s; currFrontierIdx_s += blockDim.x) { currFrontier[currFrontierStartIdx + currFrontierIdx_s] = currFrontier_s[currFrontierIdx_s]; } } The main BFS block of the kernel above will write to the local frontier as long as there is space. If the capacity is hit, all future writes will go to the global frontier.\nAfter BFS completes, a representative thread (index 0) from each block will allocate space in the global frontier, giving it a unique starting index. This allows each block to safely write to the global frontier without contention. The figure below shows the result of the privatized frontier.\nFigure 3: Privatized frontier for BFS traversal (Hwu, Kirk, and El Hajj 2022). Additional Optimizations Reducing launch overhead If the frontiers of a BFS are small, the overhead of launching a kernel for each level can be significant. In such cases, a kernel with a grid size of 1 can be launched to handle multiple levels. This block would synchronize after each level to ensure that all threads have completed the current level before moving on to the next.\nFigure 4: Reducing launch overhead by handling multiple levels in a single kernel (Hwu, Kirk, and El Hajj 2022). Improving load balance In the first vertex-centric approach we looked at, the threads were not evenly balanced due to the fact that some vertices have more edges than others. For graphs that have high variability in the number of edges per vertex, the frontier can be sorted and placed into multiple buckets. The buckets would be processed by a separate kernel.\nReferences Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. Programming Massively Parallel Processors: A Hands-on Approach. Fourth. Morgan Kaufmann. ","date":1712435160,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712435160,"objectID":"eea59c706d3e3470ad9c481dccbe0a45","permalink":"https://ajdillhoff.github.io/notes/parallel_graph_traversal/","publishdate":"2024-04-06T15:26:00-05:00","relpermalink":"/notes/parallel_graph_traversal/","section":"notes","summary":"Table of Contents Introduction Parallelization over vertices Parallelization over edges Improving work efficiency Privatization to reduce contention Additional Optimizations Introduction For an introduction on basic graph theory and traversal algorithms, see these notes.\nParallelization over vertices When it comes to parallel algorithms, the first thing that may come to your mind is that they must operate on either the edges or the vertices. In either case, it doesn\u0026rsquo;t take much to imagine that such algorithms will require communication between threads due to dependencies in the graph or traversal algorithm.","tags":["gpgpu"],"title":"Parallel Graph Traversal","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Topological Sort Strongly Connected Components Application: Recommender Graphs Topological Sort A topological sort of a directed acyclic graph \\(G = (V, E)\\) is a linear ordering of all its vertices such that for every directed edge \\((u, v) \\in E\\), vertex \\(u\\) comes before vertex \\(v\\) in the ordering.\nThe process itself can be described simply:\nCall \\(\\text{DFS}(G)\\) to compute the finishing times for each vertex \\(v\\). As each vertex is finished, insert it onto the front of a linked list. Return the linked list of vertices. The entire call takes \\(\\Theta(V + E)\\) since DFS\\((G)\\) takes \\(\\Theta(V + E)\\) time. Inserting each vertex onto the front of the list can be done in constant time.\nLemma A directed graph \\(G\\) is acyclic if and only if a DFS of \\(G\\) yields no back edges \u0026ndash; an edge \\((u, v)\\) such that \\(v\\) is an ancestor of \\(u\\) in the DFS forest.\nProof The proof is by contradiction: if a back edge exists, then there is a cycle in the graph.\nFigure 1: A back edge between edges (u) and (v) (Cormen et al. 2022). Suppose there is a back edge \\((u, v)\\) as shown in the figure above. In this case, \\(v\\) is an ancestor of \\(u\\) in the depth-first forest. There is a path \\(v \\leadsto u\\), so \\(v \\leadsto u \\rightarrow v\\) is a cycle.\nIn the other direction, suppose that \\(G\\) contains a cycle \\(c\\). Let \\(v\\) be the first vertex discovered in \\(c\\), and let \\((u, v)\\) be the preceding edge in \\(c\\). At time \\(v.d\\), vertices of \\(c\\) form a white path \\(v \\leadsto u\\). Since \\(u\\) is a descendant of \\(v\\), \\((u, v)\\) is a back edge. \\(\\blacksquare\\)\nTheorem The topological sort algorithm produces a topological sort of a directed acyclic graph.\nProof Run a DFS on the graph \\(G\\) to determine finish times for its vertices. For any pair of vertices \\((u, v)\\), if \\(G\\) contains an edge from \\(u\\) to \\(v\\), then \\(v.f \u0026lt; u.f\\). (Review DFS).\nExample Figure 2: DAG for topological sorting. Figure 20.8 from (Cormen et al. 2022). Strongly Connected Components Figure 3: Strongly connected components of a directed graph (Cormen et al. 2022). A strongly connected component of a directed graph \\(G\\) is a maximal set of vertices such that for every pair of vertices \\(u\\) and \\(v\\) in the set, there is a path from \\(u\\) to \\(v\\) and a path from \\(v\\) to \\(u\\). The algorithm goes as follows:\nCall \\(\\text{DFS}(G)\\) to compute the finishing times for each vertex \\(v\\). Compute the transpose of \\(G\\). Call \\(\\text{DFS}(G^T)\\), but in the main loop of DFS, consider the vertices in order of decreasing finishing times. Output the vertices of each tree in the depth-first forest as a separate strongly connected component. The transpose of a graph \\(G^T\\) is the graph \\(G\\) with all edges reversed.\n\\begin{align*} G^T \u0026amp;= (V, E^T) \\\\ E^T \u0026amp;= \\{(v, u) \\mid (u, v) \\in E\\} \\end{align*}\nComponent Graphs The resulting component graph is a directed graph \\(G_{SCC} = (V_{SCC}, E_{SCC})\\) where each vertex represents a strongly connected component of the original graph \\(G\\). There is an edge \\((C_i, C_j)\\) in \\(G_{SCC}\\) if there is a vertex \\(u \\in C_i\\) and a vertex \\(v \\in C_j\\) such that \\((u, v) \\in E\\). The component graph of the DAG from above is shown below.\nFigure 4: Component graph of the DAG from above (Cormen et al. 2022). Lemma The component graph \\(G^{SCC}\\) is a directed acyclic graph. Let \\(C\\) and \\(C\u0026rsquo;\\) be distinct strongly connected components in \\(G\\), where \\(u, v \\in C\\) and \\(u\u0026rsquo;, v\u0026rsquo; \\in C\u0026rsquo;\\), and suppose there is a path \\(u \\leadsto u\u0026rsquo;\\) in \\(G\\). Then there cannot also be a path \\(v\u0026rsquo; \\leadsto v\\) in \\(G\\).\nProof Suppose there is a path \\(v\u0026rsquo; \\leadsto v\\) in \\(G\\). This implies there are paths \\(u \\leadsto u\u0026rsquo; \\leadsto v\u0026rsquo;\\) and \\(v\u0026rsquo; \\leadsto v \\leadsto u\\). If this were possible, then \\(u\\) and \\(v\u0026rsquo;\\) are reachable from each other, which contradicts the assumption that \\(C\\) and \\(C\u0026rsquo;\\) are distinct strongly connected components. \\(\\blacksquare\\)\nExample Finishing Times The previous and following lemmas establish, given the algorithm presented above, the rules of the finishing times of strongly connected components. These are used to prove the correctness of the algorithm.\nLemma Let \\(C\\) and \\(C\u0026rsquo;\\) be strongly connected components in a directed graph \\(G\\). If there is an edge \\((u, v) \\in E\\) such that \\(u \\in C\\) and \\(v \\in C\u0026rsquo;\\), then \\(f( C) \u0026gt; f(C\u0026rsquo;)\\).\nProof There are two cases to consider depending on which strongly connected component had the first discovered vertex during the first DFS call.\nCase 1\nIf \\(d( C) \u0026lt; d(C\u0026rsquo;)\\), let \\(x\\) be the first vertex discovered in \\(C\\). At time \\(x.d\\), the time of discovery, all vertices in \\(C\\) and \\(C\u0026rsquo;\\) are white. Thus, there exists paths of white vertices from \\(x\\) to all vertices in \\(C\\) and \\(C\u0026rsquo;\\). By the white-path theorem, all vertices in \\(C\\) and \\(C\u0026rsquo;\\) are descendants of \\(x\\) in the depth-first tree. By the parenthesis theorem, \\(x.f = f( C) \u0026gt; f(C\u0026rsquo;)\\). Case 2\nIf \\(d( C) \u0026gt; d(C\u0026rsquo;)\\), let \\(y\\) be the first vertex discovered in \\(C\u0026rsquo;\\). At time \\(y.d\\), all vertices in \\(C\\) and \\(C\u0026rsquo;\\) are white. Thus, there exists paths of white vertices from \\(y\\) to all vertices in \\(C\u0026rsquo;\\). All vertices in \\(C\u0026rsquo;\\) become descendants of \\(y\\). Again, \\(y.f = f(C\u0026rsquo;)\\). At time \\(y.d\\), all vertices in \\(C\\) are also white. Since there is an edge \\((u, v)\\), where \\(u \\in C\\) and \\(u\u0026rsquo; \\in C\u0026rsquo;\\), we cannot have a path from \\(C\u0026rsquo;\\) to \\(C\\). No vertex in \\(C\\) is reachable from \\(y\\). Therefore, at time \\(y.f\\), all vertices in \\(C\\) are white. Therefore, for all \\(w \\in C, w.f \u0026gt; y.f\\), which implies that \\(f( C) \u0026gt; f(C\u0026rsquo;)\\). \\(\\blacksquare\\) Corollary Let \\(C\\) and \\(C\u0026rsquo;\\) be distinct strongly connected components in \\(G\\). Suppose there is an edge \\((u, v) \\in E^T\\), where \\(u \\in C\\) and \\(v \\in C\u0026rsquo;\\). Then \\(f( C) \u0026lt; f(C\u0026rsquo;)\\).\nProof \\((u, v) \\in E^T \\implies (v, u) \\in E\\) Since strongly connected components of \\(G\\) and \\(G^T\\) are the same, \\(f(C\u0026rsquo;) \u0026gt; f( C)\\). Correctness Now we can combine the previous results to prove the correctness of the algorithm.\nCorollary Let \\(C\\) and \\(C\u0026rsquo;\\) be distinct strongly connected components in \\(G\\), and suppose that \\(f( C) \u0026gt; f(C\u0026rsquo;)\\). Then there cannot be an edge from \\(C\\) to \\(C\u0026rsquo;\\) in \\(G^T\\).\nProof When we perform the second DFS call, on \\(G^T\\), it starts with the component \\(C\\) such that \\(f( C)\\) is the maximum. This call starts from some \\(x \\in C\\) and explores all vertices in \\(C\\). The corollary says that since \\(f( C) \u0026gt; f(C\u0026rsquo;)\\), there cannot be an edge from \\(C\\) to \\(C\u0026rsquo;\\) in \\(G^T\\). Therefore, DFS will visit only vertices in \\(C\\). This means that the depth-first tree rooted at \\(x\\) will contain only vertices in \\(C\\).\nThe next root chosen is in \\(C\u0026rsquo;\\) such that \\(f(C\u0026rsquo;)\\) is maximum over all strongly connected components other than \\(C\\). DFS visits all vertices in \\(C\u0026rsquo;\\), but the only edges out of \\(C\u0026rsquo;\\) go to \\(C\\), which have already been visited. Therefore, the only tree edges will be to vertices in \\(C\u0026rsquo;\\).\nAs this process continues, we can observe that each root chosen for the second DFS can reach only\nvertices in its own strongly connected component, and vertices in strongly connected components already visited in the second DFS. Application: Recommender Graphs A recommender graph is a directed graph where each vertex represents an item and each edge represents a transition between the items based on the context of the data. For example, in a movie recommender graph, each vertex represents a movie, and an edge from \\(u\\) to \\(v\\) indicates that users typically transitioned from watching movie \\(u\\) to watching movie \\(v\\). The weight of such an edge could be the number of users who made the transition or the average rating improvement when moving from \\(u\\) to \\(v\\) (Lamprecht, Strohmaier, and Helic 2017).\nFigure 5: A recommender graph (Lamprecht, Strohmaier, and Helic 2017). In such a graph, strongly connected components can be used to identify groups of items that are closely related. Based on this information, a recommender system can suggest items that are similar to the ones a user has already interacted with. If the edges contained information such as improvement of ratings, the recommendation system could suggest items that are likely to be enjoyed by the user.\nIdentifying such a strongly connected component can also provide insights into the structure of the data. Given the current recommender graph, it is possible that the strongly connected component related to a particular sub-genre of movies is small, leading to a cycle of recommendations within that sub-genre. This discovery would prompt the recommender system to suggest items from other genres to provide a more diverse set of recommendations.\nReferences Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. Introduction to Algorithms. 4th ed. MIT Press. http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/. Lamprecht, Daniel, Markus Strohmaier, and Denis Helic. 2017. “A Method for Evaluating Discoverability and Navigability of Recommendation Algorithms.” Computational Social Networks 4 (1): 9. https://doi.org/10.1186/s40649-017-0045-3. ","date":1712244060,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712244060,"objectID":"d12d71440704c74a6cc07bde2cb62166","permalink":"https://ajdillhoff.github.io/notes/topological_sort/","publishdate":"2024-04-04T10:21:00-05:00","relpermalink":"/notes/topological_sort/","section":"notes","summary":"Table of Contents Topological Sort Strongly Connected Components Application: Recommender Graphs Topological Sort A topological sort of a directed acyclic graph \\(G = (V, E)\\) is a linear ordering of all its vertices such that for every directed edge \\((u, v) \\in E\\), vertex \\(u\\) comes before vertex \\(v\\) in the ordering.\nThe process itself can be described simply:\nCall \\(\\text{DFS}(G)\\) to compute the finishing times for each vertex \\(v\\).","tags":["algorithms","computer science"],"title":"Topological Sort","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Radix Sort Optimizing Memory Access Efficiency Choosing a different Radix value Radix Sort For a background on Radix Sort, see these notes on Sorting in Linear Time.\nRadix sort relies on counting sort for each section, and each section must be processed before moving onto the next. The parallel solution will not attempt to address this sequential dependency. Instead, we will focus on the parallelization of the counting sort step.\nEach thread must determine where to place its input elements. For each bit, the thread will assign it to either a 0 or 1 bucket. Since all values will either be 0 or 1, the thread needs to compute the number of 0s and 1s that come before it in the current section. Radix sort is also a stable sort, so the order of elements with the same key must be preserved. Consider the following array separated into 4 threads of 4 elements each:\n\\begin{array}{l|cccc|cccc|cccc|cccc} Value \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 1 \u0026amp; 1 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0\\\\ Index \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 4 \u0026amp; 5 \u0026amp; 6 \u0026amp; 7 \u0026amp; 8 \u0026amp; 9 \u0026amp; 10 \u0026amp; 11 \u0026amp; 12 \u0026amp; 13 \u0026amp; 14 \u0026amp; 15\\\\ \\end{array}\nThe least significant bits for each thread are \\([1, 0, 1, 0]\\). From thread 4\u0026rsquo;s perspective, there are 2 1s and a single 0 that come before it. Its key index is 3 (using 0-based indexing), so it only needs to compute the number of 1s that come before it and subtract that from its key index: \\(3 - 2 = 1\\). More generally, for a 0 bit:\n\\[ \\text{output index} = \\text{key index} - \\text{number of 0s that come before it} \\]\nThe calculation for the 1 bit hinges on the fact that all keys mapping to 0 must come before it.\n\\[ \\text{output index} = \\text{input size} - \\text{number of ones total} + \\text{number of 1s that come before it} \\]\n__global__ void radix_sort_iter(unsigned int *input, unsigned int *output, unsigned int *bits, unsigned int N, unsigned int iter) { unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x; unsigned int key, bit; if (idx \u0026lt; N) { key = input[i]; bit = (key \u0026gt;\u0026gt; iter) \u0026amp; 1; bits[i] = bit; } exclusiveScan(bits, N); if (idx \u0026lt; N) { unsigned int numOnesBefore = bits[idx]; unsigned int numOnesTotal = bits[N]; unsigned int dst = (bit == 0) ? idx - numOnesBefore : N - numOnesTotal + numOnesBefore; output[dst] = key; } } Example Consider the fourth thread with idx = 3 using the array from above. This thread index is certainly less than the array size, so the key is read from input before extracting the least significant bit. Note that there is a call to thread synchronization inside exclusiveScan. The result of exclusiveScan is an array that indicates, for each index, the number of ones that came before it. For our array, this is:\n\\[ [0, 1, 1, 2] \\]\nThe destination can be computed for each thread. The result is shown in the table below.\nThread 0 1 2 3 Bit 1 0 1 0 #1s Before 0 1 1 2 #1s Total 2 2 2 2 Output Index 2 0 3 1 Optimizing Memory Access Efficiency Each thread write their keys to global memory in an uncoalesced manner. This can be optimized by having each block maintain local buckets in shared memory. The keys within each block will be coalesced when written to global memory.\nTODO: Show visualization similar to 13.5\nIn order to make this work, each thread needs to calculate where in the output array the values from its bucket should be placed. For 0 bits, the block\u0026rsquo;s 0 bucket will come after the 0 buckets from all previous blocks. These positions can be computed by performing an exclusive scan on the block\u0026rsquo;s local bucket sizes.\nTODO: Show visualization similar to 13.6\nChoosing a different Radix value Picking a larger radix value will reduce the number of iterations required to sort the array.\n","date":1711900200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711900200,"objectID":"e8c0b900ff343076d86c3613e0661e10","permalink":"https://ajdillhoff.github.io/notes/parallel_sorting_algorithms/","publishdate":"2024-03-31T10:50:00-05:00","relpermalink":"/notes/parallel_sorting_algorithms/","section":"notes","summary":"Table of Contents Radix Sort Optimizing Memory Access Efficiency Choosing a different Radix value Radix Sort For a background on Radix Sort, see these notes on Sorting in Linear Time.\nRadix sort relies on counting sort for each section, and each section must be processed before moving onto the next. The parallel solution will not attempt to address this sequential dependency. Instead, we will focus on the parallelization of the counting sort step.","tags":["gpgpu"],"title":"Parallel Sorting Algorithms","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Coordinate List Format (COO) Compressed Sparse Row Format (CSR) ELL Format ELL-COO Format Jagged Diagonal Storage Format (JDS) Introduction Sparse matrices are matrices with mostly zero elements. They are common in scientific computing, machine learning, and other fields. It is important to study them in the context of GPU computing because they can be very large and require a lot of memory. Effeciently representing and computing with sparse matrices provides a substantial benefit to many applications.\nThe obvious benefits of sparsity is that we can typically represent the same matrix using a smaller memory footprint. Fewer elements means fewer wasted operations as well. The challenges of GPU implementations are that the memory access patterns are not always friendly to the GPU architecture. This is especially true for sparse matrices, where the memory access patterns are often irregular.\nThese notes will review the sparse matrix formats as presented in (Hwu, Kirk, and El Hajj 2022). Each will be evaluated using the following criteria:\nCompaction: How well does the format compact the data? Flexibility: Is the format easy to modify? Accessibility: How easy is it to access the data? Memory access efficiency: Are the accesses coalesced? Load balance: Are the operations balanced across threads? Coordinate List Format (COO) This format stores non-zero elements in a 1D array of values. It also requires two 1D arrays to store the row and column indices, incurrent an overhead of 2N. The values in each array are contiguous, which is good for memory access.\nFigure 1: COO Format (Hwu, Kirk, and El Hajj 2022). Kernel Implementation __global__ void spmv_coo_kernel(COOMatrix cooMatrix, float *x, float *y) { int i = blockIdx.x * blockDim.x + threadIdx.x; if (i \u0026lt; cooMatrix.numNonzeros) { int row = cooMatrix.rowIdx[i]; int col = cooMatrix.colIdx[i]; float val = cooMatrix.values[i]; atomicAdd(\u0026amp;y[row], val * x[col]); } } Evaluation Compaction: Compared to representing the matrices in dense format, the COO format is very compact. However, it is not as compact as some other sparse matrix formats. It requires an additional over head of 2N elements to store the row and column indices. Flexibility: Indices and values can be easily modified in this format. This is good for applications that require frequent modifications. Accessibility: It is easy to access nonzero elements. It is not easy to access the original 0s in each row. Memory access efficiency: The values in this format are contiguous, resulting in coalesced memory access. Load balance: The data is uniformly distributed across threads, resulting in good load balance. One major drawback, as seen in the code above, is the use of atomic operations.\nCompressed Sparse Row Format (CSR) The key idea of this format is that each thread is responsible for all nonzeros in a row.\nFigure 2: CSR Format (Hwu, Kirk, and El Hajj 2022). Kernel Implementation __global__ void spmv_csr_kernel(CSRMatrix csrMatrix, float *x, float *y) { int row = blockIdx.x * blockDim.x + threadIdx.x; if (row \u0026lt; csrMatrix.numRows) { float sum = 0.0f; for (int j = csrMatrix.rowPtr[row]; j \u0026lt; csrMatrix.rowPtr[row + 1]; j++) { sum += csrMatrix.values[j] * x[csrMatrix.colIdx[j]]; } y[i] = sum; } } The rows are mapped to a single pointer index, so it only needs \\(m\\) entries to store them. The columns are not required to be in order. If the columns are in order, the data is represented in row-major order without the zero elements.\nEvaluation Compaction: The CSR format is more compact than the COO format since it only requires \\(m\\) entries to store the row pointers. Flexibility: The CSR format is not as flexible as the COO format. It is not easy to modify the values or indices. Accessibility: There is less parallelization than COO due to the row sizes. Memory access efficiency: The memory access pattern is poor since the data is separated over columns. Load balance: The load is not balanced across threads. Some threads will have more work than others, leading to control divergence. ELL Format ELL fixes the non-coalesced memory accesses of CSR via data padding and transposition. This is visualized below:\nStart with CSR format Pad rows to equal size Store in column-major order Figure 3: ELL Format (Hwu, Kirk, and El Hajj 2022). Kernel Implementation __global__ void spmv_ell_kernel(ELLMatrix ellMatrix, float *x, float *y) { int row = blockIdx.x * blockDim.x + threadIdx.x; if (row \u0026lt; ellMatrix.numRows) { float sum = 0.0f; for (int j = 0; j \u0026lt; ellMatrix.nnzPerRow[row]; j++) { int col = ellMatrix.colIdx[j * ellMatrix.numRows + row]; sum += ellMatrix.values[j * ellMatrix.numRows + row] * x[col]; } y[row] = sum; } } Evaluation Compaction: Padding the rows means this is less space efficient than CSR. Flexibility: More flexible than CSR; adding nonzeros in CSR requires a shift of values. This format can replaced a padded element if necessary. Accessibility: ELL can return the row given the index of a nonzero element as well as the nonzero of a row given that index. Memory access efficiency: Consecutive threads access consecutive memory locations. Load balance: Shares the same control divergence issues as CSR. ELL-COO Format ELL-COO combines the two formats to improve space efficiency and control divergence.\nFigure 4: ELL-COO Format (Hwu, Kirk, and El Hajj 2022). Evaluation Compaction: ELL-COO has the same compaction as ELL. Flexibility: ELL-COO is more flexible than ELL thanks to inclusion of the COO format. Accessibility: It is not always possible to access all nonzeros given a row index. Memory access efficiency: The memory access pattern is coalesced. Load balance: COO reduces the control divergence seen in ELL alone. Jagged Diagonal Storage Format (JDS) The last format we will consider is the Jagged Diagonal Storage format. This format reduces divergence and improves memory coalescing without padding. The main idea is to sort the rows by length from longest to shortest.\nGroup nonzeros by row Sort rows by length while preserving their original row indices Store in column-major order Figure 5: JDS Format (Hwu, Kirk, and El Hajj 2022). Evaluation Compaction: Avoid paddding, so it is more space efficient than ELL. Flexibility: Less flexible than ELL since it requires sorting when adding new elements. Accessibility: Cannot access a row and column given the index of a nonzero element. Memory access efficiency: Without padding, the starting location of memory accesses in each iteration can vary. Load balance: Since the rows are sorted, threads of the same warp are likely to iterate over rows of similar length. References Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. Programming Massively Parallel Processors: A Hands-on Approach. Fourth. Morgan Kaufmann. ","date":1711812960,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711812960,"objectID":"ce88772ee73133b70ebe7984bb022621","permalink":"https://ajdillhoff.github.io/notes/sparse_matrix_computation/","publishdate":"2024-03-30T10:36:00-05:00","relpermalink":"/notes/sparse_matrix_computation/","section":"notes","summary":"Table of Contents Introduction Coordinate List Format (COO) Compressed Sparse Row Format (CSR) ELL Format ELL-COO Format Jagged Diagonal Storage Format (JDS) Introduction Sparse matrices are matrices with mostly zero elements. They are common in scientific computing, machine learning, and other fields. It is important to study them in the context of GPU computing because they can be very large and require a lot of memory. Effeciently representing and computing with sparse matrices provides a substantial benefit to many applications.","tags":["gpgpu"],"title":"Sparse Matrix Computation","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Example 4.13 from CLRS Visualizing the characteristics of an algorithm is a great way to build intuition about its runtime. Although it can be used to prove a recurrence, it is often a good jumping off point for the Substitution Method.\nExample 4.13 from CLRS Consider the recurrence \\(T(n) = 3T(n/4) + \\Theta(n^2)\\). We start by describing \\(\\Theta(n^2) = cn^2\\), where the constant \\(c \u0026gt; 0\\) serves as an upper-bound constant. It reflects the amount of work done at each level of the recursion tree. The tree is shown below.\nFigure 1: Example 4.13 from CLRS (Cormen et al. 2022) As the tree expands out over a few levels, we can see a pattern in the cost at depth \\(i\\). Each level of increasing depth has 3 times as many nodes as the previous. With the exception of the leaves, the cost for each level is \\((\\frac{3}{16})^i cn^2\\). The total cost of the leaves is based on the number of leaves, which is \\(3^{\\log_4 n}\\) since each level has \\(3^i\\) nodes and the depth is \\(\\log_4 n\\). Using the identity \\(a^{\\log_b c} = c^{\\log_b a}\\), we can simplify the leaves to \\(n^{\\log_4 3}\\). The total cost of the leaves is \\(\\Theta(n^{\\log_4 3})\\).\nThe last step is to add up the costs over all levels:\n\\begin{align*} T(n) \u0026amp;= \\sum_{i=0}^{\\log_4 n} \\left( \\frac{3}{16} \\right)^i cn^2 + \\Theta(n^{\\log_4 3}) \\\\ \u0026amp;\u0026lt; \\sum_{i=0}^{\\infty} \\left( \\frac{3}{16} \\right)^i cn^2 + \\Theta(n^{\\log_4 3}) \\\\ \u0026amp;= \\frac{cn^2}{1 - \\frac{3}{16}} + \\Theta(n^{\\log_4 3}) \\\\ \u0026amp;= \\frac{16}{13}cn^2 + \\Theta(n^{\\log_4 3}) \\\\ \u0026amp;= \\Theta(n^2). \\end{align*}\nThe second line in the equation is a geometric series.\nVerifying using the Substitution Method Even if we weren\u0026rsquo;t so particular with the maths, the recursion tree method is a great way to build intuition about the runtime. Let\u0026rsquo;s verify that this recurrence is bounded above by \\(O(n^2)\\) using the Substitution Method.\nHere we show that \\(T(n) \\leq dn^2\\) for a constant \\(d \u0026gt; 0\\). The previous constant \\(c \u0026gt; 0\\) is reused to describe the cost at each level of the recursion tree.\n\\begin{align*} T(n) \u0026amp;\\leq 3T(n/4) + cn^2 \\\\ \u0026amp;\\leq 3(d(n/4)^2) + cn^2 \\\\ \u0026amp;= \\frac{3}{16}dn^2 + cn^2 \\\\ \u0026amp;\\leq dn^2 \\text{ if } d \\geq \\frac{16}{13}c. \\end{align*}\nExercises Solve the recurrence \\(T(n) = 2T(n/2) + cn\\) using the recursion tree method. References Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. Introduction to Algorithms. 4th ed. MIT Press. http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/. ","date":1710817800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710817800,"objectID":"e74e9da39c9abbc5641b815b8652011d","permalink":"https://ajdillhoff.github.io/notes/recursion_tree_method/","publishdate":"2024-03-18T22:10:00-05:00","relpermalink":"/notes/recursion_tree_method/","section":"notes","summary":"Table of Contents Example 4.13 from CLRS Visualizing the characteristics of an algorithm is a great way to build intuition about its runtime. Although it can be used to prove a recurrence, it is often a good jumping off point for the Substitution Method.\nExample 4.13 from CLRS Consider the recurrence \\(T(n) = 3T(n/4) + \\Theta(n^2)\\). We start by describing \\(\\Theta(n^2) = cn^2\\), where the constant \\(c \u0026gt; 0\\) serves as an upper-bound constant.","tags":["computer science","algorithms"],"title":"Recursion Tree Method","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Activity Selection Properties of Greedy Solutions Huffman Codes Greedy algorithms are a class of algorithms that yield locally optimal solutions. In cases where the local optimum is also the global optimum, greedy algorithms are ideal. Even in cases where the global solution is more elusive, a local solution may be sufficient.\nActivity Selection Given a set of activities that need to be scheduled using a common resource, the activity selection problem is to find the maximum number of activities that can be scheduled without overlapping.\nEach activity has a start time \\(s_i\\) and finish time \\(f_i\\), where \\(0 \\leq s_i \u0026lt; f_i \u0026lt; \\infty\\). An activity \\(a_i\\) takes place over the interval \\([s_i, f_i)\\). Two activities \\(a_i\\) and \\(a_j\\) are mutually compatible if \\(s_i \\geq f_j\\) or \\(s_j \\geq f_i\\).\nSort activities by their finish time. Objective: Find the largest subset of mutually compatible activities.\n\\(i\\) 1 2 3 4 5 6 7 8 9 \\(s_i\\) 1 2 4 1 5 8 9 11 13 \\(f_i\\) 3 5 7 8 9 10 11 14 16 Figure 1: Visualization of activities over time (Cormen et al. 2022). How many mutually compatible sets are there?\n\\(\\{a_1, a_3, a_6, a_8\\}\\) \\(\\{a_1, a_3, a_6, a_9\\}\\) \\(\\{a_1, a_3, a_7, a_9\\}\\) \\(\\{a_1, a_5, a_7, a_8\\}\\) \\(\\{a_1, a_5, a_7, a_9\\}\\) \\(\\{a_2, a_5, a_7, a_8\\}\\) \\(\\{a_2, a_5, a_7, a_8\\}\\) Optimal Substructure How do we verify that this problem has optimal substructure? First, it is important to formalize the problem based on the definition given previously. Define \\(S_{ij}\\) as the set of all activities that start after \\(a_i\\) finishes and finish before \\(a_j\\) starts.\n\\[ S_{ij} = \\{a_k \\in S : f_i \\leq s_k \u0026lt; f_k \\leq s_j\\} \\]\nThis defines a clear subset of the original set of data. That is, we have defined a subproblem of the original problem.\nWhich activities are those in \\(S_{ij}\\) compatible with?\nany \\(a_i\\) that finish by \\(f_i\\) any \\(a_i\\) that start no earlier than \\(s_j\\). Given this subset, our subproblem is that of finding a maximum set of mutually compatible activities in \\(S_{ij}\\), denoted \\(A_{ij}\\). If \\(a_k \\in A_{ij}\\), we are left with two subproblems:\nFind mutually compatible activities in \\(S_{ik}\\) \u0026ndash; starts after \\(a_i\\) finishes and finish before \\(a_k\\) starts. Find mutually compatible activities in \\(S_{kj}\\) \u0026ndash; starts after \\(a_k\\) finishes and finish before \\(a_j\\) start. The two subsets above are defined as \\(A_{ik} = A_{ij} \\cap S_{ik}\\) and \\(A_{kj} = A_{ij} \\cap S_{kj}\\), respectively. Then \\(A_{ij} = A_{ik} \\cup \\{a_k\\} \\cup A_{kj}\\). The size of the set is given by \\(|A_{ij}| = |A_{ik}| + 1 + |A_{kj}|\\).\nClaim: If our problem has optimal substructure, then the optimal solution \\(A_{ij}\\) must include optimal solutions for \\(S_{ik}\\) and \\(S_{kj}\\).\nThis claim can be proven using the cut-and-paste method used in Dynamic Programming. This technique works by showing that if a solution to a problem is not optimal, then there exists a way to cut the suboptimal portion and paste an optimal one. This will lead to a contradiction because the original was assumed to be optimal.\nProof: Suppose that \\(A_{kj}\\) is not optimal, and we could find a set \\(A\u0026rsquo;_{kj}\\) that is larger. Then we could replace \\(A_{kj}\\) with \\(A\u0026rsquo;_{kj}\\) in \\(A_{ij}\\) to obtain a larger set. This contradicts the assumption that \\(A_{ij}\\) is optimal.\nSimply put, if the claim is that the given solution is optimal, and the solution is constructed from optimal solutions to subproblems, then there cannot exist any other solution that is better. Another way to look at this: if we construct optimal solutions to subproblems, then the solution to the original problem must be optimal.\nRecursive Solution Let \\(c[i, j]\\) be the size of the optimal solution for \\(S_{ij}\\). Based on the above discussion, the size is computed as\n\\[ c[i, j] = c[i, k] + c[k, j] + 1. \\]\nThis dynamic programming solution assumes we know the optimal solution for all subproblems. To know this, we need to examine all possibilities which include \\(a_k\\) in the solution.\n\\[ c[i, j] = \\begin{cases} 0 \u0026amp; \\text{if } S_{ij} = \\emptyset, \\\\ \\max \\{c[i, k] + c[k, j] + 1 : a_k \\in S_{ij}\\} \u0026amp; \\text{if } S_{ij} \\neq \\emptyset. \\end{cases} \\]\nGreedy Solution The greedy solution is the naive one: select an activity that leaves the resource available for as many other activities as possible, which is the activity that finishes first. If multiple activities finish at the same time, select one arbitrarily.\nThe subproblem is that of finding a maximum size set of mutually compatible activities that start after \\(a_1\\) finishes. More generally, the optimal solution consists of all \\(a_i\\) that start after \\(a_k\\) finishes, where \\(a_k\\) is the last activity to finish:\n\\[ S_{k} = \\{a_i \\in S : s_i \\geq f_k\\}. \\]\nIs the greedy solution optimal? Suppose that \\(a_m \\in S_k\\) is the activity that finishes first. Then it must be included in the maximum size subset of mutually compatible activities \\(A_k\\). Suppose we are given \\(A_k\\) and we look at \\(a_j \\in A_k\\), the activity that finishes first. If \\(a_j = a_m\\), then the greedy solution is optimal. If \\(a_j \\neq a_m\\), then we can replace \\(a_j\\) with \\(a_m\\) since they are both compatible with all other activities in \\(A_k\\). Picking the activity that finishes first does not change the size of the optimal solution.\nThe solution is top-down:\npick the solution that finishes first, remove all activities that are incompatible with the chosen activity, repeat until no activities remain. Recursive Greedy Algorithm def recursive_activity_selector(s, f, k, n): m = k + 1 while m \u0026lt;= n and s[m] \u0026lt; f[k]: m += 1 if m \u0026lt;= n: return [m] + recursive_activity_selector(s, f, m, n) else: return [] This algorithm assumes that f is sorted in increasing order. The index k represents the index of the current subproblem. The number of activities is given by n. The while loop increments m until it finds an activity that starts after activity k finishes. If such an activity exists, it is added to the solution set and the algorithm is called recursively with the new subproblem.\nExample A run of this algorithm is visualized below.\nFigure 2: Recursive activity selector example (Cormen et al. 2022). Analysis At first glance, the algorithm appears to be \\(O(n^2)\\) because of the while loop coupled with the recursive call. Once an activity has been selected, the recursive call only considers the activities next \\(n - k\\) activities. The while loop picks up where the previous call left off, so the total number of iterations is \\(n\\). The algorithm is \\(O(n)\\).\nIterative Algorithm The above solution can be adapted to an iterative one.\ndef greedy_activity_selector(s, f): n = len(s) - 1 A = [1] k = 1 for m in range(2, n + 1): if s[m] \u0026gt;= f[k]: A.append(m) k = m return A In the for loop, the first line essentially asks if \\(a_m \\in S_k\\). If so, then add it to the solution set and update \\(k\\) to \\(m\\).\nAnalysis The analysis of the iterative approach is much clearer. The loop goes over all activities once, so the algorithm is \\(O(n)\\).\nProperties of Greedy Solutions You can probably imagine a problem for which a greedy solution would not provide the optimal solution. Path planning is one such problem. If we greedily chose the shortest path at each step, we may have missed a shorter path that is not the shortest at each step. The activity selection problem just so happens to be a perfect candidate for a greedy solution, but what makes it so?\nFirst, let\u0026rsquo;s review the major steps that led us to the greedy solution for activity selection.\nDetermine the optimal substructure. Develop a recursive solution. Show that making the greedy choice leaves only a single subproblem. Prove that making the greedy choice leads to an optimal solution. Develop a recursive algorithm. Convert it to an iterative algorithm. The first couple of steps are common to dynamic programming problems. In this case, we could have jumped straight to the greedy approach. Filtering out these extra steps leaves us with:\nCast the optimization problem as one in which we make a choice and are left with a single subproblem. Prove that the greedy choice is optimal. Demonstrate optimal substructure: if you make a greedy choice, then you are left with a subproblem such that combining an optimal solution with the greedy choice made previously, you end up with an optimal solution to the original problem. As we will see, we need two properties to prove that a greedy solution is optimal: the greedy choice property and the optimal substructure property.\nGreedy Choice Property The greedy choice property states that the optimal solution can be found by making locally greedy choices. This approach is opposite of dynamic programming, where the choices at each step are made from the knowledge of optimal solutions to subproblems. That is, dynamic programming is a bottom-up approach.\nA greedy solution also makes a choice at each step, but it is only based on local information. This is a top-down approach. They key of this property is to show that the greedy choice is optimal at each step. For the activity selection problem, the steps were\nExamine the optimal solution. If it has the greedy choice, then the greedy choice is optimal. If it does not have the greedy choice, then replace the suboptimal choice with the greedy choice. Optimal Substructure Property A problem has optimal substructure if the optimal solution contains optimal solutions to subproblems. We demonstrated this earlier for activity selection. We can start with the assumption that we arrived to a subproblem by making greedy choices. The next step is to show that the optimal solution to the subproblem combined with the greedy choice leads to an optimal solution to the original problem.\nGreedy vs. Dynamic Programming Since there is such overlap between greedy algorithms and dynamic programming in terms of their properties, it is important to understand the differences between the two. To illustrate these difference, we will look at two variations of the same problem.\n\\(0-1\\) Knapsack Problem Consider the \\(0-1\\) knapsack problem.\nYou have \\(n\\) items. Item \\(i\\) is worth \\(v_i\\) and weighs \\(w_i\\). Find the most valuable subset of items with total weight less than or equal to \\(W\\). Items cannot be divided. If the most valuable subset of items weighing at most \\(W\\) includes item \\(j\\), then the remaining weight must be the most valuable subset of items weighing at most \\(W - w_j\\) taken from \\(n-1\\) original items excluding item \\(j\\).\nFractional Knapsack Problem This is similar to the \\(0-1\\) knapsack problem, but items can be divided. The objective is to maximize the value of the items in the knapsack.\nThe optimal substructure of this problem varies slightly: if the most valuable subset weighing at most \\(W\\) includes the weight \\(w\\) of item \\(j\\), then the remaining weight must be the most valuable subset weighing at most \\(W- w\\) that can be taken from the \\(n-1\\) original items plus \\(w_j - w\\) of item \\(j\\). There is some fraction of item \\(j\\) left after taking \\(w\\) of it.\nShowing the Greedy Property It is established that both problems have optimal substructure. However, only the fractional knapsack problem has the greedy property. Examine the following code:\ndef fractional_knapsack(v, w, W): n = len(v) load = 0 i = 0 while load \u0026lt; W and i \u0026lt;= n: if w[i] \u0026lt;= W - load: load += w[i] i += 1 else: load += (W - load) * v[i] / w[i] If we are able to sort each item by its value-to-weight ratio, then the greedy choice is to take as much as possible from the most valuable item first, the second most valuable item next, and so on. This considers \\(n\\) items in the worst case, and the items need to be sorted by value-to-weight ratio. The algorithm is \\(O(n \\log n)\\).\nThis does not work for the \\(0-1\\) knapsack problem. Consider the problem visualized below.\nFigure 3: Greedy solution to the (0-1) knapsack problem (Cormen et al. 2022). In this problem, we have a knapsack whose total capacity is \\(W = 50\\). A table of the weights, values, and value-to-weight ratios is given below.\n\\(i\\) 1 2 3 \\(v_i\\) 60 100 120 \\(w_i\\) 10 20 30 \\(v_i/w_i\\) 6 5 4 The fractional algorithm would selection the first item since it has the greatest value-to-weight ratio. The \\(0-1\\) knapsack problem, however, would select the second and third items to maximize the value of the items in the knapsack.\nHuffman Codes Huffman coding is a lossless data compression algorithm that assigns variable-length codes to input characters, with lengths based on the frequencies of occurrence for those characters. Originally developed by David A. Huffman in 1952 during his Ph.D. at MIT, he published the algorithm under the title \u0026ldquo;A Method for the Construction of Minimum-Redundancy Codes\u0026rdquo;.\nHuffman coding involves:\nBuilding a Huffman tree from the input characters and their frequencies. Traversing the tree to assign codes to each character. Before getting into the specifics of the algorithm, let\u0026rsquo;s look at an example. Suppose we have a document consisting of 6 unique characters, each represented by a byte (8 bits). We could represent these 6 characters using 3 bits, since that is the minimum number of bits needed to represent 6 unique values. This is known as a fixed-length code. If we instead assigned a variable-length code to each character based on its frequency of occurrence, we would further reduce the footprint of the file size.\nA B C D E F Frequency (in thousands) 45 13 12 16 9 5 Fixed-length code 000 001 010 011 100 101 Variable-length code 0 101 100 111 1101 1100 Does this seem like the best coding? Why isn\u0026rsquo;t a code of 1 used? What about 2 bit encoding? The truth is that it depends on the document being compressed. The encoding is optimal consider the overall but length of the encoded file. Based on the above fixed-length code, the file size is 300,000 bits. The variable-length code reduces the file size to 224,000 bits.\nHow many bits are needed to encode \\(n \\geq 2\\) characters?\n\\[ \\lceil \\lg n \\rceil \\]\nPrefix-free Codes A prefix-free code is a code in which no codeword is also a prefix of another codeword. This property simplifies decoding since the code can be read from left to right without ambiguity. The codeword \u0026ldquo;beef\u0026rdquo; has the encoding \\(101110111011100 = 101 \\cdot 1101 \\cdot 1101 \\cdot 1100\\), where \\(\\cdot\\) denotes concatenation.\nThis definition may start to clarify why a code of 1 is not used. If a code of 1 were used, then the code would be a prefix of all other codes. This would make decoding ambiguous.\nHow are the codes decoded?\nOne solution to this is to keep a table of all codewords and their corresponding characters. A more compact solution is to use a binary tree. Starting with the first bit in the encoded message, traverse the tree until a leaf node is reached. The character at the leaf node is the decoded character. The tree is known as a Huffman tree.\nAn full binary tree, where each nonleaf node has two subnodes, is optimal for decoding. If the tree has this property then an optimal prefix-free code has \\(|C|\\) leaves and exactly \\(|C| - 1\\) internal nodes. The tree for the variable-length code is shown below.\nFigure 4: Huffman tree for the variable-length code (Cormen et al. 2022). Given a character \\(c\\) with frequency \\(c.freq\\), let \\(d_T( c)\\) denote the depth of character \\(c\\) in tree \\(T\\). The cost of the code in bits is given by\n\\[ B(T) = \\sum_{c \\in C} c.freq \\cdot d_T( c). \\]\nThe depth of the character \\(d_T( c)\\) is used since it also denotes the length of the codeword.\nConstructing a Huffman Code class Node: def __init__(self, freq, char=None): self.freq = freq self.char = char self.left = None self.right = None def huffman(C): n = len(C) Q = build_min_heap(C) for _ in range(n - 1): z = Node(0) z.left = x = extract_min(Q) z.right = y = extract_min(Q) z.freq = x.freq + y.freq insert(Q, z) return extract_min(Q) The function above builds a Huffman tree from a set of characters \\(C\\). At each iteration, the two nodes with the smallest frequencies are extracted from the queue \\(Q\\) and are used to create a new node \\(z\\). This node represents the sum of the frequencies of the two nodes. The node is then inserted back into the queue so that it can be used in future iterations. The result is a Huffman tree.\nFigure 5: Huffman tree for the data in the table above (Cormen et al. 2022). Analysis If the priority queue is implemented as a binary min-heap, the call to build_min_heap initializes the priority queue in \\(O(n)\\) time. The for loop runs \\(n-1\\) times, calling extract_min twice and insert once. Each call to extract_min takes \\(O(\\lg n)\\) time yielding a total of \\(O(n \\lg n)\\) time.\nCorrectness of Huffman Codes The correctness of an algorithm means that it produces the expected output based on the input and its properties. To show that the Huffman algorithm is correct, we can show that it exhibits the greedy choice and optimal substructure properties.\nLemma 15.2: Optimal prefix-free codes have the greedy-choice property For alphabet \\(C\\), let \\(x\\) and \\(y\\) be the two characters with the lowest frequencies. Then there exists an optimal prefix-free code for \\(C\\) where the codewords for \\(x\\) and \\(y\\) have the same length and differ only in the last bit.\nThis establishes the greedy choice property because the algorithm selects the two characters with the lowest frequencies at each step.\nProof\nIn the given optimal tree \\(T\\), leaves \\(a\\) and \\(b\\) are two siblings with maximum depth. It\u0026rsquo;s also given that \\(x\\) and \\(y\\) are the two characters with the lowest frequencies, but they appear in arbitrary positions.\nAssume that \\(x \\neq b\\). Swapping \\(a\\) and \\(x\\) produces tree \\(T\u0026rsquo;\\) does not increase the cost. Swapping \\(b\\) and \\(y\\) produces tree \\(T\u0026rsquo;\u0026rsquo;\\) that also does not increase the cost. This is the key argument: if swapping the lowest frequency characters with the deepest characters does not increase the cost, then the greedy choice is optimal.\nFigure 6: Creating (T\u0026rsquo;) and (T\u0026rsquo;\u0026rsquo;) from (T) (Cormen et al. 2022). Next, we need to show that exchanging \\(a\\) and \\(x\\) does not increase the cost. The cost of the tree is given by\n\\[ B(T) = \\sum_{c \\in C} c.freq \\cdot d_T( c). \\]\n\\begin{align*} B(T) - B(T\u0026rsquo;) \u0026amp;= \\sum_{c \\in C} c.freq \\cdot d_{T}( c) - \\sum_{c \\in C} c.freq \\cdot d_{T\u0026rsquo;}( c) \\\\ \u0026amp;= x.freq \\cdot d_T(x) + a.freq \\cdot d_T(a) - x.freq \\cdot d_{T\u0026rsquo;}(x) - a.freq \\cdot d_{T\u0026rsquo;}(a) \\\\ \u0026amp;= x.freq \\cdot d_T(x) + a.freq \\cdot d_T(a) - x.freq \\cdot d_{T}(a) - a.freq \\cdot d_{T}(x) \\\\ \u0026amp;= (x.freq - a.freq)(d_T(x) - d_T(a)) \\\\ \u0026amp;\\geq 0. \\end{align*}\nThe last line is true because \\(x.freq \\leq a.freq\\) and \\(d_T(x) \\geq d_T(a)\\). A similar argument can be made for \\(T\u0026rsquo;\u0026rsquo;\\). \\(B(T\u0026rsquo;\u0026rsquo;) \\leq B(T\u0026rsquo;)\\) since exchanging \\(y\\) and \\(b\\) does not increase the cost. This means that \\(B(T\u0026rsquo;\u0026rsquo;) \\leq B(T\u0026rsquo;) \\leq B(T)\\). \\(T\\) is an optimal tree, so \\(B(T) \\leq B(T\u0026rsquo;\u0026rsquo;) \\implies B(T) = B(T\u0026rsquo;\u0026rsquo;) \\implies T\\) is optimal where \\(x\\) and \\(y\\) are siblings of maximum depth.\nLemma 15.3: Optimal-substructure property Let \\(x\\) and \\(y\\) be two characters with minimum frequency in alphabet \\(C\\) and let \\(C\u0026rsquo; = (C - \\{x, y\\}) \\cup z\\) for a new character \\(z\\) with \\(z.freq = x.freq + y.freq\\). Additionally, let \\(T\u0026rsquo;\\) be a tree representing an optimal prefix-free code for \\(C\u0026rsquo;\\), and \\(T\\) be \\(T\u0026rsquo;\\) with the leaf for \\(z\\) replaced by an internal node with children \\(x\\) and \\(y\\). Then \\(T\\) represents an optimal prefix-free code for \\(C\\).\nSimplified: If the algorithm computed the optimal solution to the simplified problem, where \\(z\\) replaced \\(x\\) and \\(y\\), it can extend this to an optimal solution to the original problem by putting \\(x\\) and \\(y\\) back.\nProof\nThe first part of the proof establishes the costs of the relevant trees.\n\\(c \\in C - \\{x, y\\} \\implies d_T( c) = d_{T\u0026rsquo;}( c) \\implies c.freq \\cdot d_T( c) = c.freq \\cdot d_{T\u0026rsquo;}( c)\\)\nThe depth of \\(x\\) and \\(y\\) are equal to the depth of \\(z\\) in \\(T\u0026rsquo;\\) + 1:\n\\begin{align*} d_T(x) = d_T(y) = d_{T\u0026rsquo;}(z) + 1 \u0026amp;\\implies x.freq \\cdot d_T(x) + y.freq \\cdot d_T(y)\\\\ \u0026amp;= (x.freq + y.freq)(d_{T\u0026rsquo;}(z) + 1)\\\\ \u0026amp;= z.freq \\cdot d_{T\u0026rsquo;}(z) + (x.freq + y.freq). \\end{align*}\nThis means that \\(B(T) = B(T\u0026rsquo;) + x.freq + y.freq\\), which is equivalent to \\(B(T\u0026rsquo;) = B(T) - x.freq - y.freq\\).\nThe second part of this proof supposes that \\(T\\) is not an optimal prefix code for \\(C\\) and ends in a contradiction, thus proving the original lemma. If \\(T\\) is not optimal for \\(C\\), then \\(B(T\u0026rsquo;\u0026rsquo;) \u0026lt; B(T)\\) for some optimal tree \\(T\u0026rsquo;\u0026rsquo;\\).\nHere, \\(T\u0026rsquo;\u0026rsquo;\\) is introduced as the supposed optimal tree for \\(C\\) if it turns out that \\(T\\) is not.\nIf \\(T\u0026rsquo;\u0026rsquo;\\) is optimal, then lemma 15.2 (greedy property) from above implies that it has \\(x\\) and \\(y\\) as siblings. After all, \\(\\{x, y\\} \\in C\\) and \\(T\u0026rsquo;\u0026rsquo;\\) is an optimal tree for \\(C\\). Create a tree \\(T\u0026rsquo;\u0026rsquo;\u0026rsquo;\\) by replacing the parent of \\(x\\) and \\(y\\) with a leaf (implying we remove \\(x\\) and \\(y\\)) \\(z\\) with \\(z.freq = x.freq + y.freq\\). Then,\n\\begin{align*} B(T\u0026rsquo;\u0026rsquo;\u0026rsquo;) \u0026amp;= B(T\u0026rsquo;\u0026rsquo;) - x.freq - y.freq\\\\ \u0026amp;\u0026lt; B(T) - x.freq - y.freq\\\\ \u0026amp;= B(T\u0026rsquo;). \\end{align*}\n\\(B(T\u0026rsquo;\u0026rsquo;\u0026rsquo;) \u0026lt; B(T\u0026rsquo;)\\) is a contradiction because it was previously established that \\(T\u0026rsquo;\\) is an optimal tree for \\(C\u0026rsquo;\\). Therefore a suboptimal \\(T\\) is impossible if \\(T\u0026rsquo;\\) is optimal.\n","date":1710791100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710791100,"objectID":"b47daa5eebab97228caa1bb96cd55c4a","permalink":"https://ajdillhoff.github.io/notes/greedy_algorithms/","publishdate":"2024-03-18T14:45:00-05:00","relpermalink":"/notes/greedy_algorithms/","section":"notes","summary":"Table of Contents Activity Selection Properties of Greedy Solutions Huffman Codes Greedy algorithms are a class of algorithms that yield locally optimal solutions. In cases where the local optimum is also the global optimum, greedy algorithms are ideal. Even in cases where the global solution is more elusive, a local solution may be sufficient.\nActivity Selection Given a set of activities that need to be scheduled using a common resource, the activity selection problem is to find the maximum number of activities that can be scheduled without overlapping.","tags":["computer science","algorithms"],"title":"Greedy Algorithms","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":"See these slides.\n","date":1710447360,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710447360,"objectID":"14100eef908a51f98078482b1627d6aa","permalink":"https://ajdillhoff.github.io/notes/hash_tables/","publishdate":"2024-03-14T15:16:00-05:00","relpermalink":"/notes/hash_tables/","section":"notes","summary":"See these slides.","tags":["computer science","algorithms"],"title":"Hash Tables","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Rod Cutting Matrix-chain Multiplication Applying Dynamic Programming Longest Common Subsequence Exercises Dynamic programming is a technique for solving problems by breaking them down into simpler subproblems, very much like divide and conquer algorithms. One primary difference is that the subproblems are designed in such a way that they do not need to be recomputed.\nMany common problems have efficience dynamic programming solutions, and we will investigate several of them in these notes. In general, a dynamic programming solution can be applied if the problem has the following features.\nOptimal substructure: An optimal solution can be constructed by optimal solutions to the subproblems. Overlapping subproblems: The problem can be broken down into subproblems which can be reused. For example, the Fibonacci sequence has optimal substructure because the value of the sequence at any index is the sum of the values at the two previous indices. It also has overlapping subproblems because the value of the sequence at any index is used in the calculation of the values at the two subsequent indices. A recursive solution to the Fibonacci sequence will have exponential time complexity, but a dynamic programming solution will have linear time complexity.\nThe two main approaches to dynamic programming are top-down (memoization) and bottom-up (tabulation). Memoization involves writing a recursive solution that stores each sub=solution in a table so that it can be reused. Tabulation involves solving the problem by filling in a table of subproblems from the bottom up. In either case, a dynamic programming solution can be formulated with the following steps.\nIdentify subproblems so that the problem can be broken down. Solve the subproblems following an optimal solution. Store the solutions to avoid redundant computation. Combine solutions from the subproblems to solve the original problem. Rod Cutting Given a rod of length \\(n\\) and table of prices \\(p_i\\) for \\(i = 1, 2, \\ldots, n\\), determine the maximum revenue \\(r_n\\) that can be obtained by cutting up the rod and selling the pieces.\nLength 1 2 3 4 5 6 7 8 9 10 Price 1 5 8 9 10 17 17 20 24 30 The table of prices is shown above. For a rod of length 4, there are 8 (\\(2^{n-1}\\), where \\(n=4\\)) different ways to cut the rod.\nFigure 1: 8 different ways to cut a rod of length 4 (Cormen et al. 2022). The maximum revenue for a rod of length \\(n\\) can be determined by the following optimization problem:\n\\[ r_n = \\max(p_n, r_1 + r_{n-1}, r_2 + r_{n-2}, \\ldots, r_{n-1} + r_1), \\]\nwhere \\(r_i\\) is the maximum revenue for a rod of length \\(i\\). The maximum revenue for a rod of length \\(n\\) can be determined by solving the subproblems for rods of length \\(i\\) for \\(i = 1, 2, \\ldots, n-1\\). Each of the terms \\(r_i\\) in the equation above implies a recursive solution to the problem. You should be able to see that solving this recursively would lead to many redundant computations. For example, \\(r_1\\) is computed at least twice in the equation above.\nThis recursion is more compactly written as\n\\[ r_n = \\max_{1 \\leq i \\leq n}(p_i + r_{n-i}). \\]\nThis problem has optimal substructure. If we cut the rod into smaller subsections, we can recursively solve the subproblems and combine them. The recursive algorithm is as follows:\ndef cut_rod(p, n): if n == 0: return 0 q = -float(\u0026#39;inf\u0026#39;) for i in range(1, n+1): q = max(q, p[i] + cut_rod(p, n-i)) return q If \\(T(n)\\) is a recurrence that represents the number of times cur_rod is called recursively, then we can write the following recurrence relation:\n\\[ T(n) = 1 + \\sum_{j=0}^{n-1}T(j). \\]\nUsing the substitution method, we can show that \\(T(n) = 2^n\\).\n\\begin{align*} T(n) \u0026amp;= 1 + \\sum_{j=0}^{n-1}2^j \\\\ \u0026amp;= 1 + (2^n - 1) \\\\ \u0026amp;= 2^n. \\end{align*}\nMemoization Solution To solve this with dynamic programming, the goal is to make sure that each subproblem is computed only once. This is accomplished by saving the result of each subproblem in a table so that it can be reused. This does incur a space complexity of \\(O(n)\\), but it reduces the time complexity to \\(O(n^2)\\).\nThe solution requires a small modification to the recursive algorithm. When the solution to a subproblem is required, the table is first checked for a stored solution. If the solution is not found, the subproblem is solved recursively and the solution is stored in the table. The code is given below.\ndef memoized_cut_rod(p, n): r = [-float(\u0026#39;inf\u0026#39;) for _ in range(n+1)] return memoized_cut_rod_aux(p, n, r) def memoized_cut_rod_aux(p, n, r): if r[n] \u0026gt;= 0: return r[n] if n == 0: q = 0 else: q = -float(\u0026#39;inf\u0026#39;) for i in range(1, n+1): q = max(q, p[i] + memoized_cut_rod_aux(p, n-i, r)) r[n] = q return q The algorithm starts off with a call to memoized_cut_rod which initializes the table r and then calls memoized_cut_rod_aux. The table r is initialized with \\(-\\infty\\) so that we can check if a solution has been computed for a subproblem. Each subproblem is solved only once, leading to \\(O(1)\\) lookups after that. The time complexity of this solution is \\(O(n^2)\\).\nBottom-Up Solution The other dynamic programming solution is to first sort the subproblems by their size, solve the smaller ones first, and build up to the larger ones. This is called tabulation. The time complexity of this solution is also \\(O(n^2)\\).\ndef bottom_up_cut_rod(p, n): r = [0 for _ in range(n+1)] for j in range(1, n+1): q = -float(\u0026#39;inf\u0026#39;) for i in range(1, j+1): q = max(q, p[i] + r[j-i]) r[j] = q return r[n] The first for loop effectively sorts the problem by size. It starts with a cut of size 1 and builds up to a cut of size \\(n\\).\nSubproblem Graphs Subproblem graphs offer a concise way to visualize the subproblems and their dependencies. The subproblem graph for the rod cutting problem with \\(n=4\\) is shown below.\nFigure 2: Subproblem graph for the rod cutting problem with (n=4) (Cormen et al. 2022). Subproblem \\(n=4\\) is dependent on subproblems \\(n=3\\), \\(n=2\\), and \\(n=1\\). The bottom-up approach follows this dependency by ensuring that the subproblems are solved in the correct order.\nBesides serving as a helpful visualization, depicting the problem using a DAG can also help to identify the time complexity of the problem. This is the sum of the time needed to solve each subproblem. Each problem of size \\(n\\) requires \\(n-1\\) subproblems to be solved, and each subproblem of size \\(n-1\\) requires \\(n-2\\) subproblems to be solved. This leads to a time complexity of \\(O(n^2)\\).\nReconstructing a Solution The two dynamic programming solutions above return the maximum revenue that can be obtained by cutting up the rod. However, they do not return the actual cuts that should be made. This can be done by modifying the algorithms to store the cuts that are made. The code is given below.\ndef extended_bottom_up_cut_rod(p, n): r = [0 for _ in range(n+1)] s = [0 for _ in range(n+1)] for j in range(1, n+1): q = -float(\u0026#39;inf\u0026#39;) for i in range(1, j+1): if q \u0026lt; p[i] + r[j-i]: q = p[i] + r[j-i] s[j] = i r[j] = q return r, s def print_cut_rod_solution(p, n): r, s = extended_bottom_up_cut_rod(p, n) while n \u0026gt; 0: print(s[n]) n -= s[n] In the bottom-up approach, the table s is used to store the size of the first piece to cut off. The function print_cut_rod_solution uses this table to print the cuts that should be made.\nMatrix-chain Multiplication The next problem covered by Cormen et al. is matrix-chain multiplication (Cormen et al. 2022). Given a sequence of matrices \\(A_1, A_2, \\ldots, A_n\\), where the dimensions of matrix \\(A_i\\) are \\(p_{i-1} \\times p_i\\), determine the most efficient way to multiply the matrices. The problem is to determine the order in which the matrices should be multiplied so that the number of scalar multiplications is minimized.\nUnderstanding the solution to this problem requires understanding the problem itself. Depending on the order in which matrices are multiplied in a chain, the number of scalar multiplications can vary. Consider three matrices \\(A \\in \\mathbb{R}^{10 \\times 100}\\), \\(B \\in \\mathbb{R}^{100 \\times 5}\\), and \\(C \\in \\mathbb{R}^{5 \\times 50}\\). The number of scalar multiplications required to compute \\(A(BC)\\) is \\(10 \\times 100 \\times 5 + 10 \\times 5 \\times 50 = 7500\\), while the number of scalar multiplications required to compute \\((AB)C\\) is \\(10 \\times 100 \\times 50 + 100 \\times 5 \\times 50 = 75000\\). The order in which the matrices are multiplied can have a significant impact on the number of scalar multiplications required.\nMatrix multiplication is associative, so the order in which the matrices are grouped does not matter. The key to solving this problem is to find the most efficient way to group the matrices. The first part of the solution is to determine the number of possible groupings, or parenthesizations, we can make.\nDetermining Parenthesizations The number of possible parenthesizations of a chain of \\(n\\) matrices is given by \\(P(n)\\). When \\(n \\geq 2\\), the number of possible parenthesizations is given by\n\\[ P(n) = \\sum_{k=1}^{n-1}P(k)P(n-k). \\]\nA brute force solution to this problem would require \\(O(2^n)\\) time (see Exercise 14.2-3 in (Cormen et al. 2022)).\nDynamic Programming Solution We now review the four step process to formulating a dynamic programming solution as put forth by Cormen et al. (Cormen et al. 2022).\nOptimal Substructure What is the optimal substructure of this problem? Consider matrix-chain sequence \\(A_{i:j} = A_i A_{i+1} \\cdots A_j\\). If we split the sequence at \\(k\\), then the optimal solution to the problem is the optimal solution to the subproblems \\(A_{i:k}\\) and \\(A_{k+1:j}\\). This is because the number of scalar multiplications required to compute \\(A_{i:j}\\) is the sum of the number of scalar multiplications required to compute \\(A_{i:k}\\) and \\(A_{k+1:j}\\) plus the number of scalar multiplications required to compute the product of the two subproblems.\nHow can we ensure that there is not a more optimal grouping of \\(A_{h:l}\\), where \\(i \\leq h \u0026lt; k\\) and \\(k \u0026lt; l \\leq j\\)? The answer lies in evaluating all possible splits.\nRecursive Solution What is the cost of an optimal solution to the problem? We must first compute the minimum cost of parenthesizing \\(A_{i:j}\\) for \\(1 \\leq i \\leq j \\leq n\\). Let \\(m[i,j]\\) be the minimum number of scalar multiplications needed to compute \\(A_{i:j}\\). Starting with the base case, \\(m[i,i]\\) is the cost to compute the multiplication of a single matrix, which is 0. Assuming optimal subproblems are chosen, \\(m[i,j] = m[i,k] + m[k+1,j] + p_{i-1}p_kp_j\\), where the last term is the cost of multiplying \\(A_{i:k}A_{k+1:j}\\).\nAll possible splits must be evaluated. So, how many are there? Omitting the first and last matrices, there are \\(j - i\\) possible splits. We can now define the optimal solution in terms of the following recursion:\n\\[ m[i,j] = \\min \\{m[i,k] + m[k+1,j] + p_{i-1}p_kp_j : i \\leq k \u0026lt; j\\}. \\]\nStoring the Solutions The problem is taking shape and we have a recursive formula. However, this is no better than the brute-force method until we figure out how to select the optimal subproblems and store their solutions. That is, we need to optimally select \\(k\\). A bottom-up approach involves computing the cost of all possible combinations of the \\(n\\) matrices and building up from there. This requires \\(O(n^2)\\) memory to store both the costs \\(m[i, j]\\) as well as the value of \\(k\\) that splits them \\(s[i, j]\\).\ndef matrix_chain_order(p): n = len(p) - 1 m = [[0 for _ in range(n)] for _ in range(n)] s = [[0 for _ in range(n)] for _ in range(n)] for l in range(2, n+1): for i in range(1, n-l+2): j = i + l - 1 m[i][j] = float(\u0026#39;inf\u0026#39;) for k in range(i, j): q = m[i][k] + m[k+1][j] + p[i-1]*p[k]*p[j] if q \u0026lt; m[i][j]: m[i][j] = q s[i][j] = k return m, s The function matrix_chain_order computes the cost of all possible combinations of the \\(n\\) matrices and stores the value of \\(k\\) that splits them. The outer-most for loop controls the length of the chain being evaluated. We start at 2 since the cost of a length 1 chain is 0. Intuition tells us that the triply-nested for loop has a time complexity of \\(O(n^3)\\).\nThis algorithm computes the cost in ascending order of chain length. When \\(l=2\\), the cost of all chains of length 2 is computed. When \\(l=3\\), the cost of all chains of length 3 is computed, and so on. The recursion in the inner-most nested loop will only ever access the entries in m which have been previously computed.\nReconstructing a Solution We now have a solution which generates the optimal number of scalar multiplications needed for all possible combinations of the \\(n\\) matrices. However, we do not yet have a solution which tells us the order in which the matrices should be multiplied. This information is held in s, which records the value of \\(k\\) that splits the chain. The function print_optimal_parens is given below.\ndef print_optimal_parens(s, i, j): if i == j: print(f\u0026#34;A_{i}\u0026#34;, end=\u0026#34;\u0026#34;) else: print(\u0026#34;(\u0026#34;, end=\u0026#34;\u0026#34;) print_optimal_parens(s, i, s[i][j]) print_optimal_parens(s, s[i][j]+1, j) print(\u0026#34;)\u0026#34;, end=\u0026#34;\u0026#34;) Using figure 14.5 from (Cormen et al. 2022) as a reference, we can test the function print_optimal_parens. It is first called as print_optimal_parens(s, 1, 6). This recursively calls print_optimal_parens(s, 1, 3) and print_optimal_parens(s, 4, 6). We will work from left to right, top to bottom and fill out the values.\nSecond call: print_optimal_parens(s, 1, 3)\nThis recursively calls print_optimal_parens(s, 1, 1) and print_optimal_parens(s, 2, 3). We can see that this first call has \\(i==j\\), so it prints \\(A_1\\). The second call prints \\((A_2A_3)\\). This initial call already set up the first set of parenthesis, so the intermediate result is \\(((A_1(A_2A_3))\\cdots)\\).\nThird call: print_optimal_parens(s, 4, 6)\nThis recursively calls print_optimal_parens(s, 4, 5) and print_optimal_parens(s, 6, 6). This first call will recursively call print_optimal_parens(s, 4, 4) and print_optimal_parens(s, 5, 5). This produces \\((A_4A_5)\\) from the first subcall and \\(A_6\\) from the second subcall. The intermediate result is now \\((\\cdots((A_4A_5)A_6))\\).\nPutting it all together\nCombining these results yields \\(((A_1(A_2A_3))((A_4A_5)A_6))\\). This is the optimal parenthesization of the matrix chain \\(A_1A_2A_3A_4A_5A_6\\).\nApplying Dynamic Programming Section 14.3 of (Cormen et al. 2022) focuses on the two core components of a dynamic programming solution: optimal substructure and overlapping subproblems. By taking a closer look at how these two components were used in various dynamic programming solutions, you should have a greater understanding of how to apply dynamic programming to new problems.\nAs shown in previous examples, determining the optimal substructure is the first step in formulating a dynamic programming solution. In most cases, this comes from understanding the problem itself well. It is the result of a natural way of analysis and decomposition of the problem. When learning a new concerto, a musician must have a strong command of technique and fundamental musical concepts. Similarly, a strong understanding of the problem is required to determine the optimal substructure.\nDetermining Optimal Substructure Show that a solution to a problem requires making a choice, like where to cut in the rod cutting problem. Assume that you are given an optimal choice. Identify the subproblems that result from this choice. Show that solutions to these subproblems are optimal. In the last step, we are typically looking for a contradiction. The assumption of step 2 means that if we end up finding a more optimal solution to a subproblem, then the original choice was not optimal. The result is that we have a better overall solution.\nThe efficiency of a dynamic programming solution depends on the number of subproblems times the number of choices we have for each subproblem. When investigating solutions to a new problem, it is better to start with a simple case and expand outward as necessary. Using a subproblem graph is a great way to visualize the subproblems and their dependencies.\nCounter-Example: The Longest Simple Path Consider the following problems which first appear to have optimal substructure.\nShortest path: find a path \\(u \\leadsto v\\) with the fewest edges without cycles. Longest simple path: find a path \\(u \\leadsto v\\) with the most edges without cycles. The first problem has optimal substructure. Suppose that the shortest path \\(u \\leadsto v\\) is given by \\(p\\). Given some intermediate vertex \\(w\\), the optimal path from \\(u \\leadsto w\\) is given by \\(p_1\\) and the optimal path from \\(w \\leadsto v\\) is given by \\(p_2\\). If there were a shorter path \\(p\u0026rsquo;_1\\) from \\(u \\leadsto w\\) then we could replace \\(p_1\\) with it and get a total path with fewer edges.\nCheck your understanding\nWhy does that argument reinforce the idea of optimal substructure? By showing that the optimal solution to a subproblem is the optimal solution to the original problem. This argument becomes clearer as we consider the longest simple path problem.\nFigure 3: Subproblem graph for the longest simple path problem (Cormen et al. 2022). Consider the directed graph above. The path \\(q \\rightarrow r \\rightarrow t\\) is the longest simple path from \\(q\\) to \\(t\\). Keep in mind that the problem is to find a simple path with the most edges. If the substructure is optimal, then the subpaths must also exhibit maximal edges. The subpath \\(q \\leadsto r\\) in this case is simply \\(q \\rightarrow r\\), but the longest simple path from \\(q\\) to \\(r\\) is \\(q \\rightarrow s \\rightarrow t \\rightarrow r\\). Therefore, the subpath \\(q \\leadsto r\\) is not optimal. This is a counter-example to the idea that the longest simple path problem has optimal substructure.\nThe longest simple path problem does not have independent subproblems. Consider a path from \\(q\\) to \\(t\\). This could be broken down into subproblem \\(q \\leadsto r\\) and \\(r \\leadsto t\\). For \\(q \\leadsto r\\), we have \\(q \\rightarrow s \\rightarrow t \\rightarrow r\\). This subproblem is dependent on \\(s\\) and \\(t\\), so we cannot use them in the second subproblem \\(r \\leadsto t\\) without forming a path that is not simple. Specifically, the first subproblem includes \\(t\\), so the second subproblem cannot include \\(t\\). However, the second subproblem MUST include \\(t\\).\nQuestions What are the independent subproblems in the rod cutting and matrix-chain multiplication problems? Using Overlapping Subproblems First, do not confuse the idea of overlapping subproblems with the need for the subproblems to be independent. Subproblems are independent if they do not share resources, which the longest simple path problem does not have. Overlapping subproblems means that a subproblem may require the result of another independent subproblem. This is the case in the rod cutting problem, where the value of a subproblem is used in the calculation of the value of the next subproblem.\nA desirable trait of any recursive problem is that it have a small number of unique subproblems. The running time of such a solution is dependent on the number of subproblems, so having more of them will naturally lead to a less efficient solution. Section 14.3 reviews the bottom-up solution to matrix-chain multiplication, specifically focusing on the number of times the solution of each subproblem is required. It is recommended to review this section for further understanding.\nQuestions How many subproblem solutions are reused in the rod cutting problem of \\(n=4\\)? How many subproblem solutions are reused when computing the Fibonacci sequence of \\(n\\)? Longest Common Subsequence A longest common subsequence (LCS) of two input sequences \\(X = \\langle x_1, x_2, \\ldots, x_m \\rangle\\) and \\(Y = \\langle y_1, y_2, \\ldots, y_n \\rangle\\) is a sequence \\(Z = \\langle z_1, z_2, \\ldots, z_k \\rangle\\) such that \\(Z\\) is a subsequence of both \\(X\\) and \\(Y\\) and \\(k\\) is as large as possible. For example, given \\(X = \\langle A, B, C, B, D, A, B \\rangle\\) and \\(Y = \\langle B, D, C, A, B, A \\rangle\\), the LCS is \\(\\langle B, C, A, B \\rangle\\).\nThe subsequence is not necessarily consecutive! A subsequence \\(Z\\) is common to a sequence \\(X\\) if it corresponds to a strictly increasing sequence of indices such that \\(x_{i_j} = z_j\\).\nNaive Solution First, how would we solve this problem using a brute-force method? We could generate all possible subsequences of \\(X\\) and \\(Y\\) and then compare them. This would require \\(O(n2^m)\\) time.\nDynamic Programming Solution Following the four step process, we can formulate a dynamic programming solution to the LCS problem. Step 1 is to determine the optimal substructure of the problem.\nOptimal Substructure Let \\(X = \\langle x_1, x_2, \\ldots, x_m \\rangle\\) and \\(Y = \\langle y_1, y_2, \\ldots, y_n \\rangle\\). Let \\(Z = \\langle z_1, z_2, \\ldots, z_k \\rangle\\) be an LCS of \\(X\\) and \\(Y\\).\nIf \\(x_m = y_n\\), then \\(z_k = x_m = y_n\\) and \\(Z_{k-1}\\) is an LCS of \\(X_{m-1}\\) and \\(Y_{n-1}\\). If \\(x_m \\neq y_n\\) and \\(z_k \\neq x_m\\), then \\(Z\\) is an LCS of \\(X_{m-1}\\) and \\(Y\\). If \\(x_m \\neq y_n\\) and \\(z_k \\neq y_n\\), then \\(Z\\) is an LCS of \\(X\\) and \\(Y_{n-1}\\). The theorem above shows that the LCS problem has optimal substructure. Let\u0026rsquo;s break this down a bit. Consider two sequences (words): rocinante and canterbury. The longest common subsequence is \u0026ldquo;cante\u0026rdquo;. Since the last characters of the two original words do not match, we can remove the last character from either word and find the LCS of the two remaining words. This implies that we could have found the LCS of the two original words by finding the LCS of a smaller subproblem.\nWhat if the two words had the same last character? The LCS of the shorter strings is the same as the LCS of the original strings with the last character removed.\nRecursive Solution The next step is to write a recursive solution to the problem. Given the substrucure just presented, a bottom-up approach seems intuitive. Starting with indices \\(i=0\\) and \\(j=0\\) which indicate the length of the current strings \\(X_i\\) and \\(Y_j\\), increase the length and compute the LCS as we go.\nDefine \\(c[i, j]\\) as the LCS length of \\(X_i\\) and \\(Y_j\\). The goal is to compute \\(c[m,n]\\), where \\(m\\) and \\(n\\) are the lengths of \\(X\\) and \\(Y\\), respectively. The recursive formula is given by\n\\[ c[i, j] = \\begin{cases} 0 \u0026amp; \\text{if } i = 0 \\text{ or } j = 0, \\\\ c[i-1, j-1] + 1 \u0026amp; \\text{if } i, j \u0026gt; 0 \\text{ and } x_i = y_j, \\\\ \\max(c[i-1, j], c[i, j-1]) \u0026amp; \\text{if } i, j \u0026gt; 0 \\text{ and } x_i \\neq y_j. \\end{cases} \\]\nExample: \u0026ldquo;atom\u0026rdquo; and \u0026ldquo;ant\u0026rdquo;\nThe LCS of \u0026ldquo;atom\u0026rdquo; and \u0026ldquo;ant\u0026rdquo; is \u0026ldquo;at\u0026rdquo;. The tree below shows the recursive calls to each subproblem. A dashed line indicates that the subproblem has already been solved.\nFigure 4: Recursion tree for the LCS problem (Cormen et al. 2022). Storing the Solutions The LCS problem has \\(\\Theta(mn)\\) distinct subproblems, so storing the solutions to these subproblems will allow us to avoid redundant computation. A dynamic programming solution goes as follows:\nStore the lengths of the LCS of the prefixes of \\(X\\) and \\(Y\\) in a table \\(c\\). Additionally store the solution to the subproblems in a table \\(b\\) so that we can reconstruct the LCS. The entries are filled in a row-major order. The code is given below.\ndef lcs_length(X, Y): m = len(X) n = len(Y) b = [[0 for _ in range(n+1)] for _ in range(m+1)] c = [[0 for _ in range(n+1)] for _ in range(m+1)] for i in range(1, m+1): for j in range(1, n+1): if X[i-1] == Y[j-1]: c[i][j] = c[i-1][j-1] + 1 b[i][j] = \u0026#34;↖\u0026#34; elif c[i-1][j] \u0026gt;= c[i][j-1]: c[i][j] = c[i-1][j] b[i][j] = \u0026#34;↑\u0026#34; else: c[i][j] = c[i][j-1] b[i][j] = \u0026#34;←\u0026#34; return c, b def print_lcs(b, X, i, j): if i == 0 or j == 0: return if b[i][j] == \u0026#34;↖\u0026#34;: print_lcs(b, X, i-1, j-1) print(X[i-1], end=\u0026#34;\u0026#34;) elif b[i][j] == \u0026#34;↑\u0026#34;: print_lcs(b, X, i-1, j) else: print_lcs(b, X, i, j-1) Reconstructing a Solution Printing the solution starts with the last entry in the table \\(b\\). If the entry is \u0026ldquo;↖\u0026rdquo;, then the last characters of \\(X\\) and \\(Y\\) are the same and we print the character. If the entry is \u0026ldquo;↑\u0026rdquo;, then we move up in the table. If the entry is \u0026ldquo;←\u0026rdquo;, then we move left in the table.\nExercises Exercise 14.1-5 from (Cormen et al. 2022). Exercise 14.2-1 from (Cormen et al. 2022). Write a recursive function to compute the Fibonacci sequence. What is the time complexity of this function? What is the time complexity of the dynamic programming solution? Write a function that prints a table similar to Figure 14.8 from (Cormen et al. 2022) for the LCS problem. References Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. Introduction to Algorithms. 4th ed. MIT Press. http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/. ","date":1710430800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710430800,"objectID":"d9fcbd0d7db2e450465c1b3cce0ba42e","permalink":"https://ajdillhoff.github.io/notes/dynamic_programming/","publishdate":"2024-03-14T10:40:00-05:00","relpermalink":"/notes/dynamic_programming/","section":"notes","summary":"Table of Contents Rod Cutting Matrix-chain Multiplication Applying Dynamic Programming Longest Common Subsequence Exercises Dynamic programming is a technique for solving problems by breaking them down into simpler subproblems, very much like divide and conquer algorithms. One primary difference is that the subproblems are designed in such a way that they do not need to be recomputed.\nMany common problems have efficience dynamic programming solutions, and we will investigate several of them in these notes.","tags":["computer science","algorithms"],"title":"Dynamic Programming","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Order Statistics Minimum and Maximum Selection in expected linear time Problems and Exercises We briefly touched on a median finding algorithm when discussing Divide and Conquer Algorithms. This section will be a bit of a review, but the point is to touch on the topic of order statistics more generally.\nOrder Statistics The \\(i^{\\text{th}}\\) order statistic is the \\(i^{\\text{th}}\\) smallest element in a set of \\(n\\) elements. The median is the \\(\\frac{n}{2}^{\\text{th}}\\) order statistic. The minimum and maximum are the \\(1^{\\text{st}}\\) and \\(n^{\\text{th}}\\) order statistics, respectively. When \\(n\\) is even, there are two medians:\nthe lower median \\(\\frac{n}{2}^{\\text{th}}\\) and the upper median \\(\\frac{n}{2} + 1^{\\text{th}}\\). The goal of the algorithm of focus in these notes is to determine how select order statistic \\(i\\) in a set of \\(n\\) elements. As we saw previously, and will review in these notes, we can use a divide and conquer approach to solve this problem. Further, we will study a linear approach to this problem under the assumption that the elements are distinct.\nMinimum and Maximum One could reason very simply that the lower bound on the number of comparisons needed to find either the minimum or maximum of a set is \\(n-1\\). One such argument could be that if we left even 1 comparison out of the \\(n-1\\) comparisons, we could not guarantee that we had found the minimum or maximum. When implementing an algorithm, we would say that an optimal implementation would require \\(n-1\\) comparisons.\nAs a quick aside, there are plenty of algorithms that we implement which are not optimal in terms of their theoretical lower bound. Consider a naive matrix multiplication algorithm. There are many redundant reads from memory in this algorithm. For example, if we compute \\(C = AB\\), we need to calculate the output values \\(C_{1, 1}\\) and \\(C_{1, 2}\\), among others. Both of these outputs require reading from the first row of \\(A\\).\nWe could find both the minimum and maximum of a set in \\(2n - 2\\) operations by passing over the set twice. This is theoretically optimal since each pass is performing the optimal \\(n-1\\) comparisons. If we first compared a pair of elements with each other before comparing them to the minimum and maximum, respectively, we could find both the minimum and maximum in \\(3\\left\\lfloor\\frac{n}{2}\\right\\rfloor\\) comparisons.\nSelection in expected linear time We now turn to the problem of selection. Given a set of \\(n\\) elements and an integer \\(i\\), we want to find the \\(i^{\\text{th}}\\) order statistic. We will assume that all elements are distinct. We will also assume that \\(i\\) is between 1 and \\(n\\).\nThe randomized select algorithm returns the \\(i^{\\text{th}}\\) smallest element of an array bounded between indices \\(p\\) and \\(r\\). It relies on randomized_partition, just like Quicksort.\ndef randomized_select(A, p, r, i): if p == r: return A[p] q = randomized_partition(A, p, r) k = q - p + 1 if i == k: return A[q] elif i \u0026lt; k: return randomized_select(A, p, q-1, i) else: return randomized_select(A, q+1, r, i-k) The first conditional checks if the array had only a single element, in which case it must be the value we are looking for. If not, the array is partitioned so that each element in \\(A[p:q-1]\\) is less than or equal to \\(A[q]\\) which is less than or equal to the elements in \\(A[q+1:r]\\). The line \\(k = q - p + 1\\) calculates the number of elements less than or equal to the pivot. If the index we are looking for is equal to this number, then we have found it and can return the value immediately.\nIf the value was not yet found and \\(i \u0026lt; k\\), then \\(i\\) must be in the subarray \\(A[p:q-1]\\). Therefore, the function is recursively called on that subarray. Otherwise, the subarray \\(A[q+1:r]\\) is checked. An example from Cormen et al. is shown below (Cormen et al. 2022).\nFigure 1: Randomized select from (Cormen et al. 2022). Explanation of figure \\(A^{(0)}\\) shows that \\(A[5] = 14\\) was chosen as the pivot. The next row, \\(A^{(1)}\\), depicts the completed partitioning. Cormen et al. note that this is not a helpful partitioning since less than \\(\\frac{1}{4}\\) of the elements are ignored. A helpful partition is one that leaves at most \\(\\frac{3}{4}\\) of the elements after the partitioning.\nAnalysis The worst-case running time of randomized_select is \\(O(n^2)\\) since we are partitioning \\(n\\) elements at \\(\\Theta(n)\\) each. Since the pivot of randomized_partition is selected at random, we can expect a good split at least every 2 times it is called. The proof for this is similar to the one made when analyzing Quicksort. Briefly, the expected number of times we must partition before we get a helpful split is 2, which only doubles the running time. The recurrence is still \\(T(n) = T(3n/4) + \\Theta(n) = \\Theta(n)\\).\nThe first step to showing that the expected runtime of randomized_select is \\(\\Theta(n)\\) is to show that a partitioning is helpful with probability at least \\(\\frac{1}{2}\\) (Cormen et al. 2022). The rest of the proof requires further examination and comprehension.\nThe proof presented by Cormen et al. begins with the following terms:\n\\(h_i\\) is the event that the \\(i^{\\text{th}}\\) partitioning is helpful. \\(\\{h_0, h_1, \\dots, h_m\\}\\) is the sequence of helpful partitionings. \\(n_k = |A^{(h_k)}|\\) is the number of elements in the subarray \\(A^{(h_k)}\\) at the \\(k^{\\text{th}}\\) partitioning. \\(n_k \\leq (3/4)n_{k-1}\\) for \\(k \\geq 1\\), or \\(n_k \\leq (3/4)^kn_0\\). \\(X_k = h_{k+1} - h_k\\) is the number of unhelpful partitionings between the \\(k^{\\text{th}}\\) and \\((k+1)^{\\text{th}}\\) helpful partitionings. There are certainly partitionings that are not helpful. These are depicted as subarrays within each generation of helpful partitionings. The figure below exemplifies this.\nFigure 2: The sets within each generation of helpful partitionings are not helpful. From (Cormen et al. 2022). Given that the probability that a partitioning is helpful is at least \\(\\frac{1}{2}\\), we know that \\(E[X_k] \\leq 2\\). With this, an upper bound on the number of comparisons of partitioning is derived. The total number of comparisons made when partitioning is less than\n\\begin{align*} \\sum_{k=0}^{m-1} \\sum_{j=h_k}^{h_k + X_k - 1} |A^{(j)}| \u0026amp;\\leq \\sum_{k=0}^{m-1} \\sum_{j=h_k}^{h_k + X_k - 1} |A^{(h_k)}| \\\\ \u0026amp;= \\sum_{k=0}^{m-1} X_k|A^{(h_k)}| \\\\ \u0026amp;\\leq \\sum_{k=0}^{m-1} \\left(\\frac{3}{4}\\right)^k n_0. \\\\ \\end{align*}\nThe first term on the first line represents the total number of comparisons across all sets. The first sum loops through the \\(m\\) helpful partitionings, and the inner loop sums the number of comparisons made for each unhelpful partitioning. It is bounded by the term on the right. This is because \\(|A^{(j)}| \\leq |A^{(h_k)}|\\) if \\(A^{(j)}\\) is in the \\(k^{\\text{th}}\\) generation of helpful partitionings (see term 4 above).\nUsing term 5 from above, the second line is derived. The third line leverages term 4 again. The sum is a geometric series, and the total number of comparisons is less than\n\\begin{align*} \\text{E} \\left[\\sum_{k=0}^{m-1} X_k \\left(\\frac{3}{4}\\right)^k n_0\\right] \u0026amp;= n_0 \\sum_{k=0}^{m-1} \\left(\\frac{3}{4}\\right)^k \\text{E}[X_k]\\\\ \u0026amp;\\leq 2n_0 \\sum_{k=0}^{m-1} \\left(\\frac{3}{4}\\right)^k \\\\ \u0026amp;\u0026lt; 2n_0 \\sum_{k=0}^{\\infty} \\left(\\frac{3}{4}\\right)^k \\\\ \u0026amp;= 8n_0. \\end{align*}\nThe last line is the result of a geometric series. This concludes the proof that randomized_partition runs in expected linear time.\nProblems and Exercises Show that the second largest of \\(n\\) elements can be found with \\(n + \\lceil\\log_2 n\\rceil - 2\\) comparisons in the worst case. ","date":1710267420,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710267420,"objectID":"53674bb5891d1b3f3d5708da4eef6654","permalink":"https://ajdillhoff.github.io/notes/medians_and_order_statistics/","publishdate":"2024-03-12T13:17:00-05:00","relpermalink":"/notes/medians_and_order_statistics/","section":"notes","summary":"Table of Contents Order Statistics Minimum and Maximum Selection in expected linear time Problems and Exercises We briefly touched on a median finding algorithm when discussing Divide and Conquer Algorithms. This section will be a bit of a review, but the point is to touch on the topic of order statistics more generally.\nOrder Statistics The \\(i^{\\text{th}}\\) order statistic is the \\(i^{\\text{th}}\\) smallest element in a set of \\(n\\) elements.","tags":["computer science","algorithms"],"title":"Medians and Order Statistics","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Establishing a Lower Bound on Comparison Sorts Counting Sort Radix Sort Bucket Sort Questions and Exercises These are my personal notes for Chapter 8 of Introduction to Algorithms (Cormen et al. 2022). Readers should reference the book for more details when necessary.\nIntroduction All sorting algorithms discussed up to this point are comparison based. You may have thought, as I did, that sorting cannot be done without a comparison. If you have no way to evaluate the relative ordering of two different objects, how can you possibly arrange them in any order?\nThe answer will become clear shortly and is investigated through counting sort, radix sort, and bucket sort. First, Cormen et al. make it clear that sorting algorithms cannot reach linear time. As we will see, any comparison sort must make \\(\\Omega (n \\lg n)\\) comparisons in the worst case to sort \\(n\\) elements. This bound is motivation enough to explore a different class of sorting algorithms.\nEstablishing a Lower Bound on Comparison Sorts The basis of the proof presented in Chapter 8 it to consider that all comparison sorts can be viewed as a decision tree, where each leaf represents a unique permutation of the inputs. If there are \\(n\\) distinct elements in the original array, then there must be \\(n!\\) leaves in the decision tree. Consider the figure from (Cormen et al. 2022) below.\nFigure 1: Decision tree for comparison sort on three elements. In the figure above, each node compares two values as \\(a:b\\). If \\(a \\leq b\\), the left path is taken. The worst case of a comparison sort can be determined by the height of the tree. A proof on the lower bound goes as follows.\nConsider a binary tree of height \\(h\\) with \\(l\\) reachable leaves. Each of the \\(n!\\) permutations occurs as one of the leaves, so \\(n! \\leq l\\) since there may be duplicate permutations in the leaves. A binary tree with height \\(h\\) has no more than \\(2^h\\) leaves, so \\(n! \\leq l \\leq 2^h\\). Taking the logarithm of this inequality implies that \\(h \\geq \\lg n!\\). Since \\(\\lg n! = \\Theta(n \\lg n)\\), and is a lower bound on the height of the tree, then any comparison sort must make \\(\\Omega (n \\lg n)\\) comparisons.\nCounting Sort Counting sort can sort an array of integers in \\(O(n+k)\\) time, where \\(k \\geq 0\\) is the largest integer in the set. It works by counting the number of elements less than or equal to each element \\(x\\).\ndef counting_sort(A, k): n = len(A) B = [0 for i in range(n)] C = [0 for i in range(k+1)] for i in range(n): C[A[i]] += 1 # C[i] contains the number of elements equal to i for i in range(1, k): C[i] = C[i] + C[i-1] # C[i] contains the number of elements less than or equal to i for i in range(n - 1, -1, -1): B[C[A[i]]-1] = A[i] C[A[i]] = C[A[i]] - 1 return B The first two loops establish the number of elements less than or equal to \\(i\\) for each element \\(i\\). The main sticking point in understanding this algorithm is the last loop. It starts at the very end of loop, placing the last element from \\(A\\) into the output array \\(B\\) in its correct position as determined by \\(C\\).\nConsider a simple example with \\(\\{2, 5, 5, 3, 4\\}\\). After the second loop, \\(C = \\{0, 0, 1, 2, 3, 5\\}\\). On the first iteration of the last loop, \\(A[4] = 4\\) is used as the index into \\(C\\), which yields \\(3\\) since the value \\(4\\) is greater than or equal to \\(3\\) elements in the original array. It is then placed in the correct spot \\(B[3-1] = 4\\).\nAnother example is shown in figure 8.2 from (Cormen et al. 2022).\nRadix Sort Dating back to 1887 by Herman Hollerith\u0026rsquo;s work on tabulating machines, this algorithm places numbers in one of \\(k\\) bins based on their radix, or the number of unique digits. It was used for sorting punch cards via multi-column sorting. It works by iteratively sorting a series of inputs based on a column starting with the least-significant digit. An example is shown below.\nFigure 2: Radix sort in action (Cormen et al. 2022). In the figure above, the items are first sorted based on the least significant digits. By the end of the process, the data is numerically sorted in ascending order. The algorithm can be written simply:\ndef radix_sort(A, d): for i in range(d): A = counting_sort(A, len(A), 9) return A Counting sort is the typical sorting algorithm that is used to sort the digits in each column. In fact, any stable sorting algorithm can be used in place of counting sort.\nAnalysis We know that counting sort is \\(\\Theta(n + k)\\), and that radix sort calls it \\(d\\) times. Therefore, the time complexity of radix sort is \\(\\Theta(d(n + k))\\). If \\(k = O(n)\\), then the time complexity is \\(\\Theta(dn)\\).\nComplex Keys What if the data is not just a single integer, but a complex key or series of keys? The keys themselves can be broken up into digits. Consider a 32-bit word. If we want to sort \\(n\\) of these words, and we have \\(b = 32\\) bits per word, we can break the words into \\(r=8\\) bit digits. This yields \\(d = \\lceil b / r \\rceil = 4\\) digits. The largest value for each digit is then \\(k = 2^r - 1 = 255\\). Plugging these values into the analysis from above yields $Θ((b/r)(n + 2^r)).$\nWhat is the best choice of \\(r\\)? Consider what happens for different values of \\(r\\). As \\(r\\) increases, \\(2^r\\) increases. As it decreases, \\(\\frac{b}{r}\\) increases. The best choice depends on whether \\(b \u0026lt; \\lfloor \\lg n \\rfloor\\). If \\(b \u0026lt; \\lfloor \\lg n \\rfloor\\), then \\(r \\leq b\\) implies \\((n + 2^r) = \\Theta(n)\\) since \\(2^{\\lg n} = n\\). If \\(b \\geq \\lfloor \\lg n \\rfloor\\), then we should choose \\(r \\approx \\lg n\\). This would yield \\(\\Theta((b / \\lg n)(n + n)) = \\Theta(bn / \\lg n)\\).\nYou should spend some time to think about the choices for \\(r\\). Specifically, what will happen to the time complexity as \\(r\\) increases above \\(\\lg n\\)? What about as \\(r\\) decreases below \\(\\lg n\\)?\nExample: Sorting words Use radix sort to sort the following list of names: \u0026ldquo;Beethoven\u0026rdquo;, \u0026ldquo;Bach\u0026rdquo;, \u0026ldquo;Mozart\u0026rdquo;, \u0026ldquo;Chopin\u0026rdquo;, \u0026ldquo;Liszt\u0026rdquo;, \u0026ldquo;Schubert\u0026rdquo;, \u0026ldquo;Haydn\u0026rdquo;, \u0026ldquo;Brahms\u0026rdquo;, \u0026ldquo;Wagner\u0026rdquo;, \u0026ldquo;Tchaikovsky\u0026rdquo;. First, we need to figure out how to encode the names as integers. If we convert the input to lowercase, we only have to deal with \\(k=26\\) unique characters. This only requires 5 bits. Since each name has varying length, we can use a sentinel value of 0 to pad the shorter names. That is, 0 represents a padding character and the alphabet starts at 1. The names are then encoded as follows:\nOriginal Name Encoded Name Beethoven [2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0] Bach [2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0] Mozart [13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0] Chopin [3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0] Liszt [12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0] Schubert [19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0] Haydn [8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0] Brahms [2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0] Wagner [23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0] Tchaikovsky [20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25] No changes will be made for the first 2 iterations of the sort. The third iteration will yield the following:\nOriginal Name Encoded Name Bach [2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0] Mozart [13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0] Chopin [3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0] Liszt [12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0] Schubert [19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0] Haydn [8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0] Brahms [2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0] Wagner [23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0] Beethoven [2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0] Tchaikovsky [20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25] Iteration 3\nOriginal Name Encoded Name Bach [2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0] Mozart [13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0] Chopin [3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0] Liszt [12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0] Haydn [8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0] Brahms [2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0] Wagner [23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0] Schubert [19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0] Beethoven [2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0] Tchaikovsky [20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25] Iteration 4\nOriginal Name Encoded Name Bach [2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0] Liszt [12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0] Haydn [8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0] Schubert [19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0] Tchaikovsky [20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25] Chopin [3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0] Beethoven [2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0] Wagner [23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0] Brahms [2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0] Mozart [13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0] Iteration 5\nOriginal Name Encoded Name Bach [2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0] Schubert [19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0] Wagner [23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0] Beethoven [2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0] Tchaikovsky [20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25] Chopin [3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0] Brahms [2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0] Haydn [8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0] Mozart [13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0] Liszt [12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0] Iteration 6\nOriginal Name Encoded Name Tchaikovsky [20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25] Mozart [13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0] Haydn [8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0] Bach [2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0] Brahms [2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0] Wagner [23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0] Chopin [3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0] Beethoven [2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0] Schubert [19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0] Liszt [12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0] Iteration 7\nOriginal Name Encoded Name Brahms [2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0] Bach [2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0] Beethoven [2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0] Wagner [23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0] Tchaikovsky [20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25] Schubert [19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0] Chopin [3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0] Liszt [12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0] Haydn [8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0] Mozart [13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0] Iteration 8\nOriginal Name Encoded Name Bach [2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0] Haydn [8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0] Wagner [23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0] Tchaikovsky [20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25] Schubert [19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0] Beethoven [2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0] Chopin [3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0] Liszt [12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0] Mozart [13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0] Brahms [2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0] Iteration 9\nOriginal Name Encoded Name Bach [2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0] Beethoven [2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0] Brahms [2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0] Chopin [3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0] Haydn [8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0] Liszt [12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0] Mozart [13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0] Schubert [19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0] Tchaikovsky [20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25] Wagner [23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0] Bucket Sort The final sorting algorithm in Chapter 8 is bucket sort (Cormen et al. 2022). As the name suggests, bucket sort distributes the input into a number of distinct buckets based on the input value. The key here is the assumption that the data is uniformly distributed. If the data were not uniformly distributed, then more elements would be concentrated. The uniformity ensures that a relatively equal number of data points are placed in each bucket. This is also a convenient assumption to have for a parallelized implementation.\nThe algorithm works by placing values into a bucket based on their most significant digits. Once the values are assigned, then a simple sort, like insertion sort, is used to sort the values within each bucket. Once sorted, the buckets are concatenated together to produce the final output. Under the assumption of uniformity, each bucket will contain no more than \\(1/n\\) of the total elements. This implies that each call to insertion_sort will take \\(O(1)\\) time.\nFigure 3: Bucket sort in action (Cormen et al. 2022). def bucket_sort(A): n = len(A) B = [[] for i in range(n)] for i in range(n): B[int(n * A[i])].append(A[i]) for i in range(n): insertion_sort(B[i]) return B Analysis Initializing the array and placing each item into a bucket takes \\(\\Theta(n)\\) time. The call to each insertion sort is \\(O(n^2)\\). Therefore, the recurrence is given as\n\\[ T(n) = \\Theta(n) + \\sum_{i=0}^{n-1} O(n_i^2). \\]\nThe key is to determine the expected value \\(E[n_i^2]\\). We will frame the problem as a binomial distribution, where a success occurs when an element goes into bucket \\(i\\).\n\\(p\\) is the probability of success: \\(p = \\frac{1}{n}\\). \\(q\\) is the probability of failure: \\(q = 1 - \\frac{1}{n}\\). Under a binomial distribution, we have that \\(E[n_i] = np = n(1/n) = 1\\) and \\(\\text{Var}[n_i] = npq = 1 - 1/n\\), where \\(p = 1/n\\) and \\(q = 1 - 1/n\\). The expected value is then\n\\[ E[n_i^2] = \\text{Var}[n_i] + E[n_i]^2 = 1 - 1/n + 1 = 2 - 1/n. \\]\nThis gives way to the fact that \\(E[T(n)] = \\Theta(n) + \\sum_{i=0}^{n-1} O(2 - 1/n) = \\Theta(n) + O(n) = \\Theta(n)\\).\nQuestions and Exercises Come up with applications of counting sort when \\(k = O(n)\\). Paul wants to use radix sort to sort \\(2^{16}\\) 32-bit numbers. What is the best value of \\(r\\) to use? How many calls to a stable sort will be made? In what ways does radix sort differ from quicksort? When is one better than the other? References Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. Introduction to Algorithms. 4th ed. MIT Press. http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/. ","date":1710195000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710195000,"objectID":"f088ec7de378d2a6d1927d4c3fe00761","permalink":"https://ajdillhoff.github.io/notes/sorting_in_linear_time/","publishdate":"2024-03-11T17:10:00-05:00","relpermalink":"/notes/sorting_in_linear_time/","section":"notes","summary":"Table of Contents Introduction Establishing a Lower Bound on Comparison Sorts Counting Sort Radix Sort Bucket Sort Questions and Exercises These are my personal notes for Chapter 8 of Introduction to Algorithms (Cormen et al. 2022). Readers should reference the book for more details when necessary.\nIntroduction All sorting algorithms discussed up to this point are comparison based. You may have thought, as I did, that sorting cannot be done without a comparison.","tags":["computer science","algorithms"],"title":"Sorting in Linear Time","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Key Concepts and Challenges Introduction The Merge Operation Tiled Merge Circular Buffers Thread Coarsening Key Concepts and Challenges Dynamic input data identification Data locality Buffer management schemes Introduction The merge operation takes two sorted subarrays and combines them into a single sorted array. You may be familiar with this approach from studying Divide and Conquer Algorithms. Parallelizing the merge operation is a non-trivial task and will require the use of a few new techniques.\nThe Merge Operation The specific parallel merge operation studied in these notes is from \u0026ldquo;Perfectly load-balanced, optimal, stable, parallel merge\u0026rdquo; (Siebert and Träff 2013). Their approach works by first computing which values are needed in each merge step, and then using a parallel kernel to compute the merge. These steps can be computed by each thread independently.\nCo-rank Function The key to the parallel merge algorithm reviewed in these notes is the co-ranking function (Siebert and Träff 2013). This function computes the range of indices needed from the two input values to produce a given value in the output array, without actually needing to merge the two input arrays.\nWhen merging two sorted arrays, we can observe that the output index \\(0 \\leq k \u0026lt; m + n\\) comes from either \\(0 \\leq i \u0026lt; m\\) from input \\(A\\) or \\(0 \\leq j \u0026lt; n\\) from input \\(B\\).\nFigure 1: Merging (A) and (B) to produce (C). In the figure above, the element at \\(k = 3\\) comes from \\(A[2]\\), so \\(i = 2\\). It must be that \\(k=3\\) is the result of merging the first \\(i=2\\) elements of \\(A\\) with the first \\(j=k - i\\) elements of \\(B\\). This works both ways: for \\(k=6\\), the value is taken from \\(B[3]\\), so \\(j=3\\), and the result is the merge of the first \\(i=k-j\\) elements of \\(A\\) with the first \\(j=3\\) elements of \\(B\\).\nAn efficient method for computing the co-rank function follows the first lemma put forth in (Siebert and Träff 2013):\nLemma 1. For any \\(k, 0 \\leq k \u0026lt; m + n\\), there exists a unique \\(i, 0 \\leq i \\leq m\\), and a unique \\(j, 0 \\leq j \\leq n\\), with \\(i + j = k\\) such that\n\\(i = 0\\) or \\(A[i-1] \\leq B[j]\\) and \\(j = 0\\) or \\(B[j-1] \u0026lt; A[i]\\). Figure 2: Co-rank function visualization (Siebert and Träff 2013). Implementation Given the rank \\(k\\) of an element in an output array \\(C\\) and two input arrays \\(A\\) and \\(B\\), the co-rank function \\(f\\) returns the co-rank value for the corresponding element in \\(A\\) and \\(B\\).\nHow would the co-rank function be used in the example above? Given two threads, let thread 1 compute the co-rank for \\(k=4\\). This would return \\(i=3\\) and \\(j=1\\). We quickly verify that this passes the first lemma stated above.\n\\[ A[2] = 5 \\leq B[1] = 5$ and $B[0] = 3 \u0026lt; A[3] = 7. \\]\nCode for the co-rank function is given below. Since the input arrays are already sorted, we can use a binary search to find the co-rank values.\nint co_rank(int k, int *A, int m, int *B, int n) { int i = min(k, m); int j = k - i; int i_low = max(0, k-n); int j_low = max(0, k-m); int delta; bool active = true; while (active) { if (i \u0026gt; 0 \u0026amp;\u0026amp; j \u0026lt; n \u0026amp;\u0026amp; A[i-1] \u0026gt; B[j]) { delta = (i - i_low + 1) / 2; j_low = j; i -= delta; j += delta; } else if (j \u0026gt; 0 \u0026amp;\u0026amp; i \u0026lt; m \u0026amp;\u0026amp; B[j-1] \u0026gt;= A[i]) { delta = (j - j_low + 1) / 2; i_low = i; j -= delta; i += delta; } else { active = false; } } return i; } Consider running a merge kernel across 3 threads where each thread takes 3 sequential output values. Use the co-rank function to compute the co-rank values for \\(k=3\\) and \\(k=6\\), simulating the tasks for the second and third threads. The values for \\(k=3\\) should be \\(i=2\\) and \\(j=1\\), for reference. All values below these indices would be used by the first thread.\nParallel Kernel We can now implement a basic parallel merge kernel. Each thread is responsible for determining how many elements it will be responsible for merging. The range of input values is determined via two calls to co_rank, one for the starting and ending point.\n__global__ void merge_basic_kernel(int *A, int m, int *B, int n, int *C) { int tid = blockIdx.x * blockDim.x + threadIdx.x; int elementsPerThread = ceil((m + n) / (blockDim.x * gridDim.x)); int k_curr = tid * elementsPerThread; // start output index int k_next = min((tid + 1) * elementsPerThread, m + n); // end output index int i_curr = co_rank(k_curr, A, m, B, n); int i_next = co_rank(k_next, A, m, B, n); int j_curr = k_curr - i_curr; int j_next = k_next - i_next; merge_sequential(\u0026amp;A[i_curr], i_next - i_curr, \u0026amp;B[j_curr], j_next - j_curr, \u0026amp;C[k_curr]); } Two major issues should be clear from the code above. First, the memory being accesses is not coalesced. The binary search in co_rank also means that the memory access pattern is less than ideal. Since the main issue in both cases relates to memory efficiency, we should look at tools that address memory access patterns.\nTiled Merge The memory access pattern is sparse and thus does not take advantage of coalescing. We can improve upon this by having the threads transfer data from global memory to shared memory in a coalesced manner. That way the higher latency operation will be coalesced. The data in shared memory may be accessed out of order, but the latency is much lower.\nThe subarrays from \\(A\\) and \\(B\\) that are used by adjacent threads are also adjacent in memory. By considering block-level subarrays, we can ensure that the data is coalesced. This is the idea behind the tiled merge algorithm. The figure below visualizes this concept.\nFigure 3: Design of a tiled merge kernel (recreated from (Hwu, Kirk, and El Hajj 2022)). The shared memory blocks \\(A_s\\) and \\(B_s\\) obviously cannot store the entire range of data needed. In each iteration, the threads in a block will load a new set of data from global memory to shared memory. The light gray section of the block from \\(A\\) and \\(B\\) are loaded into \\(A_s\\) and \\(B_s\\), respectively. If they collectively load \\(2n\\) elements, only \\(n\\) elements will be used in the merge operation. This is because in the worst case, all elements going to the output array will come from one of the two input arrays. See the exercise at the end of this section for a more detailed explanation.\nEach block will use a portion of both \\(A_s\\) and \\(B_s\\) to compute the merge. This is shown with dotted lines going from the shared memory to the output array.\nPart 1 The code below shows the first part of the tiled merge kernel.\n__global__ void merge_tiled_kernel(int *A, int m, int n, int *C, int tile_size) { extern __shared__ int shareAB[]; int *A_s = \u0026amp;shareAB[0]; int *B_s = \u0026amp;shareAB[tile_size]; int C_curr = blockIdx.x * ceil((m+n)/gridDim.x); int C_next = min((blockIdx.x+1) * ceil((m+n)/gridDim.x), m+n); if (threadIdx.x == 0) { // Block-level co-rank values will be available to all threads in the block A_s[0] = co_rank(C_curr, A, m, B, n); A_s[1] = co_rank(C_next, A, m, B, n); } __syncthreads(); int A_curr = A_s[0]; int A_next = A_s[1]; int B_curr = C_curr - A_curr; int B_next = C_next - A_next; __syncthreads(); The first part establishes the shared memory and the co-rank values for the block. Each thread will have access to the start and end values for input matrices \\(A\\) and \\(B\\) as well. If the kernel is just getting started, we would have that A_curr = 0, B_curr = 0, and C_curr = 0.\nPart 2 The second part of the kernel is responsible for loading the input data into shared memory. This is done in a coalesced manner, as the threads in a block will load a contiguous section of the input arrays.\nint counter = 0; int C_length = C_next - C_curr; int A_length = A_next - A_curr; int B_length = B_next - B_curr; int total_iteration = ceil(C_length / tile_size); int C_completed = 0; int A_consumed = 0; int B_consumed = 0; while (counter \u0026lt; total_iteration) { for (int i = 0; i \u0026lt; tile_size; i += blockDim.x) { if (i + threadIdx.x \u0026lt; A_length - A_consumed) { A_s[i + threadIdx.x] = A[A_curr + A_consumed + i + threadIdx.x]; } if (i + threadIdx.x \u0026lt; B_length - B_consumed) { B_s[i + threadIdx.x] = B[B_curr + B_consumed + i + threadIdx.x]; } } __syncthreads(); Part 3 With the input in shared memory, each thread will divide up this input and merge their respective sections in parallel. This is done by calculating the c_curr and c_next first, which is the output section of the thread. Using those boundaries, two calls to co_rank will determine the input sections the thread.\nint c_curr = threadIdx.x * (tile_size / blockDim.x); int c_next = (threadIdx.x + 1) * (tile_size / blockDim.x); c_curr = (c_curr \u0026lt;= C_length - C_completed) ? c_curr : C_length - C_completed; c_next = (c_next \u0026lt;= C_length - C_completed) ? c_next : C_length - C_completed; int a_curr = co_rank(c_curr, A_s, min(tile_size, A_length - A_consumed), B_s, min(tile_size, B_length - B_consumed)); int b_curr = c_curr - a_curr; int a_next = co_rank(c_next, A_s, min(tile_size, A_length - A_consumed), B_s, min(tile_size, B_length - B_consumed)); int b_next = c_next - a_next; merge_sequential(\u0026amp;A_s[a_curr], a_next - a_curr, \u0026amp;B_s[b_curr], b_next - b_curr, \u0026amp;C[C_urr + C_completed + c_curr]); counter++; C_completed += tile_size; A_consumed += co_rank(tile_size A_s, tile_size, B_s, tile_size); B_consumed = C_completed - A_consumed; __syncthreads(); } } Example: Walkthrough of Kernel Consider the following example. We have two input arrays \\(A = [1, 3, 5, 7, 9]\\) and \\(B = [2, 4, 6, 8, 10]\\). The output array \\(C\\) will have 10 elements. We will use 2 blocks and 4 threads per block. The tile size is 4. With 10 elements and 2 blocks, each block is responsible for 5 elements.\nThe main while loop will need to iterate twice to cover the entire output array. The first iteration will load the first 4 elements of \\(A\\) and \\(B\\) into shared memory. Once the data is in memory, each thread divides the input tiles by running the co-rank function on the data that is in shared memory. The computed indices are the boundaries between each thread.\nIn each iteration, a block is responsible for 4 elements. Given that we have 4 threads per block, each thread will be responsible for 1 output element per iteration. For thread 0 we have that c_curr = 0 and c_next = 2. This results in a_curr = 0, b_curr = 0, a_next = 1, and b_next = 1. The merge operation will then be performed on the first element of \\(A\\) and \\(B\\).\nAnalysis Coalesces global memory accesses Shared memory is used to reduce latency Only half the data loaded into shared memory is used; wasted memory bandwidth Exercise Hwu et al. suggest that you can first call the co-rank function to get the current and next output sections. This would increase memory bandwidth at the cost of an additional binary search.\nWhere would this be done with respect to the tiled solution discussed in this section? How do these co-rank values differ from the ones used to calculate C_curr and C_next? Hint: If we knew the co-rank value for the start of the next section, we could ensure that only the data below that index is loaded into shared memory.\nCircular Buffers The tiled merge algorithm is a significant improvement over the basic merge kernel. One glaring issue is that only half of the data loaded into shared memory is used, leading to a waste of memory bandwidth. The circular buffer merge algorithm addresses this issue by using a circular buffer to store the input data. Instead of writing over the shared memory values on each iteration, the data to be used in the next iteration stays in shared memory. A portion of new data is loaded into shared memory based on how much was used in the previous iteration.\nFigure 4: Circular buffer scheme (recreated from (Hwu, Kirk, and El Hajj 2022)). The figure above outlines the main idea behind the circular buffer merge algorithm. Part A shows the initial data layout of global and shared memory. Only a portion of the data loaded into shared memory is used in the merge operation. This is shown in part B, where the blank portion of the shared memory depicts the data that was used. The light gray regions of shared memory are the left over data that can be used in the next iteration.\nThe next block of data is loaded into shared memory. Since some of the required data already exists from the last iteration, a smaller portion needs to be loaded. This is shown in part C, where the new data (dark gray) is loaded into shared memory. The starting indices for both arrays was already set in the previous iteration. Consecutive values are simple to calculate using the mod operator.\nPart D shows the state of the arrays after the end of the second iteration. The blank areas in shared memory are the data that was used in the merge operation. For array A_S, the index wrapped around to the beginning of the array. It is ready to be used in the next iteration.\nImplementation A_consumed can be used to keep track of how many new elements need to be read into shared memory. The co_rank and merge_sequential functions need to be updated to work with circular buffers. It is easier to treat the shared memory as an extended array, that way we avoid situations where the next index is less than the current index.\nCo-Rank Function int co_rank_circular(int k, int *A, int m, int *B, int n, int A_S_start, int B_S_start, int tile_length) { int i = min(k, m); int j = k - i; int i_low = max(0, k-n); int j_low = max(0, k-m); int delta; bool active = true; while (active) { int i_cir = (A_S_start + i) % tile_length; int j_cir = (B_S_start + j) % tile_length; int i_m_1_cir = (A_S_start + i - 1) % tile_length; int j_m_1_cir = (B_S_start + j - 1) % tile_length; if (i \u0026gt; 0 \u0026amp;\u0026amp; j \u0026lt; n \u0026amp;\u0026amp; A[i_m_1_cir] \u0026gt; B[j_cir]) { delta = ((i - i_low + 1) \u0026gt;\u0026gt; 1); j_low = j; i -= delta; j += delta; } else if (j \u0026gt; 0 \u0026amp;\u0026amp; i \u0026lt; m \u0026amp;\u0026amp; B[j_m_1_cir] \u0026gt;= A[i_cir]) { delta = ((j - j_low + 1) \u0026gt;\u0026gt; 1); i_low = i; j -= delta; i += delta; } else { active = false; } } return i; } In this updated version of the co-rank function, the user only needs to provide the start indices for the shared memory arrays along with the tile size.\nMerge Sequential void merge_sequential_circular(int *A, int m, int *B, int n, int *C, int A_S_start, int B_S_start, int tile_size) { int i = 0; int j = 0; int k = 0; while (i \u0026lt; m \u0026amp;\u0026amp; j \u0026lt; n) { int i_cir = (A_S_start + i) % tile_size; int j_cir = (B_S_start + j) % tile_size; if (A[i_cir] \u0026lt;= B[j_cir]) { C[k] = A[i_cir]; i++; } else { C[k] = B[j_cir]; j++; } k++; } if (i == m) { while (j \u0026lt; n) { int j_cir = (B_S_start + j) % tile_size; C[k] = B[j_cir]; j++; k++; } } else { while (i \u0026lt; m) { int i_cir = (A_S_start + i) % tile_size; C[k] = A[i_cir]; i++; k++; } } } Again, this revision makes it easier on the user since they only need to provide the start indices for the shared memory arrays and the tile size. These are used to compute the indices for the circular buffer.\nCircular Buffer Kernel int c_curr = threadIdx.x * (tile_size / blockDim.x); int c_next = (threadIdx.x + 1) * (tile_size / blockDim.x); c_curr = (c_curr \u0026lt;= C_length - C_completed) ? c_curr : C_length - C_completed; c_next = (c_next \u0026lt;= C_length - C_completed) ? c_next : C_length - C_completed; int a_curr = co_rank_circular(c_curr, A_s, min(tile_size, A_length - A_consumed), B_s, min(tile_size, B_length - B_consumed), A_curr, B_curr, tile_size); int b_curr = c_curr - a_curr; int a_next = co_rank_circular(c_curr, A_s, min(tile_size, A_length - A_consumed), B_s, min(tile_size, B_length - B_consumed), A_curr, B_curr, tile_size); int b_next = c_next - a_next; merge_sequential_circular(A_s, a_next - a_curr, B_s, b_next - b_curr, \u0026amp;C[C_urr + C_completed + c_curr], A_S_start + A_curr, B_S_start + B_curr, tile_size); // Compute the indices that were used counter++; A_S_consumed = co_rank_circular(min(tile_size, C_length - C_completed), A_s, min(tile_size, A_length - A_consumed), B_s, min(tile_size, B_length - B_consumed), A_S_start, B_S_start, tile_size); B_S_consumed = min(tile_size, C_length - C_completed) - A_S_consumed; A_consumed += A_S_consumed; C_completed += min(tile_size, C_length - C_completed); B_consumed = C_completed - A_consumed; // Update the start indices for the next iteration A_S_start = (A_S_start + A_S_consumed) % tile_size; B_S_start = (B_S_start + B_S_consumed) % tile_size; __syncthreads(); } } The first section of part 3 of the original kernel remains mostly unchanged, with the exceptions that the co-rank function and merge function are now called with the circular versions. The larger change is in the second half of the kernel. A_S_consumed and B_S_consumed are used to keep track of how much of the shared memory arrays were used. This is then used to offset the used indices from the original arrays. Finally, the start indices for the shared memory arrays are updated for the next iteration.\nThread Coarsening The kernels presented in these notes already utilize thread coarsening. Each thread is responsible for a range of output values. The simple example presented earlier demonstrates what a non-coarse approach would look like. Each thread was only responsible for a single output value.\nReferences Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. Programming Massively Parallel Processors: A Hands-on Approach. Fourth. Morgan Kaufmann. Siebert, Christian, and Jesper Larsson Träff. 2013. “Perfectly Load-Balanced, Optimal, Stable, Parallel Merge.” arXiv. http://arxiv.org/abs/1303.4312. ","date":1709169540,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709169540,"objectID":"e5af7ac7025177e0c4a15df300262c3d","permalink":"https://ajdillhoff.github.io/notes/gpu_pattern_merge/","publishdate":"2024-02-28T19:19:00-06:00","relpermalink":"/notes/gpu_pattern_merge/","section":"notes","summary":"Table of Contents Key Concepts and Challenges Introduction The Merge Operation Tiled Merge Circular Buffers Thread Coarsening Key Concepts and Challenges Dynamic input data identification Data locality Buffer management schemes Introduction The merge operation takes two sorted subarrays and combines them into a single sorted array. You may be familiar with this approach from studying Divide and Conquer Algorithms. Parallelizing the merge operation is a non-trivial task and will require the use of a few new techniques.","tags":["gpgpu"],"title":"GPU Pattern: Merge","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Example from CLRS Making the Wrong Guess The substitution method is a technique for solving recurrences. It works in two steps:\nGuess the solution Use mathematical induction to verify the guess This can work very well, especially if we already have some intuition about the problem. Let\u0026rsquo;s start with a simple example: The Tower of Hanoi. In this classic puzzle, we have three pegs and a number of disks of different sizes which can slide onto any peg. The puzzle starts with the disks in a neat stack in ascending order of size on one peg, with the smallest disk on top. The objective is to move the entire stack to another peg, obeying the following rules:\nOnly one disk can be moved at a time Each move consists of taking the top disk from one of the stacks and placing it on top of another stack No disk may be placed on top of a smaller disk An algorithm to solve the puzzle goes like this:\nMove \\(n-1\\) disks from peg 1 to peg 2 using peg 3 as a temporary holding area Move the $n$th disk from peg 1 to peg 3 Move the \\(n-1\\) disks from peg 2 to peg 3 using peg 1 as a temporary holding area The number of moves required to solve the Tower of Hanoi puzzle is given by the recurrence relation \\(T(n) = 2T(n-1) + 1\\) with the initial condition \\(T(1) = 1\\). We can solve this recurrence using the substitution method.\nOur hypothesis might be that \\(T(n) \\leq c2^n - 1\\) for all \\(n \\geq n_0\\), where \\(c \u0026gt; 0\\) and \\(n_0 \u0026gt; 0\\). For the base case, we have \\(T(1) = c * 2^1 - 1 = c\\), so \\(c = 1\\). Now we need to show that \\(T(n) \\leq c2^n - 1\\) for all \\(n \\geq n_0\\). Then we have:\n\\begin{align*} T(n) \u0026amp;\\leq 2T(n-1) + 1 \\\\ \u0026amp;\\leq 2\\left(2^{n-1} - 1\\right) + 1 \\\\ \u0026amp;= 2^n - 2 + 1 \\\\ \u0026amp;= 2^n - 1 \\\\ \\end{align*}\nWhat if we made a bad guess? Let\u0026rsquo;s try \\(T(n) \\leq cn\\) for all \\(n \\geq n_0\\). We have \\(T(1) = c = 1\\), so \\(c = 1\\). Now we need to show that \\(T(n) \\leq cn\\) for all \\(n \\geq n_0\\). We assume that \\(T(k) \\leq ck\\) for all \\(k \u0026lt; n\\). Then we have:\n\\begin{align*} T(n) \u0026amp;\\leq 2T(n-1) + 1 \\\\ \u0026amp;\\leq 2c(n-1) + 1 \\\\ \u0026amp;= 2cn - 2c + 1 \\\\ \\end{align*}\nThis does not work because \\(2cn - 2c + 1 \u0026gt; cn\\) for all \\(c \u0026gt; 1\\). Therefore, our guess was wrong.\nExample from CLRS Determine an asymptotic upper bound for\n\\[ T(n) = 2T(\\lfloor n/2 \\rfloor) + \\Theta(n). \\]\nGuess: \\(T(n) = O(n \\lg n)\\)\nInductive hypothesis: \\(T(n) \\leq cn \\lg n\\) for all \\(n \\geq n_0\\).\nInductive step: Assume \\(T(n) \\leq cn \\lg n\\) for all \\(n_0 \\leq k \u0026lt; n\\). For \\(T(\\lfloor n/2 \\rfloor) \\leq c\\lfloor n/2 \\rfloor \\lg \\lfloor n/2 \\rfloor\\), it holds when \\(n \\geq 2\\).\n\\begin{align*} T(n) \u0026amp;\\leq 2T(\\lfloor n/2 \\rfloor) + \\Theta(n) \\\\ \u0026amp;\\leq 2c\\lfloor n/2 \\rfloor \\lg \\lfloor n/2 \\rfloor + \\Theta(n) \\\\ \u0026amp;= cn \\lg (n / 2) + \\Theta(n) \\\\ \u0026amp;= cn \\lg n - cn\\lg 2 + \\Theta(n) \\\\ \u0026amp;= cn \\lg n - cn + \\Theta(n) \\\\ \u0026amp;\\leq cn \\lg n \\end{align*}\nMaking the Wrong Guess What if we took the same recurrence and guessed that \\(T(n) = O(n)\\)?\nGuess: \\(T(n) = O(n)\\)\nInductive hypothesis: \\(T(n) \\leq cn\\) for all \\(n \\geq n_0\\).\nInductive step: Assume \\(T(n) \\leq cn\\) for all \\(n \\geq n_0\\).\n\\begin{align*} T(n) \u0026amp;\\leq 2c\\lfloor n/2 \\rfloor + \\Theta(n) \\\\ \u0026amp;\\leq cn + \\Theta(n) \\\\ \\end{align*}\nThis does not work because \\(cn + \\Theta(n) \u0026gt; cn\\). Therefore, our guess was wrong.\n","date":1709082720,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709082720,"objectID":"f6afdeab010490f7181d9f6ba7bf6a0b","permalink":"https://ajdillhoff.github.io/notes/substitution_method/","publishdate":"2024-02-27T19:12:00-06:00","relpermalink":"/notes/substitution_method/","section":"notes","summary":"Table of Contents Example from CLRS Making the Wrong Guess The substitution method is a technique for solving recurrences. It works in two steps:\nGuess the solution Use mathematical induction to verify the guess This can work very well, especially if we already have some intuition about the problem. Let\u0026rsquo;s start with a simple example: The Tower of Hanoi. In this classic puzzle, we have three pegs and a number of disks of different sizes which can slide onto any peg.","tags":["algorithms","computer science"],"title":"Substitution Method","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Basic Quicksort Performance Randomized Quicksort Paranoid Quicksort Quicksort is a popular sorting algorithm implemented in many language libraries that has a worst-case running time of \\(\\Theta(n^2)\\). Why would anyone choose this as the default sorting algorithm if one like mergesort has better worst-case performance? As you will see, the devil is in the details. Quicksort is often faster in practice. It also has a small memory footprint and is easy to implement.\nTODO: Discuss distinct values and the impact on quicksort\nBasic Quicksort Quicksort follows a divide-and-conquer approach to sorting. Given an input array of \\(n\\) elements, it selects a pivot element and partitions the array into two sub-arrays: one with elements less than the pivot and one with elements greater than the pivot. It then recursively sorts the sub-arrays. Since the subarrays are recursively sorted, there is no need for a merge step as in mergesort.\ndef quicksort(arr, p, r): q = partition(arr, p, r) quicksort(arr, p, q - 1) quicksort(arr, q + 1, r) This algorithm looks deceptively simple. The complexity is hidden in the partitioning step. This procedure will rearrange the elements in the array such that the pivot element is in its final position and all elements less than the pivot are to the left of it and all elements greater than the pivot are to the right of it.\nPartitioning For basic quicksort, the first or last element is chosen as the pivot. Picking it this way yields a fairly obvious recurrence of \\(T(n) = T(n-1) + O(n)\\), which is \\(\\Theta(n^2)\\). As mentioned before, this algorithm is executed in-place, meaning there is no need for additional memory to store the sub-arrays. It is done through a clever use of indices.\ndef partition(arr, p, r): x = arr[r] i = p - 1 for j in range(p, r): if arr[j] \u0026lt;= x: i += 1 arr[i], arr[j] = arr[j], arr[i] arr[i + 1], arr[r] = arr[r], arr[i + 1] return i + 1 The indices are used to define the following loop invariant.\nLeft: if \\(p \\leq k \\leq i\\), then \\(A[k] \\leq x\\) Middle: if \\(i + 1 \\leq k \\leq j - 1\\), then \\(A[k] \u0026gt; x\\) Right: if \\(k = r\\), then \\(A[k] = x\\) Figure 1: Operation of partition (Cormen et al. 2022). The figure above is from CRLS which shows an example run of the partitioning algorithm. It starts with \\(i, p, j\\) on the left and \\(r\\) on the right. Since the value at \\(j\\) is less than the pivot, \\(i\\) is incremented and the values at \\(i\\) and \\(j\\) are swapped. In the next iteration, the value at \\(j\\) is greater than the pivot, so nothing is done. This continues until \\(j\\) reaches \\(r\\). At this point, the pivot is swapped with the value at \\(i + 1\\).\nExample Figure 2: Example of Quicksort. The example above starts with an unsorted array. The second row shows the array after the first call to partition. The left and right subarrays are called recursively. The first subarray in row 3 has a pivot 3 and is already partitioned. The right subarray follows the same convenience.\nOn row 4, the left subarray is modified by swapping the pivot of 2 with 1. The right subarray first swaps 5 and 7 before swapping 6 and 7. The final array is sorted as shown on the last row.\nPerformance The performance of quicksort is dependent on the pivot. If the subarrays on either side of the pivot are balanced, the running time is asymptotically similar to mergesort. In the worst case, it will run in quadratic time.\nThe worst partitioning occurs when the pivot is the smallest or largest element in the array. This creates a subarray of size \\(n - 1\\) and another of size 0. The partitioning itself takes \\(\\Theta(n)\\) time, yielding a recurrence of \\(T(n) = T(n - 1) + \\Theta(n)\\). We can use the substitution method to solve this recurrence and find that the worst-case running time is \\(\\Theta(n^2)\\). This can happen even if the input is already sorted before hand.\nCormen et al. present a recursive analysis of the running time where, at each level, the partition produces a 9-to-1 split. This is visualized in the sketch below.\nFigure 3: Recursion tree for quicksort. The subtree for the \\(\\frac{1}{10}\\) split eventually bottoms out after being called \\(\\log_{10} n\\) times. Until this happens, the cost of each level of the tree is \\(n\\). After the left-tree bottoms out, the right tree continues with an upper bound of \\(\\leq n\\). The right tree completes after \\(\\log_{10/9} n = \\Theta(\\lg n)\\) levels. Since each level of the tree cost no more than \\(n\\), the total cost is \\(\\Theta(n \\lg n)\\).\nBest Case In the best-case, the pivot is the median of the array and two balanced subarrays are created: one of size \\(n/2\\) and another of size \\(\\lfloor (n-1)/2 \\rfloor\\). The recurrence is \\(T(n) = 2T(n/2) + \\Theta(n)\\), which is \\(\\Theta(n \\log n)\\).\nUsing the substitution method, we can show the best-case running time. We start with the fact that the partitioning produces two subproblems with a total size of \\(n-1\\). This gives the following recurrence:\n\\[ T(n) = \\min_{0 \\leq q \\leq n-1} \\{T(q) + T(n - q - 1)\\} + \\Theta(n). \\]\nThe minimum function accounts for the minimum time taken to sort any partition of the array, where \\(q\\) represents the pivot element\u0026rsquo;s position.\nOur hypothesis will be that\n\\[ T(n) \\geq cn \\lg n = \\Omega(n \\lg n). \\]\nPlugging in our hypothesis, we get\n\\begin{align*} T(n) \u0026amp;\\geq \\min_{0 \\leq q \\leq n-1} \\{cq \\lg q + c(n - q - 1) \\lg (n - q - 1)\\} + \\Theta(n) \\\\ \u0026amp; c \\min_{0 \\leq q \\leq n-1} \\{q \\lg q + (n - q - 1) \\lg (n - q - 1)\\} + \\Theta(n). \\end{align*}\nIf we take the derivative of the function inside the minimum with respect to \\(q\\), we get\n\\[ \\frac{d}{dq} \\{q \\lg q + (n - q - 1) \\lg (n - q - 1)\\} = c\\{\\frac{q}{q} + \\lg q - \\lg (n - q - 1) - \\frac{(n - q - 1)}{(n - q - 1)}\\}. \\]\nSetting this equal to zero and solving for \\(q\\) yields\n\\[ q = \\frac{n - 1}{2}. \\]\nWe can then plug this value of \\(q\\) into the original function to get\n\\begin{align*} T(n) \u0026amp;\\geq c \\frac{n - 1}{2} \\lg \\frac{n - 1}{2} + c \\frac{n - 1}{2} \\lg \\frac{n - 1}{2} + \\Theta(n) \\\\ \u0026amp;= cn \\lg (n - 1) + c (n - 1) + \\Theta(n) \\\\ \u0026amp;= cn \\lg (n - 1) + \\Theta(n) \\\\ \u0026amp;\\geq cn \\lg n \\\\ \u0026amp;= \\Omega(n \\lg n). \\end{align*}\nAverage Case As it turns out, the average-case running time is \\(\\Theta(n \\log n)\\) (Cormen et al. 2022). Quicksort is highly dependent on the relative ordering of the input. Consider the case of a randomly ordered array. The cost of partitioning the original input is \\(O(n)\\). Let\u0026rsquo;s say that the pivot was the last element, yielding a split of 0 and \\(n - 1\\). Now, let\u0026rsquo;s say we get lucky on the next iteration and get a balanced split. Even if the rest of the algorithm splits between the median and the last element, the upper bound on the running time is \\(\\Theta(n \\log n)\\). It is highly unlikely that the split will be unbalanced on every iteration given a random initial ordering.\nWe can make this a tad more formal by defining a lucky \\(L(n) = 2U(n/2) + \\Theta(n)\\) and an unlucky split \\(U(n) = L(n-1) + \\Theta(n)\\). We can solve for \\(L(n)\\) by plugging in the definition of \\(U(n)\\).\n\\begin{align*} L(n) \u0026amp;= 2U(n/2) + \\Theta(n) \\\\ \u0026amp;= 2(L(n/2 - 1) + \\Theta(n/2)) + \\Theta(n) \\\\ \u0026amp;= 2L(n/2 - 1) + \\Theta(n) \\\\ \u0026amp;= \\Theta(n \\log n) \\end{align*}\nRandomized Quicksort The intuition of the crude analysis above is that we would have to be extremely unlucky to get a quadratic running time if the input is randomly ordered. Randomized quicksort builds on this intuition by selection a random pivot on each iteration. This is done by swapping the pivot with a random element before partitioning.\nimport random def randomized_partition(arr, p, r): i = random.randint(p, r) arr[i], arr[r] = arr[r], arr[i] return partition(arr, p, r) def randomized_quicksort(arr, p, r): if p \u0026lt; r: q = randomized_partition(arr, p, r) randomized_quicksort(arr, p, q - 1) randomized_quicksort(arr, q + 1, r) Analysis The lightweight analysis above reasoned that, as long as each split puts a constant amount of elements to one side of the split, then the running time is \\(\\Theta(n \\log n)\\).\nWe can understand this analysis simply by asking the right questions. First, our primary question: What is the running time of Quicksort dependent on? The biggest bottleneck is the partitioning function. At most, we get really unlucky and the first pivot is picked every time. This means it is called \\(n\\) times yielding \\(O(n)\\). The variable part of this is figuring out how many element comparisons are made. The running time is then \\(O(n + X)\\).\nThe Expected Value of \\(X\\) The number of comparisons can be expressed as\n\\[ X = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} X_{ij}, \\]\nwhere \\(X_{ij}\\) is the indicator random variable that is 1 if \\(A[i]\\) and \\(A[j]\\) are compared and 0 otherwise. This works with our worst case analysis. If we always get a split of 0 and \\(n - 1\\), then the indicator random variable is 1 for every comparison, yielding \\(O(n^2)\\). Taking the expectation of both sides:\n\\begin{align*} E[X] \u0026amp;= E\\left[\\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} X_{ij}\\right] \\\\ \u0026amp;= \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} E[X_{ij}] \\\\ \u0026amp;= \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} P(X_{ij} = 1). \\end{align*}\nWhat is P(Xij = 1)? Let \\(z_i, \\dots, z_j\\) be the indices of elements in a sorted version of the array. Under this assumption, \\(z_i\\) is compared to \\(z_j\\) only if \\(z_i\\) or \\(z_j\\) is the first pivot chosen from the subarray \\(A[i \\dots j]\\). In a set of distinct elements, the probability of picking any pivot from the array from \\(i\\) to \\(j\\) is \\(\\frac{1}{j - i + 1}\\). This means that the probability of comparing \\(z_i\\) and \\(z_j\\) is \\(\\frac{2}{j - i + 1}\\). We can now finish the calculation.\n\\begin{align*} E[X] \u0026amp;= \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} \\frac{2}{j - i + 1} \\\\ \u0026amp;= \\sum_{i=1}^{n-1} \\sum_{k=1}^{n-i} \\frac{2}{k + 1} \\\\ \u0026amp;\u0026lt; \\sum_{i=1}^{n-1} \\sum_{k=1}^{n-i} \\frac{2}{k} \\\\ \u0026amp;= \\sum_{i=1}^{n-1} O(\\log n) \\\\ \u0026amp;= O(n \\log n). \\end{align*}\nParanoid Quicksort Repeat the following until the partitioning until the left or right subarray is less than or equal to \\(\\frac{3}{4}\\) of the original array.\nChoose a random pivot. Partition the array. References Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. Introduction to Algorithms. 4th ed. MIT Press. http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/. ","date":1708903440,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708903440,"objectID":"a1de0e0169b15f32764413614103f5a7","permalink":"https://ajdillhoff.github.io/notes/quicksort/","publishdate":"2024-02-25T17:24:00-06:00","relpermalink":"/notes/quicksort/","section":"notes","summary":"Table of Contents Basic Quicksort Performance Randomized Quicksort Paranoid Quicksort Quicksort is a popular sorting algorithm implemented in many language libraries that has a worst-case running time of \\(\\Theta(n^2)\\). Why would anyone choose this as the default sorting algorithm if one like mergesort has better worst-case performance? As you will see, the devil is in the details. Quicksort is often faster in practice. It also has a small memory footprint and is easy to implement.","tags":["algorithms","computer science"],"title":"Quicksort","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Quick Facts Introduction Implementation Exercises Quick Facts Time Complexity: \\(O(\\lg n)\\) Space Complexity: \\(O(n)\\) Introduction Besides being the primary data structure for Heapsort, a heap is also used to implement a priority queue. A priority queue is a key-value data structure in which the keys are used to determine the priority of each element in the queue. There are two variants, maximum and minimum, and they support the following operations:\nInsert: Add a new element to the queue. Extract: Remove the element with the maximum/minimum key. Maximum/Minimum: Return the element with the maximum/minimum key without removing it. Increase/Decrease Key: Increase or decrease the key of a given element. You could probably imagine a few use cases for such a queue. For example, a priority queue could be used to schedule tasks in a multitasking operating system. The tasks with the highest priority would be executed first. Another example would be a network router that uses a priority queue to schedule packets for transmission. The packets with the highest priority would be transmitted first. In high performance computing, a priority queue could be used to schedule jobs on a supercomputer. The jobs with the highest priority would be executed first. SLURM is an example of a job scheduler that uses a priority queue.\nFor simple applications, you could reference your application object directly inside the heap. If the objects themselves are too complex, it is optimal to simply set the value of the heap as a reference to the object. A handle is a reference that is added to both the heap and the object; it requires little overhead. This requires that your priority queue update both its own index as well as the object\u0026rsquo;s index as changes are made.\nAn alternative approach is to establish the map using a hash table. In this case, the priority queue is the only data structure that needs to be updated.\nImplementation Let us now consider the implementation and analysis of the require operations for a priority queue.\nExtract The keys in a priority queue represent the priority. The values will need to be moved around with them as the priority queue is constructed. Getting the maximum or minimum value is a constant time operation and is executed by returning the first element in the array. To extract the item with the highest priority, the first element is removed and the heap is then heapified.\ndef max_heap_maximum(A): if len(A) \u0026lt; 1: raise ValueError(\u0026#34;Heap underflow\u0026#34;) return A[0] def max_heap_extract_max(A): max_val = max_heap_maximum(A) A[0] = A[-1] A.pop() max_heapify(A, 0) return max_val As we saw with Heapsort, max_heapify runs in \\(O(\\lg n)\\) time. The call to max_heap_extract_max only adds a few constant operations on top of that, so it runs in \\(O(\\lg n)\\) time as well.\nIncrease The max_heap_increase_key function is used to increase the key of a given element. The function first checks if the new key is less than the current key. If it is, an error is raised. The function then updates the key and then traverses up the heap to ensure that the heap property is maintained.\ndef max_heap_increase_key(A, obj, key): if key \u0026lt; obj.key: raise ValueError(\u0026#34;New key is smaller than current key\u0026#34;) obj.key = key i = A.index(obj) # gets the index of the object while i \u0026gt; 0 and A[parent(i)].key \u0026lt; A[i].key: A[i], A[parent(i)] = A[parent(i)], A[i] i = parent(i) Moving through the height of the tree is done in \\(O(\\lg n)\\) time. Depending on the how the index of the object is found, the complexity could be higher. In most cases, the index is found in constant time.\nInsert The max_heap_insert function is used to insert a new element into the heap. The function first appends the new element to the end of the array. It then sets the key of the new element to a very small value and then calls max_heap_increase_key to update the key to the correct value.\ndef max_heap_insert(A, obj, n): if len(A) == n: raise ValueError(\u0026#34;Heap overflow\u0026#34;) key = float(\u0026#34;-inf\u0026#34;) obj.key = key A.append(obj) # map obj to the last index -- dependent on the implementation max_heap_increase_key(A, obj, key) The call to max_heap_increase_key runs in \\(O(\\lg n)\\) time, so the max_heap_insert function also runs in \\(O(\\lg n)\\) time in addition to the time it takes to map the object to its index.\nExercises Implement a minimum priority queue. Implement a decrease key function for a maximum priority queue. Simulate a job scheduler using a priority queue that considers the priority of the job and the time it was submitted. References ","date":1708805400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708805400,"objectID":"e19f4cd44700696125a5f7f99d750ceb","permalink":"https://ajdillhoff.github.io/notes/priority_queues/","publishdate":"2024-02-24T14:10:00-06:00","relpermalink":"/notes/priority_queues/","section":"notes","summary":"Table of Contents Quick Facts Introduction Implementation Exercises Quick Facts Time Complexity: \\(O(\\lg n)\\) Space Complexity: \\(O(n)\\) Introduction Besides being the primary data structure for Heapsort, a heap is also used to implement a priority queue. A priority queue is a key-value data structure in which the keys are used to determine the priority of each element in the queue. There are two variants, maximum and minimum, and they support the following operations:","tags":["algorithms","computer science"],"title":"Priority Queues","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Maintaining the Heap Property Building the Heap Heapsort Running time is \\(O(n \\lg n)\\). Sorts in place, only a constant number of elements needed in addition to the input. Manages data with a heap. A binary heap can be represented as a binary tree, but is stored as an array. The root is the first element of the array. The left subnode for the element at index \\(i\\) is located at \\(2i\\) and the right subnode is located at \\(2i + 1\\). This assumes a 1-based indexing.\nUsing 0-based indexing, we can use \\(2i + 1\\) for the left and \\(2i + 2\\) for the right. The parent could be accessed via \\(\\lfloor \\frac{i-1}{2} \\rfloor\\).\nFigure 1: A binary tree as a heap with its array representation (Cormen et al. 2022). Heaps come in two flavors: max-heaps and min-heaps. They can be identified by satisfying a heap property.\nmax-heap property: \\(A[parent(i)] \\geq A[i]\\) min-heap property: \\(A[parent(i)] \\leq A[i]\\) These properties imply that the root is the largest element in a max-heap and the smallest element in a min-heap.\nWhen it comes to heapsort, a max-heap is used. Min-heaps are used in priority queues. These notes will cover both.\nMaintaining the Heap Property When using heapsort, the heap should always satisfy the max-heap property. This relies on a procedure called max_heapify. This function assumes that the root element may violate the max-heap property, but the subtrees rooted by its subnodes are valid max-heaps. The function then swaps nodes down the tree until the misplaced element is in the correct position.\ndef max_heapify(A, i, heap_size): l = left(i) r = right(i) largest = i if l \u0026lt; heap_size and A[l] \u0026gt; A[i]: largest = l if r \u0026lt; heap_size and A[r] \u0026gt; A[largest]: largest = r if largest != i: A[i], A[largest] = A[largest], A[i] max_heapify(A, largest, heap_size) Analysis of Max Heapify Given that max_heapify is a recursive function, we can analyze it with a recurrence. The driving function in this case would be the fix up that happens between the current node and its two subnodes, which is a constant time operation. The recurrence is based on how many elements are in the subheap rooted at the current node.\nIn the worst case of a binary tree, the last level of the tree is half full. That means that the left subtree has height \\(h + 1\\) compared to the right subtree\u0026rsquo;s height of \\(h\\). For a tree of size \\(n\\), the left subtree has \\(2^{h+2}-1\\) nodes and the right subtree has \\(2^{h+1}-1\\) nodes. This is based on a geometric series.\nWe now have that the number of nodes in the tree is equal to \\(1 + (2^{h+2}-1) + (2^{h+1}-1)\\).\n\\begin{align*} n \u0026amp;= 1 + 2^{h+2} - 1 + 2^{h+1} - 1 \\\\ n \u0026amp;= 2^{h+2} + 2^{h+1} - 1 \\\\ n \u0026amp;= 2^{h+1}(2 + 1) - 1 \\\\ n \u0026amp;= 3 \\cdot 2^{h+1} - 1 \\end{align*}\nThis implies that \\(2^{h+1} = \\frac{n+1}{3}\\). That means that, in the worst case, the left subtree would have \\(2^{h+2} - 1 = \\frac{2(n+1)}{3} - 1\\) nodes which is bounded by \\(\\frac{2n}{3}\\). Thus, the recurrence for the worst case of max_heapify is \\(T(n) = T(\\frac{2n}{3}) + O(1)\\).\nBuilding the Heap Given an array of elements, how do we build the heap in the first place? The solution is to build it using a bottom-up approach from the leaves. The elements from \\(\\lfloor \\frac{n}{2} \\rfloor + 1\\) to \\(n\\) are all leaves. This means that they are all 1-element heaps. We can then run max_heapify on the remaining elements to build the heap.\ndef build_max_heap(A): heap_size = len(A) for i in range(len(A) // 2, -1, -1): max_heapify(A, i, heap_size) Why does this work? Each node starting at \\(\\lfloor \\frac{n}{2} \\rfloor + 1\\) is the root of a 1-element heap. The subnodes, which are to the right of node \\(\\lfloor \\frac{n}{2} \\rfloor\\), are roots of their own max-heaps. The procedure loops down to the first node until all sub-heaps have been max-heapified.\nThe figure below is from (Cormen et al. 2022) and shows the process of building a max-heap from an array.\nFigure 2: Building a max-heap from an array (Cormen et al. 2022). Analysis of Build Max Heap A general analysis is fairly straightforward considering that the call to max_heapify is \\(O(\\lg n)\\). The loop in build_max_heap runs \\(O(n)\\) times. This means that the overall running time is \\(O(n \\lg n)\\). A more careful analysis can be done by considering the height of the tree and the number of nodes at each level.\nA heap of \\(n\\) elements has height \\(\\lfloor \\lg n \\rfloor\\) Each call to max_heapify can also be viewed in terms of the height of the tree \\(h\\), so the upper bound is \\(O(h)\\). This bounds build_max_heap at \\(\\sum_{h=0}^{\\lfloor \\lg n \\rfloor} \\lceil \\frac{n}{2^{h+1}} \\rceil ch\\). When \\(h = 0\\), the first term \\(\\lceil \\frac{n}{2^{h+1}} \\rceil = \\lceil \\frac{n}{2} \\rceil\\). When \\(h = \\lfloor \\lg n \\rfloor\\), \\(\\lceil \\frac{n}{2^{h+1}} \\rceil = 1\\). Thus, \\(\\lceil \\frac{n}{2^{h+1}} \\rceil \\geq \\frac{1}{2}\\) for \\(0 \\leq h \\leq \\lfloor \\lg n \\rfloor\\).\nLet \\(x = \\frac{n}{2^{h+1}}\\). Since \\(x \\geq \\frac{1}{2}\\), we have that \\(\\lceil x \\rceil \\leq 2x\\). This means that \\(\\lceil \\frac{n}{2^{h+1}} \\rceil \\leq \\frac{2n}{2^{h+1}} = \\frac{n}{2^h}\\). An upper bound can now be derived.\n\\begin{align*} \\sum_{h=0}^{\\lfloor \\lg n \\rfloor} \\lceil \\frac{n}{2^{h+1}} \\rceil ch \u0026amp;\\leq \\sum_{h=0}^{\\lfloor \\lg n \\rfloor} \\frac{n}{2^h} ch \\\\ \u0026amp;= cn \\sum_{h=0}^{\\lfloor \\lg n \\rfloor} \\frac{h}{2^h} \\\\ \u0026amp;\\leq cn \\sum_{h=0}^{\\infty} \\frac{h}{2^h} \\\\ \u0026amp;\\leq cn \\cdot \\frac{1 / 2}{(1 - 1/2)^2}\\quad \\text{(See CRLS for details)} \\\\ \u0026amp;= O(n) \\end{align*}\nThus, a heap can be constructed in linear time. This is independent on whether the original data is already sorted.\nHeapsort We now have all of the components necessary to implement heapsort. The algorithm is as follows:\ndef heapsort(A): build_max_heap(A) heap_size = len(A) for i in range(len(A) - 1, 0, -1): A[0], A[i] = A[i], A[0] heap_size -= 1 max_heapify(A, 0, heap_size) It starts by building a max-heap on the input array. As seen in the previous section, this is done in linear time. From there, it\u0026rsquo;s a matter of taking the root element out of the heap and then running max_heapify to maintain the max-heap property. This is done \\(n-1\\) times, so the overall running time is \\(O(n \\lg n)\\).\nFigure 3: Heapsort in action (Cormen et al. 2022). Heapsort is visualized in the figure above, starting with a constructed max-heap in (a) (Cormen et al. 2022).\nQuestions What is the running time of heapsort given an array that is already sorted in ascending order? What is the running time of heapsort given an array that is already sorted in descending order? References Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. Introduction to Algorithms. 4th ed. MIT Press. http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/. ","date":1708549080,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708549080,"objectID":"3fcd7f4b2a24bf59baaf949877bc47d4","permalink":"https://ajdillhoff.github.io/notes/heapsort/","publishdate":"2024-02-21T14:58:00-06:00","relpermalink":"/notes/heapsort/","section":"notes","summary":"Table of Contents Maintaining the Heap Property Building the Heap Heapsort Running time is \\(O(n \\lg n)\\). Sorts in place, only a constant number of elements needed in addition to the input. Manages data with a heap. A binary heap can be represented as a binary tree, but is stored as an array. The root is the first element of the array. The left subnode for the element at index \\(i\\) is located at \\(2i\\) and the right subnode is located at \\(2i + 1\\).","tags":["algorithms"],"title":"Heapsort","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents What is it? Naive Parallel Reduction Kogge-Stone Algorithm Brent-Kung Algorithm Adding Coarsening Segmented Parallel Scan Optimizing Memory Efficiency The Takeaway What is it? Parallelizes sequential problems. Works with computations that can be described in terms of a recursion. Used as a primitive operation for sorting, tree operations, and recurrences. Studying this will also reveal how parallelization can increase the complexity beyond that of a traditional sequential approach. Example: Inclusive Scan Given an array of numbers, the inclusive scan computes the sum of all elements up to a given index. For example, given the array [1, 2, 3, 4, 5], the inclusive scan would produce [1, 3, 6, 10, 15]. You could solve this recursively, but it would be horribly inefficient. A sequential solution is achievable with dynamic programming. However, a parallel solution is much more efficient.\nvoid sequential_scan(float *x, float *y, uint N) { y[0] = x[0]; for (uint i = 1; i \u0026lt; N; i++) { y[i] = y[i - 1] + x[i]; } } Naive Parallel Reduction If we have \\(n\\) elements, we could have \\(n\\) threads each compute the sum of a single element. How many operations would that take? The first thread computes the sum of 1 element, or 0 operations. The second thread computes the sum of 2 elements, 1 operation, and so on. This can be described as a sum of the first \\(n\\) natural numbers, which is \\(n(n + 1)/2\\). This parallel solution is worse than the sequential solution, coming in at \\(O(n^2)\\).\nKogge-Stone Algorithm The first solution to this problem relies on GPU Pattern: Reduction and is called the Kogge-Stone algorithm. The algorithm was published in 1973 by Peter M. Kogge and Harold S. Stone during their time at Stanford University.\nAdapting the Reduction Tree Design reduction tree so that each thread has access to relevant inputs. The input matrix is modified to that input \\(A_i\\) contains the sum of up to \\(2^k\\) elements after \\(k\\) iterations. For example, after iteration 2, \\(A_3\\) contains the sum \\(A_0 + A_1 + A_2 + A_3\\).\nFigure 1: Visualization of parallel inclusive scan based on the Kogge-Stone algorithm (Source: NVIDIA DLI). This is implemented in the following code:\n__global__ void Kogge_Stone_scan_kernel(float *X, float *Y, unsigned int N) { __shared__ float A[SECTION_SIZE]; unsigned int i = blockIdx.x * blockDim.x + threadIdx.x; if (i \u0026lt; N) { A[threadIdx.x] = X[i]; } else { A[threadIdx.x] = 0; } for (unsigned int stride = 1; stride \u0026lt; blockDim.x; stride *= 2) { __syncthreads(); float temp; if (threadIdx.x \u0026gt;= stride) { temp = A[threadIdx.x] + A[threadIdx.x - stride]; } __syncthreads(); if (threadIdx.x \u0026gt;= stride) { A[threadIdx.x] = temp; } } if (i \u0026lt; N) { Y[i] = A[threadIdx.x]; } } It is not possible to guarantee that a block can cover the entire input, so SECTION_SIZE is used to ensure that the input is covered. This should be the same as the block size. Each thread starts off by loading its initial input into shared memory. Starting with thread 2, each thread computes a sum of the value assigned to its thread as well as the one before it by a factor of stride.\nThe loop itself is moving down the reduction tree, which is bounded logarithmically. The local variable temp is used to store the intermediate result before barrier synchronization takes place. Otherwise there would be a possibility of a write-after-read race condition.\nDouble Buffering The temporary variable and second call to __syncthreads() are necessary since a thread may read from a location that another thread is writing to. If the input and output arrays were represented by two different areas of shared memory, this call could be removed. This approach is called double-buffering.\nIt works as follows: the input array is read from global memory into shared memory. At each iteration the data read from the first array is used to write new values to the second array. Since the values used in that iteration are only read from the first array, the second array can be used as the input array for the next iteration. This cycle continues back and forth until the final result is written to the output array.\nEfficiency Analysis Work efficiency is a measure of how efficient a particular algorithm is compared to the minimum number of operations required. For inclusive scan, the minimum number of add operations required is \\(n - 1\\), yielding an efficiency of \\(O(n)\\). During each iteration of the Kogge-Stone algorithm, each thread iterates over a loop that is logarithmic in size. That is, it starts with a stride of 2, then 4, 8, \\(\\dots \\frac{n}{2}\\). This yields a complexity of \\(O(n \\log_2 n)\\).\nDue to the parallel nature of the algorithm, it still requires fewer steps than the sequential algorithm. The details muddy the water a bit as threads that stop execution early still expend resources from the device. In general, we can say that the parallel algorithm takes \\(\\frac{1}{m} n \\log_2 n\\) steps, where \\(m\\) is the number of execution units. If we have as many execution units as we have elements, then we only need \\(\\log_2 n\\) steps.\nBrent-Kung Algorithm Sharing intermediate results Distribute to different threads Reduction tree Sub-sums used to calculate some of the scan output values Brent-Kung follows the same idea as Kogge-Stone, but with better work efficiency.\nUse reduction tree on the first \\(n/2\\) elements, then use the results to reverse the tree.\nAt the start of the reverse direction, the elements with index \\(2^i - 1\\), for \\(i = 1, 2, \\dots, \\log_2 n\\), already have the correct value.\nFigure 2: Visualization of parallel inclusive scan based on the Brent-Kung algorithm (Source: NVIDIA DLI). The figure above shows the state of the algorithm before the reverse direction begins. The second half is better seen as a table of values. The row labeled Initial contains the state of the array after the first half of the algorithm is completed. For the values that already have their correct value, no update is needed. The two rows following Initial show the state of the array after the first and second iterations of the reverse direction.\n0 1 2 3 4 5 6 7 Initial 0 0\u0026hellip;1 2 0\u0026hellip;3 4 4\u0026hellip;5 6 0\u0026hellip;7 Iteration 1 0\u0026hellip;5 Iteration 2 0\u0026hellip;2 0\u0026hellip;4 0\u0026hellip;6 Implementing the Forward Half The relevant reduction tree phase of Brent-Kung is implemented in CUDA C++ below.\nfor (uint stride = 1; stride \u0026lt;= blockDim.x; stride *= 2) { __syncthreads(); if ((threadIdx.x + 1) % (stride * 2) == 0) { A[threadIdx.x] += A[threadIdx.x - stride]; } } From the perspective of threadIdx.x = 7, the first iteration would add the value from threadIdx.x = 6 to its own value. On the next iteration, the stride offset would add threadIdx.x = 5. Thread 5 already has the sum from 4 and 5 before being added to 7, so 7 now has the sums from indices 4 through 7. On its last iteration, the value for stride is now 4, and 7 adds the value from 3 to its own value. This is the final result for the first half of the algorithm.\nThere is a lot of control divergence present in this code. Since fewer threads stay active as the loop goes on, it is better to organize the threads such that they are contiguous. We can do that with slightly more complicated indexing, so that contiguous threads use data from the active portions of the array.\nfor (uint stride = 1; stride \u0026lt;= blockDim.x; stride *= 2) { __syncthreads(); int index = (threadIdx.x + 1) * 2 * stride - 1; if (index \u0026lt; blockDim.x) { A[index] += A[index - stride]; } } Thread 0 maps to index, thread 1 maps to index 3, thread 2 to index 5, and so on. Only when the number of active threads drops below the warp size does control divergence become a problem.\nImplementing the Reverse Half The reverse half of the algorithm is implemented in CUDA C++ below.\nfor (uint stride = blockDim.x / 4; stride \u0026gt; 0; stride /= 2) { __syncthreads(); int index = (threadIdx.x + 1) * 2 * stride - 1; if (index + stride \u0026lt; blockDim.x) { A[index + stride] += A[index]; } } Continuing the example from above, the first thread threadIdx.x = 0 maps to index 3. This will load the value from index 3 and add it to the value at index 5. At that point, the value at index 5 will be the sum of the values from indices 0 through 5.\nFigure 3: Full Brent-Kung visualization (source: NVIDIA DLI). Efficiency Analysis The reduction tree of the first half of the algorithm require \\(n - 1\\) operations. For \\(n\\) elements, the reverse half requires \\((2 - 1) + (4 - 1) + \\dots + (n/2 - 1)\\) operations for a total of \\(N - 1 - \\log_2 n\\). This yields a work efficiency of \\(O(n)\\). Even though the theoretical work efficiency is better than Kogge-Stone, doesn\u0026rsquo;t mean its performance will always be better in practice. The drop off in active threads for Brent-Kung is much more severe than Kogge-Stone. It also requires additional steps to perform the reverse half. In general, Kogge-Stone is a better choice when we have more execution units, owing to its better parallelism.\nThe full code is given below.\n__global__ void Brent_Kung_scan_kernel(float *X, float *Y, uint N) { __shared__ float A[SECTION_SIZE]; unsigned int i = 2 * blockIdx.x * blockDim.x + threadIdx.x; if (i \u0026lt; N) { A[threadIdx.x] = X[i]; } if (i + blockDim.x \u0026lt; N) { A[threadIdx.x + blockDim.x] = X[i + blockDim.x]; } for (uint stride = 1; stride \u0026lt;= blockDim.x; stride *= 2) { __syncthreads(); int index = (threadIdx.x + 1) * 2 * stride - 1; if (index \u0026lt; blockDim.x) { A[index] += A[index - stride]; } } for (uint stride = blockDim.x / 4; stride \u0026gt; 0; stride /= 2) { __syncthreads(); int index = (threadIdx.x + 1) * 2 * stride - 1; if (index + stride \u0026lt; blockDim.x) { A[index + stride] += A[index]; } } __syncthreads(); if (i \u0026lt; N) { Y[i] = A[threadIdx.x]; } if (i + blockDim.x \u0026lt; N) { Y[i + blockDim.x] = A[threadIdx.x + blockDim.x]; } } Adding Coarsening Similar to other problems such as tiled matrix multiplication, if the hardware does not meet the capacity ti parallelize the entire problem, the price for parallelization is wasted. In such cases, we can coarsen the problem so that the available resources are fully utilized. Each thread will execute a phase of sequential scan, which is more work efficiency than other of the solutions presented above.\nPhase 1 Such a solution starts off by performing a sequential scan. The threads can also collaborate in the beginning to load data into shared memory.\nPhase 2 In the next phase, the threads execute a parallel scan via Kogge-Stone or Brent-Kung. Since each thread has already performed a sequential scan. This phase starts off with the last element assigned to each thread.\nPhase 3 In the last phase, each thread adds its last value to the first \\(n-1\\) elements of the next section, where \\(n\\) is the number of elements assigned to each thread.\nFigure 4: Three-phase parallel scan (Hwu, Kirk, and El Hajj 2022). Segmented Parallel Scan Input to scan operations may be too large to fit in memory. Hierarchical scans can be used to solve this problem. Partition input so that each section fits into shared memory for an SM. Another kernel launches after this is done to consolidate the results by adding the sum of the preceding scan blocks to each element of a scan block.\nEach scan block is treated as an individual application of one of the previous kernels. Each successive scan block initially does not contain the sums of the preceding blocks. Those will be added in a separate kernel.\nFigure 5: Segmented scan (Hwu, Kirk, and El Hajj 2022). After each scan block has computed the scan for its partition, the last elements are then processed in the next step. However, these elements are all from different blocks, which means we must write them to a global space so they can all be accessed. Completing this step yields an array that has the final values of the scan corresponding to the indices from the original scan blocks (see the figure above). These values can then be used to update the preceding elements in each scan block to complete the scan.\nKernel 1: Section Scan\nThe first kernel essentially implements one of the previously discussed parallel scan kernels. The only difference is that the accumulation array is passed so that the blocks can write their output element values.\n__syncthreads(); if (threadIdx.x == blockDim.x - 1) { accumulation[blockIdx.x] = A[threadIdx.x]; } Kernel 2: Update Scan\nFor step 2, we need to run a parallel scan kernel like Kogge-Stone or Brent-Kung on the accumulation array.\nKernel 3: Update Elements\nThe final kernel takes the accumulated values and updates the original array so that all the elements have their correct scan value.\nuint i = blockIdx.x * blockDim.x + threadIdx.x; output[i] += accumulation[blockIdx.x - 1]; Optimizing Memory Efficiency The previous solution provides a way of working with large inputs, but it is not memory efficient because of the need to store the accumulation array. There have been several attempts at optimizing this flow of information in the form of a stream-based scan. To understand the general strategy behind this approach, consider the segmented scan from the previous section.\nThe process starts by executing all the scan blocks in parallel. The first scan block has all the information it needs to compute the full scan for its partition. However, the second block needs the final output value from the first before it can update its own values. It will have to wait until block 0 has finished and written its value to global memory. It can then add that value to its own elements before sending its final value to the next block. This process continues until all the blocks have been processed.\nTo be clear, the first phase runs completely in parallel, and the second phase is sequential. If the final values are passed quickly enough between each block, the overall scan will still be efficient. Once each block has that passed value, it can continue its work in parallel since it is no longer dependent on the previous block.\nFor this to work, there needs to be a form of synchronization between blocks. The CUDA API does not provide grid-wide synchronization, so how can we accomplish this? One solution is to use a lock to effectively halt a thread until the value is ready to be read (Yan, Long, and Zhang 2013). The code below shows how this can be implemented.\n__shared__ float previous_sum; if (threadIdx.x == 0) { // Wait for previous flag while(atomicAdd(\u0026amp;flags[bid], 0) == 0); // Read previous partial sum previous_sum = scan_value[bid]; // Propagate partial sum scan_value[bid + 1] = previous_sum + local_sum; // Memory fence __threadfence(); // Update flag atomicAdd(\u0026amp;flags[bid + 1], 1); } __syncthreads(); The section ensures that only the first element of each block will perform this update. The flags array is a global array that is used to store the lock values. Once the flag is set to 1, the value from the global array scan_value is read and used to update the local sum. We haven\u0026rsquo;t used __threadfence() before, but it is used to ensure that the update to scan_value[bid + 1] is written before the call to atomicAdd is made.\nYour first thought might be that there are too many global memory accesses. Shouldn\u0026rsquo;t this incur a large access penalty? Remember that many of these are overlapping values that were accessed by previous blocks, so they are most likely in the cache. Of course, if there is ever any doubt on the performance of a kernel, we can always profile it to verify our assumptions.\nAs opposed to the previous solution, this one only requires a single kernel. In the three kernel approach, there is no overlap between the values since each kernel is executed sequentially.\nPreventing Deadlocks This issue isn\u0026rsquo;t production ready just yet. Depending on how the blocks are scheduled, it is possible that a deadlock could occur. For example, the second block could be scheduled before the first block. If there aren\u0026rsquo;t enough SMs on the device then the first block will never be able to write its value to the global array. One solution to this is dynamic block index assignment.\nIn this approach, the block index assignment is not dependent on blockIdx.x. Instead it is assigned dynamically as blocks are processed.\n__shared__ uint bid_s; if (threadIdx.x == 0) { bid_s = atomicAdd(\u0026amp;block_index, 1); } __syncthreads(); uint bid = bid_s; This ensures that each block gets a unique index and that the blocks are processed in the order they are ready.\nThe Takeaway Parallel scan is a powerful tool for solving problems that can be described in terms of a recursion. The Kogge-Stone and Brent-Kung algorithms are two ways of parallelizing the scan operation. This problem presents a unique look at how tradeoffs must be made when parallelizing a problem. At the end of the day, we must work within the constraints of the hardware and framework made available to us.\nReferences Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. Programming Massively Parallel Processors: A Hands-on Approach. Fourth. Morgan Kaufmann. Yan, Shengen, Guoping Long, and Yunquan Zhang. 2013. “StreamScan: Fast Scan Algorithms for GPUs without Global Barrier Synchronization.” In Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, 229–38. PPoPP ’13. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/2442516.2442539. ","date":1707962940,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707962940,"objectID":"373a3eae01ac31864e0598cf6b2e9cda","permalink":"https://ajdillhoff.github.io/notes/gpu_pattern_parallel_scan/","publishdate":"2024-02-14T20:09:00-06:00","relpermalink":"/notes/gpu_pattern_parallel_scan/","section":"notes","summary":"Table of Contents What is it? Naive Parallel Reduction Kogge-Stone Algorithm Brent-Kung Algorithm Adding Coarsening Segmented Parallel Scan Optimizing Memory Efficiency The Takeaway What is it? Parallelizes sequential problems. Works with computations that can be described in terms of a recursion. Used as a primitive operation for sorting, tree operations, and recurrences. Studying this will also reveal how parallelization can increase the complexity beyond that of a traditional sequential approach.","tags":["gpgpu"],"title":"GPU Pattern: Parallel Scan","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Reduction Trees A Simple Kernel Minimizing Control Divergence Memory Divergence of Reduction Reducing the number of global memory requests Hierarchical Reduction Thread Coarsening - Back Again The following notes follow Chapter 10 of Programming Massively Parallel Processors (Hwu, Kirk, and El Hajj 2022).\nIntroduction Given a set of values, a reduction produces a single output. It is an important part of many parallel algorithms including MapReduce. Other patterns that we have studied can also be viewed as reductions, such as GPU Pattern: Parallel Histogram. Implementing this parallel pattern requires careful consideration of thread communication, and will be the focus of these notes.\nMany of the operations you rely on are examples of reductions. For example, the `sum` function is a reduction, as is the `max` function. A reduction can be viewed as a linear combination of the input values, or transformed values, and is often used to compute a summary statistic. If \\(\\phi(\\cdot)\\) is a binary operator, then a reduction computes the following:\n\\[ v = \\phi(v, x_i)\\ \\text{for}\\ i = 1, 2, \\ldots, n, \\]\nwhere \\(v\\) is the accumulated value and \\(x_i\\) are the input values. The operator \\(\\phi(\\cdot)\\) can be any associative and commutative operation, such as addition or multiplication. Each operator has a corresponding identity element, such as 0 for addition or 1 for multiplication. The identity element is used to initialize the reduction and can be represented as \\(v = v_0\\) in the equation above.\nReduction Trees Reductions of any kind are well represented using trees. The first level of reduction maximizes the amount of parallelism. As the input is gradually reduced, fewer threads are needed.\nFigure 1: Sum reduce as a reduction tree. In order to implement a parallel reduction, the chosen operator must be associative. For example, \\(a + (b + c) = (a + b) + c\\). The operator must also be commutative, such that \\(a + b = b + a\\).\nReduction trees reveal the logarithmic nature of parallel reductions. Just like divide and conquer algorithms, the number of threads is halved at each level of the tree. The number of levels in the tree is \\(\\log_2(n)\\), where \\(n\\) is the number of input values. Given an input size of \\(n = 1024\\), the number of threads required is \\(\\log_2(1024) = 10\\). This is a significant reduction from the original input size. The sequential version of this reduction would require 1023 operations.\nA Simple Kernel As mentioned above, reduction requires communication between threads. Since only the threads within a single block can communicate, we will focus on a block-level reduction. For now, each block can work with a total of 2048 input values based on the limitation of 1024 threads per block.\n__global__ void sumReduceKernel(float *input, float *output) { unsigned int i = 2 * threadIdx.x; for (unsigned int stride = 1; stride \u0026lt;= blockDim.x; stride *= 2) { // Only threads in even positions participate if (threadIdx.x % stride == 0) { input[i] += input[i + stride]; } __syncthreads(); } if (threadIdx.x == 0) { *output = input[0]; } } Each thread is assigned to a single write location 2 * threadIdx.x. The stride is doubled after each iteration of the loop, effectively halving the number of active threads. The stride also determines the second value that is added to the first. By the last iteration, only one thread is active to perform that last reduction.\nFigure 2: Execution of kernel reduction (Source: NVIDIA DLI). You can see that the kernel is simple, but it is also inefficient. There is a great deal of control divergence that will be addressed in the next section.\nMinimizing Control Divergence As we just saw, the key to optimizing a reduction kernel is to minimize control divergence and make sure as many threads stay active as possible. A warp of 32 threads would consume the execution resources even if half of them are inactive. As each stage of the reduction tree is completed, the amount of wasted resources increases. Depending on the input size, entire warps could be launched and then immediately become inactive.\nThe number of execution resources consumes is proportional to the number of active warps across all iterations. We can compute the number of resources consumed as follows:\n\\[ (\\frac{5N}{64} + \\frac{N}{128} + \\frac{N}{256} + \\cdots + 1) * 32 \\]\nwhere \\(N\\) is the number of input values. Each thread operates on 2 values, so \\(\\frac{N}{2}\\) are launched in total. Since every warp has 32 threads, a total of \\(\\frac{N}{64}\\) warps are launched. For the first 5 iterations, all warps will be active. The 5th iteration only has 1 active thread in each warp. On the 6th iteration, the number of active warps is halved, and so on.\nFor an input of size \\(N = 1024\\), the number of resources consumed is \\((80 + 8 + 4 + 2 + 1) * 32 = 3040\\). The total number of results committed by the active threads is equal to the number of operations performed, which is \\(N - 1 = 1023\\). The efficiency of the kernel is then \\(\\frac{1023}{3040} = 0.34\\). Only around 34% of the resources are used to perform the reduction.\nRearranging the Threads A simple rearrangement of where the active results are stored can improve the efficiency of the kernel by reducing control divergence. The idea is to keep the threads that own the results of the reduction close together. Instead of increasing the stride, it should be decreased. The figure below shows the rearrangement of the threads.\nFigure 3: Optimized reduction kernel execution (Source: NVIDIA DLI). __global__ void sumReduceKernel(float *input, float *output) { unsigned int i = threadIdx.x; for (unsigned int stride = blockDim.x; stride \u0026gt;= 1; stride /= 2) { if (i \u0026lt; stride) { input[i] += input[i + stride]; } __syncthreads(); } if (threadIdx.x == 0) { *output = input[0]; } } The kernel itself is effectively the same, but the rearrangement of the threads ensures that each warp has less control divergence. Additionally, warps that drop off after each iteration are no longer consuming execution resources. For an input of 256, the first 4 warps are fully utilized (barring the last thread of the last warp). After the first iteration, the number of active warps is halved. Warps 3 and 4 are now fully inactive, leaving warps 1 and 2 to perform the reduction operation on all threads. We can compute the number of resources consumed under this new arrangement as follows:\n\\[ (\\frac{N}{64} + \\frac{N}{128} + \\frac{N}{256} + \\cdots + 1 + 5) * 32 \\]\nAt each iteration, half of warps become inactive and no longer consume resources. The last warp will consume execution resources for all 32 threads, even though the number of active threads is less than 32. For our input of size \\(N = 1024\\), the number of resources consumed is \\((16 + 8 + 4 + 2 + 1 + 5) * 32 = 1152\\), resulting in an efficiency of \\(\\frac{1023}{1152} = 0.89\\). This is a significant improvement over the original kernel. This will increase based on the input size.\nMemory Divergence of Reduction Does this kernel take advantage of memory coalescing? Each thread reads and writes from and to its assigned location. It also makes a read from a location that is a stride away. These locations are certainly not adjacent and will not be coalesced.\nAdjacent threads do not access adjacent locations. The warp itself is unable to coalesce the thread requests into a single global memory request. Each data element is 4 bytes. Since each of the 32 threads in a warp are accessing their assigned locations with a separation of stride, the 64 * 4 bytes will require two 128 byte memory requests to access the data. With each iteration, the assigned locations will always be separated such that two 128 byte memory requests will need to be made. Only on the last iteration, where only a single thread accesses a single assigned location, will a single memory request be made.\nThe convergent kernel from the last section takes advantage of memory coalescing, leading to fewer memory requests.\nReducing the number of global memory requests As we saw with tiling in GPU Performance Basics, we can reduce the number of global memory requests by using shared memory. Threads write their results to global memory, which is read again in the next iteration. By keeping the intermediate results in shared memory, we can reduce the number of global memory requests. If implemented correctly, only the original input values will need to be read from global memory.\n__global__ void sumReduceSharedKernel(float *input, float *output) { __shared__ float input_s[BLOCK_DIM]; unsigned int i = threadIdx.x; input_s[i] = input[i] + input[i + BLOCK_DIM]; for (unsigned int stride = blockDim.x / 2; stride \u0026gt;= 1; stride /= 2) { __syncthreads(); if (i \u0026lt; stride) { input_s[i] += input_s[i + stride]; } } if (i == 0) { *output = input_s[0]; } } At the very top of this kernel, the necessary input is loaded from global memory, added, and written to shared memory. This is the only time global memory is accessed, with the exception of the final write to the output. The call to syncthreads() moves to the top so that the shared memory is guaranteed before the next update.\nThis approach not only requires fewer global memory requests, but the original input is left unmodified.\nHierarchical Reduction One major assumption that has been made in each of these kernels is that they are running on a single block. Thread synchronization is critical to the success of the reduction. If we want to reduce a larger number of input across multiple blocks, the kernel should allow for independent execution. This is achieved by segmenting the input and performing a reduction on each segment. The final reduction is then performed on the results of the segment reductions.\n__global__ void sumReduceHierarchicalKernel(float *input, float *output) { __shared__ float input_s[BLOCK_DIM]; unsigned int segment = 2 * blockDim.x * blockIdx.x; unsigned int i = segment + threadIdx.x; unsigned int t = threadIdx.x; input_s[t] = input[i] + input[i + BLOCK_DIM]; for (unsigned int stride = blockDim.x / 2; stride \u0026gt;= 1; stride /= 2) { __syncthreads(); if (t \u0026lt; stride) { input_s[t] += input_s[t + stride]; } } if (t == 0) { atomicAdd(output, input_s[0]); } } Each block has its own shared memory and can independently perform the reduction. Depending on the completion order, an atomic operation to add the local result is necessary.\nThread Coarsening - Back Again Thread coarsening was first analyzed in the context of matrix multiplication in GPU Performance Basics. Whenever the device does not have enough resources to execute the number of threads requested, it is forced to serialize the execution. In this case, we can serialize the work done by each thread so that no extra overhead is incurred. Another benefit to thread coarsening is improved data locality.\nSuccessive iterations increase the amount of inactive warps. For reduction, thread coarsening can be applied by increasing the number of elements that each one processes. If the time to perform the arithmetic is much faster than the time to load the data, then thread coarsening can be beneficial. We could further analyze our program to determine the optimal coarsening factor.\n__global__ coarsenedSumReductionKernel(float *input, float *output) { __shared__ float input_s[BLOCK_DIM]; uint segment = COARSE_FACTOR * 2 * blockDim.x * blockIdx.x; uint i = segment + threadIdx.x; uint t = threadIdx.x; float sum = input[i]; for (uint tile = 1; tile \u0026lt; COARSE_FACTOR * 2; tile++) { sum += input[i + tile * BLOCK_DIM]; } input_s[t] = sum; for (uint stride = blockDim.x / 2; stride \u0026gt;= 1; stride /= 2) { __syncthreads(); if (t \u0026lt; stride) { input_s[t] += input_s[t + stride]; } } if (t == 0) { atomicAdd(output, input_s[0]); } } In the coarsened version, less thread communication is required since the first several steps are computed in a single thread.\n","date":1707169620,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707169620,"objectID":"ed82ec5f24f5a860e022481b37240ae2","permalink":"https://ajdillhoff.github.io/notes/gpu_pattern_reduction/","publishdate":"2024-02-05T15:47:00-06:00","relpermalink":"/notes/gpu_pattern_reduction/","section":"notes","summary":"Table of Contents Introduction Reduction Trees A Simple Kernel Minimizing Control Divergence Memory Divergence of Reduction Reducing the number of global memory requests Hierarchical Reduction Thread Coarsening - Back Again The following notes follow Chapter 10 of Programming Massively Parallel Processors (Hwu, Kirk, and El Hajj 2022).\nIntroduction Given a set of values, a reduction produces a single output. It is an important part of many parallel algorithms including MapReduce.","tags":["gpgpu"],"title":"GPU Pattern: Reduction","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Bag of Visual Words Bag of Words is a technique used in Natural Language Processing for document classification. It is a collection of word counts. To create a Bag of Words for a document, it necessary to create a dictionary first. Choosing the a dictionary is based on many factors including computational limitations. Next, the documents in a dataset are tokenized into words. The word counts are collected as part of a histogram and used as a feature vector for a machine learning model.\nThe dictionary is the same for all documents in the original dataset. Ideally, the Bag of Word vectors for each document in the same class will be similar. This technique works well for problems in natural language processing, where each input document will have a varying number of words. By using a Bag of Words, the input data is transformed into a fixed length feature vector.\nBag of Visual Words Figure 1: An image and its visual \u0026ldquo;words\u0026rdquo; (Li et al.) The Bag of Visual Words model adapts this technique to computer vision. Instead of words, distinct visual features are extracted from each image. Some images may have more features than others, similar to how some documents will have different word counts. The dictionary is created by clustering the visual features into a finite number of groups, determined as a hyperparameter. The visual features for each image are then counted and used as a feature vector for a machine learning model.\nExtract Visual Features The first step in creating a Bag of Visual Words is to extract visual features from each image. The visual features are typically extracted using a technique like SIFT, SURF, or ORB. These techniques are designed to extract features that are invariant to scaling, rotation, and translation. The visual features are then stored in a list for each image.\nCreate Visual Words Creating the dictionary requires clustering the features into a finite number of groups. The number of groups will vary depending on the complexity of the data. For a given dataset, this can be determined empirically. The most common clustering algorithm for this is K-Means, in which \\(k\\) different clusters are created and updated iteratively. The visual features are then assigned to the nearest cluster, and the cluster centers are updated. This process is repeated until the cluster centers converge.\nBuild Sparse Frequency Vectors The next step is to create a histogram of the visual features for each image. The histogram is a sparse vector, where each element represents the count of a visual feature in the image. The histogram is then normalized to create a feature vector. Given an input image, the feature vector is extracted and assigned a label based on the cluster model. That label is one of the \\(n\\) chosen words in the vocabulary, which is incremented in the histogram.\nFigure 2: Histogram creates from visual words (Li et al.) Adjust Frequency Vectors The feature vectors are then adjusted to account for the frequency of the visual features. This is done by applying a weighting scheme to them. The most common weighting scheme is called Term Frequency-Inverse Document Frequency (TF-IDF). TF-IDF scheme adjusts the frequency of a word in a document based on the frequency in the entire dataset. It is calculated as follows:\n\\[ \\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t), \\]\nwhere \\(\\text{TF}(t, d)\\) is the term frequency of term \\(t\\) in document \\(d\\) and \\(\\text{IDF}(t)\\) is the inverse document frequency of term \\(t\\) in the entire dataset.\n\\(\\text{TF}(t, d)\\) is simply the number of times that visual feature \\(t\\) appears in the image \\(d\\). \\(\\text{IDF}(t)\\) is calculated as follows:\n\\[ \\text{IDF}(t) = \\log\\left(\\frac{N}{n_t}\\right), \\]\nwhere \\(N\\) is the total number of images in the dataset and \\(n_t\\) is the number of images that contain the visual feature \\(t\\).\nCompare Vectors The last step is to compare the feature vectors in service of some downstream task like classification. Since every feature vector is a fixed length, they can be used as input to a machine learning model.\n","date":1707094440,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707094440,"objectID":"ee648a2368c789c127b7b3e173877a3a","permalink":"https://ajdillhoff.github.io/notes/bag_of_visual_words/","publishdate":"2024-02-04T18:54:00-06:00","relpermalink":"/notes/bag_of_visual_words/","section":"notes","summary":"Table of Contents Bag of Visual Words Bag of Words is a technique used in Natural Language Processing for document classification. It is a collection of word counts. To create a Bag of Words for a document, it necessary to create a dictionary first. Choosing the a dictionary is based on many factors including computational limitations. Next, the documents in a dataset are tokenized into words. The word counts are collected as part of a histogram and used as a feature vector for a machine learning model.","tags":["computer vision","machine learning"],"title":"Bag of Visual Words","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Example: Merge Sort Example: Matrix Multiplication Example: Median Finding Example: Cormen et al. Exercise 4.5-2 In the study of Divide and Conquer Algorithms, a recurrence tree can be used to determine the runtime complexity. These notes focus on the master theorem, a blueprint for solving any recurrence of the form\n\\[ T(n) = aT(n/b) + f(n). \\]\n\\(n\\) is the size of the problem, \\(a \\geq 1\\) is the number of subproblems, \\(b \u0026gt; 1\\) is the factor by which the problem size is reduced, and \\(f(n)\\) is the cost of the work done outside of the recursive calls. Each recurrence is solved in \\(T(n/b)\\) time, and \\(f(n)\\) would include the cost of dividing and recombining the problem. The full theorem as described in Introduction to Algorithms is restated below (Cormen et al. 2022).\nMaster Theorem\nLet \\(a \u0026gt; 0\\) and \\(b \u0026gt; 1\\) be constants, and let \\(f(n)\\) be a driving function that is defined and nonnegative on all sufficiently large reals. Define the recurrence \\(T(n)\\) on \\(n \\in \\mathbb{N}\\) by\n\\[ T(n) = aT(n/b) + f(n), \\]\nwhere \\(aT(n/b)\\) actually means \\(a\u0026rsquo;T(\\lfloor n/b \\rfloor) + a{\u0026rsquo;\u0026rsquo;}T(\\lceil n / b \\rceil)\\) for some constants \\(a\u0026rsquo; \\geq 0\\) and \\(a\u0026rsquo;\u0026rsquo; \\geq 0\\) such that \\(a = a\u0026rsquo; + a\u0026rsquo;\u0026rsquo;\\). Then \\(T(n)\\) has the following asymptotic bounds:\nIf \\(f(n) = O(n^{\\log_b a - \\epsilon})\\) for some constant \\(\\epsilon \u0026gt; 0\\), then \\(T(n) = \\Theta(n^{\\log_b a})\\). If \\(f(n) = \\Theta(n^{\\log_b a} \\log^k n)\\) for some constant \\(k \\geq 0\\), then \\(T(n) = \\Theta(n^{\\log_b a} \\log^{k+1} n)\\). If \\(f(n) = \\Omega(n^{\\log_b a + \\epsilon})\\) for some constant \\(\\epsilon \u0026gt; 0\\), and if \\(a f(n/b) \\leq k f(n)\\) for some constant \\(k \u0026lt; 1\\) and all sufficiently large \\(n\\), then \\(T(n) = \\Theta(f(n))\\). Theorem Breakdown The common function \\(n^{\\log_b a}\\) is called the watershed function. The driving function \\(f(n)\\) is compared to it to determine which case applies. If the watershed function grows at a faster rate than \\(f(n)\\), case 1 applies. If they grow at the same rate, case 2 applies. If \\(f(n)\\) grows at a faster rate, case 3 applies.\nIn case 1, the watershed function should grow faster than \\(f(n)\\) by a factor of \\(n^\\epsilon\\) for some \\(\\epsilon \u0026gt; 0\\). In case 2, technically the watershed function should grow at least the same rate as \\(f(n)\\), if not faster. That is, it grows faster by a factor of \\(\\Theta(\\log^k n)\\), where \\(k \\geq 0\\). You can think of the extra \\(\\log^k n\\) as an augmentation to the watershed function to ensure that they grow at the same rate. In most cases, \\(k = 0\\) which results in \\(T(n) = \\Theta(n^{\\log_b a} \\log n)\\).\nSince case 2 allows for the watershed function to grow faster than \\(f(n)\\), case 3 requires that it grow at least polynomially faster. \\(f(n)\\) should grow faster by at least a factor of \\(\\Theta(n^\\epsilon)\\) for some \\(\\epsilon \u0026gt; 0\\). Additionally, the driving function must satisfy the regularity condition \\(a f(n/b) \\leq k f(n)\\) for some constant \\(k \u0026lt; 1\\) and all sufficiently large \\(n\\). This condition ensures that the cost of the work done outside of the recursive calls is not too large.\nApplication of the Master Method In most cases, the master method can be applied by looking at the recurrence and applying the relevant case. If the driving and watershed functions are not immediately obvious, you can use a different method as discussed in Divide and Conquer Algorithms.\nWhen this can be applied, it is much simpler than the other methods. Let\u0026rsquo;s revisit some of the main problems we\u0026rsquo;ve explored before discussing applications for which the master method could not be used.\nExample: Merge Sort Merge Sort has a recurrence of the form \\(T(n) = 2T(n/2) + \\Theta(n)\\). The driving function is \\(f(n) = \\Theta(n)\\). The constants \\(a\\) and \\(b\\) are both 2, so the watershed function is \\(n^{\\log^2 2}\\), which is \\(n\\). Since \\(f(n)\\) grows at the same rate as the watershed function, case 2 applies. Therefore, \\(T(n) = \\Theta(n \\log n)\\).\nExample: Matrix Multiplication The recurrence of the divide and conquer version of matrix multiplication for square matrices is \\(T(n) = 8T(n/2) + \\Theta(1)\\). Given \\(a = 8\\) and \\(b = 2\\), we can see that the complexity is inherent in the recurrence, not the driving function. The watershed function is \\(n^{\\log_2 8}\\), which is \\(n^3\\). This grows at a faster rate than \\(\\Theta(1)\\), so case 1 applies. Therefore, \\(T(n) = \\Theta(n^3)\\).\nExample: Median Finding Median finding has a recurrence of the form \\(T(n) = T(n/5) + T(7n/10) + \\Theta(n)\\). Given the two recurrence factors, how do we evaluate the driving function? The form itself does not fit the master theorem, so it cannot be applied in this case. We could use the substitution method, recurrence trees, or the Akra-Bazzi theorem to solve this one.\nExample: Cormen et al. Exercise 4.5-2 In this exercise from Introduction to Algorithms, we are asked to find the largest integer value \\(a\\) such that an algorithm with the recurrence \\(T(n) = aT(n/4) + \\Theta(n^2)\\) is asymptotically faster than \\(\\Theta(n^{\\log_2 7})\\). Since \\(b = 4\\), the largest integer \\(a\\) will be the smallest integer such that \\(\\log_4 a \u0026lt; \\log_2 7\\). Solving for the inequality shows that \\(a = 48\\) is the largest such integer.\nReferences Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. Introduction to Algorithms. 4th ed. MIT Press. http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/. ","date":1707090540,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707090540,"objectID":"1867ec906c5cd1bea218507aa6a80aa1","permalink":"https://ajdillhoff.github.io/notes/master_theorem/","publishdate":"2024-02-04T17:49:00-06:00","relpermalink":"/notes/master_theorem/","section":"notes","summary":"Table of Contents Example: Merge Sort Example: Matrix Multiplication Example: Median Finding Example: Cormen et al. Exercise 4.5-2 In the study of Divide and Conquer Algorithms, a recurrence tree can be used to determine the runtime complexity. These notes focus on the master theorem, a blueprint for solving any recurrence of the form\n\\[ T(n) = aT(n/b) + f(n). \\]\n\\(n\\) is the size of the problem, \\(a \\geq 1\\) is the number of subproblems, \\(b \u0026gt; 1\\) is the factor by which the problem size is reduced, and \\(f(n)\\) is the cost of the work done outside of the recursive calls.","tags":["computer science","algorithms"],"title":"Master Theorem","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Histograms Latency of Atomic Operations Privatization Coarsening Aggregation The Takeaway These notes follow the presentation of the parallel histogram pattern in the book Programming Massively Parallel Processors: A Hands-on Approach (Hwu, Kirk, and El Hajj 2022).\nHistograms Examples of histograms include: Frequency of words in a document Distribution of pixel intensities in an image Distribution of particle energies in a physics simulation Distribution of thread block execution times in a GPU kernel Consider the program below which computes a histogram of the letters in a string. The input is assumed to be lower case. Since this is executed sequentially, there is no risk of multiple threads writing to the same memory location at the same time.\nvoid histogram_sequential(char *data, unsigned int length, unsigned int *hist) { for (unsigned int i = 0; i \u0026lt; length; i++) { hist[data[i] - \u0026#39;a\u0026#39;]++; } } To parallelize, we could launch a kernel which has each thread work with on character from the input. This presents a major problem when updating the histogram, as multiple threads may try and increment the same location simultaneously. This is called output interference.\nRace Conditions For example, thread 1 and thread 2 may have the letter \u0026lsquo;a\u0026rsquo; as their input. They will both issue a read-modify-write procedure where the current value of the histogram is read, incremented, and then written back to memory. If thread 1 reads the value of the histogram before thread 2 writes to it, the value that thread 1 writes back will be incorrect. This is a classic example of a race condition. Depending on the timing, one thread could have read the updated value from the other thread, or both threads could have read the same value and incremented it, resulting in a loss of data.\nAtomic Operations One solution to this problem is to perform atomic operations. This is a special type of operation that locks a memory location while it is being updated. This prevents other threads from reading or writing to the same location until the operation is complete. Each thread attempting to access a memory location will be forced to wait until the lock is released.\nThe CUDA API provides several atomic operations:\natomicAdd atomicSub atomicExch atomicMin atomicMax atomicInc atomicDec atomicCAS These are all intrinsic functions, meaning they are processed in a special way by the compiler. Instead of acting like a function call that comes with the typical overhead from the stack, these are implemented as inline machine instructions. The CUDA kernel below uses atomicAdd to increment the histogram.\n__global__ void histogram_atomic(char *data, unsigned int length, unsigned int *hist) { int i = blockIdx.x * blockDim.x + threadIdx.x; if (i \u0026lt; length) { atomicAdd(\u0026amp;hist[data[i] - \u0026#39;a\u0026#39;], 1); } } Latency of Atomic Operations Atomic operations prevent the hardware from maximizing DRAM bursts since they serialize memory accesses. This can lead to a significant performance penalty. For each atomic operation, there are two delays: the delay from loading an element and the delay from storing the updated values.\nNot all threads will be loading and storing to the same memory location; this is dependent on the number of bins used in the histogram. If the number of bins is small, the performance penalty will be greater. This analysis is further complicated based on the distribution of the data.\nAtomic operations can be performed on the last level of cache. If the value is not in the cache, it will be brought into cache for future accesses. Although this does provide a performance benefit, it is not enough to offset the performance penalty of atomic operations.\nPrivatization If much of the traffic is concentrated on a single area, the solution involves directing the traffic away in some manner. This is what privatization does. Since the bottleneck is the data load and store, privatization gives each thread its own private store so that it can update without contention. Of course, each copy must be combined in some way at the end. The cost of merging these copies is much less than the cost of the atomic operations. In practice, privatization is done for groups of threads, not individual ones.\nThe example above can be privatized by making a copy of the histogram for each thread block. The level of contention is much lower since only a single block will update their own private copy. All copies of the histogram are allocated as one monolithic array. Each individual block can use its local indices to offset the pointer. Private copies of values will likely still be cached in L2, so the cost of merging the copies is minimal.\nFor example, if we have 256 threads per block and 26 bins, we can allocate a \\(26 \\times 256\\) array of integers. Each thread block will have its own copy of the histogram. The kernel below demonstrates this.\n#define NUM_BINS 26 __global__ void histogram_privatized(char *data, unsigned int length, unsigned int *hist) { int i = blockIdx.x * blockDim.x + threadIdx.x; if (i \u0026lt; length) { int pos = data[i] - \u0026#39;a\u0026#39;; atomicAdd(\u0026amp;hist[blockIdx.x * NUM_BINS + pos], 1); } __syncthreads(); if (blockIdx.x \u0026gt; 0) { for (unsigned int bin = threadIdx.x; bin \u0026lt; NUM_BINS; bin += blockDim.x) { unsigned int binValue = hist[blockIdx.x * NUM_BINS + bin]; if (binValue \u0026gt; 0) { atomicAdd(\u0026amp;hist[bin], binValue); } } } } Each block is free to use its section of the histogram without contention. After all blocks have finished, the histogram is merged by summing the values of each bin across all blocks. This is done by having each block add its values to the global histogram. The __syncthreads function is used to ensure that all blocks have finished updating their private copies before the merge begins.\nEach block after index 0 will add its values to the global histogram, represented by the first block. Only a single thread per block will be accessing each bin, so the only contention is with other blocks. If the bins are small enough, shared memory can be used to store the private copies. Even though an atomic operation is still required, the latency for loading and storing is reduced by an order of magnitude. The shared kernel below demonstrates this.\n#define NUM_BINS 26 __global__ void histogram_privatized(char *data, unsigned int length, unsigned int *hist) { __shared__ unsigned int hist_s[NUM_BINS]; for (unsigned int bin = threadIdx.x; bin \u0026lt; NUM_BINS; bin += blockDim.x) { hist_s[bin] = 0; } __syncthreads(); int i = blockIdx.x * blockDim.x + threadIdx.x; if (i \u0026lt; length) { int pos = data[i] - \u0026#39;a\u0026#39;; atomicAdd(\u0026amp;hist_s[pos], 1); } __syncthreads(); if (blockIdx.x \u0026gt; 0) { for (unsigned int bin = threadIdx.x; bin \u0026lt; NUM_BINS; bin += blockDim.x) { unsigned int binValue = hist_s[bin]; if (binValue \u0026gt; 0) { atomicAdd(\u0026amp;hist[bin], binValue); } } } } Coarsening The bottleneck of using privatization moves from DRAM access to merging copies back to the public copy of the data. This scales up based on the number of blocks used, since each thread in a block will be sharing a bin in the worst case. If the problem exceeds the capacity of the hardware, the scheduler will serialize the blocks. If they are serialized anyway, then the cost of privatization is not worth it.\nCoarsening will reduce the overhead of privatization by reducing the number of private copies that are committed to the public one. Each thread will process multiple elements.\nContiguous Partitioning Figure 1: Contiguous partitioning. Recreated from (Hwu, Kirk, and El Hajj 2022). Each thread is assigned a contiguous range of elements to process. The kernel is a straightforward extension of the privatized kernel. This approach works better on a CPU, where there are only a small number of threads. This is due to the caching behavior of the CPU. With so many threads on a GPU, it is less likely that the data will be in the cache since so many threads are competing.\n#define NUM_BINS 26 __global__ void histogram_privatized_cc(char *data, unsigned int length, unsigned int *hist) { __shared__ unsigned int hist_s[NUM_BINS]; for (unsigned int bin = threadIdx.x; bin \u0026lt; NUM_BINS; bin += blockDim.x) { hist_s[bin] = 0; } __syncthreads(); int i = blockIdx.x * blockDim.x + threadIdx.x; if (i \u0026lt; length) { int pos = data[i] - \u0026#39;a\u0026#39;; atomicAdd(\u0026amp;hist_s[pos], 1); } __syncthreads(); if (blockIdx.x \u0026gt; 0) { for (unsigned int bin = threadIdx.x; bin \u0026lt; NUM_BINS; bin += blockDim.x) { unsigned int binValue = hist_s[bin]; if (binValue \u0026gt; 0) { atomicAdd(\u0026amp;hist[bin], binValue); } } } } Interleaved Partitioning Figure 2: Interleaved partitioning. Recreated from (Hwu, Kirk, and El Hajj 2022). Contiguous partitioning allowed for contiguous access to values relative to each thread. However, the memory was not contiguous with respect to other threads. In terms of DRAM accesses, each individual read from memory was too far apart to take advantage of coalescing. With interleaved partitioning, the memory can be accessed in a single DRAM access since the memory is coalesced.\n#define NUM_BINS 26 __global__ void histogram_privatized_ic(char *data, unsigned int length, unsigned int *hist) { __shared__ unsigned int hist_s[NUM_BINS]; for (unsigned int bin = threadIdx.x; bin \u0026lt; NUM_BINS; bin += blockDim.x) { hist_s[bin] = 0; } __syncthreads(); int tid = blockIdx.x * blockDim.x + threadIdx.x; for (int i = tid; i \u0026lt; length; i += blockDim.x * gridDim.x) { int pos = data[i] - \u0026#39;a\u0026#39;; if (pos \u0026gt;= 0 \u0026amp;\u0026amp; pos \u0026lt; 26) { atomicAdd(\u0026amp;hist_s[pos], 1); } } __syncthreads(); if (blockIdx.x \u0026gt; 0) { for (unsigned int bin = threadIdx.x; bin \u0026lt; NUM_BINS; bin += blockDim.x) { unsigned int binValue = hist_s[bin]; if (binValue \u0026gt; 0) { atomicAdd(\u0026amp;hist[bin], binValue); } } } } In the code above, the main difference is the second for loop. The index i is incremented by blockDim.x * gridDim.x. This ensures that the threads of each block access memory in a contiguous manner rather than each thread being contiguous. The differences are visualized in the figures.\nAggregation It is not uncommon that the input data will have a skewed distribution. There may be sections of the input that are locally dense. This will lead to a large number of atomic operations within a small area. To reduce the number of atomic operations, the input can be aggregated into a larger update before being committed to the global histogram. Consider the code below.\n__global__ void histogram_aggregate(char *data, unsigned int length, unsigned int *histo) { // Initialize shared memory __shared__ unsigned int hist_s[NUM_BINS]; for (unsigned int bin = threadIdx.x; bin \u0026lt; NUM_BINS; bin += blockDim.x) { hist_s[bin] = 0; } __syncthreads(); // Build histogram unsigned int accumulator = 0; int prevBinIdx = -1; unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x; for (unsigned int i = tid; i a, length; i += blockDim.x * gridDim.x) { int binIdx = data[i] - \u0026#39;a\u0026#39;; if (binIdx == prevBinIdx) { accumulator++; } else { if (prevBinIdx \u0026gt;= 0) { atomicAdd(\u0026amp;hist_s[prevBinIdx], accumulator); } accumulator = 1; prevBinIdx = binIdx; } } if (accumulator \u0026gt; 0) { atomicAdd(\u0026amp;hist_s[prevBinIdx], accumulator); } __syncthreads(); // Commit to global memory for (unsigned int bin = threadIdx.x; bin \u0026lt; NUM_BINS; bin += blockDim.x) { unsigned int binValue = hist_s[bin]; if (binValue \u0026gt; 0) { atomicAdd(\u0026amp;histo[bin], binValue); } } } The difference in this kernel is the histogram loop in the middle. The previous bin index is tracked to determine if contiguous values would be aggregated. As long as the values are the same, the accumulator is increased. As soon as a new value is encountered, a batch update is performed. This reduces the number of atomic operations by a factor of the number of contiguous values.\nIf the data is relatively uniform, the cost of aggregation exceeds the simple kernel. If you are working with images, spatially local data will usually be aggregated. This kernel would be beneficial in that case. Another downside to the aggregated kernel is that it requires more registers and has an increased chance for control divergence. As with all implementations, you should profile this against your use case.\nThe Takeaway Computing histograms is a common operation in fields such as image processing, natural language processing, and physics simulations. For example, a core preprocessing step for training a large language model is to compute the frequency of words in a corpus. This is a perfect example of a task that can be parallelized on a GPU.\nReferences Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. Programming Massively Parallel Processors: A Hands-on Approach. Fourth. Morgan Kaufmann. ","date":1706570520,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706570520,"objectID":"3bc1443fa88238dab294bc3a62f59f5b","permalink":"https://ajdillhoff.github.io/notes/gpu_pattern_parallel_histogram/","publishdate":"2024-01-29T17:22:00-06:00","relpermalink":"/notes/gpu_pattern_parallel_histogram/","section":"notes","summary":"Table of Contents Histograms Latency of Atomic Operations Privatization Coarsening Aggregation The Takeaway These notes follow the presentation of the parallel histogram pattern in the book Programming Massively Parallel Processors: A Hands-on Approach (Hwu, Kirk, and El Hajj 2022).\nHistograms Examples of histograms include: Frequency of words in a document Distribution of pixel intensities in an image Distribution of particle energies in a physics simulation Distribution of thread block execution times in a GPU kernel Consider the program below which computes a histogram of the letters in a string.","tags":["gpgpu"],"title":"GPU Pattern: Parallel Histogram","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Definition Solving Recurrences Example: Merge Sort Example: Multiplying Square Matrices Example: Convex Hull Example: Median Search Definition Divide and conquer algorithms are a class of algorithms that solve a problem by breaking it into smaller subproblems, solving the subproblems recursively, and then combining the solutions to the subproblems to form a solution to the original problem. Problems that can be solved in this manner are typically highly parallelizable. These notes investigate a few examples of classic divide and conquer algorithms and their analysis.\nA divide and conquer method is split into three steps:\nDivide the problem into smaller subproblems. Conquer the subproblems by solving them recursively. Combine the solutions to the subproblems to form a solution to the original problem. Their runtime can be characterized by the recurrence relation \\(T(n)\\). A recurrence \\(T(n)\\) is algorithmic if, for every sufficiently large threshold constant \\(n_0 \u0026gt; 0\\), the following two properties hold:\nFor all \\(n \\leq n_0\\), the recurrence defines the running time of a constant-size input. For all \\(n \\geq n_0\\), every path of recursion terminates in a defined base case within a finite number of recursive calls. The algorithm must output a solution in finite time. If the second property doesn\u0026rsquo;t hold, the algorithm is not correct \u0026ndash; it may end up in an infinite loop.\n\u0026ldquo;Whenever a recurrence is stated without an explicit base case, we assume that the recurrence is algorithmic.\u0026rdquo; (Cormen et al. 2022).\nThis assumption means that the algorithm is correct and terminates in finite time, so there must be a base case. The base case is less important for analysis than the recursive case. For example, your base case might work with 100 elements, and that would still be \\(\\Theta(1)\\) because it is a constant.\nIt is common to break up each subproblem uniformly, but it is not always the best way to do it. For example, an application such as matrix multiplication is typically broken up uniformly since there is no spatial or temporal relationship to consider. Algorithms for image processing, on the other hand, may have input values that are locally correlated, so it may be better to break up the input in a way that preserves this correlation.\nSolving Recurrences Substitution method Recursion-tree method Master method Akra-Bazzi method Example: Merge Sort Merge sort is a classic example of a divide and conquer algorithm. It works by dividing the input array into two halves, sorting each half recursively, and then merging the two sorted halves.\nDivide The divide step takes an input subarray \\(A[p:r]\\) and computes a midpoint \\(q\\) before partitioning it into two subarrays \\(A[p:q]\\) and \\(A[q+1:r]\\). These subarrays will be sorted recursively until the base case is reached.\nConquer The conquer step recursively sorts the two subarrays \\(A[p:q]\\) and \\(A[q+1:r]\\). If the base case is such that the input array has only one element, the array is already sorted.\nCombine The combine step merges the two sorted subarrays to produce the final sorted array.\nPython Implementation def merge_sort(A): if len(A) \u0026lt;= 1: # Conquer -- base case return A # Divide Step mid = len(A) // 2 left = merge_sort(A[:mid]) right = merge_sort(A[mid:]) # Combine Step return merge(left, right) def merge(left, right): result = [] i, j = 0, 0 # Merge the two subarrays while (i \u0026lt; len(left)) and (j \u0026lt; len(right)): if left[i] \u0026lt; right[j]: result.append(left[i]) i += 1 else: result.append(right[j]) j += 1 # Add the remaining elements to the final array result += left[i:] result += right[j:] return result This function assumes that the subarrays left and right are already sorted. If the value in the left subarray is less than the value in the right subarray, the left value is added to the final array. Otherwise, the right value is added. As soon as one of the subarrays is exhausted, the remaining elements in the other subarray are added to the final array. This is done with slicing in Python.\nThe divide step simply splits the data into left and right subarrays. The conquer step simplifies the sorting process by reducing it down to the base case \u0026ndash; a single element. Finally, the combine step merges or folds the two sorted subarrays together.\nExample code can be found here.\nAnalysis When analyzing the running time of a divide and conquer algorithm, it is safe to assume that the base case runs in constant time. The focus of the analysis should be on the recurrence equation. For merge sort, we originally have a problem of size \\(n\\). We then divide the problem into 2 subproblems of size \\(n/2\\). Therefore the recurrence is \\(T(n) = 2T(n/2)\\). This recurrence continues for as long as the base case is not reached.\nOf course we also have to factor in the time it takes for the divide and combine steps. These can be represented as \\(D(n)\\) and \\(C(n)\\), respectively. The total running time of the algorithm is then \\(T(n) = 2T(n/2) + D(n) + C(n)\\) when \\(n \u0026gt;= n_0\\), where \\(n_0\\) is the base case.\nFor merge sort specifically, the base case is \\(D(n) = \\Theta(1)\\) since all it does is compute the midpoint. As we saw above, the conquer step is the recurrence \\(T(n) = 2T(n/2)\\). The combine step is \\(C(n) = \\Theta(n)\\) since it takes linear time to merge the two subarrays. Thus, the worst-case running time of merge sort is \\(T(n) = 2T(n/2) + \\Theta(n)\\).\nNot every problem will have a recurrence of \\(2T(n/2)\\). We can generalize this to \\(aT(n/b)\\), where \\(a\\) is the number of subproblems and \\(b\\) is the size of the subproblems.\nWe haven\u0026rsquo;t finished the analysis yet since it is still not clear what the asymptotic upper bound is. Recurrence trees can be used to visualize the running time of a divide and conquer algorithms. After inspecting the result of the tree, we will be able to easily determine the complexity of merge sort in terms of big-O notation.\nFigure 1: Expansion of recursion tree for merge sort (Cormen et al. 2022). In the figure above from Introduction to Algorithms, the root of the tree represents the original problem of size \\(n\\) in (a). In (b), the divide step splits the problem into two problems of size \\(n/2\\). The cost of this step is indicated by \\(c_2n\\). Here, \\(c_2\\) represents the constant cost per element for dividing and combining. As mentioned above, the combine step is dependent on the size of the subproblems, so the cost is \\(c_2n\\). Subfigure (c) shows a third split, where each new subproblem has size \\(n/4\\). This would continue recursively until the base case is reached, as shown in the figure below.\nFigure 2: Full recursion tree for merge sort (Cormen et al. 2022). The upper bound for each level of the tree is \\(c_2n\\). The height of a binary tree is \\(\\log_b n\\). The total cost of the tree is the sum of the costs at each level. In this case, the cost is \\(c_2n \\log n + c_1n\\), where the last \\(c1_n\\) comes from the base case. The first term is the dominating factor in the running time, so the running time of merge sort is \\(\\Theta(n \\log n)\\).\nQuestions Assume that the base case is \\(n \u0026gt; 1\\). What is the running time of the conquer step dependent on? Example: Multiplying Square Matrices Matrix multiplication is defined as follows:\ndef square_matrix_multiply(A, B): n = len(A) C = [[0 for _ in range(n)] for _ in range(n)] for i in range(n): for j in range(n): for k in range(n): C[i][j] += A[i][k] * B[k][j] return C Initializing values takes \\(\\Theta(n^2)\\). The full process takes \\(\\Theta(n^3)\\).\nDivide and Conquer In this approach, the matrix will be split into block matrices of size \\(n/2\\). Each submatrix can be multiplied with the corresponding submatrix of the other matrix. The resulting submatrices can be added together to form the final matrix. This is permissible based on the definition of matrix multiplication.\nBase case is \\(n=1\\) where only a single addition and multiplication are performed. This is \\(T(1) = \\Theta(1)\\). For \\(n \u0026gt; 1\\), the recursive algorithm starts by splitting into 8 subproblems of size \\(n/2\\). There are 8 subproblems because there are 4 submatrices in each matrix, and each submatrix is multiplied with the corresponding submatrix in the other matrix.\nEach recursive call contributes \\(T(n/2)\\) to the running time. There are 8 recursive calls, so the total running time is \\(8T(n/2) + \\Theta(n^2)\\). There is no need to implement a combine step since the matrix is updated in place. The final running time is \\(T(n) = 8T(n/2) + \\Theta(1)\\) for the recursive portion.\nThis method easily adapts to parallel processing. The size of each tile can be adjusted to fit the number of processors available. The algorithm can be parallelized by assigning each processor to a subproblem.\nWe now walk through an example on a \\(4 \\times 4\\) matrix. Assume that each \\(A_{ij}\\) and \\(B_{ij}\\) is a \\(2 \\times 2\\) matrix.\n\\begin{bmatrix} A_{11} \u0026amp; A_{12} \\\\ A_{21} \u0026amp; A_{22} \\end{bmatrix}\n\\begin{bmatrix} B_{11} \u0026amp; B_{12} \\\\ B_{21} \u0026amp; B_{22} \\end{bmatrix}\nThese matrices are already partitioned. They currently don\u0026rsquo;t meet the base case, so 8 recursive calls are made which compute the following products:\n\\(A_{11}B_{11}\\) \\(A_{12}B_{21}\\) \\(A_{11}B_{12}\\) \\(A_{12}B_{22}\\) \\(A_{21}B_{11}\\) \\(A_{22}B_{21}\\) \\(A_{21}B_{12}\\) \\(A_{22}B_{22}\\) Peeking into the first recursive call, the \\(2 \\times 2\\) matrices are partitioned into 4 \\(1 \\times 1\\) matrices, or scalars. The base case is reached, and the product is computed. The same process is repeated for the other 7 recursive calls. The final matrix is then formed by adding the products together.\ndef partition(A): n = len(A) mid = n // 2 A11 = [row[:mid] for row in A[:mid]] A12 = [row[mid:] for row in A[:mid]] A21 = [row[:mid] for row in A[mid:]] A22 = [row[mid:] for row in A[mid:]] return A11, A12, A21, A22 def matrix_multiply_recursive(A, B, C, n): if n == 1: C[0][0] += A[0][0] * B[0][0] else: # Partition the matrices A11, A12, A21, A22 = partition(A) B11, B12, B21, B22 = partition(B) C11, C12, C21, C22 = partition(C) # Recursively compute the products matrix_multiply_recursive(A11, B11, C11, n/2) matrix_multiply_recursive(A12, B21, C11, n/2) matrix_multiply_recursive(A11, B12, C11, n/2) matrix_multiply_recursive(A12, B22, C11, n/2) matrix_multiply_recursive(A21, B11, C11, n/2) matrix_multiply_recursive(A22, B21, C11, n/2) matrix_multiply_recursive(A21, B12, C11, n/2) matrix_multiply_recursive(A22, B22, C11, n/2) Analysis Each recursive call contributes \\(T(n/2)\\) to the running time. Unless the base case is reached, each call contributes 8 recursive calls to the recurrence, yielding a running time of \\(T(n) = 8T(n/2) + \\Theta(1)\\).\nExample: Convex Hull Given \\(n\\) points in plane, the convex hull is the smallest convex polygon that contains all the points.\nNo two points have the same \\(x\\) or \\(y\\) coordinate.\nSequence of points on boundary in clockwise order as doubly linked list.\nFigure 3: Convex Hull (source: Wikipedia) Naive Solution Draw lines between each pair of points. If all other points are on the same side of the line, the line is part of the convex hull. This is \\(\\Theta(n^3)\\).\nDivide and Conquer Sort the points by \\(x\\) coordinate. Split into two halves by \\(x\\). Recursively find the convex hull of each half. Merge the two convex hulls.\nMerging Find upper tangent and lower tangent.\nWhy not just select the highest point from each half?\nThe highest point in each half may not be part of the convex hull. The question assumes that the two convex hulls are relatively close to each other.\nTwo Finger Algorithm Start at the rightmost point of the left convex hull and the leftmost point of the right convex hull. Move the right finger clockwise and the left finger counterclockwise until the tangent is found. The pseudocode is as follows:\ndef merge_convex_hulls(left, right): # Find the rightmost point of the left convex hull left_max = max(left, key=lambda p: p[0]) # Find the leftmost point of the right convex hull right_min = min(right, key=lambda p: p[0]) # Find the upper tangent while True: # Move the right finger clockwise right_max = max(right, key=lambda p: p[0]) if is_upper_tangent(left_max, right_max): break # Move the left finger counterclockwise left_max = max(left, key=lambda p: p[0]) # Find the lower tangent while True: # Move the right finger clockwise right_min = min(right, key=lambda p: p[0]) if is_lower_tangent(left_min, right_min): break # Move the left finger counterclockwise left_min = min(left, key=lambda p: p[0]) This runs in \\(\\Theta(n)\\) time.\nRemoving the lines that are not part of the convex hull require the cut and paste operations. Starting at the upper tangent, move clockwise along the right convex hull until you reach the point in the lower tangent of the right convex hull. Make the connection to the corresponding point on the left convex hull based on the lower tangent, then move clockwise until you reach the upper tangent of the left convex hull. This is \\(\\Theta(n)\\).\nOrientation Test The orientation test is a technique from computational geometry which determines the orientation of three points. For our purposes, it will tell us if a third point lies below or above a given line segment. The orientation test is used to determine if a point is part of the convex hull.\nGiven three points \\(p\\), \\(q\\), and \\(r\\), the orientation is determined by the sign of the cross product of the vectors \\(\\overrightarrow{pq}\\) and \\(\\overrightarrow{pr}\\). If the cross product is positive, the orientation is clockwise. If the cross product is negative, the orientation is counterclockwise. If the cross product is zero, the points are collinear.\nThis is expressed by a simple formula:\n\\[ \\text{orientation}(p, q, r) = (q_y - p_y)(r_x - q_x) - (q_x - p_x)(r_y - q_y) \\]\nConsider the visualization below. Let \\(p\\) be a point in the left convex hull and \\(q\\) be a point in the right convex hull. The orientation test will tell us if \\(r\\) is above or below the line segment \\(\\overline{pq}\\). If the test is negative, \\(r\\) is above the line segment and is part of the convex hull, and vice versa.\nFigure 4: Visualization of the orientation test. When checking to see if a line is an upper tangent, consider the points \\(p\\), \\(q\\), and \\(r\\), where \\(p\\) is from the left convex hull and \\(q\\) is from the right convex hull. Let \\(r\\) be the point immediately after \\(q\\) in a clockwise direction.\nExample: Median Search Finding the median value of a set can be performed in linear time without fully sorting the data. The recurrence is based on discarding a constant fraction of the elements at each step.\nAlgorithm Divide: Partition the set into groups of 5 elements. Depending on the size of the set, there may be less than 5 elements in the last set. Conquer: Sort each group and find the median of each group. Since the subsets are of constant size, this is done in constant time. Combine: Given the median of each group from step 2, find the median of medians. This value will be used as a pivot for the next step. Partition: Use the pivot to separate values smaller and larger than the pivot. Select: If the given pivot is the true median based on its position in the original set, select it. If not, recursively select the median from the appropriate partition. Figure 5: Visualization of median of medians (Cormen et al. 2022). Given a set of \\(n\\) numbers, define \\(rank(X)\\) as the number in the set that are less than or equal to \\(X\\).\n\\(Select(S, i)\\)\nPick \\(x \\in S\\) Compute \\(k = rank(x)\\) B = {y in S | y \u0026lt; x} C = {y in S | y \u0026gt; x} If \\(i = k\\), return \\(x\\). else if \\(k \u0026gt; i\\), return \\(Select(B, i)\\). else return \\(Select(C, i-k)\\).\nHow do we get balanced partitions?\nArrange \\(S\\) into columns of size 5. Sort each column descending (linear time). Find \u0026ldquo;median of medians\u0026rdquo; as \\(X\\)\nIf the columns are sorted, it is trivial to find the median of each column.\nHalf of the groups contribute at least 3 elements greater than \\(X\\), except for the last group. We have one group that contains \\(x\\).\nAnalysis Given a set \\(A = \\{a_1, a_2, \\ldots, a_n\\}\\), the median search algorithm returns the $k$th smallest element of \\(A\\). We now analyze the runtime of this algorithm.\nDivide In the divide step, the set is partitioned into \\(\\lceil n/5 \\rceil\\) groups of 5 elements each. This is done in linear time.\nConquer Each group is sorted using insertion sort or some other algorithm. Even though the search itself may have a higher complexity, the sorting of the groups is \\(\\Theta(1)\\) since the groups are of constant size. Collectively across all groups, the sorting is \\(\\Theta(n)\\).\nCombine The median of each group is found, introducing a recurrence of \\(T(\\lceil n/5 \\rceil)\\). Once each median is found, the median of medians is computed.\nPartition As reasoned above, the median of medians is used as a pivot to partition the set into two groups. At least 30% of the elements are greater than the pivot. This leaves a search space of at most \\(7n/10\\). Partitioning is done in linear time.\nSelect If the pivot is the $k$th smallest element, the algorithm terminates. Otherwise, the algorithm recursively searches the appropriate partition. Comparing the size of the partition to the size of the original set, the recurrence is \\(T(7n/10)\\).\nPseudocode is available here.\nReferences Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. Introduction to Algorithms. 4th ed. MIT Press. http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/. ","date":1706020680,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706020680,"objectID":"e99e02d509893a4a69ce59b6006c1ce0","permalink":"https://ajdillhoff.github.io/notes/divide_and_conquer_algorithms/","publishdate":"2024-01-23T08:38:00-06:00","relpermalink":"/notes/divide_and_conquer_algorithms/","section":"notes","summary":"Table of Contents Definition Solving Recurrences Example: Merge Sort Example: Multiplying Square Matrices Example: Convex Hull Example: Median Search Definition Divide and conquer algorithms are a class of algorithms that solve a problem by breaking it into smaller subproblems, solving the subproblems recursively, and then combining the solutions to the subproblems to form a solution to the original problem. Problems that can be solved in this manner are typically highly parallelizable.","tags":["algorithms","computer science"],"title":"Divide and Conquer Algorithms","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Differential Equations Stencils Example: Basic Stencil Tiled Stencil Thread Coarsening Register Tiling Summary Questions Used in differential equations Frequently use higher precision Some similarity to convolutions Differential Equations Any computational problem requires discretization of data or equations so that they can be solved numerically. This is fundamental in numerical analysis, where differential equations need to be approximated.\nStructured grids are used in finite-difference methods for solving partial differential equations (PDEs). Approximate derivatives can be computed point-wise by considering the neighbors on the grid. As we saw earlier in this course, a grid representation is a natural way to think about data parallelism.\nDepending on the function and level of discretization, interpolation will be more or less accurate. Consider the logistic sigmoid function sampled at 4 points. In the middle, linear interpolation would work just fine. Near the bends of the function, a linear approximation would introduce error. The closer the spacing, the more accurate a linear approximation becomes. The downside is that more memory is required to store the points.\nFigure 1: Logistic sigmoid function (Wikipedia). The precision of the data type also plays an important role. Higher precision data types like double require more bandwidth to transfer and will typically require more cycles when computing arithmetic operations.\nStencils A stencil is a geometric pattern of weights applied at each point of a structured grid. The points on the grid will derive their values from neighboring points using some numerical approximation. For example, this is used to solve differential equations. Consider a 1D grid of discretized values from a function \\(f(x)\\). The finite difference approximation can be used to find \\(f\u0026rsquo;(x)\\):\n\\[ f\u0026rsquo;(x) = \\frac{f(x+h) - f(x-h)}{2h} + O(h^2) \\]\nIn code, this would look like:\n__global__ void finite_difference(float *f, float *df, float h) { int i = blockIdx.x * blockDim.x + threadIdx.x; df[i] = (f[i+1] - f[i-1]) / (2 * h); } A PDE of two variables can be solved using a 2D stencil. Likewise, a PDE of three variables can be solved using a 3D stencil. The figures below show examples of common 2D and 3D stencils. Note that they typically have an odd number of points so that there is a center point.\nFigure 2: 1D stencils. Recreated from (Hwu, Kirk, and El Hajj 2022). The 1D stencils shown above are used to approximate the first-order (A), second-order (B), and third-order (C) derivatives of a function \\(f(x)\\).\nFigure 3: 2D and 3D stencils. Recreated from (Hwu, Kirk, and El Hajj 2022). The 2D stencils shown above are used to approximate first-order (A) and second-order (B) derivatives of a function \\(f(x, )\\). Likewise, the 3D stencils are used to approximate first-order (C) and second-order (D) derivatives of a function \\(f(x, y, z)\\).\nA stencil sweep is the process of applying the stencil to all points on the grid, similar to how a convolution is applied. There are many similarities between the two, but the subtle differences will require us to think differently about how to optimize them.\nExample: Basic Stencil The code below presents a naive kernel for a stencil pattern using a 3D seven-point stencil.\n__global__ void stencil_kernel(float *in, float *out, unsigned int N) { unsigned int i = blockIdx.z * blockDim.z + threadIdx.z; unsigned int j = blockIdx.y * blockDim.y + threadIdx.y; unsigned int k = blockIdx.x * blockDim.x + threadIdx.x; if (i \u0026gt;= 1 \u0026amp;\u0026amp; i \u0026lt; N - 1 \u0026amp;\u0026amp; j \u0026gt;= 1 \u0026amp;\u0026amp; j \u0026lt; N - 1 \u0026amp;\u0026amp; k \u0026gt;= 1 \u0026amp;\u0026amp; k \u0026lt; N - 1) { out[i*N*N + j*N + k] = c0 * in[i*N*N + j*N + k] + c1 * in[i*N*N + j*N + (k - 1)] + c2 * in[i*N*N + j*N + (k + 1)] + c3 * in[i*N*N + (j - 1)*N + k] + c4 * in[i*N*N + (j + 1)*N + k] + c5 * in[(i - 1)*N*N + j*N + k] + c6 * in[(i + 1)*N*N + j*N + k]; } } This example assumes the input and output are 3D grids. For this particular stencil, you should try to identify the number of memory accesses and operations performed. Can you already see some opportunities for optimization?\nTiled Stencil Just like with convolution, it is possible to use shared memory to improve the performance of a stencil. The code below shows a tiled stencil kernel that uses shared memory.\n__global__ void stencil_kernel(float *in, float *out, unsigned int N) { __shared__ float tile[IN_TILE_DIM][IN_TILE_DIM][IN_TILE_DIM]; int i = blockIdx.z * OUT_TILE_DIM + threadIdx.z - 1; int j = blockIdx.y * OUT_TILE_DIM + threadIdx.y - 1; int k = blockIdx.x * OUT_TILE_DIM + threadIdx.x - 1; if (i \u0026gt;= 0 \u0026amp;\u0026amp; i \u0026lt; N \u0026amp;\u0026amp; j \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt; N \u0026amp;\u0026amp; k \u0026gt;= 0 \u0026amp;\u0026amp; k \u0026lt; N) { tile[threadIdx.z][threadIdx.y][threadIdx.x] = in[i*N*N + j*N + k]; } __syncthreads(); if (i \u0026gt;=1 \u0026amp;\u0026amp; i \u0026lt; N-1 \u0026amp;\u0026amp; j \u0026gt;= 1 \u0026amp;\u0026amp; j \u0026lt; N-1 \u0026amp;\u0026amp; k \u0026gt;= 1 \u0026amp;\u0026amp; k \u0026lt; N-1) { if (threadIdx.z \u0026gt;= 1 \u0026amp;\u0026amp; threadIdx.z \u0026lt; IN_TILE_DIM-1 \u0026amp;\u0026amp; threadIdx.y \u0026gt;= 1 \u0026amp;\u0026amp; threadIdx.y \u0026lt; IN_TILE_DIM-1 \u0026amp;\u0026amp; threadIdx.x \u0026gt;= 1 \u0026amp;\u0026amp; threadIdx.x \u0026lt; IN_TILE_DIM-1) { out[i*N*N + j*N + k] = c0 * tile[threadIdx.z][threadIdx.y][threadIdx.x] + c1 * tile[threadIdx.z][threadIdx.y][threadIdx.x - 1] + c2 * tile[threadIdx.z][threadIdx.y][threadIdx.x + 1] + c3 * tile[threadIdx.z][threadIdx.y - 1][threadIdx.x] + c4 * tile[threadIdx.z][threadIdx.y + 1][threadIdx.x] + c5 * tile[threadIdx.z - 1][threadIdx.y][threadIdx.x] + c6 * tile[threadIdx.z + 1][threadIdx.y][threadIdx.x]; } } } Just like before, the threads of a block will first collaborate to load a tile with the relevant data. Stencil patterns are more sparse than convolutional filters and require less data to be loaded into shared memory. This will be the central detail in our analysis of stencil patterns.\nScaling stencil size vs. Convolution filter size Convolutions are more efficient as the size increases since more values are accessed in shared memory. For a \\(3 \\times 3\\) convolution, the upper bound on compute to memory ratio is 4.5 OP/B. A 5 point 2D stencil has a ratio of 2.5 OP/B due to the sparsity of the pattern. The threads in the block would load the diagonal values from global memory, but each thread would only use the 5 points defined by the kernel.\nTiling Analysis Let us consider the effectiveness of shared memory tiling where each thread performs 13 floating-point ops (7 multiplies and 6 adds) with each block using \\((T - 2)^3\\) threads. Each block also performs \\(T^3\\) loads of 4 bytes each. The compute to memory ratio can be express as:\n\\[ \\frac{13(T - 2)^3}{4T^3} = \\frac{13}{4} \\left(1 - \\frac{2}{T}\\right)^3 \\]\nDue to the low limit on threads, the size of \\(T\\) is typically small. This means there is a smaller amount of reuse of data in shared memory. The ratio of floating-point ops to memory accesses will be low.\nEach warp loads values from 4 distant locations in global memory. This means that the memory accesses are not coalesced: the memory bandwidth is low. Consider an \\(8 \\times 8 \\times 8\\) block. A warp of 32 threads will load 4 rows of 8 values each. The values within each row contiguous, but the rows are not contiguous.\nThread Coarsening Stencils do not benefit from shared memory as much as convolutions due to the sparsity of the sampled points. Most applications of the stencil patterns work with a 3D grid, resulting in relatively small tile sizes per block.\nA solution to this is to increase the amount of work each thread performs, AKA thread coarsening. The price paid for parallelism in the stencil pattern is the low frequency of memory reuse.\nEach thread performs more work in the \\(z\\) direction for a 3D seven-point stencil. All threads collaborate to load in a \\(z\\) layer at time from \\(z-1\\) to \\(z+1\\). There are then 3 different shared memory tiles per block. After computing values in the current output tile, the shared memory is rearranged for the next layer. This means that there are more transfers between shared memory as opposed to global memory.\nThe threads are launched to work with a 2D tile at a time, so the size of the block is now \\(T^2\\). This means we can use a larger value for \\(T\\). The compute to memory ratio is almost doubled under this scheme. Additionally, the amount of shared memory required is \\(3T^2\\) rather than \\(T^3\\).\n__global__ void stencil_kernel(float *in, float *out, unsigned int N) { int iStart = blockIdx.z * OUT_TILE_DIM; int j = blockIdx.y * OUT_TILE_DIM + threadIdx.y - 1; int k = blockIdx.x * OUT_TILE_DIM + threadIdx.x - 1; __shared__ float inPrev_s[IN_TILE_DIM][IN_TILE_DIM]; __shared__ float inCurr_s[IN_TILE_DIM][IN_TILE_DIM]; __shared__ float inNext_s[IN_TILE_DIM][IN_TILE_DIM]; if (iStart - 1 \u0026gt;= 0 \u0026amp;\u0026amp; iStart - 1 \u0026lt; N \u0026amp;\u0026amp; j \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt; N \u0026amp;\u0026amp; k \u0026gt;= 0 \u0026amp;\u0026amp; k \u0026lt; N) { inPrev_s[threadIdx.y][threadIdx.x] = in[(iStart - 1)*N*N + j*N + k]; } if (iStart \u0026gt;= 0 \u0026amp;\u0026amp; iStart \u0026lt; N \u0026amp;\u0026amp; j \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt; N \u0026amp;\u0026amp; k \u0026gt;= 0 \u0026amp;\u0026amp; k \u0026lt; N) { inCurr_s[threadIdx.y][threadIdx.x] = in[iStart*N*N + j*N + k]; } for (int i = iStart; i \u0026lt; iStart + OUT_TILE_DIM; i++) { if (i + 1 \u0026gt;= 0 \u0026amp;\u0026amp; i + 1 \u0026lt; N \u0026amp;\u0026amp; j \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt; N \u0026amp;\u0026amp; k \u0026gt;= 0 \u0026amp;\u0026amp; k \u0026lt; N) { inNext_s[threadIdx.y][threadIdx.x] = in[(i + 1)*N*N + j*N + k]; } __syncthreads(); if (i \u0026gt;= 1 \u0026amp;\u0026amp; i \u0026lt; N - 1 \u0026amp;\u0026amp; j \u0026gt;= 1 \u0026amp;\u0026amp; j \u0026lt; N - 1 \u0026amp;\u0026amp; k \u0026gt;= 1 \u0026amp;\u0026amp; k \u0026lt; N - 1) { if (threadIdx.y \u0026gt;= 1 \u0026amp;\u0026amp; threadIdx.y \u0026lt; IN_TILE_DIM - 1 \u0026amp;\u0026amp; threadIdx.x \u0026gt;= 1 \u0026amp;\u0026amp; threadIdx.x \u0026lt; IN_TILE_DIM - 1) { out[i*N*N + j*N + k] = c0 * inCurr_s[threadIdx.y][threadIdx.x] + c1 * inCurr_s[threadIdx.y][threadIdx.x - 1] + c2 * inCurr_s[threadIdx.y][threadIdx.x + 1] + c3 * inCurr_s[threadIdx.y - 1][threadIdx.x] + c4 * inCurr_s[threadIdx.y + 1][threadIdx.x] + c5 * inPrev_s[threadIdx.y][threadIdx.x] + c6 * inNext_s[threadIdx.y][threadIdx.x]; } } __syncthreads(); inPrev_s[threadIdx.y][threadIdx.x] = inCurr_s[threadIdx.y][threadIdx.x]; inCurr_s[threadIdx.y][threadIdx.x] = inNext_s[threadIdx.y][threadIdx.x]; } } This kernel is visualized below with a block size of \\(6 \\times 6\\). The left and top sides of the tile have been removed for clarity. All of the blue blocks are loaded from global memory. The dark blue blocks represent the active plane that is used to compute the corresponding output values. After the current plane is completed, the block synchronizes before moving the current values to the previous plane and loads the next plane\u0026rsquo;s values into the current plane inCurr_s.\nFigure 4: Visualization of the thread coarsening tile. Register Tiling In the coarsening solution presented above, each thread works with a single element in the previous and next shared memory tiles. There are 4 elements in that example that really need to be loaded from shared memory. For the 3 elements that are only required by the current thread, they can be loaded into registers.\nSince only the values in the \\(x-y\\) direction are required for shared memory, the amount of memory used is reduced by \\(\\frac{1}{3}\\).\n__global__ void stencil_kernel(float *in, float *out, unsigned int N) { int iStart = blockIdx.z * OUT_TILE_DIM; int j = blockIdx.y * OUT_TILE_DIM + threadIdx.y - 1; int k = blockIdx.x * OUT_TILE_DIM + threadIdx.x - 1; float inPrev; float inCurr; float inNext; __shared__ float inCurr_s[IN_TILE_DIM][IN_TILE_DIM]; if (iStart - 1 \u0026gt;= 0 \u0026amp;\u0026amp; iStart - 1 \u0026lt; N \u0026amp;\u0026amp; j \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt; N \u0026amp;\u0026amp; k \u0026gt;= 0 \u0026amp;\u0026amp; k \u0026lt; N) { inPrev = in[(iStart - 1)*N*N + j*N + k]; } if (iStart \u0026gt;= 0 \u0026amp;\u0026amp; iStart \u0026lt; N \u0026amp;\u0026amp; j \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt; N \u0026amp;\u0026amp; k \u0026gt;= 0 \u0026amp;\u0026amp; k \u0026lt; N) { inCurr = in[iStart*N*N + j*N + k]; inCurr_s[threadIdx.y][threadIdx.x] = inCurr; } for (int i = iStart; i \u0026lt; iStart + OUT_TILE_DIM; i++) { if (i + 1 \u0026gt;= 0 \u0026amp;\u0026amp; i + 1 \u0026lt; N \u0026amp;\u0026amp; j \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt; N \u0026amp;\u0026amp; k \u0026gt;= 0 \u0026amp;\u0026amp; k \u0026lt; N) { inNext = in[(i + 1)*N*N + j*N + k]; } __syncthreads(); if (i \u0026gt;= 1 \u0026amp;\u0026amp; i \u0026lt; N - 1 \u0026amp;\u0026amp; j \u0026gt;= 1 \u0026amp;\u0026amp; j \u0026lt; N - 1 \u0026amp;\u0026amp; k \u0026gt;= 1 \u0026amp;\u0026amp; k \u0026lt; N - 1) { if (threadIdx.y \u0026gt;= 1 \u0026amp;\u0026amp; threadIdx.y \u0026lt; IN_TILE_DIM - 1 \u0026amp;\u0026amp; threadIdx.x \u0026gt;= 1 \u0026amp;\u0026amp; threadIdx.x \u0026lt; IN_TILE_DIM - 1) { out[i*N*N + j*N + k] = c0 * inCurr + c1 * inCurr_s[threadIdx.y][threadIdx.x - 1] + c2 * inCurr_s[threadIdx.y][threadIdx.x + 1] + c3 * inCurr_s[threadIdx.y - 1][threadIdx.x] + c4 * inCurr_s[threadIdx.y + 1][threadIdx.x] + c5 * inPrev + c6 * inNext; } } __syncthreads(); inPrev = inCurr; inCurr = inNext; inCurr_s[threadIdx.y][threadIdx.x] = inNext_s[threadIdx.y][threadIdx.x]; } } The kernel always has the active plane in shared memory. Every thread collectively store the previous and next planes in registers.\nThe larger the stencil size, the more registers are required per thread. In this case, a tradeoff between shared memory space and register usage could be made. This will be explored in your lab.\nSummary Stencils are a useful pattern for solving differential equations. They have some similarities to convolutions, but present unique challenges in terms of optimization. The sparsity of the pattern means that shared memory is not as effective as it is for convolutions. Thread coarsening and register tiling are two techniques that can be used to improve performance.\nQuestions How many registers per thread are required for a 3D seven-point stencil, 3D nine-point stencil, and 3D 27-point stencil? How do convolutions relate to stencil patterns? Could you implement a stencil pattern using a convolution filter? References Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. Programming Massively Parallel Processors: A Hands-on Approach. Fourth. Morgan Kaufmann. ","date":1705973940,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705973940,"objectID":"d301bab8ceb400e0864317e3fb4a5a24","permalink":"https://ajdillhoff.github.io/notes/gpu_pattern_stencils/","publishdate":"2024-01-22T19:39:00-06:00","relpermalink":"/notes/gpu_pattern_stencils/","section":"notes","summary":"Table of Contents Differential Equations Stencils Example: Basic Stencil Tiled Stencil Thread Coarsening Register Tiling Summary Questions Used in differential equations Frequently use higher precision Some similarity to convolutions Differential Equations Any computational problem requires discretization of data or equations so that they can be solved numerically. This is fundamental in numerical analysis, where differential equations need to be approximated.\nStructured grids are used in finite-difference methods for solving partial differential equations (PDEs).","tags":["gpgpu","computer science"],"title":"GPU Pattern: Stencils","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Convolution Properties of Convolutions Implementing a Convolution Kernel Constant Memory and Caching Tiled Convolutions Caching the Halo Cells This pattern involves tiling and input data staging.\nRecall from Lab 1 where we implementing a kernel to blur an image. This kernel worked on each individual output pixel by computing the weighted average of each pixel from the input image, centered on the output pixel location. When we implemented this, we set the weight of every pixel to 1. Whether you were aware of it or not, you implemented a convolution between an input image and weighted kernel.\nThe convolution is an extremely important operator that works on time-varying signals of different dimensionality. In computer vision, they are commonly used to compute responses between a known pattern and an input image. The pixels that return a greater response as a result of convolution indicate a match between the pattern and the input image.\nThis operator can and has been efficiently implemented in a GPU. Through studying this pattern, you will learn to utilize constant memory storage and shared memory storage to efficiently implement a convolution kernel. These techniques will be useful in later applications as well.\nConvolution A convolution is a function that takes two functions as input and produces a third function as output. The first function is the input and the second function is the kernel. The output is called the feature map. The kernel is also sometimes called the filter.\n\\[ (f * g)(t) = \\int f(t-a)g(a)da \\]\nWe can view them more concretely by considering the functions to be vectors. For example, let the function \\(f\\) be an input vector \\(x\\) and \\(w\\) be a kernel representing a filter. The convolution operator is then\n\\[ (x * w)(t) = \\int x(t-a)w(a)da. \\]\nThe result the feature map representing the response of the kernel at each location in the input.\nIn the case of discrete values, it is common to use an odd-sized kernel and center it on an input value. The kernel size is given by some radius \\(r\\). The convolution operator is then\n\\[ (x * w)(t) = \\sum_{-r}^r x(t-r)w( r). \\]\nThe figure below shows an example of a 1D convolution of a vector if size 8 with a kernel of size 5, centered on the \\(t = 2\\).\nFigure 1: 1D Convolution between a vector of size 8 and a kernel of size 5. Convolution is defined in such a way that the kernel is traversed in an inverted manner. In the example above, \\(y_2\\) is computed by applying the kernel to \\(\\mathbf{x}\\) centered on \\(x_2\\). The calculation in terms of the locations accesses is\n\\[ y_2 = x_4 w_{-2} + x_3 w_{-1} + x_2 w_0 + x_1 w_1 + x_0 w_2. \\]\nThis operation is very similar to the correlation operator, which is defined as\n\\[ (x \\star w)(t) = \\sum_{-r}^r x(t+r)w( r). \\]\nWe can use the correlation operator to compute the convolution by flipping the kernel. In this case, the calculation can be represented using the dot product. We can also slightly adjust the indexing so that the first index is 0.\n\\[ y_i = \\sum_{k=0}^{2r} x_{i+k-r} w_{2r-k}. \\]\nNote that the convolution shown above would be undefined for \\(i = 0\\) and \\(i = 1\\) since the kernel would be accessing negative indices. Based on the definition, we would ignore these values. This is called a valid convolution. The output size is then \\(n - 2r\\). There is also a full convolution where the output size is \\(n\\). In this case, the kernel is padded with zeros so that it can be applied to all elements of the input.\n2D Convolution Image convolutions use 2D filters applied to 2D images. For a filter with radius \\(r\\), size of the filter is \\((2r + 1) \\times (2r + 1)\\). The convolution is then\n\\[ (x * w)(i, j) = \\sum_{-r}^r \\sum_{-r}^r x(i-r, j-r)w(r, s). \\]\nProperties of Convolutions Convolutional networks are commonly built on full or valid convolutions. Other variants have also been explored. Here, we will briefly discuss the different properties of this operator. A more detailed treatment can be found in (Dumoulin and Visin 2018).\nPadding By definition, a convolution of an input with a filter of size \\(n\\times n\\) will produce an output of size \\((m-n+1)\\times(m-n+1)\\), where \\(m\\) is the size of the input. This means that the output will be smaller than the input. This is often referred to as a valid convolution. The figure below shows a convolution between a \\(3\\times3\\) kernel and a \\(5\\times5\\) input.\nFigure 2: A valid convolution (Dumoulin and Visin 2018). The output of this convolution is a \\(3\\times3\\) feature map. This is a problem if we want to build a deep network. Each convolution will reduce the size of the input. If we were to stack multiple convolutional layers, the output would eventually be too small to be useful. If we want our output to be same size as the input, we can add padding to the original input image before convolving it. This is often known as a full convolution. An example is shown below.\nFigure 3: A full convolution (Dumoulin and Visin 2018). Stride So far, we have only looked at convolutions which step by 1 unit as they shift over the image. We can control the size of this step, or stride, to produce different outcomes. Picking a non-unit stride has a number of effects on the features that are learned in a convolutional neural network.\nDimensionality reduction: Skipping over pixels reduces the size of the output feature map. This provides another way of downsampling the input. Less computation: Fewer computations are required to produce the output feature map. Increased field of view: A larger stride increases the field of view of the kernel, leading to larger receptive fields in deeper layers. Given an input of size \\(m\\times m\\) and a kernel of size \\(n\\times n\\), the output size of a convolution with stride \\(s\\) is given by\n\\[ \\left\\lfloor\\frac{m-n}{s}\\right\\rfloor + 1. \\]\nThe figure below shows a convolution with stride 2 on a \\(5\\times5\\) input.\nFigure 4: A convolution with stride 2 (Dumoulin and Visin 2018). Kernel Size The size of the kernel has a large impact on the features that are learned. A larger kernel will have a larger receptive field. This means that the kernel will be able to capture more information about the input. However, this comes at the cost of increased computation. Common kernel sizes in most CNNs are \\(3\\times3\\), \\(5\\times5\\), and \\(7\\times7\\). It is also convenient to pick an odd kernel size so that the kernel has a center pixel.\nDilation Around 2015, a research trend for CNNs was to find a way to increase the receptive field without adding more parameters. The result is a dilated convolution. The output of a dilated convolution is computed by skipping over pixels in the input. The figure below shows a \\(3\\times3\\) kernel with a dilation of 2.\nFigure 5: A dilated convolution (Dumoulin and Visin 2018). The output size is computed as\n\\[ \\left\\lfloor\\frac{m + 2p - n - (n-1)(d-1)}{s}\\right\\rfloor + 1, \\]\nwhere \\(p\\) is the amount of padding and \\(d\\) is the dilation factor.\nImplementing a Convolution Kernel It is straightforward to write the convolution operation in CUDA C++. Each thread will compute the value for a single output pixel using the filter. We already implemented something very similar with the blurring kernel. The kernel itself should accept the following arguments:\nThe input image The output image The kernel The radius of the kernel The width of the output image The height of the output image A more robust implementation would consider things like padding, stride, dilation, and whether or not a valid or full convolution is desired. For now, we will focus on the simplest case: a valid convolution with a stride of 1 and no padding or dilation. First, let\u0026rsquo;s review the initial naive solution from Programming Massively Parallel Processors (Hwu, Kirk, and El Hajj 2022).\n__global__ void conv2D(float *input, float *filter, float *output, int r, int width, int height) { int outCol = blockIdx.x * blockDim.x + threadIdx.x; int outRow = blockIdx.y * blockDim.y + threadIdx.y; float sum = 0.0f; for (int row = 0; row \u0026lt; 2*r+1; row++) { for (int col = 0; col \u0026lt; 2*r+1; col++) { int inRow = outRow + row - r; int inCol = outCol + col - r; if (inRow \u0026gt;= 0 \u0026amp;\u0026amp; inRow \u0026lt; height \u0026amp;\u0026amp; inCol \u0026gt;= 0 \u0026amp;\u0026amp; inCol \u0026lt; width) { sum += input[inRow * width + inCol] * filter[row * (2*r+1) + col]; } } } output[outRow * width + outCol] = sum; } With this kernel, the input and output sizes are assumed to be the same. There is a boundary check in the inner-most loop to account for pixels for which a convolution cannot be computed. Based on this check, we can see that this kernel is performing a valid convolution. The extra \\(2r\\) pixels on each side of the output are skipped. This presents a computational problem in the form of control divergence. Recall that all threads in a warp must execute the same instruction. If the boundary check fails for some threads, they will still execute the instructions in the loop, but will not contribute to the output. This is a waste of resources.\nIt is also a waste of resources in terms of memory used for the output. If we already know that we want to perform a valid convolution, we can allocate the output image to be the appropriate size before calling it. A slightly modified version is shown below.\n__global__ void conv2D(float *input, float *filter, float *output, int r, int width, int height) { int outCol = blockIdx.x * blockDim.x + threadIdx.x; int outRow = blockIdx.y * blockDim.y + threadIdx.y; float sum = 0.0f; for (int row = 0; row \u0026lt; 2*r+1; row++) { for (int col = 0; col \u0026lt; 2*r+1; col++) { int inRow = outRow + row; int inCol = outCol + col; sum += input[inRow * width + inCol] * filter[row * (2*r+1) + col]; } } output[outRow * width + outCol] = sum; } Constant Memory and Caching There is a much larger issue present in both versions of this kernel in terms of memory bandwidth. Similar to the matrix multiplication kernel, this kernel can benefit from tiling. However, there is a new problem that arises specifically with convolution. The same filter is accessed by every single thread. This filter does not change for the entire duration of the kernel. This means that we are wasting memory bandwidth by having every thread access the same filter.\nGiven its relatively small size, this kernel is a perfect candidate for constant memory. This is a special type of memory that is cached on the GPU. It is read-only and has a limited size, but it is much faster than global memory. We can write to the devices constant memory from the host code.\n#define FILTER_RADIUS 1 __constant__ float kFilter_d[2*FILTER_RADIUS+1][2*FILTER_RADIUS+1]; This informs the compiler to allocate a 2D array of floats in constant memory. The size of the array is determined by the constant `FILTER_RADIUS`. We can then copy the filter to the device using the `cudaMemcpyToSymbol` function.\ncudaMemcpyToSymbol(kFilter_d, filter_h, (2*FILTER_RADIUS+1)*(2*FILTER_RADIUS+1)*sizeof(float)); The line above assumes there is some data on the host in the array `filter_h`. This array is copied to the device. A small note on naming convention, Google\u0026rsquo;s C++ style guide recommends naming constant variables with a k prefix. I have adopted this convention here.\nAt this point, kFilter_d is accessible from the kernel as a global variable. There is no need to pass it as an argument. The kernel can be modified to use this constant memory as follows.\n__global__ void conv2D(float *input, float *output, int r, int width, int height) { int outCol = blockIdx.x * blockDim.x + threadIdx.x; int outRow = blockIdx.y * blockDim.y + threadIdx.y; float sum = 0.0f; for (int row = 0; row \u0026lt; 2*r+1; row++) { for (int col = 0; col \u0026lt; 2*r+1; col++) { int inRow = outRow + row; int inCol = outCol + col; sum += input[inRow * width + inCol] * F_d[row][col]; } } output[outRow * width + outCol] = sum; } If you organize your files such that the kernel is in a separate file from the host code, you will need to declare the constant variable in the kernel file as well.\nConstant memory variables are stored in DRAM with global memory. The CUDA runtime will cache them since it knows they will not be modified. Processors use caches to reduce the latency of memory accesses by keeping frequently used data in a small, fast memory that is often located directly on the chip. This type of constant cache is preferable to one that would support high-throughput writes in terms of chip design. It would require specialized hardware to support both which would increase the cost of the chip.\nTiled Convolutions Even with caching, the convolutional kernel still makes many accesses to DRAM. Similar to matrix multiplication, we can tile the input image to reduce the number of accesses. Similar to that example, we will use a \\(4 \\times 4\\) tile size. If the input is a \\(16 \\times 16\\) image and we apply a kernel with radius \\(r=2\\), the output image under a valid convolution will be \\(12 \\times 12\\). This is visualized in the figure below.\nFigure 6: Left: The input image and its tiling. Middle: the filter. Right: The output image and its tiling. The parallel solution to this problem will follow the tiled approach used for matrix multiplication. One key difference in this case is that the input tile size will be larger than the output tile size. This size difference would further be complicated if we left the kernel size as a parameter.\nFollowing the design presented by (Hwu, Kirk, and El Hajj 2022) in Chapter 7, there are two immediate approaches to this problem based on the tile size. The first is to choose a block size that matches the size of the input tiles. The benefit to this approach is that each thread can load a single input element into shared memory. The drawback is that some of the threads will be disabled when computing the output value since the output tile is smaller. This is a form of control divergence and will result in wasted resources.\n#define FILTER_RADIUS 1 #define IN_TILE_DIM 4 #define OUT_TILE_DIM ((IN_TILE_DIM) - 2*(FILTER_RADIUS)) __constant__ float kFilter_d[2*FILTER_RADIUS+1][2*FILTER_RADIUS+1]; __global__ void conv2DTiledConstKernel(float *input, float *output, int width, int height) { __shared__ float inputTile[IN_TILE_DIM][IN_TILE_DIM]; // Input tile coordinates int col = blockIdx.x * OUT_TILE_DIM + threadIdx.x; int row = blockIdx.y * OUT_TILE_DIM + threadIdx.y; if (row \u0026lt; height \u0026amp;\u0026amp; col \u0026lt; width) { inputTile[threadIdx.y][threadIdx.x] = input[row * width + col]; } else { inputTile[threadIdx.y][threadIdx.x] = 0.0f; } __syncthreads(); // Output tile coordinates int tileCol = threadIdx.x - FILTER_RADIUS; int tileRow = threadIdx.y - FILTER_RADIUS; // In a valid convolution, the output is smaller than the input row -= FILTER_RADIUS; col -= FILTER_RADIUS; if (tileCol \u0026gt;= 0 \u0026amp;\u0026amp; tileCol \u0026lt; OUT_TILE_DIM \u0026amp;\u0026amp; tileRow \u0026gt;= 0 \u0026amp;\u0026amp; tileRow \u0026lt; OUT_TILE_DIM) { float sum = 0.0f; for (int fRow = 0; fRow \u0026lt; 2*FILTER_RADIUS+1; fRow++) { for (int fCol = 0; fCol \u0026lt; 2*FILTER_RADIUS+1; fCol++) { sum += inputTile[tileRow + fRow][tileCol + fCol] * kFilter_d[fRow][fCol]; } } output[row * (width - 2 * FILTER_RADIUS) + col] = sum; } } There are a few things to consider here. The first phase of this kernel collaboratively loads data into a shared memory space, similar to what we have seen before. This kernel assumes a convenient indexing scheme where the row and column will always be \u0026gt;= 0. We could adopt a scheme that centers the convolution on the center point of the kernel by allowing for negative indices. In this case, it would be necessary to check if the row and column are less than 0. This implementation only needs to verify that the row and column are within the given image size.\nWhen it comes to computing the output, not every thread will contribute. This is depicted by the lightly shaded areas in the figure below. You should also note which threads are active for output computation per block. In this simple example, a \\(3 \\times 3\\) filter is used. The input tile dimension is \\(4 \\times 4\\) which means the output tile will be \\(2 \\times 2\\). Only the threads corresponding to the darker blue on the left contribute to the output calculation. Since this one block computes 4 output values, the next block should start 2 units to the right of this one.\nFigure 7: The active threads for computing the output tile. Performance Analysis The purpose of this approach was to increase the ratio of arithmetic operations to global memory accesses. For the threads that compute an output tile, there is one multiplication and one addition which yields \\(\\mathtt{OUT\\_TILE\\_DIM}^2*(2*\\mathtt{FILTER\\_RADIUS} + 1)^2*2\\) operations total. Each thread in the input tile loads a single float value for a total of \\(\\mathtt{IN\\_TILE\\_DIM}^2 * 4\\) bytes. For our small example above, this gives\n\\[ \\frac{2^2 * 3^2 * 2}{4^2 * 4} = 1.125\\ \\text{Ops/byte}. \\]\nIn a more realistic example, we would maximize our input tile size to take advantage of the available threads on the device. Currently, the maximum number of supported threads is 1024. This allows for an input tile size of \\(32 \\times 32\\). The resulting operations per byte under this tile size is\n\\[ \\frac{30^2 * 3^2 * 2}{32^2 * 4} = 3.955\\ \\text{Ops/byte}. \\]\nThis ratio increases with the size of the filter.\nCaching the Halo Cells In the previous example, the size of the input tile compared to the output tile means that there were some threads that did not contribute to the output computation. These are the threads managing the lightly shaded cells in the figure above. We will refer to these as halo cells.\nThis implementation is going to take advantage of the caching behavior in the chip itself. Values that have been recently used are more likely to already be in L2 cache. This is a safe assumption since the neighboring blocks will have loaded these values into shared memory. This means that the input and output tile sizes can be the same; there is no need to waste any threads in the block. The full kernel is given below.\n__global__ void conv2DTiledCachedConstKernel(float *input, float *output, int width, int height) { __shared__ float inputTile[IN_TILE_DIM][IN_TILE_DIM]; // Input tile coordinates int col = blockIdx.x * IN_TILE_DIM + threadIdx.x; int row = blockIdx.y * IN_TILE_DIM + threadIdx.y; if (row \u0026lt; height \u0026amp;\u0026amp; col \u0026lt; width) { inputTile[threadIdx.y][threadIdx.x] = input[row * width + col]; } else { inputTile[threadIdx.y][threadIdx.x] = 0.0f; } __syncthreads(); if (row \u0026lt; FILTER_RADIUS || col \u0026lt; FILTER_RADIUS || col \u0026gt;= (width - FILTER_RADIUS) || row \u0026gt;= (height - FILTER_RADIUS)) return; // Output tile coordinates row -= FILTER_RADIUS; col -= FILTER_RADIUS; int tileCol = threadIdx.x - FILTER_RADIUS; int tileRow = threadIdx.y - FILTER_RADIUS; float sum = 0.0f; for (int fRow = 0; fRow \u0026lt; 2 * FILTER_RADIUS + 1; fRow++) { for (int fCol = 0; fCol \u0026lt; 2 * FILTER_RADIUS + 1; fCol++) { // If this value is in shared memory, access it there if (tileCol + fCol \u0026gt;= 0 \u0026amp;\u0026amp; tileCol + fCol \u0026lt; IN_TILE_DIM \u0026amp;\u0026amp; tileRow + fRow \u0026gt;= 0 \u0026amp;\u0026amp; tileRow + fRow \u0026lt; IN_TILE_DIM) { sum += inputTile[tileRow + fRow][tileCol + fCol] * kFilter_d[fRow][fCol]; } else { // Otherwise, access it from global memory sum += input[(row + fRow) * width + (col + fCol)] * kFilter_d[fRow][fCol]; } } } output[row * (width - 2 * FILTER_RADIUS) + col] = sum; } References Dumoulin, Vincent, and Francesco Visin. 2018. “A Guide to Convolution Arithmetic for Deep Learning.” Arxiv:1603.07285 [Cs, Stat], January. http://arxiv.org/abs/1603.07285. Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. Programming Massively Parallel Processors: A Hands-on Approach. Fourth. Morgan Kaufmann. ","date":1705376100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705376100,"objectID":"74c806241b9bd583149f279d3be02e46","permalink":"https://ajdillhoff.github.io/notes/gpu_pattern_convolution/","publishdate":"2024-01-15T21:35:00-06:00","relpermalink":"/notes/gpu_pattern_convolution/","section":"notes","summary":"Table of Contents Convolution Properties of Convolutions Implementing a Convolution Kernel Constant Memory and Caching Tiled Convolutions Caching the Halo Cells This pattern involves tiling and input data staging.\nRecall from Lab 1 where we implementing a kernel to blur an image. This kernel worked on each individual output pixel by computing the weighted average of each pixel from the input image, centered on the output pixel location. When we implemented this, we set the weight of every pixel to 1.","tags":["gpgpu","computer science"],"title":"GPU Pattern: Convolution","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Overview of Nsight Getting Started with Nsight Case Study: Matrix Multiplication Tips and Best Practices OCL Notes Overview of Nsight NVIDIA NSight Compute is a profiling tool for CUDA kernels. It features an expert system that can help you identify performance bottlenecks in your code. It is essential for methodically optimizing your code. These notes will cover the basics of using Nsight Compute to profile your CUDA applications.\nGetting Started with Nsight Profiling our first program In Lab 0, you implemented a vector addition kernel that is embarrassingly parallel. We will now use Nsight to profile its performance. Realistically, there is not much we can do to increase the performance of this kernel, but it will still help us understand the information that Nsight gives. To profile the application, simply launch ncu with your application.\nncu ./build/main\nDepending on where you are running this program, it may be necessary to launch it with sudo. If everything was successful, it will output something similar to the following:\nNsight Output\nvec_add(float *, float *, float *, int), 2024-Jan-16 10:42:52, Context 1, Stream 7 Section: GPU Speed Of Light Throughput ---------------------------------------------------------------------- --------------- ------------------------------ DRAM Frequency cycle/nsecond 5.71 SM Frequency cycle/nsecond 1.15 Elapsed Cycles cycle 3,279 Memory [%] % 7.54 DRAM Throughput % 7.54 Duration usecond 2.85 L1/TEX Cache Throughput % 4.32 L2 Cache Throughput % 4.86 SM Active Cycles cycle 623.58 Compute (SM) [%] % 0.82 ---------------------------------------------------------------------- --------------- ------------------------------ WRN This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details. Section: Launch Statistics ---------------------------------------------------------------------- --------------- ------------------------------ Block Size 256 Function Cache Configuration cudaFuncCachePreferNone Grid Size 16 Registers Per Thread register/thread 16 Shared Memory Configuration Size Kbyte 8.19 Driver Shared Memory Per Block Kbyte/block 1.02 Dynamic Shared Memory Per Block byte/block 0 Static Shared Memory Per Block byte/block 0 Threads thread 4,096 Waves Per SM 0.07 ---------------------------------------------------------------------- --------------- ------------------------------ WRN The grid for this launch is configured to execute only 16 blocks, which is less than the GPU\u0026#39;s 38 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations. Section: Occupancy ---------------------------------------------------------------------- --------------- ------------------------------ Block Limit SM block 16 Block Limit Registers block 16 Block Limit Shared Mem block 100 Block Limit Warps block 6 Theoretical Active Warps per SM warp 48 Theoretical Occupancy % 100 Achieved Occupancy % 15.85 Achieved Active Warps Per SM warp 7.61 ---------------------------------------------------------------------- --------------- ------------------------------ WRN This kernel\u0026#39;s theoretical occupancy is not impacted by any block limit. The difference between calculated theoretical (100.0%) and measured achieved occupancy (15.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy. Viewing Results in the GUI Nsight comes with both CLI and GUI clients. It is recommended to parse the information from the GUI. The GUI can launch programs both locally and remotely. It can also display the result of a previous launch. To generate a profiling report for our vector addition kernel, run the following command:\nncu -o vec_add_profile ./build/main\nThe argument after -o is the name of the output file. Open Nsight Compute and load the saved file. It should look something like this.\nFigure 1: Nsight Compute GUI output This basic report only includes three sections: GPU Speed of Light, Launch Statistics, and Occupancy Analysis. We will go over each of these sections in detail.\nGPU Speed of Light This section displays high level aspects of your kernel. The main metrics report what your application is doing relative to peak performance. Sparing the full details of the documentation, the most important metrics are:\nDuration: The total time spent executing the kernel. This is the most important metric for performance. SM [%]: The relative throughput of the SMs as compared to the theoretical maximum. Memory [%]: The relative throughput of the memory as compared to the theoretical maximum. Do not get lost in the numbers!\nRemember that this tool is simply reporting facts about your kernel. Take care not to misinterpret the data. In the run from above, the kernel throughput is only 0.85%. There are a number of reasons as to why this number is so low.\nLatency Issues: The kernel may have to wait for memory operations, resulting in a low throughput. Workload Characteristics: Your particular kernel may not need to do much work, resulting in a low throughput. Launch Statistics This section shows us the launch configuration that was used for this kernel. In our earlier programs, we may set these manually for testing. Later on, we will want our programs to adapt to changing input sizes, so these statistics will becomes more useful.\nMore importantly, this shows you the resource usage per block.\nIf you are profiling an application for which you are not familiar with the code, it is convenient to know the grid and block sizes that were used when launching the kernel.\nOccupancy Analysis Memory Workload Analysis Case Study: Matrix Multiplication Tips and Best Practices Do not confuse high throughput for high performance. Throughput is a measure of how much work is being done, not how fast it is being done. Using a larger grid size is not always better. More grids introduce more overhead and can lead to lower performance. OCL Notes Analysis Driven Optimization Understanding Performance Limiters Metrics Review Memory Bound Analysis Compute Bound Analysis Goals Make efficient use of memory subsystem Expose enough parallelism to hide latency\nAnalysis Driven Optimization Cyclical process\nProfile Determine Limiter Inspect Optimize Determine if memory or compute bound. If neither, analyze where the latency is coming from.\nMetrics Latency\nsm efficiency Memory\ndram utilization L2 utilization shared utilization Compute\nDP utilization SP utilization HP utilization TC utilization Integer ","date":1705351680,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705351680,"objectID":"747b0c8971064f9e290a19bb0be09e86","permalink":"https://ajdillhoff.github.io/notes/profiling_cuda_applications/","publishdate":"2024-01-15T14:48:00-06:00","relpermalink":"/notes/profiling_cuda_applications/","section":"notes","summary":"Table of Contents Overview of Nsight Getting Started with Nsight Case Study: Matrix Multiplication Tips and Best Practices OCL Notes Overview of Nsight NVIDIA NSight Compute is a profiling tool for CUDA kernels. It features an expert system that can help you identify performance bottlenecks in your code. It is essential for methodically optimizing your code. These notes will cover the basics of using Nsight Compute to profile your CUDA applications.","tags":["gpgpu","CUDA"],"title":"Profiling CUDA Applications","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Memory Coalescing Hiding Memory Latency Thread Coarsening Optimization Checklist Identifying Bottlenecks The Takeaway These notes are on \u0026ldquo;Chapter 6: Performance Considerations\u0026rdquo; from the book Programming Massively Parallel Processors (Hwu, Kirk, and El Hajj 2022).\nMemory Coalescing Global memory accesses are one of the largest bottlenecks in GPU applications. DRAM has high latency based on its design. Each cell has a transistor and a capacitor. If the capacitor is charged, it represents a 1. The process to detect the charges in these cells is on the order of 10s of nanoseconds. DRAM can read consecutive groups of cells via bursts. This means that if the data we wish to access is stored consecutively, it can be accessed within the same burst. Contrast that was random access, in which the DRAM will have to make multiple bursts to read the required data. Memory coalescing refers to optimizing our global memory accesses to take advantage of DRAM bursts.\nMatrices are naturally coalesced, so we have already been utilizing this performance pattern in previous examples.\nFigure 1: Memory accesses for a matrix in row-major ordering (Hwu, Kirk, and El Hajj 2022). Strategies to optimize code for memory coalescing are to rearrange the threads, the data, or transfer the data first to shared memory so that accesses are faster, referred to as corner turning.\nExample: Matrix Multiplication Consider \\(C = AB\\), where \\(A\\) is in row-major order and \\(B\\) is in column-major order. The naive implementation of this algorithm will have poor memory coalescing. The figure below demonstrates the memory accesses for this scenario. The values required are not consecutive in memory, so the DRAM will have to make multiple bursts to read the data.\nFigure 2: Memory accesses for a matrix in column-major ordering (Hwu, Kirk, and El Hajj 2022). The accesses to the elements in \\(B\\) will be slower since the data is not coalesced. Accessing the elements is efficient if we assign \\(N\\) consecutive threads to load \\(N\\) consecutive elements from the same column of \\(B\\). This works in conjunction with tiling. The original loads to shared memory pull from consecutive elements in \\(B\\) which allows the application to take advantage of DRAM bursts. Once the data is in shared memory, the rest of the algorithm can be performed with coalesced accesses. Shared memory uses SRAM instead of DRAM, so coalescing is not an issue.\nHiding Memory Latency DRAMS have banks and channels. A controller has a bus that connects banks to the processor. When the DRAM accesses data, the decoder enables the cells so that they can share the information stored with the sensing amplifier. This presents a high latency relative to the time it takes to actually transfer the data. This is why there are multiple banks per channel. The controller can initiate accesses on other banks instead of sitting and waiting for a single bank to finish.\nFigure 3: Single versus Multi-bank burst timings (Hwu, Kirk, and El Hajj 2022). It is possible that the controller will initiate a request to a bank that is already busy. This is called a bank conflict. The controller will have to wait for the bank to finish its current request before it can service the new request. The more banks that are available, the less likely it is that a bank conflict will occur.\nExample: Matrix Multiplication Consider DRAM with 4 channels and 2 banks per channel. The burst size of this DRAM is 8 bytes, or 2 elements. When data is written to DRAM in the first place, it is distributed in an interleaved fashion across the different channels and banks. The first figure below shows the input matrix \\(M\\) and output matrix \\(P\\). The second input matrix is omitted for brevity. The indices of \\(M\\) are linearized in row-major order to show how they are distributed across the DRAM banks. \\(P\\) is split into 4 blocks of size \\(2 \\times 2\\).\nFigure 4: Matrix M with linearized indices and matrix P split into 4 blocks. \\(M\\) is loaded into DRAM in an interleaved fashion. The first 8 bytes are loaded into bank 0 of channel 0. The next 8 bytes go into bank 0 of channel 1, and so on. Each burst returns 8 bytes. While the first access is being performed on bank 0 channel 0, the controller can initiate a request to bank 0 channel 1. This is visualized in the figure below.\nFigure 5: DRAM distribution for matrix M. Given the distribution visualized above, we can see that the data accesses for the blocks of \\(P\\) will be coalesced. Output tile 1 in matrix \\(P\\) requires $M_0, M_1, M_4,$ and \\(M_5\\). The first two are available in a single burst from channel 0 bank 0, and the second two are available in a single burst from channel 2 bank 0.\nThread Coarsening The price of parallelism may refer to the cost of\nlaunching threads synchronization redundant work redundant memory accesses etc. It there are enough hardware resources available, parallelism at the finest level is ideal. If there are not enough resources, there is a price to pay for this parallelism. The hardware will need to serialize the work into blocks of threads that can be executed in parallel.\nIf this is the case for a particular application, it may be beneficial to apply some form of thread coarsening. If the hardware would serialize the work due to inefficient resources, the price of parallelism was paid for nothing. As the programmer, you have the ability to coarsen the threads to alleviate the price of parallelism.\nExample: Coarsening Tiled Matrix Multiplication In tiled matrix multiplication, it is possible that two separate blocks work with the same tile of data from an input matrix. We pay a price for this redundancy, but the benefit is that we can parallelize the work. If the hardware does not have sufficient resource, it will serialize these two blocks. This results in paying the price of data redundancy without the benefit of parallelism.\nLet \\(A, B \\in \\mathbb{R}^{6 \\times 6}\\), then \\(C = AB \\in \\mathbb{R}^{6 \\times 6}\\). If we use a \\(2 \\times 2\\) tile size, then we have 9 blocks of work that can be executed concurrently. For argument\u0026rsquo;s sake, let\u0026rsquo;s say that the hardware can only execute 3 blocks of work at a time. We can use thread coarsening to reduce the number of blocks to 3. Each block will be responsible for a single row of the tiled output matrix. That is, if the output matrix is \\(6 \\times 6\\), then each block will be responsible for a \\(2 \\times 6\\) tile of the output matrix. This is visualized in the figure below.\nFigure 6: Thread coarsening for tiled matrix multiplication. The block itself will perform a similar function as the implementation of tiled matrix multiplication we saw previously. We will need to modify the kernel so that it processes values to fill for 3 blocks of work, spanning each row. In the figure above, this is represented by the three gray, numbered blocks. Although each block uses a different column from matrix \\(N\\), they all use the same row from matrix \\(M\\). Our solution will take advantage of this reuse of data.\nConsider the thread that computes the value for the top left entry of block 1. This thread will compute the output value as normal before looping to compute the corresponding relative position in blocks 2 and 3. That is, if the first entry computed is \\((0, 0)\\) of block 1, then the next entry computed will be \\((0, 0)\\) of block 2, and so on. This is visualized by the three solid black cells in the figure below.\nFigure 7: A single thread loops through three blocks as a result of thread coarsening. The kernel code is given below. The additional loop controls the switch between the three consecutive tiles. The values from matrix M are loaded inside the outer-most loop and are reused across the coarse tiles.\n#define TILE_WIDTH 2 #define COARSE_FACTOR 3 __global__ void matMulCoarse(float *M, float *N, float *P, int width) { __shared__ float Mds[TILE_WIDTH][TILE_WIDTH]; __shared__ float Nds[TILE_WIDTH][TILE_WIDTH]; int bx = blockIdx.x; int by = blockIdx.y; int tx = threadIdx.x; int ty = threadIdx.y; // Identify the row and column of the element to work on int row = by * TILE_WIDTH + ty; int colStart = bx * TILE_WIDTH * COARSE_FACTOR; // Initialize Pvalue float Pvalue[COARSE_FACTOR]; for (int i = 0; i \u0026lt; COARSE_FACTOR; i++) { Pvalue[i] = 0.0f; } // Loop over the tiles required to compute the current output value for (int ph = 0; ph \u0026lt; width / TILE_WIDTH; ph++) { Mds[ty][tx] = M[row * width + ph * TILE_WIDTH + tx]; for (int c = 0; c \u0026lt; COARSE_FACTOR; c++) { int col = colStart + c * TILE_WIDTH; Nds[ty][tx] = N[(ph * TILE_WIDTH + ty) * width + col]; __syncthreads(); for (int k = 0; k \u0026lt; TILE_WIDTH; k++) { Pvalue[c] += Mds[ty][k] * Nds[k][tx]; } __syncthreads(); } } for (int c = 0; c \u0026lt; COARSE_FACTOR; c++) { int col = colStart + c * TILE_WIDTH; P[row * width + col] = Pvalue[c]; } } How to use coarsening in your applications Thread coarsening is yet another technique that can be applied to optimize your parallel programs. The previous section demonstrated how it can be applied, but you are probably wondering when it should be applied. Deciding on whether to apply this technique is largely determined by careful analysis of your current application. This analysis should include benchmarking and profiling. There is work that provides an automatic solution (Stawinoga and Field 2018), but we will rely on determining that for ourselves.\nFor now, we can at least discuss when not to apply coarsening. The most obvious instance is when coarsening is completely unnecessary. Consider the vector addition kernel. Each thread performs an independent computation that has no overlapping data with other thread. There is no need to apply coarsening in this case.\nAnother bad case for implementation would be when the coarsening factor causes hardware to be underutilized. Parallelization in hardware is scalable. If we take away the opporunity for scale, there may be unused compute. This is typically something we can determine via benchmarking.\nIn the coarsened version of matrix multiplication above, we had to create additional private variables to store the coarsened values. These use additional registers per thread. If our application required more than the 32 registers available on our H100, for example, this would have a direct effect on occupancy. Keep that in mind when developing your thread coarsened solution.\nOptimization Checklist Section 6.4 of Programming Massively Parallel Processors (Hwu, Kirk, and El Hajj 2022) provides a checklist of items to consider when optimizing your GPU applications. These are summarized below.\nMaximizing Occupancy Having more threads than physical cores available is beneficial, as it hides the latency required for other operations such as data fetching. Instead of waiting on some operation to return, the hardware can switch to another thread to perform work. This is implemented by adjusting the launch configurations or optimizing the number of registers used per thread, for example. We also discussed solutions for hiding memory-based latency.\nCoalesced Global Memory Accesses Random accesses to memory are less efficient than consecutive ones. This is a theme that is repeated through many themes of computer science, such as sorting. Understanding of how the underlying hardware works brought to light new ways to optimize our applications. We can rearrange our data to take advantage of DRAM bursts, or we can use shared memory to reduce the latency of memory accesses.\nMinimizing Control Divergence Although we have not used any applications that exhibit control divergence, we have studied the concept. During SIMD execution, the hardware executes the same instructions on multiple data elements. If a thread or group of threads would diverge from the others, the hardware would have to make multiple passes to cover all of the possible paths.\nTiling Global memory accesses exhibit higher latency due to the nature of DRAM. We can reduce the number of global memory accesses by using shared memory. This was exemplified in the tiled matrix multiplication examples, where there are many redundant data accesses. Moving these data to shared memory reduces the number of global memory accesses.\nThread Coarsening In cases where the hardware would serialize execution of a kernel, thread coarsening can eliminate redundant work. In the tiled matrix multiplication example, we saw that the hardware would serialize execution of the kernel if there were not enough resources available. In this case, the same redundant loads to shared memory would be performed. To reduce this overhead, we coarsened the thread by having a single kernel perform the work of multiple blocks.\nIdentifying Bottlenecks Knowing when to apply each of these optimization techniques comes down to understanding your application. The single most important step in optimizing your application is to identify the bottleneck. What resource is limiting the performance of your solution? Benchmarking and profiling are two techniques that can be used to identify these bottlenecks. We will begin learning these tools in the next lecture.\nThe Takeaway At this point, you have learned the basics of GPU programming with CUDA. You should be familiar with writing kernels, setting launch configurations, and compiling them. You should be familiar with a few optimization techniques that can be applied to your applications, but you are probably not confident in your ability to identify when they should be used.\nThe next module of this course will focus on problems for which a straightforward solution is not obvious. These are problems that come from other domains of computer science, such as graph theory and linear algebra. We will learn how to apply the techniques we have learned to these problems, and we will learn new techniques that are specific to these problems. Even though the applications themselves may be specific, the techniques used to optimize them are not.\nReferences Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. Programming Massively Parallel Processors: A Hands-on Approach. Fourth. Morgan Kaufmann. Stawinoga, Nicolai, and Tony Field. 2018. “Predictable Thread Coarsening.” Acm Transactions on Architecture and Code Optimization 15 (2): 23:1–23:26. https://doi.org/10.1145/3194242. ","date":1705260660,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705260660,"objectID":"6ee18308236b31e9d455cf9397924d55","permalink":"https://ajdillhoff.github.io/notes/gpu_performance_basics/","publishdate":"2024-01-14T13:31:00-06:00","relpermalink":"/notes/gpu_performance_basics/","section":"notes","summary":"Table of Contents Memory Coalescing Hiding Memory Latency Thread Coarsening Optimization Checklist Identifying Bottlenecks The Takeaway These notes are on \u0026ldquo;Chapter 6: Performance Considerations\u0026rdquo; from the book Programming Massively Parallel Processors (Hwu, Kirk, and El Hajj 2022).\nMemory Coalescing Global memory accesses are one of the largest bottlenecks in GPU applications. DRAM has high latency based on its design. Each cell has a transistor and a capacitor. If the capacitor is charged, it represents a 1.","tags":["gpgpu","computer science"],"title":"GPU Performance Basics","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Memory Access Memory Types Tiling Example: Tiled Matrix Multiplication Boundary Checking Memory Use and Occupancy Dynamically Changing the Block Size The Takeaway Introduction So far, the kernels we have used assume everything is on global memory. Even though there are thousands of cores that can effectively hide the latency of transferring data to and from global memory, we will see this delay will become a bottleneck in many applications. These notes explore the different types of memory available on the GPU and how to use them effectively.\nMemory Access Transferring memory is one of the biggest bottlenecks in GPU programming. Companies like NVIDIA devote a lot of resources to improving the bandwidth and latency of memory transfers. When training a deep learning model, the datasets used are far too large to fit on the GPU. This means that the data must be transferred to the GPU before the actual training code can execute on the device. Training large models can take days or weeks, so the time spent transferring data can be significant.\nThe example provided in Chapter 5 of \u0026ldquo;Programming Massively Parallel Processors\u0026rdquo; is a great introduction to understanding memory access efficiency (Hwu, Kirk, and El Hajj 2022). In matrix multiplication, the data accesses are limited to a single line of code in the inner-most loop. This means that the memory access pattern is very regular and predictable. The example code is shown below:\nfor (int k = 0; i k \u0026lt; numCols; k++) { Cvalue += A[row * numCols + k] * B[k * numCols + col]; } This line consists of a floating-point multiplication, floating-point addition, and two memory accesses. Note that we are not storing the result yet, so there is no access to the C matrix. The operation effiency can be described in terms of floating-point operations per second (FLOP/s) and the accesses can be measured in the number of bytes transferred. In this case, we have 2 FLOPs and 8 bytes transferred. This means that the ratio of FLOPs to bytes transferred is 0.25 FLOP/B. This is described as computational intensity.\nWith this definition, we get a clearer picture on how to improve the performance of our code. If our kernel relies on too many memory accesses, then the computational intensity will be low. This means that the GPU will be spending more time waiting for data to be transferred than actually performing computations. The goal is to increase the computational intensity as much as possible.\nTo put this in perspective, the H100 SXM5 has 3TB/s of memory bandwidth. This global memory bandwidth limits the kernel to 3000 * 0.25 = 750 GFLOP/s. The peak performance of the H100 is 66.9 TFLOPS. If the specialized Tensor cores are utilized, the peak performance is 494.7 TFLOPS. That means that are kernel is only using 0.15% of the peak performance of the GPU. This program is certainly memory bound. Our theoretical limit to computational intensity is the peak performance of the GPU. Programs that achieve this peak are called compute bound.\nBased on the tools we have discussed so far, it is not clear how we can optimize this kernel. The only way to improve the computational intensity is to reduce the number of memory accesses. Modern GPUs have more than just global memory. The next section will explore the different types of memory available on the GPU.\nMemory Types Global Memory Local Memory Resides on global memory, but is not shared between threads. This includes local variables and function arguments. Shared Memory Resides on the chip. Allocated to thread blocks. Shared between threads in the same block. Constant Memory Registers Resides on the chip. Each thread has its own registers. Very fast memory. Data in CPU registers are swapped depending on the context of the program. GPU registers are consistent even when other threads are launched to hide latency. This results in a larger register file on the GPU.\nFollowing the von Neumann architecture, memory that is closer to the chip is faster but more expensive. Data residing on registers is the most ideal for performance since the processor can work directly with the register values. This benefit comes in the form of energy consumption as well. Transferring data from global memory to the chip requires additional cycles resulting in more energy used.\nWhen a private variable is declared in a kernel, every single thread will have its own copy of that variable.\nVariable declaration Memory Scope Lifetime Automatic variables (not arrays) Register Thread Grid Automatic array variables Local Thread Grid __device__ __shared__ int SharedVar; Shared Block Grid __device__ int GlobalVar; Global Grid Application __device__ __constant__ int ConstVar; Constant Grid Application Automatic array variables should seldom be used. It may have seemed convenient to use a static array for computing channel-specific values in an image processing kernel, but it is more efficient to use three separate variables. Each variable will be allocated to a register resulting in faster access times.\nGlobal variables are more commonly used to pass information to another kernel that is being launched.\nTiling These memory types serve as tools that we can use to increase efficiency. The first pattern discussed is tiling. Throughout the rest of the course, we will add many more patterns to our repertoire. Tiling is a well-described technique that has a fitting analogy. If a wall needs to be tiled, it is more efficient to use many small tiles that are lighter and easier to handle. In GPU programming, the wall represents the entire global memory space. The individual tiles are local memory that is allocated to each thread block.\nFigure 1: Global memory access pattern (source: NVIDIA DLI). The kernels we have seen so far have used a global memory access pattern. In this pattern, all threads have access to every data point from the input. Using a tiling pattern, we can optimize memory accesses by moving shared resources to local memory that is faster to access.\nFigure 2: Tiling pattern (source: NVIDIA DLI). The tool itself is quite simple in concept, but the challenge will be identifying when the tool can be properly applied. Consider matrix multiplication. The naive kernel we explored previously uses each thread to compute one value of the output matrix. This kernel uses a global memory access pattern, and we can identify that many of the computations require the same input. They key to introducing tiling for matrix multiplication will be identifying which data use reused.\nFigure 3: Memory accesses for matrix multiplication (source: NVIDIA DLI). In the figure above, the block size is \\(2 \\times 2\\). Each row of the block relies on the same input row from the matrix on the left. That is, \\(P_{0,0}\\) and \\(P_{0,1}\\) will use the same data from the first row of \\(M\\). In our original kernel, this requires 8 global memory accesses. If we placed this row in shared memory, each output thread could access the values much quicker. We can see a similar pattern for the column values in \\(N\\).\nSince we are using tiling with a block size of \\(B\\), we will consider working with \\(2B\\) values from the input at a time. If the number of values we need to compute an output entry exceeds \\(2B\\), then we can synchronize the threads before moving to the next section.\nFigure 4: Tiled matrix multiplication overview (source: NVIDIA DLI). Verify that the potential reduction in global memory traffic in matrix multiplication is proportional to the dimension of the blocks used. Verify that the reduction is by a factor of \\(N\\) if the tiles are \\(N \\times N\\). Example: Tiled Matrix Multiplication The concept of tiled matrix multiplication is this: load a subset of data from \\(M\\) and \\(N\\) into shared memory before using that data to perform the dot product. We have a few limitations to think about here. First, the amount of shared memory is much smaller than global memory; we cannot fit all the data at once. Second, the block size will limit how many elements can be loaded into shared memory at once. As suggested by tiling, we are only working with a small chunk at a time.\nUsing a \\(2 \\times 2\\) block gives us 4 threads to work with. Overlaying that block on the input only allows us to grab 2 values from the first 2 rows in \\(M\\) and 2 values from the first 2 columns in \\(M\\). For each tile, the subset of data will be loaded in followed by adding the dot product of the subset to the current value.\nFigure 5: Loading the first tile (source: NVIDIA DLI). Figure 6: Computing the dot product of the first subset (source: NVIDIA DLI). In this example, the block will move to the next subset of data to finish computing the first block of the output matrix. This process can be arbitrarily scaled up to support larger matrices without necessarily increasing the block size. Although, we would want to increase the block size to take advantage of the additional threads. The figure below shows a table of the computations required for each phase.\nFigure 7: Tiled matrix multiplication computations (source: NVIDIA DLI). Check your understanding\nBy using tiling with a block size of \\(B \\times B\\), what is the total reduction in global memory traffic?\nImplementation in CUDA Our implementation should follow these steps:\nEstablish shared memory for the input from matrix \\(M\\) and matrix \\(N\\). Load the first subset of data from \\(M\\) and \\(N\\) into shared memory (remember to synchronize threads). Compute the dot product of the subset (remember to synchronize threads). Repeat steps 2 and 3 until all subsets have been computed. Step 1 is obvious. We need to establish the shared memory for this solution. Steps 2 and 3 are the same as described above, but we do need to remember to synchronize the threads. Without synchronization, the computation may continue before all the data is properly loaded. Step 4 implies that each thread will loop through the subsets until all values have been computed. The kernel is shown below.\n__global__ void MatMulKernel(float* M, float* N, float* P, int Width) { // Block index int bx = blockIdx.x; int by = blockIdx.y; // Thread index int tx = threadIdx.x; int ty = threadIdx.y; __shared__ float Mds[TILE_WIDTH][TILE_WIDTH]; __shared__ float Nds[TILE_WIDTH][TILE_WIDTH]; // Identify the row and column of the P element to work on int Row = by * TILE_WIDTH + ty; int Col = bx * TILE_WIDTH + tx; float Pvalue = 0; for (int ph = 0; ph \u0026lt; Width / TILE_WIDTH; ++ph) { // Collaborative loading of M and N tiles into shared memory Mds[ty][tx] = M[Row * Width + ph * TILE_WIDTH + tx]; Nds[ty][tx] = N[(ph * TILE_WIDTH + ty) * Width + Col]; __syncthreads(); for (int k = 0; k \u0026lt; TILE_WIDTH; ++k) { Pvalue += Mds[ty][k] * Nds[k][tx]; } __syncthreads(); } P[Row * Width + Col] = Pvalue; } Let\u0026rsquo;s break this down with a small example. Consider multiplying two \\(4 \\times 4\\) matrices. We will use a block size of \\(2 \\times 2\\), as seen in the figure below. Our block will compute the top left submatrix of the output, \\(P_{0,0}\\), \\(P_{0,1}\\), \\(P_{1,0}\\), and \\(P_{1,1}\\). We will view the computation from the perspective of the thread for \\(P_{0,0}\\).\nFigure 8: Setup of tiled matrix multiplication example. The row and column of the output computed by the current thread is calculated using the block and thread indices. Of course, this is simply \\((0, 0)\\) for the first thread. It gets slightly more complicated when computing the input subset in the loop. The input needs to be transferred to shared memory. The loop will skip over a tile at a time. At this point, we already know which row of \\(M\\) and column of \\(N\\) we need to access. We need to compute the column index for \\(M\\) and the row index for \\(N\\).\nFor \\(M\\), we start with Row * Width. This needs to be offset by the tile offset index ph of the main loop, yielding Row * Width + ph * TILE_WIDTH. Finally, we need to add the thread index tx to get the final index Row * Width + ph * TILE_WIDTH + tx. The same process is applied to \\(N\\). Note that this only transfers a single value from each matrix to shared memory, but our computation relies on 2 values from each matrix. Each thread in the block is collaboratively loading the data into shared memory. This is why the call to __syncthreads() is necessary.\nSpecifically, the thread for \\(P_{0, 0}\\) copies \\(M_{0, 0}\\) and \\(N_{0, 0}\\) to shared memory. The thread for \\(P_{0, 1}\\) copies \\(M_{0, 1}\\) and \\(N_{1, 0}\\) to shared memory. The thread for \\(P_{1, 0}\\) copies \\(M_{1, 0}\\) and \\(N_{0, 1}\\) to shared memory. Finally, the thread for \\(P_{1, 1}\\) copies \\(M_{1, 1}\\) and \\(N_{1, 1}\\) to shared memory.\nThe next step is to compute the dot product of the subset. Again, we see a call to __syncthreads(). Without this synchronization, the loop may be allowed to continue and overwrite the data in shared memory before a thread has finished. Once the final value is computed, each thread can freely write it back to global memory. Since each thread is computing a different value, there is no need to synchronize the threads before writing to global memory.\n\\begin{align*} P_{0, 0} \u0026amp;+= 2 \\times 2 + 1 \\times 1 \\\\ P_{0, 1} \u0026amp;+= 2 \\times 1 + 1 \\times 0 \\\\ P_{1, 0} \u0026amp;+= 1 \\times 2 + 0 \\times 1 \\\\ P_{1, 1} \u0026amp;+= 1 \\times 1 + 0 \\times 0 \\end{align*}\nFigure 9: Updated values for the first subset. The next iteration of the loop will grab the next subset of the data and repeat the process. The result after this step is shown below.\nFigure 10: Updated values for the second subset. To summarize, ph is the tile offset index, Row and Col are the row and column of the output computed by the current thread, and tx and ty will give the offset with respect to the current tile.\nThe kernel above has an outer loop that calls another loop managed by thread synchronization, breaking the computation up into several distinct phases. This is called strip-mining and is an important part of tiling. This existed even before GPUs were used (Hwu, Kirk, and El Hajj 2022).\nPerformance Analysis In the naive implementation, we had a computational intensity of 0.25 FLOP/B. With a \\(16 \\times 16\\) tile, the number of global memory accesses is reduced by a factor of 16. This gives us a computational intensity of 4 FLOP/B. We previously stated that the H100 has a global memory bandwidth of 3TB/s. This means that the theoretical limit for the performance of this kernel is 3000 * 4 = 12000 GFLOP/s which is much better than the 750 GFLOP/s we had before.\nThis is not the most optimal way to implement matrix multiplication, and you should always refer to the cuBLAS library for matrix operations. The purpose of this example is to demonstrate the use of tiling.\nBoundary Checking The previous implementation assumed that the width of the matrices was a multiple of the tile width and that the input would always be square matrices. Consider changing our \\(2 \\times 2\\) block to a \\(3 \\times 3\\) block using the same input sizes.\nFigure 11: Using a 3x3 block (source: NVIDIA DLI). Our implementation would follow the same process for the first subset of pattern. An issue arises when computing the second tile offset since the block exceeds the boundaries of our input and output. One solution would be to check the boundary condition on both the input, when transferring the data to shared memory, and the output, when reading the data from shared memory. This would require a conditional statement in the inner loop. This is not ideal since the conditional statement would be executed for every thread in the block.\nAnother solution is to pad the input with zeros. If the index is outside our boundary, adding a 0 will not affect the result of the dot product. This allows for a simpler implementation while still being flexible enough to handle matrices of any size. The relevant portion of the kernel is shown below.\nfloat Pvalue = 0; for (int ph = 0; ph \u0026lt; ceil(Width/(float)TILE_WIDTH); ph++) { // Collaborative loading of M and N tiles into shared memory if (Row \u0026lt; Width \u0026amp;\u0026amp; ph * TILE_WIDTH + tx \u0026lt; Width) { Mds[ty][tx] = M[Row * Width + ph * TILE_WIDTH + tx]; } else { Mds[ty][tx] = 0.0; } if (ph * TILE_WIDTH + ty \u0026lt; Width \u0026amp;\u0026amp; Col \u0026lt; Width) { Nds[ty][tx] = N[(ph * TILE_WIDTH + ty) * Width + Col]; } else { Nds[ty][tx] = 0.0; } __syncthreads(); } The rest of the kernel remains the same. In Lab 2, you will implement this and adapt it to work with non square matrices as well.\nMemory Use and Occupancy Just like exceeding the number of registers per thread can negatively affect occupancy, so can over allocating shared memory. The H100 can have up to 228 KB per SM. If we are maximizing the 2048 threads available per SM, each block cannot exceed 228 KB / 2048 threads = 112 B/thread.\nHow much shared memory is used by each block? Each block has 2 arrays of size \\(TILE\\_WIDTH \\times TILE\\_WIDTH\\) of type float. This gives us a total of \\(2 \\times TILE\\_WIDTH \\times TILE\\_WIDTH \\times 4 = 8(TILE\\_WIDTH)^2\\) B. Each block uses \\(TILE\\_WIDTH^2\\) threads, resulting in 8 B/thread. This is well below the limit of 112 B/thread.\nDynamically Changing the Block Size The solution presented above uses a constant to determine the tile size. What if this tile size was not optimal for a given hardware configuration? We would surely want to adjust this dynamically to maximize performance. In CUDA, we can support this by using the extern keyword. First, we need to define our shared memory as one array: extern __shared__ float Mds_Nds[];. This is a 1D array that represents the shared memory for both input matrices.\nWhen launching this kernel, we need some way to inform it of the tile size. First, we would query the device properties and determine the optimal tile size based on the hardware. This size can be used as a third launch configuration input, as shown below. Additionally, the size of the shared memory for each input matrix is provided as two additional arguments to the kernel.\nsize_t size = compute_optimal_size(); // Determine optimal tile size MatMulKernel\u0026lt;\u0026lt;\u0026lt;dimGrid, dimBlock, size\u0026gt;\u0026gt;\u0026gt;(M_d, N_d, P_d, Width, size/2, size/2); The kernel will need to be modified to use the new shared memory array. The first step is to determine the offset for each matrix. This is done by multiplying the tile size by the thread index. The second step is to use the offset to access the correct value in the shared memory array. The kernel is shown below.\n__global__ void MatMulKernel(float* M, float* N, float* P, int Width, int Mds_offset, int Nds_offset) { extern __shared__ float Mds_Nds[]; float *Mds = (float *)Mds_Nds; float *Nds = (float *)Mds_Nds + Mds_offset; // Rest of the kernel } Completing this modification would require us to use linear indexing for Mds and Nds.\nThe Takeaway Tiling is a powerful tool that can be used to improve the performance of a kernel. It is important to understand the memory access pattern of your kernel and identify which data is reused. This will allow you to move that data to shared memory and reduce the number of global memory accesses. Tiling is the first of many patterns that we will explore. Just like not every tool is useful for every job, not every pattern will be useful for each problem we face. Increasing the number of tools, or patterns, that we have available will allow us to solve a wider range of problems efficiently.\nReferences Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. Programming Massively Parallel Processors: A Hands-on Approach. Fourth. Morgan Kaufmann. ","date":1705007220,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705007220,"objectID":"93612ae6d1df15d78de8d253c4df7026","permalink":"https://ajdillhoff.github.io/notes/cuda_memory_architecture/","publishdate":"2024-01-11T15:07:00-06:00","relpermalink":"/notes/cuda_memory_architecture/","section":"notes","summary":"Table of Contents Introduction Memory Access Memory Types Tiling Example: Tiled Matrix Multiplication Boundary Checking Memory Use and Occupancy Dynamically Changing the Block Size The Takeaway Introduction So far, the kernels we have used assume everything is on global memory. Even though there are thousands of cores that can effectively hide the latency of transferring data to and from global memory, we will see this delay will become a bottleneck in many applications.","tags":["gpgpu","computer science"],"title":"CUDA Memory Architecture","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Architecture Block Scheduling Synchronization Warps Control Divergence Warp Scheduling Resource Partitioning Dynamic Launch Configurations The Takeaway Architecture A GPU consists of chip that is composed of several streaming multiprocessors (SMs). Each SM has a number of cores that execute instructions in parallel. The H100, seen below, has 144 SMs (you can actually count them by eye). Each SM has 128 FP32 cores for a total of 18,432 cores. Historically, CUDA has used DDR memory, but newer architectures use high-bandwidth memory (HBM). This is closely integrated with the GPU for faster data transfer.\nIn the image below, you can see 6 HBM3 memory modules surrounding the GPU, 3 on either side of the die. HBM3 is capable of 3 TB/s of bandwidth. The platform shown only uses 5 of these modules. The full version will utilize all 6.\nFigure 1: NVIDIA H100 GPU with 144 SMs (NVIDIA). Block Scheduling When a kernel is launched, the blocks that we configure in our code are assigned to SMs. All threads in each block will be assigned to each SM. Depending on the platform, the number of blocks that can be assigned to an SM will vary. This is discussed in more detail below. Since all threads in a block are on the same SM, they can share data and communicate with each other.\nSynchronization Threads that run on the same block can be synchronized using __syncthreads(). This is a pretty straightforward concept, but it is important to understand the caveats. When a kernel reaches this call, the execution of the threads will stop until all of them have reached that point. This construct is typically used when threads need to share data or are dependent on the results of other threads.\nBe careful on using this call. An example of incorrect usage is shown below.\n__global__ void kernel(int *a, int *b, int *c) { if (threadIdx.x % 2 == 0) { // Perform some work __syncthreads(); else { // Perform some other work __syncthreads(); } } Unlike a general-purpose processor, a GPU does not have control hardware for each individual core. This means that all threads must execute the same instructions using shared resources. In the example above, it is possible for some threads to branch off into a different part of the program. However, only one of the paths can be executed based on this limitation. This is called control divergence and is discussed in more detail below.\nEven though the call looks the same, each __syncthreads() is different. The first call will only synchronize the threads that executed the first path. The second call will only synchronize the threads that executed the second path. The result is either undefined output or a deadlock, in which the threads will never reach the second call.\nSince threads in separate blocks cannot be synchronized, the blocks can be executed in any arbitrary order. You might immediately ask yourself how a complex problem that requires synchronization between all parts of the data can get around this limitation. We will explore more complex patterns and their solutions in later sections.\nWarps Figure 2: Warps across several blocks (credit: NVIDIA DLI). Streaming Multiprocessors in a CUDA chip execute threads in a group of 32 called warps. Since Compute Capability 1.0, the warp size has not changed. When a block is assigned to an SM, it is divided into warps. Given this size, you can easily determine the number of warps assigned to an SM. For example, if you have a block of 256 threads, the SM has 256 / 32 = 8 warps. If the block size is not evenly divisible by the number of warps per SM, the last warp will be padded with inactive threads.\nWhen multi-dimensional thread blocks are assigned to an SM, the threads are linearly mapped in a row-major order before being partitioned into warps. For example, a 2D block of \\(16 \\times 16\\) threads will be mapped to a 1D array of 256 threads. The first 32 threads will be assigned to the first warp, the next 32 to the second warp, and so on.\nWarps are executed following the Single-Instruction, Multiple-Data (SIMD) model. There is a single program that runs the same instruction on all threads in the same order. If a thread would have executed a different path based on its input data, it would not be executed with the others. This is called control divergence and is explained in the next section.\nFigure 3: SM layout (source: NVIDIA DLI) The advantage of this model is that more physical space can be dedicated to ALUs instead of control logic. In a traditional CPU, each processing core would have its own control logic. The tradeoff is that different cores can execute their own programs at varying points in time.\nControl Divergence Since a traditional CPU has separate control logic for each core, it can execute different programs at the same time. If the program has a conditional statement, it does not need to worry about synchronizing instructions with another core. This is not the case with a GPU. Since every thread in a warp executes the same instruction, only threads that would execute the same path can be processed at the same time. If a thread would execute a different path, it is not executed with the others. This is called control divergence.\nWhat exactly happens then if a warp has 32 threads of which only 16 would execute the same path? Simply, multiple passes are made until all possible paths of execution are considered based on the divergence of the threads. The SM would process the first 16 threads that all follow the same path before processing the second 16 threads.\nThis also applies to other control flow statements such as loops. Consider a CUDA program that processes the elements of a vector. Depending on the loop and data used, the threads may execute a different number of iterations. As threads finished their iterations, they would be disabled while the remaining threads continue.\nThere are some cases in which it is apparent that your program will exhibit control divergence. For example, if you have a conditional statement based on the thread index, you can be sure that the threads will execute different paths.\nExample Consider a \\(200 \\times 150\\) image that is processed by a CUDA program. The kernel is launched with \\(16 \\times 16\\) blocks which means there are \\(200 / 16 = 13\\) blocks in the x-direction and \\(150 / 16 = 10\\) blocks in the y-direction. The total number of blocks is \\(13 \\times 10 = 130\\). Each block has 256 threads, or 8 warps. That means that the total number of warps is \\(130 \\times 8 = 1040\\).\nWarp Scheduling An SM can only execute instructions for a small number of warps. The architecture allows for more warps than the SM can execute since warps will often be waiting for some result or data transfer. Warps are selected based on a priority mechanism. This is called latency tolerance.\nZero-overhead thread scheduling allows for selecting warps without any overhead. A CPU has more space on the chip for caching and branch prediction so that latency is as low as possible. GPUs have more floating point units and can switch between warps, effectively hiding latency.\nThe execution states for all assigned warps are stored in the hardware registers, eliminating the need to save and restore registers when switching between warps.\nUnder this model, it is ideal for an SM to be assigned more threads than it can execute at once. This increases the odds that the SM will have a warp ready to execute when another warp is waiting for data.\nResource Partitioning There is a limit on the number of warps that an SM can support. In general, we want to maximize the throughput of an SM by assigning as many warps as possible. The ratio of warps assigned to the number of warps an SM supports is called occupancy. If we understand how the architecture partitions the resources, we can optimize our programs for peak performance. Consider the NVIDIA GH100 GPU, pictured below.\nFigure 4: GH100 Full GPU with 144 SMs (NVIDIA). The H100 architecture shares the same limitations in compute capability as the A100, so this example will follow the book closely (Hwu, Kirk, and El Hajj 2022). The H100 supports 32 threads per warp, 64 warps per SM, 32 blocks per SM, and 2048 threads per SM. Depending on the block size chosen, the number of blocks per SM will differ. For example, a block size of 256 threads means that there are 2048 / 256 = 8 blocks per SM. This block size would maximize occupancy since the architecture supports more than 8 blocks per SM. Also, the number of threads per block is less than the limit of 1024.\nWhat if we chose 32 threads per block? Then there would be 2048 / 32 = 64 blocks per SM. However, the device only supports 32 blocks per SM. With only 32 blocks allocated with 32 threads per block, a total of 1024 threads would be utilized. The occupancy in this case is 1024 / 2048 = 50%.\nHistorically, NVIDIA provided an excel spreadsheet to compute occupancy. It has since been deprecated in favor of Nsight Compute, a tool that provides more information about the performance of your program. We will cover this tool in a later section.\nIncluding Registers Another factor for occupancy is the number of registers used per thread. The H100 has 65,536 registers available for use. As long as your program does not use more than this, you can follow the simpler occupancy calculation from above. With 2048 threads, that leaves 65,536 / 2048 = 32 registers per thread. If we run a program with 256 threads/block, there would be 2048 / 256 = 8 blocks per SM. This means that there are 8 * 256 = 2048 threads per SM. With 31 registers per thread, the total number of registers used per SM is 2048 * 31 = 63,488. In this case we still maximize occupancy since 63,488 \u0026lt; 65,536.\nWhat if each thread required 33 registers? In that case, the total number of registers used per SM would be 2048 * 33 = 67,584. How would these resources be partitioned? Only 7 blocks could be assigned since 7 * 256 * 33 = 59,136 \u0026lt; 65,536. This means that only 7 * 256 = 1792 threads would be used, reducing the occupancy to 1792 / 2048 = 87.5%.\nDynamic Launch Configurations Depending on our application requirements, we may need to support a range of devices across several compute capabalities. The CUDA API makes this simple by providing several different functions for querying device properties. These can be called from the host before configuring and launching a kernel. This is not an exhaustive list, but it covers the most important properties. When we first launch a program that utilizes CUDA, we will want to know how many devices are available. Later in this course, we will develop programs that utilize multiple GPUs, but we would also want our code to adapt dynamically to a single GPU.\nint deviceCount; cudaGetDeviceCount(\u0026amp;deviceCount); Once the device count is known, the properties of each device can be acquired with cudaGetDeviceProperties. This function takes a pointer to a cudaDeviceProp struct. The struct contains several properties that can be used to configure the kernel launch. The most important properties are listed below. A full list can be found in the CUDA documentation.\nProperty Description name Name of the device totalGlobalMem Total amount of global memory available on the device in bytes sharedMemPerBlock Shared memory available per block in bytes regsPerBlock 32-bit registers available per block warpSize Warp size in threads maxThreadsPerBlock Maximum number of threads per block maxThreadsDim Maximum size of each dimension of a block maxGridSize Maximum size of each dimension of a grid multiProcessorCount Number of SMs on the device maxThreadsPerMultiProcessor Maximum number of threads per SM The following example iterates through all devices and queries their properties.\nfor (int i = 0; i \u0026lt; deviceCount; i++) { cudaDeviceProp prop; cudaGetDeviceProperties(\u0026amp;prop, i); // Use properties to configure kernel launch } The Takeaway The CUDA architecture is designed to maximize the number of threads that can be executed in parallel. This is achieved by partitioning the resources of the GPU into SMs. Each SM can execute a small number of warps at a time. The number of warps that can be assigned to an SM is called occupancy. The occupancy is determined by the number of threads per block, the number of blocks per SM, and the number of registers used per thread. The CUDA API provides functions for querying device properties so that the kernel launch can be configured dynamically.\n","date":1704768540,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704768540,"objectID":"c44986281716bc0f5bd3d54cb595f5c0","permalink":"https://ajdillhoff.github.io/notes/cuda_architecture/","publishdate":"2024-01-08T20:49:00-06:00","relpermalink":"/notes/cuda_architecture/","section":"notes","summary":"Table of Contents Architecture Block Scheduling Synchronization Warps Control Divergence Warp Scheduling Resource Partitioning Dynamic Launch Configurations The Takeaway Architecture A GPU consists of chip that is composed of several streaming multiprocessors (SMs). Each SM has a number of cores that execute instructions in parallel. The H100, seen below, has 144 SMs (you can actually count them by eye). Each SM has 128 FP32 cores for a total of 18,432 cores.","tags":["gpgpu","computer science"],"title":"CUDA Architecture","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Summary Multidimensional Grid Organization Example: Color to Grayscale No longer embarrassing: overlapping data Matrix Multiplication What\u0026rsquo;s Next? Summary The CUDA Programming model allows us to organize our data in a multidimensional grid. The purpose of this is primarily for our own convenience, but it also allows us to take advantage of the GPU\u0026rsquo;s memory hierarchy. In Lab 0, we only required a single dimension for our grid as well as each block since the input was a vector. When performing computations on multidimensional data like matrices, we can match the dimensions of our launch configuration to the dimensions of our data.\nMultidimensional Grid Organization All threads share a block index, blockIdx, and a thread index, threadIdx. These indices are three-dimensional vectors of type dim3. The dim3 type is defined as follows:\nstruct dim3 { unsigned int x, y, z; }; Each grid is a 3D array of blocks, and every block a 3D array of threads. Consider the kernel execution for vecAdd from Lab 0:\ndim3 blocksPerGrid(32, 1, 1); dim3 threadsPerBlock(128, 1, 1); vecAdd\u0026lt;\u0026lt;\u0026lt;blocksPerGrid, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(a_d, b_d, c_d, n); This will execute with \\(32 \\times 128 = 4096\\) threads.\nIf our input is a matrix, we should organize our launch dimensions to match its 2D structure. We seemingly have two options: either the grid size or the block size. Consider the figure below, there are 4 blocks in the grid, each with 16 threads organized as a \\(4 \\times 2 \\times 2\\) volume.\nFigure 1: A 2D grid of blocks, each with 16 threads arranged in a 3D configuration (source: NVIDIA DLI). Under such a configuration, we would make use of gridDim.x, gridDim.y, and gridDim.z to access the dimensions of the grid. The dimensions of the block would be accessed with blockDim.x, blockDim.y, and blockDim.z. The thread indices would be accessed with threadIdx.x, threadIdx.y, and threadIdx.z. Would this be the best way to organize our launch configuration? Not exactly. We have no use for the 3D structure if we are only working with matrices.\nConsider an \\(n \\times m\\) matrix. If the matrix is small enough, we could launch a single block with a 2D arrangement of threads to perform the necessary computation. For larger matrices, we would optimally split the work into multiple blocks. This would allow us to perform more work in parallel. Let \\(n=62\\) and \\(m=76\\). If we chose a \\(16 \\times 16\\) block size, we would need \\(4 \\times 5 = 20\\) blocks to cover the entire matrix, as shown in the figure below.\nFigure 2: A 2D grid of blocks, each with 16 threads arranged in 2D (source: NVIDIA DLI). Notes on Compute Capability It is more important to dynamically adjust the grid size so that your program can adapt to varying input sizes. As of CC 9.0, the maximum number of threads a block can have is 1024, this means that a \\(32 \\times 32\\) block is the largest we can do for matrix data.\nIf the input matrix is smaller than \\(32 \\times 32\\), then only a single block is needed. The additional threads allocated to that block will be inactive for indices outside the range of our input.\nIf the input matrix is larger than \\(32 \\times 32\\), additional blocks should be added to the grid to accommodate the increased size. It is safe to keep the block size fixed, but the grid size must be dynamic.\nOptimal Launch Parameters Is it better to have fewer blocks that maximize the amount of threads per block? Or is it better to have more blocks with fewer threads per block? The current maximum number of threads per block is 1024. In practice, a maximum block dimension size of 128 or 256 is ideal. This has more to do with the specific problem and the amount of shared memory required. You will explore this question in Lab 1.\nExample: Color to Grayscale Given the layout just described, we will write a kernel that converts a color image to grayscale. This is an embarrassingly parallel problem since each pixel can be converted independently of the others. We will use the following formula to convert each pixel:\ngray = 0.299f * red + 0.587f * green + 0.114f * blue A CPU implementation would require a for loop over the exact number of pixels. The CUDA kernel for this is straightforward since it only depends on the current pixel. The only real challenge is to compute the correct indices for each thread.\n__global__ void colorToGrayscale(unsigned char *rgbImage, unsigned char *grayImage, int numRows, int numCols) { int x = blockIdx.x * blockDim.x + threadIdx.x; int y = blockIdx.y * blockDim.y + threadIdx.y; if (x \u0026gt;= numCols || y \u0026gt;= numRows) return; int index = y * numCols + x; int rgbOffset = index * 3; unsigned char r = rgbImage[rgbOffset]; unsigned char g = rgbImage[rgbOffset + 1]; unsigned char b = rgbImage[rgbOffset + 2]; float channelSum = 0.299f * r + 0.587f * g + 0.114f * b; grayImage[index] = channelSum; } In this example, we assume an RGB image where each pixel is represented by three unsigned characters. It is standard convention in C to pass a pointer to the first element of the array. This implies that we cannot use the [] operator to access the elements in a multidimensional way. Instead, we must compute the index ourselves. If you are not currently familiar with flat indexing, you certainly will be by the end of this course.\nIn C, multi-dimensional arrays are stored in row-major order. To compute the index of row j and column i in a 2D array, we need to skip over j rows and i columns. The total number of columns is the width of the array. The total number of rows is the height of the array. The index is computed as follows:\nint index = j * width + i; This is represented in the following figure.\nFigure 3: A 2D array stored in row-major order (source: NVIDIA DLI). Since the image is now represented as a flat 1D array, we can use the index computed above to access the correct pixel. The image is typically stored in the same row-major format, although this is not always the case. You should always check the documentation for the image format you are using.\nLaunch Configuration As stated above, we are going to launch 20 blocks in a \\(4 \\times 5\\) grid. Each block will have 256 threads arranged in a \\(16 \\times 16\\) 2D configuration. This totals to \\(20 \\times 256 = 5120\\) threads. The example figure above shows this configuration overlaid on a \\(76 \\times 62\\) image. That means we have 4712 pixels that need to be converted. The remaining 408 threads will be idle.\nYou might be wondering if all 5120 threads launch at the same time. What if the number of pixels exceeded the number of threads available on the GPU? The short answer is that the GPU will launch as many threads as possible, but the long answer is slightly more complicated and will be discussed in a later lesson.\nIn any case, our kernel can be launched using the following code:\ndim3 blockSize(16, 16, 1); dim3 gridSize(4, 5, 1); colorToGrayscale\u0026lt;\u0026lt;\u0026lt;gridSize, blockSize\u0026gt;\u0026gt;\u0026gt;(rgbImage, grayImage, numRows, numCols); No longer embarrassing: overlapping data At this point, you should have a basic understanding of how to solve problems that are embarrassingly parallel. Now comes the next step in shaping your parallel thinking skills. What if the thread relies on multiple data points that may be used by other threads. This is further complicated with problems that require some computation to complete before a thread can begin its work. Let\u0026rsquo;s take a step into deeper waters by looking at image blurring. This is a common technique used in image processing to reduce noise and detail. The basic idea is to replace each pixel with a weighted average of its neighboring pixels. The size of the neighborhood is called the kernel size. The kernel size is typically an odd number so that the pixel of interest is in the center of the neighborhood.\nThe core operation behind blurring is called a convolution. We will explore this operation in depth as it serves as a more advanced pattern for parallelism. For now, we will focus on the basic idea. Given a kernel size of \\(5 \\times 5\\) centered on a pixel, we will compute the weighted average of the 25 pixels in the neighborhood. To keep it simple, the weights will be uniform.\nFigure 4: A blurring kernel (red) centered on a pixel (source: NVIDIA DLI). Given a pixel location \\((x, y)\\), we can compute the index of the pixel in the neighborhood as follows:\nint index = (y + ky) * numCols + (x + kx); Where \\(ky\\) and \\(kx\\) are the row and column indices of the kernel. The kernel is centered on the pixel of interest, so \\(ky\\) and \\(kx\\) range from \\(-2\\) to \\(2\\). The total number of pixels in the neighborhood is \\(5 \\times 5 = 25\\). The weighted average is computed as follows:\nfloat sum = 0.0f; int numPixels = 0; for (int ky = -2; ky \u0026lt;= 2; ky++) { for (int kx = -2; kx \u0026lt;= 2; kx++) { if (x + kx \u0026lt; 0 || x + kx \u0026gt;= numCols) continue; if (y + ky \u0026lt; 0 || y + ky \u0026gt;= numRows) continue; int index = (y + ky) * numCols + (x + kx); sum += image[index]; numPixels++; } } image[y * numRows + x] = sum / numPixels; Some extra care will be needed to account for pixels outside the boundaries. There are several strategies to handle out-of-bounds pixels. The simplest is to ignore them. We will explore other strategies when discussing convolutions. In Lab 1, you will implement a blur kernel that can support a varying kernel size.\nMatrix Multiplication Matrix multiplication is one of the most important operations in linear algebra. Many high performance computing applications rely on it. It is one of the most widely called operations in deep learning, for example. Parallelizing this and other linear algebra operations has resulted in an explosion of research and applications ranging from computer vision to computational fluid dynamics. Exploring the parallelism of matrix multiplication will give us a deeper understanding of the CUDA programming model. It will also serve as a jumping off point for more advanced topics like shared memory and convolutional neural networks.\nDefinition Let \\(A = \\mathbb{R}^{m \\times n}\\) and \\(B = \\mathbb{R}^{n \\times p}\\) be two matrices. The product \\(C = AB\\) is defined as follows:\n\\[ C_{ij} = \\sum_{k=1}^n A_{ik} B_{kj}\\quad \\text{for } i = 1, \\ldots, m \\text{ and } j = 1, \\ldots, p \\]\nThis operation is only defined on compatible matrices. That is, the number of columns in \\(A\\) must equal the number of rows in \\(B\\). The resulting matrix \\(C\\) will have \\(m\\) rows and \\(p\\) columns.\nCPU Implementation The CPU implementation of matrix multiplication is straightforward. There is a double for loop to iterate through each element in the output matrix. The inner loop computes the dot product of the $i$th row of \\(A\\) and the $j$th column of \\(B\\). The dot product is computed by summing the element-wise product of the two vectors.\nvoid matrixMultiplyCPU(float *A, float *B, float *C, int m, int n, int p) { for (int i = 0; i \u0026lt; m; i++) { for (int j = 0; j \u0026lt; p; j++) { float sum = 0.0f; for (int k = 0; k \u0026lt; n; k++) { sum += A[i * n + k] * B[k * p + j]; } C[i * p + j] = sum; } } } GPU Implementation For a parallel implementation, we can reason that each thread should compute a single element of the output matrix. To compute element \\(C_{ij}\\), the thread needs access to row \\(i\\) from \\(A\\) and column \\(j\\) from \\(B\\). Each thread is simply computing the dot product between these two vectors. The figure below visualizes this process.\nFigure 5: Matrix multiplication (source: NVIDIA DLI). The output matrix is separated into blocks based on our block size. When writing the kernel, it is necessary to make sure that the index is not out of bounds.\n__global__ void matrixMultiplyGPU(float *A, float *B, float *C, int m, int n, int p) { int row = blockIdx.y * blockDim.y + threadIdx.y; int col = blockIdx.x * blockDim.x + threadIdx.x; if (row \u0026gt;= m || col \u0026gt;= p) return; float sum = 0.0f; for (int k = 0; k \u0026lt; n; k++) { sum += A[row * n + k] * B[k * p + col]; } C[row * p + col] = sum; } Launch Configuration The launch configuration is similar to the previous examples. We will launch a 2D grid of blocks, each with a 2D arrangement of threads. The block size will be \\(16 \\times 16\\) and the grid size will be \\(m / 16 \\times p / 16\\). The kernel is launched as follows:\ndim3 blockSize(16, 16, 1); dim3 gridSize((p + blockSize.x - 1) / blockSize.x, (m + blockSize.y - 1) / blockSize.y, 1); matrixMultiplyGPU\u0026lt;\u0026lt;\u0026lt;gridSize, blockSize\u0026gt;\u0026gt;\u0026gt;(A_d, B_d, C_d, m, n, p); What happens when the output matrix size exceeds the number of blocks per grid and threads per block? Either multiple kernels will be launched, each working with a submatrix of the original input, or each thread will be responsible for multiple elements.\nWhat\u0026rsquo;s Next? The complexity was slightly increased by considering multidimensional data. Matrices are a prime example of this. The algorithms explored required us to consider multiple input values to compute a single output value. However, the computation did not rely on any thread synchronization, so the task was still simple enough.\nBefore diving into more complex operations like thread synchronization, was need a better understanding of the GPU\u0026rsquo;s architecture and memory hierarchy. With this knowledge at our disposal, we can begin to optimize our kernels for maximum performance.\n","date":1704477360,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704477360,"objectID":"ca6fcd78bc53009d7dd847006f19c64c","permalink":"https://ajdillhoff.github.io/notes/multidimensional_grids_and_data/","publishdate":"2024-01-05T11:56:00-06:00","relpermalink":"/notes/multidimensional_grids_and_data/","section":"notes","summary":"Table of Contents Summary Multidimensional Grid Organization Example: Color to Grayscale No longer embarrassing: overlapping data Matrix Multiplication What\u0026rsquo;s Next? Summary The CUDA Programming model allows us to organize our data in a multidimensional grid. The purpose of this is primarily for our own convenience, but it also allows us to take advantage of the GPU\u0026rsquo;s memory hierarchy. In Lab 0, we only required a single dimension for our grid as well as each block since the input was a vector.","tags":["gpgpu","computer science"],"title":"Multidimensional Grids and Data","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Key Concepts Summary CUDA C Programs Example: Vector Addition Error Checking Key Concepts Task Parallelism vs. Data Parallelism kernels threads grids blocks global memory data transfer error checking compilation of CUDA programs Summary This topic introduces the basics of data parallelism and CUDA programming. The most important concept is that data parallelism is achieved through independent computations on each sample or groups of samples. The basic structure of a CUDA C program consists of writing a kernel that is executed independently on many threads. Memory must be allocated on the GPU device before transferring the data from the host machine (CPU). Upon completion of the kernel, the results need to be transferred back to the host.\nCUDA C Programs A basic CUDA program consists of:\nA kernel function defining the work to be performed on each thread. Data that is accessible on the device. Device memory allocation. Memory transfer from the host to the device. Execution of the kernel from the host machine. Data transfer from the device back to the host. Memory cleanup. At first glance, the execution flow of a CUDA program appears sequential; you launch the threads on the GPU and wait for it to complete. A more realistic program would launch the threads and continue local execution, if necessary.\nExample: Vector Addition Hwu et al. refer to vector addition as the \u0026ldquo;Hello World\u0026rdquo; of GPU programming (Hwu, Kirk, and El Hajj 2022). It is a simple problem that can be described as embarrassingly parallel. Vector addition is a simple operation. Given two vectors of the same length, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), the vector addition operation is defined as:\n\\[ \\mathbf{z}_i = \\mathbf{x}_i + \\mathbf{y}_i \\quad \\forall i \\in \\{1, \\ldots, n\\} \\]\nThe vector addition operation is commutative and associative. The operation can be performed in parallel on each element of the vectors. This can be implemented simply in C.\nvoid vecAdd(float *x_h, float *y_h, float *z_h, int n) { for (int i = 0; i \u0026lt; n; i++) { z_h[i] = x_h[i] + y_h[i]; } } One small note about the variable names: it is common to use the suffix `_h` to denote a variable that is allocated on the host (CPU) and `_d` to denote a variable that is allocated on the device (GPU). In this case, the vector addition operation is performed on the host machine.\nAn equivalent implementation in CUDA C is shown below.\n__global__ void vecAdd(float *x_d, float *y_d, float *z_d, int n) { int i = blockIdx.x * blockDim.x + threadIdx.x; if (i \u0026lt; n) { z_d[i] = x_d[i] + y_d[i]; } } This kernel executes on a single thread. The thread index is computed using built-in variables `blockIdx.x`, `blockDim.x`, and `threadIdx.x`. The details of how these variables are defined are not important right now. The main point is that each kernel is executed on a single thread. For a GPU with thousands of individual threads, this kernel will be executed thousands of times in parallel.\nThe __global__ keyword placed before the function definition indicates that the function can be called from both the host and the device, but it is only executed on the device. The table below shows the different keywords used to define functions in CUDA C.\nKeyword Description __global__ Executed on the device, callable from the host and device __device__ Executed on the device, callable from the device only __host__ Executed on the host, callable from the host only Unless otherwise specified, functions that you define will be executed on the host. That is, it is not necessary to specify the __host__ keyword. If you want the compiler to generate both host and device code, you can use the __host__ __device__ keyword combination.\nThe kernel is executed on the host machine using the following code.\nint main() { // Allocate memory on the host float *x_h, *y_h, *z_h; int n = 1024; x_h = malloc(n * sizeof(float)); y_h = malloc(n * sizeof(float)); z_h = malloc(n * sizeof(float)); // Allocate memory on the device float *x_d, *y_d, *z_d; cudaMalloc(\u0026amp;x_d, n * sizeof(float)); cudaMalloc(\u0026amp;y_d, n * sizeof(float)); cudaMalloc(\u0026amp;z_d, n * sizeof(float)); // Transfer data from host to device cudaMemcpy(x_d, x_h, n * sizeof(float), cudaMemcpyHostToDevice); cudaMemcpy(y_d, y_h, n * sizeof(float), cudaMemcpyHostToDevice); // Execute kernel vecAdd\u0026lt;\u0026lt;\u0026lt;ceil(n / 256.0), 256\u0026gt;\u0026gt;\u0026gt;(x_d, y_d, z_d, n); // Transfer data from device to host cudaMemcpy(z_h, z_d, n * sizeof(float), cudaMemcpyDeviceToHost); // Free memory on host and device free(x_h); free(y_h); free(z_h); cudaFree(x_d); cudaFree(y_d); cudaFree(z_d); } There is a lot to unpack here, so we\u0026rsquo;ll start from the top.\nMemory Allocation It doesn\u0026rsquo;t really matter where the host data comes from or how it is allocated, but the above example allocates memory using malloc anyway. Before transferring data to the device, we must allocate memory on it. This is done via cudaMalloc. The first argument is a pointer to address of the variable. Remember that taking the address of a pointer will result in a double pointer. This is necessary because the function will need to dereference the pointer to store the address to the allocated data. Once the memory is allocated on the device, it cannot be accessed from the host until it is transferred back.\nFigure 1: Overview of memory layout (source: NVIDIA DLI). The memory that is allocated on the device is called global memory. It is accessible by all threads on the device. There is also a small amount of shared memory that is accessible by threads within a single block along with a unified memory model.\nMemory Transfer Now that the memory has been allocated, the data can be safely transferred from the host to the device. This is accomplished using cudaMemcpy. The arguments are the destination pointer, source pointer, size, and direction. The direction is an enumerated type that can be one of the following:\ncudaMemcpyHostToDevice cudaMemcpyDeviceToHost cudaMemcpyDeviceToDevice We will only focus on the first two for now.\nGrids, Blocks, and Threads The CUDA programming model is based on a hierarchy of grids, blocks, and threads. A grid is a collection of blocks. A block is a collection of threads. The number of blocks and threads is defined by the programmer. The number of blocks and threads that can be executed in parallel is limited by the hardware. The number of blocks and threads that can be executed in parallel is called the grid size and block size, respectively.\nFigure 2: A single block of 256 threads (source: NVIDIA DLI). The figure above shows a single block of 256 threads. This could be one of many blocks in a grid. The threads within each block are executed in parallel and do not interact with threads in other blocks. For threads within a single block, there is a small amount of shared memory as well as other tools for communication. We will explore these in more depth as we dive into the details of the CUDA architecture.\nKernel Execution Calling the kernel function almost looks like any ordinary function call. The main difference is the inclusion of the \u0026lt;\u0026lt;\u0026lt; and \u0026gt;\u0026gt;\u0026gt; syntax. These are used to specify the size of the grid and blocks, respectively. In this example, we specified that each block has 256 threads. We can use that specification to dynamically determine the number of blocks based on the input size. The number of blocks is computed as the ceiling of the input size divided by the number of threads per block. This ensures that there are enough blocks to cover the entire input size.\nReturning to the kernel function, the thread index is computed using built-in variables blockIdx.x, blockDim.x, and threadIdx.x. These are defined as struct variables. Modern GPUs have a 3-dimensional grid, but we only need to worry about the first dimension for now. The thread index is computed as the product of the block index and the number of threads per block plus the thread index within the block. This is a common pattern for computing the thread index.\nYou may have noticed that it is possible to have more threads than there are blocks. As much as possible, you should try and work with powers of 2. This will ensure that the hardware is used as efficiently as possible. You can always request more threads than there are data points and ignore the threads that are not needed. In this example, we check to see if the thread index is less than the input size. If it is, the vector addition operation is performed. Otherwise, the function exits.\nThere are limits to the number of blocks and threads that can be executed in parallel. These limits are based on the compute capability of the device, referenced here.\nCompiling CUDA code is compiled using the NVCC compiler driver. It works by compiling host code using the host\u0026rsquo;s native C/C++ compiler and device code to PTX, the CUDA instruction set architecture. Each snippet of code is separated based on the CUDA keyword used to define it. For example, the __global__ keyword used to define the kernel function informs nvcc that it should be compiled to a PTX file.\nError Checking The functions we use in the CUDA API return an error code. We can use this to create robust code that checks for errors and either corrects them or exits gracefully. The following example shows a simple way to check the result of `cudaMalloc`:\ncudaError_t err = cudaMalloc(\u0026amp;x_d, n * sizeof(float)); if (err != cudaSuccess) { fprintf(stderr, \u0026#34;Error: %s\\n\u0026#34;, cudaGetErrorString(err)); exit(EXIT_FAILURE); } A common pattern is to define a macro that checks the result of a CUDA function and exits if there is an error. This is shown below.\n#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); } inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true) { if (code != cudaSuccess) { fprintf(stderr,\u0026#34;GPUassert: %s %s %d\\n\u0026#34;, cudaGetErrorString(code), file, line); if (abort) exit(code); } } A small note on the above macro, it is technically C++ code. As of writing this, CUDA does not support all features of C++, but much of the code you will see is written as a mix of C and C++. CUDA was originally developed for C, but C++ features have slowly been introduced over time. If you view the official documentation, you can see that the link is defined as `cuda-c-programming-guide`, but the actual document has been renamed to `CUDA C++ Programming Guide`.\nDon\u0026rsquo;t overthink the C/C++ distinction. The main point is that you can use C++ features in your CUDA code, but you should be aware that not all features are supported.\nReferences Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. Programming Massively Parallel Processors: A Hands-on Approach. Fourth. Morgan Kaufmann. ","date":1703968860,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703968860,"objectID":"16ba7b610805d7a0da12c970eb5d03d4","permalink":"https://ajdillhoff.github.io/notes/heterogeneous_data_parallel_computing/","publishdate":"2023-12-30T14:41:00-06:00","relpermalink":"/notes/heterogeneous_data_parallel_computing/","section":"notes","summary":"Table of Contents Key Concepts Summary CUDA C Programs Example: Vector Addition Error Checking Key Concepts Task Parallelism vs. Data Parallelism kernels threads grids blocks global memory data transfer error checking compilation of CUDA programs Summary This topic introduces the basics of data parallelism and CUDA programming. The most important concept is that data parallelism is achieved through independent computations on each sample or groups of samples. The basic structure of a CUDA C program consists of writing a kernel that is executed independently on many threads.","tags":["gpgpu","computer science"],"title":"Heterogeneous Data Parallel Computing","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Structure of the Course Heterogeneous Parallel Computing Measuring Speedup GPU Programming History Applications What to expect from this course Structure of the Course The primary of this goal is of course to learn how to program GPUs. A key skill that will be developed is the ability to think in parallel. We will start with simple problems that are embarrassingly parallel and then move on to more complex problems that require synchronization. One of the biggest challenges will be in converting processes that are simple to reason about in serial to parallel processes.\nThe course is divided into three parts. The first part will cover the fundamentals of heterogeneous parallel computing and the CUDA programming model. We will focus on problems that are mostly embarrassingly parallel, but will also step into more complicated problems.\nThe second part will cover primitive parallel patterns. These are patterns from well-known algorithms that can be used to solve a wide variety of problems. Think of these as useful blueprints for solving problems in parallel. During the second part, we will also dive into more advanced usages of CUDA.\nPart three will cover advanced patterns from more specific applications, such as iterative MRI reconstruction. The course will conclude with expert practices.\nThere will be regular assignments that focus on the concepts learned throughout the course. These will typically be accompanied by a series of questions to reinforce and verify that you are successful in each step. Quizzes will be given after each assignment to serve as a checkpoint.\nHeterogeneous Parallel Computing Heterogeneous computing refers to systems that use more than one kind of processor or core. One common theme in the course will be to focus on a perfect union between the CPU and GPU. Not every task can be fully parallelized. Many tasks are well suited for sequential processing and others are better suited for parallel processing. Parallelism can be further broken down into data parallelism and task parallelism. The majority of our time will be focused on data parallelism, but it is important to keep in mind that not everything fits into this category. Over time, you will develop a sense for what fits this paradigm and what does not.\nThe idea of parallelism is certainly not new, but it has become ubiquitous in the computing space. Consider 30 years ago, when most consumer computers had a single core. The race between chip designers resulted in increasing single-core performance year after year in the form of increased clock speeds. This was a great way to increase performance, but it came at the cost of increased power consumption and heat. Scaling down transistors has also be a tried and true way of decreasing processor size and increasing performance. However, we are quickly reaching a physical limit on the size of a transistor.\nThe solution to these problems is the same solution seen in scaling up large systems: horizontal scaling. The intuition is straightforward: many things can do the work faster than a single thing. For large-scale systems, the answer is distributed systems in which no single unit needs to be overly powerful or complicated. For consumer processors, this comes in the form of additional cores on a chip.\nIn the context of CPUs, adding multiple cores means that we have a multi-core homogeneous system. These are general-purpose processors that can complete any computational task. The cores are identical and can be used interchangeably. The cores are also tightly coupled, meaning that they share memory and can communicate with each other. A similar statement can be made for GPUs. Let\u0026rsquo;s take a look at the differences between them.\nLatency vs. Throughput CPUs follow a latency-first design. The space on the chip itself is not fully dedicated to the processing units. Instead, space is reserved for things like cache, branch prediction, and other features that reduce latency. All computational tasks can be completed on a CPU, but the throughput may be lower than a GPU.\nFigure 1: CPU Architecture from CUDA C++ Programming Guide. GPUs follow a throughput-first design. The space on the chip is dedicated to processing units such as ALUs. The cores themselves are not as sophisticated as those found on a CPU. Communication between cores takes more time and is more difficult, but having more of them means that the raw throughput of the chip is higher.\nThe development of GPUs was driven by the gaming industry, specifically with rendering, where many vertices and pixels need to be processed in parallel. As we explore GPU solutions to different problems, we will see that data delivery is a key bottleneck. There are techniques available to get around this, which we will need to study closely.\nFigure 2: GPU Architecture from CUDA C++ Programming Guide. GPUs and Supercomputing GPUs are featured in many of the top 500 supercomputers. This goes to show that they are a powerful and cost-efficient tool for solving problems. The table below shows the top 5 supercomputers as of November 2023. 4 of them utilize some form of GPU acceleration.\nName CPUs GPUs Peak PFlop/s Frontier (Oak Ridge NL) 606,208 cores 37,888 AMD MI250X 1,679.72 Aurora (Argonne NL) 1,100,000 cores (est.) 63,744 Intel GPU Max 1,059.33 Eagle (Microsoft Azure) 1,123,200 cores (combined) Unknown Split (NVIDIA H100) 846.74 Fugaku 7,630,848 cores None 537.21 LUMI 362,496 cores 11,712 AMD MI250X 531.51 The results are clear: heterogeneous parallel computing is a powerful tool for solving problems. Learning how to use these tools will be a valuable skill for the future.\nMeasuring Speedup In general, if system A takes \\(T_A\\) time to complete a task and system B takes \\(T_B\\) time to complete the same task, then the speedup of system B over system A is given by \\(S = \\frac{T_A}{T_B}\\).\nAmdahl\u0026rsquo;s law is defined as follows:\n\\[S(s) = \\frac{1}{(1 - p) + \\frac{p}{s}}\\]\nwhere \\(p\\) is the fraction of the task that can be parallelized and \\(s\\) is the speedup of the part of the task that can be parallelized.\nIt is not common that 100% of a task can be parallelized. Amdah\u0026rsquo;s law takes this into account. Suppose that 40% of a given task can benefit from parallelization. If that part of the task can be sped up by a factor of 10, then the overall speedup is given by:\n\\[S = \\frac{1}{(1 - 0.4) + \\frac{0.4}{10}} = 1.56\\]\nIn virtually every lab that you will do in this course, you will be asked to measure the speedup of your solution. This is a good way to verify that your solution is correct and that it is actually faster than the serial version. This will also be a critical part of your project, where you will first need to create a serial version of your solution and then parallelize it.\nGPU Programming History Early GPU programming was done using OpenGL and DirectX. These were graphics APIs, so everything had to be done in terms of pixel shaders. Researchers found ways to use these APIs to do general purpose computing, but it was very difficult since one could not easily debug the code. Essentially, the input had to be encoded as a texture or color. The GPU would then process the texture and output the result as a texture. The output would then have to be decoded to get the result.\nIn 2006, NVIDIA unveiled the GeForce 8800 GTX, which was the first DirectX 10 GPU. More importantly, it was the first GPU built using the CUDA architecture. CUDA also refers to the programming model that NVIDIA developed to facilitate general purpose GPU programming. A key piece of the CUDA architecture is the unified shader pipepline, which allows each ALU to be utilized for general purpose computations.\nThe different ALUs have access to a global memory space as well as a shared memory space managed by software. We will explore the specifics of this architecture in part 1 of this course. Since that time, many major changes have been made to the CUDA architecture. Additionally, many other standards have been developed to facilitate GPU programming and parallel computing in general.\nOne of the most important standards, which we also study in this course, is OpenCL. OpenCL is an open standard that allows for heterogeneous parallel computing. It is supported by many different vendors, including NVIDIA, AMD, and Intel. OpenCL is a C-like language that allows for the creation of kernels that can be executed on a variety of devices. The OpenCL standard is maintained by the Khronos Group, which also maintains the OpenGL standard.\nApplications We are currently in the midst of a data explosion. Vertical scaling, the idea of improving a single system, cannot meet the demands of modern challenges. Horizontal scaling is the most sure solution for now. Distributed systems utilize cheap, commodity servers in lieu of complex supercomputers to distribute applications to mass markets. Parallel computation has applications in just about every field imaginable. We will try to cover a wide variety of applications, as many of them feature parallel solutions that are helpful in other domains.\nLinear Algebra Libraries One of the most widely utilized applications of data parallelism is in linear algebra libraries. Common matrix operations such as matrix multiplication and matrix inversion are highly parallelizable. The cuBLAS library is a highly optimized implementation of these operations.\nFor a great overview of the evolution of linear algebra libraries and the impact of GPUs, see Jack Dongarra\u0026rsquo;s keynote speech at the 50 Years of Computing at UTA event.\nMachine Learning Model training and optimization in machine learning is a perfect candidate for data parallelism. Large models such as Llama2 require a massive amount of data to train (Touvron et al. 2023). Deep learning models such as this are trained on many GPUs that can execute functions on independent data points in parallel.\nNVIDIA has developed a useful library, which we will study in this course, called cuDNN that implements highly optimized implementations of common functions used in a deep learning pipeline. High level frameworks build off of this library to provide easier development interfaces for machine learning practitioners. Popular examples include PyTorch, TensorFlow, and JAX.\nComputer Vision Most of the current state-of-the-art computer vision methods are driven by deep learning, so they also benefit greatly from data parallelism. Convolutional Neural Networks (CNN) have been the driving force behind machine-learning based computer vision methods. They are parameter efficient and take advantage of data parallelism. We will study the core operation behind this model, the convolutional opreator.\nFigure 3: 2D Convolution on a 4x4 grid using a 3x3 filter with unit stride (Dumoulin and Visin 2018) Computational Chemistry CUDA has been utilized for computing heat transfer calculations efficiently (Sosutha and Mohana 2015). The authors found that the computations could be computed independently, which is perfect for a parallel architecture like a GPU, where throughput is preferred to latency.\nOther Applications There are many other applications of data parallelism, some of which we will explore and learn from in this course. Examples include the following.\nFinancial Analysis Scientific Simulation Engineering Simulation Data Intensive Analytics Medical Imaging Digital Audio Processing Digital Video Processing Biomedical Informatics Electronic Design Automation Statistical Modeling Numerical Methods Ray Tracing Rendering Interactive Physics What to expect from this course This course is extremely hands-on. Almost every topic we cover will have an associated programming exercise. Some of these exercises will be integrated into assignments, other will be presented as in-class demonstrations. The fact that there are so many applications means you will need to be able to adapt to new domains quickly. By the end of this course, you should have acquired the following skills:\nAdvanced familiarity with the CUDA programming model Ability to think in parallel Identify sections of code that can be parallelized Implementation of parallel solutions Debugging parallel code Measuring performance increase from parallelization References Dumoulin, Vincent, and Francesco Visin. 2018. “A Guide to Convolution Arithmetic for Deep Learning.” Arxiv:1603.07285 [Cs, Stat], January. http://arxiv.org/abs/1603.07285. Sosutha, S., and D. Mohana. 2015. “Heterogeneous Parallel Computing Using Cuda for Chemical Process.” Procedia Computer Science, Graph Algorithms, High Performance Implementations and Its Applications ( ICGHIA 2014 ), 47 (January): 237–46. https://doi.org/10.1016/j.procs.2015.03.203. Touvron, Hugo, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, et al. 2023. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” arXiv. https://doi.org/10.48550/arXiv.2307.09288. ","date":1703052000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703052000,"objectID":"6bcf7a4ba1680d443c44a44c069a2922","permalink":"https://ajdillhoff.github.io/notes/introduction_to_gpgpu_programming/","publishdate":"2023-12-20T00:00:00-06:00","relpermalink":"/notes/introduction_to_gpgpu_programming/","section":"notes","summary":"Table of Contents Structure of the Course Heterogeneous Parallel Computing Measuring Speedup GPU Programming History Applications What to expect from this course Structure of the Course The primary of this goal is of course to learn how to program GPUs. A key skill that will be developed is the ability to think in parallel. We will start with simple problems that are embarrassingly parallel and then move on to more complex problems that require synchronization.","tags":["cuda","gpgpu"],"title":"Introduction to GPGPU Programming","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents What is MapReduce? Hadoop Distributed File System (HDFS) MapReduce Overview Hadoop v2 AKA YARN Summary These are my personal notes from the book Fundamentals of Database Systems by (Elmasri and Navathe 2015). I highly recommend reading the original source material. The contents of the article should only serve as a brief overview of the topic.\nWhat is MapReduce? MapReduce is a programming model for processing large datasets in parallel. It was originally developed by Jeffrey Dean and Sanjay Ghemawat at Google in 2004 (Dean and Ghemawat 2008). It is based on the functional programming paradigm and is inspired by the map and reduce functions in Lisp and other functional languages. The MapReduce programming model is implemented in the Hadoop framework.\nHadoop is made up of\nHadoop Distributed File System (HDFS) Yet Another Resource Negotiator (YARN) MapReduce Hadoop Common Figure 1: Diagram of MapReduce execution (Elmasri and Navathe). Example: Word Count The classic introductory example for MapReduce is word count, as described in (Dean and Ghemawat 2008). Let\u0026rsquo;s say we have a collection of text documents that we want to preprocess for a Natural Language Processing pipeline. One of the first steps is to count the number of times each word appears in the corpus. This is a simple task that can be done in a single machine, but let\u0026rsquo;s assume that the corpus is too large to fit in memory on a single machine. We can use MapReduce to distribute the work across multiple machines.\nThe problem is split into two steps: map and reduce. Each step is represented as a function that can run on any arbitrary number of nodes. For now, we will not worry about how the original data is split up efficiently. Instead, assume that each machine gets a single document.\ndef map(doc): for word in doc: emit(word, 1) def reduce(word, counts): emit(word, sum(counts)) The map function takes a document and emits a key-value pair for each word in the document. The key is the word and the value is 1. The reduce function takes a word and a list of counts and emits a key-value pair with the word and the sum of the counts. The output of the map function is a list of key-value pairs that are grouped by key. The reduce function is then applied to each group of key-value pairs.\nHadoop Distributed File System (HDFS) Store metadata and application data on different nodes. Metadata is stored on the NameNode. Application data is stored on DataNodes. This data is replicated across multiple DataNodes. HDFS uses primary-secondary architecture. The NameNode is the primary and the DataNodes are the secondaries. DataNodes are typically partitioned into 1 node per machine. NameNodes maintain inodes about file and directories. These inodes are used to map file blocks to DataNodes. NameNodes instruct the DataNodes to create, delete, and replicate blocks. Data is retrieved by contacting the NameNode to get the block locations and then contacting the DataNodes directly.\nNameNode Maintain an image of the file system. Maintain a journal of changes to the file system.\nSecondary NameNodes are used to create checkpoints of the NameNode\u0026rsquo;s state.\nDataNode Periodically send heartbeats to the NameNode to indicate their current state (BlockReport). Block locations are not part of the namespace image. BlockReports are used by services like the MapReduce JobTracker to determine where to schedule tasks.\nFile I/O Operations HDFS is single-writer, multiple-reader. A file consists of blocks. Data that is written on the last block becomes available after an hflush operation.\nMapReduce Overview MapReduce is also a primary-secondary architecture. The JobTracker is the primary process and the TaskTrackers are the secondaries.\nJobTracker Manages life cycles of jobs and schedules tasks on the cluster.\nJob Submission Gets a new ID from the job tracker. Verifies output specifications. Computes input splits for the job. Copies any resources needed to run the job. Informs the job tracker that it is ready for execution. Job Initialization When a job is initialized, it is placed in a queue with all information related to executing and tracking the job. A map task is created for each of the input splits. Further, a job setup and cleanup task are created.\nTask Assignment Each TaskTracker periodically sends a heartbeat to the JobTracker with status updates. These inform the JobTracker that it is alive or is able to run a new task. When a TaskTracker is ready to run a new task, the JobTracker will allocate a new one by selecting it using some defined scheduler. There is a default scheduler that will pick based on priority, but a custom scheduler can be given to the job.\nAnother consideration for assigning is a map task is data locality. Map tasks are based on the input splits, so the JobTracker will try to run the task on the same node that the data is located. If this is not possible, it will prioritize based on the distance between the node and the data. For reduce tasks, there are no locality considerations.\nTask Execution When a task is executed, any pertinent information is copied from the shared filesystem to a local filesystem. A JVM is launched to run each task so that any errors will only affect the JVM and not the entire TaskTracker. The JVM will run the task and then report back to the JobTracker with status updates.\nJob Completion A job is completed once the last task is finished. In this case, a job cleanup task is run to clean up any resources used by the job.\nTaskTracker TaskTrackers run one per worker node on a cluster. Both map and reduce tasks run on Worker nodes. When a TaskTracker is started, it registers with the JobTracker so that the JobTracker can assign tasks to it. The actual task is run in a separate process on the Worker node which is managed by the TaskTracker.\nFault Tolerance In Hadoop v1, three types of failures must be considered. The first two are the TaskTracker and JobTracker. The third is the spawned process that runs the task on the TaskTracker.\nTask Failure Tasks can fail due to bad input, faulty code, hardware failure, or some other run time error. When an individual task fails, the error is logged and the failure is reported back to the parent TaskTracker. Since the TaskTracker is still running, it can notify the JobTracker that it is free to run another task.\nThere is also a default timeout duration for tasks that are not making progress. If a task exceeds this timeout, it is killed and the TaskTracker is notified. The default timeout is 10 minutes but can be configured by the user. There are also settings dictating how many times a task can fail before the job is considered failed or the percentage of tasks that can fail before the job is considered failed. There may be circumstances in which task failures are acceptable as long as some of the work is completed.\nTaskTracker Failure TaskTrackers that fail or are unresponsive past the heartbeat timeout are considered dead. The JobTracker will remove it from its pool of trackers to schedule tasks on. Tasks that were completed or in progress on the failed TaskTracker are rescheduled since the intermediate data is no longer available.\nTaskTrackers that fail repeatedly are added to a blacklist and are not used for scheduling tasks. This can occur if the TaskTracker is not configured correctly or if the TaskTracker is running on faulty hardware.\nJobTracker Failure The JobTracker is a single point of failure in Hadoop v1. If the JobTracker fails, all jobs that are running or waiting to run are lost. The rate of failure is typically low since the chance that a single machine fails is low. If it does fail, a restart is attempted and all running jobs need to be restarted.\nShuffle and Sort The shuffle and sort phase is a key process which defines how data is moved from the map tasks to the reduce tasks. Data may be split among many map tasks, but reducers get all rows for a given key together. The shuffle and sort phase is responsible for this. It is split up into three phases.\nMap Phase Output from the map tasks is stored in memory in a circular buffer. Once that buffer becomes full, it spills over to the disk. Before going to the disk, it is partitioned based on the reducer that it will ultimately be sent to. This acts as a sort of pre-sorting to optimize the shuffle and sort phase.\nDepending on the setting for the size of spill files, the map phase may produce multiple spill files. These files are merged into a single file before being sent to the reducers. The final result is a single file per reducer that is sorted by key.\nCopy Phase The data needed by a particular reducer task is split up among many map tasks. The copy phase is responsible for copying the data from the map tasks to the reducer tasks. The reducer will begin copying as data is made available by the map tasks, even if all map tasks have not been completed.\nCopies can be executed in parallel via threading. The JobTracker is responsible for assigning the reducers to the map tasks so that the data is copied to the correct reducer. When all map tasks have completed, the reducer will begin the reduce phase.\nReduce Phase In this phase, data is merged while maintaining their sorted order. The reduce function is this executed for each key in the sorted output. The output is written directly to the output filesystem, which is commonly HDFS.\nTypes of Schedulers Early versions of Hadoop used a simple FIFO scheduler. This scheduler would run jobs in the order that they were submitted. This is not ideal since it does not take into account the size of the job or the priority of the job. A job that is submitted after a large job will have to wait until the large job is completed. Considering longer running jobs like machine learning training, a better scheduler is needed.\nThe Fair Scheduler The fair scheduler aims to give every user an equal amount of cluster capacity over time. This allows multiple jobs to be running simultaneously. The scheduler does this by placing jobs in pools assigned to each user. This allows short jobs to run without waiting for long jobs to complete.\nThe fair scheduler also supports preemption. This allows the scheduler to kill tasks that are running for too long to make room for other jobs. This is useful for long running jobs that are not making progress. Note that this does not kill the entire job. The tasks that are killed are rescheduled on other TaskTrackers.\nThe Capacity Scheduler In capacity scheduling, a cluster is divided into multiple queues. Each queue is assigned some amount of cluster capacity. These queues are hierarchical, so a queue can be assigned to a user or a group of users. This allows for more fine grained control over the cluster capacity. In effect, this allows for multiple clusters to be managed by a single cluster.\nMerging Database Functions and MapReduce The power of MapReduce was apparent, but all operations had to be framed in the context of a mapping and reduce functions. This made it difficult or tedious to perform common database operations. Several projects were started to bridge the gap between MapReduce and databases.\nApache Pig Developed by Yahoo, Pig Latin is a high level language that glues together SQL and MapReduce. It was not meant to replace SQL. In fact, the authors note that people in data analysis fine SQL to be unnatural for data analysis. Their work was mean to provide something that meets the declarative style of SQL and procedural style of MapReduce. Consider their opening example.\nGiven a table of urls: (url, category, pagerank), find the average pagerank of high-pagerank urls in that category. In SQL, a query might look like this.\nSELECT category, AV G(pagerank) FROM urls WHERE pagerank \u0026gt; 5 GROUP BY category HAVING COUNT(*) \u0026gt; 10**6 An equivalent query in Pig Latin would look like this.\ngood_urls = FILTER urls BY pagerank \u0026gt; 0.2; groups = GROUP good_urls BY category; big_groups = FILTER groups BY COUNT(good_urls) \u0026gt; 10**6; output = FOREACH big_groups GENERATE category, AVG(good_urls.pagerank); Each statement in Pig Latin describes a data transformation. These statements are converted into an ensemble of MapReduce jobs, but each statement is not necessarily a single MapReduce job. The Pig Latin compiler is responsible for optimizing the statements.\nApache Hive Hive was developed at Facebook to provide an SQL-like interface for processing queries on big data. In addition to providing a high-level language, it also treats Hadoop like a DBMS. This allows users to impose structure on the data and query it using as if it were a traditional database.\nFigure 2: Hive Architecture (Elmasri and Navathe). Hive comes with a query-based language called HiveQL that includes joints, aggregations, and other common SQL operations. The tables in Hive are linked directly to directories in HDFS. This allows for data to be loaded into Hive from HDFS and vice versa. Hive also supports partitioning and bucketing to improve performance. Bucketing stores the data physically in the same file, partitioned by the bucketing key.\nAdvantages of Hadoop/MapReduce Consider scanning a 100 TB dataset using a single machine. At a rate of 50 Mbps, this would take around 24 days. Using 1000 machines in parallel would reduce this to about 30 minutes. The resources available can be scaled easily by adding more machines to the cluster. In the event that a machine fails, the tasks can be reassigned to other machines without losing the job completely.\nHadoop v2 AKA YARN Hadoop v1 was well received and solved many of the problems efficiently through its MapReduce programming model. However, many problems arise naturally as the size of the cluster grows.\nThe JobTracker is a single point of failure. As the cluster size increases, this becomes more of an issue. Resources are allocated statically, resulting in a large amount of unused compute when processing jobs. Not every job fits cleanly into the MapReduce programming model. In addition to these issues, YARN aims to improve scalability, resource utilization, and flexibility. By increasing the availability of nodes at any given time, there are less wasted resources on a cluster. This is especially useful in enterprise-level clusters where there are many users and jobs running at any given time.\nSupporting multiple programming models also aids to this scalability. Consider a machine learning model being trained for long periods of time. In Hadoop v1, developers would frame these as MapReduce jobs. A major problem with this is that after the original update, the jobs would exchange data outside of the purview of the JobTracker. This also means that the fault tolerance features built into Hadoop v1 were not available for these jobs.\nArchitecture Figure 3: Hadoop v1 vs. YARN Architecture (Elmasri and Navathe). ResourceManager The ResourceManager is the master process in YARN. It is responsible for allocating resources to applications and scheduling tasks. Allocations are based on the chosen scheduler. ApplicationMasters will request resources from the ResourceManager. The ResourceManager will then allocate resources to the ApplicationMaster and notify the NodeManager to start the containers.\nSince the ResourceManager is only responsible for scheduling the available resources, different applications can make use of the same cluster at the same time. This is a major improvement over Hadoop v1 where the JobTracker was responsible for scheduling all jobs.\nNodeManager The NodeManager runs on every worker node in the cluster. It launches and monitors containers on the node as well as reports the resource utilization back to the ResourceManager. It additionally provides services to Containers such as security, logging, and local file management.\nApplicationMaster The ApplicationMaster manages the execution of an application\u0026rsquo;s processes. These applications can range from a traditional MapReduce job to a long-running machine learning model. The ApplicationMaster is responsible for negotiating resources with the ResourceManager and working with the NodeManager to execute and monitor the containers. It sends resource status updates to the ResourceManager as requirements change.\nContainer A container is a collection of resources allocated to an application. These resources are allocated by the ResourceManager and managed by the NodeManager. These resources refer directly to the resources available on the node. This includes CPU, memory, disk, and network.\nFault Tolerance The ResourceManager is a single point of failure in YARN. If it fails, it can restart and recover its state. Any containers in the cluster are killed and restarted. The ApplicationMaster is responsible for restarting any tasks that were running in the containers.\nExecution Flow The following gives an example of how a typical MapReduce job would be executed in YARN.\nThe client submits a job to the ResourceManager. The ResourceManager allocates a container for the ApplicationMaster and launches the ApplicationMaster on a NodeManager. The ApplicationMaster negotiates resources with the ResourceManager. The ResourceManager allocates containers for the MapReduce tasks and launches them on the NodeManagers. The MapReduce tasks are executed in the containers. The ApplicationMaster monitors the MapReduce tasks and reports status updates to the ResourceManager. When the job is complete, the ApplicationMaster is unregistered and the containers are released. Summary References Dean, Jeffrey, and Sanjay Ghemawat. 2008. “MapReduce: Simplified Data Processing on Large Clusters.” Communications of the Acm 51 (1): 107–13. https://doi.org/10.1145/1327452.1327492. Elmasri, Ramez, and Shamkant B. Navathe. 2015. Fundamentals of Database Systems. 7th ed. Pearson. https://www.pearson.com/en-us/subject-catalog/p/fundamentals-of-database-systems/P200000003546/9780137502523. ","date":1700805600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700805600,"objectID":"9bd3e96e7e59e698632e0a2a98f4a303","permalink":"https://ajdillhoff.github.io/notes/mapreduce/","publishdate":"2023-11-24T00:00:00-06:00","relpermalink":"/notes/mapreduce/","section":"notes","summary":"Table of Contents What is MapReduce? Hadoop Distributed File System (HDFS) MapReduce Overview Hadoop v2 AKA YARN Summary These are my personal notes from the book Fundamentals of Database Systems by (Elmasri and Navathe 2015). I highly recommend reading the original source material. The contents of the article should only serve as a brief overview of the topic.\nWhat is MapReduce? MapReduce is a programming model for processing large datasets in parallel.","tags":["computer science","databases"],"title":"MapReduce","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Unsupervised Pre-training From GPT to GPT2 These notes provide an overview of pre-training large language models like GPT and Llama.\nUnsupervised Pre-training Let\u0026rsquo;s start by reviewing the pre-training procedure detailed in the GPT paper (Radford et al. 2020). The Generative in Generative Pre-Training reveals much about how the network can be trained without direct supervision. It is analogous to how you might have studied definitions as a kid: create some flash cards with the term on the front and the definition on the back. Given the context of the word, you try and recite the definition. For a pre-training language model, it is given a series of tokens and is tasked with generating the next token in the sequence. Since we have access to the original documents, we can easily determine if it was correct.\nGiven a sequence of tokens \\(\\mathcal{X} = \\{x_1, x_2, \\ldots, x_n\\}\\), the model is trained to predict the next token \\(x_{n+1}\\) in the sequence. The model is trained to maximize the log-likelihood of the next token:\n\\[\\mathcal{L}(\\mathcal{X}) = \\sum_{i=1}^{n} \\log p(x_{i+1} \\mid x_{i-k}, \\ldots, x_i)\\]\nwhere \\(k\\) is the size of the context window.\nLarge language models are typically based on the Transformers model. The original model was trained for language translation. Depending on the task, different variants are employed. For GPT models, a decoder-only architecture is used, as see below.\nFigure 1: Decoder-only diagram from (Vaswani et al. 2017). The entire input pipeline for GPT can be expressed rather simply. First, the tokenized input is passed through an embedding layer \\(W_{e}\\). Embedding layers map the tokenized input into a lower-dimensional vector representation. A positional embedding matrix of the same size as \\(\\mathcal{X} W_{e}\\) is added in order to preserve the order of the tokens.\nThe embedded data \\(h_0\\) is then passed through \\(n\\) transformer blocks. The output of this is passed through the softmax function in order to produce an output distribution over target tokens.\nFrom GPT to GPT2 GPT2 is a larger version of GPT, with an increased context size of 1024 tokens and a vocabulary of 50,257 vocabulary. In this paper, they posit that a system should be able to perform many tasks on the same input. For example, we may want our models to summarize complex texts as well as provide answers to specific questions we have about the content. Instead of training multiple separate models to perform these tasks individually, the model should be able to adapt to these tasks based on the context. In short, it should model \\(p(output \\mid input, task)\\) instead of \\(p(output \\mid input)\\).\nReferences Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2020. “Improving Language Understanding by Generative Pre-Training,” 12. Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need,” 11. ","date":1700114400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700114400,"objectID":"49a23a5b57d87ec8a4f3138e9d80013d","permalink":"https://ajdillhoff.github.io/notes/pretraining_large_language_models/","publishdate":"2023-11-16T00:00:00-06:00","relpermalink":"/notes/pretraining_large_language_models/","section":"notes","summary":"Table of Contents Unsupervised Pre-training From GPT to GPT2 These notes provide an overview of pre-training large language models like GPT and Llama.\nUnsupervised Pre-training Let\u0026rsquo;s start by reviewing the pre-training procedure detailed in the GPT paper (Radford et al. 2020). The Generative in Generative Pre-Training reveals much about how the network can be trained without direct supervision. It is analogous to how you might have studied definitions as a kid: create some flash cards with the term on the front and the definition on the back.","tags":["large language models","machine learning"],"title":"Pretraining Large Language Models","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Overview Data Fragmentation Data Replication Data Concurrency Distributed systems excel at partitioning large problems into smaller chunks that can be processed in parallel. This requires parallel thinking instead of serial thinking. Many algorithms and solutions that run serially may be easier to adapt to parallel applications than others.\nDistributed solutions are the natural next step to scaling up a system. In the context of databases, the main challenges related to distribution, replication, distributed transactions, distributed metadata management, and distributed query processing.\nOverview According to Elmasri and Navathe (Elmasri and Navathe 2015), a distributed database should satisfy at least the following conditions:\ndatabase nodes should be connected by a network, the information on each node should be logically related, and each node does not necessarily need to be identicaly in terms of data, hardware, and software. Transparency Transparency is the concept of hiding the complex details of a distributed database from the user. There are several types of transparency:\nDistribution transparency - the user does not need to know how the data is distributed across the nodes. This could refer to the location of the data, the replication of the data, or the fragmentation of the data. Replication transparency - data may be stored in multiple nodes. This type of transparency improves availability by allowing the system to continue operating even if a node goes down. Fragmentation transparency - data is either horizontally or vertically fragmented across nodes. Horizontal fragmentation, also called sharding, refers to decomposing tuples of a table into multiple systems. For example, we could horizontally fragment our Character table based on the class_id. Vertical fragmentation refers to decomposing the columns of a table into multiple systems. For example, we could vertically fragment our Character table into a Character table and a CharacterStats table. Availability and Reliability Having more than one point of failure means that a distributed database is more reliable than a centralized database. With technologies like replication, the availability of the database also increases.\nScalability Scalability in a database that is distributed over multiple nodes can be categorized into two types:\nHorizontal scalability - adding more nodes to the system. Vertical scalability - adding more resources to the nodes. A centralized database can only support vertical scalability. If it goes down or is fragmented from a portion of a broader network, the data is no longer accessible. In a distributed system, the nodes can be partitioned into smaller networks that can still operate independently depending on the type of failure. This is called partition tolerance.\nAutonomy Autonomy refers to the ability of a node to operate independently of other nodes. This is important for distributed systems because it allows for the system to continue operating even if a node goes down.\nDesign autonomy - Data model usage and transaction managament are independent of other nodes. Communication autonomy - Nodes can communicate with each other without the need for a central coordinator. Execution autonomy - Nodes can execute transactions independently of other nodes. While this type of autonomy leads to more availability and higher performance, it can also create problems with consistency since nodes may not be able to agree on the order of operations. Data Fragmentation As mentioned at the beginning of these notes, breaking up a problem into smaller chunks is the key to parallelism. In the context of databases, this means figuring out which nodes have which portions of the data. We will discuss fragmentation under the assumption that no data replication is being used.\nHorizontal Fragmentation (Sharding) Imagine a scenario in which we shard our Users table based on the geographic location of their IP address. If we have 3 nodes in (west coast, central, east coast), then we can separate our table into 3 tables, one for each region. This is called horizontal fragmentation or sharding. The main advantage of sharding is that it allows us to scale horizontally. The main disadvantage is that it makes it more difficult to perform queries that require data from multiple regions.\nVertical Fragmentation Vertical fragmentation can make sense when we have a table with a large number of columns. For example, we could vertically fragment our Users table into a Users table and a UserStats table. When vertically fragmenting data, there should be a common attribute between the two tables. In this case, the user_id would be the common attribute.\nData Replication Data replication is the process of storing the same data in multiple nodes. There are obvious tradeoffs when it comes to selecting a replication strategy. First, let\u0026rsquo;s consider the extreme cases. If no replication is used, then the system is more consistent since there is only one copy of the data. The availability suffers, however, since there is only a single copy of the data.\nIf the data is replicated to every single node, then the availability and performance of the system increases. However, the consistency of the system suffers since there are multiple copies of the data that need to be kept in sync. Picking a replication strategy will largely depend on the needs of the application. Deciding how this data is fragmented is the process of data distribution.\nExample The following example is from Elmasri and Navathe (Elmasri and Navathe 2015). In this example, a company has three nodes for each of its departments. Node 2 stores data for the Research department and Node 3 stores data for the Administration department. The idea behind this is that the EMPLOYEE and PROJECT information for each department will be frequently accessed by that department. This would be more efficient than having to access the data from a centralized database. Node 1 is located at the company\u0026rsquo;s headquarters and includes data for all departments.\nThe data in the DEPARTMENT table is horizontally fragmented using the department number Dnumber. Since there are foreign key relationships in EMPLOYEE, PROJECT, and DEPT_LOCATIONS, they are also fragmented. This is a special type of fragmentation called derived fragmentation. These are easier to fragment since they have a direct foreign key relationship.\nA more difficulty decision comes with the WORKS_ON table. It does not have an attribute that indicates which department each tuple belongs to. The authors choose to fragment based on the department that the employee works for. This is further fragmented based on the department that controls the projects that the employee is working on.\nFigure 1: Fragmentation of WORKS_ON table for department 5. \u0026lt;@elmasri_fundamentals_2015\u0026gt; In the figure above, all of the fragments include employees of the research department. The first fragment includes employees that work on projects controlled by the research department. The second fragment includes employees that work on projects controlled by the administration department. The third fragment includes employees that work on projects controlled by headquarters.\nData Concurrency Distributed systems that employ data replication or allow for multiple users to access the same data at the same time need to be concerned with data concurrency. This is the process of ensuring that the data remains consistent when multiple users are accessing the same data at the same time. Several problems can occur in a DDBMS, such as\ninconsistency between multiple copies of the data, failure of a node, network outages that sever the connection between nodes, failure of a transaction that is applied to multiple nodes, and deadlocks between transactions. Concurrency Control Many control solutions for distributed systems are based on the idea of a centralized locking authority. This authority is responsible for granting locks to transactions that request them. The authority is also responsible for granting access to data that is locked by other transactions. When an object is locked, it cannot be accessed by other transactions.\nIn this case, the central authority may be a distinguished copy of the data. All requests to lock or unlock are sent to that copy.\nPrimary Site Technique All locks are kept at a primary site. This site is responsible for granting locks to transactions that request them. The primary site is also responsible for granting access to data that is locked by other transactions. This is a simple technique that is easy to implement. However, it is not very scalable since all requests must go through the primary site. Note that this does not prevent transactions with read locks from accessing any copy of the item. If a transaction has a write lock, the primary site must update all copies of the data before releasing the lock.\nPrimary Site with Backup If the primary site fails in the first approach, the system effectively becomes unavailable. To prevent this, we can have a backup primary site that takes over if the primary site fails. This is a simple solution that is easy to implement. If the primary site fails in this case, a backup takes over and becomes the new primary. A new backup is chosen so that the system can continue to operate. One downside to this approach is that locks must be recorded at both the primary and backup sites.\nPrimary Copy Technique Lock coordination is distributed among various sites. Distinguished copies for different items are distributed to different sites. A failure at one site would only affect the transactions that are accessing its distinguished copies. Other items not on the site would remain functional. In the case of a failure, the sites that are still running can choose a new coordinator based on some strategy. One such strategy is to have all running sites vote on a new coordinator. The site with the most votes becomes the new coordinator.\nReferences Elmasri, Ramez, and Shamkant B. Navathe. 2015. Fundamentals of Database Systems. 7th ed. Pearson. https://www.pearson.com/en-us/subject-catalog/p/fundamentals-of-database-systems/P200000003546/9780137502523. ","date":1699941600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699941600,"objectID":"3ab12ea9ee4c2922449f269c1fb4640f","permalink":"https://ajdillhoff.github.io/notes/distributed_databases/","publishdate":"2023-11-14T00:00:00-06:00","relpermalink":"/notes/distributed_databases/","section":"notes","summary":"Table of Contents Overview Data Fragmentation Data Replication Data Concurrency Distributed systems excel at partitioning large problems into smaller chunks that can be processed in parallel. This requires parallel thinking instead of serial thinking. Many algorithms and solutions that run serially may be easier to adapt to parallel applications than others.\nDistributed solutions are the natural next step to scaling up a system. In the context of databases, the main challenges related to distribution, replication, distributed transactions, distributed metadata management, and distributed query processing.","tags":["computer science","databases"],"title":"Distributed Databases","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents NOSQL Characteristics for Distributed Systems NOSQL Data Models CAP Theorem Document-Based NOSQL Systems Key-Value NOSQL Systems Column-Based NOSQL Systems Graph-Based NOSQL Systems NOSQL refers to Not Only SQL. A NOSQL system is commonly a distributed one that focuses on semi-structured data storage, high performance, availability, replication and scalability. These type of systems developed to meet the needs of large-scale internet applications where a traditional SQL database could not.\nConsider an application like Amazon which manages a high volume of data and user requests. The application needs to be able to store and retrieve this data quickly and reliably. They created their own database system called DynamoDB which is a key-value store. DynamoDB has been used for many applications that require high performance and availability such as video streaming through services like Disney+.\nThe data that is used in these systems does not usually fit the mold of a traditional SQL database. For example, a relational database might store an object by disassembling it into its components and storing each component in a separate table. This is not ideal for a system that needs to store and retrieve data quickly. A NOSQL system will store the object as a whole and retrieve it as a whole.\nNOSQL Characteristics for Distributed Systems Given the nature of the applications that utilize NOSQL systems, the most important characteristic is high availability. Of course, performance is also important given the number of users that expect the service to remain responsive at all times.\nScalability NOSQL systems typically aim for horizontal scalability. The applications that use these systems are expected to grow rapidly and the system needs to be able to handle the increased load. This sort of dynamic scaling means that implementations should not rely on a fixed number of nodes.\nFor example, during the holiday season, Amazon will need to rapidly scale up their infrastructure to handle the increased load. Cloud technologies are capable of doing this automatically, but the database system needs to be able to handle the increased load as well.\nAvailability NOSQL systems are expected to be highly available. This means that the system should be able to handle failures and continue to operate. Data is typically replicated over multiple nodes. However, this replication comes with increased complexity for writing data. To deal with this, many NOSQL systems implement a relaxed version called eventual consistency.\nReplication Models There are two main replication models for NOSQL systems: primary-replica and primary-primary. In primary-replica replication, only one copy is the primary for which all write operations are applied. The write is propagated asynchronously to the replicas.\nIn primary-primary replication, all copies are equal and can accept write operations. This is more complex to implement, but it allows for better performance and availability. If multiple users write to the same object, the system needs to be able to handle the conflict through a reconciliation process.\nSharding Depending on the application, a NOSQL collection could have millions of documents. These may need to be accessed simultaneously by a large number fo users. Sharding is a technique that allows the data to be distributed across multiple nodes. In this way, multiple nodes can work in parallel to handle the load. This has an added benefit of ensuring that no single node is overloaded.\nHigh-Performance Data Access In a distributed system with millions upon millions of objects distributed across many nodes, how do you find the object you are looking for? NOSQL systems typically use a hash-based approach to find the object. This is done by hashing the key of the object and using the hash to determine which node the object is stored on. This is a very fast operation and allows for the system to scale to millions of objects.\nAnother solution is called range partitioning in which the location is determined based on a range of key values. Each node would handle a different partition of the keys.\nOther Characteristics NOSQL systems do not require a schema. This means that the data does not need to be structured in a specific way. This is useful for applications that need to store a variety of data types. For example, a social media application might need to store user profiles, posts, comments, etc. These are all different types of data that would not fit well into a relational database. Instead of a schema, a language for describing the data is used. A common language is JSON.\nGiven the common application of NOSQL systems, a complex query language is not required. Many of the requests are in the form of a simple read or write operation. This allows for the system to be optimized for these operations. These operations are typically provided by an API and are called CRUD operations (Create, Read, Update, and Delete). Without the full power of SQL, complex operations such as JOIN or CONSTRAINTS must be handled by the application.\nNOSQL Data Models There are four main data models used by NOSQL systems: key-value, column, document, and graph. Each of these models has its own advantages and disadvantages. The model that is chosen depends on the application and the type of data that is being stored.\nKey-Value The key-value model is the simplest of the four. It is essentially a hash table where the key is used to retrieve the value. The value can be any type of data. This model is very fast and can scale to millions of objects.\nColumn Tables are partitioned by columns into column families. Each column family is stored in its own files.\nDocument Documents are stored in collections. Each document is stored as a JSON object. This model is very flexible and can store a variety of data types. It is also very fast and can scale to millions of objects. The documents are typically queried using their document ID, but other indices can be created to speed up queries.\nGraph Graphs are used to represent relationships between objects. Each object is represented as a node and the relationships are represented as edges. This model is useful for applications that need to represent complex relationships between objects.\nCAP Theorem The CAP theorem states that a distributed system can only guarantee two of the following three properties: consistency, availability, and partition tolerance. Consistency means that all nodes see the same data at the same time. Availability means that every request receives a response. Partition tolerance means that the system continues to operate despite network failures.\nDocument-Based NOSQL Systems In document-based NOSQL systems, the data is self-describing as there is no need for a schema. These sytems store documents which are essentially JSON objects. The documents are stored in collections which are similar to tables in a relational database. The documents are retrieved using their document ID.\nMongoDB MongoDB is a document-based NOSQL database that is flexibile, scalable, and high-performance. It stores data in a JSON-like format called BSON (Binary JSON). Inidividual documents are stored in a collection. No schema is needed to begin storing data. The python code below will create a new collection for our RPG Users with a simple command in pymongo:\ndb[\u0026#39;users\u0026#39;] This will create a new collection named users with the default settings. If you want to specify additional options, call the create_collection function. Common parameters include determining of a collection is capped by the storage size and maximum number of documents.\nRepresenting Data Whenever a new item is inserted to a colletion, a unique ObjectId is created and indexed. If the ID of a document should match a user-defined protocol, it can be set manually. Since there is no schema to specify a relationship, document relationships can be created by including the ObjectIds of objects you wish to reference in your data.\nThere are multiple ways to represent relationships between documents. Consider a Character that holds multiple items in an Inventory. The items could be referenced as an array of Item objects within the Character object itself. Alternatively, the Character could hold an array of ObjectId~s that reference the ~Item objects in the Inventory collection. A third approach would have each Item reference the Character that owns it. The best approach depends on the application and the type of queries that will be performed.\nCRUD Operations CRUD stands for Create, Read, Update, and Delete. Single or multiple documents can be implemented with the insert function. In pymongo, you can use either Collections.insert_one or Collections.insert_many. The insert_one function takes a single document as an argument and returns the ObjectId of the inserted document. The insert_many function takes a list of documents as an argument and returns a list of ObjectIds.\ndb[\u0026#39;users\u0026#39;].insert_one({\u0026#39;name\u0026#39;: \u0026#39;Naomi\u0026#39;, \u0026#39;age\u0026#39;: 25}) db[\u0026#39;users\u0026#39;].insert_many([{\u0026#39;name\u0026#39;: \u0026#39;Naomi\u0026#39;, \u0026#39;age\u0026#39;: 25}, {\u0026#39;name\u0026#39;: \u0026#39;James\u0026#39;, \u0026#39;age\u0026#39;: 30}]) Reading objects is done with the find function. There are several variants of this available in pymongo.\nfind_one returns a single document that matches the query. find returns a cursor that can be iterated over to retrieve all documents that match the query. find_one_and_delete returns a single document that matches the query and deletes it. find_one_and_replace returns a single document that matches the query and replaces it with the specified document. find_one_and_update returns a single document that matches the query and updates it with the specified document. val = db[\u0026#39;users\u0026#39;].find_one({\u0026#39;name\u0026#39;: \u0026#39;Naomi\u0026#39;}) # Print the document print(val) # Print the name print(val[\u0026#39;name\u0026#39;]) Updating documents is done with the update function. We saw an updated combined with find above, but pymongo also implements update_one and update_many. The update_one function takes a query and an update document as arguments. The update_many function takes a query and an update document as arguments. Both functions return a UpdateResult object that contains information about the operation.\ndb[\u0026#39;users\u0026#39;].update_one({\u0026#39;name\u0026#39;: \u0026#39;Naomi\u0026#39;}, {\u0026#39;$set\u0026#39;: {\u0026#39;age\u0026#39;: 26}}) Deleting documents is done with the delete_one and delete_many functions. Both functions take a query as an argument and return a DeleteResult object that contains information about the operation.\ndb[\u0026#39;users\u0026#39;].delete_one({\u0026#39;name\u0026#39;: \u0026#39;Naomi\u0026#39;}) Characteristics MongoDB uses a two-phase commit method to ensure transaction atomicity and consistency. In the first phase of the process, a coordinator sends a message to all nodes to prepare for the transaction. Each node then responds with an acknowledgement. If all nodes respond with an acknowledgement, the coordinator sends a commit message to all nodes. If any node fails to respond with an acknowledgement, the coordinator sends a message to roll back the transaction.\nFor data replication, a variation on the primary-replica model is used. A primary node is chosen with at least one replica. More nodes can be added at the cost of increased time for writes. The total number of nodes for a replica set is at least 3, so if only a primary and one replica are used, an arbiter must be chosen to break ties. In fact, any replica set with an even number of nodes must have an arbiter.\nAll write operations mus be performed on the primary copy before being propagated to the replicas. Users can determine the read preference for their application. The default is to read from the primary copy, but users can choose to read from the nearest copy or a specific copy. If a copy other than the primary is chosen for the read preference, it is not guaranteed that the user will get the lastest version of the data.\nSharding We previously discussed that having all of the data in a single collection can lead to performance issues. Sharding is a technique that allows the data to be distributed across multiple nodes. This allows for multiple nodes to work in parallel to handle the load. Sharding splits the data into disjoint partitions which can then be stored on different nodes.\nThe partitions can be determined via hash partitioning or range partitioning. In either case, a document field must be chosen to determine the partition. This partition field is called the shard key. It must exist in every document and be indexed.\nWhen using sharding on MongoDB, a query router keeps tracks of which nodes contain which shards. The actual query is then routed to the node containing the shard. In the event that a query is sent to a node that does not contain the shard, the query router will forward the query to all nodes.\nKey-Value NOSQL Systems Key-value systems use a simple data model and typically do not have a query language. The data is stored as a key-value pair. The key is used to retrieve the value. The value can be any type of data. This model is very fast and can scale to millions of objects. Popular key-value stores include DynamoDB, Voldemort, Redis, and Cassandra. We will briefly discuss each of them below.\nDynamoDB DynamoDB was developed by Amazon to meet the needs of their large-scale internet applications. It is a key-value store that is highly available and scalable. It is also a managed service which means that Amazon handles the scaling and replication for you. It uses tables, items, and attributes without the need for a schema. The table itself holds multiple items which are self-describing. That is, the items have (attribute, value) pairs.\nTables must have primary keys which can be either a single attribute or pair of attributes. For single attributes, DynamoDB will build a hash index on this attribute. For pairs of attribute, a hash and range primary key is used. The primary key is the pair of attributes and the hash index is built on the first attribute. This allows for fast retrieval of items based on the first attribute. The second attribute can be used to sort the items for which the first attribute is the same.\nVoldemort Voldemort is a distributed key-value store based on DynamoDB and developed by LinkedIn and Microsoft. The distribution of data is handled via consistent hashing. Since Voldemort is based on DynamoDB, many of the characteristics described below also apply to DynamoDB.\nOperations Like DynamoDB, key-value pairs are the primary data structure. These are kept in a data store. Three basic operations are implemented: get, put, and delete. Data is stored as a byte array.\nFormatted Data Voldemort supports multiple formats for the data. The default format is a byte array, but other formats such as JSON and Protocol Buffers are supported. It provides default serializers for these formats, but users can also implement their own. As long as a Serializer class is implemented, it can be used to serialize and deserialize data.\nConsistent Hashing Voldemort distributes data based on a hash function that is applied to each key. The range of values on which the key is mapped corresponds to a node. The figure below shows an example of 7 regions being mapped to 3 nodes (Elmasri and Navathe 2015).\nFigure 1: Consistent hashing in Voldemort. Consistent hashing naturally permits data replication and horizontal scaling. As new nodes are added, only a small subset of the data needs to be rehashed to the new node. Replicas are created by mapping the key to multiple nodes.\nConsistency Concurrent writes are allowed which means there can exist multiple versions of the same key at different nodes. Consistency occurs when an item is read. If the system can reconcile the different versions of the key to a single value, it will pass that final value on. Otherwise, multiple versions may be sent to the application to be resolved.\nRedis Redis is an in-memory key-value store. This implies that is basic operations perform very quickly. However, it is not well suited for general purpose applications that require high volumes of data. A typical use-case for Redis would be caching, session management, or real-time analytics.\nFor example, Twitter uses Redis to drive their timeline feature. The posts are indexed using an ID and stored in Redis. When a user requests their timeline, the IDs are retrieved from Redis as a chain of IDs.\nCassandra Cassandra can be used as a wide-column database (discussed below) or key-value database. It was originally developed at Facebook to handle large amounts of data across multiple commodity servers. It implements the Cassandra Query Language (CQL) which is similar to SQL. The data it partitioned similarly to other NOSQL datastores in that data is distributed in partitions across multiple nodes. CQL does not support cross-partition queries.\nColumn-Based NOSQL Systems The largest differentiator of a column-based system and key-value system is the way the key is defined. A popular implementation of this type of system is known as BigTable which was developed by Google. It uses the Google File System (GFS) to store data. There is an open source equivalent named Apache Hbase which we will focus on below.\nHbase organizes data using namespaces, tables, column families, column qualifiers, columns, rows, and data cells. A column is identified by a family and qualifier. It can store multiple versions of the same data, differentiating each version using a timestamp. Each data cell is identified by a unique key. Tables are associated with column families. When loading data, the column qualifiers must be specified.\nNew column qualifiers can be created as needed, producing new rows of data. However, application developers must keep track of which qualifiers belnog to which family. This is a form of vertical partitioning. Since the columns belong to the same column family, they are stored in the same file.\nCells are reference by their key which is a combination of the row key, column family, column qualifier, and timestamp. For relational semantics, namespaces are used to define a collection of tables.\nHbase divides tables into regions which hold a range of row keys into the table. It is for this reason that they keys must be sortable lexicographically. Each region has a number of stores for which a column family is assigned. These regions of data are assigned to nodes in the cluster. To manage splitting and merging of regions, a primary server is used.\nGraph-Based NOSQL Systems The last category of NOSQL databases discussed in these notes are Graph Databases. These databases are used to represent relationships between objects. Each object is represented as a node and the relationships are represented as edges. This model is useful for applications that need to represent complex relationships between objects. A popular implementation of this type of system is known as Neo4j.\nNodes and relationships can have a unique collection of properties to describe them. Nodes are labeled, and nodes with the same label are grouped into collections for querying. Relationship types are useful for grouping relationships based on a common property.\nPaths specify a traversal of a subgraph. They are used to specify a query and consist of nodes and relationships. The subgraph is used as a pattern to find other subgraphs that match the pattern. The query can be further refined by specifying constraints on the nodes and relationships.\nReferences Elmasri, Ramez, and Shamkant B. Navathe. 2015. Fundamentals of Database Systems. 7th ed. Pearson. https://www.pearson.com/en-us/subject-catalog/p/fundamentals-of-database-systems/P200000003546/9780137502523. ","date":1699941600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699941600,"objectID":"df47404d31f56f5ae51156a3555ba788","permalink":"https://ajdillhoff.github.io/notes/nosql/","publishdate":"2023-11-14T00:00:00-06:00","relpermalink":"/notes/nosql/","section":"notes","summary":"Table of Contents NOSQL Characteristics for Distributed Systems NOSQL Data Models CAP Theorem Document-Based NOSQL Systems Key-Value NOSQL Systems Column-Based NOSQL Systems Graph-Based NOSQL Systems NOSQL refers to Not Only SQL. A NOSQL system is commonly a distributed one that focuses on semi-structured data storage, high performance, availability, replication and scalability. These type of systems developed to meet the needs of large-scale internet applications where a traditional SQL database could not.","tags":["computer science","databases"],"title":"NOSQL","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":"When we had full knowledge of the states, we could use Markov Decision Processes to find the optimal policy. When this assumption breaks down, we need to come up with our best approximation. This is not a far stretch from how we might handle new scenarios in our own lives. When we begin a new task, we are certainly not experts. We may learn from a teacher or set off to explore on our own. As we practice and churn out the seemingly endless variations of our endeavour, we begin to develop a sense of what works and what doesn\u0026rsquo;t. We may not be able to articulate the exact rules that we follow, but we can certainly tell when we are doing well or poorly.\nIn lieu of a conscious agent with human intelligence, we can approximate the policy using a gradient-based approach. We will use the gradient of the expected reward with respect to the policy parameters to update the policy. Methods that use this approach are called policy gradient methods.\n","date":1699768800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699768800,"objectID":"a2be0420fc1b40d3ff70917fe51205d0","permalink":"https://ajdillhoff.github.io/notes/policy_gradient_methods/","publishdate":"2023-11-12T00:00:00-06:00","relpermalink":"/notes/policy_gradient_methods/","section":"notes","summary":"When we had full knowledge of the states, we could use Markov Decision Processes to find the optimal policy. When this assumption breaks down, we need to come up with our best approximation. This is not a far stretch from how we might handle new scenarios in our own lives. When we begin a new task, we are certainly not experts. We may learn from a teacher or set off to explore on our own.","tags":["reinforcement learning"],"title":"Policy Gradient Methods","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents History and Development Schemas Data Types Creation Constraints Retrieving Data Modifying Data Nested Queries Joined Tables Aggregate Functions Grouping WITH Clause Modifying Tables Summary History and Development Structured Query Language (SQL) is a database language for managing data in a relation DBMS. Its original inception was based on a paper by Edgar F. Codd in 1970 titled A Relational Model of Data for Large Shared Data Banks (Codd 1970). Two employees working at IBM in the 1970s, Donald D. Chamberlin and Raymond F. Boyce, developed the first version of SQL in 1974 (Chamberlin and Boyce 1974).\nThe first official standard of SQL was SQL-86, or SQL1, which was published in 1986 by the American National Standards Institute (ANSI). The following table shows the release dates of major SQL standards along with a brief description of the changes made in each version.\nStandard Name Description SQL-86 SQL1 First official standard of SQL SQL-89 SQL2 Added support for integrity constraints, views, and assertions SQL-92 SQL2 Added support for triggers, recursive queries, and support for procedural programming SQL:1999 SQL3 Added support for object-relational features SQL:2003 SQL3 Added support for XML, window functions, and support for regular expressions SQL:2006 SQL3 Added more XML storage features and XQuery support SQL:2008 SQL3 Added support for TRUNCATE TABLE and enhanced MERGE statements SQL:2011 SQL3 Added support for temporal data SQL:2016 SQL3 Added support for JSON SQL:2023 SQL3 Added support for Propery Graph Queries and new JSON features Schemas In our Introduction to Databases we discussed the concept of a schema as a definition of the structure of a database. In SQL, a schema is a collection of database objects, such as tables, views, and indexes. A schema is owned by a database user and has the same name as the user. A database user can own multiple schemas, and a schema can be owned by multiple users. A schema can also be owned by a role, which is a collection of users. A role can own multiple schemas, and a schema can be owned by multiple roles.\nThere are several practical reasons for which we would want to create multiple schemas. For example, a database might be used by both a Human Resources and Healthcare Management application. Creating two separate schemas would ensure that data for each application is kept secure from unauthorized users. Multiple schemas are also used for testing and development processes. Large structural changes to an application may require a new scheme to be created. New features can be developed in the new schema while the old schema is still being used by the application.\nThe following command creates a new schema named MedApp and assigns it to the user MedAdmin.\nCREATE SCHEMA MedApp AUTHORIZATION MedAdmin; Data Types SQL supports a wide variety of data types. The following table shows the most common data types supported by SQL.\nData Type Description CHAR(n) Fixed-length character string. The maximum length is n characters. VARCHAR(n) Variable-length character string. The maximum length is n characters. INT Integer value. The maximum value is 2^31 - 1. SMALLINT Integer value. The maximum value is 2^15 - 1. DECIMAL(i,j) Fixed-point number. The maximum precision is 38 digits. The maximum scale is 38 digits. NUMERIC(i,j) Fixed-point number. The maximum precision is 38 digits. The maximum scale is 38 digits. REAL Floating-point number. The maximum precision is 6 digits. DOUBLE Floating-point number. The maximum precision is 15 digits. DATE Date value. The range is 1000-01-01 to 9999-12-31. TIME Time value. The range is 00:00:00 to 23:59:59. TIMESTAMP Date and time value. The range is 1000-01-01 00:00:00 to 9999-12-31 23:59:59. CLOB(n) Specifies columns with large text values. Maximum length specified in kilobytes (K), megabytes (M), or gigabytes (G) BIT(n) Fixed-length bit string. BIT VARYING(n) Variable-length bit string. BLOB(n) Binary Large Object - used for images, video, and other large items. Creation Creating schemas, databases, and tables is done with the CREATE command. The following command creates a new database named RPG.\nCREATE DATABASE RPG; When creating a new table, we must specify the name of the table and the attributes of the table. The following command creates a new table named Users with four attributes.\nCREATE TABLE Users ( user_id INT, username VARCHAR(50), email VARCHAR(50), created_at TIMESTAMP ); Constraints Constraints allow us to add rules to our database that ensure the integrity of our data. There are several types of constraints that can be added to a table. For example, if a user is deleted, we may want to delete all of the user\u0026rsquo;s posts as well. This can be accomplished by adding a CASCADE constraint to the DELETE statement. We can also set a default value to each attribute. Constraints such as CHECK and UNIQUE can be added to ensure that the data is valid and unique. The following table shows the most common constraints supported by SQL.\nConstraint Description NOT NULL Ensures that a column cannot have a NULL value. UNIQUE Ensures that all values in a column are unique. PRIMARY KEY A combination of a NOT NULL and UNIQUE. FOREIGN KEY Ensures that values in a column match values in another table\u0026rsquo;s column. CHECK Ensures that all values in a column satisfy a specific condition. DEFAULT Sets a default value for a column when no value is specified. INDEX Used to create and retrieve data from the database very quickly. AUTO INCREMENT Automatically generates a unique number when a new record is inserted into a table. When creating the Users table above, we may want to ensure that the user_id attribute is unique. We can do this by adding a UNIQUE constraint to the user_id attribute. It is also possible to have it auto increment so that we do not have to specify a value for it when inserting a new user.\nCREATE TABLE Users ( user_id INT UNIQUE AUTO_INCREMENT, username VARCHAR(50), email VARCHAR(50), created_at TIMESTAMP ); The following command creates a new table named Characters with a VARCHAR attribute named Name which is set to NOT NULL.\nCREATE TABLE Characters ( Name VARCHAR(50) NOT NULL ); Constraints can also be added after the initial attribute declaration. When creating the Characters table, if we want to state that the user_id field should be a foreign key, we can add a FOREIGN KEY constraint to the user_id attribute.\nCREATE TABLE Characters ( id INT UNIQUE AUTO_INCREMENT, Name VARCHAR(50) NOT NULL, user_id INT, CONSTRAINT fk_user_id FOREIGN KEY (user_id) REFERENCES Users(user_id) ); The constraint is given the name fk_user_id and is added to the user_id attribute. The FOREIGN KEY constraint states that the user_id attribute references the user_id attribute in the Users table.\nRetrieving Data Retrieving data from an SQL database is done with an SFW query, SELECT-FROM-WHERE.\nSELECT \u0026lt;attribute list\u0026gt; FROM \u0026lt;table list\u0026gt; WHERE \u0026lt;condition\u0026gt; For example, we can get the experience and level of a character named Atticus from the Characters table with the following query.\nSELECT experience, level FROM Characters WHERE Name = \u0026#39;Atticus\u0026#39;; The attributes we retrieve in a query are referred to as the projection attributes. This query SELECT~s a ~Character from all rows of the Character table which satisfy the selection condition of the WHERE clause. We can also query the e-mail addresses of all users who have a character that is a human.\nSELECT email FROM Users, Characters, Races WHERE Users.user_id = Characters.user_id AND Characters.race_id = Races.id AND Races.name = \u0026#39;Human\u0026#39;; The WHERE clause in this example is an example of a join condition since it combines attributes from multiple tables. Note that there are two tables which have a user_id attribute, so we must differentiate them by prepending the table name before the attribute name. This is how ambiguities are solved in SQL.\nYou can also use the AS keyword to shorthand the table names in your query. The previous query can be rewritten as\nSELECT U.username FROM Users AS U, Characters AS C, Races AS R WHERE U.user_id = C.user_id AND R.id = C.race_id AND R.name = \u0026#39;Human\u0026#39;; Duplicate Return Values The previous query returns the names of all users who have a Human character. If a user has multiple characters that are Human, it will return their name multiple times. If we are instead only interested in the names of users who have a Human character, we can use the DISTINCT keyword to remove duplicate values.\nSELECT DISTINCT U.username FROM Users AS U, Characters AS C, Races AS R WHERE U.user_id = C.user_id AND R.id = C.race_id AND R.name = \u0026#39;Human\u0026#39;; Tables as Sets SQL uses some set operations from set theory. It supports the UNION, set difference EXCEPT, and set intersection INTERSECT operations. The following query returns the names of all users who have a Human character or a Gnome character.\n(SELECT DISTINCT U.username FROM Users as U, Characters as C, Races as R WHERE U.user_id = C.user_id AND C.race_id = R.id AND R.name = \u0026#39;Human\u0026#39;) UNION (SELECT DISTINCT U.username FROM Users as U, Characters as C, Races as R WHERE U.user_id = C.user_id AND C.race_id = R.id AND R.name = \u0026#39;Gnome\u0026#39;); If we wanted to find the users who had both a Human character and a Gnome character, we could use the INTERSECT operator instead.\n(SELECT DISTINCT U.username FROM Users as U, Characters as C, Races as R WHERE U.user_id = C.user_id AND C.race_id = R.id AND R.name = \u0026#39;Human\u0026#39;) INTERSECT (SELECT DISTINCT U.username FROM Users as U, Characters as C, Races as R WHERE U.user_id = C.user_id AND C.race_id = R.id AND R.name = \u0026#39;Gnome\u0026#39;); We can also use the EXCEPT operator to find the users who have a Human character but not a Gnome character.\n(SELECT DISTINCT U.username FROM Users as U, Characters as C, Races as R WHERE U.user_id = C.user_id AND C.race_id = R.id AND R.name = \u0026#39;Human\u0026#39;) EXCEPT (SELECT DISTINCT U.username FROM Users as U, Characters as C, Races as R WHERE U.user_id = C.user_id AND C.race_id = R.id AND R.name = \u0026#39;Gnome\u0026#39;); Pattern Matching SQL supports pattern matching with the LIKE operator. The LIKE operator is used in the WHERE clause to search for a specified pattern in a column. This is different from equality operators since it allows us to search for patterns rather than exact matches. The following table shows the most common wildcards used in SQL.\nWildcard Description % Matches any string of zero or more characters. _ Matches any single character. [] Matches any single character within the brackets. [^] Matches any single character not within the brackets. The following query returns the names of all simple items in the Items table. These can be found based on their description, since the term simple is not explicitly mentioned in the name.\nSELECT name FROM Items WHERE description LIKE \u0026#39;%simple%\u0026#39;; We can also query based on arithmetic ranges. For example, we might be interested in the items that are less than 100 gold.\nSELECT name FROM Items WHERE value \u0026lt; 100; Ordering SQL allows us to order the results of our query with the ORDER BY clause. The following query returns the names of all items in the Items table ordered by their value.\nSELECT name FROM Items ORDER BY value; We can also order by multiple attributes. The following query returns the names of all items in the Items table ordered by their value and then their name.\nSELECT name FROM Items ORDER BY value, name; Modifying Data Inserting Data We previously saw an example of inserting new data. Let\u0026rsquo;s insert a new user account to our table. If we are inserting a value for every attribute, we can omit the attribute list.\nINSERT INTO Users VALUES (7, \u0026#39;Alex\u0026#39;, \u0026#39;alex.dillhoff@uta.edu\u0026#39;, \u0026#39;2023-10-31 15:26:17\u0026#39;); If we are only inserting values for some attributes, we must specify the attribute list.\nINSERT INTO Users (user_id, username, email) VALUES (7, \u0026#39;Alex\u0026#39;, \u0026#39;alex.dillhoff@uta.edu\u0026#39;); If we attempt to leave out a value for an attribute that is not nullable, we will get an error. While working on our database, we may have realized that some of these important attributes should always be specified. We can add a NOT NULL constraint to these attributes to ensure that they are always specified. We will look at ways of modifying tables in the next section.\nUpdating Data Updating data is a common task and is easily supported by the UPDATE command. In an RPG, players will use items, gain experience, and level up. All of these will require modifications to existing tables. For example, if we wish to update the experience of a character, we can use the following query.\nUPDATE Characters SET experience = experience + 100 WHERE name = \u0026#39;Atticus\u0026#39;; Deleting Data Deleting a tuple or several tuples is straightforward in SQL. The following query deletes the user with the user_id of 7 from the Users table.\nDELETE FROM Users WHERE user_id = 7; If we want to delete all tuples from a table, we can use the TRUNCATE TABLE command. This command is faster than deleting all tuples with the DELETE command since it does not log each deletion. However, it cannot be used if the table is referenced by a foreign key constraint.\nTRUNCATE TABLE Users; When deleting tuples from a database, it\u0026rsquo;s important to consider any foreign key constraints that the table may have. If we delete a tuple from a table that is referenced by a foreign key constraint, we may end up with orphaned tuples. For example, if we delete a user from the Users table, we may end up with a character that has no user. We can avoid this by adding a CASCADE constraint to the DELETE statement. This will delete all tuples that reference the tuple we are deleting.\nNested Queries Nested queries allow us to make more complex queries on subsets of data returned from an original query. A nested query can be placed in any of the SELECT, FROM, or WHERE clauses. The query that uses the results of the nested query is called the outer query.\nWhat if we wanted a list of users who had at least 1 character whose class was the least represented class across all characters? We could first make a query to identify which class is the least represented before using that to find the users who have a character with that class.\nSELECT username FROM Users WHERE user_id IN (SELECT user_id FROM Characters WHERE class_id = (SELECT class_id FROM Characters GROUP BY class_id ORDER BY COUNT(*) ASC LIMIT 1)); The innert-most query returns the class_id of the least represented class by counting the number of characters in each class and ordering them in ascending order. The middle query returns the user_id of all characters whose class is the least represented class. The outer query returns the username of all users who have a character with the least represented class.\nPay attention to the second WHERE clause that uses the = operator instead of IN. This is because the nested query returns a single value and single tuple. If we used the IN operator, we would get an error since MySQL does not support the LIMIT and IN/ALL/ANY/SOME operators together.\nCorrelated Nested Queries Some nested queries would have to execute for each tuple in the outer query. Consider the following query which returns the name and ID of all Human characters.\nSELECT C.id, C.name FROM Characters as C WHERE C.race_id IN (SELECT R.id FROM Races as R WHERE R.name = \u0026#39;Human\u0026#39;); This query is an example of a correlated nested query since the inner query is dependent on the outer query. The inner query must be executed for each tuple in the outer query. This can be inefficient if the outer query returns a large number of tuples.\nSince this query uses only an IN operator, we can rewrite it as a single block query.\nSELECT C.id, C.name FROM Characters as C, Races as R WHERE C.race_id = R.id AND R.name = \u0026#39;Human\u0026#39;; Let\u0026rsquo;s revisit an earlier query to introduce the EXISTS operator. If we want to query the user names and IDs of all users who have an elf character, we can use the following query.\nSELECT U.id, U.username FROM Users as U WHERE EXISTS (SELECT * FROM Characters as C, Races as R WHERE C.user_id = U.id AND C.race_id = R.id AND R.name = \u0026#39;Elf\u0026#39;); We can also use the NOT EXISTS operator to find the users who do not have an elf character. This works opposite to the EXISTS operator.\nJoined Tables Joined tables are the result of a query that combines rows from two or more tables. The syntax itself may be easier to understand as compared to the nested queries written above. The following query returns the names of all users who have a Human character.\nSELECT U.username, C.name FROM (Users as U JOIN Characters as C ON U.user_id = C.user_id), Races as R WHERE C.race_id = R.id AND R.name = \u0026#39;Human\u0026#39;; Here, the JOIN function is used to combine both tables on the condition that the user_id of the Users table is equal to the user_id of the Characters table. The WHERE clause is used to filter the results to only those that have a Human character.\nThis type of join is also referred to as an inner join since it only returns rows that satisfy the join condition. There are several other types of joins that we can use to combine tables. If we wanted to return the same information along with all of the users who do not have a Human character, we can use a left outer join.\nSELECT U.username, C.name FROM (Users as U LEFT OUTER JOIN (Characters as C JOIN Races as R ON C.race_id = R.id AND R.name = \u0026#39;Human\u0026#39;) ON U.user_id = C.user_id); Aggregate Functions Aggregate functions are used to perform calculations on a set of values and return a single value. The following table shows the most common aggregate functions supported by SQL.\nFunction Description AVG() Returns the average value of a numeric column. COUNT() Returns the number of rows that match a specified criteria. MAX() Returns the maximum value of a column. MIN() Returns the minimum value of a column. SUM() Returns the sum of all values in a column. These can be used in the SELECT clause. The following query returns the average level of all characters.\nSELECT AVG(level) FROM Characters; These statistics can be used with more complex queries. The following query returns the name of the user with the highest level character.\nSELECT U.username FROM Users as U, Characters as C WHERE U.user_id = C.user_id AND C.level = (SELECT MAX(level) FROM Characters); In the next example, the query returns the tuple of the highest level Human character.\nSELECT C.* FROM (Characters as C JOIN Races as R ON C.race_id = R.id AND R.name = \u0026#39;Human\u0026#39;) WHERE C.level = (SELECT MAX(level) FROM Characters); Grouping As we just saw, aggregate functions permit some preliminary data analysis. We can take this further using grouping. For example, we calculated above the average level of all characters. What if we wanted to compute the average level of each class? We can use the GROUP BY clause to group the tuples by class.\nSELECT C.class_id, AVG(C.level) FROM Characters as C GROUP BY C.class_id; In the next example, we will run a similar query except we will also include the number of distinct users who have a character of that class.\nSELECT C.class_id, AVG(C.level), COUNT(DISTINCT C.user_id) FROM Characters as C GROUP BY C.class_id; WITH Clause Let\u0026rsquo;s say we wanted to get the actual class name along with the users that had a character with the least represented class. We can use a nested query in the FROM clause to get the class name.\nSELECT username, Classes.name FROM Users, Classes WHERE user_id IN (SELECT user_id FROM Characters WHERE class_id = (SELECT class_id FROM Characters GROUP BY class_id ORDER BY COUNT(*) ASC LIMIT 1)) AND Classes.id = (SELECT class_id FROM Characters GROUP BY class_id ORDER BY COUNT(*) ASC LIMIT 1); This query might immediately come across as inefficient since we are making the same query multiple times. If you agree, your intuition would be right. We can use the WITH clause to make this query more efficient.\nWITH LeastUsedClass AS ( SELECT class_id FROM Characters GROUP BY class_id ORDER BY COUNT(*) ASC LIMIT 1 ) SELECT U.username, C.name FROM Users U JOIN Characters CH ON U.user_id = CH.user_id JOIN Classes C ON C.id = CH.class_id WHERE CH.class_id = (SELECT class_id FROM LeastUsedClass); Modifying Tables Modifying databases used in production is inevitable. Typically, you will modify an offline test version before deploying it, but the process is the same. The ALTER TABLE command is used to modify tables. The following table shows the most common modifications that can be made to a table.\nCommand Description ADD Adds a new column to the table. DROP Deletes a column from the table. MODIFY Changes the data type of a column. RENAME Changes the name of a column. When we originally created the Users table, we did not specify a NOT NULL constraint for the username attribute. We can add this constraint with the following command.\nALTER TABLE Users MODIFY username VARCHAR(50) NOT NULL; Another mistake that was made was with the name of the ID column. We can rename this column with the following command.\nALTER TABLE Users RENAME COLUMN user_id TO id; Scenario: Updating Foreign Keys Let\u0026rsquo;s say we want to delete anything related to a user if that user is deleted from the Users table. That means all characters and inventories associated with those characters should be deleted. Currently, attempting to delete a user will fail with the following error:\nSQL Error [1451] [23000]: Cannot delete or update a parent row: a foreign key constraint fails (`rpg`.`Inventory`, CONSTRAINT `Iventory_ibfk_1` FOREIGN KEY (`character_id`) REFERENCES `Characters` (`id`)) It looks like the foreign key also has a typo. Let\u0026rsquo;s recreate this foreign key so that it will delete all characters and inventories associated with a user. First we drop the current key.\nALTER TABLE Inventory DROP FOREIGN KEY Iventory_ibfk_1; Then we add a new one.\nALTER TABLE Inventory ADD CONSTRAINT Inventory_ibfk_1 FOREIGN KEY (character_id) REFERENCES Characters(id) ON DELETE CASCADE; Summary A typical SQL query consists of a SELECT clause, a FROM clause, and a WHERE clause. The SELECT clause specifies the attributes to be returned. The FROM clause specifies the tables to be queried. The WHERE clause specifies the conditions that must be satisfied for a tuple to be returned.\nAs we saw in the examples above, there are up to 6 clauses that can be used in a query. The following table shows the clauses that can be used in a query and the order in which they must appear.\nClause Description SELECT Specifies the attributes to be returned. FROM Specifies the tables to be queried. WHERE Specifies the conditions that must be satisfied for a tuple to be returned. GROUP BY Groups the tuples by a specified attribute. HAVING Specifies the conditions that must be satisfied for a group to be returned. ORDER BY Specifies the order in which the tuples are returned. References Chamberlin, Donald D., and Raymond F. Boyce. 1974. “SEQUEL: A Structured English Query Language.” In Proceedings of the 1974 ACM SIGFIDET (Now SIGMOD) Workshop on Data Description, Access and Control, 249–64. SIGFIDET ’74. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/800296.811515. Codd, E. F. 1970. “A Relational Model of Data for Large Shared Data Banks.” Communications of the Acm 13 (6): 377–87. https://doi.org/10.1145/362384.362685. ","date":1698642000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698642000,"objectID":"3cd2d98d654a4ed902465704df9083fb","permalink":"https://ajdillhoff.github.io/notes/structured_query_language/","publishdate":"2023-10-30T00:00:00-05:00","relpermalink":"/notes/structured_query_language/","section":"notes","summary":"Table of Contents History and Development Schemas Data Types Creation Constraints Retrieving Data Modifying Data Nested Queries Joined Tables Aggregate Functions Grouping WITH Clause Modifying Tables Summary History and Development Structured Query Language (SQL) is a database language for managing data in a relation DBMS. Its original inception was based on a paper by Edgar F. Codd in 1970 titled A Relational Model of Data for Large Shared Data Banks (Codd 1970).","tags":["computer science","databases"],"title":"Structured Query Language","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents An Online RPG Database From Schema to Database Database Management Systems Creating our RPG Database Recommended Reading: Chapters 1 and 2 from (Elmasri and Navathe 2015)\nDatabases allow us to store, retrieve, and edit different types of data. They should be scalable, secure, and reliable. They should also be able to handle concurrent access and be able to recover from failures. There are multiple types of databases that are optimized for different use cases. Tabular data, for example, is typically stored in a relational database. Large format data such as images, videos, and audio are typically stored in a non-relational database.\nCreating, deploying, and maintaining databases is facilitated through a database management system (DBMS). A DBMS is a software system that allows us to interact with a database. It provides an interface for us to create, read, update, and delete data. It also provides a way for us to define the structure of our data and the relationships between different pieces of data. Examples of DBMSs include MySQL, PostgreSQL, and MongoDB.\nOnce a database is deployed, we can interact with it a number of ways. Most DBMSs include a client which allows us to interact with the database through a command line interface. We can also interact with the database through a programming language such as Python or Java.\nIt is important to emphasize that a database is not the same thing as a file system. A file system is a way to store data on a disk, whereas a database is a way to store data in a file system. File systems are good at managing unstructured data with little regard to the relationships inherit in the data itself. What if multiple people working on the same document try to save their changes at the same time? What if a user tries to delete a file that is currently being used by another user? These are problems that a file system is not designed to handle.\nAn Online RPG Database To introduce some foundational terms and concepts of databases, let\u0026rsquo;s design and create a database for an online RPG. In this game, users can create accounts, make multiple characters, store items for their characters, and embark on quests to level up their characters. Even from this simple description, we can start separating our data into different entities and relationships. Each logical entity in our game will be represented by a table in our database. The attributes of each table will be represented by columns in our database. For this database, we will need at least the following tables:\nUsers Characters Items Inventory Quests We may add or modify these depending on the finer details. If you are not familiar with online RPG games, don\u0026rsquo;t worry. We will be sure to include the necessities to get us started. Let\u0026rsquo;s start with the first table, Users.\nUsers A User represents an online account that is unique to each person who plays the game. It should contain the username, email, and date that it was created, which we will call created_at. This is enough information for now. Using this, we can create our first table. There is one more attribute that wasn\u0026rsquo;t explicitly mentioned. Each User in our table should have a unique identifier. This is called a primary key. We will use a sequentially increasing number starting at 1 for our primary key. This is a common practice, but it is not the only way to do it. We will call this column user_id. The full table is showing below.\nUsers\nuser_id: primary key username email created_at Characters It is common for users to have multiple characters so they can experience the full range of our game. This table will have more attributes than the Users table since there are a wide range of stats that our characters can have, such as their name, level, experience, and health. We will also need to know which user each character belongs to. We can do this by adding a column called user_id which will be a foreign key to the Users table. This will allow us to link each character to the user that created it. The full table is shown below.\nCharacters\ncharacter_id: primary key user_id: foreign key name level experience health created_at Items As our user\u0026rsquo;s play, they will collect items such as weapons, armor, and potions. As our game evolves, our game designers will add more items to the game. A table for our items is shown below.\nItems\nitem_id: primary key name value Inventory Our users will need a way to store their items. We can do this by creating a table called Inventory. This table will have a foreign key to the Characters table so we can link each item to the character that owns it. It will also have a foreign key to the Items table so we can link each item to the item that it represents. We will also need to know how many of each item our users have. We can do this by adding a column called quantity. The full table is shown below.\nInventory\ninventory_id: primary key character_id: foreign key item_id: foreign key quantity Quests No RPG would be complete without quests that our player\u0026rsquo;s could embark upon. The Quests table will have a name, description, and a reward. In the case of multiple rewards, we can create a separate table called QuestRewards that will have a foreign key to the Quests table and a foreign key to the Items table. This will allow us to link each quest to the items that it rewards. This means that the Quests table does not need an explicit reference to the reward item. We can look those up separately. The full table is shown below.\nQuests\nquest_id: primary key name description reward_experience min_level QuestRewards\nquest_reward_id: primary key quest_id: foreign key item_id: foreign key A Few Extras There are a few more tables we should add to round out our characters. Most RPGs allow the users to create characters of different races, such as a human, orc, or elf, as well as the characters class, which defines what sort of abilities the character will have.\nRace\nrace_id: primary key name Class\nclass_id: primary key name With the addition of these two tables, let\u0026rsquo;s add foreign keys to our original Characters table. We will add a race_id and a class_id. The full table is shown below.\nCharacters\ncharacter_id: primary key user_id: foreign key name level experience health race_id: foreign key class_id: foreign key created_at That\u0026rsquo;s it! We have all the tables we need to get us started. All tables with example data are shown below. You\u0026rsquo;ll notice that each of the primary IDs in the tables below have been renamed to id. Besides giving us extra room to display the table, the primary key is always unique to the table, so we don\u0026rsquo;t need to include the table name in the column name.\nUsers\n\\begin{array}{|r|l|l|l|} \\hline \\text{id} \u0026amp; \\text{username} \u0026amp; \\text{email} \u0026amp; \\text{created_at} \\\\ \\hline 1 \u0026amp; \\text{Naomi} \u0026amp; \\text{player1@example.com} \u0026amp; \\text{2023-01-01 10:00:00} \\\\ 2 \u0026amp; \\text{Clarissa} \u0026amp; \\text{player2@example.com} \u0026amp; \\text{2023-01-02 11:00:00} \\\\ 3 \u0026amp; \\text{Avasarala} \u0026amp; \\text{player3@example.com} \u0026amp; \\text{2023-01-03 12:00:00} \\\\ \\hline \\end{array}\nCharacters\n\\begin{array}{|r|r|l|r|r|r|r|r|l|} \\hline \\text{id} \u0026amp; \\text{user_id} \u0026amp; \\text{name} \u0026amp; \\text{class_id} \u0026amp; \\text{race_id} \u0026amp; \\text{level} \u0026amp; \\text{experience} \u0026amp; \\text{health} \u0026amp; \\text{created_at} \\\\ \\hline 1 \u0026amp; 1 \u0026amp; \\text{Atticus} \u0026amp; 1 \u0026amp; 1 \u0026amp; 10 \u0026amp; 1000 \u0026amp; 100 \u0026amp; \\text{2023-01-01 10:10:00} \\\\ 2 \u0026amp; 1 \u0026amp; \\text{Bobbie} \u0026amp; 2 \u0026amp; 2 \u0026amp; 15 \u0026amp; 1500 \u0026amp; 200 \u0026amp; \\text{2023-01-01 10:20:00} \\\\ 3 \u0026amp; 2 \u0026amp; \\text{Raimi} \u0026amp; 3 \u0026amp; 3 \u0026amp; 8 \u0026amp; 800 \u0026amp; 90 \u0026amp; \\text{2023-01-02 11:10:00} \\\\ 4 \u0026amp; 3 \u0026amp; \\text{Beef} \u0026amp; 4 \u0026amp; 4 \u0026amp; 12 \u0026amp; 1200 \u0026amp; 110 \u0026amp; \\text{2023-01-03 12:10:00} \\\\ 5 \u0026amp; 2 \u0026amp; \\text{Demon} \u0026amp; 4 \u0026amp; 4 \u0026amp; 12 \u0026amp; 1200 \u0026amp; 110 \u0026amp; \\text{2023-01-05 12:10:00} \\\\ \\hline \\end{array}\nItems\n\\begin{array}{|r|l|r|r|} \\hline \\text{id} \u0026amp; \\text{name} \u0026amp; \\text{value} \\\\ \\hline 1 \u0026amp; \\text{Sword} \u0026amp; 100 \\\\ 2 \u0026amp; \\text{Shield} \u0026amp; 150 \\\\ 3 \u0026amp; \\text{Staff} \u0026amp; 200 \\\\ 4 \u0026amp; \\text{Bow} \u0026amp; 250 \\\\ \\hline \\end{array}\nInventory\n\\begin{array}{|r|r|r|r|} \\hline \\text{id} \u0026amp; \\text{character_id} \u0026amp; \\text{item_id} \u0026amp; \\text{quantity} \\\\ \\hline 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \\\\ 2 \u0026amp; 2 \u0026amp; 2 \u0026amp; 1 \\\\ 3 \u0026amp; 3 \u0026amp; 3 \u0026amp; 1 \\\\ 4 \u0026amp; 4 \u0026amp; 4 \u0026amp; 1 \\\\ \\hline \\end{array}\nQuests\n\\begin{array}{|r|l|l|r|l|r|} \\hline \\text{id} \u0026amp; \\text{name} \u0026amp; \\text{description} \u0026amp; \\text{reward_experience} \u0026amp; \\text{min_level} \\\\ \\hline 1 \u0026amp; \\text{Linken\u0026rsquo;s Sword} \u0026amp; \\text{Desc1} \u0026amp; 100 \u0026amp; 5 \\\\ 2 \u0026amp; \\text{Mankrik\u0026rsquo;s Wife} \u0026amp; \\text{Desc2} \u0026amp; 200 \u0026amp; 10 \\\\ 3 \u0026amp; \\text{The Hermit} \u0026amp; \\text{Desc3} \u0026amp; 300 \u0026amp; 15 \\\\ 4 \u0026amp; \\text{The Great Masquerade} \u0026amp; \\text{Desc4} \u0026amp; 400 \u0026amp; 20 \\\\ \\hline \\end{array}\nQuestRewards\n\\begin{array}{|r|r|r|} \\hline \\text{id} \u0026amp; \\text{quest_id} \u0026amp; \\text{item_id} \\\\ \\hline 1 \u0026amp; 1 \u0026amp; 1 \\\\ 2 \u0026amp; 2 \u0026amp; 2 \\\\ 3 \u0026amp; 3 \u0026amp; 3 \\\\ 4 \u0026amp; 4 \u0026amp; 4 \\\\ \\hline \\end{array}\nRaces\n\\begin{array}{|r|l|} \\hline \\text{race_id} \u0026amp; \\text{name} \\\\ \\hline 1 \u0026amp; \\text{Human} \\\\ 2 \u0026amp; \\text{Elf} \\\\ 3 \u0026amp; \\text{Dwarf} \\\\ 4 \u0026amp; \\text{Orc} \\\\ \\hline \\end{array}\nClasses\n\\begin{array}{|r|l|} \\hline \\text{class_id} \u0026amp; \\text{name} \\\\ \\hline 1 \u0026amp; \\text{Warrior} \\\\ 2 \u0026amp; \\text{Mage} \\\\ 3 \u0026amp; \\text{Rogue} \\\\ 4 \u0026amp; \\text{Paladin} \\\\ \\hline \\end{array}\nFrom Schema to Database What we did in the previous example is created a database schema based on our entities. A schema does not represent the entire picture of our data model. Relationships and other constraints are not represented in the schema. The data model itself defines the structure of a database, including data types, relationships, constraints, and a set of operations for performing basic functions like retrieving and updating data.\nThe Three-Schema Architecture The three-schema architecture is a way to separate the different aspects of a database. The three schemas are the external schema, the conceptual schema, and the internal schema. The internal schema describes how the data is stored on disk. Unless we are working on the backend of the database, we typically do not need to worry about the internal level. The external schema describes how the data is viewed by the user. This is the level that we interact with when we use a DBMS. The conceptual schema is the middle layer that describes the logical structure of the data. This is the level that we are working with when we create a schema.\nUnder this architecture, we can modify the internal schema without affecting the external schema. This is important because it allows us to change the way that the data is stored without affecting the applications that use it. We can also modify the external schema without affecting the internal schema. This allows us to change the way that the data is viewed without affecting the applications that use it. This concept of data independence is one of the most important features of a DBMS.\nDatabase Management Systems With our database defined, we can use it to make queries about the records that it stores. How we access that database depends on the DBMS that we are using. The database itself is can be modified and changed without affecting the applications that use it. We can also create multiple views of our data dynamically. For example, we can create a view that shows all of the items that a user has in their inventory, or show all of the characters that belong to a specific user. This is all done without modifying the underlying data. This is a powerful feature of databases that allows us to create complex applications that can be easily modified and updated.\nA transaction is a set of operations that are performed on a database. Transactions are typically used to ensure that the database is in a consistent state. For example, if we want to transfer money from one account to another, we need to make sure that the money is removed from one account and added to the other. If we fail to do this, we could end up with money that is neither in the original account nor the destination account. Transactions allow us to perform these operations in a way that guarantees that the database is in a consistent state.\nA DBMS must ensure transactional properties such as isolation, which ensure that each transaction executes in isolation from others, and atomicity, which ensures that either all operations in a transaction are executed or none are.\nDBMS Languages A DBMS provides a way for us to interact with the database. Depending on the level of abstraction and the DBMS itself, a specific language is used to perform basic operations on the database. The most common languages are data definition languages (DDLs) and data manipulation languages (DMLs). A DDL is used to define the structure of the database, such as creating tables and defining relationships between them. A DML is used to perform operations on the data itself, such as inserting, updating, and deleting records.\nA common query language called Structured Query Language (SQL) defines both DDLs and DMLs. For example, to create our User table from above, we can use the following SQL statement:\nCREATE TABLE Users ( user_id INT PRIMARY KEY, username VARCHAR(255), email VARCHAR(255), created_at DATETIME ); Note that we must specify a type for each attribute in our table. SQL also provides a DML, we can use to insert records into our table:\nINSERT INTO Users (user_id, username, email, created_at) VALUES (1, \u0026#39;Naomi\u0026#39;, \u0026#39;player1@example.com\u0026#39;, \u0026#39;2023-01-01 10:00:00\u0026#39;); DBMS Interfaces A DBMS provides an interface for us to interact with the database. This interface can be a command line interface, a graphical user interface, or a programming language interface. Other interfaces using natural language or voice can also be found in the wild. With the rapid advancement of machine learning, these interfaces are becoming more and more common. Here is an example of a chatbot that can be used to query a database.\nCreating our RPG Database For this example, we will be using MySQL. We only want to make sure that we have MySQL installed and are able to interface with the command line. You can find a thorough installation guide here. Once it is installed and configured, start the MySQL server and log in using the following command:\nmysql -u root -p You should be prompted for a password. If you have not set a password, you can leave it blank. Once you are logged in, you should see a prompt that looks like this:\nmysql\u0026gt; Let\u0026rsquo;s create a database for our RPG. We can do this with the following command:\nCREATE DATABASE rpg; We can verify that the database was created by listing all of the databases on the server:\nSHOW DATABASES; You should see the rpg database in the list. We can now use this database to create our tables. We can do this with the following command:\nUSE rpg; This will tell MySQL to use the rpg database for all subsequent commands. We can now create our Users table:\nCREATE TABLE Users ( user_id INT PRIMARY KEY, username VARCHAR(255), email VARCHAR(255), created_at DATETIME ); We can verify that the table was created by listing all of the tables in the database:\nSHOW TABLES; You should see the Users table in the list. We can now insert some data into the table:\nINSERT INTO Users (user_id, username, email, created_at) VALUES (1, \u0026#39;Naomi\u0026#39;, \u0026#39;player1@example.com\u0026#39;, \u0026#39;2023-01-01 10:00:00\u0026#39;), (2, \u0026#39;Clarissa\u0026#39;, \u0026#39;player2@example.com\u0026#39;, \u0026#39;2023-01-02 11:00:00\u0026#39;), (3, \u0026#39;Avasarala\u0026#39;, \u0026#39;player3@example.com\u0026#39;, \u0026#39;2023-01-03 12:00:00\u0026#39;); We can verify that the data was inserted by querying the table:\nSELECT * FROM Users; You should see the data that we inserted in the table. We can now create the rest of our tables:\nCREATE TABLE Characters ( character_id INT PRIMARY KEY, user_id INT, name VARCHAR(255), level INT, experience INT, health INT, created_at DATETIME ); CREATE TABLE Items ( item_id INT PRIMARY KEY, name VARCHAR(255), value INT ); CREATE TABLE Inventory ( inventory_id INT PRIMARY KEY, character_id INT, item_id INT, quantity INT ); CREATE TABLE Quests ( quest_id INT PRIMARY KEY, name VARCHAR(255), description VARCHAR(255), reward_experience INT, min_level INT ); CREATE TABLE QuestRewards ( quest_reward_id INT PRIMARY KEY, quest_id INT, item_id INT ); Try creating the tables for the Races and Classes yourself. Once you are done, you can insert some data into the tables. Use the samples from above or create your own. Once you are done, you can query the tables to verify that the data was inserted correctly.\nReferences Elmasri, Ramez, and Shamkant B. Navathe. 2015. Fundamentals of Database Systems. 7th ed. Pearson. https://www.pearson.com/en-us/subject-catalog/p/fundamentals-of-database-systems/P200000003546/9780137502523. ","date":1698469200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698469200,"objectID":"85830aa0149b307a38bed69c592b16a7","permalink":"https://ajdillhoff.github.io/notes/introduction_to_databases/","publishdate":"2023-10-28T00:00:00-05:00","relpermalink":"/notes/introduction_to_databases/","section":"notes","summary":"Table of Contents An Online RPG Database From Schema to Database Database Management Systems Creating our RPG Database Recommended Reading: Chapters 1 and 2 from (Elmasri and Navathe 2015)\nDatabases allow us to store, retrieve, and edit different types of data. They should be scalable, secure, and reliable. They should also be able to handle concurrent access and be able to recover from failures. There are multiple types of databases that are optimized for different use cases.","tags":["computer science","databases"],"title":"Introduction to Databases","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Definition Finding the Minimum Spanning Tree Kruskal\u0026rsquo;s Algorithm Prim\u0026rsquo;s Algorithm Minimum spanning trees are undirected graphs that connect all of the vertices such that there are no redundant edges and the total weight is minimized. They are useful for finding the shortest path between two points in a graph. Useful application of MSTs include\nnetwork design: it is useful to know the least expensive path with respect to either latency or resource cost for telecommunications networks, transportation networks, or electrical grids. approximation algorithms: MSTs can be used to approximate the solution to the traveling salesman problem. clustering: MSTs can be used to cluster data points in a graph. image segmentation: MSTs can be used to segment images into smaller regions. Definition Let \\(G\\) be a connected, undirected graph with edges \\(E\\), vertices \\(V\\), and edge weights \\(w\\). A minimum spanning tree is a subset \\(T \\subseteq E\\) that connects all of the vertices such that the total weight is minimized. The original graph \\(G\\) is shown below.\nFigure 1: An undirected graph with redundant edges. The minimum spanning tree of the above graph is show below. All of the redundant edges have been removed, but there is still a path between each pair of nodes.\nFigure 2: The minimum spanning tree of (G). As described in Introduction to Algorithms there are two greedy algorithms for finding the minimum spanning tree of a graph (Cormen et al. 2022). These notes will review both of these, but first let\u0026rsquo;s look at a general algorithm for finding the minimum spanning tree of a graph.\nFinding the Minimum Spanning Tree The general algorithm for finding the minimum spanning tree of a graph grows a set of edges \\(T\\) from an empty set. At each step, the algorithm adds the edge with the smallest weight that does not create a cycle. The algorithm terminates when \\(T\\) is a complete tree.\nT = {} while T is not a spaning tree find the edge e with the smallest weight that does not create a cycle T = T union {e} Each edge \\(e\\) that is added must result in a tree that is a subset of the minimum spanning tree. The challenge of this algorithm is actually finding such an edge. How would we know such an edge if we saw it? We first need to define a few properties which will shine light on this.\nA cut of a graph \\(G\\) is a partition of the vertices \\(V\\) into two disjoint sets \\(S\\) and \\(V - S\\). An edge \\(e\\) crosses the cut if one of its endpoints is in \\(S\\) and the other is in \\(V - S\\). If no edge in a given set \\(E\\) crosses the cut, then that cut respects \\(E\\). An edge that is the minimum weight edge that crosses a cut is called a light edge. With these definitions, we can now formally define how to find a safe edge, which is an edge that can be added to the current set of edges \\(T\\) without creating a cycle.\nTheorem 21.1 (Cormen et al. 2022)\nLet \\(G = (V, E)\\) be a connected, undirected graph with a real-valued weight function \\(w\\) defined on \\(E\\). Let \\(A\\) be a subset of \\(E\\) that is included in some minimum spanning tree for \\(G\\), let \\((S, V - S)\\) be any cut of \\(G\\) that respects \\(A\\), and let \\(e\\) be a light edge crossing \\((S, V - S)\\). Then, edge \\(e\\) is safe for \\(A\\).\nFigure 3: Visual proof of Theorem 21.1 (Cormen et al. 2022). Proof\nThe two sets in the figure above represent vertices in \\(S\\) (orange) and vertices in \\(V - S\\) (tan). \\(T\\) is the original MST depicted in the figure. The dotted line is the new edge \\((u, v)\\) to consider. \\(A\\) is a subset of edges in \\(T\\) represented by the blue lines. If the safe edge \\((u, v)\\) is already in the original MST \\(T\\), then we are done.\nThe vertices \\(u\\) and \\(v\\) lie on opposite sides of the cut. The edge \\((u, v)\\) would introduce a cycle since there is already a path from \\(u\\) to \\(v\\) in \\(T\\) that crosses the cut via \\((x, y)\\). Since both \\((u, v)\\) and \\((x, y)\\) are light edges that cross the cut, then it must be that \\(w(u, v) \\leq w(x, y)\\).\nLet \\(T\u0026rsquo;\\) be the minimum spanning tree with \\((x, y)\\) replaced by \\((u, v)\\). That is \\(T\u0026rsquo; = T - \\{(x, y)\\} \\cup \\{(u, v)\\}\\). Since \\(T\\) is a minimum spanning tree, then \\(w(T) \\leq w(T\u0026rsquo;)\\). Since \\(w(T) = w(T\u0026rsquo;)\\), then \\(T\u0026rsquo;\\) is also a minimum spanning tree. Therefore, \\((u, v)\\) is safe for \\(A\\).\nCorollary 21.2 (Cormen et al. 2022)\nWe can also view this in terms of connected components, which are subsets of vertices that are connected by a path. If \\(C\\) and \\(C\u0026rsquo;\\) are two connected components in \\(T\\) and \\((u, v)\\) is a light edge connecting \\(C\\) and \\(C\u0026rsquo;\\), then \\((u, v)\\) is safe for \\(T\\).\nThe figure below shows a graph with two individual components. If the edge \\((u, v)\\) is a light edge, then it is safe to add it to the set of edges \\(T\\).\nFigure 4: Two connected components from a graph (left). Adding a safe edge (right). Kruskal\u0026rsquo;s Algorithm The first solution to the minimum spanning tree that we will study is called Kruskal\u0026rsquo;s algorithm. This algorithm grows a forest of trees from an empty set. At each step, the algorithm adds the lightest edge that does not create a cycle. The algorithm terminates when the forest is a single tree. This can be viewed as an agglomerative clustering algorithm. The algorithm starts with each vertex in its own cluster. At each step, the algorithm merges the two clusters that are closest together. The algorithm terminates when there is only one cluster.\nThe algorithm is given below (Cormen et al. 2022).\nKruskal\u0026rsquo;s Algorithm\nA = {} for each vertex v in G.V MAKE-SET(v) sort the edges of G.E into nondecreasing order by weight w for each edge (u, v) in G.E, taken in nondecreasing order by weight if FIND-SET(u) != FIND-SET(v) A = A union {(u, v)} UNION(u, v) return A A step-by-step example of an implementation in Python is available here.\nAnalysis The running time is dependent on how the disjoint-set of vertices is implemented. In the best known case, a disjoint-set-forest implementation should be used (Cormen et al. 2022). Creating a list of edges takes \\(O(E)\\) time. Sorting the edges takes \\(O(E \\log E)\\) time. The for loop iterates over each edge, which is \\(O(E)\\). All disjoin-set operations take \\(O((V + E)\\alpha(V))\\) time. Since the graph is connected, \\(E \\geq V - 1\\), so the total running time is \\(O(E \\log E + E + E \\alpha(V)) = O(E \\log E + E \\alpha(V)) = O(E \\log V)\\).\nPrim\u0026rsquo;s Algorithm The second solution starts at an arbitrary vertex in a set \\(A\\) and adds a new vertex to \\(A\\) in a greedy fashion. To efficiently select a new edge to add, Prim\u0026rsquo;s algorithm uses a priority queue to keep track of the lightest edge that crosses the cut. The algorithm terminates when \\(A\\) is a complete tree. The full algorithm is given below. We will step through it in more detail after.\nPrim\u0026rsquo;s Algorithm\nA = {} for each vertex v in G.V key[v] = infinity pi[v] = NIL key[r] = 0 Q = G.V while Q is not empty u = EXTRACT-MIN(Q) A = A union {u} for each vertex v in G.Adj[u] if v in Q and w(u, v) \u0026lt; key[v] pi[v] = u key[v] = w(u, v) You might look at this and wonder how the MST is represented. Prim\u0026rsquo;s algorithm implicitly maintains the set \\(A = \\{(v, v.\\pi) : v \\in V - \\{r\\} - Q\\}\\). When the while loop terminates, \\(A = \\{(v, v.\\pi) : v \\in V - \\{r\\}\\}\\), since the queue is empty. The critical part of this is to understand how the algorith changes the key values.\nA step-by-step example of an implementation in Python is available here.\nAnalysis Prim\u0026rsquo;s algorithm uses a priority queue to keep track of the lightest edge that crosses the cut. If the priority queue is implemented as a min-heap, which has a worst-case running time of \\(O(\\log V)\\) for both EXTRACT-MIN and DECREASE-KEY. The algorithm calls EXTRACT-MIN once for each vertex, which is \\(O(V \\log V)\\). The algorithm calls DECREASE-KEY once for each edge, which is \\(O(E \\log V)\\). The total running time is \\(O(V \\log V + E \\log V) = O(E \\log V)\\).\nReferences Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. Introduction to Algorithms. 4th ed. MIT Press. http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/. ","date":1697864400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697864400,"objectID":"afb8da22c3362523bbd30b8c2f572af6","permalink":"https://ajdillhoff.github.io/notes/minimum_spanning_trees/","publishdate":"2023-10-21T00:00:00-05:00","relpermalink":"/notes/minimum_spanning_trees/","section":"notes","summary":"Table of Contents Definition Finding the Minimum Spanning Tree Kruskal\u0026rsquo;s Algorithm Prim\u0026rsquo;s Algorithm Minimum spanning trees are undirected graphs that connect all of the vertices such that there are no redundant edges and the total weight is minimized. They are useful for finding the shortest path between two points in a graph. Useful application of MSTs include\nnetwork design: it is useful to know the least expensive path with respect to either latency or resource cost for telecommunications networks, transportation networks, or electrical grids.","tags":["computer science","algorithms","graphs","minimum spanning trees"],"title":"Minimum Spanning Trees","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Definition Bellman-Ford Shortest Paths on a DAG Dijkstra\u0026rsquo;s Algorithm When you hear the term shortest path, you may think of the shortest physical distance between your current location and wherever it is you\u0026rsquo;re going. Finding the most optimal route via GPS is one of the most widely used mobile applications. Physical paths are not the only types we may wish to find a shortest path for. Other examples include:\nNetwork Routing: To improve network performance, it is critical to know the shortest path from one system to another in terms of latency. Puzzle Solving: For puzzles such as a Rubik\u0026rsquo;s cube, the vertices could represents states of the cube and edges could correspond to a single move. Robotics: Shortest paths in terms of robotics have a lot to do with physical distances, but it could also relate the completing a task efficiently. These notes will cover classical single-source shortest path algorithms, but first we must formally define the problem.\nDefinition Given a weighted, directed graph \\(G = (V, E)\\) with weight function \\(w: E \\rightarrow \\mathbb{R}\\), a source vertex \\(s \\in V\\), and a destination vertex \\(t \\in V\\), find the shortest path from \\(s\\) to \\(t\\). The weight of a path is defined as the sum of the weights of its edges:\n\\[ w(p) = \\sum_{e \\in p} w(e). \\]\nThe shortest-path weight between two vertices \\(u\\) and \\(v\\) is given by\n\\[ \\delta(u, v) = \\begin{cases} \\min_{p \\in P(u, v)} w(p) \u0026amp; \\text{if } P(u, v) \\neq \\emptyset \\\\ \\infty \u0026amp; \\text{otherwise} \\end{cases} \\]\nwhere \\(P(u, v)\\) is the set of all paths from \\(u\\) to \\(v\\). The shortest-path weight from \\(s\\) to \\(t\\) is given by \\(\\delta(s, t)\\).\nThe output of a shortest-path algorithm will produce, for each vertex \\(v \\in V\\):\n\\(v.d\\): The shortest-path estimate from \\(s\\) to \\(v\\). \\(v.\\pi\\): The predecessor of \\(v\\) in the shortest path from \\(s\\) to \\(v\\). Shortest-path algorithms rely on an optimal substructure property that is defined by Lemma 22.1 (Cormen et al. 2022).\nLemma 22.1\nGiven a weighted, directed graph \\(G = (V,E)\\) with weight function \\(w: E \\rightarrow \\mathbb{R}\\), let \\(p = \\langle v_0, v_1, \\dots, v_k \\rangle\\) be a shortest path from vertex \\(v_0\\) to vertex \\(v_k\\). For any \\(i\\) and \\(j\\) such that \\(0 \\leq i \\leq j \\leq k\\), let \\(p_{ij} = \\langle v_i, v_{i+1}, \\dots, v_j \\rangle\\) be the subpath of \\(p\\) from vertex \\(v_i\\) to vertex \\(v_j\\). Then, \\(p_{ij}\\) is a shortest path from \\(v_i\\) to \\(v_j\\).\nIt is also important to note here that a shortest path should contain no cycles. Some shortest-path algorithms require that the edge weights be strictly positive. For those that do not, they may have some mechanism for detecting negative-weight cycles. In any case, a cycle of any kind cannot be included in a shortest path. This is because if a cycle were included, we could simply traverse the cycle as many times as we wanted to reduce the weight of the path. For positive-weight cycles, if a shortest path included a cycle, then surely we could remove the cycle to get a lower weight.\nAs we build a shortest path, we need to keep track of which vertices lead us from the source to the destination. Some algorithms maintain this by keeping a predecessor attribute for each vertex in the path. Solutions such as the Viterbi algorithm keep an array of indices that correspond to the vertices in the path. In any case, we will need to keep track of the vertices in the path as we build it.\nRelaxation There is one more important property to define before discussing specific algorithms: relaxation. Relaxing an edge \\((u, v)\\) is to test whether going through vertex \\(u\\) improves the shortest path to \\(v\\). If so, we update the shortest-path estimate and predecessor of \\(v\\) to reflect the new shortest path. Relaxation requires that we maintain the shortest-path estimate and processor for each vertex. This is initialized as follows.\ndef initialize_single_source(G, s): for v in G.V: v.d = float(\u0026#39;inf\u0026#39;) v.pi = None s.d = 0 When the values are changed, we say that the vertex has been relaxed. Relaxing an edge \\((u, v)\\) is done as follows.\ndef relax(u, v, w): if v.d \u0026gt; u.d + w(u, v): v.d = u.d + w(u, v) v.pi = u Properties Relaxation has the following properties.\nIf the shortest-path estimate of a vertex is not \\(\\infty\\), then it is always an upper bound on the weight of a shortest path from the source to that vertex.\nThe shortest-path estimate of a vertex will either stay the same or decrease as the algorithm progresses.\nOnce a vertex\u0026rsquo;s shortest-path estimate is finalized, it will never change.\nThe shortest-path estimate of a vertex is always greater than or equal to the actual shortest-path weight.\nAfter \\(i\\) iterations of relaxing on all \\((u, v)\\), if the shortest path to \\(v\\) has \\(i\\) edges, then \\(v.d = \\delta(s, v)\\).\nFollowing Introduction to Algorithms, we will first discuss the Bellman-Ford algorithm, which has a higher runtime but works with graphs that have negative edge weights. Then, we will discuss Dijkstra\u0026rsquo;s algorithm, which has a lower runtime but only works with graphs that have non-negative edge weights.\nBellman-Ford The Bellman-Ford algorithm is a dynamic programming algorithm that solves the single-source shortest-paths problem in the general case in which edge weights may be negative. If a negative-weight cycle is reachable from the source, then the algorithm will report its existence. Otherwise, it will report the shortest-path weights and predecessors. It works by relaxing edges, decreasing the shortest-path estimate on the weight of a shortest path from \\(s\\) to each vertex \\(v\\) until it reaches the shortest-path weight.\ndef bellman_ford(G, w, s): initialize_single_source(G, s) for i in range(1, len(G.V)): for (u, v) in G.E: relax(u, v, w) for (u, v) in G.E: if v.d \u0026gt; u.d + w(u, v): return False return True Example In the figure below, graph (a) shows the original graph before iterating over the edges. Graphs (b)-(e) show the result of looping over both edges originating from \\(s\\). Depending on the implementation, the first iteration of the vertices would result directly in graph (c). You can find a Python implementation of this example here.\nFigure 1: Step-by-step execution of Bellman-Ford on a graph with negative-weight edges (Cormen et al. 2022). Correctness Bellman-Ford is guaranteed to converge after \\(|V| - 1\\) iterations, assuming no negative-weight cycles.\nProof The first iteration relaxes \\((v_0, v_1)\\). The second iteration relaxes \\((v_1, v_2)\\), and so on. The path-relaxation property from before implies that \\(v.d = v_k.d = \\delta(s, v_k) = \\delta(s, v)\\). If there is a negative-weight cycle, then the shortest path to \\(v_k\\) is not well-defined. This is verified in the final loop over the edges.\nfor (u, v) in G.E: if v.d \u0026gt; u.d + w(u, v): return False If there exists a negative-weight cycle \\(c = \\langle v_0, v_1, \\dots, v_k \\rangle\\), where \\(v_0 = v_k\\) that can be reached from \\(s\\), then\n\\[ \\sum_{i=1}^{k} w(v_{i-1}, v_i) \u0026lt; 0. \\]\nTo complete the proof by contradiction, assume that Bellman-Ford returns True. Then we would have that \\(v_i.d \\leq v_{i-1}.d + w(v_{i-1}, v_i)\\) for \\(i = 1, 2, \\dots, k\\) by the triangle inequality property. If we sum around the cycle, we get\n\\begin{align*} \\sum_{i=1}^k v_i.d \u0026amp;\\leq \\sum_{i=1}^k (v_{i-1}.d + w(v_{i-1}, v_i))\\\\ \u0026amp;= \\sum_{i=1}^k v_{i-1}.d + \\sum_{i=1}^k w(v_{i-1}, v_i)\\\\ \\end{align*}\nSince the vertices are in a cycle, each vertex appears only once in each summation \\(\\sum_{i=1}^k v_{i}.d\\) and \\(\\sum_{i=1}^k v_{i-1}.d\\). Subtracting this from both sides of the inequality, we get\n\\[ 0 \\leq \\sum_{i=1}^k w(v_{i-1}, v_i). \\]\nThis contradicts the assumption that there is a negative-weight cycle. Therefore, if Bellman-Ford returns True, then there are no negative-weight cycles.\nAnalysis Using an adjacency list representation, the runtime of Bellman-Ford is \\(O(V^2 + VE)\\). The initialization takes \\(\\Theta(V)\\). Each of the \\(|V| - 1\\) iterations over the edges takes \\(\\Theta(V + E)\\), and the final check for negative-weight cycles takes \\(\\Theta(V + E)\\). If the number of edges and vertices is such that the number of vertices are a lower bound on the edges, then the runtime is \\(O(VE)\\).\nExample 22.1-1 Run Bellman-Ford on the given path using \\(z\\) as the source. Then change the weight of \\((z, x)\\) to 4 and run it again with \\(s\\) as the source.\nFigure 2: Figure 22.4 from (Cormen et al. 2022). Shortest Paths on a DAG If we are given a directed acyclic graph (DAG), we can solve the single-source shortest path problem in \\(O(V + E)\\) time. By definition, the graph has no cycles and thus no negative-weight cycles.\ndef dag_shortest_paths(G, w, s): initialize_single_source(G, s) for u in topological_sort(G): for v in G.adj[u]: relax(u, v, w) Example Run dag_shortest_paths on the graph given below with \\(s\\) as the source.\nFigure 3: Figure 22.5 from (Cormen et al. 2022). Analysis The runtime of dag_shortest_paths is \\(O(V + E)\\), where \\(V\\) is the number of vertices and \\(E\\) is the number of edges. The topological sort takes \\(O(V + E)\\) time. Initializing the vertices takes \\(O(V)\\) time. The first for loop makes on iteration per vertex, and the inner loop relaxes each edge only once.\nDijkstra\u0026rsquo;s Algorithm Dijkstra\u0026rsquo;s algorithm also solves the single-source shortest path problem on a weighted, directed graph \\(G = (V,E)\\) but requires nonnegative weights on all edges. It works in a breadth-first manner. A minimum priority queue is utilized to keep track of the vertices that have not been visited based on their current minimum shortest-path estimate. The algorithm works by relaxing edges, decreasing the shortest-path estimate on the weight of a shortest path from \\(s\\) to each vertex \\(v\\) until it reaches the shortest-path weight.\ndef dijkstra(G, w, s): initialize_single_source(G, s) S = [] Q = G.V while Q: u = extract_min(Q) S.append(u) for v in G.adj[u]: prev_d = v.d relax(u, v, w) if v.d \u0026lt; prev_d: decrease_key(Q, v) Example A Python example of the figure below is available here.\nFigure 4: A step-by-step execution of Dijkstra\u0026rsquo;s algorithm on a graph with non-negative edge weights (Cormen et al. 2022). Analysis See Chapter 22 of Introduction to Algorithms for a detailed analysis of Dijkstra\u0026rsquo;s algorithm. Inserting the nodes and then extracting them from the queue yields \\(O(V \\log V)\\). After extracting a node, its edges are iterated with a possible update to the queue. This takes \\(O(E \\log V)\\). The total runtime is \\(O((V + E) \\log V)\\).\nReferences Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. Introduction to Algorithms. 4th ed. MIT Press. http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/. ","date":1697864400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697864400,"objectID":"4d280a181eeff8feebb6a316b1e284c6","permalink":"https://ajdillhoff.github.io/notes/single_source_shortest_paths/","publishdate":"2023-10-21T00:00:00-05:00","relpermalink":"/notes/single_source_shortest_paths/","section":"notes","summary":"Table of Contents Definition Bellman-Ford Shortest Paths on a DAG Dijkstra\u0026rsquo;s Algorithm When you hear the term shortest path, you may think of the shortest physical distance between your current location and wherever it is you\u0026rsquo;re going. Finding the most optimal route via GPS is one of the most widely used mobile applications. Physical paths are not the only types we may wish to find a shortest path for. Other examples include:","tags":["computer science","algorithms","graphs","shortest path"],"title":"Single-Source Shortest Paths","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents What are Graphs? Graph Traversal Algorithms Breadth First Search Depth First Search What are Graphs? A graph is a data structure that is used to represent pairwise relationships between objects. Graphs are used in many applications, such as social networks, maps, and routing algorithms. These notes accompany the series of lectures on graphs for my Foundations of Computing course at the University of Texas - Arlington.\nDefinitions A directed graph \\(G\\) is represented as a pair \\((V, E)\\) of a set of vertices \\(V\\) and edges \\(E\\). Edges are represented as ordered pairs.\nAn undirected graph \\(G\\) is represented as a pair \\((V, E)\\) of a set of vertices \\(V\\) and edges \\(E\\). The edges are represented as unordered pairs, as it does not matter which direction the edge is going.\nLet \\((u, v)\\) be an edge in a graph \\(G\\). If \\(G\\) is a directed graph, then the edge is incident from \\(u\\) and is incident to \\(v\\). In this case, \\(v\\) is also adjacent to \\(u\\). If \\(G\\) is an undirected graph, then the edge is incident on \\(u\\) and \\(v\\). For undirected graphs, the adjacency relation is symmetric.\nThe degree is a graph is the number of edges incident on a vertex. For directed graphs, the in-degree is the number of edges incident to a vertex, and the out-degree is the number of edges incident from a vertex.\nA path from a vertex \\(u\\) to another vertex \\(v\\) is a sequence of edges that starts at \\(u\\) and ends at \\(v\\). This definition can include duplicates. A simple path is a path that does not repeat any vertices. A cycle is a path that starts and ends at the same vertex. If a path exists from \\(u\\) to \\(v\\), then \\(u\\) is reachable from \\(v\\).\nA connected graph is a graph where there is a path between every pair of vertices. A strongly connected graph is a directed graph where there is a path between every pair of vertices. The connected components of a graph are the subgraphs in which each pair of nodes is connected by a path. In image processing, connected-component labeling is used to find regions of connected pixels in a binary image.\nLet \\(G = (V, E)\\) and \\(G\u0026rsquo; = (V\u0026rsquo;, E\u0026rsquo;)\\). \\(G\\) and \\(G\u0026rsquo;\\) are isomorphic if there is a bijection between their vertices such that \\((u, v) \\in E\\) if and only if \\((f(u), f(v)) \\in E\u0026rsquo;\\).\nA complete graph is an undirected graph in which every pair of vertices is adjacent. A bipartite graph is an undirected graph in which the vertices can be partitioned into two sets such that every edge connects a vertex in one set to a vertex in the other set.\nA multi-graph is a graph that allows multiple edges between the same pair of vertices. These are commonly in social network analysis, where multiple edges between two people can represent different types of relationships.\nTODO: Add figures demonstrating the above definitions\nRepresentations Graphs can be represented in many different ways. The most common representations are adjacency lists and adjacency matrices. Adjacency lists are more space-efficient for sparse graphs, while adjacency matrices are more space-efficient for dense graphs. Adjacency lists are also more efficient for finding the neighbors of a vertex, while adjacency matrices are more efficient for checking if an edge exists between two vertices.\nExample: Adjacency Matrix and Reachability Consider the graph in the figure below.\nFigure 1: A directed graph The adjacency matrix for this graph is:\n\\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ \\end{bmatrix}\nThe rows and columns represent the vertices in the graph. The value at row \\(i\\) and column \\(j\\) is 1 if there is an edge from vertex \\(i\\) to vertex \\(j\\). Otherwise, the value is 0. Let \\(A\\) be the adjacency matrix for a graph \\(G\\). The matrix \\(A^k\\) represents the number of paths of length \\(k\\) between each pair of vertices. For example, \\(A^2\\) for the above graph is:\n\\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ \\end{bmatrix}\nThe value at row \\(i\\) and column \\(j\\) is the number of paths of length 2 from vertex \\(i\\) to vertex \\(j\\). For example, is a path from vertex 0 to vertex 6 via 0 -\u0026gt; 3 -\u0026gt; 6.\nGraph Traversal Algorithms Graph traversal algorithms are used to explore the structure of a graph. You might initially find this a useless endeavour. If we have defined our own graph, what about it would we need to explore? In scenarios with highly complex and large datasets, the search takes on slightly different meanings. Perhaps we are searching for specific trends based on a range of values such as the Bacon number of a particular celebrity.\nBreadth First Search Breadth First search explores the graph broadly, ensuring one level has been exhausted before moving onto the next. We previously studied breadth-first search in the context of binary search trees. The algorithm is the same when applied on general graphs, but our perspective is slightly different now. The function studied before did not use node coloring. Let\u0026rsquo;s investigate the algorithm given by Cormen et al. in Introduction to Algorithms (Cormen et al. 2022).\nThe algorithm adds a color to each node to keep track of its state. The colors are:\nWHITE: The node has not been discovered yet. GRAY: The node has been discovered, but not all of its neighbors have been discovered. BLACK: The node has been discovered, and all of its neighbors have been discovered. First, every vertex is painted white and the distance is set to \\(\\infty\\). The first node s is immediately set to have 0 distance. The queue then starts with s. While there are any grey vertices, dequeue the next available node and add its adjacent vertices to the queue. The distance of each adjacent vertex is set to the distance of the current vertex plus one. Once all of its neighbors have been discovered, the current vertex is painted black.\nThe algorithm and an example run from Cormen et al. are shown below.\ndef bfs(G, s): for u in G.V: u.color = WHITE u.d = inf u.pi = None s.color = GRAY s.d = 0 s.pi = None Q = Queue() Q.enqueue(s) while not Q.empty(): u = Q.dequeue() for v in G.adj[u]: if v.color == WHITE: v.color = GRAY v.d = u.d + 1 v.pi = u Q.enqueue(v) u.color = BLACK Figure 2: Breadth First Search from Cormen et al. Analysis The running time of BFS is \\(O(V + E)\\), where \\(V\\) is the number of vertices and \\(E\\) is the number of edges. Each vertex is queued and dequeued once, so the queue operations take \\(O(V)\\) time. Each edge is examined once, so the edge operations take \\(O(E)\\) time. The total running time is \\(O(V + E)\\).\nBreath-first Trees The blue lines in the previous example depict a breadth-first tree which was built by BFS. The tree is defined by the \\(\\pi\\) values updated throughout the course of the algorithm. These can also be used to reconstruct the shortest path from \\(s\\) to any other vertex \\(v\\).\nA predecessor subgraph is a graph \\(G_{\\pi} = (V_{\\pi}, E_{\\pi})\\), where\n\\begin{align*} V_{\\pi} \u0026amp;= \\{v \\in V : v.\\pi \\neq \\text{None}\\} \\cup \\{s\\} \\text{ and}\\\\ E_{\\pi} \u0026amp;= \\{(v.\\pi, v) : v \\in V_{\\pi} - \\{s\\}\\}. \\end{align*}\nSuch a graph is a breadth-first tree if \\(V_{\\pi}\\) consists of the vertices reachable from \\(s\\) and, for all \\(v \\in V_{\\pi}\\), the subgraph \\(G_{\\pi}\\) contains a unique simple path from \\(s\\) to \\(v\\) that is also a shortest path from \\(s\\) to \\(v\\) in \\(G\\).\nTo print the vertices on a shortest path from \\(s\\) to \\(v\\):\ndef print_path(G, s, v): if v == s: print(s) elif v.pi == None: print(\u0026#34;No path from\u0026#34;, s, \u0026#34;to\u0026#34;, v, \u0026#34;exists.\u0026#34;) else: print_path(G, s, v.pi) print(v) Exercise Run BFS on the following graph, starting from \\(s\\).\nFigure 3: Graph for BFS exercise from Cormen et al. Depth First Search Like the BFS algorithm presented in Introduction to Algorithms by Cormen et al., the DFS algorithm also uses colors to keep track of the state of each node. The colors are similar to the BFS algorithm, but the meaning is slightly different:\nWHITE: The node has not been discovered yet. GRAY: The node has been visited for the first time. BLACK: The adjacency list of the node has been examined completely. First, all vertices are colored white. The time is set to 0. The function dfs_visit is called on each vertex. The function is defined as follows:\ndef dfs(G): for u in G.V: u.color = WHITE u.pi = None time = 0 for u in G.V: if u.color == WHITE: dfs_visit(G, u) def dfs_visit(G, u): time += 1 u.d = time u.color = GRAY for v in G.adj[u]: if v.color == WHITE: v.pi = u dfs_visit(G, v) u.color = BLACK time += 1 u.f = time When a node is discovered via dfs_visit, the time is recorded and the color is changed to gray. The start and finish times are useful in understanding the structure of the graph. After all of the node\u0026rsquo;s neighbors have been discovered, the color is changed to black and the finish time is recorded. That is, the depth from the current node must be fully explored before it is considered finished. The figure below shows the progress of DFS on a directed graph.\nFigure 4: Depth First Search from Cormen et al. Analysis Each vertex is added and removed to the queue once. Since these operations are performed in \\(O(1)\\) time, the total time for these operations in \\(O(V)\\). When a vertex is dequeued, its adjacency list is scanned. The total number of entries in all adjacency lists is equal to the number of edges, so the time spent scanning these lists of \\(O(V + E)\\). In summary, the operation is linear in terms of the adjacency-list representation.\nProperties of DFS The predecessor subgraph \\(G_{\\pi}\\) is a forest of trees. That is, it creates a collection of depth first trees.\nWhat is a forest? How does DFS generate a forest? A vertex \\(u = v.{\\pi}\\) if and only if \\(\\text{DFS-VISIT}(G, v)\\) was called during a search of \\(u\\)\u0026rsquo;s adjacency list. Vertex \\(v\\) is a descendant of vertex \\(u\\) in the depth-first forest if and only if \\(v\\) is discovered during the time in which \\(u\\) is gray (Cormen et al. 2022).\nEssentially, each call to dfs_visit from dfs finds a new tree. Let\u0026rsquo;s consider this on the example above.\nExercise: Draw the DFS forest using the graph above.\nParenthesis Theorem In a DFS, the discovery and finish times have parenthesis structure. For all \\(u, v\\), exactly only of the following holds:\nthe intervals \\([u.d, u.f]\\) and \\([v.d, v.f]\\) are entirely disjoint and neither \\(u\\) nor \\(v\\) is a descendant of the other in the depth-first forest. the interval \\([u.d, u.f]\\) is entirely contained within the interval \\([v.d, v.f]\\) and \\(u\\) is a descendant of \\(v\\) in the depth-first forest. the interval \\([v.d, v.f]\\) is entirely contained within the interval \\([u.d, u.f]\\) and \\(v\\) is a descendant of \\(u\\) in the depth-first forest. It is called the parenthesis theorem because if dfs_visit printed \u0026ldquo;\\((u\\)\u0026rdquo; when it first encountered \\(u\\) and printed \u0026ldquo;\\(u)\\)\u0026rdquo; when it finished, the expression would be well formed.\nFigure 5: Example of parenthesis structure from Cormen et al. White-path Theorem In a depth-first forest of a graph \\(G\\), vertex \\(v\\) is a descendant of vertex \\(u\\) if and only if at the time \\(u.d\\) that dfs_visit is called on \\(u\\), there is a path of white vertices from \\(u\\) to \\(v\\) in \\(G\\).\nMore on DFS Forests Consider the result of DFS on the graph below.\nFigure 6: DFS forest from Cormen et al. The edges are labeled as either\ntree edges: edges in the depth-first forest. back edges: edges that point from a vertex to an ancestor in the depth-first forest. forward edges: edges that point from a vertex to a descendant in the depth-first forest. The above graph can be visualized as a DFS forest, as shown below.\nFigure 7: DFS forest from Cormen et al. References Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. Introduction to Algorithms. 4th ed. MIT Press. http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/. ","date":1697518800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697518800,"objectID":"6602c9f998120efe72b5ce6f5f8f62d1","permalink":"https://ajdillhoff.github.io/notes/introduction_to_graph_theory/","publishdate":"2023-10-17T00:00:00-05:00","relpermalink":"/notes/introduction_to_graph_theory/","section":"notes","summary":"Table of Contents What are Graphs? Graph Traversal Algorithms Breadth First Search Depth First Search What are Graphs? A graph is a data structure that is used to represent pairwise relationships between objects. Graphs are used in many applications, such as social networks, maps, and routing algorithms. These notes accompany the series of lectures on graphs for my Foundations of Computing course at the University of Texas - Arlington.","tags":["computer science","data structures"],"title":"Introduction to Graph Theory","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Definition Operations Exercises Red-Black Trees are modified Binary Search Trees that maintain a balanced structure in order to guarantee that operations like search, insert, and delete run in \\(O(\\log n)\\) time.\nDefinition A red-black tree is a binary search tree with the following properties:\nEvery node is either red or black. The root is black. Every NULL leaf is black. If a node is red, then both its children are black. For each node, all simple paths from the node to descendant leaves contain the same number of black nodes. Figure 1: Red-Black Tree from CLRS Chapter 13. The only structural addition we need to make over a BST is the addition of a color attribute to each node. This attribute can be either RED or BLACK.\nProperty 5 implies that the black-height of a tree is an important property. This property is used to prove that the height of a red-black tree with \\(n\\) internal nodes is at most \\(2 \\log(n + 1)\\).\nOperations Rotate If a is balanced, then searching for a node takes \\(O(\\log n)\\) time. However, if the tree is unbalanced, then searching can take \\(O(n)\\) time. When items are inserted or deleted from a tree, it can become unbalanced. Without any way to correct for this, a BST is less desirable unless the data will not change.\nWhen nodes are inserted or deleted into a red-black tree, the rotation operation is used in functions that maintain the red-black properties. This ensures that the tree remains balanced and that operations like search, insert, and delete run in \\(O(\\log n)\\) time. The figure below shows the two types of rotations that can be performed on a red-black tree.\nFigure 2: Rotations in a red-black tree (CLRS Figure 13.2). Python implementations of both left and right rotations are given below.\ndef left_rotate(self, x): y = x.right x.right = y.left if y.left != self.nil: y.left.p = x y.p = x.p if x.p == self.nil: self.root = y elif x == x.p.left: x.p.left = y else: x.p.right = y y.left = x x.p = y def right_rotate(self, y): x = y.left y.left = x.right if x.right != self.nil: x.right.p = y x.p = y.p if y.p == self.nil: self.root = x elif y == y.p.left: y.p.left = x else: y.p.right = x x.right = y y.p = x Cormen et al. Figure 13.3 (below) shows the result of performing a left rotation on node \\(x\\).\nFigure 3: Left rotation on node (x) (CLRS Figure 13.3). A rotation only changes pointer assignments, so it takes \\(O(1)\\) time.\nInsert The insert operation in a red-black tree starts off identically to the insert operation in a BST. The new node is inserted into the tree as a leaf node. Since the NULL leaf nodes must be black by definition, the added node is colored red. The function in Python is shown below.\ndef insert(self, z): y = self.nil x = self.root while x != self.nil: y = x if z.key \u0026lt; x.key: x = x.left else: x = x.right z.p = y if y == self.nil: self.root = z elif z.key \u0026lt; y.key: y.left = z else: y.right = z z.left = self.nil z.right = self.nil z.color = RED self.insert_fixup(z) By adding the node and setting its color to red, we have possibly violated properties 2 and 4. Property 2 is violated if z is the root. Property 4 is violated if the parent of the new node is also red. The final line of the function calls insert_fixup to restore the red-black properties. It is defined as follows.\ndef insert_fixup(self, z): while z.p.color == RED: if z.p == z.p.p.left: y = z.p.p.right if y.color == RED: z.p.color = BLACK y.color = BLACK z.p.p.color = RED z = z.p.p else: if z == z.p.right: z = z.p self.left_rotate(z) z.p.color = BLACK z.p.p.color = RED self.right_rotate(z.p.p) else: y = z.p.p.left if y.color == RED: z.p.color = BLACK y.color = BLACK z.p.p.color = RED z = z.p.p else: if z == z.p.left: z = z.p self.right_rotate(z) z.p.color = BLACK z.p.p.color = RED self.left_rotate(z.p.p) self.root.color = BLACK Case 1 Inside the while loop, the first and second conditions are symmetric. One considers the case where z\u0026rsquo;s parent is a left child, and the other considers the case where z\u0026rsquo;s parent is a right child. Further, if z\u0026rsquo;s parent is a left child, then we start by setting y to z\u0026rsquo;s aunt. Let\u0026rsquo;s investigate the first if statement, where y is RED. In this case, both z\u0026rsquo;s parent and aunt are RED. We can fix this by setting both to BLACK and setting z\u0026rsquo;s grandparent to RED. This may violate property 2, so we set z to its grandparent and repeat the loop.\nif y.color == RED: z.p.color = BLACK y.color = BLACK z.p.p.color = RED z = z.p.p Case 2 If y is BLACK, then we need to consider the case where z is a right child. In this case, we set z to its parent and perform a left rotation. This automatically results in the third case, where z is a left child.\nif z == z.p.right: z = z.p self.left_rotate(z) Case 3 If z is a left child, then we set z\u0026rsquo;s parent to BLACK and its grandparent to RED. Then we perform a right rotation on the grandparent.\nz.p.color = BLACK z.p.p.color = RED self.right_rotate(z.p.p) Figure 13.4 from Cormen et al. demonstrates the addition of a node to a red-black tree. The node is inserted as a leaf node and colored red. Then insert_fixup is called to restore the red-black properties.\nFigure 4: Inserting a node into a red-black tree (CLRS Figure 13.4). The insert operation takes \\(O(\\log n)\\) time since it performs a constant number of rotations.\nDelete Like the delete operation of a BST, the delete operation of a RBT uses a transplant operation to replace the deleted node with its child. The transplant operation is defined as follows.\ndef transplant(self, u, v): if u.p == self.nil: self.root = v elif u == u.p.left: u.p.left = v else: u.p.right = v v.p = u.p The full delete operation follows a similar structure to that of its BST counterpart. There are a few distinct differences based on the color of the node being deleted. The function begins as follows.\ndef delete(self, z): y = z y_original_color = y.color The first line sets y to the node to be deleted. The second line saves the color of y. This is necessary because y will be replaced by another node, and we need to know the color of the replacement node. The first two conditionals check if z has any children. If there is right child, then the z is replaced by the left child. If there is a left child, then z is replaced by the right child. If z has no children, then z is replaced by NULL.\nif z.left == None: x = z.right self.transplant(z, z.right) elif z.right == None: x = z.left self.transplant(z, z.left) If z has two children, then we find the successor of z and set y to it. The successor is the node with the smallest key in the right subtree of z. The successor is guaranteed to have at most one child, so we can use the code above to replace y with its child. Then we replace z with y.\nelse: y = self.minimum(z.right) y_original_color = y.color x = y.right if y != z.right: # y is farther down the tree self.transplant(y, y.right) y.right = z.right y.right.p = y else: x.p = y self.transplant(z, y) y.left = z.left y.left.p = y y.color = z.color The procedure kept track of y_original_color to see if any violations occurred. This would happen if y was originally BLACK because the transplant operation, or the deletion itself, could have violated the red-black properties. If y_original_color is BLACK, then we call delete_fixup to restore the properties.\nDelete Fixup If the node being deleted is BLACK, then the following scenarios can occur. If y is the root and a RED child of y becomes the new root, property 2 is violated. Let x be a RED child of y, if a new parent of x is RED, then property 4 is violated. Lastly, removing y may have caused a violation of property 5, since any path containing y has 1 less BLACK node in it.\nCorrecting violation 5 can be done by transferring the BLACK property from y to x, the node that moves into y\u0026rsquo;s original position. This requires us to allow nodes to take on multiple counts of colors. That is, if x was already BLACK, it becomes double BLACK. If it was RED, it becomes RED-AND-BLACK. There is a good reason to this extension, as it will help us decide which case of delete_fixup to use.\nThe delete_fixup function will restore violations of properties 1, 2, and 4. It is called after the delete operation, and it takes a single argument, x, which is the node that replaced the deleted node. It performs a series of rotations and color changes to restore the violated properties.\nLet\u0026rsquo;s look at the delete_fixup function from the ground up. It is a little more complex than insert_fixup because it has to handle the case where the node being deleted is BLACK. In total, there are 4 distinct cases per side. Like insert_fixup, it is enough to understand the first half, as the second is symmetric. The function begins as follows, where x is a left child.\nCase 1 def delete_fixup(self, x): while x != self.root and x.color == BLACK: if x == x.p.left: w = x.p.right if w.color == RED: w.color = BLACK x.p.color = RED self.left_rotate(x.p) w = x.p.right In the first case, x\u0026rsquo;s sibling w is RED. If this is true, then w must have two BLACK subnodes. The colors of w and x\u0026rsquo;s parent are then switched, and a left rotation is performed on x\u0026rsquo;s parent. The result of case 1 converts to one of cases 2, 3, or 4. The figure below shows the result of the first case.\nFigure 5: Case 1 of delete_fixup (CLRS Figure 13.7). Case 2 if w.left.color == BLACK and w.right.color == BLACK: w.color = RED x = x.p If both of w\u0026rsquo;s subnodes are BLACK and both w and x are also black (actually, x is doubly BLACK), then there is an extra BLACK node on the path from w to the leaves. The colors of both x and w are switched, which leaves x with a single BLACK count and w as RED. The extra BLACK property is transferred to x\u0026rsquo;s parent. The figure below shows the result of the second case.\nFigure 6: Case 2 of delete_fixup (CLRS Figure 13.7). Case 3 else: if w.right.color == BLACK: w.left.color = BLACK w.color = RED self.right_rotate(w) w = x.p.right If w is BLACK, its left child is RED, and its right child is BLACK, then the colors of w and its left child are switched. Then a right rotation is performed on w. This rotation moves the BLACK node to w\u0026rsquo;s position, which is now the new sibling of x. This leads directly to case 4. A visualization of case 3 is shown below.\nFigure 7: Case 3 of delete_fixup (CLRS Figure 13.7). Case 4 w.color = x.p.color x.p.color = BLACK w.right.color = BLACK self.left_rotate(x.p) x = self.root At this point, w is BLACK and its right child is RED. Also remember that x still holds an extra BLACK count. This last case performs color changes and a left rotation which remedy the extra BLACK count. The figure below shows the result of case 4.\nFigure 8: Case 4 of delete_fixup (CLRS Figure 13.7). The full delete_fixup function is shown below.\ndef delete_fixup(self, x): while x != self.root and x.color == BLACK: if x == x.p.left: w = x.p.right if w.color == RED: w.color = BLACK x.p.color = RED self.left_rotate(x.p) w = x.p.right if w.left.color == BLACK and w.right.color == BLACK: w.color = RED x = x.p else: if w.right.color == BLACK: w.left.color = BLACK w.color = RED self.right_rotate(w) w = x.p.right w.color = x.p.color x.p.color = BLACK w.right.color = BLACK self.left_rotate(x.p) x = self.root else: w = x.p.left if w.color == RED: w.color = BLACK x.p.color = RED self.right_rotate(x.p) w = x.p.left if w.right.color == BLACK and w.left.color == BLACK: w.color = RED x = x.p else: if w.left.color == BLACK: w.right.color = BLACK w.color = RED self.left_rotate(w) w = x.p.left w.color = x.p.color x.p.color = BLACK w.left.color = BLACK self.right_rotate(x.p) x = self.root x.color = BLACK The delete operation takes \\(O(\\log n)\\) time since it performs a constant number of rotations. The delete_fixup operation also takes \\(O(\\log n)\\) time since it performs a constant number of color changes and at most 3 rotations. Case 2 of delete_fixup could move the violation up the tree, but this would happen no more than \\(O(\\log n)\\) times. In total, the delete operation takes \\(O(\\log n)\\) time.\nExercises Create a red-black tree class in Python that supports the operations discussed in these notes. Using the created class from exercise 1, implement a Hash Map class that uses a red-black tree for collision resolution via chaining. ","date":1697346000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697346000,"objectID":"48726e9a212db00c135bdb0404fea1cc","permalink":"https://ajdillhoff.github.io/notes/red_black_trees/","publishdate":"2023-10-15T00:00:00-05:00","relpermalink":"/notes/red_black_trees/","section":"notes","summary":"Table of Contents Definition Operations Exercises Red-Black Trees are modified Binary Search Trees that maintain a balanced structure in order to guarantee that operations like search, insert, and delete run in \\(O(\\log n)\\) time.\nDefinition A red-black tree is a binary search tree with the following properties:\nEvery node is either red or black. The root is black. Every NULL leaf is black. If a node is red, then both its children are black.","tags":["computer science","data structures"],"title":"Red-Black Trees","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Binary Search Trees Operations Analysis A $n$-ary tree is a graph-based data structure in which each node has up to \\(n\\) subnodes. It is supported by the following operations (not exclusive):\nSearch Insert Delete Tree-based data structures are defined by the following properties.\nThe size of a tree \\(T\\) is determined by the total number of nodes in \\(T\\). The root of a tree \\(T\\) is the starting point of \\(T\\). A leaf node of a tree \\(T\\) is a node that has no sub-nodes. The height of a tree is determined by the length of the shortest path between the root of \\(T\\) and the lowest leaf node of \\(T\\). If we limit the number of subnodes each node may have to 2, the structure becomes known as a binary tree. Limiting the structure in this way is of interest to us because of the efficiency benefits seen in operations applied to binary trees. If we narrow the scope of these trees further, we can define a binary search tree whose search operation, as the name might suggest, runs in \\(\\Theta(lg n)\\) worst-case time.\nBinary Search Trees A binary search tree is a regular binary tree with references to the left, right, and parent nodes, defined by the following property:\nLet \\(x\\) be a node in a binary search tree. If \\(y\\) is a node in the left subtree of \\(x\\), then \\(y.key \\leq x.key\\). If \\(y\\) is a node in the right subtree of \\(x\\), then \\(y.key \\geq x.key\\).\nUnder this definition, operations such as search, insert, and delete can be performed in \\(\\Theta(lg n)\\) worst-case time assuming that the tree is balanced. Later, we will explore a variant of the binary search tree that guarantees a balanced tree.\nA tree node implemented in Python might look like this:\nclass Node: def __init__(self, key): self.key = key self.left = None self.right = None self.parent = None Operations Traversals Like any other graph-based structure, a tree can be traversed using either depth-first or breadth-first search. Only an inorder depth-first search is of interest for a binary search tree, as we will see below. Consider the given tree in the figure below. Performing an inorder traversal on this tree yields the keys in sorted order from smallest to largest.\ndef inorder_tree_walk(x): if x is not None: inorder_tree_walk(x.left) print(x.key) inorder_tree_walk(x.right) Traversing the entire tree takes \\(\\Theta(n)\\) time, as each node must be visited once. Searching a tree, however, only takes \\(\\Theta(lg n)\\) time. The search algorithm is defined recursively as follows:\ndef tree_search(x, k): if x is None or k == x.key: return x if k \u0026lt; x.key: return tree_search(x.left, k) else: return tree_search(x.right, k) Consider the balanced tree shown in the figure below. If we search for the key 15, notice that after the first comparison with the root, the search space goes from 15 nodes to 7 nodes. After the second comparison, the search space goes from 7 nodes to 3 nodes. After the third comparison, the search space goes from 3 nodes to 1 node. This is the essence of binary search, and it is why the search operation runs in \\(\\Theta(lg n)\\) time.\nFigure 1: A balanced binary search tree Minimum In a BST, the minimum value is the leftmost node. Finding the minimum is as easy as traversing down the left branch until a leaf node is reached.\ndef tree_minimum(x): while x.left is not None: x = x.left return x Maximum In a BST, the maximum value is the rightmost node. Finding the maximum is as easy as traversing down the right branch until a leaf node is reached.\ndef tree_maximum(x): while x.right is not None: x = x.right return x Successor The successor and predecessor operations are useful for the delete operation defined below. The successor of a node \\(x\\) is the node with the smallest key greater than \\(x.key\\). If \\(x\\) has a right subtree, then the successor of \\(x\\) is the minimum of the right subtree. If \\(x\\) has no right subtree, then the successor of \\(x\\) is the lowest ancestor of \\(x\\) whose left child is also an ancestor of \\(x\\).\ndef tree_successor(x): if x.right is not None: return tree_minimum(x.right) y = x.parent while y is not None and x == y.right: x = y y = y.parent return y Predecessor The predecessor of a node \\(x\\) is the node with the largest key less than \\(x.key\\). If \\(x\\) has a left subtree, then the predecessor of \\(x\\) is the maximum of the left subtree. If \\(x\\) has no left subtree, then the predecessor of \\(x\\) is the lowest ancestor of \\(x\\) whose right child is also an ancestor of \\(x\\).\ndef tree_predecessor(x): if x.left is not None: return tree_maximum(x.left) y = x.parent while y is not None and x == y.left: x = y y = y.parent return y Insert Inserting an item into a binary search tree follows the same logic as traversal. Starting at the root, the key is compared to see if it is greater than the root\u0026rsquo;s key. If so, recursively traverse down the right branch. If not, recursively traverse down the left branch. This process continues until a leaf node is reached, at which point the new node is inserted as a child of the leaf node.\nThis process will not necessarily result in a balanced tree. In fact, if the keys are inserted in sorted order, the tree will be a linked list. This is the worst-case scenario for a binary search tree, as the search operation will then run in \\(\\Theta(n)\\) time.\nThe full algorithm is given below.\ndef tree_insert(T, z): y = None x = T.root while x is not None: y = x if z.key \u0026lt; x.key: x = x.left else: x = x.right z.parent = y if y is None: T.root = z elif z.key \u0026lt; y.key: y.left = z else: y.right = z Delete Deleting a node is not a straightforward as insert. Depending on the structure of the subtree, one of three cases must be considered.\nIf \\(z\\) has no subnodes, simply remove \\(z\\) from the tree. If \\(z\\) has one subnode, replace \\(z\\) with its subnode. If \\(z\\) has two subnodes, replace \\(z\\) with its successor. It is a bit more complicated than this, as we explore below. In case 3, node \\(z\\) has both a left and right subnode. The first step is to find the successor of \\(z\\), \\(y\\). Since \\(z\\) has 2 subnodes, its successor has no left subnode (convince yourself of this). Likewise, its predecessor has no right subnode. If \\(y\\) is the right subnode of \\(z\\), replace \\(z\\) by \\(y\\).\nIf \\(y\\) is not the right subnode of \\(z\\), it is somewhere further down the right branch. In this case, replace \\(y\\) by its right subnode before replacing \\(z\\) by \\(y\\). The figure below shows the removal of node 12 from the tree in the figure above.\nFigure 2: Deleting node 12 from the tree Even though only 1 node was moved (13 to 12\u0026rsquo;s old position), the process of deleting a node actually involves transplanting a subtree to a new position. This is defined algorithmically as follows:\ndef transplant(T, u, v): if u.parent is None: T.root = v elif u == u.parent.left: u.parent.left = v else: u.parent.right = v if v is not None: v.parent = u.parent In the code above, u is the node to be replaced, and v is the node to replace it. Updating v\u0026rsquo;s left and right subnodes are done in the calling function tree_delete, as seen below.\ndef tree_delete(T, z): if z.left is None: # Case 1 and 2 transplant(T, z, z.right) elif z.right is None: # Also case 1 and 2 transplant(T, z, z.left) else: # Case 3 y = tree_minimum(z.right) # get successor if y != z.right: transplant(T, y, y.right) y.right = z.right y.right.parent = y transplant(T, z, y) y.left = z.left y.left.parent = y Analysis Insert, delete, and search all run in \\(\\Theta(h)\\) time, where \\(h\\) is the height of the tree. If the tree is balanced, \\(h = \\Theta(lg n)\\), and all operations run in \\(\\Theta(lg n)\\) time. If the tree is not balanced, \\(h = \\Theta(n)\\), and all operations run in \\(\\Theta(n)\\) time.\n","date":1696914000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696914000,"objectID":"ced91f60d0a798ea462f41c5e80c4bc3","permalink":"https://ajdillhoff.github.io/notes/binary_search_trees/","publishdate":"2023-10-10T00:00:00-05:00","relpermalink":"/notes/binary_search_trees/","section":"notes","summary":"Table of Contents Binary Search Trees Operations Analysis A $n$-ary tree is a graph-based data structure in which each node has up to \\(n\\) subnodes. It is supported by the following operations (not exclusive):\nSearch Insert Delete Tree-based data structures are defined by the following properties.\nThe size of a tree \\(T\\) is determined by the total number of nodes in \\(T\\). The root of a tree \\(T\\) is the starting point of \\(T\\).","tags":["computer science","data structures","algorithms"],"title":"Binary Search Trees","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction to Data Structures Review: Pointers Arrays Matrices Multi-Dimensional Arrays Stacks Queues Introduction to Data Structures Data structures are fundamental concepts in computer science that allow us to organize and store data in a way that enables efficient access and modification. They are essential building blocks for creating efficient and sophisticated computer programs and databases. Different types of data structures include arrays, linked lists, stacks, queues, trees, graphs, and many more, each serving a specific purpose and suited to specific applications.\nUnderstanding data structures is therefore important because they are used in almost every software application. For instance, social media applications use data structures to store user information and their connections, while search engines use them to index and retrieve web pages quickly. The choice of the right data structure significantly impacts the performance, scalability, and resource utilization of software applications.\nHaving a strong grasp of data structures and their properties is critical for anyone working with software or data in general. Through studying the benefits and limitations of each data structure, you will be equipped to analyze the efficacy of existing systems as well as make the right choice when developing new ones.\nWhy Do We Need So Many? There are many data structures available because no single dataset works best for all cases. Each data structure has its unique characteristics, advantages, and disadvantages. These differences can often be evaluated quantitatively, providing rigorous backing when selecting the appropriate one for the task at hand.\nFor example, arrays are excellent when the size of the data is known and constant, but they are not efficient when it comes to frequent insertions and deletions. Linked lists, on the other hand, allow for efficient insertions and deletions but are not as quick as arrays when it comes to accessing elements. Trees are invaluable when we need to maintain a sorted list of data and perform quick searches, insertions, and deletions, while hash tables are optimal for scenarios where we need to perform fast lookups.\nReview: Pointers What are Pointers? Pointers are variables in programming that store the memory address of another variable. They are a powerful feature in many programming languages, including C and C++, allowing programmers to directly access memory locations and manipulate data efficiently. Pointers are crucial for implementing dynamic data structures like linked lists, trees, and graphs.\nIn Python, pointers are not exposed explicitly as in languages like C, but references, which are similar to pointers, are used to hold the memory address of objects. Understanding the concept of pointers and references is essential for managing memory effectively and avoiding issues like memory leaks and dangling pointers in languages that allow direct memory manipulation. Even if we are not dealing with pointers directly, studying them is beneficial for understanding algorithms and data structures in general.\nHow are They Represented? In languages like C, pointers are represented using the asterisk (*) symbol, and the address operator (\u0026amp;) is used to retrieve the memory address of a variable. For example, int *p; declares a pointer to an integer, and p = \u0026amp;x; assigns the address of the variable x to the pointer p.\nIn Python, pointers are not explicitly represented, but references to objects are used to achieve similar functionality. For instance, when a list is assigned to a new variable, the new variable holds a reference to the same list object, not a copy of the list. Any modifications made through one variable are reflected in the other.\nArrays How are Arrays Represented in Memory? Arrays are fundamental data structures that store elements of the same type in contiguous memory locations. The elements can be accessed randomly by indexing into the array. In memory, an array is represented as a block of memory cells, each holding an element of the array placed side by side. The size of each cell is determined by the size of the array\u0026rsquo;s element type.\nThe base address of the array is the memory address of the first element (index 0), and it is used, along with the index and the size of each element, to calculate the address of any element in the array. For example, if the base address is `B`, the size of each element is `S`, and the index of the element is `i`, the address of the element can be calculated as `B + (i * S)`.\nFigure 1: Memory layout of an integer array of size 8. How Many Bytes Does Each Element Use? The number of bytes used by each element in an array depends on the data type of the elements. For example, in most systems, an int uses 4 bytes, a char uses 1 byte, and a double uses 8 bytes. When an array is declared, the total memory allocated for the array is the product of the number of elements and the size of each element.\nIn Python, the sys module can be used to find the size of an object in bytes. However, Python’s dynamic typing and object-oriented nature mean that the size of an array element can vary significantly, as each element is an object that can have additional overhead and can hold references to other objects.\nSince Python does not expose pointers explicitly, we can safely program efficient programs without worrying about making common mistakes related to pointers and memory management.\nHow are Arrays Indexed? Arrays are indexed using a zero-based indexing system, where the first element is accessed using index 0, the second element with index 1, and so on. To access an element at a specific index, the address of the element is calculated using the base address of the array, the size of each element, and the index of the element.\nIn languages that support pointers, the address of an element in an array can be calculated using pointer arithmetic. If p is a pointer to the base address of the array, and i is the index of the element, the address of the element can be calculated as p + i, where i is automatically scaled by the size of the array\u0026rsquo;s element type.\nMatrices Fixed-sized Arrays vs. Ragged Arrays Matrices are two-dimensional arrays that can be represented using fixed-sized arrays or ragged arrays. A fixed-sized array is a regular matrix where each row has the same number of columns, and it is represented in memory as a contiguous block. It allows for efficient access to elements using row and column indices but can waste memory if the matrix is sparse.\nA ragged array, on the other hand, is an irregular matrix where each row can have a different number of columns. It is represented using an array of pointers, where each pointer points to a one-dimensional array representing a row of the matrix. Ragged arrays are more memory-efficient for sparse matrices but can be more complex to manage and traverse.\nChoosing between fixed-sized and ragged arrays depends on the requirements of the application, the characteristics of the matrix, and the trade-offs between memory efficiency and complexity. Understanding the differences between the two representations is crucial for implementing matrices effectively and optimizing memory usage.\nFlat Indexing, Back and Forth Flat indexing is a technique used to represent a two-dimensional array or matrix using a one-dimensional array. In this representation, the elements of the matrix are stored in a single array in row-major or column-major order, and the two-dimensional indices (row and column) are mapped to a single index in the one-dimensional array.\nFor a matrix with M rows and N columns, the mapping from two-dimensional indices to a one-dimensional index in row-major order is done using the formula index = (row * N) + column, and in column-major order using the formula index = (column * M) + row. Flat indexing allows for efficient memory utilization and easy serialization of matrices but requires conversion between one-dimensional and two-dimensional indices.\nPython Example of Matrices In Python, matrices can be represented using lists of lists, where each inner list represents a row of the matrix. Elements can be accessed and modified using two indices, one for the row and one for the column. For example, to create a 2x3 matrix and access the element in the second row and third column, you can do the following:\nmatrix = [[1, 2, 3], [4, 5, 6]] element = matrix[1][2] # access value 6 NumPy also provides support for multi-dimensional arrays and matrices along with a host of functions to perform operations on them. Using Numpy, you can create a matrix and access its elements as follows:\nimport numpy as np matrix = np.array([[1, 2, 3], [4, 5, 6]]) element = matrix[1, 2] # access value 6 Multi-Dimensional Arrays Introduction to Multi-Dimensional Arrays Multi-dimensional arrays are arrays of more than 1 dimension. They are used to represent complex data structures like matrices, tensors, and tables. They are crucial for various applications, including scientific computing, image processing, and machine learning, where data is often multi-dimensional.\nIn a multi-dimensional array, each element is identified by multiple indices, one for each dimension of the array. For example, in a two-dimensional array representing a matrix, each element is identified by two indices representing the row and column of the element. The number of dimensions and the size of each dimension determine the structure and capacity of the array.\nDifferent languages have different approaches to handling multi-dimensional arrays. C, for example, arranges the memory of any array contiguously. Java and Python use jagged arrays, where the row lengths can differ in size. The data in each row might be contiguous, but the entire array is not.\nNumPy Arrays and Their Representation A NumPy array is represented in memory as a contiguous block, and it allows for efficient access and manipulation of elements using multiple indices. The shape of the array, represented as a tuple of integers, determines the number of dimensions and the size of each dimension of the array.\nTo determine how to index the contiguous stream of values represented as an $n$-dimensional array, each np.array specifies the strides for each dimension. Consider the following array with shape (2, 3, 4) and data type int32:\nimport numpy as np x = np.array([[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]], [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]], dtype=np.int32) The strides of the array are (48, 16, 4), which means that to move to the next depth, we need to move 48 bytes, to move to the next row, we need to move 16 bytes, and to move to the next column, we need to move 4 bytes. The strides are calculated based on the size of each element and the shape of the array.\nPython Example with NumPy Here’s an example of how to create a two-dimensional array (matrix) using Numpy and how to access its elements:\nimport numpy as np # Creating a 2x3 Numpy array array = np.array([[1, 2, 3], [4, 5, 6]]) # Accessing the element in the second row and third column element = array[1, 2] # element will be 6 NumPy also provides various functions to perform operations on arrays, such as reshaping, transposing, and aggregating. For example, to calculate the sum of all elements in the array, you can use the np.sum function:\ntotal = np.sum(array) # total will be 21 Stacks How are Stacks Represented with an Array? A stack is a linear data structure that follows the Last In First Out (LIFO) principle, meaning the last element added to the stack is the first one to be removed. Stacks can be easily implemented using arrays, where elements are added and removed from one end of the array, known as the top of the stack.\nWhen representing a stack with an array, one must keep track of the index of the top of the stack. Elements are added to the stack by placing them at the position indicated by the top index and then incrementing the top index. Elements are removed from the stack by decrementing the top index and then accessing the element at that position.\nDifference Between Using the Beginning or the End of the Array as the Top When implementing a stack using an array, one can choose to use either the beginning or the end of the array as the top of the stack. The choice affects the implementation of the push and pop operations and the way the top index is managed.\nIf the beginning of the array is used as the top, elements are added and removed from the first position of the array, and the other elements must be shifted to make room or fill the gap. This can lead to higher time complexity for push and pop operations. If the end of the array is used as the top, elements are added and removed from the last position of the array, allowing for constant-time push and pop operations without the need to shift other elements.\nPython Example of Stack with Array Here’s an example of how to implement a stack using a Python list, with the end of the list as the top of the stack:\nstack = [] # Initializing an empty stack stack.append(1) # Pushing an element onto the stack stack.append(2) # Pushing another element onto the stack top_element = stack.pop() # Popping the top element from the stack, top_element will be 2 This implementation allows for efficient and simple push and pop operations, with constant time complexity. However, the size of the stack is limited by the available memory, and care must be taken to handle underflow and overflow conditions.\nUnderstanding how to implement and use stacks is crucial for solving problems that involve reversing, balancing, or processing data in a LIFO manner. Stacks are a versatile and fundamental data structure used in various applications, including expression evaluation, syntax parsing, and undo mechanisms.\nQueues How are Queues Represented with an Array? A queue is a linear data structure that follows the First In First Out (FIFO) principle, meaning the first element added to the queue is the first one to be removed. Queues can be implemented using arrays, where elements are added at the rear and removed from the front.\nWhen representing a queue with an array, two indices are maintained, one for the front and one for the rear of the queue. Elements are enqueued by placing them at the position indicated by the rear index and then incrementing the rear index. Elements are dequeued by accessing the element at the front index and then incrementing the front index.\nDownside to Using One Side of the Array as the Front and the Other as the Rear When implementing a queue using an array, using one side of the array as the front and the other as the rear can lead to inefficient use of space. Once elements are dequeued from the front, the space they occupied cannot be reused, and overflow can occur even if there is free space at the front of the array.\nTo overcome this limitation, a circular queue can be implemented, where the front and rear indices wrap around to the beginning of the array when they reach the end. This allows for efficient use of space and avoids overflow as long as there is free space in the array. However, it requires more complex index management and can be harder to implement correctly.\nMore Efficient Approach by Using a Reference to a Head and Tail A more efficient approach to implementing a queue is to use a linked list, where each element holds a reference to the next element in the queue. This allows for dynamic resizing of the queue and efficient enqueue and dequeue operations, without the need for complex index management or wasted space.\nIn this approach, two pointers are maintained, one for the head (front) and one for the tail (rear) of the queue. Elements are enqueued by adding them at the position pointed to by the tail pointer and updating the tail pointer to the new element. Elements are dequeued by accessing the element pointed to by the head pointer and updating the head pointer to the next element.\nThis same approach can be done by using indices for the head and tail. The data itself is \u0026ldquo;circular\u0026rdquo; in the sense that the indices wrap around to the beginning of the array when they reach the end. This allows for efficient use of space and avoids overflow as long as there is free space in the array.\nPython Example of a Queue using an Array Here’s an example of how to implement a simple queue using a Python list, with the front at the beginning of the list and the rear at the end of the list:\nfrom collections import deque queue = deque() # Initializing an empty queue queue.append(1) # Enqueueing an element queue.append(2) # Enqueueing another element front_element = queue.popleft() # Dequeueing the front element from the queue, front_element will be 1 This implementation uses the deque class from the collections module, which allows for efficient appending and popping from both ends of the list. It provides a simple and versatile way to implement a queue in Python, with dynamic resizing and constant-time enqueue and dequeue operations.\nUnderstanding how to implement and use queues is essential for solving problems that involve processing data in a FIFO manner. Queues are a fundamental and versatile data structure used in various applications, including task scheduling, order processing, and breadth-first search.\n","date":1696136400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696136400,"objectID":"c43adb26bf0e538cd185ff8baec88e1f","permalink":"https://ajdillhoff.github.io/notes/introduction_to_data_structures/","publishdate":"2023-10-01T00:00:00-05:00","relpermalink":"/notes/introduction_to_data_structures/","section":"notes","summary":"Table of Contents Introduction to Data Structures Review: Pointers Arrays Matrices Multi-Dimensional Arrays Stacks Queues Introduction to Data Structures Data structures are fundamental concepts in computer science that allow us to organize and store data in a way that enables efficient access and modification. They are essential building blocks for creating efficient and sophisticated computer programs and databases. Different types of data structures include arrays, linked lists, stacks, queues, trees, graphs, and many more, each serving a specific purpose and suited to specific applications.","tags":["computer science","data structures"],"title":"Introduction to Data Structures","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Singly-Linked Lists Doubly-Linked Lists Operations Exercises A linked list is a dynamic and aggregate data structure made up of a collection of nodes. The nodes of a linked list can store any data type and are not enforced to contain the same data type. A basic node structure may be defined as\nstruct node { void *data; struct node *next; }; Singly-Linked Lists A singly-linked list, the most basic form, has a reference to the first node in the list, called the head, and a single link between each node.\nFigure 1: Diagram of a linked list with 3 nodes. The top sections contain data and the bottom sections contain pointers to the next node. The definition of a linked list node allows the list to grow dynamically. Nodes can be added at any time to any position in the node, as long as a reference to the node before the new one is known. The last node in a list points to NULL.\nDoubly-Linked Lists More commonly, linked lists are doubly-linked in that there is a link to the next node and a link to the previous node. This allows for more flexibility in traversing the list, but requires more memory to store the extra link. A standard implementation will also keep a reference to both the head and the tail of the list. This permits efficient insertion and deletion at both ends of the list.\nOperations Insertion A new node can be inserted either at the beginning, the end, or somewhere in between. Inserting at the beginning or end is a constant time operation, but inserting in the middle requires traversing the list to find the correct position. To insert at the beginning, the new node\u0026rsquo;s next reference is updated to the old head and the head is updated to the new node.\ndef insert_at_beginning(head, data): new_node = Node(data) new_node.next = head head = new_node To insert at the end without a reference to the tail, the list must be traversed to find the last node. The new node\u0026rsquo;s next reference is set to NULL and the last node\u0026rsquo;s next reference is set to the new node.\ndef insert_at_end(head, data): new_node = Node(data) new_node.next = None if head is None: head = new_node else: last_node = head while last_node.next is not None: last_node = last_node.next last_node.next = new_node To insert at the end with a reference to the tail, the new node\u0026rsquo;s next reference is set to NULL and the tail\u0026rsquo;s next reference is set to the new node. The tail is then updated to the new node.\ndef insert_at_end(head, tail, data): new_node = Node(data) new_node.next = None if head is None: head = new_node tail = new_node else: tail.next = new_node tail = new_node To insert in the middle, the list must be traversed to find the correct position. The new node\u0026rsquo;s next reference is set to the next node and the previous node\u0026rsquo;s next reference is set to the new node.\ndef insert_in_middle(head, data, position): new_node = Node(data) if head is None: head = new_node else: current_node = head for i in range(position - 1): current_node = current_node.next new_node.next = current_node.next current_node.next = new_node Searching Searching a linked list is a linear time operation. The list is traversed until the desired node is found or the end of the list is reached.\ndef search(head, data): current_node = head while current_node is not None: if current_node.data == data: return current_node current_node = current_node.next return None Deletion Deletion is similar to insertion. A node can be deleted from the beginning, the end, or somewhere in between. Deleting at the beginning or end is a constant time operation, but deleting in the middle requires traversing the list to find the correct position. To delete at the beginning, the head is updated to the second node and the first node is deleted.\ndef delete_at_beginning(head): if head is not None: head = head.next To delete at the end without a reference to the tail, the list must be traversed to find the last node. The second to last node\u0026rsquo;s next reference is set to NULL and the last node is deleted.\ndef delete_at_end(head): if head is not None: if head.next is None: head = None else: last_node = head while last_node.next.next is not None: last_node = last_node.next last_node.next = None To delete at the end with a reference to the tail, the tail is updated to the second to last node and the last node is deleted.\ndef delete_at_end(head, tail): if head is not None: if head.next is None: head = None tail = None else: last_node = head while last_node.next.next is not None: last_node = last_node.next last_node.next = None tail = last_node To delete in the middle, the list must be traversed to find the correct position. The previous node\u0026rsquo;s next reference is set to the next node and the current node is deleted.\ndef delete_in_middle(head, position): if head is not None: current_node = head for i in range(position - 1): current_node = current_node.next current_node.next = current_node.next.next Exercises Reverse a singly-linked list in linear time and constant space. Implement a queue using a linked list. ","date":1696136400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696136400,"objectID":"4503865aa3a005b9f5b525528459b074","permalink":"https://ajdillhoff.github.io/notes/linked_lists/","publishdate":"2023-10-01T00:00:00-05:00","relpermalink":"/notes/linked_lists/","section":"notes","summary":"Table of Contents Singly-Linked Lists Doubly-Linked Lists Operations Exercises A linked list is a dynamic and aggregate data structure made up of a collection of nodes. The nodes of a linked list can store any data type and are not enforced to contain the same data type. A basic node structure may be defined as\nstruct node { void *data; struct node *next; }; Singly-Linked Lists A singly-linked list, the most basic form, has a reference to the first node in the list, called the head, and a single link between each node.","tags":["computer science","data structures"],"title":"Linked Lists","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents The notation of complexity analysis Formal Definition of Asymptotic Notation Common Functions The notation of complexity analysis $O$-notation $O$-notation, often referred to as \u0026ldquo;Big Oh\u0026rdquo; notation, describes an upper bound on the behavior of a function. It really means that the function will not grow faster than the a given rate. This rate is typically the highest-order term in the function, and is often referred to as the \u0026ldquo;dominant term\u0026rdquo; or \u0026ldquo;dominant function\u0026rdquo;.\nFor example, the function \\(f(n) = 3n^2 + 2n + 1\\) has a dominant term of \\(n^2\\), and so we would say that \\(f(n) = O(n^2)\\). We could also accurately describe \\(f(n)\\) as \\(O(n^3)\\) since it technically does not grow at a faster rate than \\(n^3\\), but this is less common as it misleads the reader into thinking that the function is bounded at \\(n^3\\).\n$Ω$-notation $Ω$-notation is used to describe the lower bound on the asymptotic behavior of a function. Specifically, it means that the function grows at least as fast as the given rate. The function \\(f(n) = 3n^2 + 2n + 1\\) grows at least as fast as \\(n^2\\), so we would say that \\(f(n) = \\Omega(n^2)\\). It does not grow as fast as \\(n^3\\), however.\nJust like $O$-notation, we can abuse this definition and say that something that grows at least as fast as \\(n^2\\) also grows as fast as \\(n\\). This would lead the reader to believe that the function is bounded at \\(n\\), which is not true. For this reason, we typically use the tightest bound possible.\n$Θ$-notation $Θ$-notation gives a tightly bound characterization of a function\u0026rsquo;s behavior. It gives the rate of growth within a constant factor bounded above as well as constant factor bounded below.\nTo show that a function is \\(\\Theta(f(n))\\), we must show that it is both \\(O(f(n))\\) and \\(\\Omega(f(n))\\). Taking our example from above, the function \\(f(n) = 3n^2 + 2n + 1\\) is \\(\\Theta(n^2)\\).\nExample: Insertion Sort Let\u0026rsquo;s put this notation to work and characterize the running time of insertion sort. We\u0026rsquo;ll start by writing out the pseudocode for the algorithm:\ndef insertion_sort(A): for i in range(1, len(A)): key = A[i] j = i - 1 while i \u0026gt;= 0 and A[j] \u0026gt; key: A[j + 1] = A[j] j = j - 1 A[j + 1] = key From our Introduction to Algorithms lecture, we already know that the outer loop runs \\((n-1)\\) times (although the loop is checked \\(n\\) times). This is not dependent on the order of the \\(n\\) inputs either. The inner loop is dependent on the values of our input. It could run anywhere between 0 and \\(i-1\\) times. In the worst case, we saw that it would run \\(n-1\\) times as well. With this, we concluded that the running time of insertion sort is \\(O(n^2)\\). Since this was derived for the worst-case, it is reasonable to say that insertion sort is \\(O(n^2)\\) for all cases.\nThe key to the number of operations that the inner loop takes is A[j + 1] = A[j], or the number of times a value is shifted to the right. Given an input of \\(n\\) elements in the worst-case scenario, we can split the input into 3 partitions where the largest \\(\\lfloor\\frac{n}{4}\\rfloor\\) values are in the first partition. The second partition has size \\(\\lceil\\frac{n}{2}\\rceil\\), and the last partition has size \\(\\lfloor\\frac{n}{4}\\rfloor\\). By using the floor and ceiling functions, we can accommodate for odd values of \\(n\\).\nWhen the array is finally sorted, the largest \\(\\lfloor\\frac{n}{4}\\rfloor\\) values will be in the last partition. That means that they would have passed through the middle \\(\\lceil\\frac{n}{2}\\rceil\\) values one at a time. Therefore, we can state that the worst-case is proportional to\n\\[ \\left(\\left\\lfloor\\frac{n}{4}\\right\\rfloor\\right)\\left(\\left\\lceil\\frac{n}{2}\\right\\rceil\\right) \\leq \\frac{n^2}{8}. \\]\nThis is \\(\\Omega(n^2)\\), so we can conclude that insertion sort is \\(\\Theta(n^2)\\).\nBonus Example: Selection Sort Use a similar analysis to show that the worst-case for selection sort is \\(\\Theta(n^2)\\). As a reminder, selection sort is defined as\ndef selection_sort(A): for i in range(0, len(A)-1): min_j = i for j in range(i + 1, len(A)): if A[j] \u0026lt; A[min_j]: min_j = j A[i], A[min_j] = A[min_j], A[i] We have already observed that the outer loop iterates \\(n-1\\) times. Even in the best case, the inner loop runs proportional to \\(n\\) times. This is sufficient to conclude that the running time is \\(O(n^2)\\) for all cases.\nFor showing that the worst case is \\(\\Omega(n^2)\\), we could use the same argument as insertion sort. However, that isn\u0026rsquo;t necessary. In any case, the inner loop will run proportional to \\(n\\) times. It is not dependent on any specific arrangement of the input as selection sort is. Therefore, we can conclude that the worst-case is \\(\\Omega(n^2)\\), and so selection sort is \\(\\Theta(n^2)\\).\nFormal Definition of Asymptotic Notation Now that we have established some understanding of the notation, let\u0026rsquo;s define it formally. We typically use functions whose domains are over the set of natural or real numbers.\n$O$-notation We previously established that $O$-notation described as asymptotic upper bound. It was briefly mentioned that this bound holds within a constant factor, which we will now define more thoroughly. For a function \\(g(n)\\), \\(O(g(n)) = \\{f(n) : \\exists c \u0026gt; 0, n_0 \u0026gt; 0 \\text{ such that } 0 \\leq f(n) \\leq cg(n) \\text{ for all } n \\geq n_0\\}\\). It might make more sense to visualize this definition.\nFigure 1: Visualization of $O$-notation (source: Cormen et al.) Notice that the function \\(f(n)\\) is bounded above by \\(cg(n)\\) for all \\(n \\geq n_0\\) in the figure above.\nLet\u0026rsquo;s put this definition to the test with an example. Given a function \\(f(n) = 3n^2 + 200n + 1000\\), show that \\(f(n) = O(n^2)\\). The goal is to find positive constants \\(c\\) and \\(n_0\\) such that \\(3n^2 + 200n + 1000 \\leq cn^2\\) for all \\(n \\geq n_0\\). Dividing both sides by \\(n^2\\) yields\n\\[ 3 + \\frac{200}{n} + \\frac{1000}{n^2} \\leq c. \\]\nThis equation has many possible solutions. Let\u0026rsquo;s choose \\(n_0 = 2\\), then\n\\[ 3 + \\frac{200}{2} + \\frac{1000}{2^2} = 3 + 100 + 250 = 353 \\leq c. \\]\nTherefore, we can conclude that \\(f(n) = O(n^2)\\).\n$Ω$-notation The notation used to describe an asymptotic lower bound is formally defined as \\(\\Omega(g(n)) = \\{f(n) : \\exists c \u0026gt; 0, n_0 \u0026gt; 0 \\text{ such that } 0 \\leq cg(n) \\leq f(n) \\text{ for all } n \\geq n_0\\}\\). Again, it is helpful to visualize this.\nFigure 2: Visualization of $Ω$-notation (source: Cormen et al.) Notice that the function \\(f(n)\\) is bounded below by \\(cg(n)\\) for all \\(n \\geq n_0\\) in the figure above.\nLet\u0026rsquo;s revisit our function from above and show that \\(f(n) = \\Omega(n^2)\\). The goal is to find positive constants \\(c\\) and \\(n_0\\) such that \\(3n^2 + 200n + 1000 \\geq cn^2\\) for all \\(n \\geq n_0\\). Dividing both sides by \\(n^2\\) yields\n\\[ 3 + \\frac{200}{n} + \\frac{1000}{n^2} \\geq c. \\]\nThis holds when \\(c = 3\\) and \\(n_0\\) is any positive integer. To see this, think about what happens to this function as \\(n\\) approaches infinity. The first term will always be 3, and the second and third terms will approach 0. Therefore, we can conclude that \\(f(n) = \\Omega(n^2)\\).\n$Θ$-notation Lastly, the notation used for an asymptotically tight bound is \\(\\Theta(g(n)) = \\{f(n) : \\exists c_1, c_2 \u0026gt; 0, n_0 \u0026gt; 0 \\text{ such that } 0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n) \\text{ for all } n \\geq n_0\\}\\).\nFigure 3: Visualization of $Θ$-notation (source: Cormen et al.) We had mentioned previously that if \\(f(n) = \\Omega(g(n))\\) and \\(f(n) = O(g(n))\\), then \\(f(n) = \\Theta(g(n))\\). This is formalized in the following theorem, as stated in Cormen et al.\nFor any two functions \\(f(n)\\) and \\(g(n)\\), we have \\(f(n) = \\Theta(g(n))\\) if and only if \\(f(n) = O(g(n))\\) and \\(f(n) = \\Omega(g(n))\\).\nFunction Properties The following properties are useful when analyzing the asymptotic behavior of functions.\nTransitivity If \\(f(n) = O(g(n))\\) and \\(g(n) = O(h(n))\\), then \\(f(n) = O(h(n))\\). If \\(f(n) = \\Omega(g(n))\\) and \\(g(n) = \\Omega(h(n))\\), then \\(f(n) = \\Omega(h(n))\\). If \\(f(n) = \\Theta(g(n))\\) and \\(g(n) = \\Theta(h(n))\\), then \\(f(n) = \\Theta(h(n))\\). Reflexivity \\(f(n) = O(f(n))\\) \\(f(n) = \\Omega(f(n))\\) \\(f(n) = \\Theta(f(n))\\) Symmetry \\(f(n) = \\Theta(g(n))\\) if and only if \\(g(n) = \\Theta(f(n))\\). Transpose Symmetry \\(f(n) = O(g(n))\\) if and only if \\(g(n) = \\Omega(f(n))\\). Common Functions The functions used to describe both time and space complexity are visualized below.\nFigure 4: Common functions used in complexity analysis (source: Wikipedia) ","date":1695618000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695618000,"objectID":"2e6bf41f4deb3ffa0071eeebb242a1e0","permalink":"https://ajdillhoff.github.io/notes/complexity_analysis/","publishdate":"2023-09-25T00:00:00-05:00","relpermalink":"/notes/complexity_analysis/","section":"notes","summary":"Table of Contents The notation of complexity analysis Formal Definition of Asymptotic Notation Common Functions The notation of complexity analysis $O$-notation $O$-notation, often referred to as \u0026ldquo;Big Oh\u0026rdquo; notation, describes an upper bound on the behavior of a function. It really means that the function will not grow faster than the a given rate. This rate is typically the highest-order term in the function, and is often referred to as the \u0026ldquo;dominant term\u0026rdquo; or \u0026ldquo;dominant function\u0026rdquo;.","tags":["computer science","algorithms","complexity analysis"],"title":"Complexity Analysis","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction to Algorithms Insertion Sort Example: Sorting Numbers Worst-Case Analysis Best-Case Analysis Rate of Growth Example: Analysis of Selection Sort Introduction to Algorithms One of the major goals of computer science is to solve important problems. In order to do that, we must be able to express those solutions both mathematically and in a way that can be executed by a computer. Further, those solutions need to be aware of the resources that are available to them. It does us no good to come up with a solution that could never be run by current hardware or executed in a reasonable amount of time.\nThere are of course other considerations besides runtime. How much memory does the solution require? Does it require a lot of data to be stored on disk? What about distributed solutions that can be run on multiple machines? Some solutions can be so complex, that we must also consider their environmental impact. For example, Meta\u0026rsquo;s Llama 2 large language models required 3,311,616 combined GPU hours to train. They report that their total carbon emissions from training were 539 tons of CO2 equivalent (Touvron et al. 2023).\nWe begin our algorithmic journey by studying a simple sorting algorithm, insertion sort. First, we need to formally define the problem of sorting. Given a sequence of \\(n\\) objects \\(A = \\langle a_1, a_2, \\ldots, a_n \\rangle\\), we want to rearrange the elements such that \\(a_1\u0026rsquo; \\leq a_2\u0026rsquo; \\leq \\ldots \\leq a_n\u0026rsquo;\\). We will assume that the elements are comparable, meaning that we can use the operators \\(\u0026lt;\\) and \\(\u0026gt;\\) to compare them. Some sets, such as the set of all real numbers, have a natural ordering. A useful programming language would provide the required comparison operators. For other types of elements, such as strings, this may not be the case. For example, how would you compare the strings \u0026ldquo;apple\u0026rdquo; and \u0026ldquo;banana\u0026rdquo;? In these cases, we will need to define our own comparison operators. Either way, we will assume that the comparison operators are available to us.\nThis example follows the one given in Chapter 2 of Cormen et al. (2009).\nInsertion Sort Insertion sort is defined as\ndef insertion_sort(A): for i in range(1, len(A)): key = A[i] j = i - 1 while j \u0026gt;= 0 and A[j] \u0026gt; key: A[j + 1] = A[j] j = j - 1 A[j + 1] = key Example: Sorting Numbers TODO: Add a step-by-step example of sorting a list of numbers.\nWorst-Case Analysis Given the definition from above, we can compute \\(T(n)\\), the running time of the algorithm on an input of size \\(n\\). To do this, we need to sum the products of the cost of each statement and the number of times each statement is executed.\nAt first glance, the first statement for i in range(1, len(A)) appears to execute \\(n-1\\) times since it starts at 1 and only goes up to, but not including, \\(n\\). Remember that the for statement must be checked to see if it should exit, so the test is executed one more time than the number of iterations. Therefore, the first statement is executed \\(n\\) times. If we say that the cost to execute each check is \\(c_1\\), then the total cost of the first statement is \\(c_1 n\\).\nWith the exception of the while loop, the statement inside the for loop is executed once per iteration. The cost of executing statement \\(i\\) is \\(c_i\\). Therefore, the total cost of the second statement is \\(c_2 n\\). The costs are updated in the code below.\ndef insertion_sort(A): for i in range(1, len(A)): # c_1 n key = A[i] # c_2 n j = i - 1 # c_3 n while j \u0026gt;= 0 and A[j] \u0026gt; key: A[j + 1] = A[j] j = j - 1 A[j + 1] = key # c_7 n For the while loop, we can denote the number of times it runs by \\(t_i\\), where \\(i\\) is the iteration of the for loop. If the while condition check costs \\(c_4\\) and is executed \\(t_i\\) times for each for loop iteration, the total cost is given as \\(c_4 \\sum_{i=1}^{n-1} t_i\\).\nThe statement inside the while loop are executed 1 fewer times than the number of times the condition check is executed. Therefore, the total cost of the statements inside the while loop is \\(c_5 \\sum_{i=1}^{n-1} (t_i - 1) + c_5 \\sum_{i=1}^{n-1} (t_i - 1)\\). The cost of the while loop is updated in the code below.\ndef insertion_sort(A): for i in range(1, len(A)): # c_1 * n key = A[i] # c_2 * (n-1) j = i - 1 # c_3 * (n-1) while j \u0026gt;= 0 and A[j] \u0026gt; key: # c_4 * [t_i for i in range(1, len(A))] A[j + 1] = A[j] # c_5 * [t_i - 1 for i in range(1, len(A))] j = j - 1 # c_6 * [t_i - 1 for i in range(1, len(A))] A[j + 1] = key # c_7 * (n-1) To get the total running time \\(T(n)\\), we sum up all of the costs.\n\\begin{align} T(n) \u0026amp;= c_1 n + c_2 (n-1) + c_3 (n-1) + c_4 \\sum_{i=1}^{n-1} t_i + c_5 \\sum_{i=1}^{n-1} (t_i - 1) + c_6 \\sum_{i=1}^{n-1} (t_i - 1) + c_7 (n-1) \\\\ \\end{align}\nThis analysis is a good start, but it doesn\u0026rsquo;t paint the whole picture. The number of actual executions will depend on the input that is given. For example, what if the input is already sorted, or given in reverse order? It is common to express the worst-case runtime for a particular algorithm. For insertion sort, that is when the input is in reverse order. In this case, each element \\(A[i]\\) is compared to every other element in the sorted subarray. This means that \\(t_i = i\\) for every iteration of the for loop. Therefore, the worst-case runtime is given as\n\\begin{align} T(n) \u0026amp;= c_1 n + c_2 (n-1) + c_3 (n-1) + c_4 \\sum_{i=1}^{n-1} i + c_5 \\sum_{i=1}^{n-1} (i - 1) + c_6 \\sum_{i=1}^{n-1} (i - 1) + c_7 (n-1) \\\\ \\end{align}\nTo express this runtime solely in terms of \\(n\\), we can use the fact that \\(\\sum_{i=1}^{n-1} i = (\\sum_{i=0}^{n-1} i) - 1 = \\frac{n(n-1)}{2} - 1\\) and \\(\\sum_{i=1}^{n-1} (i - 1) = \\sum_{i=0}^{n-2} i = \\frac{n(n-1)}{2}\\). This gives us\n\\begin{align} T(n) \u0026amp;= c_1 n + c_2 (n-1) + c_3 (n-1) + c_4 \\left(\\frac{n(n-1)}{2} - 1\\right)\\\\ \u0026amp;+ c_5 \\left(\\frac{n(n-1)}{2}\\right) + c_6 \\left(\\frac{n(n-1)}{2}\\right) + c_7 (n-1) \\\\ \u0026amp;= \\left(\\frac{c_4}{2} + \\frac{c_5}{2} + \\frac{c_6}{2}\\right)n^2 + \\left(c_1 + c_2 + c_3 + \\frac{c_4}{2} - \\frac{c_5}{2} - \\frac{c_6}{2} + c_7\\right)n - (c_2 + c_3 + c_4 + c_7) \\\\ \\end{align}\nWith the appropriate choice of constants, we can express this as a quadratic function \\(an^2 + bn + c\\).\nBest-Case Analysis The best-case runtime for insertion sort is when the input is already sorted. In this case, the while check is executed only once per iteration of the for loop. That is, \\(t_i = 1\\) for every iteration of the for loop. Therefore, the best-case runtime is given as\n\\begin{align} T(n) \u0026amp;= c_1 n + c_2 (n-1) + c_3 (n-1) + c_4 (n-1) + c_7 (n-1) \\\\ \u0026amp;= (c_1 + c_2 + c_3 + c_4 + c_7)n - (c_2 + c_3 + c_4 + c_7) \\\\ \\end{align}\nLet \\(a = c_1 + c_2 + c_3 + c_4 + c_7\\) and $b = -(c_2 + c_3 + c_4 + c_7)$Then the best-case runtime is given as \\(an + b\\), a linear function of \\(n\\).\nRate of Growth We can simplify how we express the runtime of both these cases by considering only the highest-order term. Consider the worst-case, \\(T(n) = an^2 + bn + c\\). As \\(n\\) grows, the term \\(an^2\\) will dominate the runtime, rendering the others insignificant by comparison. This simplification is typically expressed using \\(\\Theta\\) notation. For the worst-case, we say that \\(T(n) = \\Theta(n^2)\\). It is a compact way of stating that the runtime is proportional to \\(n^2\\) for large values of \\(n\\).\nExample: Analysis of Selection Sort Based on the analysis above, let\u0026rsquo;s check our understanding and see if we can characterize the runtime of another sorting algorithm, selection sort. Selection sort is defined as\ndef selection_sort(A): for i in range(0, len(A) - 1): min_index = i for j in range(i + 1, len(A)): if A[j] \u0026lt; A[min_index]: min_index = j A[i], A[min_index] = A[min_index], A[i] The first statement for i in range(0, len(A) - 1) will be evaluated \\(n\\) times. With the exception of the inner for loop, the rest of the statements in the scope of the first for loop are executed once per iteration. Their costs are \\(c_2\\) and \\(c_6\\), respectively.\nThe inner for loop will be checked \\(n-i\\) times for each iteration of the outer for loop. The cost of the condition check is \\(c_3\\). The cost of the statements inside the for loop are \\(c_4\\) and \\(c_5\\). The if check is evaluated for every iteration of the inner loop, but the statements inside the if are only executed when the condition is true. We can denote this as \\(t_i\\), the number of times the if condition is true for each iteration of the inner for loop. The cost of the inner loop is given as\n\\begin{align} c_3 \\sum_{i=1}^{n-1} (n-i) + c_4 \\sum_{i=0}^{n-1} (n-i-1) + c_5 \\sum_{i=0}^{n-1} t_i\\\\ \\end{align}\nCombining this with the cost of the outer for loop, we get\n\\begin{align} T(n) \u0026amp;= c_1 n + c_2 (n-1) + c_6 (n-1) + c_3 \\sum_{i=0}^{n-1} (n-i) + c_4 \\sum_{i=0}^{n-1} (n-i-1) + c_5 \\sum_{i=0}^{n-1} t_i\\\\ \\end{align}\nReferences Touvron, Hugo, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, et al. 2023. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” arXiv. https://doi.org/10.48550/arXiv.2307.09288. ","date":1695099600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695099600,"objectID":"ae729bf0a8e3739ea8dbfc7bb807a7db","permalink":"https://ajdillhoff.github.io/notes/introduction_to_complexity_analysis/","publishdate":"2023-09-19T00:00:00-05:00","relpermalink":"/notes/introduction_to_complexity_analysis/","section":"notes","summary":"Table of Contents Introduction to Algorithms Insertion Sort Example: Sorting Numbers Worst-Case Analysis Best-Case Analysis Rate of Growth Example: Analysis of Selection Sort Introduction to Algorithms One of the major goals of computer science is to solve important problems. In order to do that, we must be able to express those solutions both mathematically and in a way that can be executed by a computer. Further, those solutions need to be aware of the resources that are available to them.","tags":["computer science","algorithms"],"title":"Introduction to Algorithms","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":"Python Review Questions The following questions are meant to help you review introductory concepts in Python. They are based on the Python 3 Tutorial and Python 3 Documentation and were written to accompany a 5 lecture series on Python.\nThere are three types of questions:\nVerify the Code: Determine the output of a code snippet. Fill in the Code: Fill in the code to complete a code snippet. Create the Function: Create a function that satisfies the given requirements. Verify the Code Operations on Strings\nprint(\u0026#34;Hello\u0026#34; + \u0026#34; \u0026#34; + \u0026#34;World\u0026#34; * 2) What will the output be?\nSolution Hello World World Explanation: The + operator concatenates strings, and the * operator repeats strings.\nLooping Over Lists\nfor i in [1, 2, 3]: print(i * 2) What will the output be?\nSolution 2 4 6 Explanation: The for loop iterates over the elements of the list.\nMutability of Strings\ns = \u0026#34;hello\u0026#34; s[0] = \u0026#34;H\u0026#34; Is this code valid? If not, why?\nSolution This code is not valid. Strings are immutable, so you cannot change their elements.\nStatic Methods vs. Instance Methods\nclass MyClass: def instance_method(self): return \u0026#39;instance method\u0026#39; @staticmethod def static_method(): return \u0026#39;static method\u0026#39; obj = MyClass() print(obj.instance_method()) print(MyClass.static_method()) What will the output be?\nSolution instance method static method Explanation: Instance methods are called on an instance of a class, whereas static methods are called on the class itself.\nyield Keyword Example\ndef my_gen(): yield \u0026#34;A\u0026#34; yield \u0026#34;B\u0026#34; gen = my_gen() print(next(gen)) print(next(gen)) print(next(gen)) What will the output be?\nSolution A B Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; StopIteration Explanation: The yield keyword is used to create generators. Generators are iterators that can be iterated over only once.\nType Validation\nx = 42 print(isinstance(x, int)) What will the output be?\nSolution True Explanation: The isinstance function checks if an object is of a certain type.\ninput Always Returns a str\nx = input(\u0026#34;Enter a number: \u0026#34;) print(type(x) is int) If the user enters 42, what will the output be?\nSolution False Explanation: The input function always returns a str, even if the user enters a number.\nSearch in List Example\nmy_list = [1, 2, 3, 4, 5] print(3 in my_list) What will the output be?\nSolution True Explanation: The in operator checks if an element is in a list.\nFormatted Printing\nname = \u0026#34;Alice\u0026#34; age = 30 print(f\u0026#34;{name} is {age} years old.\u0026#34;) What will the output be?\nSolution Alice is 30 years old. Explanation: The f prefix allows you to use formatted strings.\nEquality Versus is\nx = [1, 2, 3] y = [1, 2, 3] print(x == y) print(x is y) What will the output be?\nSolution True False Explanation: The == operator checks if two objects are equal, whereas the is operator checks if two objects are the same object.\nCSV Line to List Using split\nline = \u0026#34;apple,banana,cherry\u0026#34; fruits = line.split(\u0026#39;,\u0026#39;) print(fruits) What will the output be?\nSolution [\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;cherry\u0026#39;] Explanation: The split method splits a string into a list of strings using a delimiter.\nDeep vs. Shallow Copy\nimport copy a = [1, [2, 3]] b = copy.copy(a) c = copy.deepcopy(a) a[1][0] = 99 print(b) print(c) What will the output be?\nSolution [1, [99, 3]] [1, [2, 3]] Explanation: The copy function creates a shallow copy, whereas the deepcopy function creates a deep copy.\nAccess Class Variable vs. Instance Variable\nclass Dog: kind = \u0026#39;canine\u0026#39; def __init__(self, name): self.name = name d = Dog(\u0026#39;Fido\u0026#39;) print(d.kind) print(d.name) What will the output be?\nSolution canine Fido Explanation: The kind variable is a class variable, whereas the name variable is an instance variable.\nAccessing global Variable\nx = 10 def foo(): global x x += 5 print(x) foo() What will the output be?\nSolution 15 Explanation: The global keyword allows you to access a global variable inside a function.\nFill in the Code Looping Over 2D Lists\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] # Fill in the code to print each element in the 2D list. Solution for row in matrix: for element in row: print(element) Explanation: The outer loop iterates over the rows, and the inner loop iterates over the elements in each row.\nStatic Methods vs. Instance Methods\nclass Calculator: @staticmethod def add(a, b): return a + b # Fill in the code to create an instance method that multiplies two numbers. Solution class Calculator: @staticmethod def add(a, b): return a + b def multiply(self, a, b): return a * b Explanation: Instance methods take self as the first argument.\nList Comprehension for 2D List\n# Fill in the code to create a 2D list with list comprehension. # The 2D list should contain rows from 0 to 4 and columns from 0 to 4, # where each element is the sum of its row and column index. Solution matrix = [[row + col for col in range(5)] for row in range(5)] Explanation: The outer loop iterates over the rows, and the inner loop iterates over the columns.\nDictionary Add, Iterating Over Keys and Values\nmy_dict = {\u0026#39;apple\u0026#39;: 1, \u0026#39;banana\u0026#39;: 2} # Fill in the code to add a key-value pair (\u0026#39;cherry\u0026#39;, 3) to my_dict and print all keys and values. Solution my_dict[\u0026#39;cherry\u0026#39;] = 3 for key in my_dict: print(key, my_dict[key]) Explanation: The for loop iterates over the keys of the dictionary.\nAdd List to Existing List (Zip vs. Extend)\nlist1 = [1, 2, 3] list2 = [4, 5, 6] # Fill in the code to append the elements of list2 to list1. Solution list1.extend(list2) Explanation: The extend method appends the elements of a list to another list.\nOverride Special Method So Class Can Be Sorted\nclass Person: def __init__(self, name, age): self.name = name self.age = age # Fill in the code to make instances of this class sortable by age. Solution class Person: def __init__(self, name, age): self.name = name self.age = age def __lt__(self, other): return self.age \u0026lt; other.age Explanation: The __lt__ method overrides the \u0026lt; operator. Either this method or the __gt__ method must be defined for the class to be sortable.\nCreate the Function Reading Input From the User\nnumbers = read_numbers_from_user(5) print(\u0026#34;The sum is:\u0026#34;, sum(numbers)) Create the function read_numbers_from_user that takes an integer ( n ) and reads ( n ) numbers from the user.\nSolution def read_numbers_from_user(n): numbers = [] for i in range(n): numbers.append(int(input(\u0026#34;Enter a number: \u0026#34;))) return numbers Explanation: The input function reads a string from the user. The int function converts a string to an integer.\nMatch Statement vs. If-Elif-Else\nprint(match_fruit_color(\u0026#34;apple\u0026#34;)) Create the function match_fruit_color that takes a fruit name and returns its color using a match statement.\nSolution def match_fruit_color(fruit): match fruit: case \u0026#34;apple\u0026#34;: return \u0026#34;red\u0026#34; case \u0026#34;banana\u0026#34;: return \u0026#34;yellow\u0026#34; case \u0026#34;cherry\u0026#34;: return \u0026#34;red\u0026#34; case _: return \u0026#34;unknown\u0026#34; Explanation: The match statement is used to compare a value against a number of patterns. It is similar to the switch statement in other languages.\nFormatted Printing\nprint_formatted_string(\u0026#34;John\u0026#34;, 25) Create the function print_formatted_string that takes a name and an age and prints them in a formatted string.\nSolution def print_formatted_string(name, age): print(f\u0026#34;{name} is {age} years old.\u0026#34;) Explanation: The f prefix allows you to use formatted strings.\nType Validation\nprint(is_valid_number(\u0026#34;42\u0026#34;)) Create the function is_valid_number that takes a string and returns True if it can be converted to an integer or a float, otherwise False.\nSolution def is_valid_number(s): try: float(s) return True except ValueError: return False Explanation: The try statement allows you to handle exceptions. The float function converts a string to a float. If a string could be converted to a float, it can also be converted to an int.\nAccessing global Variable\nincrement_global_x() print(x) Create the function increment_global_x that increments the global variable ( x ) by 1.\nSolution def increment_global_x(): global x x += 1 Explanation: The global keyword allows you to access a global variable inside a function.\n","date":1694494800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694494800,"objectID":"4be737f5d2b79e20db4eee0b2e491ba7","permalink":"https://ajdillhoff.github.io/notes/python_exam_review/","publishdate":"2023-09-12T00:00:00-05:00","relpermalink":"/notes/python_exam_review/","section":"notes","summary":"Python Review Questions The following questions are meant to help you review introductory concepts in Python. They are based on the Python 3 Tutorial and Python 3 Documentation and were written to accompany a 5 lecture series on Python.\nThere are three types of questions:\nVerify the Code: Determine the output of a code snippet. Fill in the Code: Fill in the code to complete a code snippet. Create the Function: Create a function that satisfies the given requirements.","tags":["python"],"title":"Python Review Questions","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" NumPy Quickstart This notebook is a quick introduction to NumPy. It is an interactive version of the NumPy Quickstart Tutorial.\nAll credits go to the original authors of the tutorial © Copyright 2008-2023, NumPy Developers.\nThe Basics NumPy’s main object is the homogeneous multidimensional array. It is a table of elements (usually numbers), all of the same type, indexed by a tuple of non-negative integers. In NumPy dimensions are called axes.\nFor example, the array for the coordinates of a point in 3D space, [1, 2, 1], has one axis. That axis has 3 elements in it, so we say it has a length of 3. In the example pictured below, the array has 2 axes. The first axis has a length of 2, the second axis has a length of 3.\n[[ 1., 0., 0.], [ 0., 1., 2.]] NumPy’s array class is called ndarray. It is also known by the alias array. Note that numpy.array is not the same as the Standard Python Library class array.array, which only handles one-dimensional arrays and offers less functionality. The more important attributes of an ndarray object are:\nndarray.ndim\nthe number of axes (dimensions) of the array.\nndarray.shape\nthe dimensions of the array. This is a tuple of integers indicating the size of the array in each dimension. For a matrix with n rows and m columns, shape will be (n,m). The length of the shape tuple is therefore the number of axes, ndim.\nndarray.size\nthe total number of elements of the array. This is equal to the product of the elements of shape.\nndarray.dtype\nan object describing the type of the elements in the array. One can create or specify dtype’s using standard Python types. Additionally NumPy provides types of its own. numpy.int32, numpy.int16, and numpy.float64 are some examples.\nndarray.itemsize\nthe size in bytes of each element of the array. For example, an array of elements of type float64 has itemsize 8 (=64/8), while one of type complex32 has itemsize 4 (=32/8). It is equivalent to ndarray.dtype.itemsize.\nndarray.data\nthe buffer containing the actual elements of the array. Normally, we won’t need to use this attribute because we will access the elements in an array using indexing facilities.\nAn example import numpy as np a = np.arange(15).reshape(3, 5) print(\u0026#34;a = \\n{}\u0026#34;.format(a)) print(\u0026#34;a.shape = {}\u0026#34;.format(a.shape)) print(\u0026#34;a.ndim = {}\u0026#34;.format(a.ndim)) print(\u0026#34;a.dtype.name = {}\u0026#34;.format(a.dtype.name)) print(\u0026#34;a.itemsize = {}\u0026#34;.format(a.itemsize)) print(\u0026#34;a.size = {}\u0026#34;.format(a.size)) print(\u0026#34;type(a) = {}\u0026#34;.format(type(a))) a = [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14]] a.shape = (3, 5) a.ndim = 2 a.dtype.name = int64 a.itemsize = 8 a.size = 15 type(a) = \u0026lt;class 'numpy.ndarray'\u0026gt; Array Creation There are several ways to create arrays.\nFor example, you can create an array from a regular Python list or tuple using the array function. The type of the resulting array is deduced from the type of the elements in the sequences.\na = np.array([2,3,4]) print(\u0026#34;a = {}\u0026#34;.format(a)) print(\u0026#34;a.dtype = {}\u0026#34;.format(a.dtype)) b = np.array([1.2, 3.5, 5.1]) print(\u0026#34;b = {}\u0026#34;.format(b)) print(\u0026#34;b.dtype = {}\u0026#34;.format(b.dtype)) a = [2 3 4] a.dtype = int64 b = [1.2 3.5 5.1] b.dtype = float64 A frequent error consists in calling array with multiple numeric arguments, rather than providing a single list of numbers as an argument.\na = np.array(1, 2, 3, 4) # WRONG --------------------------------------------------------------------------- TypeError Traceback (most recent call last) /home/alex/dev/teaching/python-examples/numpy/quickstart.ipynb Cell 6 line 1 ----\u0026gt; \u0026lt;a href='vscode-notebook-cell:/home/alex/dev/teaching/python-examples/numpy/quickstart.ipynb#X14sZmlsZQ%3D%3D?line=0'\u0026gt;1\u0026lt;/a\u0026gt; a = np.array(1, 2, 3, 4) TypeError: array() takes from 1 to 2 positional arguments but 4 were given a = np.array([1, 2, 3, 4]) # RIGHT array transforms sequences of sequences into two-dimensional arrays, sequences of sequences of sequences into three-dimensional arrays, and so on.\nb = np.array([(1.5, 2, 3), (4, 5, 6)]) print(\u0026#34;b =\\n{}\u0026#34;.format(b)) b = [[1.5 2. 3. ] [4. 5. 6. ]] The type of the array can also be explicitly specified at creation time:\nc = np.array([[1, 2], [3, 4]], dtype=complex) print(\u0026#34;c =\\n{}\u0026#34;.format(c)) c = [[1.+0.j 2.+0.j] [3.+0.j 4.+0.j]] Often, the elements of an array are originally unknown, but its size is known. Hence, NumPy offers several functions to create arrays with initial placeholder content. These minimize the necessity of growing arrays, an expensive operation.\nThe function zeros creates an array full of zeros, the function ones creates an array full of ones, and the function empty creates an array whose initial content is random and depends on the state of the memory. By default, the dtype of the created array is float64.\nprint(\u0026#34;np.zeros((3, 4)) =\\n{}\u0026#34;.format(np.zeros((3, 4)))) print(\u0026#34;np.ones((2, 3, 4), dtype=np.int16) =\\n{}\u0026#34;.format(np.ones((2, 3, 4), dtype=np.int16))) print(\u0026#34;np.empty((2, 3)) =\\n{}\u0026#34;.format(np.empty((2, 3)))) np.zeros((3, 4)) = [[0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.]] np.ones((2, 3, 4), dtype=np.int16) = [[[1 1 1 1] [1 1 1 1] [1 1 1 1]] [[1 1 1 1] [1 1 1 1] [1 1 1 1]]] np.empty((2, 3)) = [[1.39069238e-309 1.39069238e-309 1.39069238e-309] [1.39069238e-309 1.39069238e-309 1.39069238e-309]] To create sequences of numbers, NumPy provides the arange function which is analogous to the Python built-in range, but returns an array.\nprint(\u0026#34;np.arange(10, 30, 5) = {}\u0026#34;.format(np.arange(10, 30, 5))) print(\u0026#34;np.arange(0, 2, 0.3) = {}\u0026#34;.format(np.arange(0, 2, 0.3))) # it accepts float arguments np.arange(10, 30, 5) = [10 15 20 25] np.arange(0, 2, 0.3) = [0. 0.3 0.6 0.9 1.2 1.5 1.8] When arange is used with floating point arguments, it is generally not possible to predict the number of elements obtained, due to the finite floating point precision. For this reason, it is usually better to use the function linspace that receives as an argument the number of elements that we want, instead of the step:\nfrom numpy import pi print(\u0026#34;np.linspace(0, 2, 9) = {}\u0026#34;.format(np.linspace(0, 2, 9))) # 9 numbers from 0 to 2 x = np.linspace(0, 2*pi, 100) # useful to evaluate function at lots of points f = np.sin(x) np.linspace(0, 2, 9) = [0. 0.25 0.5 0.75 1. 1.25 1.5 1.75 2. ] Print Arrays When you print an array, NumPy displays it in a similar way to nested lists, but with the following layout:\nthe last axis is printed from left to right, the second-to-last is printed from top to bottom, the rest are also printed from top to bottom, with each slice separated from the next by an empty line. One-dimensional arrays are then printed as rows, bidimensionals as matrices and tridimensionals as lists of matrices.\na = np.arange(6) # 1d array print(\u0026#34;a =\\n{}\u0026#34;.format(a)) b = np.arange(12).reshape(4, 3) # 2d array print(\u0026#34;b =\\n{}\u0026#34;.format(b)) c = np.arange(24).reshape(2, 3, 4) # 3d array print(\u0026#34;c =\\n{}\u0026#34;.format(c)) a = [0 1 2 3 4 5] b = [[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]] c = [[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] If an array is too large to be printed, NumPy automatically skips the central part of the array and only prints the corners:\nprint(\u0026#34;np.arange(10000) = {}\u0026#34;.format(np.arange(10000))) print(\u0026#34;np.arange(10000).reshape(100, 100) =\\n{}\u0026#34;.format(np.arange(10000).reshape(100, 100))) np.arange(10000) = [ 0 1 2 ... 9997 9998 9999] np.arange(10000).reshape(100, 100) = [[ 0 1 2 ... 97 98 99] [ 100 101 102 ... 197 198 199] [ 200 201 202 ... 297 298 299] ... [9700 9701 9702 ... 9797 9798 9799] [9800 9801 9802 ... 9897 9898 9899] [9900 9901 9902 ... 9997 9998 9999]] To disable this behaviour and force NumPy to print the entire array, you can change the printing options using set_printoptions.\nimport sys np.set_printoptions(threshold=sys.maxsize) # force NumPy to print the entire array Basic Operations Arithmetic operators on arrays apply elementwise. A new array is created and filled with the result.\na = np.array([20, 30, 40, 50]) b = np.arange(4) print(\u0026#34;b =\\n{}\u0026#34;.format(b)) c = a - b print(\u0026#34;c =\\n{}\u0026#34;.format(c)) print(\u0026#34;b**2 =\\n{}\u0026#34;.format(b**2)) print(\u0026#34;10*np.sin(a) =\\n{}\u0026#34;.format(10*np.sin(a))) print(\u0026#34;a \u0026lt; 35 =\\n{}\u0026#34;.format(a \u0026lt; 35)) b = [0 1 2 3] c = [20 29 38 47] b**2 = [0 1 4 9] 10*np.sin(a) = [ 9.12945251 -9.88031624 7.4511316 -2.62374854] a \u0026lt; 35 = [ True True False False] Unlike in many matrix languages, the product operator * operates elementwise in NumPy arrays. The matrix product can be performed using the @ operator (in python \u0026gt;=3.5) or the dot function or method:\nA = np.array([[1, 1], [0, 1]]) B = np.array([[2, 0], [3, 4]]) print(\u0026#34;A * B =\\n{}\u0026#34;.format(A * B)) # elementwise product print(\u0026#34;A @ B =\\n{}\u0026#34;.format(A @ B)) # matrix product print(\u0026#34;A.dot(B) =\\n{}\u0026#34;.format(A.dot(B))) # another matrix product A * B = [[2 0] [0 4]] A @ B = [[5 4] [3 4]] A.dot(B) = [[5 4] [3 4]] Some operations, such as += and *=, act in place to modify an existing array rather than create a new one.\nrg = np.random.default_rng(1) # create instance of default random number generator a = np.ones((2, 3), dtype=int) b = rg.random((2, 3)) a *= 3 print(\u0026#34;a =\\n{}\u0026#34;.format(a)) b += a print(\u0026#34;b =\\n{}\u0026#34;.format(b)) a += b # b is not automatically converted to integer type a = [[3 3 3] [3 3 3]] b = [[3.51182162 3.9504637 3.14415961] [3.94864945 3.31183145 3.42332645]] --------------------------------------------------------------------------- UFuncTypeError Traceback (most recent call last) /home/alex/dev/teaching/python-examples/numpy/quickstart.ipynb Cell 29 line 1 \u0026lt;a href='vscode-notebook-cell:/home/alex/dev/teaching/python-examples/numpy/quickstart.ipynb#Y110sZmlsZQ%3D%3D?line=7'\u0026gt;8\u0026lt;/a\u0026gt; b += a \u0026lt;a href='vscode-notebook-cell:/home/alex/dev/teaching/python-examples/numpy/quickstart.ipynb#Y110sZmlsZQ%3D%3D?line=9'\u0026gt;10\u0026lt;/a\u0026gt; print(\u0026quot;b =\\n{}\u0026quot;.format(b)) ---\u0026gt; \u0026lt;a href='vscode-notebook-cell:/home/alex/dev/teaching/python-examples/numpy/quickstart.ipynb#Y110sZmlsZQ%3D%3D?line=11'\u0026gt;12\u0026lt;/a\u0026gt; a += b UFuncTypeError: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind' When operating with arrays of different types, the type of the resulting array corresponds to the more general or precise one (a behavior known as upcasting).\na = np.ones(3, dtype=np.int32) b = np.linspace(0, pi, 3) print(\u0026#34;b.dtype.name = {}\u0026#34;.format(b.dtype.name)) c = a + b print(\u0026#34;c =\\n{}\u0026#34;.format(c)) print(\u0026#34;c.dtype.name = {}\u0026#34;.format(c.dtype.name)) d = np.exp(c*1j) print(\u0026#34;d =\\n{}\u0026#34;.format(d)) print(\u0026#34;d.dtype.name = {}\u0026#34;.format(d.dtype.name)) b.dtype.name = float64 c = [1. 2.57079633 4.14159265] c.dtype.name = float64 d = [ 0.54030231+0.84147098j -0.84147098+0.54030231j -0.54030231-0.84147098j] d.dtype.name = complex128 Many unary operations, such as computing the sum of all the elements in the array, are implemented as methods of the ndarray class.\na = rg.random((2, 3)) print(\u0026#34;a =\\n{}\u0026#34;.format(a)) print(\u0026#34;a.sum() = {}\u0026#34;.format(a.sum())) print(\u0026#34;a.min() = {}\u0026#34;.format(a.min())) print(\u0026#34;a.max() = {}\u0026#34;.format(a.max())) a = [[0.82770259 0.40919914 0.54959369] [0.02755911 0.75351311 0.53814331]] a.sum() = 3.1057109529998157 a.min() = 0.027559113243068367 a.max() = 0.8277025938204418 By default, these operations apply to the array as though it were a list of numbers, regardless of its shape. However, by specifying the axis parameter you can apply an operation along the specified axis of an array:\nb = np.arange(12).reshape(3, 4) print(\u0026#34;b =\\n{}\u0026#34;.format(b)) print(\u0026#34;b.sum(axis=0) = {}\u0026#34;.format(b.sum(axis=0))) # sum of each column print(\u0026#34;b.min(axis=1) = {}\u0026#34;.format(b.min(axis=1))) # min of each row print(\u0026#34;b.cumsum(axis=1) =\\n{}\u0026#34;.format(b.cumsum(axis=1))) # cumulative sum along each row b = [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] b.sum(axis=0) = [12 15 18 21] b.min(axis=1) = [0 4 8] b.cumsum(axis=1) = [[ 0 1 3 6] [ 4 9 15 22] [ 8 17 27 38]] Universal Functions NumPy provides familiar mathematical functions such as sin, cos, and exp. In NumPy, these are called “universal functions”(ufunc). Within NumPy, these functions operate elementwise on an array, producing an array as output.\nB = np.arange(3) print(\u0026#34;B = {}\u0026#34;.format(B)) print(\u0026#34;np.exp(B) = {}\u0026#34;.format(np.exp(B))) print(\u0026#34;np.sqrt(B) = {}\u0026#34;.format(np.sqrt(B))) C = np.array([2., -1., 4.]) print(\u0026#34;np.add(B, C) = {}\u0026#34;.format(np.add(B, C))) B = [0 1 2] np.exp(B) = [1. 2.71828183 7.3890561 ] np.sqrt(B) = [0. 1. 1.41421356] np.add(B, C) = [2. 0. 6.] Indexing, Slicing and Iterating One-dimensional arrays can be indexed, sliced and iterated over, much like lists and other Python sequences.\na = np.arange(10)**3 print(\u0026#34;a = {}\u0026#34;.format(a)) print(\u0026#34;a[2] = {}\u0026#34;.format(a[2])) print(\u0026#34;a[2:5] = {}\u0026#34;.format(a[2:5])) a[:6:2] = -1000 # equivalent to a[0:6:2] = -1000 # from start to position 6, exclusive, set every 2nd element to -1000 print(\u0026#34;a = {}\u0026#34;.format(a)) print(\u0026#34;a[ : :-1] = {}\u0026#34;.format(a[ : :-1])) # reversed a for i in a: print(i**(1/3.)) a = [ 0 1 8 27 64 125 216 343 512 729] a[2] = 8 a[2:5] = [ 8 27 64] a = [-1000 1 -1000 27 -1000 125 216 343 512 729] a[ : :-1] = [ 729 512 343 216 125 -1000 27 -1000 1 -1000] nan 1.0 nan 3.0 nan 4.999999999999999 5.999999999999999 6.999999999999999 7.999999999999999 8.999999999999998 /tmp/ipykernel_15700/4153428083.py:14: RuntimeWarning: invalid value encountered in power print(i**(1/3.)) Multidimensional arrays can have one index per axis. These indices are given in a tuple separated by commas:\ndef f(x, y): return 10 * x + y b = np.fromfunction(f, (5, 4), dtype=int) print(\u0026#34;b =\\n{}\u0026#34;.format(b)) print(\u0026#34;b[2, 3] = {}\u0026#34;.format(b[2, 3])) print(\u0026#34;b[0:5, 1] = {}\u0026#34;.format(b[0:5, 1])) # each row in the second column of b print(\u0026#34;b[ : , 1] = {}\u0026#34;.format(b[ : , 1])) # equivalent to the previous example print(\u0026#34;b[1:3, : ] =\\n{}\u0026#34;.format(b[1:3, : ])) # each column in the second and third row of b b = [[ 0 1 2 3] [10 11 12 13] [20 21 22 23] [30 31 32 33] [40 41 42 43]] b[2, 3] = 23 b[0:5, 1] = [ 1 11 21 31 41] b[ : , 1] = [ 1 11 21 31 41] b[1:3, : ] = [[10 11 12 13] [20 21 22 23]] When fewer indices are provided than the number of axes, the missing indices are considered complete slices:\nprint(\u0026#34;b[-1] = {}\u0026#34;.format(b[-1])) # the last row. Equivalent to b[-1, : ] b[-1] = [40 41 42 43] The expression within brackets in b[i] is treated as an i followed by as many instances of : as needed to represent the remaining axes. NumPy also allows you to write this using dots as b[i,...].\nThe dots (...) represent as many colons as needed to produce a complete indexing tuple. For example, if x is an array with 5 axes, then\nx[1,2,...] is equivalent to x[1,2,:,:,:], x[...,3] to x[:,:,:,:,3] and x[4,...,5,:] to x[4,:,:,5,:]. c = np.array([[[ 0, 1, 2], # a 3D array (two stacked 2D arrays) [ 10, 12, 13]], [[100,101,102], [110,112,113]]]) print(\u0026#34;c.shape = {}\u0026#34;.format(c.shape)) print(\u0026#34;c[1,...] =\\n{}\u0026#34;.format(c[1,...])) # same as c[1,:,:] or c[1] print(\u0026#34;c[...,2] =\\n{}\u0026#34;.format(c[...,2])) # same as c[:,:,2] c.shape = (2, 2, 3) c[1,...] = [[100 101 102] [110 112 113]] c[...,2] = [[ 2 13] [102 113]] Iterating over multidimensional arrays is done with respect to the first axis:\nfor row in b: print(row) [0 1 2 3] [10 11 12 13] [20 21 22 23] [30 31 32 33] [40 41 42 43] ","date":1694322000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694322000,"objectID":"731b4582036527f840208a4d0fa0a672","permalink":"https://ajdillhoff.github.io/notes/numpy_basics/","publishdate":"2023-09-10T00:00:00-05:00","relpermalink":"/notes/numpy_basics/","section":"notes","summary":"NumPy Quickstart This notebook is a quick introduction to NumPy. It is an interactive version of the NumPy Quickstart Tutorial.\nAll credits go to the original authors of the tutorial © Copyright 2008-2023, NumPy Developers.\nThe Basics NumPy’s main object is the homogeneous multidimensional array. It is a table of elements (usually numbers), all of the same type, indexed by a tuple of non-negative integers. In NumPy dimensions are called axes.","tags":["python"],"title":"NumPy: Basics","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" NumPy Quickstart This notebook is a quick introduction to NumPy. It is an interactive version of the NumPy Quickstart Tutorial.\nAll credits go to the original authors of the tutorial © Copyright 2008-2023, NumPy Developers.\nCopies and Views When operating and manipulating arrays, their data is sometimes copied into a new array and sometimes not. This is often a source of confusion for beginners. There are three cases:\nNo Copy at All Simple assignments make no copy of array objects or of their data.\nimport numpy as np a = np.array([[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]) b = a # no new object is created print(b is a) # a and b are two names for the same ndarray object True Python passes mutable objects as references, so function calls make no copy.\ndef f(x): print(id(x)) print(id(a)) # id is a unique identifier of an object f(a) # a is passed to the function under the name x 139846954913200 139846954913200 View or Shallow Copy Different array objects can share the same data. The view method creates a new array object that looks at the same data.\nc = a.view() # c is a view of the data owned by a print(\u0026#34;c is a = {}\u0026#34;.format(c is a)) print(\u0026#34;c.base is a = {}\u0026#34;.format(c.base is a)) # c is a view of the data owned by a print(\u0026#34;c.flags.owndata = {}\u0026#34;.format(c.flags.owndata)) # c does not own the data c = c.reshape((2, 6)) # a\u0026#39;s shape doesn\u0026#39;t change print(\u0026#34;a.shape = {}\u0026#34;.format(a.shape)) c[0, 4] = 1234 # a\u0026#39;s data changes print(\u0026#34;a =\\n{}\u0026#34;.format(a)) c is a = False c.base is a = True c.flags.owndata = False a.shape = (3, 4) a = [[ 0 1 2 3] [1234 5 6 7] [ 8 9 10 11]] Slicing an array returns a view of it:\ns = a[:, 1:3] s[:] = 10 # s[:] is a view of s. Note the difference between s=10 and s[:]=10 print(\u0026#34;a =\\n{}\u0026#34;.format(a)) a = [[ 0 10 10 3] [1234 10 10 7] [ 8 10 10 11]] Deep Copy The copy method makes a complete copy of the array and its data.\nd = a.copy() # a new array object with new data is created print(\u0026#34;d is a = {}\u0026#34;.format(d is a)) print(\u0026#34;d.base is a = {}\u0026#34;.format(d.base is a)) # d doesn\u0026#39;t share anything with a d[0, 0] = 9999 print(\u0026#34;a =\\n{}\u0026#34;.format(a)) d is a = False d.base is a = False a = [[ 0 10 10 3] [1234 10 10 7] [ 8 10 10 11]] Sometimes copy should be called after slicing if the original array is not required anymore. For example, suppose a is a huge intermediate result and the final result b only contains a small fraction of a, a deep copy should be made when constructing b with slicing:\na = np.arange(int(1e8)) b = a[:100].copy() del a # the memory of ``a`` can be released. If b = a[:100] is used instead, a is referenced by b and will persist in memory even if del a is executed.\nFunctions and Methods Overview See Routines for the full list of routines available in NumPy.\n","date":1694322000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694322000,"objectID":"ee88950d9dda019a9121368dfb64ca91","permalink":"https://ajdillhoff.github.io/notes/numpy_copies_and_views/","publishdate":"2023-09-10T00:00:00-05:00","relpermalink":"/notes/numpy_copies_and_views/","section":"notes","summary":"NumPy Quickstart This notebook is a quick introduction to NumPy. It is an interactive version of the NumPy Quickstart Tutorial.\nAll credits go to the original authors of the tutorial © Copyright 2008-2023, NumPy Developers.\nCopies and Views When operating and manipulating arrays, their data is sometimes copied into a new array and sometimes not. This is often a source of confusion for beginners. There are three cases:\nNo Copy at All Simple assignments make no copy of array objects or of their data.","tags":["python"],"title":"NumPy: Copies and Views","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" # `NumPy` Quickstart This notebook is a quick introduction to NumPy. It is an interactive version of the NumPy Quickstart Tutorial.\nAll credits go to the original authors of the tutorial © Copyright 2008-2023, NumPy Developers.\nShape Manipulation Changing the shape of an array An array has a shape given by the number of elements along each axis:\nimport numpy as np a = np.floor(10 * np.random.random((3,4))) print(\u0026#34;a = \\n{}\u0026#34;.format(a)) print(\u0026#34;a.shape = {}\u0026#34;.format(a.shape)) a = [[6. 7. 2. 1.] [5. 9. 4. 9.] [3. 5. 3. 2.]] a.shape = (3, 4) The shape of an array can be changed with various commands. Note that the following three commands all return a modified array, but do not change the original array:\nprint(\u0026#34;a.ravel() = {}\u0026#34;.format(a.ravel())) # flatten the array print(\u0026#34;a.reshape(6,2) = \\n{}\u0026#34;.format(a.reshape(6,2))) # reshape the array print(\u0026#34;a.T = \\n{}\u0026#34;.format(a.T)) # transpose the array print(\u0026#34;a.T.shape = {}\u0026#34;.format(a.T.shape)) a.ravel() = [6. 7. 2. 1. 5. 9. 4. 9. 3. 5. 3. 2.] a.reshape(6,2) = [[6. 7.] [2. 1.] [5. 9.] [4. 9.] [3. 5.] [3. 2.]] a.T = [[6. 5. 3.] [7. 9. 5.] [2. 4. 3.] [1. 9. 2.]] a.T.shape = (4, 3) The order of the elements in the array resulting from ravel is normally “C-style”, that is, the rightmost index “changes the fastest”, so the element after a[0,0] is a[0,1]. If the array is reshaped to some other shape, again the array is treated as “C-style”. NumPy normally creates arrays stored in this order, so ravel will usually not need to copy its argument, but if the array was made by taking slices of another array or created with unusual options, it may need to be copied. The functions ravel and reshape can also be instructed, using an optional argument, to use FORTRAN-style arrays, in which the leftmost index changes the fastest.\nThe reshape function returns its argument with a modified shape, whereas the ndarray.resize method modifies the array itself:\na.resize((2,6)) # resize the array print(\u0026#34;a.resize((2,6)) = \\n{}\u0026#34;.format(a)) a.resize((2,6)) = [[6. 7. 2. 1. 5. 9.] [4. 9. 3. 5. 3. 2.]] If a dimension is given as -1 in a reshaping operation, the other dimensions are automatically calculated:\nprint(\u0026#34;a.reshape(3, -1) = \\n{}\u0026#34;.format(a.reshape(3, -1))) a.reshape(3, -1) = [[6. 7. 2. 1.] [5. 9. 4. 9.] [3. 5. 3. 2.]] Stacking together different arrays Several arrays can be stacked together along different axes:\na = np.floor(10 * np.random.random((2,2))) print(\u0026#34;a = \\n{}\u0026#34;.format(a)) b = np.floor(10 * np.random.random((2,2))) print(\u0026#34;b = \\n{}\u0026#34;.format(b)) print(\u0026#34;np.vstack((a,b)) = \\n{}\u0026#34;.format(np.vstack((a,b)))) print(\u0026#34;np.hstack((a,b)) = \\n{}\u0026#34;.format(np.hstack((a,b)))) a = [[8. 0.] [5. 1.]] b = [[7. 7.] [0. 1.]] np.vstack((a,b)) = [[8. 0.] [5. 1.] [7. 7.] [0. 1.]] np.hstack((a,b)) = [[8. 0. 7. 7.] [5. 1. 0. 1.]] The function column_stack stacks 1D arrays as columns into a 2D array. It is equivalent to hstack only for 2D arrays:\nfrom numpy import newaxis print(\u0026#34;np.column_stack((a,b)) = \\n{}\u0026#34;.format(np.column_stack((a,b)))) # with 2D arrays a = np.array([4.,2.]) b = np.array([3.,8.]) print(\u0026#34;np.column_stack((a,b)) = \\n{}\u0026#34;.format(np.column_stack((a,b)))) # returns a 2D array print(\u0026#34;np.hstack((a,b)) = \\n{}\u0026#34;.format(np.hstack((a,b)))) # returns a 1D array print(\u0026#34;a[:,newaxis] = \\n{}\u0026#34;.format(a[:,newaxis])) # view a as a 2D column vector print(\u0026#34;np.column_stack((a[:,newaxis],b[:,newaxis])) = \\n{}\u0026#34;.format(np.column_stack((a[:,newaxis],b[:,newaxis])))) print(\u0026#34;np.hstack((a[:,newaxis],b[:,newaxis])) = \\n{}\u0026#34;.format(np.hstack((a[:,newaxis],b[:,newaxis])))) np.column_stack((a,b)) = [[4. 3.] [2. 8.]] np.column_stack((a,b)) = [[4. 3.] [2. 8.]] np.hstack((a,b)) = [4. 2. 3. 8.] a[:,newaxis] = [[4.] [2.]] np.column_stack((a[:,newaxis],b[:,newaxis])) = [[4. 3.] [2. 8.]] np.hstack((a[:,newaxis],b[:,newaxis])) = [[4. 3.] [2. 8.]] In general, for arrays of with more than two dimensions, hstack stacks along their second axes, vstack stacks along their first axes, and concatenate allows for an optional arguments giving the number of the axis along which the concatenation should happen.\nNote\nIn complex cases, r_ and c_ are useful for creating arrays by stacking numbers along one axis. They allow the use of range literals :\nprint(\u0026#34;np.r_[1:4,0,4] = \\n{}\u0026#34;.format(np.r_[1:4,0,4])) # concatenate along the first axis np.r_[1:4,0,4] = [1 2 3 0 4] When used with arrays as arguments, r_ and c_ are similar to vstack and hstack in their default behavior, but allow for an optional argument giving the number of the axis along which to concatenate.\nSplitting one array into several smaller ones Using hsplit, you can split an array along its horizontal axis, either by specifying the number of equally shaped arrays to return, or by specifying the columns after which the division should occur:\na = np.floor(10 * np.random.random((2,12))) print(\u0026#34;a = \\n{}\u0026#34;.format(a)) # split a into 3 print(\u0026#34;np.hsplit(a,3) = \\n{}\u0026#34;.format(np.hsplit(a,3))) # split a after the third and the fourth column print(\u0026#34;np.hsplit(a,(3,4)) = \\n{}\u0026#34;.format(np.hsplit(a,(3,4)))) a = [[7. 0. 8. 1. 4. 7. 8. 1. 1. 8. 1. 2.] [6. 7. 7. 5. 7. 5. 0. 7. 7. 4. 6. 1.]] np.hsplit(a,3) = [array([[7., 0., 8., 1.], [6., 7., 7., 5.]]), array([[4., 7., 8., 1.], [7., 5., 0., 7.]]), array([[1., 8., 1., 2.], [7., 4., 6., 1.]])] np.hsplit(a,(3,4)) = [array([[7., 0., 8.], [6., 7., 7.]]), array([[1.], [5.]]), array([[4., 7., 8., 1., 1., 8., 1., 2.], [7., 5., 0., 7., 7., 4., 6., 1.]])] vsplit splits along the vertical axis, and array_split allows one to specify along which axis to split.\n","date":1694322000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694322000,"objectID":"9a635cd092ed235fef43b647c1c954e6","permalink":"https://ajdillhoff.github.io/notes/numpy_shape_manipulation/","publishdate":"2023-09-10T00:00:00-05:00","relpermalink":"/notes/numpy_shape_manipulation/","section":"notes","summary":"# `NumPy` Quickstart This notebook is a quick introduction to NumPy. It is an interactive version of the NumPy Quickstart Tutorial.\nAll credits go to the original authors of the tutorial © Copyright 2008-2023, NumPy Developers.\nShape Manipulation Changing the shape of an array An array has a shape given by the number of elements along each axis:\nimport numpy as np a = np.floor(10 * np.random.random((3,4))) print(\u0026#34;a = \\n{}\u0026#34;.","tags":["python"],"title":"NumPy: Shape Manipulation","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Loading Data for ML Applications In this notebook, we will implement code to load data for ML applications. Following the approach used by PyTorch, we will implement a Dataset class and a DataLoader class. The Dataset class will be used to load the data and the DataLoader class will be used to iterate over the data in batches.\nWe will test it on a simple image dataset.\nThe Dataset Class First, we need some way of representing how each individual item will be pre-processed as the dataloader iterates over the data. We will do this by creating a Dataset class. Since this class represents multiple items in our dataset, we will need to define the following special methods:\n__len__: returns the length of the dataset __getitem__: returns the item at a given index The Intel Image Classification Dataset We will use the Intel Image Classification Dataset from Kaggle. This dataset contains images of natural scenes around the world. The dataset contains 25,000 images of size 150x150 distributed under 6 categories. The dataset is divided into a training set and a test set. The training set contains 14k images. The images are organized under folder representing each category. When initializing our dataset, it should iterate through the folders to enumerate all the images and their corresponding labels.\nimport os class IntelDataset: def __init__(self, data_path): self.data_path = data_path self.classes, self.class_to_idx = self._find_classes(self.data_path) self.samples = self._make_dataset(self.data_path, self.class_to_idx) def __len__(self): return len(self.samples) def __getitem__(self, idx): raise NotImplementedError def _find_classes(self, path): \u0026#34;\u0026#34;\u0026#34;Summarized from torchvision.datasets.ImageFolder\u0026#34;\u0026#34;\u0026#34; classes = [d.name for d in os.scandir(path) if d.is_dir()] classes.sort() class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)} return classes, class_to_idx def _make_dataset(self, dir, class_to_idx): \u0026#34;\u0026#34;\u0026#34;Summarized from torchvision.datasets.ImageFolder\u0026#34;\u0026#34;\u0026#34; images = [] dir = os.path.expanduser(dir) for target in sorted(os.listdir(dir)): d = os.path.join(dir, target) if not os.path.isdir(d): continue for root, _, fnames in sorted(os.walk(d)): for fname in sorted(fnames): path = os.path.join(root, fname) item = (path, class_to_idx[target]) images.append(item) return images Here is the first iteration of our dataset. Let\u0026rsquo;s test it on the Intel Image Classification dataset available here: https://www.kaggle.com/datasets/puneet6060/intel-image-classification.\nWe will use the training samples to test our Dataset class. There are 14,000 images in the training set over 6 categories. Each category is contained in its own subfolder.\ndataset = IntelDataset(data_path=\u0026#39;data/seg_train\u0026#39;) print(dataset.classes) print(dataset.class_to_idx) print(len(dataset)) # Sample the first 5 items for i in range(5): print(dataset.samples[i]) ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street'] {'buildings': 0, 'forest': 1, 'glacier': 2, 'mountain': 3, 'sea': 4, 'street': 5} 14034 ('data/seg_train/buildings/0.jpg', 0) ('data/seg_train/buildings/10006.jpg', 0) ('data/seg_train/buildings/1001.jpg', 0) ('data/seg_train/buildings/10014.jpg', 0) ('data/seg_train/buildings/10018.jpg', 0) Loading the Images Now that we can successfully initialize the dataset, we can implement __getitem__ to load the actual image and return a tuple of the image and its label. We will use the PIL library to load the image and convert it to a numpy array.\nimport os import numpy as np from PIL import Image import matplotlib.pyplot as plt class IntelDataset: def __init__(self, data_path): self.data_path = data_path self.classes, self.class_to_idx = self._find_classes(self.data_path) self.samples = self._make_dataset(self.data_path, self.class_to_idx) def __len__(self): return len(self.samples) def __getitem__(self, idx): path, target = self.samples[idx] image = Image.open(path).convert(\u0026#39;RGB\u0026#39;) image = np.array(image) return image, target def _find_classes(self, path): \u0026#34;\u0026#34;\u0026#34;Summarized from torchvision.datasets.ImageFolder\u0026#34;\u0026#34;\u0026#34; classes = [d.name for d in os.scandir(path) if d.is_dir()] classes.sort() class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)} return classes, class_to_idx def _make_dataset(self, dir, class_to_idx): \u0026#34;\u0026#34;\u0026#34;Summarized from torchvision.datasets.ImageFolder\u0026#34;\u0026#34;\u0026#34; images = [] dir = os.path.expanduser(dir) for target in sorted(os.listdir(dir)): d = os.path.join(dir, target) if not os.path.isdir(d): continue for root, _, fnames in sorted(os.walk(d)): for fname in sorted(fnames): path = os.path.join(root, fname) item = (path, class_to_idx[target]) images.append(item) return images # Sample 9 random images and display them with their labels dataset = IntelDataset(data_path=\u0026#39;data/seg_train\u0026#39;) fig, axes = plt.subplots(3, 3, figsize=(10, 10)) for i in range(3): for j in range(3): idx = np.random.randint(len(dataset)) image, target = dataset[idx] axes[i, j].imshow(image) axes[i, j].set_title(dataset.classes[target]) axes[i, j].axis(\u0026#39;off\u0026#39;) plt.show() The DataLoader Class Now that we have a Dataset to manage loading each individual item, we need to create a DataLoader that is responsible for iterating over the dataset in batches.\nclass DataLoader: def __init__(self, dataset, batch_size, shuffle=False): self.dataset = dataset self.batch_size = batch_size self.shuffle = shuffle def __len__(self): return len(self.dataset) // self.batch_size def __iter__(self): if self.shuffle: indices = np.random.permutation(len(self.dataset)) else: indices = np.arange(len(self.dataset)) for i in range(len(self)): batch_indices = indices[i*self.batch_size:(i+1)*self.batch_size] batch = [self.dataset[idx] for idx in batch_indices] images, targets = zip(*batch) images = np.stack(images, axis=0) targets = np.stack(targets, axis=0) yield images, targets In this example, the __iter__ uses Python generators to yield a batch of data instead of overriding the __next__ method.\nLet\u0026rsquo;s test this with our dataset.\ndataset = IntelDataset(data_path=\u0026#39;data/seg_train\u0026#39;) dataloader = DataLoader(dataset, batch_size=32, shuffle=False) # Print the first 5 batches for i, (images, targets) in enumerate(dataloader): print(f\u0026#39;Batch {i} images shape: {images.shape}\u0026#39;) print(f\u0026#39;Batch {i} targets shape: {targets.shape}\u0026#39;) if i == 5: break Batch 0 images shape: (32, 150, 150, 3) Batch 0 targets shape: (32,) Batch 1 images shape: (32, 150, 150, 3) Batch 1 targets shape: (32,) Batch 2 images shape: (32, 150, 150, 3) Batch 2 targets shape: (32,) Batch 3 images shape: (32, 150, 150, 3) Batch 3 targets shape: (32,) Batch 4 images shape: (32, 150, 150, 3) Batch 4 targets shape: (32,) Batch 5 images shape: (32, 150, 150, 3) Batch 5 targets shape: (32,) Cleaning the Data Our data loader failed because there are some images that are not $150 \\times 150$. We need to figure out exactly what to do with those images. We can either remove them or resize them so that they are all the same size. We will resize them to $150 \\times 150$. This will be easiest to do in the __getitem__ method of our Dataset class.\nimport os import numpy as np from PIL import Image import matplotlib.pyplot as plt class IntelDataset: def __init__(self, data_path): self.data_path = data_path self.classes, self.class_to_idx = self._find_classes(self.data_path) self.samples = self._make_dataset(self.data_path, self.class_to_idx) def __len__(self): return len(self.samples) def __getitem__(self, idx): path, target = self.samples[idx] image = Image.open(path).convert(\u0026#39;RGB\u0026#39;) # Resize if the image is not 150x150 if image.size != (150, 150): image = image.resize((150, 150)) image = np.array(image) return image, target def _find_classes(self, path): \u0026#34;\u0026#34;\u0026#34;Summarized from torchvision.datasets.ImageFolder\u0026#34;\u0026#34;\u0026#34; classes = [d.name for d in os.scandir(path) if d.is_dir()] classes.sort() class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)} return classes, class_to_idx def _make_dataset(self, dir, class_to_idx): \u0026#34;\u0026#34;\u0026#34;Summarized from torchvision.datasets.ImageFolder\u0026#34;\u0026#34;\u0026#34; images = [] dir = os.path.expanduser(dir) for target in sorted(os.listdir(dir)): d = os.path.join(dir, target) if not os.path.isdir(d): continue for root, _, fnames in sorted(os.walk(d)): for fname in sorted(fnames): path = os.path.join(root, fname) item = (path, class_to_idx[target]) images.append(item) return images dataset = IntelDataset(data_path=\u0026#39;data/seg_train\u0026#39;) dataloader = DataLoader(dataset, batch_size=32, shuffle=False) # Verify that our fix works! # Only print the first 5 batches for i, (images, targets) in enumerate(dataloader): print(f\u0026#39;Batch {i} images shape: {images.shape}\u0026#39;) print(f\u0026#39;Batch {i} targets shape: {targets.shape}\u0026#39;) if i == 5: break Batch 0 images shape: (32, 150, 150, 3) Batch 0 targets shape: (32,) Batch 1 images shape: (32, 150, 150, 3) Batch 1 targets shape: (32,) Batch 2 images shape: (32, 150, 150, 3) Batch 2 targets shape: (32,) Batch 3 images shape: (32, 150, 150, 3) Batch 3 targets shape: (32,) Batch 4 images shape: (32, 150, 150, 3) Batch 4 targets shape: (32,) Batch 5 images shape: (32, 150, 150, 3) Batch 5 targets shape: (32,) ","date":1693803600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693803600,"objectID":"2c3a8997dfc68ddc5c5418e348822690","permalink":"https://ajdillhoff.github.io/notes/dataloader/","publishdate":"2023-09-04T00:00:00-05:00","relpermalink":"/notes/dataloader/","section":"notes","summary":"Loading Data for ML Applications In this notebook, we will implement code to load data for ML applications. Following the approach used by PyTorch, we will implement a Dataset class and a DataLoader class. The Dataset class will be used to load the data and the DataLoader class will be used to iterate over the data in batches.\nWe will test it on a simple image dataset.\nThe Dataset Class First, we need some way of representing how each individual item will be pre-processed as the dataloader iterates over the data.","tags":["python","examples"],"title":"Example: Writing a Data Loader in Python","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Classes When a class is defined, a namespace is created for it. All assignments to local variables are part of this namespace. The code below defines a class, creates an instance of the class, and calls a method on the instance.\nclass Shape(): \u0026#34;\u0026#34;\u0026#34;Represents any shape.\u0026#34;\u0026#34;\u0026#34; def __init__(self, color): self.color = color self.orientation = 0.0 def rotate(self, angle): self.orientation += angle s = Shape(\u0026#34;red\u0026#34;) s.rotate(45.0) print(s.orientation) 45.0 Class and Instance Variables The class above has two instance variables, color and orientation. These variables are accessed using the self keyword. The self keyword is used to access instance variables and methods.\nClasses can also have class variables that are accessible, and shared, by all instances of the class. Let\u0026rsquo;s add a class variable to the Shape class.\nPrivate Variables Python does not have a formal mechanism for describing a private variable. You can still create them using naming conventions. A common approach to creating private variables is to prefix each identifier with double underscores. If we wanted to make the orientation variable private, we would rename it to __orientation, for example.\nclass Shape(): \u0026#34;\u0026#34;\u0026#34;Represents any shape.\u0026#34;\u0026#34;\u0026#34; max_area = 100.0 def __init__(self, color): self.color = color self.orientation = 0.0 def rotate(self, angle): self.orientation += angle s = Shape(\u0026#34;red\u0026#34;) s.rotate(45.0) r = Shape(\u0026#34;blue\u0026#34;) print(s.orientation) print(\u0026#34;Maximum area for a shape:\u0026#34;, Shape.max_area) 45.0 Maximum area for a shape: 100.0 Special Methods We already saw one special method, __init__(), that serves as our constructor for a class. There are several others that are useful for customizing our classes. They are\n__str__(): called when str() is called on an instance of the class __repr__(): called when repr() is called on an instance of the class __len__(): called when len() is called on an instance of the class __add__(): called when + is used on two instances of the class __eq__(): called when == is used on two instances of the class __lt__(): called when \u0026lt; is used on two instances of the class __gt__(): called when \u0026gt; is used on two instances of the class __le__(): called when \u0026lt;= is used on two instances of the class __ge__(): called when \u0026gt;= is used on two instances of the class __ne__(): called when != is used on two instances of the class __hash__(): called when hash() is called on an instance of the class __bool__(): called when bool() is called on an instance of the class Let\u0026rsquo;s modify the Shape class to add a few of these methods. We will also add an area attribute so that we can override the comparison operators.\nclass Shape(): \u0026#34;\u0026#34;\u0026#34;Represents any shape.\u0026#34;\u0026#34;\u0026#34; max_area = 100.0 def __init__(self, color, area): self.color = color self.orientation = 0.0 self.area = area def rotate(self, angle): self.orientation += angle def __eq__(self, other): return self.area == other.area def __lt__(self, other): return self.area \u0026lt; other.area def __gt__(self, other): return self.area \u0026gt; other.area def __le__(self, other): return self.area \u0026lt;= other.area def __ge__(self, other): return self.area \u0026gt;= other.area def __ne__(self, other): return self.area != other.area def __str__(self): return \u0026#34;Shape, color: {0}, area: {1}\u0026#34;.format(self.color, self.area) s1 = Shape(\u0026#34;red\u0026#34;, 10.0) s2 = Shape(\u0026#34;blue\u0026#34;, 20.0) print(\u0026#34;s1 == s2:\u0026#34;, s1 == s2) print(\u0026#34;s1 != s2:\u0026#34;, s1 != s2) print(\u0026#34;s1 \u0026lt; s2:\u0026#34;, s1 \u0026lt; s2) print(\u0026#34;s1 \u0026gt; s2:\u0026#34;, s1 \u0026gt; s2) print(\u0026#34;s1 \u0026lt;= s2:\u0026#34;, s1 \u0026lt;= s2) print(\u0026#34;s1 \u0026gt;= s2:\u0026#34;, s1 \u0026gt;= s2) print(s1) print(s2) s1 == s2: False s1 != s2: True s1 \u0026lt; s2: True s1 \u0026gt; s2: False s1 \u0026lt;= s2: True s1 \u0026gt;= s2: False Shape, color: red, area: 10.0 Shape, color: blue, area: 20.0 Since we have defined the \u0026lt; operator, list.sort() can sort our shapes. If the __lt__() operator was not defined, list.sort() would use the __gt__() operator. If neither are defined, attemping to sort would result in an error. Let\u0026rsquo;s add a few more and verify this.\nimport random colors = [\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;green\u0026#34;, \u0026#34;yellow\u0026#34;, \u0026#34;black\u0026#34;, \u0026#34;white\u0026#34;] # Generate 10 shapes with random colors and areas shapes = [] for i in range(10): color = random.choice(colors) area = random.uniform(0.0, 100.0) shapes.append(Shape(color, area)) # Print the shapes, sorted by area for shape in sorted(shapes): print(shape) Shape, color: red, area: 13.697816464863 Shape, color: white, area: 42.56718610585648 Shape, color: white, area: 47.443134198872464 Shape, color: white, area: 53.85070279838825 Shape, color: yellow, area: 66.78631236791435 Shape, color: green, area: 70.55065950752918 Shape, color: blue, area: 73.19818592952365 Shape, color: white, area: 74.03228452807117 Shape, color: black, area: 86.72544463003362 Shape, color: red, area: 94.59245601130148 Inheritance Inheritance allows us to create a specialized version of another class. Generally, this means that our specialized class has access to the methods and instance variables of the parent class. Let\u0026rsquo;s create a Circle and Square that inherit from shape. Their areas will be calculated based on their properties.\nimport math class Shape(): \u0026#34;\u0026#34;\u0026#34;Represents any shape.\u0026#34;\u0026#34;\u0026#34; max_area = 100.0 def __init__(self, color): self.color = color self.orientation = 0.0 def rotate(self, angle): self.orientation += angle def __eq__(self, other): return self.area == other.area def __lt__(self, other): return self.area \u0026lt; other.area def __gt__(self, other): return self.area \u0026gt; other.area def __le__(self, other): return self.area \u0026lt;= other.area def __ge__(self, other): return self.area \u0026gt;= other.area def __ne__(self, other): return self.area != other.area def __str__(self): return \u0026#34;Shape, color: {0}, area: {1}\u0026#34;.format(self.color, self.area) class Circle(Shape): \u0026#34;\u0026#34;\u0026#34;Represents a circle.\u0026#34;\u0026#34;\u0026#34; def __init__(self, color, radius): Shape.__init__(self, color) self.radius = radius self.area = self.get_area() def __str__(self): return \u0026#34;Circle, color: {0}, area: {1}, radius: {2}\u0026#34;.format(self.color, self.area, self.radius) def get_area(self): return 2 * math.pi * self.radius ** 2 class Rectangle(Shape): \u0026#34;\u0026#34;\u0026#34;Represents a rectangle.\u0026#34;\u0026#34;\u0026#34; def __init__(self, color, width, height): Shape.__init__(self, color) self.width = width self.height = height self.area = self.get_area() def __str__(self): return \u0026#34;Rectangle, color: {0}, area: {1}, width: {2}, height: {3}\u0026#34;.format(self.color, self.area, self.width, self.height) def get_area(self): return self.width * self.height shape_classes = [Rectangle, Circle] colors = [\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;green\u0026#34;, \u0026#34;yellow\u0026#34;, \u0026#34;black\u0026#34;, \u0026#34;white\u0026#34;] # Generate 10 shapes with random colors and areas shapes = [] for i in range(10): color = random.choice(colors) shape_class = random.choice(shape_classes) if shape_class == Rectangle: width = random.uniform(0.0, math.sqrt(Shape.max_area)) height = random.uniform(0.0, math.sqrt(Shape.max_area)) shape = Rectangle(color, width, height) else: radius = random.uniform(0.0, math.sqrt(Shape.max_area / (2 * math.pi))) shape = Circle(color, radius) shapes.append(shape) # Print the shapes, sorted by area for shape in sorted(shapes): print(shape) Circle, color: blue, area: 0.5110628296555956, radius: 0.2851984845159934 Rectangle, color: yellow, area: 0.6186484099066036, width: 0.2740689854907352, height: 2.25727259433927 Circle, color: white, area: 5.867493292628993, radius: 0.9663542627217231 Circle, color: green, area: 18.28116762721217, radius: 1.7057368476298893 Rectangle, color: blue, area: 20.114023003818147, width: 5.075190544004695, height: 3.963205485472614 Rectangle, color: blue, area: 23.171307446004665, width: 2.586024349725701, height: 8.960204666465124 Circle, color: red, area: 42.43901187799147, radius: 2.598918721375873 Rectangle, color: white, area: 45.198912747710224, width: 9.793600740960953, height: 4.615147578833736 Circle, color: yellow, area: 47.991651978311594, radius: 2.7637128359318064 Rectangle, color: green, area: 83.54726788281138, width: 9.643204828660805, height: 8.66384872739595 global and nonlocal keywords The global keyword is used to declare an identifier that can be used for the entire code block. This is useful when we want to use a variable in a function that is defined outside of the function.\nx = 1 def f(): global x # global keyword is used to access a global variable from a function x = 2 f() print(x) 2 The nonlocal keyword is used to declare an identifier that is defined in the nearest enclosing scope. This is useful when we want to use a variable in a nested function that is defined outside of the nested function.\ndef f(): x = 1 def g(): nonlocal x x = 2 g() print(x) f() 2 The following example from the official Python docs shows the relationship between global, local, and nonlocal variables.\ndef scope_test(): def do_local(): spam = \u0026#34;local spam\u0026#34; def do_nonlocal(): nonlocal spam spam = \u0026#34;nonlocal spam\u0026#34; def do_global(): global spam spam = \u0026#34;global spam\u0026#34; spam = \u0026#34;test spam\u0026#34; do_local() print(\u0026#34;After local assignment:\u0026#34;, spam) do_nonlocal() print(\u0026#34;After nonlocal assignment:\u0026#34;, spam) do_global() print(\u0026#34;After global assignment:\u0026#34;, spam) scope_test() print(\u0026#34;In global scope:\u0026#34;, spam) After local assignment: test spam After nonlocal assignment: nonlocal spam After global assignment: nonlocal spam In global scope: global spam The unexpected result here is that spam is still equal to nonlocal even though it was changed in do_global by declaring global spam. When declaring something as nonlocal, the variable must already exist in the enclosing namespace. The declaration of global spam created a new instance of spam in the global namespace.\nThe example below shows how local, nonlocal, and global variables work in the context of classes.\nclass User: \u0026#34;\u0026#34;\u0026#34;Represents a user.\u0026#34;\u0026#34;\u0026#34; def __init__(self, id, name, password): self.id = id self.name = name self.password = password self.domain = \u0026#34;unknown\u0026#34; def __str__(self): return \u0026#34;User: {0}, id: {1}\u0026#34;.format(self.name, self.id) def global_login(self): global domain self.domain = domain # def nonlocal_login(self): # nonlocal domain # domain = \u0026#34;compuserve.net\u0026#34; def nonlocal_login(self): domain = \u0026#34;compuserve.net\u0026#34; def set_domain(): nonlocal domain self.domain = domain set_domain() def local_login(self): self.domain = \u0026#34;tx.rr.com\u0026#34; domain = \u0026#34;gmail.com\u0026#34; u = User(1, \u0026#34;John\u0026#34;, \u0026#34;password\u0026#34;) u.global_login() print(u.domain) u.nonlocal_login() print(u.domain) u.local_login() print(u.domain) gmail.com compuserve.net tx.rr.com Data Classes Sometimes in our work, we may want to represent a simple class consisting only of attributes, similar to a struct in C. Python provides a way to do this using the dataclass decorator. The dataclass decorator will automatically generate a constructor, __repr__(), and __eq__() method for us. The follow example shows how to implement such a class.\nfrom dataclasses import dataclass @dataclass class Product: \u0026#34;\u0026#34;\u0026#34;Represents a product.\u0026#34;\u0026#34;\u0026#34; id: int name: str price: float quantity: int = 0 def __str__(self): return \u0026#34;Product: {0}, id: {1}\u0026#34;.format(self.name, self.id) # Let\u0026#39;s create a list of graphics cards and list them products = [] products.append(Product(1, \u0026#34;GeForce RTX 2080 Ti\u0026#34;, 1200.0)) products.append(Product(2, \u0026#34;GeForce RTX 2080\u0026#34;, 800.0)) products.append(Product(3, \u0026#34;GeForce RTX 2070\u0026#34;, 600.0)) products.append(Product(4, \u0026#34;GeForce RTX 2060\u0026#34;, 350.0)) products.append(Product(5, \u0026#34;GeForce GTX 1660 Ti\u0026#34;, 275.0)) products.append(Product(6, \u0026#34;GeForce GTX 1660\u0026#34;, 200.0)) products.append(Product(7, \u0026#34;GeForce GTX 1650\u0026#34;, 150.0)) products.append(Product(8, \u0026#34;GeForce GTX 1080 Ti\u0026#34;, 800.0)) products.append(Product(9, \u0026#34;GeForce GTX 1080\u0026#34;, 500.0)) products.append(Product(10, \u0026#34;GeForce GTX 1070 Ti\u0026#34;, 450.0)) for product in products: print(product) Product: GeForce RTX 2080 Ti, id: 1 Product: GeForce RTX 2080, id: 2 Product: GeForce RTX 2070, id: 3 Product: GeForce RTX 2060, id: 4 Product: GeForce GTX 1660 Ti, id: 5 Product: GeForce GTX 1660, id: 6 Product: GeForce GTX 1650, id: 7 Product: GeForce GTX 1080 Ti, id: 8 Product: GeForce GTX 1080, id: 9 Product: GeForce GTX 1070 Ti, id: 10 Iterators We have used iterator objects, like a list, in previous examples. When defining our own custom classes, we can also define them as iterators. To do this, we need to implement the __iter__() and __next__() methods. The __iter__() method should return the iterator object itself. The __next__() method should return the next item in the sequence. When there are no more items in the sequence, __next__() should raise a StopIteration exception.\nThis will be useful for our final example, where we will implement a dataloader for a machine learning application. To illusterate iterators with a simpler example, we need to justify the need to implement our own iterator. If we create something simple that iterates over a simple list of objects, why not just use the list itself?\nLet\u0026rsquo;s create a class that represents a 3D object. A 3D object has a list of vertices and a list of faces. Each face is a list of indices into the list of vertices. We will create a class that represents a 3D object. Our iterator for this class will iterator over the faces of the object, returning the vertices that make up each face.\nclass Model: \u0026#34;\u0026#34;\u0026#34;Represents a 3D model.\u0026#34;\u0026#34;\u0026#34; def __init__(self, vertices, faces): self.vertices = vertices self.faces = faces self.index = 0 def __str__(self): return \u0026#34;{} vertices, {} faces\u0026#34;.format(len(self.vertices), len(self.faces)) def __len__(self): return len(self.faces) def __iter__(self): return self def __next__(self): if self.index \u0026gt;= len(self.faces): raise StopIteration face = self.faces[self.index] vertices = [] for vertex_index in face: vertices.append(self.vertices[vertex_index]) self.index += 1 return vertices def __getitem__(self, key): vertices = [] for vertex_index in self.faces[key]: vertices.append(self.vertices[vertex_index]) return vertices # Create a cube model vertices = [(0.0, 0.0, 0.0), (1.0, 0.0, 0.0), (1.0, 1.0, 0.0), (0.0, 1.0, 0.0), (0.0, 0.0, 1.0), (1.0, 0.0, 1.0), (1.0, 1.0, 1.0), (0.0, 1.0, 1.0)] faces = [(0, 1, 2, 3), (1, 5, 6, 2), (5, 4, 7, 6), (4, 0, 3, 7), (3, 2, 6, 7), (4, 5, 1, 0)] cube = Model(vertices, faces) # Iterator over the model for face in cube: print(\u0026#34;Face {}: {}\u0026#34;.format(cube.index, face)) Face 1: [(0.0, 0.0, 0.0), (1.0, 0.0, 0.0), (1.0, 1.0, 0.0), (0.0, 1.0, 0.0)] Face 2: [(1.0, 0.0, 0.0), (1.0, 0.0, 1.0), (1.0, 1.0, 1.0), (1.0, 1.0, 0.0)] Face 3: [(1.0, 0.0, 1.0), (0.0, 0.0, 1.0), (0.0, 1.0, 1.0), (1.0, 1.0, 1.0)] Face 4: [(0.0, 0.0, 1.0), (0.0, 0.0, 0.0), (0.0, 1.0, 0.0), (0.0, 1.0, 1.0)] Face 5: [(0.0, 1.0, 0.0), (1.0, 1.0, 0.0), (1.0, 1.0, 1.0), (0.0, 1.0, 1.0)] Face 6: [(0.0, 0.0, 1.0), (1.0, 0.0, 1.0), (1.0, 0.0, 0.0), (0.0, 0.0, 0.0)] Generators Generators provide a much cleaner way to implement iterators. Instead of implementing the __iter__() and __next__() methods, we can use the yield keyword. The yield keyword is used to return a value from a generator. The generator will remember its place in the sequence and return the next value when next() is called on it.\nSince our class already included __getitem__(), we can use the yield keyword to implement our iterator.\ndef get_next_face(model): for face in range(len(model)): yield model[face] # Generator function for face in get_next_face(cube): print(face) [(0.0, 0.0, 0.0), (1.0, 0.0, 0.0), (1.0, 1.0, 0.0), (0.0, 1.0, 0.0)] [(1.0, 0.0, 0.0), (1.0, 0.0, 1.0), (1.0, 1.0, 1.0), (1.0, 1.0, 0.0)] [(1.0, 0.0, 1.0), (0.0, 0.0, 1.0), (0.0, 1.0, 1.0), (1.0, 1.0, 1.0)] [(0.0, 0.0, 1.0), (0.0, 0.0, 0.0), (0.0, 1.0, 0.0), (0.0, 1.0, 1.0)] [(0.0, 1.0, 0.0), (1.0, 1.0, 0.0), (1.0, 1.0, 1.0), (0.0, 1.0, 1.0)] [(0.0, 0.0, 1.0), (1.0, 0.0, 1.0), (1.0, 0.0, 0.0), (0.0, 0.0, 0.0)] ","date":1693803600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693803600,"objectID":"26577078c16291b5ad773dc839b174fc","permalink":"https://ajdillhoff.github.io/notes/oop/","publishdate":"2023-09-04T00:00:00-05:00","relpermalink":"/notes/oop/","section":"notes","summary":"Classes When a class is defined, a namespace is created for it. All assignments to local variables are part of this namespace. The code below defines a class, creates an instance of the class, and calls a method on the instance.\nclass Shape(): \u0026#34;\u0026#34;\u0026#34;Represents any shape.\u0026#34;\u0026#34;\u0026#34; def __init__(self, color): self.color = color self.orientation = 0.0 def rotate(self, angle): self.orientation += angle s = Shape(\u0026#34;red\u0026#34;) s.rotate(45.0) print(s.orientation) 45.0 Class and Instance Variables The class above has two instance variables, color and orientation.","tags":["python"],"title":"Object-Oriented Programming with Python","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" File I/O in Python This notebook will cover the following topics:\nOpening and closing files Reading and writing text files Reading and writing binary files Using the with statement Using the pickle module Serializing objects with pickle Reading and writing JSON files Opening and closing files Let\u0026rsquo;s start with the basics. To open a file, we use the built-in open() function. The open() function takes two arguments: the name of the file to open and the mode in which to open it. The mode can be one of the following:\n'r' - open for reading (default) 'w' - open for writing, truncating the file first 'x' - open for exclusive creation, failing if the file already exists 'a' - open for writing, appending to the end of the file if it exists 'b' - binary mode 't' - text mode (default) '+' - open a disk file for updating (reading and writing) To make this tutorial more interesting, let\u0026rsquo;s create a purpose for our code. Our goal is to create a logging system for players and their rolls. Every time a player rolls, we will add it to our log. We will also add a timestamp to each roll. We will store the log in a file called log.txt.\nimport random import datetime # First, we\u0026#39;ll open a file fp = open(\u0026#34;log.txt\u0026#34;, \u0026#34;w\u0026#34;) # Let\u0026#39;s create a list of players players = [\u0026#34;Naomi\u0026#34;, \u0026#34;James\u0026#34;, \u0026#34;Amos\u0026#34;, \u0026#34;Bobbie\u0026#34;] # Let\u0026#39;s roll a d20 for each player and write it in the log, including a timestamp for player in players: roll = random.randint(1, 20) timestamp = datetime.datetime.now() fp.write(f\u0026#34;{timestamp} - {player} rolled a {roll}\\n\u0026#34;) # Finally, let\u0026#39;s close the file fp.close() It isn\u0026rsquo;t recommended to open and close the file manually like this. Instead, we will use the with statement. This will ensure that the file is closed properly even if an exception is raised.\nAdditionally, it is possible that calling write without using with will not write to the file immediately. Instead, it will be buffered and written later. Using with will ensure that the file is written to immediately.\nimport datetime with open(\u0026#39;log.txt\u0026#39;, \u0026#39;a\u0026#39;) as f: f.write(f\u0026#39;{datetime.datetime.now()}: {player} rolled {roll}\\n\u0026#39;) Reading and writing binary files For the next example, we will write the user\u0026rsquo;s rolls as binary. Each user will get their own file so that we can easily read their rolls later.\nfor player in players: rolls = [random.randint(1, 20) for _ in range(5)] with open(\u0026#34;rolls/\u0026#34; + player + \u0026#34;.bin\u0026#34;, \u0026#34;wb\u0026#34;) as fp: fp.write(bytes(rolls)) # To verify, let\u0026#39;s open the files and print the contents for player in players: with open(\u0026#34;rolls/\u0026#34; + player + \u0026#34;.bin\u0026#34;, \u0026#34;rb\u0026#34;) as fp: print(player, list(fp.read())) Naomi [2, 13, 18, 8, 16] James [7, 10, 17, 15, 15] Amos [19, 9, 11, 6, 3] Bobbie [16, 5, 18, 13, 1] Reading from CSV files Python has a built-in module for reading and writing CSV files. CSV stands for comma-separated values. It is a common format for storing tabular data.\n# Load and parse CSV import csv # Store the data in a list of dictionaries data = [] with open(\u0026#34;../data/musicnet_metadata.csv\u0026#34;, \u0026#34;r\u0026#34;) as fp: reader = csv.reader(fp) keys = next(reader) for row in reader: data.append(dict(zip(keys, row))) # Let\u0026#39;s print the first 5 rows for row in data[:5]: print(row) # Count the number of Violin pieces are in the dataset count = 0 for row in data: if \u0026#34;Violin\u0026#34; in row[\u0026#34;ensemble\u0026#34;]: count += 1 print(f\u0026#34;There are {count} Violin pieces in the dataset\u0026#34;) {'id': '1727', 'composer': 'Schubert', 'composition': 'Piano Quintet in A major', 'movement': '2. Andante', 'ensemble': 'Piano Quintet', 'source': 'European Archive', 'transcriber': 'http://tirolmusic.blogspot.com/', 'catalog_name': 'OP114', 'seconds': '447'} {'id': '1728', 'composer': 'Schubert', 'composition': 'Piano Quintet in A major', 'movement': '3. Scherzo: Presto', 'ensemble': 'Piano Quintet', 'source': 'European Archive', 'transcriber': 'http://tirolmusic.blogspot.com/', 'catalog_name': 'OP114', 'seconds': '251'} {'id': '1729', 'composer': 'Schubert', 'composition': 'Piano Quintet in A major', 'movement': '4. Andantino - Allegretto', 'ensemble': 'Piano Quintet', 'source': 'European Archive', 'transcriber': 'http://tirolmusic.blogspot.com/', 'catalog_name': 'OP114', 'seconds': '444'} {'id': '1730', 'composer': 'Schubert', 'composition': 'Piano Quintet in A major', 'movement': '5. Allegro giusto', 'ensemble': 'Piano Quintet', 'source': 'European Archive', 'transcriber': 'http://tirolmusic.blogspot.com/', 'catalog_name': 'OP114', 'seconds': '368'} {'id': '1733', 'composer': 'Schubert', 'composition': 'Piano Sonata in A major', 'movement': '2. Andantino', 'ensemble': 'Solo Piano', 'source': 'Museopen', 'transcriber': 'Segundo G. Yogore', 'catalog_name': 'D959', 'seconds': '546'} There are 35 Violin pieces in the dataset Application: List all works in the dataset by Bach for Solo Violin Now that we have our data loaded. Let\u0026rsquo;s use list comprehensions to print out all the works by Bach for solo violin. Our formatted table should show the following columns:\nid composition name seconds # Filter all lines that are written by Bach bach = [row for row in data if \u0026#34;Bach\u0026#34; in row[\u0026#34;composer\u0026#34;] and \u0026#34;Solo Violin\u0026#34; in row[\u0026#34;ensemble\u0026#34;]] # Print the number of Bach pieces print(f\u0026#34;There are {len(bach)} Bach pieces in the dataset\u0026#34;) # Print them all out for row in bach: print(row) There are 9 Bach pieces in the dataset {'id': '2186', 'composer': 'Bach', 'composition': 'Violin Partita No 3 in E major', 'movement': '1. Preludio', 'ensemble': 'Solo Violin', 'source': 'Oliver Colbentston', 'transcriber': 'suzumidi', 'catalog_name': 'BWV1006', 'seconds': '214'} {'id': '2191', 'composer': 'Bach', 'composition': 'Violin Partita No 3 in E major', 'movement': '6. Bourree', 'ensemble': 'Solo Violin', 'source': 'Oliver Colbentston', 'transcriber': 'suzumidi', 'catalog_name': 'BWV1006', 'seconds': '102'} {'id': '2241', 'composer': 'Bach', 'composition': 'Violin Sonata No 1 in G minor', 'movement': '1. Adagio', 'ensemble': 'Solo Violin', 'source': 'European Archive', 'transcriber': 'suzumidi', 'catalog_name': 'BWV1001', 'seconds': '242'} {'id': '2242', 'composer': 'Bach', 'composition': 'Violin Sonata No 1 in G minor', 'movement': '2. Fuga', 'ensemble': 'Solo Violin', 'source': 'European Archive', 'transcriber': 'suzumidi', 'catalog_name': 'BWV1001', 'seconds': '312'} {'id': '2243', 'composer': 'Bach', 'composition': 'Violin Sonata No 1 in G minor', 'movement': '3. Siciliana', 'ensemble': 'Solo Violin', 'source': 'European Archive', 'transcriber': 'suzumidi', 'catalog_name': 'BWV1001', 'seconds': '193'} {'id': '2244', 'composer': 'Bach', 'composition': 'Violin Sonata No 1 in G minor', 'movement': '4. Presto', 'ensemble': 'Solo Violin', 'source': 'European Archive', 'transcriber': 'suzumidi', 'catalog_name': 'BWV1001', 'seconds': '214'} {'id': '2288', 'composer': 'Bach', 'composition': 'Violin Partita No 1 in B minor', 'movement': '2. Corrente', 'ensemble': 'Solo Violin', 'source': 'John Garner', 'transcriber': 'suzumidi', 'catalog_name': 'BWV1002', 'seconds': '191'} {'id': '2289', 'composer': 'Bach', 'composition': 'Violin Partita No 1 in B minor', 'movement': '3. Sarabande', 'ensemble': 'Solo Violin', 'source': 'John Garner', 'transcriber': 'suzumidi', 'catalog_name': 'BWV1002', 'seconds': '203'} {'id': '2659', 'composer': 'Bach', 'composition': 'Violin Partita No 1 in B minor', 'movement': '6. Double', 'ensemble': 'Solo Violin', 'source': 'John Garner', 'transcriber': 'suzumidi', 'catalog_name': 'BWV1002', 'seconds': '108'} It looks like this dataset is missing quite a few pieces. Bach wrote 6 sonatas and partitas for solo violin. We only have 3 of them in this dataset. Of the 3 that are included, partitas 1 and 3 are incomplete as well.\nBased on the order of the ids, it looks like this data should be present. Let\u0026rsquo;s see if we can fill some of this in.\n# The times are based on Hilary Hahn\u0026#39;s recordings missing_pieces = [ { \u0026#34;id\u0026#34;: \u0026#34;2679\u0026#34;, \u0026#34;composer\u0026#34;: \u0026#34;Bach\u0026#34;, \u0026#34;composition\u0026#34;: \u0026#34;Violin Partita No 3 in E major\u0026#34;, \u0026#34;movement\u0026#34;: \u0026#34;2. Loure\u0026#34;, \u0026#34;ensemble\u0026#34;: \u0026#34;Solo Violin\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;DASC 5300 Fall 2023\u0026#34;, \u0026#34;transcriber\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;catalog_name\u0026#34;: \u0026#34;BWV 1006\u0026#34;, \u0026#34;seconds\u0026#34;: \u0026#34;287\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;2680\u0026#34;, \u0026#34;composer\u0026#34;: \u0026#34;Bach\u0026#34;, \u0026#34;composition\u0026#34;: \u0026#34;Violin Partita No 3 in E major\u0026#34;, \u0026#34;movement\u0026#34;: \u0026#34;3. Gavotte en Rondeau\u0026#34;, \u0026#34;ensemble\u0026#34;: \u0026#34;Solo Violin\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;DASC 5300 Fall 2023\u0026#34;, \u0026#34;transcriber\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;catalog_name\u0026#34;: \u0026#34;BWV 1006\u0026#34;, \u0026#34;seconds\u0026#34;: \u0026#34;196\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;2681\u0026#34;, \u0026#34;composer\u0026#34;: \u0026#34;Bach\u0026#34;, \u0026#34;composition\u0026#34;: \u0026#34;Violin Partita No 3 in E major\u0026#34;, \u0026#34;movement\u0026#34;: \u0026#34;4. Menuet I\u0026#34;, \u0026#34;ensemble\u0026#34;: \u0026#34;Solo Violin\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;DASC 5300 Fall 2023\u0026#34;, \u0026#34;transcriber\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;catalog_name\u0026#34;: \u0026#34;BWV 1006\u0026#34;, \u0026#34;seconds\u0026#34;: \u0026#34;113\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;2682\u0026#34;, \u0026#34;composer\u0026#34;: \u0026#34;Bach\u0026#34;, \u0026#34;composition\u0026#34;: \u0026#34;Violin Partita No 3 in E major\u0026#34;, \u0026#34;movement\u0026#34;: \u0026#34;5. Menuet II\u0026#34;, \u0026#34;ensemble\u0026#34;: \u0026#34;Solo Violin\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;DASC 5300 Fall 2023\u0026#34;, \u0026#34;transcriber\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;catalog_name\u0026#34;: \u0026#34;BWV 1006\u0026#34;, \u0026#34;seconds\u0026#34;: \u0026#34;183\u0026#34; }, ] Formatting Our Output Viewing raw lines of a dictionary or CSV file is less than ideal. Let\u0026rsquo;s format our output to make it easier to read. We will format the filtered Bach data. Since we know that every piece was written by him, we don\u0026rsquo;t need to show that column.\n# Let\u0026#39;s add the missing pieces to our dataset data.extend(missing_pieces) # Refilter the Bach pieces bach = [row for row in data if \u0026#34;Bach\u0026#34; in row[\u0026#34;composer\u0026#34;] and \u0026#34;Solo Violin\u0026#34; in row[\u0026#34;ensemble\u0026#34;]] print(f\u0026#34;There are {len(bach)} Bach pieces in the dataset\u0026#34;) # Print them all out, sorted by composition then movement composition_width = max(len(row[\u0026#34;composition\u0026#34;]) for row in bach) movement_width = max(len(row[\u0026#34;movement\u0026#34;]) for row in bach) id_width = max(len(row[\u0026#34;id\u0026#34;]) for row in bach) print(\u0026#34;{:\u0026lt;{}} | {:\u0026lt;{}} | {:\u0026lt;{}}\u0026#34;.format(\u0026#34;Composition\u0026#34;, composition_width, \u0026#34;Movement\u0026#34;, movement_width, \u0026#34;ID\u0026#34;, id_width)) print(\u0026#34;-\u0026#34; * (composition_width + movement_width + id_width + 6)) for row in sorted(bach, key=lambda x: (x[\u0026#34;composition\u0026#34;], x[\u0026#34;movement\u0026#34;])): print(\u0026#34;{:\u0026lt;{}} | {:\u0026lt;{}} | {:\u0026lt;{}}\u0026#34;.format(row[\u0026#34;composition\u0026#34;], composition_width, row[\u0026#34;movement\u0026#34;], movement_width, row[\u0026#34;id\u0026#34;], id_width)) There are 13 Bach pieces in the dataset Composition | Movement | ID ------------------------------------------------------------- Violin Partita No 1 in B minor | 2. Corrente | 2288 Violin Partita No 1 in B minor | 3. Sarabande | 2289 Violin Partita No 1 in B minor | 6. Double | 2659 Violin Partita No 3 in E major | 1. Preludio | 2186 Violin Partita No 3 in E major | 2. Loure | 2679 Violin Partita No 3 in E major | 3. Gavotte en Rondeau | 2680 Violin Partita No 3 in E major | 4. Menuet I | 2681 Violin Partita No 3 in E major | 5. Menuet II | 2682 Violin Partita No 3 in E major | 6. Bourree | 2191 Violin Sonata No 1 in G minor | 1. Adagio | 2241 Violin Sonata No 1 in G minor | 2. Fuga | 2242 Violin Sonata No 1 in G minor | 3. Siciliana | 2243 Violin Sonata No 1 in G minor | 4. Presto | 2244 # Now that it looks good, let\u0026#39;s write the updated dataset to a new file with open(\u0026#34;../data/musicnet_metadata_updated.csv\u0026#34;, \u0026#34;w\u0026#34;) as fp: writer = csv.DictWriter(fp, fieldnames=keys) writer.writeheader() writer.writerows(data) Reading and Writing JSON JSON stands for JavaScript Object Notation. It is a common format for storing and transmitting data. It is often used for web APIs. Python has a built-in module for reading and writing JSON files.\nLet\u0026rsquo;s open the CSV file from the previous example and write it to JSON.\n# Load and parse CSV import csv import json # Store the data in a list of dictionaries data = [] with open(\u0026#34;../data/musicnet_metadata.csv\u0026#34;, \u0026#34;r\u0026#34;) as fp: reader = csv.reader(fp) keys = next(reader) for row in reader: data.append(dict(zip(keys, row))) # Write the data to a JSON file with open(\u0026#34;musicnet_metadata.json\u0026#34;, \u0026#34;w\u0026#34;) as fp: json.dump(data, fp) Reading and Writing pickle Files Python has a built-in module for reading and writing pickle files. pickle is a binary format for serializing Python objects. It is not human-readable, but it is very useful for storing and transmitting data. Note that pickle is not secure. It is possible to create malicious pickle files that can execute arbitrary code when loaded. Also, other languages cannot natively read pickle files. However, there are usually libraries available for reading pickle files in other languages.\nLet\u0026rsquo;s start by writing the list from the previous example to a pickle file.\n# Write the data to a pickle file import pickle with open(\u0026#34;musicnet_metadata.pkl\u0026#34;, \u0026#34;wb\u0026#34;) as fp: pickle.dump(data, fp) # Get the file size of the pickle file we just wrote import os print(os.path.getsize(\u0026#34;musicnet_metadata.pkl\u0026#34;)) 52353 Serializing Class Objects Serializing objects is simple with Python. Let\u0026rsquo;s create a simple class to represent the attributes of our dataset. We can then convert our previous list of dictionary data to a list of objects. Finally, we can serialize the list of objects to a pickle file.\n# Create a class for our data class Piece: def __init__(self, data): self.id = data[\u0026#34;id\u0026#34;] self.composer = data[\u0026#34;composer\u0026#34;] self.composition = data[\u0026#34;composition\u0026#34;] self.movement = data[\u0026#34;movement\u0026#34;] self.ensemble = data[\u0026#34;ensemble\u0026#34;] self.source = data[\u0026#34;source\u0026#34;] self.transcriber = data[\u0026#34;transcriber\u0026#34;] self.catalog_name = data[\u0026#34;catalog_name\u0026#34;] self.seconds = data[\u0026#34;seconds\u0026#34;] def __repr__(self): return f\u0026#34;\u0026lt;Piece {self.id}: {self.composer} - {self.composition}\u0026gt;\u0026#34; def __str__(self): return f\u0026#34;{self.composer} - {self.composition}\u0026#34; # Convert our data into a list of objects and write it to a pickle file pieces = [Piece(d) for d in data] with open(\u0026#34;musicnet_metadata.pkl\u0026#34;, \u0026#34;wb\u0026#34;) as fp: pickle.dump(pieces, fp) # Print the size of this new file print(os.path.getsize(\u0026#34;musicnet_metadata.pkl\u0026#34;)) 54352 Bonus: Write the class objects to JSON We can\u0026rsquo;t write the class objects to JSON directly. We need to convert them to a dictionary first. We can do this by implementing the __dict__ method. Without explicitly defining __dict__, it will return the default dictionary for the class. However, we can override it to return a custom dictionary. The default version will include all of the class attributes. We can use this to our advantage to convert the class to a dictionary.\n# Bonus: Save the class data as JSON import json # Write the list of class objects to a JSON file with open(\u0026#34;musicnet_metadata.json\u0026#34;, \u0026#34;w\u0026#34;) as fp: json.dump(pieces, fp, default=lambda o: o.__dict__) ","date":1693371600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693371600,"objectID":"eee3603d2b9f6cba33b4e8b622a97af9","permalink":"https://ajdillhoff.github.io/notes/file_io/","publishdate":"2023-08-30T00:00:00-05:00","relpermalink":"/notes/file_io/","section":"notes","summary":"File I/O in Python This notebook will cover the following topics:\nOpening and closing files Reading and writing text files Reading and writing binary files Using the with statement Using the pickle module Serializing objects with pickle Reading and writing JSON files Opening and closing files Let\u0026rsquo;s start with the basics. To open a file, we use the built-in open() function. The open() function takes two arguments: the name of the file to open and the mode in which to open it.","tags":["python"],"title":"Python File I/O","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Functions We learned the importance of functions early on in mathematics. It is a compact way of represented a complex process dependent on a set of variables. In programming, functions are used to encapsulate a set of instructions that are used repeatedly. Functions are also used to make code more readable and easier to debug.\nIn this notebook, we will look at a few important built-in functions in Python as well as how to define our own functions.\nBuilt-in Functions Python has a good number of built-in functions that cover a general range of tasks.\n# Find the max of a list of numbers max_val = max(1, 2, 3) print(f\u0026#34;The max value is {max_val}\u0026#34;) # Find the min of a list of numbers min_val = min(1, 2, 3) print(f\u0026#34;The min value is {min_val}\u0026#34;) # Get the length of a string text = input(\u0026#34;Enter some text: \u0026#34;) print(f\u0026#34;The length of \\\u0026#34;{text}\\\u0026#34; is {len(text)}\u0026#34;) The max value is 3 The min value is 1 The length of \u0026quot;test\u0026quot; is 4 The len function returns the length of a string, list, or other iterable object. Since these functions are built-in, we should treat them as keywords. However, we can still overwrite them if we want to.\nType Conversion Functions Python has built-in functions to convert between data types. These are int, float, str, bool, and list. Converting between incompatible types will result in an error.\n# Try entering a number and text val = input(\u0026#34;Enter a number: \u0026#34;) val_int = int(val) print(f\u0026#34;The value of {val} is {val_int}\u0026#34;) # Converting an integer type to float will truncate the decimal value float_val = 3.14 int_val = int(float_val) print(f\u0026#34;The value of {float_val} is {int_val}\u0026#34;) # Converting a number to a string also has its uses big_number = 184759372934 big_number_str = str(big_number) print(f\u0026#34;There are {len(big_number_str)} digits in {big_number_str}\u0026#34;) The value of 10 is 10 The value of 3.14 is 3 There are 12 digits in 184759372934 Math Functions Python has many more functions that are available through named modules. For example, the math module contains many useful functions for mathematical operations. To use these functions, we need to import the module first.\nimport math signal_power = 10 noise_power = 5 ratio = signal_power / noise_power decibels = 10 * math.log10(ratio) print(f\u0026#34;The decibel value is {decibels}\u0026#34;) # The math module also has a function to convert from radians to degrees radians = 0.7 degrees = math.degrees(radians) print(f\u0026#34;{radians} radians is {degrees} degrees\u0026#34;) # The math module also has a function to convert from degrees to radians degrees = 45 radians = math.radians(degrees) print(f\u0026#34;{degrees} degrees is {radians} radians\u0026#34;) The decibel value is 3.010299956639812 0.7 radians is 40.10704565915762 degrees 45 degrees is 0.7853981633974483 radians Example: Getting the number of digits without len We can use the math.log10 function to get the number of digits in a number.\nbig_number = 184759372934 big_number_log = math.log10(big_number) print(f\u0026#34;The log of {big_number} is {big_number_log}\u0026#34;) # We can use this to get the number of digits in a number big_number_log_int = int(big_number_log) print(f\u0026#34;There are {big_number_log_int + 1} digits in {big_number}\u0026#34;) The log of 184759372934 is 11.266606479598726 There are 12 digits in 184759372934 Random Numbers The random module contains functions for generating random numbers. The random function returns a random number between 0 and 1. The randint function returns a random integer between two numbers. Other functions in the random module include randrange, choice, choices, shuffle, and sample.\nimport random # Generate 10 random numbers between 0 and 1 for i in range(10): print(random.random()) # Generate 10 random integers between 4 and 10 for i in range(10): print(random.randint(4, 10)) # Randomly select a value from a list outcomes = [\u0026#34;rock\u0026#34;, \u0026#34;paper\u0026#34;, \u0026#34;scissors\u0026#34;] print(random.choice(outcomes)) 0.5791115192498043 0.25376108559783617 0.3024994224981705 0.09072083021401978 0.42037740159252324 0.6617409553852582 0.4379309412534801 0.2475132325137145 0.8452657960508129 0.9268148237541722 4 5 7 5 7 10 8 6 7 4 scissors Third-party modules such as numpy and scipy contain many more functions for generating random numbers.\nimport numpy as np # Generate 10 random numbers between 0 and 1 numbers = np.random.rand(10) print(numbers) # Sample 10 random numbers from a normal distribution numbers = np.random.randn(10) print(numbers) [0.47426298 0.22698408 0.42633457 0.97584666 0.68835279 0.57067918 0.56958053 0.86689698 0.54039587 0.59397872] [ 0.97783258 -0.4043045 0.05416324 -2.21162364 -0.60327265 -0.39077797 -0.83294774 0.56285811 -0.28047169 -0.60044315] Defining Functions We can define our own functions using the def keyword. The syntax for defining a function is as follows:\ndef function_name(parameters): # function body return value Python functions can have multiple parameters and return multiple values. The return keyword is optional. If it is not used, the function will return None.\nimport math def calculate_stats(numbers): \u0026#34;\u0026#34;\u0026#34;Calculate the mean and standard deviation of a list of numbers\u0026#34;\u0026#34;\u0026#34; mean = sum(numbers) / len(numbers) variance = sum((x - mean) ** 2 for x in numbers) / len(numbers) std_dev = math.sqrt(variance) return mean, std_dev # Calculate the mean and standard deviation of a list of numbers numbers = [1, 2, 3, 4, 5] mean, std_dev = calculate_stats(numbers) print(f\u0026#34;The mean is {mean} and the standard deviation is {std_dev}\u0026#34;) The mean is 3.0 and the standard deviation is 1.4142135623730951 Default Argument Values Python functions can have default values for their arguments. This allows us to call the function without specifying the value for that argument. If we do not specify a value for an argument, the default value will be used.\n# To demonstrate default argument values, let\u0026#39;s make a function that # allows the user to select the axis along which to calculate the mean def calculate_mean(numbers, axis=0): \u0026#34;\u0026#34;\u0026#34;Calculates the mean of a 2D array along a given axis\u0026#34;\u0026#34;\u0026#34; # Check that the input is a 2D python list if not isinstance(numbers, list) or not isinstance(numbers[0], list): raise ValueError(\u0026#34;Input must be a 2D list\u0026#34;) if axis == 0: # Here, zip(*numbers) will return a list of tuples, # where each tuple is a column of our 2D list. return [sum(col) / len(col) for col in zip(*numbers)] elif axis == 1: return [sum(row) / len(row) for row in numbers] else: raise ValueError(\u0026#34;Invalid axis value\u0026#34;) # Calculate the mean along the rows numbers = [[1, 2, 3], [4, 5, 6]] mean = calculate_mean(numbers, axis=1) print(\u0026#34;Mean of each row:\u0026#34;, mean) # Calculate the mean along the columns numbers = [[1, 2, 3], [4, 5, 6]] mean = calculate_mean(numbers, axis=0) print(\u0026#34;Mean of each column\u0026#34;, mean) Mean of each row: [2.0, 5.0] Mean of each column [2.5, 3.5, 4.5] Keyword Arguments Python functions can also have keyword arguments. This allows us to specify the name of the argument when calling the function. This is useful when a function has many arguments and we only want to specify a few of them.\ndef matrix_max(matrix, axis=0, return_indices=False): \u0026#34;\u0026#34;\u0026#34;Calculates the maximum of a 2D array along a given axis\u0026#34;\u0026#34;\u0026#34; # Check that the input is a 2D python list if not isinstance(matrix, list) or not isinstance(matrix[0], list): raise ValueError(\u0026#34;Input must be a 2D list\u0026#34;) if axis == 0: # Here, zip(*matrix) will return a list of tuples, # where each tuple is a column of our 2D list. max_vals = [max(col) for col in zip(*matrix)] if return_indices: max_indices = [col.index(max(col)) for col in zip(*matrix)] return max_vals, max_indices else: return max_vals elif axis == 1: max_vals = [max(row) for row in matrix] if return_indices: max_indices = [row.index(max(row)) for row in matrix] return max_vals, max_indices else: return max_vals else: raise ValueError(\u0026#34;Invalid axis value\u0026#34;) # Calculate the max along the rows numbers = [[1, 2, 3], [4, 5, 6]] max_vals = matrix_max(numbers, axis=1) print(\u0026#34;Max of each row:\u0026#34;, max_vals) # Calculate the max along the columns numbers = [[1, 2, 3], [4, 5, 6]] max_vals = matrix_max(numbers, axis=0) print(\u0026#34;Max of each column\u0026#34;, max_vals) # Calculate the max along the rows and return the indices numbers = [[1, 2, 3], [4, 5, 6]] max_vals, max_indices = matrix_max(numbers, axis=1, return_indices=True) print(\u0026#34;Max of each row:\u0026#34;, max_vals) print(\u0026#34;Indices of max values:\u0026#34;, max_indices) Max of each row: [3, 6] Max of each column [4, 5, 6] Max of each row: [3, 6] Indices of max values: [2, 2] List Unpacking and Variable Arguments Python functions can have variable arguments. This allows us to pass in a variable number of arguments to a function. The * operator is used to unpack a list or tuple into separate arguments.\n# The range function expects up to 3 arguments: start, stop, and step # We can use list unpacking to put our arguments in a list args = [1, 10, 2] for i in range(*args): print(i) 1 3 5 7 9 # We can create a function that support multiple arguments as well as keyword arguments def print_args(*args, **kwargs): print(\u0026#34;Positional arguments:\u0026#34;, args) print(\u0026#34;Keyword arguments:\u0026#34;, kwargs) print_args(1, 2, 3, a=4, b=5) Positional arguments: (1, 2, 3) Keyword arguments: {'a': 4, 'b': 5} ","date":1693371600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693371600,"objectID":"84951135ebd9fe6eddadf2a6c8d8f3a5","permalink":"https://ajdillhoff.github.io/notes/functions/","publishdate":"2023-08-30T00:00:00-05:00","relpermalink":"/notes/functions/","section":"notes","summary":"Functions We learned the importance of functions early on in mathematics. It is a compact way of represented a complex process dependent on a set of variables. In programming, functions are used to encapsulate a set of instructions that are used repeatedly. Functions are also used to make code more readable and easier to debug.\nIn this notebook, we will look at a few important built-in functions in Python as well as how to define our own functions.","tags":["python"],"title":"Python Functions","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents What is Version Control? What is Git? What is a Repository? Configuring Git Creating a Repository Staging Files Committing Changes Ignoring Files Branching Merging Remotes Cloning an Existing Repository Summary What is Version Control? Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later. This is useful not just for team projects, for for individual projects as well.\nWith version control, you can:\nrevert files back to a previous state revert the entire project back to a previous state compare changes over time see who last modified something that might be causing a problem who introduced an issue and when and much more When tracking changes to a project over time, the simplest approach is one that you might recognize if you\u0026rsquo;ve ever worked on an essay for class. Imagine you\u0026rsquo;ve just finished the first draft of an assignment. You decide to save this document as essay_first_draft.docx. After working on it a bit more, you choose to save the updated copy to a new file so that you can compare the initial and final draft. This one is then named essay_first_draft_COMPLETE.docx. You end up reading some new information and realize you missed a key requirement of the assignment. After adding in the new information you save it as essay_first_draft_COMPLETE_v2.docx. After many such iterations you end up with a collection of ill-named files.\nMaybe you\u0026rsquo;ve never done this yourself, but this example actually depicts a version control system. Luckily for us, there have been many improvements to this naive method. A more ideal choice, especially in a team environment, would be a Centralized VCS. Project files would be hosted on a server that keeps track of the different changes. Team members can download the latest versions, modify them, and update the server once they are done.\nFigure 1: Centralized VCS (source) This is a welcome improvement over the naive version, but it still has its downsides. What if the server or connection goes down? There are many scenarios that would lead to a catastrophic loss of data. For important projects, you would not want to keep all of your eggs in once basket. A more ideal solution would be a Distributed VCS, this is what Git is.\nFigure 2: Distributed VCS (source) In a DVCS, every user has a complete copy of the project. If the server goes down, or a connection is lost, you can still work on the project. Since every user has a complete copy of the project, there is no single point of failure. Another huge advantage is speed. The operations are performed locally. It is only when you want to share your changes that you need to connect to the server. This means that you can commit changes, create branches, and perform other operations without an internet connection.\nWhat is Git? There are two primary ways of thinking about versioning in general: snapshots and differences. The first starts with your original files and records each change as a delta between the latest version and the previous.\nFigure 3: Difference-based Version Control (source) The second starts with your original files and records each change as a snapshot of the entire project. Files that have not changed will not be duplicated. Instead, Git will create a reference to the previous version of the file. This is the approach that Git uses, and it comes with a great benefit that we will see when we get to branching.\nFigure 4: Snapshot-based Version Control (source) What is a Repository? A repository is a collection of files and folders that are tracked by Git. It is the project folder that you will be working in. You can create a repository from scratch, or you can clone an existing repository. We will cover examples of both during class. Cloning is the process of copying an existing repository to your local machine. We will start with the first approach: creating a repository from scratch.\nBefore starting, it is important to at least know the three major states of Git. Files can be modified, staged, or committed.\nA modified file has been changed locally, but has not been committed to the repository. A staged file is a modified file that has been marked to be included in the next commit. A committed file is a staged file that has been saved to the repository. Figure 5: The main sections of Git. (source) The figure above depicts the three major sections of working with a Git repository. Each repository has a .git directory that contains all of the information about the project. The working directory is the root directory where the latest versions of the files exist. Once modifications are made, these changes are sent to the staging area. This is where you can choose which changes to include in the next commit. Once you are happy with the changes, you can commit them to the repository. This will save the changes to the .git directory.\nConfiguring Git Once you have installed Git, there are a few important configuration options to get started. If you have already been using Git, you can skip this section. If you are using Git for the first time, you will need to set your name and email address. This information will be used to identify you as the author of the commits that you make.\ngit config --global user.name \u0026#34;Naomi Nagata\u0026#34; git config --global user.email \u0026#34;naomi@rocinante.exp\u0026#34; If you have already used a service like GitHub, note that this name and email does not need to match the one you used to log into that service.\nYou can view your current configuration at any time by running the following command:\ngit config --list --show-origin Creating a Repository For this example, our first project will be a Python program that resizes images to a specified width. This is to ensure that the aspect ratio is maintained.\nNow that we have Git installed and configured, we can create our first repository. First, create a new directory for the project. I will use pyresize in this document. Then, navigate to that directory and run the following command:\nmkdir pyresize \u0026amp;\u0026amp; cd pyresize git init You may see the following warning when creating a new repository:\nhint: Using \u0026#39;master\u0026#39; as the name for the initial branch. This default branch name hint: is subject to change. To configure the initial branch name to use in all hint: of your new repositories, which will suppress this warning, call: hint: hint: git config --global init.defaultBranch \u0026lt;name\u0026gt; hint: hint: Names commonly chosen instead of \u0026#39;master\u0026#39; are \u0026#39;main\u0026#39;, \u0026#39;trunk\u0026#39; and hint: \u0026#39;development\u0026#39;. The just-created branch can be renamed via this command: hint: hint: git branch -m \u0026lt;name\u0026gt; Initialized empty Git repository in /home/alex/dev/pyresize/.git/ Let\u0026rsquo;s first set the default branch name to main.\ngit config --global init.defaultBranch main Next, we will change the name of the current branch to main. We could also delete the .git directory and start over, but this is a good opportunity to learn how to rename a branch.\ngit branch -m main You can view the status of your repository at any time by using the git status command. This will show you the current branch, the files that have been modified, and the files that have been staged. Our newly created repository looks like this:\n$ git status On branch main No commits yet nothing to commit (create/copy files and use \u0026#34;git add\u0026#34; to track) Staging Files Let\u0026rsquo;s create our first file and add it to the repository. We will create a file called pyresize.py that contains the following code:\nimport sys from PIL import Image def resize_image(image_path, width): image = Image.open(image_path) wpercent = (width / float(image.size[0])) hsize = int((float(image.size[1]) * float(wpercent))) image = image.resize((width, hsize), Image.LANCZOS) image.save(image_path) if __name__ == \u0026#34;__main__\u0026#34;: resize_image(sys.argv[1], int(sys.argv[2])) At this point, we have a local change that our repository is not aware of. We can see this by running the git status command again.\n$ git status On branch main No commits yet Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) pyresize.py nothing added to commit but untracked files present (use \u0026#34;git add\u0026#34; to track) Let\u0026rsquo;s add our file with git add pyresize.py and check the status again.\n$ git add pyresize.py $ git status On branch main No commits yet Changes to be committed: (use \u0026#34;git rm --cached \u0026lt;file\u0026gt;...\u0026#34; to unstage) new file: pyresize.py Committing Changes Finally, we will commit this change via git commit. There are a few things to note about this command. If you haven\u0026rsquo;t configured your default editor, it might be set to something like nano or vim by default. If you are not familiar with these editors, you can set your default editor to something else by running the following command:\ngit config --global core.editor \u0026#34;code --wait\u0026#34; This will set your default editor to Visual Studio Code. Obviously, this should be installed on your system if you are using it. If you are using a different editor, you can replace code with the command that you would use to open a file in that editor. For example, if you are using vim, you would use vim. The --wait flag above will wait for the editor to close before continuing. This is important for Git to know when you are done writing your commit message. Note that not every application supports this flag.\nOnce you have set your default editor, you can run git commit to open the editor and write your commit message. The first line should be a short description of the change. The following lines should be a more detailed description of the change. You can see an example below:\nAdded our first file. # Please enter the commit message for your changes. Lines starting # with \u0026#39;#\u0026#39; will be ignored, and an empty message aborts the commit. # # On branch main # # Initial commit # # Changes to be committed: #\tnew file: pyresize.py # Once we save this message, the commit will be complete. You can view the commit history by running git log. This will show you the commit hash, the author, the date, and the commit message. You can also commit changes and add a message in one command.\ngit commit -m \u0026#34;Added our first file.\u0026#34; We do not yet have a remote repository to push to, so we will save that for later. For now, we will continue to work locally. Let\u0026rsquo;s add an image to our repository so that we can test it. I am going to use the UTA Logo for this example. You can download this and variations from the UTA Branding Resources page.\nFigure 6: UTA Logo Create a new imgs folder and add your image(s) to it. We can then add the directory along with all of its contents using git add imgs. You can see the status of your repository by running git status again. Let\u0026rsquo;s go ahead and commit these changes.\ngit commit -am \u0026#34;Added the UTA logo.\u0026#34; Making a Change For this project, we don\u0026rsquo;t really need to have a bunch of test images. It is sufficient to have one or two. The name of our image folder should probably change to reflect its purpose. Let\u0026rsquo;s start by renaming imgs to test_imgs. We can do this with the mv command in bash. Our repository will now look like this:\n$ git status On branch main Changes not staged for commit: (use \u0026#34;git add/rm \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) deleted: imgs/uta.png Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) test_imgs/ no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) This might be a tad unexpected. We renamed the folder, but Git is telling us that we deleted a file. This is because Git is tracking the file imgs/uta.png. When we renamed the folder, Git no longer knew where to find the file. We can fix this by running git rm imgs/uta.png. This will remove the file from the repository. We can then add the new folder with git add test_imgs. However, if we simply use git add test_imgs, Git will not know that we renamed the folder. We can fix this by using the -A flag. This will tell Git to add all changes, including renames. Our repository will now look like this:\n$ git status On branch main Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) renamed: imgs/uta.png -\u0026gt; test_imgs/uta.png Go ahead and commit these changes.\nIgnoring Files There are certain files and directories that will end up in our project folder that we do not want to track. For example, we may want to resize images and save them in a local output directory. However, we do not want to track any of these images. We can have Git remember what we want or do not want using an ignore file. This file will contain a list of files and directories that we want to ignore. Let\u0026rsquo;s create a .gitignore file and add the following line:\noutput/ Make sure you add and commit the .gitignore file.\nBranching Let\u0026rsquo;s go ahead and test our program by resizing one of our images. I\u0026rsquo;m going to to resize uta.png to have a width of 500 pixels using the following command.\npython pyresize.py test_imgs/uta.png 500 Our program doesn\u0026rsquo;t support creating a new file when resizing. Instead, it resizes the file and overwrites the original. We should add this feature and modify it without messing up our current code base. This is where branching comes in. We can create a new branch that will contain our new feature. We can then test it and merge it back into the main branch once we are happy with it.\nEvery commit that we make is a snapshot of the entire project up to that point. There is a unique identifier attached to each commit. If we want to work on a specific bug or new feature without affecting the current code base, we can create a branch to track those changes independently of the other branches. The main benefits are that we can potentially break the code base without affecting the production-ready code. We can also work on multiple features at the same time without affecting each other.\nLet\u0026rsquo;s create a new branch called output_write. We can do this with the git branch command.\ngit branch output_write Figure 7: The result of creating a new branch named testing. (source) This only creates a new branch, but we are still on the main branch. We can see this by running git branch. The current branch will be highlighted with an asterisk. The current branch is pointed to by the HEAD pointer. We can switch to the new branch using git checkout.\ngit checkout output_write Figure 8: The HEAD pointer after switching to a new branch. (source) Modifying the Code Now that we are on the output_write branch, we can modify the code without affecting the main branch. Let\u0026rsquo;s modify our original function to take in an additional argument: the output path. We can then use this path to save the resized image to a new file. Our new code will look like this:\nimport sys from PIL import Image def resize_image(image_path, width, output_path): image = Image.open(image_path) wpercent = (width / float(image.size[0])) hsize = int((float(image.size[1]) * float(wpercent))) image = image.resize((width, hsize), Image.LANCZOS) image.save(output_path) if __name__ == \u0026#34;__main__\u0026#34;: resize_image(sys.argv[1], int(sys.argv[2]), sys.argv[3]) Go ahead and commit these changes. Since we have already moved the HEAD pointer to the new branch, this change will not affect our main branch. The figure below is analagous to this scenario.\nFigure 9: The result of committing changes to a new branch. (source) Let\u0026rsquo;s test our program once more by resizing an image and saving it to the output directory. We can do this with the following command:\npython pyresize.py test_imgs/uta.png 500 output/uta.png Notice that when you run git status after resizing the image and saving to the output directory, it does not report any changes. Our ignore file is working as intended!\nMerging Now that we have completed our new feature and tested it, we should merge these changes back to the main branch. We can do this with the git merge command. First, we need to switch back to the main branch.\ngit checkout main We can then merge the output_write branch into the main branch.\ngit merge output_write This will merge the changes from the output_write branch into the main branch. If there are any conflicts, Git will let you know and you can resolve them manually. Once the merge is complete, you can delete the output_write branch.\ngit branch -d output_write Figure 10: The result of merging a branch into the master branch. (source) The figure above shows the history of a repository in which a branch named iss53 was created, modified with new commits, and eventually merged back into the master branch.\nRemotes We have now covered the basics of using Git locally. Eventually, we will want our changes to be backed up on a remote server. This will allow us to collaborate with others and work on our projects from multiple machines. There are many services that provide this functionality. We will use GitHub for this example, but the process is similar for other services.\nCreating a Repository First, we need to create a new repository on GitHub. You can do this by clicking the New button on the GitHub homepage. I am only going to add a short description of this program. Go ahead and click Create repository.\nGitHub supports both SSH and HTTPS. I already have an SSH key set up. If you haven\u0026rsquo;t configured one yet, check out Adding a new SSH key to your GitHub account for instructions on how to do so. You can also use HTTPS, this requires a personal access token. More information can be found at Creating a personal access token.\nOnce created, we will have the option to use either our HTTPS or SSH URL. Mine is git@github.com:ajdillhoff/pyresize.git. We can add this as a remote repository using the git remote add command.\ngit remote add origin git@github.com:ajdillhoff/pyresize.git Pushing to a Remote Now that we have a remote repository, we can push our changes to it. We can do this with the git push command. However, we need to specify the remote repository and the branch that we want to push. We can do this with the following command:\ngit push -u origin main That\u0026rsquo;s it! Our changes are now backed up on GitHub. We can view our repository by navigating to the URL in our browser. We can also view the commit history by clicking the Commits link.\nCloning an Existing Repository If our repository already exists on GitHub, we can clone it to our local machine. This will create a new directory with the same name as the repository. We can do this with the git clone command. Let\u0026rsquo;s clone the pyresize repository that we just created.\ngit clone git@github.com:ajdillhoff/pyresize.git You can clone using either the HTTPS or SSH URLs. Make sure you have the appropriate key or access token to do so.\nPulling from a Remote Now that we have cloned the repository, we can make changes and push them to the remote. However, if someone else makes changes to the remote repository, we will need to pull those changes to our local repository. We can do this with the git pull command.\ngit pull origin main Git will always require that we are up-to-date with the remote before we can push our changes. If someone else has made changes to the remote, we will need to pull those changes before we can push our own. This is to prevent conflicts.\nSummary We have covered the basics of using Git. We have created a repository, staged and committed changes, created branches, merged branches, and pushed our changes to a remote repository. There are many more features that we have not covered, but this should be enough to get you started. If you are interested in learning more, check out the Git Book.\nCommand Reference\nCommand Description git init Create a new repository git config Configure Git git status View the status of your repository git add Add files to the staging area git commit Commit changes to the repository git branch Create, list, or delete branches git checkout Switch branches or restore working tree files git merge Join two or more development histories together git remote Manage set of tracked repositories git push Update remote refs along with associated objects git clone Clone a repository into a new directory git pull Fetch from and integrate with another repository or a local branch git log Show commit logs git rm Remove files from the working tree and from the index git mv Move or rename a file, a directory, or a symlink git branch -d Delete a branch git remote add Add a remote repository ","date":1693026000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693026000,"objectID":"dad86f5b79754b913113ce9031015069","permalink":"https://ajdillhoff.github.io/notes/introduction_to_git/","publishdate":"2023-08-26T00:00:00-05:00","relpermalink":"/notes/introduction_to_git/","section":"notes","summary":"Table of Contents What is Version Control? What is Git? What is a Repository? Configuring Git Creating a Repository Staging Files Committing Changes Ignoring Files Branching Merging Remotes Cloning an Existing Repository Summary What is Version Control? Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later. This is useful not just for team projects, for for individual projects as well.","tags":["git","programming"],"title":"Introduction to Git","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Lists and List Comprehensions List comprehensions provide a concise way to create lists, and are often faster than using a for-loop. They are inspired by set-builder notation in mathematics.\nThis notebook demonstrates common list functions as well as the syntax and basic usage of list comprehensions.\nnames = [\u0026#34;Naomi\u0026#34;, \u0026#34;James\u0026#34;, \u0026#34;Amos\u0026#34;, \u0026#34;Bobbie\u0026#34;] # Append \u0026#34;Miller\u0026#34; to the end of the list names.append(\u0026#34;Miller\u0026#34;) # This can also be accomplished using the + operator or the extend() method # These are similar, where `+=` is shorthand for `extend` names.extend([\u0026#34;Chrisjen\u0026#34;]) names += [\u0026#34;Alex\u0026#34;] # `extend()` works on any iterable names.extend([\u0026#34;Fred\u0026#34;, \u0026#34;Dawes\u0026#34;, \u0026#34;Ashford\u0026#34;]) # This can also be done using slicing # names[len(names):] = [\u0026#34;Fred\u0026#34;, \u0026#34;Dawes\u0026#34;, \u0026#34;Ashford\u0026#34;] # Insert \u0026#34;Holden\u0026#34; at the beginning of the list names.insert(0, \u0026#34;Holden\u0026#34;) # We can easily remove items names.remove(\u0026#34;Alex\u0026#34;) # We can also remove items by value names.remove(\u0026#34;Ashford\u0026#34;) # We can also remove items by index. This will return the removed item eros_passenger = names.pop(5) print(eros_passenger + \u0026#34; is on his way to Venus\u0026#34;) # If your list has duplicates, you can count the number of times a value appears print(\u0026#34;Naomi\u0026#34; + \u0026#34; appears \u0026#34; + str(names.count(\u0026#34;Naomi\u0026#34;)) + \u0026#34; time(s)\u0026#34;) # Reversing a list is easy names.reverse() # We can also create a shallow copy of a list names_copy = names.copy() Miller is on his way to Venus Naomi appears 1 time(s) Sorting A list can be sorted by calling the sort function. The list is sorted in-place, meaning that the original list is modified. Since a list can contain any type of object, the objects must be comparable to each other. For example, a list of strings can be sorted alphabetically, but a list of strings and integers cannot be sorted.\nIn cases with mixed types, a custom key function can be passed to the sort function. The key function is called on each element of the list, and the return value is used to sort the list. For example, to sort a list of strings and integers by the length of the string, the key function would transform the integers into strings.\n\u0026gt;\u0026gt;\u0026gt; l = [\u0026#39;abc\u0026#39;, 1, \u0026#39;ab\u0026#39;] \u0026gt;\u0026gt;\u0026gt; l.sort(key=str) \u0026gt;\u0026gt;\u0026gt; l [1, \u0026#39;abc\u0026#39;, \u0026#39;ab\u0026#39;] # Sorting is a very common operation, and Python gives us some level of control over how the items are sorted. # The default is to sort in ascending order # Sort the list in ascending order names.sort() print(names) # Sort the list in descending order names.sort(reverse=True) print(names) # We can also sort by a key function, such as the length of each name names.sort(key=len) print(names) ['Amos', 'Bobbie', 'Chrisjen', 'Dawes', 'Fred', 'Holden', 'James', 'Naomi'] ['Naomi', 'James', 'Holden', 'Fred', 'Dawes', 'Chrisjen', 'Bobbie', 'Amos'] ['Fred', 'Amos', 'Naomi', 'James', 'Dawes', 'Holden', 'Bobbie', 'Chrisjen'] Let\u0026rsquo;s introduce a slightly more complex scenario in which each person in the list rolls a 20-sided die. We\u0026rsquo;ll use the random module to generate a random number between 1 and 20 for each person. Using those values, we can then sort the list by the roll of the die, in descending order.\nimport random # Roll a 20-sided die for each person using a for loop # rolls = [] # for name in names: # rolls.append(random.randint(1, 20)) # The above code is commented out by default because it can be written more succinctly using a list comprehension rolls = [random.randint(1, 20) for _ in names] print(names) names.index(\u0026#39;Fred\u0026#39;) # Sort the names by the roll of the die, in descending order # First, create a function that returns the roll based on the name def get_roll(name): # print(names) # reveals an empty list! return rolls[names.index(name)] # names.sort(key=get_roll, reverse=True) # The above will not work because `names` does not exist within the scope of the function. # We can instead combine the rolls with the names using the zip() function names_and_rolls = list(zip(names, rolls)) names_and_rolls.sort(key=lambda roll: roll[1], reverse=True) print(names_and_rolls) ['Fred', 'Amos', 'Naomi', 'James', 'Dawes', 'Holden', 'Bobbie', 'Chrisjen'] [('Dawes', 19), ('Fred', 18), ('Amos', 12), ('Naomi', 11), ('Bobbie', 10), ('Holden', 8), ('Chrisjen', 7), ('James', 5)] List Comprehensions We can also create nested list comprehensions, which is equivalent to nested for loops. For example, let\u0026rsquo;s create a 3x4 matrix using a nested list comprehension.\nNested list comprehension Advanced list comprehension # Nested list comprehension # Create a 2D list where each row represents the top 5 rolls for each person top_rolls = [[random.randint(1, 20) for _ in range(5)] for _ in names] print(top_rolls) # We can create a similar 2D list where each row is a tuple of the name and the top 5 rolls top_rolls = [(name, [random.randint(1, 20) for _ in range(5)]) for name in names] print(top_rolls) # If we wrote this with loops, it would look like this: top_rolls = [] for name in names: rolls = [] for _ in range(5): rolls.append(random.randint(1, 20)) top_rolls.append((name, rolls)) [[7, 1, 11, 15, 14], [3, 14, 15, 10, 9], [7, 16, 17, 15, 19], [12, 6, 10, 17, 19], [10, 4, 7, 17, 11], [16, 1, 14, 2, 12], [3, 10, 2, 12, 13], [7, 17, 16, 3, 6]] [('Fred', [15, 7, 1, 3, 16]), ('Amos', [7, 15, 9, 4, 6]), ('Naomi', [19, 7, 12, 5, 6]), ('James', [6, 11, 4, 10, 17]), ('Dawes', [13, 5, 15, 19, 8]), ('Holden', [14, 18, 16, 5, 15]), ('Bobbie', [12, 15, 8, 18, 20]), ('Chrisjen', [18, 16, 19, 10, 11])] Performance of list comprehensions versus for loops A strong argument for list comprehensions is that they are more elegant and easier to read. However, they are also faster than for loops. Let\u0026rsquo;s compare the performance of list comprehensions and for loops.\nTwo common benchmarks to test their performance are to append numbers to a list and to square numbers. Let\u0026rsquo;s compare the performance of list comprehensions and for loops for these two tasks.\nimport timeit # Benchmark #1: Append vs. List Comprehension # Append def for_append(): names = [] for i in range(1000000): names.append(i) # print the average out of 10 runs (in milliseconds) print(\u0026#34;Append: \u0026#34; + str(timeit.timeit(for_append, number=10) * 1000)) # List Comprehension def list_comprehension(): names = [i for i in range(1000000)] # print the average out of 10 runs (in milliseconds) print(\u0026#34;Append: \u0026#34; + str(timeit.timeit(for_append, number=10) * 1000)) Append: 371.44755799999984 Append: 351.0218179999356 # Benchmark 2: Squaring Numbers # For loop def for_loop(): squares = [] for i in range(1000000): squares.append(i**2) # print the average out of 10 runs (in milliseconds) print(\u0026#34;For Loop: \u0026#34; + str(timeit.timeit(for_loop, number=10) * 1000)) # List Comprehension def list_comprehension(): squares = [i**2 for i in range(1000000)] # print the average out of 10 runs (in milliseconds) print(\u0026#34;List Comprehension: \u0026#34; + str(timeit.timeit(list_comprehension, number=10) * 1000)) For Loop: 593.4785219997138 List Comprehension: 572.2285600004398 ","date":1693026000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693026000,"objectID":"59bb38b9c22df96f8411c343f872cd6b","permalink":"https://ajdillhoff.github.io/notes/list_comprehensions/","publishdate":"2023-08-26T00:00:00-05:00","relpermalink":"/notes/list_comprehensions/","section":"notes","summary":"Lists and List Comprehensions List comprehensions provide a concise way to create lists, and are often faster than using a for-loop. They are inspired by set-builder notation in mathematics.\nThis notebook demonstrates common list functions as well as the syntax and basic usage of list comprehensions.\nnames = [\u0026#34;Naomi\u0026#34;, \u0026#34;James\u0026#34;, \u0026#34;Amos\u0026#34;, \u0026#34;Bobbie\u0026#34;] # Append \u0026#34;Miller\u0026#34; to the end of the list names.append(\u0026#34;Miller\u0026#34;) # This can also be accomplished using the + operator or the extend() method # These are similar, where `+=` is shorthand for `extend` names.","tags":["python"],"title":"Python List Comprehensions","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Control Flow Control flow allows us to build programs that react to some pre-determined condition. For example, what happens when a user logs in with the correct credentials? What if they don\u0026rsquo;t give valid credentials?\nThis notebook covers the basic tools to writing conditional statements in Python. It follows Chapter 3 in Python for Everyone by Charles Severance along with my own examples.\nBoolean Expressions A boolean expression evaluates to either True or False. This type of expression would be used to check status codes or to check if a user has entered the correct password, for example.\nprint(\u0026#34;1==1 is {}\u0026#34;.format(1==1)) print(\u0026#34;1==2 is {}\u0026#34;.format(1==2)) 1==1 is True 1==2 is False Operators like == are called relational operators and they compare two operands and return either True or False. Other relational operators include:\nx != y # x is not equal to y x \u0026gt; y # x is greater than y x \u0026lt; y # x is less than y x \u0026gt;= y # x is greater than or equal to y x \u0026lt;= y # x is less than or equal to y x is y # x is the same as y x is not y # x is not the same as y The last two operators, is and is not, are used to check if two variables are referencing the same object. The fact that the operands must be objects is important here. You should avoid comparing a value with a variable. Python will let you do this, but it will also output a warning.\nx = 5 y = 5 x is y # True x is 5 # True, but not recommended x is not 5 # False, but not recommended x = 5 y = 10 x is 5 \u0026lt;\u0026gt;:3: SyntaxWarning: \u0026quot;is\u0026quot; with a literal. Did you mean \u0026quot;==\u0026quot;? \u0026lt;\u0026gt;:3: SyntaxWarning: \u0026quot;is\u0026quot; with a literal. Did you mean \u0026quot;==\u0026quot;? /var/folders/vd/wbzsx0g538nfr96xq81fp7k40000gn/T/ipykernel_24914/759655086.py:3: SyntaxWarning: \u0026quot;is\u0026quot; with a literal. Did you mean \u0026quot;==\u0026quot;? x is 5 True Python includes three logical operators that are verbose compared to other languages.\nx and y # True if both x and y are True x or y # True if either x or y are True not x # True if x is False # Example: FizzBuzz # Consider two possible solutions to the FizzBuzz problem # Solution 1 n = 15 if not n % 3 and not n % 5: print(\u0026#34;FizzBuzz\u0026#34;) # Solution 2 n = 15 if n % 3 == 0: print(\u0026#34;Fizz\u0026#34;) if n % 5 == 0: print(\u0026#34;Buzz\u0026#34;) FizzBuzz Fizz Buzz In the second solution, the output was separated to two separate lines since the print function automatically adds a newline. We can change this behavior by adding a second argument to the print function.\nn = 15 if n % 3 == 0: print(\u0026#34;Fizz\u0026#34;, end=\u0026#34;\u0026#34;) if n % 5 == 0: print(\u0026#34;Buzz\u0026#34;) FizzBuzz Conditional Execution We have already used a key conditional execution tool: the if statement. The if statement allows us to execute a block of code if a condition is met. The general syntax is:\nif condition: # code to execute if condition is True Also note that Python is particular about indentation. The code that is executed if the condition is met must be indented. The standard is to use four spaces for each level of indentation.\nWe can also chain conditional statements together using elif and else. The elif statement is short for \u0026ldquo;else if\u0026rdquo; and allows us to check another condition if the previous condition was not met. The else statement is used to execute code if none of the previous conditions were met. The general syntax is:\nif condition: # code to execute if condition is True elif condition: # code to execute if the first condition is False and this condition is True else: # code to execute if all other conditions are False Switch Statements Until version 3.10, Python did not have a switch statement. This is a conditional statement that allows us to check a variable against a series of values.\nWith version 3.10 comes the match statement. This statement is similar to the switch statement in other languages. The general syntax is:\nmatch variable: case value1: # code to execute if variable == value1 case value2: # code to execute if variable == value2 case value3: # code to execute if variable == value3 case _: # code to execute if none of the previous conditions were met language = input(\u0026#34;What is your favorite programming language? \u0026#34;) match language: case \u0026#34;Python\u0026#34;: print(\u0026#34;You\u0026#39;re in the right place.\u0026#34;) case \u0026#34;Java\u0026#34;: print(\u0026#34;Do you despise C++ as much as the creator of Java?\u0026#34;) case \u0026#34;C++\u0026#34;: print(\u0026#34;You probably like game development.\u0026#34;) case \u0026#34;C\u0026#34;: print(\u0026#34;Speed is your thing.\u0026#34;) case _: print(\u0026#34;You like something else!\u0026#34;) You like something else! Unlike other languages that implement a switch statement, Python\u0026rsquo;s match statement does not have a break statement. We can still utilize fall-through behavior by including multiple values in a single case separated by |.\nmatch variable: case value1 | value2: # code to execute if variable == value1 or variable == value2 case value3: # code to execute if variable == value3 case _: # code to execute if none of the previous conditions were met language = input(\u0026#34;What is your favorite programming language? \u0026#34;) match language: case \u0026#34;Python\u0026#34; | \u0026#34;python\u0026#34;: print(\u0026#34;You\u0026#39;re in the right place.\u0026#34;) case \u0026#34;Java\u0026#34;: print(\u0026#34;Do you despise C++ as much as the creator of Java?\u0026#34;) case \u0026#34;C++\u0026#34;: print(\u0026#34;You probably like game development.\u0026#34;) case \u0026#34;C\u0026#34;: print(\u0026#34;Speed is your thing.\u0026#34;) case _: print(\u0026#34;You like something else!\u0026#34;) You like something else! Iterations Iterations allow us to execute a block of code multiple times. This is useful for iterating over a list of items or for executing a block of code until a condition is met.\nPython supports both a while loop and a for loop. The while loop will execute a block of code until a condition is met. The for loop will iterate over a sequence of items.\nFor Loops As opposed to something like C, Python\u0026rsquo;s for loop is more like a foreach loop. The for loop will iterate over a sequence of items. The general syntax is:\nfor item in sequence: # code to execute for each item in the sequence It is commonly used with the range function to iterate over a sequence of numbers. The range function takes three arguments: start, stop, and step. The start argument is the first number in the sequence. The stop argument is the last number in the sequence. The step argument is the amount to increment the sequence by. The step argument is optional and defaults to 1. The stop argument is required. The start argument is optional and defaults to 0.\nfor i in range(5): print(i) While Loops The while loop will execute a block of code until a condition is met. The general syntax is:\nwhile condition: # code to execute while condition is True Lists Lists are a sequence of values. They are similar to arrays in other languages. The values in a list are called elements or items. Lists are mutable, meaning that we can change the values in a list. Lists are also ordered, meaning that the order of the elements in a list is important.\nWe can create a list by separating the elements with commas and surrounding the list with square brackets.\nnumbers = [1, 2, 3, 4, 5] numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9] names = [\u0026#34;Naomi\u0026#34;, \u0026#34;Bobbie\u0026#34;, \u0026#34;James\u0026#34;, \u0026#34;Amos\u0026#34;, \u0026#34;Chrisjen\u0026#34;, \u0026#34;Alex\u0026#34;, \u0026#34;Clarissa\u0026#34;] names_and_numbers = [\u0026#34;Naomi\u0026#34;, 5, \u0026#34;Bobbie\u0026#34;, 7, \u0026#34;James\u0026#34;, 9, \u0026#34;Amos\u0026#34;, 11, \u0026#34;Chrisjen\u0026#34;, 13, \u0026#34;Alex\u0026#34;, 15, \u0026#34;Clarissa\u0026#34;, 17] # We can even include lists in our lists nested_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] Iterating Over Lists We can iterate over a list using a for loop. The for loop will iterate over each element in the list. The general syntax is:\nfor item in list: # code to execute for each item in the list If the list contains tuples, we can use tuple unpacking to assign the values in the tuple to multiple variables.\nnumbers = [(1, 2), (3, 4), (5, 6)] for x, y in numbers: print(x, y) Combining Lists with zip The zip function allows us to combine two lists into a single list of tuples. The first element in the first list will be paired with the first element in the second list, the second element in the first list will be paired with the second element in the second list, and so on. The general syntax is:\nuser_ids = [1, 2, 3] usernames = [\u0026#39;alice\u0026#39;, \u0026#39;bob\u0026#39;, \u0026#39;charlie\u0026#39;] users = zip(user_ids, usernames) user_ids = [1, 2, 3, 4, 5] user_names = [\u0026#34;Naomi\u0026#34;, \u0026#34;Bobbie\u0026#34;, \u0026#34;James\u0026#34;, \u0026#34;Amos\u0026#34;, \u0026#34;Chrisjen\u0026#34;] # We can combine these lists into a single list of tuples user_ids_and_names = zip(user_ids, user_names) # We can also convert the zip object into a list user_ids_and_names = list(user_ids_and_names) for users in user_ids_and_names: print(users) (1, 'Naomi') (2, 'Bobbie') (3, 'James') (4, 'Amos') (5, 'Chrisjen') ","date":1692680400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692680400,"objectID":"6870d25c01cc5ff4afe7ab4ce0e783a2","permalink":"https://ajdillhoff.github.io/notes/control_flow_in_python/","publishdate":"2023-08-22T00:00:00-05:00","relpermalink":"/notes/control_flow_in_python/","section":"notes","summary":"Control Flow Control flow allows us to build programs that react to some pre-determined condition. For example, what happens when a user logs in with the correct credentials? What if they don\u0026rsquo;t give valid credentials?\nThis notebook covers the basic tools to writing conditional statements in Python. It follows Chapter 3 in Python for Everyone by Charles Severance along with my own examples.\nBoolean Expressions A boolean expression evaluates to either True or False.","tags":["python"],"title":"Control Flow in Python","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Programming with Python Variables, Values, and Data Types Basic Operators Statements and Expressions Basic I/O Commenting Code These notes are focused on introducing programming with Python for those without a technical background.\nIntroduction The official website provides the following description of Python.\nPython is an interpreted, interactive, object-oriented programming language. It incorporates modules, exceptions, dynamic typing, very high level dynamic data types, and classes. It supports multiple programming paradigms beyond object-oriented programming, such as procedural and functional programming. Python combines remarkable power with very clear syntax. It has interfaces to many system calls and libraries, as well as to various window systems, and is extensible in C or C++. It is also usable as an extension language for applications that need a programmable interface. Finally, Python is portable: it runs on many Unix variants including Linux and macOS, and on Windows.\nIf you have never studied any programming languages before, much of this description will be useless to you. There is no context for the types of programming (object-oriented, functional, etc.), and the fact that is extensible in C or C++ may mean absolutely nothing.\nWhy Python? Given that this course is for prospective data science practitioners, and the fact that we have less than a month to cover a programming language, Python is a natural choice. It is widely used in the field of Machine Learning and is gaining more and more ground over R (or so I think) for statistics. There are many third-party libraries for data analysis, visualization, and just about any other data science application we can think of.\nHow to use these notes These notes are organized to follow each major topic in Python. They will also follow the free online book Python for Everybody. These particular lecture notes will start with Chapter 2: Variables, expressions, and statements. It is highly recommended that you run the examples on your own machine, and you are encouraged to make changes. Try to break the code and fix it again. Have it output something different and change the program\u0026rsquo;s purpose entirely.\nA Python notebook will accompany each lecture and will be accessible on my GitHub page. I will also include code snippets directly in this article to highlight a particular example or point.\nProgramming is Hard Before we dive into the language itself, there are a few things that are important to keep in mind. First, programming is hard. It is a skill that requires practice. The tools that you use to program are constantly evolving to keep up with the use-cases of the day. A processor is a complex calculator which means we have to be extremely explicit about the instructions we provide. If you are picking this up for the first time, remember to be patient and be kind to yourself. You will be able to work on big projects that are important to you, but we all have to start somewhere.\nResources are Finite Another important thing to remember is that we are working with limited resources. There is only so much memory and storage space that we can access. These notes are not meant to accompany a full course on hardware architectures, so we will use the following diagram to visualize this point.\nFigure 1: Figure 1.3 from Python for Everybody. The closer the memory is to the CPU, the quicker it can be accessed. Most of the algorithms and data structures we will study in this course will be used with data in main memory. The trade off is that memory that is closer to the CPU is more expensive and has reduced capacity when compared with secondary memory. When we start working with larger sources of data, we will need to adapt our solutions to work with memory that is not directly accessible through a local machine. For now, keep this picture in mind as we dive into Python.\nProgramming with Python Programming languages provide the following features:\na way to write instructions (syntax, statements, expressions) a way to execute a complex series of instructions (functions) a way to store the results of computations and represent data (variables, data structures) They mostly differ in how the language is written, the syntax. Consider the following snippet of Python code:\nusername = \u0026#34;test\u0026#34; password = \u0026#34;securePass1\u0026#34; user_logged_in = False if login(username, password) == True: user_logged_in = True Even if you have never seen any Python code before, you could probably figure out what this block of code is doing. First, 3 variables are defined which store the username, password, and a flag that represents whether or not the user is logged into the system.\nThe control statement if is declared to evaluate if an expression is true. This expression calls a function named login and passes the user\u0026rsquo;s credentials as its arguments. We can assume that the call to login is doing something like validating the user\u0026rsquo;s information and registering their request with a server. If the user was successfully logged in, the function will return True. In this case, we can updated our variable user_logged_in to reflect this.\nVariables, Values, and Data Types The first 3 lines in the example above are variable initializations. The first line is username = \u0026quot;test\u0026quot; which instructs our machines to create a new variable named username and assign it the value \u0026quot;test\u0026quot;. All variables require memory to store their values. When a variable is created, our machine will assign it an address so that it knows where to access that variable\u0026rsquo;s value. This concept is rather simple: in order for something to exist, there must be space for it. As Python developers, we will rarely think about where and how these values are being stored.\nMost languages have rules about what names we can give to variables, and Python is no exception. A variable can use any combination of letters, numbers, and underscores, as long as it does not start with a number and is not the same as a reserved word.\nReserved words in Python\nand continue finally is raise as def for lambda return assert del from None True async elif global nonlocal try await else if not while break except import or with class False in pass yield Data Types Different values are represented differently depending on their type. An integer can be represented in binary in a very straightforward manner. 10 in base 10 is represented as 1010 in binary, for example. Characters in a string like \u0026quot;securePass1\u0026quot; are represented using an encoding such as ASCII or Unicode. Real numbers are typically represented as floating-point types using the IEEE 754 Standard for Floating-Point Arithmetic.\nWhen we create a variable in Python, we do not need to explicitly declare what type that variable it is. That is what makes Python dynamically typed. Instead, it will infer the type based on the value. We can always ask Python how it is representing each variable, as seen in the following code.\n\u0026gt;\u0026gt;\u0026gt; type(\u0026#34;securePass1\u0026#34;) \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; \u0026gt;\u0026gt;\u0026gt; type(10) \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; \u0026gt;\u0026gt;\u0026gt; type(3.14) \u0026lt;class \u0026#39;float\u0026#39;\u0026gt; Basic Operators A programming language would be pretty useless if it did not offer some way to do basic arithmetic. Python supports the following arithmetic operators: +, -, *, /, //, ** and %. You may instantly recognize the first 4, but the last 3 may not be so familiar. Let us start with //, integer division.\nAn integer data type cannot represent decimal values. So what happens if you try to execute something like 1 / 2? We recognize this to be 0.5, but that is not the case with every programming language. In C, for example, 1 and 2 are treated as integer types by default. If you attempt to evaluate 1 / 2, the result is 0 since there is no way to store the decimal information. Essentially, the decimal portion of the result is truncated.\nPython is a little more forgiving. 1 / 2 evaluates to 0.5, as we may expect. However, if you want to perform division between these operands as if they were both integers, you can use //. Indeed, 1 // 2 evalutes to 0 in Python.\nThe ** operator is more straightforward: it raises the left-hand operand to whatever value is provided on the right side. For example, 2**4 evaluates to 16.\nFinally, the modulus operator % provides the remainder of integer division. So something like 5 % 2 would return 1.\nStatements and Expressions A statement is anything that can be executed. This could be a simple variable assignment or a function call.\nusername = \u0026#34;user1\u0026#34; print(username == \u0026#34;user1\u0026#34;) An expression is a statement that evaluates into some result. This could be the result of a function call, an assignment, or a complex computation.\ny = x**2 get_user_by_id(10) Basic I/O Python provides functions to read input from the user\u0026rsquo;s keyboard as well as print information back to the terminal using input and print. When input is evaluated, it will wait for the user to press Enter before processing the input. If you would like to provide a text prompt to the user before entering, you can pass the prompt as a string.\ntext = input(\u0026#34;Enter some text: \u0026#34;) print(text) An example run of this program may look like the following.\nEnter some text: OK here it is OK here it is Notice that the result of the input function is the actual data entered by the user. This is immediately assigned to the text variable. We can easily print out the value of text by passing it as an argument to the print function.\nFormatted Output We can work with more detailed output using formatted strings, as demonstrated in the following example.\npi = 3.14159265359 print(f\u0026#34;The value of pi is approximately {pi:.3f}.\u0026#34;) Output\nThe value of pi is approximately 3.141. More details about the different ways of using formatting strings is documented here.\nCommenting Code The last topic of this introduction is about commenting. Communicating the purpose of your program is not only important when working with others, but you will find it to be extremely helpful as you build larger and larger projects. It is a common trap to dive into an idea with absolute focus, quickly hacking away as your program takes shape. This sort of approach is like a house of cards. As soon as your attention is diverted, it takes time to build that model up in your head again.\nFigure 2: Daily experiences with programming. Writing down your program\u0026rsquo;s purpose and design while documenting its function is paramount for a product that is both robust and maintainable. The simplest way to communicate ideas is to leave comments in the code itself. There are two ways to leave basic comments in Python: single-line and multi-line. The code below demonstrates both.\n\u0026#34;\u0026#34;\u0026#34; This small code example shows how to comment in Python. By the way, this is a multi-line comment. \u0026#34;\u0026#34;\u0026#34; a = \u0026#34;user1\u0026#34; # stores the user\u0026#39;s name As see above, multi-line comments are wrapped in \u0026quot;\u0026quot;\u0026quot;. These are typically reserved for things like function documentation (more on that later). Single-line comments start with # and can be placed on the same line as a statement.\nThere is a third type of commenting called self-commenting. The same example above will motivate this type of commenting. There is nothing invalid about the statement a = \u0026quot;user1\u0026quot;. It defines a variable named a whose value is the string \u0026quot;user1\u0026quot;. However, if there wasn\u0026rsquo;t a comment on the same line describing its purpose, it might not be so clear. There is an easier way to communicate this without commenting at all. We could instead write something like username = \u0026quot;user1\u0026quot;. The variable name itself resolves any ambiguity about its purpose and obviates the need for an additional comment.\n","date":1692507600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692507600,"objectID":"d7999fc245dac666c793fcc58759b001","permalink":"https://ajdillhoff.github.io/notes/introduction_to_python/","publishdate":"2023-08-20T00:00:00-05:00","relpermalink":"/notes/introduction_to_python/","section":"notes","summary":"Table of Contents Introduction Programming with Python Variables, Values, and Data Types Basic Operators Statements and Expressions Basic I/O Commenting Code These notes are focused on introducing programming with Python for those without a technical background.\nIntroduction The official website provides the following description of Python.\nPython is an interpreted, interactive, object-oriented programming language. It incorporates modules, exceptions, dynamic typing, very high level dynamic data types, and classes. It supports multiple programming paradigms beyond object-oriented programming, such as procedural and functional programming.","tags":["python"],"title":"Introduction to Python","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Key Terms Defining Goals Policies and Values Bellman Equations Optimality Optimizing the Policy Key Terms Agent: The learner or decision maker. Environment: The world that the agent can interact with. State: A representation of the agent and environment. Action: The agent can take an action in the environment. Reward: Given to the agent based on actions taken. Goal: Maximize rewards earned over time.\nAt time \\(t\\), the agent observes the state of the environment \\(S_t \\in \\mathcal{S}\\) and can select an action \\(A_t \\in \\mathcal{A}(s)\\), where \\(\\mathcal{A}(s)\\) suggests that the available actions are dependent on the current state. At time \\(t + 1\\), the agent receives a reward \\(R_{t+1} \\in \\mathcal{R}\\).\nFigure 1: The agent-environment in a Markov decision process (Credit: Sutton \u0026amp; Barto). To improve its knowledge about an environment or increase its performance on a task, an agent must first be able to interpret or make sense of that environment in some way. Second, there must be a well defined goal. For an agent playing Super Mario, for example, the goal would be to complete each level while maximizing the score. Third, the agent must be able to interact with its environment by taking actions. If the Mario-playing agent could not move Mario around, it would never be able to improve. If the agent makes a decision which leads to Mario\u0026rsquo;s untimely demise, it would update its knowledge of the world so that it would tend towards a more favorable action. These three requirements: sensations, actions, and goals, are encapsulated by Markov Decision Processes.\nA Markov decision process is defined by\n\\(\\mathcal{S}\\) - a set of states, \\(\\mathcal{A}\\) - a set of actions, \\(\\mathcal{R}\\) - a set of rewards, \\(P\\) - the transition probability function to determine transition between states, \\(\\gamma\\) - discount factor for future rewards. At time \\(t\\), an agent in state \\(S_t\\) selects an action \\(A_t\\). At \\(t+1\\), it receives a reward \\(R_{t+1}\\) based on that action.\nIn a finite MDP, the states, actions, and rewards have a finite number of elements. Random variables \\(R_t\\) and \\(S_t\\) have discrete probability distributions dependent on the preceding state and action.\n\\[ p(s\u0026rsquo;, r|s, a) = P\\{S_t = s\u0026rsquo;, R_t = r|S_{t-1} = s, A_{t-1} = a\\} \\]\nIf we want the state transition probabilities, we can sum over the above distribution:\n\\[ p(s\u0026rsquo;|s, a) = P\\{S_t = s\u0026rsquo;|S_{t-1} = s, A_{t-1}=a\\} = \\sum_{r\\in\\mathcal{R}}p(s\u0026rsquo;, r|s, a). \\]\nThe reward function \\(r\\) gives the expected next reward given some state and action:\n\\[ r(s, a) = \\mathbb{E}[R_{t+1}|S_t=s, A_t=a] = \\sum_{r}r \\sum_{s\u0026rsquo;}p(s\u0026rsquo;, r|s, a). \\]\nDefining Goals In reinforcement learning, the goal is encoded in the form of a reward signal. The agent sets out to maximize the total amount of reward it receives over an episode. An episode is defined dependent on the problem context and ends in a terminal state. It could be a round of game, a single play, or the result of moving a robot. Typically, the rewards come as a single scalar value at teach time step. This implies that an agent might take an action that results in a negative reward if it is optimal in the long run.\nFormally, the expected return includes a discount factor that allows us to control the trade-off between short-term and long-term rewards:\n\\[ G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}, \\]\nwhere \\(0 \\leq \\gamma \\leq 1\\). This can be written in terms of the expected return itself as well:\n\\[ G_t = R_{t+1} + \\gamma G_{t+1}. \\]\nPolicies and Values Two important concepts that help our agent make decisions are the policy and value functions. A policy, typically denoted by \\(\\pi\\), maps states to actions. Such a function can be deterministic, \\(\\pi(s) = a\\), or stochastic, \\(\\pi(a|s)\\).\nThe value of a particular state under a policy \\(\\pi\\) is defined as\n\\[ v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t | S_t = s] = \\mathbb{E}_{\\pi}\\Bigg[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}\\Bigg|S_t=s\\Bigg]. \\]\nWe also must define the value of taking an action \\(a\\) in state \\(s\\) following policy \\(\\pi\\):\n\\[ q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_t|S_t=s, A_t=a] = \\mathbb{E}_{\\pi}\\Bigg[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}\\Bigg|S_t=s, A_t=a\\Bigg]. \\]\nThis function defines the expected return of following a particular policy and starting in state \\(s\\). Both the state-value and action-value functions can be updated as a result of the agent\u0026rsquo;s experience. How it is updated is method-dependent. Certain methods will also dictate how the policy itself can be updated.\nBellman Equations The recursive relationship between the value of a state and its future states can be represented using Bellman equations. In RL, we are interested in the equations for both the state-value and action-value. Given the diagram of an MDP, we can see that they are related to each other. To make the following equations easier to understand, it is important to remember the flow of a Markov decision process:\ntake an action, arrive at a state and sense the reward, consult the policy for the next action. With that in mind, let\u0026rsquo;s look at the state-value function first. This function considers the expected value of starting in a state \\(s\\) and following policy \\(\\pi\\). In other words, we must consider all possible actions and their future rewards.\n\\begin{align*} v_{\\pi}(s) \u0026amp;= \\mathbb{E}_{\\pi}[G_t | S_t = s]\\\\ \u0026amp;= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t = s]\\\\ \u0026amp;= \\sum_{a} \\pi(a|s) \\sum_{s\u0026rsquo;}\\sum_{r}p(s\u0026rsquo;, r|s, a)\\Big[r + \\gamma \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t = s]\\Big]\\\\ \u0026amp;= \\sum_{a} \\pi(a|s) \\sum_{s\u0026rsquo;, r}p(s\u0026rsquo;, r|s, a)\\Big[r + \\gamma v_{\\pi}(s\u0026rsquo;)\\Big]\\\\ \u0026amp;= \\sum_{a} \\pi(a | s)\\big[r(s, a) + \\gamma \\sum_{s\u0026rsquo;}p(s\u0026rsquo;|s, a)v_{\\pi}(s\u0026rsquo;)\\big]\\\\ \\end{align*}\nThe first sum over actions considers all possible actions. This is followed by a transition to possible states \\(s\u0026rsquo;\\) conditioned on taking each action multiplied by the expected value of being at the new state.\nThe action-value function follows a similar derivation:\n\\begin{align*} q_{\\pi}(s, a) \u0026amp;= \\mathbb{E}_{\\pi}[G_t|S_t = s, A_t = a]\\\\ \u0026amp;= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t = s, A_t = a]\\\\ \u0026amp;= r(s, a) + \\sum_{s\u0026rsquo;}p(s\u0026rsquo;|s, a) v_{\\pi}(s\u0026rsquo;) \\end{align*}\nThere is a very similar looking set of terms in the state-value function above, and we should expect that! If we want to evaluate the current state, we need to look ahead at the possible actions and their resulting rewards. Similarly, evaluating the current action requires us to look head at the value of future states.\nLet\u0026rsquo;s expand \\(q_{\\pi}(s,a)\\) once more so that it is written in terms of itself.\n\\begin{align*} q_{\\pi}(s, a) \u0026amp;= r(s, a) + \\gamma \\sum_{s\u0026rsquo;}p(s\u0026rsquo;|s, a) v_{\\pi}(s\u0026rsquo;)\\\\ \u0026amp;= r(s, a) + \\gamma \\sum_{s\u0026rsquo;}p(s\u0026rsquo;|s, a) \\sum_{a\u0026rsquo;} \\pi(s\u0026rsquo;, a\u0026rsquo;) \\big[r(s\u0026rsquo;, a\u0026rsquo;) + \\gamma \\sum_{s\u0026rsquo;\u0026rsquo;} p(s\u0026rsquo;\u0026rsquo;|s\u0026rsquo;, a\u0026rsquo;)v_{\\pi}(s\u0026rsquo;\u0026rsquo;)]\\\\ \u0026amp;= r(s, a) + \\gamma \\sum_{s\u0026rsquo;}p(s\u0026rsquo;|s, a) \\sum_{a\u0026rsquo;} \\pi(s\u0026rsquo;, a\u0026rsquo;) q_{\\pi}(s\u0026rsquo;, a\u0026rsquo;) \\end{align*}\nOptimality To solve a reinforcement learning problem, we are interested in finding the policy \\(\\pi_{*}\\) whose expected return is greater than all other possible policies over all states. An optimal policy will use an *optimal state-value function and optimal action-value function:\n\\begin{align*} v_{*}(s) \u0026amp;= \\max_{\\pi}v_{\\pi}(s)\\\\ q_{*}(s, a) \u0026amp;= \\max_{\\pi}q_{\\pi}(s, a). \\end{align*}\nThe optimal state-value function would select the best possible action instead of summing over all possibley actions starting in state \\(s\\):\n\\begin{align*} v_{*}(s) \u0026amp;= \\max_{a} q_{\\pi_*}(s, a)\\\\ \u0026amp;= \\max_{a}\\big[r(s, a) + \\gamma \\sum_{s\u0026rsquo;} p(s\u0026rsquo;|s, a) v_{*}(s\u0026rsquo;)\\big] \\end{align*}\nSimilarly, the optimal action-value function selects the best possible action from the next state \\(s\u0026rsquo;\\):\n\\[ q_{*}(s) = r(s, a) + \\gamma \\sum_{s\u0026rsquo;} p(s\u0026rsquo;|s, a) \\max_{a} q_{*}(s\u0026rsquo;, a\u0026rsquo;). \\]\nOptimizing the Policy For smaller problems with reasonably small state and action spaces, we can use Dynamic Programming to compute the optimal policy. These methods quickly become intractable as the complexity of our problem increases. As is common in machine learning, we would resort to approximation methods for complex spaces.\n\u0026ldquo;In fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.\u0026rdquo;\n\u0026ndash; Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction\nImagine if you had a set policy that dictated the actions you would take from work to home. In this example, assume the policy is not an optimal policy. One day, you decide to take a left at a particular intersection rather than going forward. After that, you follow your policy as described. If this decision ultimately resulted in you arriving home sooner, you would probably update your policy to always take that left. This intuition describes a result of the policy improvement theorem.\nLet \\(\\pi\\) and \\(\\pi\u0026rsquo;\\) be two deterministic policies where\n\\[ q_{\\pi}(s, \\pi\u0026rsquo;(s)) \\geq v_{\\pi}(s),\\quad \\forall s. \\]\n","date":1690174800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690174800,"objectID":"29ea88dc65c270723dd6982dc5081bf4","permalink":"https://ajdillhoff.github.io/notes/markov_decision_processes/","publishdate":"2023-07-24T00:00:00-05:00","relpermalink":"/notes/markov_decision_processes/","section":"notes","summary":"Table of Contents Key Terms Defining Goals Policies and Values Bellman Equations Optimality Optimizing the Policy Key Terms Agent: The learner or decision maker. Environment: The world that the agent can interact with. State: A representation of the agent and environment. Action: The agent can take an action in the environment. Reward: Given to the agent based on actions taken. Goal: Maximize rewards earned over time.\nAt time \\(t\\), the agent observes the state of the environment \\(S_t \\in \\mathcal{S}\\) and can select an action \\(A_t \\in \\mathcal{A}(s)\\), where \\(\\mathcal{A}(s)\\) suggests that the available actions are dependent on the current state.","tags":["reinforcement learning"],"title":"Markov Decision Processes","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Notes from (Friedman 2001) Notes from (Friedman 2001) Many machine learning methods are parameterized functions that are optimized using some numerical optimization techniques, notably steepest-descent.\nInitial learner is a stump, subsequent learners are trees with depth as some power of 2 (commonly).\nNumerical optimization in function space \\[ g_m(\\mathbf{x}) = E_y\\Big[\\frac{\\partial L(y, F(\\mathbf{x}))}{\\partial F(\\mathbf{x})}|\\mathbf{x}\\Big]_{F(\\mathbf{x})=F_{m-1}(\\mathbf{x})} \\] The optimal step size found by solving\n\\[ \\rho_m = \\mathop{\\arg \\min}_{\\rho} E_{y,\\mathbf{x}}L(y,F_{m-1}(\\mathbf{x})-\\rho g_m(\\mathbf{x})) \\] Then the function \\(m\\) is updated: \\[ f_m(\\mathbf{x}) = -\\rho_m g_m(\\mathbf{x}) \\]\nWalking through it\u0026hellip;\nMake an initial guess with \\(f_0(\\mathbf{x})\\)\nEvaluate \\(L(y, f_0(\\mathbf{x}))\\)\nImprove model by boosting \\(f_1(\\mathbf{x}) = -\\rho_1 g_1(\\mathbf{x})\\), where \\[ g_1(\\mathbf{x}) = \\frac{\\partial L(y, f_0(\\mathbf{x}))}{\\partial f_0(\\mathbf{x})}. \\] This implies that \\(f_1\\) is predicting the gradient of the previous function.\nIf the model is nonparametric, the expected value of the function conditioned on the input cannot be estimated accurately because we cannot sample the entire distribution of \\(\\mathbf{x}\\). The author\u0026rsquo;s note that \u0026ldquo;\u0026hellip;even if it could, one would like to estimate \\(F^*(\\mathbf{x})\\) at \\(\\mathbf{x}\\) values other than the training sample points.\u0026rdquo;\nSmoothness is imposed by approximating the function with a parametric model. I think this means that the distribution is approximated as well.\n\\begin{equation} (\\beta_m, \\mathbf{a}_m) = \\mathop{\\arg \\min}_{\\beta, \\mathbf{a}}\\sum_{i=1}^N L(y_i, F_{m-1}(\\mathbf{x}_i) + \\beta h(\\mathbf{x}_i; \\mathbf{a})) \\end{equation}\nWhat if a solution to the above equation is difficult to obtain? Instead, view \\(\\beta_m h(\\mathbf{x};\\mathbf{a}_m)\\) as the best greedy step toward \\(F^*(\\mathbf{x})\\), under the constraint that the step direction, in this case \\(h(\\mathbf{x};\\mathbf{a}_m)\\), is a member of the class of functions \\(h(\\mathbf{x};\\mathbf{a})\\). The negative gradient can be evaluated at each data point: \\[ -g_m(\\mathbf{x}_i) = -\\frac{\\partial L(y_i, F_{m-1}(\\mathbf{x}_i))}{\\partial F_{m-1}(\\mathbf{x}_i)}. \\]\nThis gradient is evaluated at every data point. However, we cannot generalize to new values not in our dataset. The proposed solution comes via \\(\\mathbf{h}_m = \\{h(\\mathbf{x}_i;\\mathbf{a}_m)\\}_{1}^N\\) \u0026ldquo;most parallel to\u0026rdquo; \\(-\\mathbf{g}_m \\in \\mathbb{R}^N\\).\nAs long as we can compute a derivative for the original loss function, our subsequent boosting problems are solved via least-squared error: \\[ \\mathbf{a}_m = \\mathop{\\arg \\min}_{\\mathbf{a}, \\beta} \\sum_{i=1}^N \\Big(-g_m(\\mathbf{x}_i)-\\beta h(\\mathbf{x}_i;\\mathbf{a})\\Big)^2 \\]\nFigure 1: Original generic algorithm from (Friedman 2001). Check out a basic implementation in Python here.\nReferences Friedman, Jerome H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” The Annals of Statistics 29 (5): 1189–1232. https://www.jstor.org/stable/2699986. ","date":1689570000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720310400,"objectID":"f575924412ebf4cfa533e035a22bcd88","permalink":"https://ajdillhoff.github.io/notes/gradient_boosting/","publishdate":"2023-07-17T00:00:00-05:00","relpermalink":"/notes/gradient_boosting/","section":"notes","summary":"Table of Contents Notes from (Friedman 2001) Notes from (Friedman 2001) Many machine learning methods are parameterized functions that are optimized using some numerical optimization techniques, notably steepest-descent.\nInitial learner is a stump, subsequent learners are trees with depth as some power of 2 (commonly).\nNumerical optimization in function space \\[ g_m(\\mathbf{x}) = E_y\\Big[\\frac{\\partial L(y, F(\\mathbf{x}))}{\\partial F(\\mathbf{x})}|\\mathbf{x}\\Big]_{F(\\mathbf{x})=F_{m-1}(\\mathbf{x})} \\] The optimal step size found by solving\n\\[ \\rho_m = \\mathop{\\arg \\min}_{\\rho} E_{y,\\mathbf{x}}L(y,F_{m-1}(\\mathbf{x})-\\rho g_m(\\mathbf{x})) \\] Then the function \\(m\\) is updated: \\[ f_m(\\mathbf{x}) = -\\rho_m g_m(\\mathbf{x}) \\]","tags":["machine learning"],"title":"Gradient Boosting","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":"Introduction Hidden Markov Models provide a way of modeling the dynamics of sequential information. They have been used for speech recognition, part-of-speech tagging, machine translation, handwriting recognition, and, as we will see in this article, gesture recognition.\nConsider a somewhat practical use-case: you are going to throw a party with a meticulously curated playlist. You would rather not let anyone have the remote as it might get lost, and letting anyone interrupt the playlist with their own selections may derail the entire event. However, you still want to give your guests the ability to control the volume and skip back and forth between tracks in the playlist. We will also assume that guests will use change tracks and control the volume responsibly.\nThe solution to this problem is to implement a gesture recognition system to identify simple hand motions. In this case, we only have to model 4 separate gestures: VolumeUp, VolumeDown, PrevTrack, NextTrack. Since the motions are temporal in nature, we can model each gesture using Hidden Markov Models. First, we need to cover a bit of background on what a Hidden Markov Model actually is.\nBackground First, introduce Markov Chains Then the Markov assumption At the core of our problem, we want to model a distribution over a sequence of states. Consider a sequence of only 3 states \\(p(x_1, x_2, x_3)\\). The full computation of this can be done using the chain rule of probability:\n\\[ p(x_1, x_2, x_3) = p(x_1) p(x_2 | x_1) p(x_3 | x_1, x_2). \\]\nIf the random variables of our problem are not conditionally independent, the complexity of calculating this is exponential in the number of random variables.\nThe Markov in Hidden Markov Models addresses this complexity. The Markov Assumption states that the probability of an event at time \\(t\\) is conditioned only on the previously observed event: \\(p(x_t | x_{t-1})\\). This is compactly represented with a graphical model, as seen in figure TODO.\nTODO: Figure of basic Markov Chain\nThe hidden qualifier comes from the fact that the data we wish to model was generated from some underlying process that is not directly observable. A classic example for HMMs uses the weather. Imagine you had a log which had the number of water bottles a person had drank per day over the entire year. To make the problem slightly more difficult, the log entries were not associated with a date. It is reasonable to say that the amount of water a person drinks is influenced by how hot or cold it is on a particular day. So, the hidden state in this case is the weather: hot or cold. We can model this with an HMM by establishing that the amount of water (observed state) is conditioned on the weather (hidden state). Figure TODO shows this HMM graphically.\nFigure 1: An HMM with 4 states and 2 observation symbols (y_1) or (y_2). Formally, a Hidden Markov Model is defined by\nThe number of hidden states \\(N\\). A transition probability matrix \\(A \\in \\mathbb{R}^{N \\times N}\\), where \\(a_{ij} = p(z_t = j | z_{t-1} = i)\\). An observation symbol probability distribution \\(B = \\{b_j(k)\\} = p(\\mathbf{x}_t = k | z_t = j)\\). An initial state distribution \\(\\pi_i = p(z_t = i)\\). The trainable parameters of our model are \\(\\lambda = (A, B, \\pi)\\).\nFunctions of an HMM Given the basic definition of what an HMM is, how can we train the parameters defined in \\(\\lambda\\). If we somehow already knew the parameters, how can we extract useful information from the model? Depending on our task, we can use HMMs to answer many important questions:\nFiltering computes \\(p(z_t | \\mathbf{x}_{1:t})\\). That is, we are computing this probability as new samples come in up to time \\(t\\). Smoothing is accomplished when we have all the data in the sequence. This is expressed as \\(p(z_t|\\mathbf{x}_{1:T})\\). Fixed lag smoothing allows for a trade off between accuracy and delay. It is useful in cases where we might not have the full sequence, but we wish to compute \\(p(z_{t-l}|\\mathbf{x}_{1:t})\\) for some \\(l \u0026gt; 0\\). Predictions are represented as \\(p(z_{t+h}|\\mathbf{x}_{1:t})\\), where \\(h \u0026gt; 0\\). MAP estimation yields the most probably state sequence \\(\\text{arg}\\max_{\\mathbf{z}_{1:T}}p(\\mathbf{z}_{1:T}|\\mathbf{x}_{1:T})\\). We can sample the posterior \\(p(\\mathbf{z}_{1:T}|\\mathbf{x}_{1:T})\\). We can also compute \\(p(\\mathbf{x}_{1:T})\\) by summing up over all hidden paths. This is useful for classification tasks. Of course not all of these functions make sense for every possible task, more on that later. This article is not meant to be an exhaustive resource for all HMM functions; we will only look at the tasks necessary to train and use HMMs for isolated gesture recognition TODO: offer additional reading suggestions.\nData Processing As far as the efficacy of our model goes, how we process the data is the most important. Our system will start with a camera that records our guests performing one of the four simple motions. For simplicity, let\u0026rsquo;s pretend that the camera has an onboard chip that detects the 2D centroids of the left hand for each frame. That helps a lot, but there is still the problem of isolating a group of frames based on when the user wanted to start and finish the command. Assuming we have a solution for both of these problems, we still need to take into account that users will gesture at different speeds. Since all of these problems are challenging in their own right, we will assume the computer vision fairy has taken care of this for us.\nEach gesture in our dataset consists of 30 \\((x, y)\\) locations of the center of the left hand with respect to image coordinates. Even with this simplified data, we have another problem: different users may gesture from different locations. The hand locations for one user performing the VolumeUp gesture may be vastly different from another. This isn\u0026rsquo;t too bad to deal with. We could normalize or training data by subtracting the location of the hand in the first frame from the gesture. That way every input would start at \\((0, 0)\\). We can simplify this even further by using relative motion states.\nRelative Motion States Relative motion states discretize our data, thus simplifying the input space. The idea is quite simple: if the hand moved to the right relative to the previous frame, we assign \\(x = 1\\) for that frame. If it moved to the left, assign \\(x = -1\\). If it didn\u0026rsquo;t move at all, or did not move a significant amount, assign \\(x = 0\\). We apply similar rules for the \\(y\\) locations as well. The TODO: figure below shows the relative motion grid.\nBesides greatly simplifying our input space, meaning we can use a simple categorical distribution to model these observations, we no longer have to worry about the discrepency between where each user performed the gesture.\nModeling a Gesture Our system will consist of 4 HMM models to model the dynamics of each gesture. To determine which gesture was performed, we will given our input sequence to each one and have it compute \\(p(\\mathbf{x}_{1:T}; \\lambda_i)\\), the probability of the observation given the parameters of model \\(i\\). Whichever model gives the high probability wins.\nTODO\nDescribe EM at a high level, show the breakdown of probabilities that need to be known Go into forward-backwards Go back to EM and plug them in Training: Expectation-Maximization If we cannot observe the hidden states directly, how are we supposed to update the model parameters \\(\\lambda = (A, B, \\pi)\\)? We may not have all of the information, but we do have some information. We can use that to fill in the missing values with what we would expect them to be given what we already know. Then, we can update our parameters using those expected values. This is accomplished through a two-stage algorithm called Expectation-Maximization. Those familiar with k-Nearest Neighbors should already be familiar with this process.\nUpdating with Perfect Information It is useful to know how we would update our parameters assuming we had perfect information. If the hidden states were fully observable, then updating our model parameters would be as straightforward as computing the maximum likelihood estimates. For \\(A\\) and \\(\\pi\\), we first tally up the following counts:\n\\[ \\hat{a}_{ij} = \\frac{N_{ij}}{\\sum_j N_{ij}}, \\]\nthe number of times we expect to transition from \\(i\\) to \\(j\\) divided by the number of times we transition from \\(i\\) to any other state. Put simply, this computes the expected transitions from \\(i\\) to \\(j\\) normalized by all the times we expect to start in state \\(i\\).\nFor \\(\\pi\\), we have\n\\[ \\hat{\\pi_i} = \\frac{N_i}{\\sum_i N_i}, \\]\nthe number of times we expect to start in state \\(i\\) divided by the number of times we start in any other state.\nEstimating the parameters for \\(B\\) depends on which distribution we are using for our observation probabilities. For a multinomial distribution, we would compute the number of times we are in state \\(j\\) and observe a symbol \\(k\\) divided by the number of times we are in state \\(j\\):\n\\[ \\hat{b}_{jk} = \\frac{N_{jk}}{N_k}, \\]\nwhere\n\\[ N_{jk} = \\sum_{i=1}^N \\sum_{t=1}^T \\mathbb{1} (z_{i, t}=j, x_{i, t}=k). \\]\nIt is also common to model our emission probability using a Normal distribution. We can even use a parameterized model like a neural network. TODO: provide links to examples of these\nUpdating with Missing Information Now to the real problem: fill in our missing information using our observable data and the current parameter estimates. There are two important statistics that we need to compute, called the sufficient statistics.\nThe expected number of transitions from \\(i\\) to \\(j\\). The expected number of times we are transitioning from \\(i\\) to any other state. Both of these can be computed starting with the same probability conditioned on our observable data:\n\\[ p(z_t = i, z_{t+1} = j|\\mathbf{x}_{1:T}). \\]\nForwards-Backwards Algorithm Computing joint distribution can be very computationally expensive. Fortunately for us, the Markov assumption along with operations on graphs open the door to a dynamic programming approach named the Forward-Backward algorithm.\nThe Forwards-Backwards Algorithm, also known as the Baum-Welch algorithm, provides an effective solution to computing the joint described above. In fact, there are many useful distributions that can be computed with this algorithm such as the filtering and smoothing tasks.\nForward Probability The forward probability, often denoted as \\(\\alpha\\), represents the probability of ending up at a particular hidden state \\(i\\) at time \\(t\\) having seen the observations up to that time:\n\\[ \\alpha_t(i) = p(z_t = i, \\mathbf{x}_{1:t} | \\lambda). \\]\nThis value is computed recursively starting from \\(t=1\\) and going forwards to \\(t=T\\).\nInitialization\nFor \\(t=1\\), we calculate:\n\\[ \\alpha_1(i) = \\pi_i b_i(x_1),\\quad 1 \\leq i \\leq N, \\]\nwhere \\(\\pi_i\\) is the initial probability of state \\(i\\) and \\(b_i(x_1)\\) is the emission probability of the first observation \\(x_1\\) given that we are in state \\(i\\).\nRecursion\nAfter that, we calculate the remaining \\(\\alpha_t(i)\\) as follows:\n\\[ \\alpha_{t+1}(j) = b_j(x_{t+1}) \\sum_{i=1}^{N} \\alpha_{t}(i)a_{ij}, \\]\nwhere \\(N\\) is the number of hidden states, and \\(a_{ij}\\) is the transition probability from state \\(i\\) to state \\(j\\).\nBackward Probability The backward probability, denoted as \\(\\beta\\), gives the probability of observing the remaining observations from time \\(t+1\\) to \\(T\\) given that we are in state \\(i\\) at time \\(t\\):\n\\[ \\beta_t(i) = p(\\mathbf{x}_{t+1:T} | z_t = i, \\lambda). \\]\nAgain, this is calculated recursively but this time starting from \\(t=T\\) and going backwards to \\(t=1\\).\nInitialization\nFor \\(t=T\\), we initialize:\n\\[ \\beta_T(i) = 1, \\forall i. \\]\nRecursion\nThen we calculate the remaining \\(\\beta_t(i)\\) as:\n\\[ \\beta_{t}(i) = \\sum_{j=1}^{N} a_{ij}b_j(x_{t+1})\\beta_{t+1}(j). \\]\nCalculating the Sufficient Statistics With these two sets of probabilities, we can calculate the two required sufficient statistics as follows:\nThe expected number of transitions from \\(i\\) to \\(j\\): \\[ \\frac{\\sum_{t=1}^{T-1} \\alpha_t(i) a_{ij} b_j(x_{t+1}) \\beta_{t+1}(j)}{P(X|\\lambda)} \\]\nThe expected number of times we are transitioning from \\(i\\) to any other state: \\[ \\frac{\\sum_{t=1}^{T-1} \\alpha_t(i) \\beta_t(i)}{P(X|\\lambda)} \\]\nWhere \\(P(X|\\lambda)\\) is the total probability of the observations, calculated as:\n\\[ P(X|\\lambda) = \\sum_{i=1}^{N} \\alpha_T(i) \\]\nHow does this give us \\(p(z_t = i, z_{t+1} = j|\\mathbf{x}_{1:T})\\)? To understand how the variables of the Forwards-Backwards algorithm relate to the original probabilities, we can express the term \\(p(z_t = i, z_{t+1} = j|\\mathbf{x}_{1:T})\\) in terms of the original probability distributions in the HMM:\n\\(\\pi_i\\) - the probability of starting in state \\(i\\), \\(a_{ij}\\) - the probability of transitioning from state \\(i\\) to state \\(j\\), \\(b_j(x_t)\\) - the probability that state \\(j\\) will emit observation \\(x_t\\). The joint probability \\(p(z_t = i, z_{t+1} = j, \\mathbf{x}_{1:T})\\) would represent the probability of being in state \\(i\\) at time \\(t\\), moving to state \\(j\\) at time \\(t+1\\), and observing the sequence of emissions \\(\\mathbf{x}_{1:T}\\). This can be factored as follows due to the Markov property:\n\\[ p(z_t = i, z_{t+1} = j, \\mathbf{x}_{1:T}) = p(\\mathbf{x}_{1:t}, z_t = i)p(z_{t+1} = j| z_t = i)p(\\mathbf{x}_{t+1:T} | z_{t+1} = j, \\mathbf{x}_{1:t}). \\]\nUsing our definitions of \\(\\alpha\\) and \\(\\beta\\), we can rewrite this in terms of our HMM quantities:\n\\[ p(z_t = i, z_{t+1} = j, \\mathbf{x}_{1:T}) = \\alpha_t(i)a_{ij}b_j(x_{t+1})\\beta_{t+1}(j). \\]\nHere, \\(\\alpha_t(i)\\) represents \\(p(\\mathbf{x}_{1:t}, z_t = i)\\), the joint probability of the observations until time \\(t\\) and being in state \\(i\\) at time \\(t\\), and \\(\\beta_{t+1}(j)\\) represents \\(p(\\mathbf{x}_{t+1:T} | z_{t+1} = j)\\), the probability of the observations from time \\(t+1\\) to \\(T\\) given we\u0026rsquo;re in state \\(j\\) at time \\(t+1\\).\nThen, to obtain \\(p(z_t = i, z_{t+1} = j|\\mathbf{x}_{1:T})\\), we divide by \\(p(\\mathbf{x}_{1:T})\\) to normalize the probabilities, which is the sum over all states of \\(\\alpha_T(i)\\), or equivalently, the sum over all states of \\(\\beta_1(i)\\pi_i b_i(x_1)\\).\nThis gives us:\n\\[ p(z_t = i, z_{t+1} = j|\\mathbf{x}_{1:T}) = \\frac{\\alpha_t(i)a_{ij}b_j(x_{t+1})\\beta_{t+1}(j)}{\\sum_{i=1}^{N}\\alpha_T(i)}. \\]\nThis is the same expression as before, but broken down in terms of the original HMM quantities and the forward and backward variables. This can also be explained through graph properties and operations. See Sargur Srihari\u0026rsquo;s excellent lecture slides for more details.\nImplementation in Python Conclusion ","date":1689397200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689397200,"objectID":"f274eff17c529462e71940e1c4965e60","permalink":"https://ajdillhoff.github.io/articles/intro_to_hmms/","publishdate":"2023-07-15T00:00:00-05:00","relpermalink":"/articles/intro_to_hmms/","section":"articles","summary":"Hidden Markov Models provide a way of modeling the dynamics of sequential information. They have been used for speech recognition, part-of-speech tagging, machine translation, handwriting recognition, and, as we will see in this article, gesture recognition.","tags":["machine learning"],"title":"An Introduction to Hidden Markov Models for Gesture Recognition","type":"articles"},{"authors":null,"categories":null,"content":" Table of Contents Topics Introduction Definition Markov Decision Processes RL vs. MDP Passive RL Resources Topics What is reinforcement learning? Examples Finite Markov Decision Processes Passive vs. Active Methods Adaptive Dynamic Programming Monte Carlo Methods Temporal-Different Learning Q-Learning Function Approximation Deep Q-Learning Policy and Value Iteration Case Studies Introduction When placed in a new environment, we orient and learn about it by interacting with it. When given a novel task, we may not be able to explicitly describe the task or even perform well at it, but we can learn a lot about it through trial and error.\nImage if you were to be thrown into an alien world teaming with unknown life forms. You would not be able to identify these life forms, but you would be able to learn about their behaviors, shape, surface anatomy, and other attributes based on perception alone. Simply learning about the structure of the environment is the task of unsupervised learning.\nIn supervised machine learning, there is typically some objective function that is minimized based on ground truth or target variables that are known ahead of time. In settings like the ones depicted above, there is no form of supervision. Instead, a representation of the environment is learned from one\u0026rsquo;s experience and sensory input.\nReinforcement learning maps environment input to actions. For example, an agent trained to play Chess will evaluate the current board and make a decision based on experience. Whether or not that decision was beneficial may not immediately be known. If it results in winning against an opponent, the model would strengthen its knowledge of that particular play. The concept of trial-and-error and the fact that an agent may not know if the correct decision was made until later are two of the most important concepts of reinforcement learning.\nThere are many challenges present in reinforcement learning. The agents consider the entire environment and attempt to maximize some reward value based on a well defined goal. There is then a trade off between exploiting actions that are known to give a positive reward and exploring new actions that may lead to a better payoff.\nReinforcement learning is the study of goal-seeking agents that can sense some aspect of their environment and can perform actions that directly affect that environment. An agent is not always a robot, although it is a popular use case. Taking from recent popularity, agents such as AlphaGo exist within a game playing environment where their goal is win.\nExamples Gaming: Agents for traditional board games such as Chess and Go have been implemented, surpassing even the best players in the world. Video games are also a popular environment for constructing complex agents. Developers can use RL to construct sophisticated characters in the game that behave in complex way as well as react more realistically towards the player. DeepMind introduced a powerful agent for StarCaft II which beat some of the best human players in the world. OpenAI debuted OpenAI Five at the 2019 Dota 2 World Championships, defeating the championship team back-to-back.\nOpenAI released OpenAI gym, now an open source project named Gymnasium, which is a library for developing and comparing reinforcement learning algorithms. One of recent interest is RLGym, a Rocket League agent.\nDefinition A reinforcement learning system is identified by the environment and an agent acting in that environment. An agent\u0026rsquo;s behavior in the environment is defined by a policy. A policy usually describes a set of rules in response to the current state of the environment.\nIn order to improve on some task, a reward signal is defined. Over time, an agent should maximize the reward. In general, an action leading to a favorable outcome should present a high reward value.\nA value function describes the estimated total reward over the long run given the agent\u0026rsquo;s current state. Taking an action that results in an immediate reward may lead to a lower payoff in the long run. This can be predicted from the value function.\nAdditionally, a model of the environment will include prior knowledge about that environment. This allows the agent to act optimally over a longer sequence of states.\nMarkov Decision Processes RL vs. MDP Reinforcement learning does not assume the transition model or the reward function.\nPassive RL The policy is fixed while the transition model and reward function are learned over time. The goal is to compute the utility of each state.\nAdaptive Dynamic Programming \\(R(s)\\) and \\(p(s\u0026rsquo;|s,a)\\) can be updated at each step based solely on the observations. Using these observations, the utility of each state can be estimated following policy evaluation (TODO: link algorithm for policy evaluation).\nResources https://web.stanford.edu/class/cs234/modules.html https://lilianweng.github.io/posts/2018-02-19-rl-overview/ ","date":1689138000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689138000,"objectID":"ecba1c8a3e4b25ff5f3fb7fdc2a0f549","permalink":"https://ajdillhoff.github.io/notes/reinforcement_learning/","publishdate":"2023-07-12T00:00:00-05:00","relpermalink":"/notes/reinforcement_learning/","section":"notes","summary":"Table of Contents Topics Introduction Definition Markov Decision Processes RL vs. MDP Passive RL Resources Topics What is reinforcement learning? Examples Finite Markov Decision Processes Passive vs. Active Methods Adaptive Dynamic Programming Monte Carlo Methods Temporal-Different Learning Q-Learning Function Approximation Deep Q-Learning Policy and Value Iteration Case Studies Introduction When placed in a new environment, we orient and learn about it by interacting with it. When given a novel task, we may not be able to explicitly describe the task or even perform well at it, but we can learn a lot about it through trial and error.","tags":null,"title":"Reinforcement Learning","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Generalization Bias Variance Bias-Variance Tradeoff Generalization When fitting machine learning models to data, we want them to generalize well to the distribution that we have sampled from. We can measure a model\u0026rsquo;s ability to generalize by evaluating it on previously unseen data that is sampled from the same distribution as the training set. However, we often do not know the true underlying distribution. So we must fit the models to empirical distributions derived from observed data.\nMeasuring bias and variance is crucial for determining the quality of a model. Bias refers to the difference between the average prediction of a model and the correct value we are trying to predict. A model with high bias oversimplifies the problem and leads to high error on both training and test data. Variance refers to the sensitivity of a model to fluctuations in the training set. High variance suggests that the model\u0026rsquo;s performance changes significantly when it is fit on different samplings of the training data, which can lead to overfitting.\nTo achieve good generalization, it is essential to find a balance between bias and variance, minimizing the total error. This can be done by selecting appropriate model complexity and using regularization techniques to prevent overfitting or underfitting. Additionally, model validation techniques, such as hold-out validation and cross-validation, can be employed to assess a model\u0026rsquo;s ability to generalize to unseen data.\nBias Consider fitting a simple linear model to nonlinear data. The model will not be able to generalize well, regardless of the size of the training set. In fact, it would also exhibit poor performance when evaluated on the training set as well. When a model has not learned the patterns in the training data and is likewise unable to generalize to new data, it is known as underfitting. In this case, such a model has high bias.\nFigure 1: Regardless of the dataset sampled, a linear model exhibits high bias. Variance Variance is described in terms of the model fitting procedure and the training data. In terms of data, variance measures dispersion. It could also be interpreted as a measure of diversity. Sets with low variance contain samples that are close to the mean, and sampling from such a set would produce rather consistent data points.\nIn terms of model fitting, a model that fits the training data well but not the test data describes overfitting. This is because the training data is only an empirical sample of the true underlying distribution. A different sampling of the distribution may yield a set that more closely resembles the test set. Due to the variance of the underlying distribution, our model overfits the patterns that exist in the training set.\nFigure 2: A 5th degree polynomial trained on 3 different samplings of the distribution. Bias-Variance Tradeoff If a model is not complex enough to capture the underlying distribution, it will perform poorly on both the training and test sets. Indeed, the model has low bias. If the model is too complex, it will exhibit low bias and high variance, overfitting the training set while failing to generalize well to unseen data. The solution then is to find a tradeoff between bias and variance with respect to the model complexity.\n","date":1688446800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688446800,"objectID":"957c85788398153f94cf803baeab4aad","permalink":"https://ajdillhoff.github.io/notes/bias_and_variance/","publishdate":"2023-07-04T00:00:00-05:00","relpermalink":"/notes/bias_and_variance/","section":"notes","summary":"Table of Contents Generalization Bias Variance Bias-Variance Tradeoff Generalization When fitting machine learning models to data, we want them to generalize well to the distribution that we have sampled from. We can measure a model\u0026rsquo;s ability to generalize by evaluating it on previously unseen data that is sampled from the same distribution as the training set. However, we often do not know the true underlying distribution. So we must fit the models to empirical distributions derived from observed data.","tags":["machine learning"],"title":"Bias and Variance","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Text Preprocessing Tasks Models Perplexity Introduction Text Preprocessing Character-level tokenization Word-level tokenization Subword tokenization Stopwords Batching Padding Unsupervised Pre-Training Autoregression BERT loss Tasks Text Classification Named Entity Recognition Question Answering Summarization Translation Text Generation Text Preprocessing Text preprocessing is an essential step in NLP that involves cleaning and transforming unstructured text data to prepare it for analysis. Some common text preprocessing techniques include:\nExpanding contractions (e.g., \u0026ldquo;don\u0026rsquo;t\u0026rdquo; to \u0026ldquo;do not\u0026rdquo;) [7] Lowercasing text[7] Removing punctuations[7] Removing words and digits containing digits[7] Removing stopwords (common words that do not carry much meaning) [7] Rephrasing text[7] Stemming and Lemmatization (reducing words to their root forms) [7] Common Tokenizers Tokenization is the process of breaking a stream of textual data into words, terms, sentences, symbols, or other meaningful elements called tokens. Some common tokenizers used in NLP include:\nWhitespace Tokenizer: Splits text based on whitespace characters (e.g., spaces, tabs, and newlines) [2]. NLTK Tokenizer: A popular Python library that provides various tokenization functions, including word and sentence tokenization[1]. SpaCy Tokenizer: Another popular Python library for NLP that offers a fast and efficient tokenizer, which can handle large documents and is customizable[5]. WordPiece Tokenizer: A subword tokenizer used in models like BERT, which breaks text into smaller subword units to handle out-of-vocabulary words more effectively[3]. Byte Pair Encoding (BPE) Tokenizer: A subword tokenizer that iteratively merges the most frequent character pairs in the text, resulting in a vocabulary of subword units[12]. SentencePiece Tokenizer: A library that provides both BPE and unigram-based subword tokenization, which can handle multiple languages and does not rely on whitespace for tokenization[6]. These tokenizers differ in the way they split text into tokens and handle language-specific considerations, such as handling out-of-vocabulary words, dealing with punctuation, and managing whitespace characters. The choice of tokenizer depends on the specific NLP task and the characteristics of the text data being processed.\nCitations: [1] https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/ [2] https://neptune.ai/blog/tokenization-in-nlp [3] https://towardsdatascience.com/comparing-transformer-tokenizers-686307856955 [4] https://www.analyticsvidhya.com/blog/2021/09/essential-text-pre-processing-techniques-for-nlp/ [5] https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/ [6] https://www.reddit.com/r/MachineLearning/comments/rprmq3/d_sentencepiece_wordpiece_bpe_which_tokenizer_is/ [7] https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/ [8] https://towardsdatascience.com/top-5-word-tokenizers-that-every-nlp-data-scientist-should-know-45cc31f8e8b9 [9] https://www.projectpro.io/recipes/explain-difference-between-word-tokenizer [10] https://www.telusinternational.com/insights/ai-data/article/what-is-text-mining [11] https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4 [12] https://towardsdatascience.com/a-comprehensive-guide-to-subword-tokenisers-4bbd3bad9a7c [13] https://towardsdatascience.com/text-preprocessing-in-natural-language-processing-using-python-6113ff5decd8 [14] https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/ [15] https://docs.tamr.com/new/docs/tokenizers-and-similarity-functions [16] https://pitt.libguides.com/textmining/preprocessing [17] https://medium.com/@ajay_khanna/tokenization-techniques-in-natural-language-processing-67bb22088c75 [18] https://datascience.stackexchange.com/questions/75304/bpe-vs-wordpiece-tokenization-when-to-use-which [19] https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing [20] https://www.tokenex.com/blog/ab-what-is-nlp-natural-language-processing-tokenization/ [21] https://hungsblog.de/en/technology/learnings/difference-between-the-tokenizer-and-the-pretrainedtokenizer-class/ [22] https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html [23] https://medium.com/nlplanet/two-minutes-nlp-a-taxonomy-of-tokenization-methods-60e330aacad3 [24] https://www.geeksforgeeks.org/text-preprocessing-in-python-set-1/ [25] https://medium.com/@utkarsh.kant/tokenization-a-complete-guide-3f2dd56c0682 [26] https://stackoverflow.com/questions/380455/looking-for-a-clear-definition-of-what-a-tokenizer-parser-and-lexers-are [27] https://blog.floydhub.com/tokenization-nlp/ [28] https://medium.com/product-ai/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908 [29] https://pub.towardsai.net/in-depth-tokenization-methods-of-14-nlp-libraries-with-python-example-297ecdd14c1 [30] https://datascience.stackexchange.com/questions/88680/what-is-the-difference-between-countvectorizer-and-tokenizer-or-are-they-the [31] https://www.freecodecamp.org/news/train-algorithms-from-scratch-with-hugging-face/ [32] https://exchange.scale.com/public/blogs/preprocessing-techniques-in-nlp-a-guide [33] https://huggingface.co/docs/transformers/tokenizer_summary [34] https://blog.octanove.org/guide-to-subword-tokenization/ [35] https://www.enjoyalgorithms.com/blog/text-data-pre-processing-techniques-in-ml/ [36] https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/\nTasks Text Classification Commonly seen in the form of sentiment analysis, where the objective is to classify whether some input text is positive or negative. Document classification, in which documents are identified by their content, is also useful.\nNamed Entity Recognition Extract important nouns from a body of text.\nQuestion Answering Summarization Translation Text Generation Generate text from a prompt. This could be in the form of a simple question or some initial dialog. This is also seen in tools like GitHub Co-Pilot to generate code based on contextual code in the same project.\nModels Discuss GPT2\nPerplexity A measure of confidence of a language model. A naive model may predict a word by randomly selecting any of the \\(N\\) words in its vocabulary. As the model is optimized and the distribution of possible sequences is acquired, the perplexity decreases.\n","date":1682226000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682226000,"objectID":"c130cc8d69fc0079b385a484f01820d6","permalink":"https://ajdillhoff.github.io/notes/natural_language_processing/","publishdate":"2023-04-23T00:00:00-05:00","relpermalink":"/notes/natural_language_processing/","section":"notes","summary":"Table of Contents Introduction Text Preprocessing Tasks Models Perplexity Introduction Text Preprocessing Character-level tokenization Word-level tokenization Subword tokenization Stopwords Batching Padding Unsupervised Pre-Training Autoregression BERT loss Tasks Text Classification Named Entity Recognition Question Answering Summarization Translation Text Generation Text Preprocessing Text preprocessing is an essential step in NLP that involves cleaning and transforming unstructured text data to prepare it for analysis. Some common text preprocessing techniques include:\nExpanding contractions (e.","tags":["deep learning","NLP"],"title":"Natural Language Processing","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Definition Attention Key-value Store Scaled Dot Product Attention Multi-Head Attention Encoder-Decoder Architecture Encoder Decoder Usage Resources Introduction The story of Transformers begins with \u0026ldquo;Attention Is All You Need\u0026rdquo; (Vaswani et al., n.d.). In this seminal work, the authors describe the current landscape of sequential models, their shortcomings, and the novel ideas that result in their successful application.\nTheir first point highlights a fundamental flaw in how Recurrent Neural Networks process sequential data: their output is a function of the previous time step. Given the hindsight of 2022, where large language models are crossing the trillion parameter milestone, a model requiring recurrent computation dependent on previous time steps without the possibility of parallelization would be virtually intractable.\nThe second observation refers to attention mechanisms, a useful addition to sequential models that enable long-range dependencies focused on specific contextual information. When added to translation models, attention allows the model to focus on particular words (Bahdanau, Cho, and Bengio 2016).\nThe Transformer architecture considers the entire sequence using only attention mechanisms. There are no recurrence computations in the model, allowing for higher efficiency through parallelization.\nDefinition The original architecture consists of an encoder and decoder, each containing one or more attention mechanisms. Not every type of model uses both encoders and decoders. This is discussed later [TODO: discuss model types]. Before diving into the architecture itself, it is important to understand what an attention mechanism is and how it functions.\nAttention Attention mechanisms produce relationships between sequences. When we look at an image of a dog running in a field with the intent of figuring out what the dog is doing in the picture, we pay greater attention to the dog and look at contextual cues in the image that might inform us of their task. This is an automatic process which allows us to efficiently process information.\nAttention mechanisms follow the same concept. Consider a machine translation task in which a sentence in English is translated to French. Certain words between the input and output will have stronger correlations than others.\nSoft Attention Use of context vector that is dependent on a sequence of annotations. These contain information about the input sequence with a focus on the parts surrounding the $i$-th word.\n\\[ c_i = \\sum_{j=1}^{T_x}\\alpha_{ij}h_j \\]\nWhat is \\(\\alpha_{ij}\\) and how is it computed? This comes from an alignment model which assigns a score reflecting how well the inputs around position \\(j\\) and output at position \\(i\\) match, given by\n\\[ e_{ij} = a(s_{i-1}, h_j), \\]\nwhere \\(a\\) is a feed-forward neural network and \\(h_j\\) is an annotation produced by the hidden layer of a BRNN. These scores are passed to the softmax function so that \\(\\alpha_{ij}\\) represents the weight of annotation \\(h_j\\):\n\\[ \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp (e_{ik})}. \\]\nThis weight reflects how important \\(h_j\\) is at deciding the next state \\(s_i\\) and generating \\(y_i\\).\nSoft vs. Hard Attention This mechanism was also described in the context of visual attention as \u0026ldquo;soft\u0026rdquo; attention (Xu et al. 2016). The authors also describe an alternative version they call \u0026ldquo;hard\u0026rdquo; attention. Instead of providing a probability of where the model should look, hard attention provides a single location that is sampled from a multinoulli distribution parameterized by \\(\\alpha_i\\).\n\\[ p(s_{t,i} = 1 | s_{j\u0026lt;t}, \\mathbf{a}) = \\alpha_{t,i} \\]\nHere, \\(s_{t,i}\\) represents the location \\(i\\) at time \\(t\\), \\(s_{j\u0026lt;t}\\) are the location variables prior to \\(t\\), and \\(\\mathbf{a}\\) is an image feature vector.\nFigure 1: Hard attention for \u0026ldquo;A man and a woman playing frisbee in a field.\u0026rdquo; (Xu et al.) Figure 2: Soft attention for \u0026ldquo;A woman is throwing a frisbee in a park.\u0026rdquo; (Xu et al.) The two figures above show the difference between soft and hard attention. Hard attention, while faster at inference time, is non-differentiable and requires more complex methods to train (TODO: cite Luong).\nSelf-Attention Self attention is particularly useful for determining the relationship between different parts of an input sequence. The figure below demonstrates self-attention given an input sentence (Cheng, Dong, and Lapata 2016).\nFigure 3: Line thickness indicates stronger self-attention (Cheng et al.). How aligned the two vectors are. Cross Attention TODO\nKey-value Store Query, key, and value come from the same input (self-attention).\nCheck query against all possible keys in the dictionary. They have the same size. The value is the result stored there, not necessarily the same size. Each item in the sequence will generate a query, key, and value.\nThe attention vector is a function of they keys and the query.\nHidden representation is a function of the values and the attention vector.\nThe Transformer paper talks about queries, keys, and values. This idea comes from retrieval systems. If you are searching for something (a video, book, song, etc.), you present a system your query. That system will compare your query against the keys in its database. If there is a key that matches your query, the value is returned.\n\\[ att(q, \\mathbf{k}, \\mathbf{v}) = \\sum_i v_i f(q, k_i), \\] where \\(f\\) is a similarity function.\nThis is an interesting and convenient representation of attention. To implement this idea, we need some measure of similarity. Why not orthogonality? Two vectors that are orthogonal produce a scalar value of 0. The maximum value two vectors will produce as a result of the dot product occurs when the two vectors have the exact same direction. This is convenient because the dot product is simple and efficient and we are already performing these calculations in our deep networks in the form of matrix multiplication.\nScaled Dot Product Attention Figure 4: Scaled dot-product attention ((Vaswani et al., n.d.)) Each query vector is multiplied with each key using the dot product. This is implemented more efficiently via matrix multiplication. A few other things are added here to control the output. The first is scaling.\nMulti-Head Attention A single attention head can transform the input into a single representation. Is this analagous to using a single convolutional filter? The benefit of having multiple filters is to create multiple possible representations from the same input.\nEncoder-Decoder Architecture The original architecture of a transformer was defined in the context of sequence transduction tasks, where both the input and output are sequences. The most common task of this type is machine translation.\nEncoder The encoder layer takes an input sequence \\(\\{\\mathbf{x}_t\\}_{t=0}^T\\) and transforms it into another sequence \\(\\{\\mathbf{z}_t\\}_{t=0}^T\\).\nWhat is \\(\\mathbf{z}_t\\)?\nHow is it used? Input as key and value into second multi-head attention layer of the decoder.\nCould you create an encoder only model? Yes. Suitable for classification tasks \u0026ndash; classify the representation produced by the encoder. How does this representation relate to understanding?\nIt\u0026rsquo;s a transformation to another representation.\nGenerated representation also considers the context of other parts of the same sequence (bi-directional).\nDecoder Generates an output sequence.\nDecoder-only models? Suitable for text generation.\nWhat does the input represent?\nWhat does the output represent?\nWhat if we don\u0026rsquo;t use an encoder, what information is added in lieu of the encoder output?\n\u0026nbsp;Model\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Examples\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;Tasks\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;Encoder\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;ALBERT,\u0026nbsp;BERT,\u0026nbsp;DistilBERT,\nELECTRA,\u0026nbsp;RoBERTa\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;Sentence\u0026nbsp;classification,\u0026nbsp;named\u0026nbsp;entity\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\nrecognition,\u0026nbsp;extractive\u0026nbsp;question\u0026nbsp;answering\u0026nbsp; \u0026nbsp;Decoder\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;CTRL,\u0026nbsp;GPT,\u0026nbsp;GPT-2,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\nTransformer\u0026nbsp;XL\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;Text\u0026nbsp;generation\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;Encoder-decoder\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;BART,\u0026nbsp;T5,\u0026nbsp;Marian,\u0026nbsp;mBART\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;Summarization,\u0026nbsp;translation,\u0026nbsp;generative\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\nquestion\u0026nbsp;answering\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Usage TODO\nResources https://twitter.com/labmlai/status/1543159412940242945?s=20\u0026t=EDu5FzDWl92EqnJlWvfAxA https://en.wikipedia.org/wiki/Transduction_(machine_learning) https://www.apronus.com/math/transformer-language-model-definition https://lilianweng.github.io/posts/2018-06-24-attention/ http://nlp.seas.harvard.edu/annotated-transformer/ ","date":1667710800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667710800,"objectID":"0a389e4db0e6fb95e6eb83d1b1358c6d","permalink":"https://ajdillhoff.github.io/notes/transformers/","publishdate":"2022-11-06T00:00:00-05:00","relpermalink":"/notes/transformers/","section":"notes","summary":"Table of Contents Introduction Definition Attention Key-value Store Scaled Dot Product Attention Multi-Head Attention Encoder-Decoder Architecture Encoder Decoder Usage Resources Introduction The story of Transformers begins with \u0026ldquo;Attention Is All You Need\u0026rdquo; (Vaswani et al., n.d.). In this seminal work, the authors describe the current landscape of sequential models, their shortcomings, and the novel ideas that result in their successful application.\nTheir first point highlights a fundamental flaw in how Recurrent Neural Networks process sequential data: their output is a function of the previous time step.","tags":["deep learning","llms"],"title":"Transformers","type":"notes"},{"authors":null,"categories":null,"content":" Table of Contents Introduction Cloning a Repository Adding a new file Committing Changes Pushing your local changes to the remote repository Introduction This article walks through the steps needed to complete Assignment 0. For this course, we only need to use 5 commands. Although it is not required for this course, it is highly recommended that you learn the basics of git. The documentation page provided by git-scm is extremely helpful. It includes links to a free book on git as well as a cheat sheet with the most common git commands.\nCloning a Repository After accepting the assignment, you are provided with a link to your own private repository. To work with it on your local machine, you will first need to clone it. To clone it, you will need to authenticate that you are allowed to work with that repository. This is done by either SSH or HTTPS.\nIf you want to add and use an SSH key to authenticate, follow the instructions listed here. This article will walk through authentication via HTTPS. To clone your repository, you will need the HTTPS link. This is the same as the link to your actual repository. You can also view it by clicking on the green Code button and selecting HTTPS.\nFigure 1: Viewing the repository link. In a terminal window, run the command git clone [link], where [link] is the URL you copied for your respository. You should be prompted to enter your GitHub username and password. If you use your regular account password, you will see something similar to the output below\nFigure 2: Attempting to authenticate using your GitHub password. As stated in the error message, GitHub does not support using your password to authenticate via HTTPS. The link that is provided in the error message directs your to an article explaining this decision. The article links to another article on how to set up a personal access token (direct link). Create a personal access token as described in the direct link. This will be used in place of your regular password, so make sure you keep it somewhere safe.\nWhen creating the access token, be sure to at least select the repo scope. This will ensure that your access token is authorized to clone your repository.\nFigure 3: Selecting scopes. With the generated access token, you can successfully clone your repository via HTTPS. As a reminder, entering text into a password prompt in terminal will not show the characters you are typing. Do not worry! It is still reading the input.\nFigure 4: Successfully cloning the repository. Adding a new file Now that the repository is cloned, we can begin working with it locally. The assignment requires us to create a program which will print the lines of CSV to the terminal window. This requires opening data.csv, looping through its contents, and printing each line as our program reads it.\nStart by creating a new file named read_csv.c using your favorite code editor. It does not matter which editor you use. It only matters that the file you create is in the repository folder. This assignment is really about making sure you can use git properly, so a solution has been given below.\n#include \u0026lt;stdio.h\u0026gt; #define BUF_SIZE 128 int main() { char buffer[BUF_SIZE] = { 0 }; FILE *fp = fopen(\u0026#34;data.csv\u0026#34;, \u0026#34;r\u0026#34;); if (fp == NULL) return 1; while (fgets(buffer, BUF_SIZE, fp)) { printf(\u0026#34;%s\u0026#34;, buffer); } return 0; } Once we have finished editing the code, we can add it to our local repository. To check the current status of changes in our repo, use git status.\nFigure 5: Checking the status. We have created a file named read_csv.c, but it is not currently tracked by our repository. To track this file, we need to add it via git add read_csv.c. After adding the file, we can see that our local repo\u0026rsquo;s status has changed.\nFigure 6: Status after adding a file. Now that the file is being tracked, any modifications we make to it will be recorded. Git keeps snapshots of each state the file is in. We can always review a history of our file\u0026rsquo;s changes and see how the code has developed over the lifetime of a project. In that status output above, our repository detects changes to read_csv.c.\nCommitting Changes If we are happy with the changes, we can commit them to the repository using git commit. A commit should be accompanied by a message explaining what was changed. This will be very useful later on when you need to review what changes were made and why. Since this code fulfills the requirements of the assignment, let\u0026rsquo;s form our message that way. We can commit with a message with the following command.\ngit commit -m \u0026#34;Completed assignment.\u0026#34; If you have other tracked files with changes that needed to be committed, you can use the flag -a to add them with your commit.\nFigure 7: Committing the changes. Pushing your local changes to the remote repository When you clone a repository, you get a full copy of that repository including all of its data. If the server that is hosting your project crashes, you will still have a full copy of the repository on your local machine. To synchronize your changes with the local repository, you can push the local files with git push.\nFigure 8: Pushing local changes to the remote repo. If we view the website for our repository, it shows our changes.\nFigure 9: Repository website after pushing local changes. That is all that is needed for submitting your assignments. If the code is on your remote repository, than it will be considered as your submission.\n","date":1662181200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662181200,"objectID":"ceaf5b8b3a982a3798fd6d2d5ff2d578","permalink":"https://ajdillhoff.github.io/notes/submitting_assignments_using_github/","publishdate":"2022-09-03T00:00:00-05:00","relpermalink":"/notes/submitting_assignments_using_github/","section":"notes","summary":"Table of Contents Introduction Cloning a Repository Adding a new file Committing Changes Pushing your local changes to the remote repository Introduction This article walks through the steps needed to complete Assignment 0. For this course, we only need to use 5 commands. Although it is not required for this course, it is highly recommended that you learn the basics of git. The documentation page provided by git-scm is extremely helpful.","tags":null,"title":"Submitting Assignments using GitHub","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Box Constraints Updating the Lagrangians The Algorithm Implementation Introduction Paper link: https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/\nSequential Minimal Optimization (SMO) is an algorithm to solve the SVM Quadratic Programming (QP) problem efficiently (Platt, n.d.). Developed by John Platt at Microsoft Research, SMO deals with the constraints of the SVM objective by breaking it down into a smaller optimization problem at each step.\nThe two key components of SMO are\nan analytic method to solving for two Lagrange multipliers at a time and a heuristic for choosing which multipliers to optimize. The original objective is to maximize the margin between the nearest positive and negative examples. For the linear case, if the output is given as\n\\[ u = \\mathbf{w}^T \\mathbf{x} - b, \\]\nwhere \\(\\mathbf{w}\\) is the normal vector to the hyperplane separating the classes, then the margin is given as\n\\[ m = \\frac{1}{\\|w\\|_2}. \\]\nMaximizing this margin yielded the primal optimization problem\n\\begin{align*} \\min_{\\mathbf{w},b} \\frac{1}{2} \\|\\mathbf{w}\\|^2\\\\ \\textrm{s.t.} \\quad \u0026amp; y_i(\\mathbf{w}^T \\mathbf{x} - b) \\geq 1, \\forall i\\\\ \\end{align*}\nThe dual form of the objective function for a Support Vector Machine is\n\\[ \\min_{\\vec\\alpha} \\Psi(\\vec{\\alpha}) = \\min_{\\vec{\\alpha}} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)\\alpha_i\\alpha_j - \\sum_{i=1}^N \\alpha_i \\]\nwith inequality constraints\n\\[ \\alpha_i \\geq 0, \\forall i, \\]\nand a linear equality constraint\n\\[ \\sum_{i=1}^N y_i \\alpha_i = 0. \\]\nFor a linear SVM, the output is dependent on a weight vector \\(\\mathbf{w}\\) and threshold \\(b\\):\n\\[ \\mathbf{w} = \\sum_{i=1}^N y_i \\alpha_i \\mathbf{x}_i, \\quad b = \\mathbf{w}^T \\mathbf{x}_k - y_k. \\]\nThe threshold is also dependent on the weight vector? The weight vector \\(\\mathbf{w}\\) is computed using the training data. The threshold is only dependent on non-zero support vectors, \\(\\alpha_k \u0026gt; 0\\).\nOverlapping Distributions Slack variables were introduced to allow misclassifications at the cost of a linear penalty. This is useful for datasets that are not linearly separable. In practice, this is accomplished with a slight modification of the original objective function:\n\\begin{align*} \\min_{\\mathbf{w},b} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^N \\xi_i\\\\ \\textrm{s.t.} \\quad \u0026amp; y_i(\\mathbf{w}^T \\mathbf{x} - b) \\geq 1 - \\xi_i, \\forall i\\\\ \\end{align*}\nThe convenience of this formulation is that the parameters \\(\\xi_i\\) do not appear in the dual formulation at all. The only added constraint is\n\\[ 0 \\leq \\alpha_i \\leq C, \\forall i. \\]\nThis is referred to as the box constraint for reasons we shall see shortly.\nBox Constraints The smallest optimization step that SMO solves is that of two variables. Given the constraints above, the solution lies on a diagonal line \\(\\sum_{i=1}^N y_i \\alpha_i = 0\\) bounded within a box \\(0 \\leq \\alpha_i \\leq C, \\forall i\\).\nIsolating for two samples with alphas \\(\\alpha_1\\) and \\(\\alpha_2\\), the constraint \\(\\sum_{i=1}^n y_i \\alpha_i = 0\\) suggests that\n\\[ y_1 \\alpha_1 + y_2 \\alpha_2 = w. \\]\nWe first consider the case when \\(y_1 \\neq y_2\\). Let \\(y_1 = 1\\) and \\(y_2 = -1\\), then \\(a_1 - a_2 = w\\). As \\(\\alpha_1\\) increases, \\(\\alpha_2\\) must also increase to satisfy the constraint.\nFigure 1: Equality constraint for case 1 (from Platt\u0026rsquo;s SMO paper). The other case is when \\(y_1 = y_2\\), then \\(\\alpha_1 + \\alpha_2 = w\\). As \\(\\alpha_1\\) is increased, \\(\\alpha_2\\) is decreased to satisfy the constraint.\nFigure 2: Box constraint for samples of the same class (from Platt\u0026rsquo;s SMO paper). Updating the Lagrangians SMO solves for only two Lagrange multipliers at a time. Solving for only 1 at a time would be impossible under the constraint \\(\\sum_{i=1}^N y_i \\alpha_i = 0\\). The first step is to compute \\(\\alpha_2\\) and constrain it between the ends of the diagonal line segment from the box constraints.\nIf \\(y_1 \\neq y_2\\), then the following bounds are applied to \\(\\alpha_2\\):\n\\begin{equation*} L = \\max(0, \\alpha_2 - \\alpha_1), \\quad H = \\min(C, C + \\alpha_2 - \\alpha_1) \\end{equation*}\notherwise, the bounds are computed as:\n\\begin{equation*} L = \\max(0, \\alpha_2 + \\alpha_1 - C), \\quad H = \\min(C, \\alpha_2 + \\alpha_1) \\end{equation*}\nUpdating the actual parameter is done following the update rule of gradient descent:\n\\[ \\alpha_2^{\\text{new}} = \\alpha_2 + \\frac{y_2(E_1 - E_2)}{\\eta}. \\]\nHow do we arrive at this update rule?\nSecond Derivative of the Objective Function Here, \\(\\eta\\) represents the step size and direction. It is computed from the second derivative of the objective function along the diagonal line. To see that this is the case, consider the original objective function\n\\begin{align*} \\min_{\\mathbf{\\alpha}} \\quad \u0026amp; \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j) \\mathbf{\\alpha}_1 \\mathbf{\\alpha}_2 - \\sum_{i=1}^N \\alpha_i\\\\ \\textrm{s.t.} \\quad \u0026amp; 0 \\leq \\alpha_i \\leq C, \\forall i\\\\ \u0026amp; \\sum_{i=1}^N y_i \\alpha_i = 0\\\\ \\end{align*}\nSince we are optimizing with respect to only 2 Lagrangian multipliers at a time, we can write the Lagrangian function as\n\\[ \\frac{1}{2} y_1^2 K_{11} \\alpha_1^2 + \\frac{1}{2} y_2^2 K_{22} \\alpha_2^2 + y_1 \\alpha_1 \\sum_{j=3}^N y_j \\alpha_j K_{1j} + y_2 \\alpha_2 \\sum_{j=3}^N y_j \\alpha_j K_{2j} - \\alpha_1 - \\alpha_2 + \\sum_{j=3}^N \\alpha_j \\]\nWe are only optimizing with respect to \\(\\alpha_1\\) and \\(\\alpha_2\\), the next step is to extract those terms from the sum. This is simplified further by noting that \\(\\sum_{j=3}^N y_j \\alpha_j K_{ij}\\) looks very similar to the output of an SVM:\n\\[ u = \\sum_{j=1}^N y_j \\alpha_j K(\\mathbf{x}_j, \\mathbf{x}) - b. \\]\nThis allows us to introduce a variable \\(v_i\\) based on \\(u_i\\), the output of an SVM given sample \\(\\mathbf{x}_i\\):\n\\[ v_i = \\sum_{j=3}^N y_j \\alpha_j K_{ij} = u_i + b - y_1 \\alpha_1 K_{1i} - y_2 \\alpha_2 K_{2i}. \\]\nThe objective function is then written as\n\\[ \\frac{1}{2} y_1^2 K_{11} \\alpha_1^2 + \\frac{1}{2} y_2^2 K_{22} \\alpha_2^2 + y_1 \\alpha_1 v_1 + y_2 \\alpha_2 v_2 - \\alpha_1 - \\alpha_2 + \\sum_{j=3}^N \\alpha_j. \\]\nNote that the trailing sum \\(\\sum_{j=3}^N \\alpha_j\\) is treated as a constant since those values are not considered when optimizing for \\(\\alpha_1\\) and \\(\\alpha_2\\).\nGiven the box constraints from above, we must update \\(\\alpha_1\\) and \\(\\alpha_2\\) such that\n\\[ \\alpha_1 + s \\alpha_2 = \\alpha_1^* + s \\alpha_2^* = w. \\]\nThis linear relationship allows us to express the objective function in terms of α_2:\n\\[ \\Psi = \\frac{1}{2} y_1^2 K_{11} (w - s \\alpha_2)^2 + \\frac{1}{2} y_2^2 K_{22} \\alpha_2^2 + y_1 (w - s \\alpha_2) v_1 + y_2 \\alpha_2 v_2 - \\alpha_1 - \\alpha_2 + \\sum_{j=3}^N \\alpha_j. \\]\nThe extremum of the function is given by the first derivative with respect to \\(\\alpha_2\\):\n\\[ \\frac{d\\Psi}{d\\alpha_2} = -sK_{11}(w - s\\alpha_2) + K_{22}\\alpha_2 - K_{12}\\alpha_2 + s K_{12} (w - s \\alpha_2) - y_2 v_2 + s + y_2 v_2 - 1 = 0. \\]\nIn most cases, the second derivative will be positive. The minimum of \\(\\alpha_2\\) is where\n\\begin{align*} \\alpha_2 (K_{11} + K_{22} - 2 K_{12}) \u0026amp;= s(K_{11} - K_{12})w + y_2(v_1 - v_2) + 1 - s\\\\ \u0026amp;= s(K_{11} - K_{12})(s\\alpha_2^*+\\alpha_1^*)\\\\ \u0026amp;+ y_2(u_1-u_2+y_1\\alpha_1^*(K_{12} - K_{11}) + y_2 \\alpha_2^* (K_{22} - K_{21})) + y_2^2 - s\\\\ \u0026amp;= \\alpha_2^*(K_{11}+K_{22} - 2K_{12}) + y_2(u_1 - u_2 + y_2 - y_1). \\end{align*}\nIf we let \\(E_1 = u_1 - y_1\\), \\(E_2 = u_2 - y_2\\), and \\(\\eta = K_{11} + K_{22} - 2K_{12}\\), then\n\\[ \\alpha_2^{\\text{new}} = \\alpha_2 + \\frac{y_2(E_1 - E_2)}{\\eta}. \\]\nThe Algorithm Sequential Minimal Optimization (SMO) solves the SVM problem which usually requires a Quadratic Programming (QP) solution. It does this by breaking down the larger optimization problem into a small and simple form: solving for two Lagrangians. Solving for one would not be possible without violating KKT conditions. There are two components to Sequential Minimal Optimization: the first is how the Lagrangians are selected and the second is the actual optimization step.\nChoosing the First Lagrangian The algorithm first determines which samples in the dataset violate the given KKT conditions. Only those violating the conditions are eligible for optimization. A solution is found when the following are true for all \\(i\\):\n\\begin{align*} \\alpha_i = 0 \\iff y_i u_i \\geq 1,\\\\ 0 \u0026lt; \\alpha_i \u0026lt; C \\iff y_i u_i = 1,\\\\ \\alpha_i = C \\iff y_i u_i \\leq 1. \\end{align*}\nAdditionally, samples that are not on the bounds are selected (those with \\(\\alpha_i \\neq 0\\) and \\(\\alpha_i \\neq C\\)). This continues through the dataset until no sample violates the KKT constraints within \\(\\epsilon\\).\nAs a last step, SMO searches the entire dataset to look for any bound samples that violate KKT conditions. It is possible that updating a non-bound sample would cause a bound sample to violate the KKT conditions.\nChoosing the Second Lagrangian The second Lagrangian is chosen to maximize the size of the step taken during joint optimization. Noting that the step size is based on\n\\[ \\alpha_2^{\\text{new}} = \\alpha_2 + \\frac{y_2(E_1 - E_2)}{\\eta}, \\]\nit is approximated by computing \\(|E_1 - E_2|\\).\nIf positive progress cannot be made given the choice of Lagrangian, SMO will begin iterating through non-bound examples. If no eligible candidates are found in the non-bound samples, the entire dataset is searched.\nUpdating the Parameters With the second derivative of the objective function, we can take an optimization step along the diagonal line. To ensure that this step adheres to the box constraints defined above, the new value of \\(\\alpha_2\\) is clipped:\n\\begin{equation*} \\alpha_2^{\\text{new,clipped}} = \\begin{cases} H \u0026amp;\\text{if} \\quad \\alpha_2^{\\text{new}} \\geq H;\\\\ \\alpha_2^{\\text{new}} \u0026amp;\\text{if} \\quad L \u0026lt; \\alpha_2^{\\text{new}} \u0026lt; H;\\\\ L \u0026amp;\\text{if} \\quad \\alpha_2^{\\text{new}} \\geq L.\\\\ \\end{cases} \\end{equation*}\nWith the new value of \\(\\alpha_2\\), \\(\\alpha_1\\) is computed such that the original KKT condition is preserved:\n\\[ \\alpha_1^{\\text{new}} = \\alpha_1 + s(\\alpha_2 - \\alpha_2^{\\text{new,clipped}}), \\]\nwhere \\(s = y_1y_2\\).\nPoints that are beyond the margin are given an alpha of 0: \\(\\alpha_i = 0\\). Points that are on the margin satisfy \\(0 \u0026lt; \\alpha_i \u0026lt; C\\). These are the support vectors. Points inside the margin satisfy \\(\\alpha_i = C\\).\nLinear SVMs In the case of linear SVMs, the parameters can be stored as a single weight vector\n\\[ \\mathbf{w}^{\\text{new}} = \\mathbf{w} + y_1 (\\alpha_1^{\\text{new}} - \\alpha_1)\\mathbf{x}_1 + y_2(\\alpha_2^{\\text{new,clipped}} - \\alpha_2)\\mathbf{x}_2. \\]\nThe output of a linear SVM is computed as\n\\[ u = \\mathbf{w}^T \\mathbf{x} - b. \\]\nNonlinear SVMs In the nonlinear case, the output of the model is computed as\n\\[ u = \\sum_{i=1}^N y_i \\alpha_i K(\\mathbf{x}_i, \\mathbf{x}) - b. \\]\nImplementation An implementation of SMO in Python is available at https://github.com/ajdillhoff/CSE6363/blob/main/svm/smo.ipynb\nReferences Platt, John C. n.d. “Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines,” 21. ","date":1656910800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706745600,"objectID":"d82d40256130c8b03c7cfd6f8e1a5a3b","permalink":"https://ajdillhoff.github.io/notes/sequential_minimal_optimization/","publishdate":"2022-07-04T00:00:00-05:00","relpermalink":"/notes/sequential_minimal_optimization/","section":"notes","summary":"Table of Contents Introduction Box Constraints Updating the Lagrangians The Algorithm Implementation Introduction Paper link: https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/\nSequential Minimal Optimization (SMO) is an algorithm to solve the SVM Quadratic Programming (QP) problem efficiently (Platt, n.d.). Developed by John Platt at Microsoft Research, SMO deals with the constraints of the SVM objective by breaking it down into a smaller optimization problem at each step.\nThe two key components of SMO are","tags":["machine learning"],"title":"Sequential Minimal Optimization","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Binary Classification Plotting a Decision Boundary Multiple Classes Sensitivity to Outliers Introduction The notebook for this lesson can be found here.\nWith linear regression, we were able to fit a model to our data in order to make inferences on unseen data points. In the examples, both the input features and observation were continuous. With discriminant functions, we will use similar models to classify the data points based on their input features. We start out with the simplest approach: we assume that the data is linearly separable and can be assigned one of \\(K\\) discrete classes.\nFor classification with linear discriminant functions, we will use a \\(K\\) dimensional vector that has a 1 corresponding to the class encoding for that input and a 0 for all other positions. For example, if our possible target classes were \\(\\{\\text{car, truck, person}\\}\\), then a target vector for \\(\\text{person}\\) would be \\(\\mathbf{y} = [0, 0, 1]^T\\).\nThis article will stick to a discriminative approach to classification. That is, we define a discriminant function which assigns each data input \\(\\mathbf{x}\\) to a class. For a probabilistic perspective, see Linear Discriminant Analysis.\nWe will again start with a linear model \\(y = f(\\mathbf{x}; \\mathbf{w})\\). Unlike the model used with linear regression, ours will need to predict a discrete class label. In other words, we need to predict a vector with a 1 corresponding to the class encoding.\nBinary Classification Consider a simple dataset with 2 features per data sample. Our goal is to classify the data as being one of two possible classes. This only requires a single function which classifies the sample as being in class 0 if \\(f(\\mathbf{x};\\mathbf{w}) \\geq 0\\) and class 1 otherwise.\nFigure 1: Two groups of data that are very clearly linearly separable. The model output is such that \\(f(\\mathbf{x};\\mathbf{w}) = [1, 0]\\) when \\(\\mathbf{x}\\) is predicted as class 1. If \\(f(\\mathbf{x};\\mathbf{w}) = [0, 1]\\) then \\(\\mathbf{x}\\) is assigned to class 2. In practice, the actual output will not be a one-hot vector. There will be some values in all positions of the vector.\nFor example, a model trained on a binary classification task outputs the following vector given a randomly selected input sample:\n\\[ [0.1224, 0.8776] \\]\nA class would be assigned by taking the argmax of this output vector. That is, the model predicts that this sample belongs to class 1.\nMeasuring Classifier Performance L1 loss can be used to measure classifier performance for linear discriminant function models.\n\\[ E = \\sum_{i=1}^N \\sum_{j=1}^M |\\hat{y}_{ij} - y_{ij}| \\]\nPlotting a Decision Boundary In the case of binary classification, a sample is predicted as class 1 if the output vector has the highest value at index 0. Otherwise, it is classified as class 2. If we were to plot the decision regions, we would see that the boundary is at the point when the output for both classes is equal.\nFigure 2: Binary classification with decision regions shown. Multiple Classes Extending this to multiple classes is as easy as encoding the classes in a one-hot vector whose length equals the number of classes. The parameters of the model can be obtained using gradient descent, the normal equations, or any other method that optimizes the least squares criterion.\nThe figure below shows an example of a linear discriminant function model fit to a dataset with 3 classes.\nFigure 3: Multiclass classification using linear discriminant functions. Sensitivity to Outliers One major flaw with least squares models is their sensitivity to outliers in the data. Consider the dataset shown below.\nFigure 4: Linearly separable dataset This dataset is clearly linearly separable. This will be no problem for our linear classifier, as seen below.\nFigure 5: Linear classifier fit to data using least squares. This dataset has a convenient property that the samples from each class are tightly clustered. What happens if our data is slightly more diverse?\nFigure 6: 2-class dataset in which one class is not as tightly clustered as the other. In the dataset above, we can still clearly see that it should be linearly separable. Unfortunately, our least squares model will be very sensitive to the 20 points at the top left of the plot. Training a linear discriminant function using least squares results in the following decision boundary.\nFigure 7: The model misclassifies samples that should be linearly separable. If we determine that a linear classifier is adequate for a given dataset, we may wish to use a slightly more robust model such as Logistic Regression instead of linear discriminant functions.\n","date":1654578000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654578000,"objectID":"4eeb3b49e53888a547d60e92fc258530","permalink":"https://ajdillhoff.github.io/notes/discriminant_functions/","publishdate":"2022-06-07T00:00:00-05:00","relpermalink":"/notes/discriminant_functions/","section":"notes","summary":"Table of Contents Introduction Binary Classification Plotting a Decision Boundary Multiple Classes Sensitivity to Outliers Introduction The notebook for this lesson can be found here.\nWith linear regression, we were able to fit a model to our data in order to make inferences on unseen data points. In the examples, both the input features and observation were continuous. With discriminant functions, we will use similar models to classify the data points based on their input features.","tags":null,"title":"Discriminant Functions","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Papers Evaluating Object Detection Methods Datasets An Incomplete History of Deep-Learning-based Object Detection Papers https://awesomeopensource.com/projects/object-detection Evaluating Object Detection Methods Object detection algorithms are evaluated using the mean of Average Precision (mAP) across all classes in the dataset.\nPrecision and recall are computed from the predictions and the ground truth. A sample and the model\u0026rsquo;s prediction can either be positive or negative when it comes to classification. Either it belongs to a class or it does not. The table below summarizes the outcomes between the model\u0026rsquo;s prediction and the true underlying class.\nObject detection algorithms are evaluated using the mean of Average Precision (mAP) across all classes in the dataset.\nPrecision and recall are computed from the predictions and the ground truth. A sample and the model\u0026rsquo;s prediction can either be positive or negative when it comes to classification. Either it belongs to a class or it does not. The table below summarizes the outcomes between the model\u0026rsquo;s prediction and the true underlying class.\nPrecision\n\\[ \\frac{TP}{TP + FP} \\]\nRecall\n\\[ \\frac{TP}{TP + FN} \\]\nObject detection models predict a bounding box for a given class. A correct bounding box can be identified as one that has an Intersection-over-Union (IoU) score of \u0026gt; 0.5.\nFigure 2: IoU Visualization (Szeliski 2021) If you were to plot the precision versus recall of a single class, the area under the curve would be the average precision.\nFigure 3: Average Precision Curves (Girshick, 2020) The curve implicitly represents varying probability thresholds! As recall increases, precision will generally decrease. This reflects the fact that a model that recalls all input samples as a particular class will sure be misclassifying them. Keep in mind that recall by itself is not a measure of correctness. Ideally, the curve will be closer to the top right of the graph, indicating high precision and recall.\nDatasets Pascal VOC COCO An Incomplete History of Deep-Learning-based Object Detection Rich Feature Hierarchies for Accuracy Object Detection and Semantic Segmentation (Girshick et al. 2014) The last decade of progress on various visual recognition tasks has been based considerably on the use of SIFT and HOG (Girshick et al. 2014).\nThis is one of the earliest papers to leverage deep learning for object detection. The overall approach is a piecewise one, where the CNN is only used for classification.\nFigure 4: System overview of the R-CNN approach (Girshick et al. 2014) Key Insights\nIncreased mean average precision (mAP) by more than 30% on VOC 2012. Candidate regions are generated using Selective Search (Uijlings et al. 2013). CNNs are used to perform object classification for each region proposal. Employs bounding box regression to refine the predicted bounding boxes. \u0026ldquo;\u0026hellip;when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost.\u0026rdquo; Region Proposals The Selective Search algorithm is used to generate region proposals. The algorithm is based on hierarchical grouping of superpixels. Given an input image, approximately 2000 region proposals are generated. Each region proposal is a rectangular bounding box that encloses a region of interest. Since the bounding boxes are not square, the prposals are warped to a fixed size before being fed into the CNN.\nFigure 5: Selective Search region proposals (Uijlings et al. 2013). Feature Extraction Each warped image is passed through a CNN backbone pre-trained on ImageNet. The output is a 4096-dimensional feature vector. In the context of today\u0026rsquo;s methods, this certainly does not seem like a very sophisticated approach. Given the context of the time, this was a significant improvement over the SIFT and HOG features that were previously used. The other benefit is that the CNN can be fine-tuned on the target dataset, if desired.\nClassification The feature vectors from the CNN are used as input to a linear SVM for classification. The SVM is trained to classify the region proposals into one of the classes in the dataset. As stated before, this really is a piecewise solution. It is also very slow, as each region proposal must be passed through the CNN and SVM.\nTraining The CNN and SVM both need to be trained. The CNN is first pre-trained on ImageNet, but it still needs to be fine-tuned on the target dataset. Not only is the target dataset out of domain, but the images themselves are warped. The authors do admit that it is reasonable to simply use the output of the softmax layer for classification in stead of training an SVM. In their experiments, they found that the SVM provided a slight improvement in performance.\nInference Given roughly 2000 region proposals, how is a final prediction made? For each class, non-maximum suppression is applied to the region proposals. A region is rejected if it has an IoU overlap with a higher scoring region above a certain threshold.\nFast R-CNN (Girshick 2015) Published roughly a year after the original R-CNN paper, Fast R-CNN addresses many of the shortcomings and ineffeciencies of the original approach. The main innovation is the introduction of the Region of Interest (RoI) pooling layer. This layer allows the CNN to be applied to the entire image, rather than to each region proposal individually.\nFigure 6: System overview of Fast R-CNN (Girshick 2015) Key Insights\nSingle-stage training joinly optimizes a softmax classifier and bounding box regressor. RoI pooling layer allows the CNN to be applied to the entire image. Object proposals are still provided by Selective Search, but only a single forward pass is needed to extract features and make predictions. RoI Pooling Given an input image, a feature extracting CNN produces a feature map. For each region proposal the RoI pooling layer extracts a feature vector directly from the feature map. The feature vector is passed through a fully connected network to produce a class score and bounding box regression.\nA region proposal is defined by a four-tuple \\((r, c, h, w)\\) defining the top-left corner \\((r, c)\\) and the height and width \\((h, w)\\) of the region. The RoI pooling layer divides the window into a \\(H \\times W\\) grid of sub-windows, where \\(H\\) and \\(W\\) are hyperparameters. A max pooling operation is applied to each sub-window to produce a fixed-size output.\nTraining Three different CNN backbones were tested. The last max pooling layer is replaced with the RoI pooling layer. From there, the two branches of the network are added: a softmax classifier and a bounding box regressor. The softmax classifier is trained to classify the region proposals into one of the classes in the dataset. The bounding box regressor is trained to refine the predicted bounding boxes.\nAnother key efficiency improvement comes from hierarchical sampling during training. Since the RoI pooling layer allows the CNN to be applied to the entire image, the same feature map can be used for multiple region proposals. This is in contrast to the original R-CNN approach, where each region proposal was passed through the CNN individually. When training, the authors sample a mini-batch of \\(N\\) images and \\(R\\) regions, yielding \\(R / N\\) RoIs from each image.\nResults This work achieved state-of-the-art results on VOC 2007 (70.0), 2010 (68.8), and 2012 (68.4). Additionally, the model was able to run at about 5 frames per second on a GPU.\nFaster R-CNN (Ren et al. 2017) Fast R-CNN improved on many of the glaring issues presented in the original R-CNN paper. One major bottleneck left was the Selective Search algorithm used to generate region proposals. In the third iteration, Faster R-CNN, the authors introduce the Region Proposal Network (RPN) to replace slower region proposal methods.\nKey Insights\nRegion Proposal Network (RPN) generates region proposals. RPN is a fully convolutional network that shares features with the object detection network. Anchor boxes of varying aspect ratios are used to predict region proposals. Region Proposal Network The RPN uses a sliding window approach to generate region proposals. Instead of using a separate CNN, it leverages the feature maps generated by a backbone CNN such as VGG or ResNet.\nFigure 7: Region Proposal Network (RPN) (Ren et al. 2017) The RPN produces a feature vector for each sliding window, which is fed into a bounding box regressor and classifier. For each of these sliding windows, \\(k\\) anchor boxes are generated. In their experiments, they generate 9 anchors based on 3 scales and 3 aspect ratios.\nFigure 8: Anchor boxes (Ren et al. 2017) Why use anchor boxes instead of directly predicting bounding boxes? The authors argue that using anchor boxes simplifies the problem. It adds translation invariance: if an object is translated, the proposal should translate accordingly. This approach is also more cost-efficient. Consider an input image for which multiple scales of bounding boxes are generated. To generate multiple scales, the image would need to be resized and passed through the network multiple times. With anchor boxes, the network only needs to be run once.\nFigure 9: Different schemes for addressing multiple scales and sizes (Ren et al. 2017). Results Faster R-CNN achieved state-of-the-art results on VOC 2007 (73.2) and 2012 (70.4). These scores increase to 78.8 and 75.9 when using the COCO dataset. Additionally, they evaluate on MS COCO, achieving a mAP@.5 of 42.7, meaning that the model is able to detect 42.7% of the objects in the dataset with an IoU of 0.5 or greater. This score goes down to 21.9% when the IoU threshold is expanded in the range of 0.5 to 0.95.\nYou Only Look Once You Only Look Once (YOLO) is a single-stage object detection algorithm that is able to predict multiple bounding boxes and class probabilities in a single forward pass (Redmon et al. 2016). Since 2016, it has benefitted from many improvements, some documented by peer-reviewed papers and others by the community. For a recent survey of YOLO and its variants, see (Terven, Córdova-Esparza, and Romero-González 2023).\nYOLOv1 (Redmon et al. 2016) Figure 10: YOLOv1 Overview (Redmon et al. 2016) The original YOLO method works by dividing an input image into a \\(S \\times S\\) grid. Each grid cell predicts \\(B\\) bounding boxes and confidence scores for each box, along with \\(C\\) class probabilities.\nFigure 11: The Model (Redmon et al. 2016). During training, only one predictor should be responsible for each object. This is enforced by assigning a predictor to an object based on the highest IoU with the ground truth. For inference, the model outputs bounding boxes with a confidence score greater than a threshold. The entire model is trained using a multi-part loss function that includes terms for objectness, classification, and bounding box regression.\nFigure 12: Annotated description of YOLO loss (Terven, Córdova-Esparza, and Romero-González 2023). YOLOv1 was evaluated on the VOC 2007 dataset, achieving a mAP of 63.4. The model was able to run at 45 frames per second on a GPU.\nYOLOv2: Better, Faster, and Stronger (Redmon and Farhadi 2016) YOLOv2 was publised at CVPR 2017 and introduced several key improvements over YOLOv1.\nBatch normalization acts as a regularizer to reduce overfitting during training. Fully convolutional. All dense layers are replaced with convolutional layers. Following Faster R-CNN, YOLOv2 uses anchor boxes to predict bounding boxes. A predetermined set of anchor box sizes are computed using k-means clustering on the training data. The model was trained to be robust to varying sizes via multi-scale training, where the input image is randomly resized during training. Multi-task Learning. The network is pre-trained on ImageNet, where no object detection labels are used. In the event that an input sample does not contain a bounding box annotation, the model is trained to predict the background class. YOLOv2 achieved a 73.4% mAP on VOC 2012, beating Faster R-CNN.\nYOLOv3 (Redmon and Farhadi 2018) The third improvement to YOLO was not published at a major conference, but was released on arXiv. The main improvement is a deeper 53-layer backbone network with residual connections. The new method also supports multi-scale predictions by predicting three boxes at three different scales. The same anchor box computation via k-means is done in this work, with the number of priors being expanded to support three different scales.\nYOLOv4 and beyond The remaining iterations were developed by other members of the community and introduce several key improvements while maintaining the spirit and goals of the original paper. For more information, see the informative survey paper by (Terven, Córdova-Esparza, and Romero-González 2023).\nReferences Girshick, Ross. 2015. “Fast R-CNN,” April. https://doi.org/10.48550/arXiv.1504.08083. Girshick, Ross, Jeff Donahue, Trevor Darrell, and Jitendra Malik. 2014. “Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.” Arxiv:1311.2524 [Cs], October. http://arxiv.org/abs/1311.2524. Redmon, Joseph, and Ali Farhadi. 2016. “YOLO9000: Better, Faster, Stronger.” Arxiv:1612.08242 [Cs], December. http://arxiv.org/abs/1612.08242. ———. 2018. “YOLOv3: An Incremental Improvement,” 6. Redmon, Joseph, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. “You Only Look Once: Unified, Real-Time Object Detection.” Arxiv:1506.02640 [Cs], May. http://arxiv.org/abs/1506.02640. Ren, Shaoqing, Kaiming He, Ross Girshick, and Jian Sun. 2017. “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.” Ieee Transactions on Pattern Analysis and Machine Intelligence 39 (6): 1137–49. https://doi.org/10.1109/TPAMI.2016.2577031. Szeliski, Richard. 2021. Computer Vision: Algorithms and Applications. 2nd ed. http://szeliski.org/Book/2ndEdition.htm. Terven, Juan, Diana-Margarita Córdova-Esparza, and Julio-Alejandro Romero-González. 2023. “A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS.” Machine Learning and Knowledge Extraction 5 (4): 1680–1716. https://doi.org/10.3390/make5040083. Uijlings, J. R. R., K. E. A. van de Sande, T. Gevers, and A. W. M. Smeulders. 2013. “Selective Search for Object Recognition.” International Journal of Computer Vision 104 (2): 154–71. https://doi.org/10.1007/s11263-013-0620-5. ","date":1650258000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1722038400,"objectID":"157659a284a698a07fb413bbd2270867","permalink":"https://ajdillhoff.github.io/notes/object_detection/","publishdate":"2022-04-18T00:00:00-05:00","relpermalink":"/notes/object_detection/","section":"notes","summary":"Table of Contents Papers Evaluating Object Detection Methods Datasets An Incomplete History of Deep-Learning-based Object Detection Papers https://awesomeopensource.com/projects/object-detection Evaluating Object Detection Methods Object detection algorithms are evaluated using the mean of Average Precision (mAP) across all classes in the dataset.\nPrecision and recall are computed from the predictions and the ground truth. A sample and the model\u0026rsquo;s prediction can either be positive or negative when it comes to classification. Either it belongs to a class or it does not.","tags":["computer vision","machine learning"],"title":"Object Detection","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":"The recurrent nature of RNNs means that gradients get smaller and smaller as the timesteps increase. This is known as the vanishing gradient problem. One of the first popular solutions to this problem is called Long Short-Term Memory, a recurrent network architecture by Hochreiter and Schmidhuber.\nAn LSTM is made up of memory blocks as opposed to simple hidden units. Each block is differentiable and contains a memory cell along with 3 gates: the input, output, and forget gates. These components allow the blocks to maintain some history of information over longer range dependencies.\nFigure 1: LSTM memory block with a single cell (adapted from Andrew Ng\u0026rsquo;s diagram). The original LSTM only had an input and output gate. Forget gates were added in 2000 by Gers et al. to control the amount of context that could be reset, if the task called for it. Peephole connections were proposed by Gers et al. in 2002. These are weights that combine the previous cell state to the gates in order to learn tasks that require precise timing.\nBy controlling when information can either enter or leave the memory cell, LSTM blocks are able to maintain more historical context than RNNs.\nForward Pass The equations listed here follow notation and description from Alex Graves\u0026rsquo; thesis.\nThe weight from unit \\(i\\) to \\(j\\) is given as \\(w_{ij}\\). The input to unit \\(j\\) at time \\(t\\) is \\(a_j^t\\) and the result of its activation function is \\(b_j^t\\). Let \\(\\psi\\), \\(\\phi\\), and \\(\\omega\\) be the input, forget, and output gates. A memory cell is denoted by \\(c \\in C\\), where \\(C\\) is the set of cells in the network. The activation, or state, of a given cell \\(c\\) at time \\(t\\) is \\(s_c^t\\). The output of each gate passes through an activation function \\(f\\), while the input and output activation functions of a memory block are given by \\(g\\) and \\(h\\).\nThe forward pass for the input gates is\n\\[ a_{\\psi}^t = \\sum_{i=1}^I w_{i\\psi}x_i^t + \\sum_{h=1}^H w_{h\\psi}b_h^{t-1} + \\sum_{c=1}^C w_{c\\psi}s_c^{t-1}. \\]\nThe output of the forget gates is\n\\[ a_{\\phi}^t = \\sum_{i=1}^I w_{i\\phi}x_i^t + \\sum_{h=1}^H w_{h\\phi}b_h^{t-1} + \\sum_{c=1}^C w_{c\\phi}s_c^{t-1}. \\]\nThe output of the output gates is\n\\[ a_{\\omega}^t = \\sum_{i=1}^I w_{i\\omega}x_i^t + \\sum_{h=1}^H w_{h\\omega}b_h^{t-1} + \\sum_{c=1}^C w_{c\\omega}s_c^{t-1}. \\]\nEach of the outputs above is passed through an activation function \\(f\\).\nThe output of each cell is computed as\n\\[ a_c^t = \\sum_{i=1}^I w_{ic}x_i^t + \\sum_{i=1}^H w_{hc}b_h^{t-1} \\]\nand the internal state is updated via\n\\[ s_c^t = b_{\\phi}^t s_c^{t-1} + b_{\\psi}^t g(a_c^t). \\]\nThe state update considers the state at the previous timestep multiplied by the output of the forget gate. That is, it controls how much of the current memory to keep.\nThe final cell output is given as\n\\[ b_c^t = b_{\\omega}^t h(s_c^t). \\]\n","date":1649739600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649739600,"objectID":"4db6081bc11780061ee225406efb2939","permalink":"https://ajdillhoff.github.io/notes/long_short_term_memory/","publishdate":"2022-04-12T00:00:00-05:00","relpermalink":"/notes/long_short_term_memory/","section":"notes","summary":"The recurrent nature of RNNs means that gradients get smaller and smaller as the timesteps increase. This is known as the vanishing gradient problem. One of the first popular solutions to this problem is called Long Short-Term Memory, a recurrent network architecture by Hochreiter and Schmidhuber.\nAn LSTM is made up of memory blocks as opposed to simple hidden units. Each block is differentiable and contains a memory cell along with 3 gates: the input, output, and forget gates.","tags":["deep learning"],"title":"Long Short-Term Memory","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Definition Bidirectional Recurrent Neural Networks References Introduction Neural networks are an effective tool for regression and classification tasks, but they do not consider the dependencies of information over time. Many tasks have implicit information that is dependent on input that may have already been processed or may not be seen until the future.\nRecurrent Neural Networks (RNN) consider the historical context of time-series data. Bi-directional Recurrent Neural Networks (BRNN) consider both historical and future context. This is necessary for tasks like language tanslation.\nParameter sharing across different parts of the model is key for sequence models. Different instances of a particular feature may appear at different time steps.\n\u0026ldquo;I see Naomi there.\u0026rdquo; and \u0026ldquo;Naomi is right there\u0026rdquo; both convey that Naomi is present, but we would not require the model to have separate parameters just because the word position is different between the two.\nRecurrent connections provide a memory of sorts. This enables important contextual information to be \u0026ldquo;remembered\u0026rdquo; throughout time. These models are not without their limitations. When trained with gradient descent, the gradient information passed throughout multiple time steps can become insignificant. There are several ways to address the vanishing gradient problem which are explored in alternative models such as Long Short-Term Memory and Transformers.\nDefinition The definition of RNNs start with that of Neural Networks. One layer of an RNN has some number of hidden units that transforms the input into an intermediate representation. In addition to transforming the input, another set of parameters is used to transform the hidden context over time. The difference is that the hidden layer is shared over time, as seen in the equation below.\n\\[ \\mathbf{h}^{(t)} = f(\\mathbf{h}^{(t-1)}, \\mathbf{x}^{(t)}; \\mathbf{\\theta}) \\]\nFigure 1: Computation graph of an RNN (By fdeloche - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=60109157) In the computation graph above, a recurrent network has three weight matrices associated with its forward pass. An input weight matrix \\(U \\in \\mathbb{R}^{H \\times D}\\) processes the features for each frame of the input sequence. The hidden layer has a weight matrix \\(V \\in \\mathbb{R}^{H \\times H}\\), where \\(H\\) is the number of hidden nodes. The output layer will have a weight matrix \\(W \\in \\mathbb{R}^{O \\times H}\\).\nForwards Pass To understand the computation graph of an RNN, consider an input of length \\(T\\) with \\(D\\) features. That is, each input sample is a sequence of features. This could be represented as encoded video data, text data, or any other sequence signals. To compute the output of a hidden layer \\(\\mathbf{h}\\) at time \\(t\\), take a linear combination of all input feature \\(x_i^t\\) at time \\(t\\) in addition to the output of the previous hidden layer and then add the linear combination of output activations for each node in the hidden layer:\n\\[ a_h^t = \\sum_{d=1}^D w_{dh} x_d^t + \\sum_{h\u0026rsquo;=1}^H w_{h\u0026rsquo;h} b_{h\u0026rsquo;}^{t-1}, \\]\nwhere \\(b_h^t = \\theta_h(a_h^t)\\) and we assume the bias term is concatenated with the weights.\nWeights in the hidden layer are crucial for RNNs to adapt to contextual features based on their occurrence relative to time. For example, a character-based language model based on a traditioinal network would produce similar output for consecutive letters that are the same. In an RNN, the hidden weights would produce a different output for each consecutive character even if it were the same.\nThe hidden layer outputs are used in both the subsequent computations through time as well as the output node for each instance \\(t\\). The inputs to the output node are computed from the hidden node at the same time as the output to the hidden activation:\n\\[ a_k^t = \\sum_{h=1}^H w_{hk}b_h^t. \\]\nBackwards Pass The gradients of a recurrent network are computed using backpropagation, similar to neural networks. Since the forward pass is over \\(t\\) time step, the backward pass must consider them as well. This variant of backpropagation for recurrent models is calling backpropagation through time (BPTT).\nLike a feed forward network, the output is dependent on the activation of the hidden layer. For a recurrent model, its dependence is through the output of the hidden layer as well as the pass to the next hidden time step.\n\\[ \\frac{\\partial \\mathcal{L}}{\\partial a_h^t} = \\frac{\\partial \\mathcal{L}}{\\partial b_h^t} \\frac{\\partial b_h^t}{\\partial a_h^t} \\]\n\\[ \\frac{\\partial \\mathcal{L}}{\\partial b_h^t} = \\sum_{k=1}^K \\frac{\\partial \\mathcal{L}}{\\partial a_k^t} \\frac{\\partial a_k^t}{\\partial b_h^t} + \\sum_{h\u0026rsquo;=1}^H \\frac{\\partial \\mathcal{L}}{\\partial a_{h\u0026rsquo;}^{t+1}} \\frac{\\partial a_{h\u0026rsquo;}^{t+1}}{\\partial a_{h}^t} \\]\nThe derivatives with respect to the weights are given as\n\\[ \\frac{\\partial \\mathcal{L}}{\\partial w_{ij}} = \\sum_{t=1}^T \\frac{\\partial \\mathcal{L}}{\\partial a_j^t} \\frac{\\partial a_j^t}{\\partial w_{ij}}. \\]\nBidirectional Recurrent Neural Networks Standard RNNs work for many problems with sequential input. Training such a model would consider the full input through time \\(T\\), but inference may only be able to consider the data up to time \\(t \u0026lt; T\\). There are sequential tasks which could leverage from both past and future context, such as language translation. For this case, BRNNs were proposed \u0026lt;\u0026amp;schusterBidirectionalRecurrentNeural1997\u0026gt;.\nFigure 2: Diagram of BRNN from Graves et al. References The Unreasonable Effectiveness of RNNs by Andrej Karpathy \u0026lt;\u0026amp;gravesSupervisedSequenceLabelling2012\u0026gt; Understanding LSTM Networks by Christopher Colah ","date":1649566800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649566800,"objectID":"7e13e6664e4126b5516049955a7d3ccb","permalink":"https://ajdillhoff.github.io/notes/recurrent_neural_networks/","publishdate":"2022-04-10T00:00:00-05:00","relpermalink":"/notes/recurrent_neural_networks/","section":"notes","summary":"Table of Contents Introduction Definition Bidirectional Recurrent Neural Networks References Introduction Neural networks are an effective tool for regression and classification tasks, but they do not consider the dependencies of information over time. Many tasks have implicit information that is dependent on input that may have already been processed or may not be seen until the future.\nRecurrent Neural Networks (RNN) consider the historical context of time-series data. Bi-directional Recurrent Neural Networks (BRNN) consider both historical and future context.","tags":["deep learning"],"title":"Recurrent Neural Networks","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Resources Introduction Gradient Descent and its Variants Adaptive Learning Rate Methods Parameter Initialization Resources Resources https://ruder.io/optimizing-gradient-descent/ https://www.deeplearningbook.org/contents/optimization.html Introduction empirical risk minimization - minimizing over an empirical distribution. Differs from risk minimization which is minimizing over the true distribution. We typically do not know the true distribution.\nComplex models are able to memorize the dataset.\nIn many applications for training, what we want to optimize is different from what we actually optimize since we need to have useful derivatives for gradient descent. For example, the 0-1 loss\n\\begin{equation*} L(i, j) = \\begin{cases} 0 \\qquad i = j \\\\ 1 \\qquad i \\ne j \\end{cases} \\qquad i,j \\in M \\end{equation*}\nis what we would really want to minimize for classification tasks. In practice we use something like Cross Entropy Loss. As pointed out by (Goodfellow et al.), there are sometimes advantages with using surrogate loss functions. A 0-1 loss may eventually fit the training set with 100% accuracy. At this point, no further optimization could take place as the error would be 0. With losses like negative log likelihood, optimization could continue which may result in increasing the margin between classes.\nLarger batch sizes provide a more accurate estimate of the gradient.\nRandomly selecting samples is crucial for learning. Datasets may be arranged in such a strong bias is present. Shuffling once isn\u0026rsquo;t enough because the data is biased after the first iteration. We could only get around this if we had the true distribution to generate new samples.\nIf our training set is extremely large, we may converge to a solution without ever having gone through all samples. Typically, models are able to train on multiple passes of the dataset to increase their generalization error. Each subsequent pass may increase the bias, but not enough to decrease generalization performance.\nThe gradient norm can be monitored while training to see if the issue is local minima or any other critical point (Zhao, Zhang, and Hu, n.d.). If the parameters were to get stuck at a critical point, the gradient norm should shrink over time.\nFigure 1: The gradient norm decreases as it settles into some minima (Zhao et al.). Gradient Descent and its Variants Original gradient descent update:\n\\[ \\theta = \\theta - \\eta \\nabla_{\\theta}J(\\theta) \\]\nHaving a constant value for \\(\\eta\\) means that the network will usually be unable to converge to a local minimum. As the parameters reach a minimum, the constant learning update means that it will jump around the true minimum point. This is usually remedied in part by setting up a decreasing learning rate schedule. This necessarily requires more manual guess work as to what the best annealing schedule would be.\nMomentum When the loss surface is more steep in one dimension than others, SGD will move back and forth in the directions of greatest descent while only slowly moving in the direction with a smaller decline. The figure below gives an example.\nFigure 2: SGD moves slower towards covergence for non-uniform surfaces. If the gradient had some momentum which built up over time, it would take fewer iterations to converge.\nFigure 3: SGD with momentum converges in fewer iterations. In practice, this can be implemented by adding some fraction of the previous update to the current step:\n\\begin{align*} \\mathbf{g}_t \u0026amp;= \\alpha \\mathbf{g}_{t-1} + \\eta \\nabla_{\\theta}J(\\theta)\\\\ \\theta \u0026amp;= \\theta - \\mathbf{g}_t \\end{align*}\nNesterov Momentum If we allow the momentum to keep increasing, the steps become greater and greater. This could lead to the parameters \u0026ldquo;rolling\u0026rdquo; out of the minimum up a steep incline. If our algorithm knew that it was coming up to an incline, it would be smarter to slow down. This is essentially what Nesterov momentum does.\nFigure 4: Nesterov momentum computes the gradient after applying momentum. \\begin{align*} \\mathbf{g}_t \u0026amp;= \\alpha \\mathbf{g}_{t-1} + \\eta \\nabla_{\\theta}J(\\theta - \\alpha \\mathbf{g}_{t-1})\\\\ \\theta \u0026amp;= \\theta - \\mathbf{g}_t \\end{align*}\nAdaptive Learning Rate Methods The rate at which a model converges to some solution is dependent on many factors. One that we can control is the learning rate. If the learning rate is too large, the model may never converge because it jumps too far in each iteration. If the learning rate is too small, it may take much longer to converge to any solution.\nIt would be ideal if the optimization algorithm could adapt its learning rate to local changes in the loss landscape. In that way, the algorithm would be less dependent on the initial learning rate.\nAdagrad Adagrad adapts the learning rate to the parameters following the idea that parameters associated with salient features should be updated less frequently (Duchi, Hazan, and Singer 2011). If they occur often, updating them with a larger step would result in a solution that is more dependent on them at the expense of other features.\nAdagrad uses a different learning rate for every parameter:\n\\[ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}}g_t \\]\nHere, \\(g_t = \\frac{\\partial J(\\theta)}{\\partial \\theta}\\). This provides a partial derivative for every parameter \\(\\theta_i\\). A history of gradient changes are accumulated in a matrix \\(G_t \\in \\mathbb{R}^{d \\times d}\\) which is a diagonal matrix containing the sum of squares of the gradients with respect to each \\(\\theta_i\\) up to the current step.\nIn effect, the parameters with larger partial derivatives have a sharper decrease in learning rate. The downside to this method is that, as squared gradients are accumulated in \\(G_t\\), the sum increases causing the learning rate to eventually be too small to learn.\nRMSProp To remedy the long term issues of Adagrad, Geoffrey Hinton proposed RMSProp. There was no formal publication for this. It was discussed and taught in Coursera course on Neural Networks. Instead of accumulating gradients, RMSProp uses an exponentially weighted moving average:\n\\[ \\mathbf{s}_t = \\rho \\mathbf{s} + (1 - \\rho)\\mathbf{g}_{t-1} \\odot \\mathbf{g}_t \\]\nA new parameter \\(\\rho\\) controls how much of the historical gradient is used. The update is\n\\[ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\mathbf{s}_t + \\epsilon}}\\mathbf{g}_t. \\]\nHinton proposed that \\(\\rho=0.9\\) in the original lectures.\nAdam One of the most popular gradient descent variants in used today is Adam (Kingma and Ba 2017). Short for Adaptive Moment Estimation, Adam adapts the learning rate to each parameter. Similar to RMSProp, it stores an exponentially moving average of past squared gradients. Adam additionally stores first-order moments of the gradients.\nAfter calculating the gradients \\(g_t\\) at time \\(t\\) the first and second moment estimates are updated as\n\\begin{align*} m_t \u0026amp;= \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t\\\\ v_t \u0026amp;= \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2 \\end{align*}\nThe estimates \\(m_t\\) and \\(v_t\\) are initialized to zero leading to updated estimates that are biased to zero. The authors counteract this by computing bias-corrected estimates:\n\\begin{align*} \\hat{m}_t \u0026amp;= \\frac{m_t}{1 - \\beta_1^t}\\\\ \\hat{v}_t \u0026amp;= \\frac{v_t}{1 - \\beta_2^t} \\end{align*}\nThe final update rule step is\n\\[ \\theta_t = \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}} + \\epsilon}. \\]\nThere are several other varients. A good overview of these can be found on Sebastian Ruder\u0026rsquo;s blog. The figures below provide some visual intuition of the behavior of common gradient descent variants. These visualizations were provided by Alec Radford.\nFigure 5: Behavior of algorithms at a saddle point (Credit: Alec Radford). Figure 6: Behavior of each algorithm on a loss surface (Credit: Alec Radford). Additional visualizations can be found here.\nParameter Initialization Due to the complexity of their loss landscapes, the choice of initialization can have a significant impact on the solution. This affects how quickly the model converges. Although recent work aims to smooth loss surfaces so that models are easier to train, deep learning models can be tricky to reproduce.\nThere is not much known about what makes the most optimal initialization strategy, but one property is that of weight symmetry. If all weights are initialized to the same value, their update will also be uniform. If two nodes are connected to the same input, there update will be uniform as well. Understanding this, a reasonable initialization strategy would be to ensure that the weights to not permit any symmetry in nodes connected to the same input.\nSmall weights during initialization may lead to vanishing gradients. Large weights may lead to exploding gradients as successive multiplications are applied. The parameter values should be large enough to propagate information effectively through the network.\nNormalized Initialization (Xavier) Normalized initialization chooses an initial scale of the weights of a fully connected layer based on the number input and output nodes:\n\\[ W_{i,j} \\sim U\\Bigg(-\\sqrt{\\frac{6}{m + n}}, \\sqrt{\\frac{6}{m+n}}\\Bigg), \\]\nwhere \\(m\\) and \\(n\\) are the number of input and output nodes, respectively. This initialization was empirically validated by (Glorot and Bengio, n.d.) with the goal that all layers have the same activation variance and back-propagated gradient variance.\nHe Initialization Xavier initialization is based on successive matrix multiplications without any non-linearities. Any deep learning model will surely break this assumption. He et al. derive another initialization strategy while considering rectified linear units (ReLU) and parametric rectified linear units (PReLU) (He et al. 2015).\nResources https://spell.ml/blog/lr-schedulers-and-adaptive-optimizers-YHmwMhAAACYADm6F\nReferences Duchi, John, Elad Hazan, and Yoram Singer. 2011. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.” Journal of Machine Learning Research 12 (61): 2121–59. http://jmlr.org/papers/v12/duchi11a.html. Glorot, Xavier, and Yoshua Bengio. n.d. “Understanding the Difﬁculty of Training Deep Feedforward Neural Networks,” 8. He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.” Arxiv:1502.01852 [Cs], February. http://arxiv.org/abs/1502.01852. Kingma, Diederik P., and Jimmy Ba. 2017. “Adam: A Method for Stochastic Optimization.” Arxiv:1412.6980 [Cs], January. http://arxiv.org/abs/1412.6980. Zhao, Yang, Hao Zhang, and Xiuyuan Hu. n.d. “Penalizing Gradient Norm for Efﬁciently Improving Generalization in Deep Learning,” 11. ","date":1649307600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649307600,"objectID":"4b310c5d80a400fae4538445a9f954e7","permalink":"https://ajdillhoff.github.io/notes/optimization_for_deep_learning/","publishdate":"2022-04-07T00:00:00-05:00","relpermalink":"/notes/optimization_for_deep_learning/","section":"notes","summary":"Table of Contents Resources Introduction Gradient Descent and its Variants Adaptive Learning Rate Methods Parameter Initialization Resources Resources https://ruder.io/optimizing-gradient-descent/ https://www.deeplearningbook.org/contents/optimization.html Introduction empirical risk minimization - minimizing over an empirical distribution. Differs from risk minimization which is minimizing over the true distribution. We typically do not know the true distribution.\nComplex models are able to memorize the dataset.\nIn many applications for training, what we want to optimize is different from what we actually optimize since we need to have useful derivatives for gradient descent.","tags":["deep learning"],"title":"Optimization for Deep Learning","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Convolution Operator Properties of Convolutions Parameter Sharing Pooling Backwards Pass Example Neural Networks for Image Classification Useful Resources Key Concepts\nInvariance and Equivariance Definition Padding, Stride, Kernel size, dilation Purpose of multiple feature maps Receptive fields and hierarchies of features Downsampling, Upsampling, Examples in research Introduction Dense neural networks made up of linear layers and a chosen activation function are not practical for image data. Consider an image of size \\(224\\times224\\times3\\). The first layer of a dense network would require a \\(150,528\\times n\\) parameter matrix, where \\(n\\) is the number of nodes in the first layer. It is common to build dense networks where the first layer has more nodes than input features. In this case, we would need a minimum of \\(150,528^2\\) parameters in the first layer. Even if we chose something much smaller like \\(n=1024\\), this would require \\(154,140,672\\) parameters for just the first layer. This is clearly impractical.\nAside from requiring a large number of parameters, we might ask whether it is beneficial to feed raw pixel values into a dense network. The network itself would be learning pixel-wise features with no regard to their spatial relationship. This makes our network\u0026rsquo;s job much more difficult because the spatial arrangement of features tells us so much about what we see. In practice, this means that the network would have to learn the same features at every location in the image. We would instead prefer this network to learn features that are invariant to translation. That is, the network should learn features that are the same regardless of where they appear in the image.\nInvariance to translation is very convenient and can save our network a lot of work in learning the same feature at every point in the input. It is also desirable that our network is invariant to other transformations such as rotation, scaling, skewing, and warping. Formally, a function \\(f(\\mathbf{x})\\) of an image \\(\\mathbf{x}\\) is invariant to a transformation \\(t(\\mathbf{x})\\) if\n\\[ f(t(\\mathbf{x})) = f(\\mathbf{x}). \\]\nAside from invariance, some models should be equivariant to certain transformations. That is, the output of the model should change in the same way as the input. Image segmentation models should be equivariant to translation. If we were to shift an image by a few pixels, the output segmentation mask should also shift by the same amount. Convolutional neural networks are equivariant to translation.\nConvolution Operator A convolution is a function that takes two functions as input and produces a third function as output. The first function is the input and the second function is the kernel. The output is called the feature map. The kernel is also sometimes called the filter.\n\\[ (f * g)(t) = \\int f(t-a)g(a)da \\]\nWe can view them more concretely by considering the functions to be vectors. For example, let the function \\(f\\) be an input vector \\(x\\) and \\(w\\) be a kernel representing a filter. The convolution operator is then\n\\[ (x * w)(t) = \\int x(t-a)w(a)da. \\]\nThe result the feature map representing the response of the kernel at each location in the input.\nIn the case of discrete values, the operator is written as\n\\[ (x * w)(t) = \\sum_{a}x(t-a)w(a). \\]\nIn machine learning, the kernel \\(w\\) is usually represented by some set of parameters that is optimized.\nCNNs for images use a 2D convolution defined as\n\\[ (I * K)(i, j) = \\sum_m \\sum_n I(i-m, j-n)K(m, n). \\]\nIn this formulation, the kernel is effectively flipped across the vertical and horizontal axis.\nFigure 1: 2D Convolution (Image Credit: Song Ho Ahn (linked above)). In practice, most deep learning APIs implement cross-correlation. Whether the function is implemented as true convolution makes no difference when it comes to optimizing a deep model since filter weights that are produced with cross-correlation would be produced, albeit flipped, with convolution.\n\\[ (K * I)(i, j) = \\sum_m \\sum_n I(i+m, j+n)K(m, n). \\]\nProperties of Convolutions Convolutional networks are commonly built on full or valid convolutions. Other variants have also been explored. Here, we will briefly discuss the different properties of this operator. A more detailed treatment can be found in (Dumoulin and Visin 2018).\nPadding By definition, a convolution of an input with a filter of size \\(n\\times n\\) will produce an output of size \\((m-n+1)\\times(m-n+1)\\), where \\(m\\) is the size of the input. This means that the output will be smaller than the input. This is often referred to as a valid convolution. The figure below shows a convolution between a \\(3\\times3\\) kernel and a \\(5\\times5\\) input.\nFigure 2: A valid convolution (Dumoulin and Visin 2018). The output of this convolution is a \\(3\\times3\\) feature map. This is a problem if we want to build a deep network. Each convolution will reduce the size of the input. If we were to stack multiple convolutional layers, the output would eventually be too small to be useful. If we want our output to be same size as the input, we can add padding to the original input image before convolving it. This is often known as a full convolution. An example is shown below.\nFigure 3: A full convolution (Dumoulin and Visin 2018). Stride So far, we have only looked at convolutions which step by 1 unit as they shift over the image. We can control the size of this step, or stride, to produce different outcomes. Picking a non-unit stride has a number of effects on the features that are learned in a convolutional neural network.\nDimensionality reduction: Skipping over pixels reduces the size of the output feature map. This provides another way of downsampling the input. Less computation: Fewer computations are required to produce the output feature map. Increased field of view: A larger stride increases the field of view of the kernel, leading to larger receptive fields in deeper layers. Given an input of size \\(m\\times m\\) and a kernel of size \\(n\\times n\\), the output size of a convolution with stride \\(s\\) is given by\n\\[ \\left\\lfloor\\frac{m-n}{s}\\right\\rfloor + 1. \\]\nThe figure below shows a convolution with stride 2 on a \\(5\\times5\\) input.\nFigure 4: A convolution with stride 2 (Dumoulin and Visin 2018). Kernel Size The size of the kernel has a large impact on the features that are learned. A larger kernel will have a larger receptive field. This means that the kernel will be able to capture more information about the input. However, this comes at the cost of increased computation. Common kernel sizes in most CNNs are \\(3\\times3\\), \\(5\\times5\\), and \\(7\\times7\\). It is also convenient to pick an odd kernel size so that the kernel has a center pixel.\nDilation Around 2015, a research trend for CNNs was to find a way to increase the receptive field without adding more parameters. The result is a dilated convolution. The output of a dilated convolution is computed by skipping over pixels in the input. The figure below shows a \\(3\\times3\\) kernel with a dilation of 2.\nFigure 5: A dilated convolution (Dumoulin and Visin 2018). The output size is computed as\n\\[ \\left\\lfloor\\frac{m + 2p - n - (n-1)(d-1)}{s}\\right\\rfloor + 1, \\]\nwhere \\(p\\) is the amount of padding and \\(d\\) is the dilation factor.\nParameter Sharing In a densely connected layer, each input has a corresponding weight attached to it. For example, we ran a few introductory experiments on the CIFAR10 dataset using a deep, densely connected network. To reduce the amount of parameters in the first layer, we converted each image to grayscale. The input also had to be vectorized in order to be processed. With a processed image of size \\(32 \\times 32\\), this resulted in a \\(1024\\) dimensional vector for each input. Our first layer had \\(512\\) nodes resulting in a parameter matrix of size \\(1024 \\times 512\\).\nConvolution layers have shared parameters, meaning the same parameters are used for each region on the input. A single channel 2D filter of size \\(n \\times n\\) only requires \\(n \\times n\\) parameters. Each kernel is applied to every location in the original input using the same parameters.\nKernels are equivariant to translation because of their shared parameters. That is, as the input changes, the output will change in the same way. Formally, two functions \\(f\\) and \\(g\\) are equivarient if\n\\[ f(g(x)) = g(f(x)). \\]\nIn the context of image features, a kernel applied across an image will produce strong responses in regions that exhibit the same local features. For example, a kernel that detects horizontal lines will produce strong responses across all parts of the image that show a large contrast between vertical pixels.\nPooling When a convolution is applied to some input image, the resulting output feature map represents the responses of the kernel applied to each location in the image. If this original image were to be shifted by a few pixels, the reponses would also be shifted. In order to increase the robustness of a model to small perturbations such as translation, a pooling layer was historically employed after each non-linear activation following a convolutional layer.\nThey effectively provide a summary statistic of a local region by selecting the average or maximum responses in a small window. This provides translation invariance since the maximum response will be the same for a region even if it is translated by a small amount. It also acts as a quick way to downsample the image, leading to fewer parameters in the model.\nModern works do not employ pooling operations as often. For example (He et al. 2016) perform dimensionality reduction with \\(1 \\times 1\\) convolutions. (Springenberg et al. 2015) argue that fully convolutional networks can achieve the same performance without max pooling.\n\u0026ldquo;The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster.\u0026rdquo; - Geoffrey Hinton\nBackwards Pass The parameters of a convolutional layer are updated via backpropagation like any other layer with trainable parameters. Given a kernel \\(w\\), it is necessary to compute \\(\\frac{\\partial \\mathcal{L}}{\\partial w_{m\u0026rsquo;,n\u0026rsquo;}}\\), where \\(w_{m\u0026rsquo;, n\u0026rsquo;}\\) is the \\((m\u0026rsquo;, n\u0026rsquo;)th\\) entry of the kernel. This entry affects all entries in the feature map, so \\(\\frac{\\partial \\mathcal{L}}{\\partial w_{m\u0026rsquo;,n\u0026rsquo;}}\\) will sum over all such entries.\nTo show the gradient calculation, we will assume a convolutional layer with zero padding and unit stride with a square \\(2 \\times 2\\) kernel applied to a square \\(3 \\times 3\\) input. The output map is then \\((3 - 2 + 1) \\times (3 - 2 + 1) = 2 \\times 2\\).\n\\[ \\frac{\\partial \\mathcal{L}}{\\partial w_{m\u0026rsquo;,n\u0026rsquo;}} = \\sum_{i=0}^2 \\sum_{j=0}^2 \\frac{\\partial \\mathcal{L}}{\\partial x_{i, j}} \\frac{\\partial x_{i,j}}{\\partial w_{m\u0026rsquo;, n\u0026rsquo;}} \\]\nIf \\(\\mathbf{z}^{(l-1)}\\) is the output from the previous layer, then\n\\begin{align*} \\frac{\\partial x_{i, j}}{\\partial w_{m\u0026rsquo;, n\u0026rsquo;}} \u0026amp;= \\frac{\\partial}{\\partial w_{m\u0026rsquo;, n\u0026rsquo;}} \\sum_{m} \\sum_{n} w_{m, n} z_{i+m, j+n}^{(l-1)} + b\\\\ \u0026amp;= \\frac{\\partial}{\\partial w_{m\u0026rsquo;, n\u0026rsquo;}} w_{m\u0026rsquo;, n\u0026rsquo;}z_{i+m\u0026rsquo;, j+n\u0026rsquo;}^{(l-1)}\\\\ \u0026amp;= z_{i+m\u0026rsquo;, j+n\u0026rsquo;}^{(l-1)} \\end{align*}\nThen \\(\\frac{\\partial \\mathcal{L}}{\\partial w_{m\u0026rsquo;,n\u0026rsquo;}}\\) becomes\n\\begin{align*} \\frac{\\partial \\mathcal{L}}{\\partial w_{m\u0026rsquo;,n\u0026rsquo;}} \u0026amp;= \\sum_{i=0}^2 \\sum_{j=0}^2 \\frac{\\partial \\mathcal{L}}{\\partial x_{i, j}} z_{i+m\u0026rsquo;, j+n\u0026rsquo;}^{(l-1)}\\\\ \u0026amp;= \\frac{\\partial \\mathcal{L}}{\\partial x_{i, j}} * z_{m\u0026rsquo;, n\u0026rsquo;}^{(l-1)}. \\end{align*}\n\\(\\frac{\\partial \\mathcal{L}}{\\partial x_{i, j}}\\) represent the gradients with respect to the feature maps. To match the flipped kernel used in the forward pass, they are flipped in an opposite manner.\nExample Let\u0026rsquo;s train and evaluate a convolutional neural network on the OG network: LeNet5 (LeCun et al. 1989).\nNeural Networks for Image Classification ILSVRC The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is the most popular image classification and object detection challenge starting in 2010. It now exists as the ILSVRC 2012-2017 challenge on Kaggle.\nAlexNet https://code.google.com/archive/p/cuda-convnet/\nThe network that arguably popuarlized deep learning by achieving a 37.5% top-1 and 17% top-5 error rate on the ILSVRC-2010 test set. This model performed significantly better than leading competitors (Krizhevsky, Sutskever, and Hinton 2017).\nFigure 6: ILSVRC-2010 results reported by Krizhevsky et al. This performance was based on many different insights and techniques including ReLU activations and dropout. The authors stated in their original publication that the large capacity of the model is necessary to fully describe the diversity of objects in ImageNet.\nArchitecture Figure 7: AlexNet architecture (from Krizhevsky et al.) AlexNet is made up of 5 convolutional layers followed by 3 fully-connected layers. The outputs of the last layer are used as input to the softmax function.\nEach layer uses the ReLU activation function.\n\\[ f(x) = \\max(0, x) \\]\nThe justification for switching to ReLU as opposed to sigmoid or tanh is the faster training times. Experiments on smaller CNNs show that networks with ReLU reach 25% training error on CIFAR-10 six times faster than those with tanh activations.\nFigure 8: Training loss over time using ReLU (solid) versus tanh (dotted) (from Krizhevsky et al.) Another benefit of ReLU activations is that they are less reliant on input normalization. In a saturating activation function like tanh, large absolute values in the inputs will be clamped to either -1 or 1. ReLU is unbounded above 0. Networks can still train as long as some input is positive. Local response normalization (LRN) is used after the first and second convolutional layers.\nThe motivation behind LRN is taken from lateral inhibition in neurobiology. An overly excited neuron (one with a high response) can subdue or dampen the responses from local neighbors. If all responses in a local region are uniformly large, which can happend since ReLU is unbounded, it will dampen them all.\nIn practice, they showed that applying LRNs to their model reduced the top-1 and top-5 error rates by 1.4% and 1.2%, respectively.\nRegularization The entire network has 60 million parameters. Even with so many parameters and training on a dataset with over 8 million images, their model overfits the training data quickly without the aid of regularization. They employ both image translations and horizontal reflections.\nThe use of translations is where the popular \\(224 \\times 224\\) training size originated. The original size of the images in the dataset is \\(256 \\times 256\\). To work with random translations without worrying about padding, they crop the final output to \\(224 \\times 224\\). The final output of the network extracts 5 \\(224 \\times 224\\) patches from the test input and averages the network prediction made on each patch.\nAdditionally, they alter the RGB intensities so that the network is less reliant on specific intensities and illumination for each object. The intuition is that the identity of an object is invariant to lighting conditions.\nAs a last form of regularization, they employ dropout in the first two fully-connected layers.\nTraining They trained their model on a training set of 1.2 million images using two NVIDIA GTX 580 3GB GPUs. They had to write their own optimized CUDA code for this since deep learning frameworks such as Tensorflow and PyTorch did not exist yet. The training took ~6 days to pass 90 epochs.\nVGG Published in 2015, (Simonyan and Zisserman 2015) explore how depth plays a role in convolutional neural networks. They systematically increase the depth of the network while keep other hyperparameters fixed. The filter sizes are also kept at \\(3 \\times 3\\).\nSimilar to (Krizhevsky, Sutskever, and Hinton 2017), they use ReLU activations and in only one of their models to they employ Local Response Normalization. They found that adding LRN to their model did not increase performance. Instead, it only increased computation time and memory consumption. Their models are summarized in the table below.\nFigure 9: Model configurations used (Simonyan and Zisserman). The number of parameters for each network is 133 million, 133 million, 134 million, 138 million, and 144 million starting from A to E.\nGoogLeNet Figure 10: The network-in-network architecture pairs perfectly with the Inception meme. Proposed a 22-layer network architecture that has \\(12 \\times\\) fewer parameters than (Krizhevsky, Sutskever, and Hinton 2017).\nThe authors were already thinking about applications in mobile computing, where hardware limitations would require smaller networks that still perform well.\n\u0026ldquo;In this paper, we will focus on an efficient deep neural network architecture for computer vision, codenamed Inception, which derives its name from the Network in network paper by Lin et al. in conjunction with the famous “we need to go deeper” internet meme.\u0026rdquo; - Szegedy et al.\nIt was apparent at the time that building larger networks would generally lead to better performance. Adding more parameters leads to easier overfitting. Bigger networks also mean more computation. If the goal is to adapt high quality networks into mobile computing, solutions would have to include more sophistication than simply adding more components.\nHebbian Learning A linear increase in filters leads to a quadratic increase in computation. If most filter parameters end up being close to 0, then this increase in model capacity is wasted. One solution is to include sparsity in the network instead of having dense connections. Szegedy et al. were motivated by the work of Arora et al., which they summarized as follows.\n\u0026ldquo;Their main result states that if the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs.\u0026rdquo; - Szegedy et al.\nThis result relates with Hebbian theory on synaptic plasticity which is summarized as \u0026ldquo;neurons that fire together, wire together.\u0026rdquo;\nFrom Theory to Architecture Motivated by sparse connections, the architecture is designed to approximate sparsity given current dense components like convolutional layers.\nFigure 11: Naive version of the Inception module (Szegedy et al.) The Inception module design as seen above is motivated as follows. In layers closer to the raw input, filters would be grouped into local regions. In this case, a \\(1 \\times 1\\) convolution would summarize these groups.\nFor clusters that are spread out, a larger filter would be needed to cover the larger regions. This motivates the use of \\(3 \\times 3\\) and \\(5 \\times 5\\) filters.\nThe choice to include a max pooling function in each module is based on previous successes of using max pooling.\nFigure 12: Description of layers from Szegedy et al. Vanishing Gradients Creating a deeper network means that training is more susceptible to the vanishing gradient problem. They noted that shallower networks that perform well on image classification would surely provide strong disciminative features. They leverage this idea by computing 2 additional intermediate outputs: one in the middle of the network and an additional output 3 layers beyond that one. This permits the gradients to be strengthened by intermediate losses when combined with the original gradients.\nFigure 13: GoogLeNet model (Szegedy et al.) Results GoogLeNet took 1st place in the 2014 ILSVRC with a 6.67% top-5 error rate.\nResNet By 2016, it was clear that deeper models could build a richer hierarchy of features leading to better performance on a wide range of computer vision tasks. However, with deeper networks comes the vanishing gradient problem. Training them remained difficult for a time, but initialization and other normalization techniques found ways to resolve this issue.\nWith deeper networks, a new problem appeared. Adding more layers generally results in higher accuracy. At a certain point, adding additional layers leads to a decrease in accuracy. Many experiments ruled out the possibility of overfitting by observing that the training error was increasing as well.\nFigure 14: Result of experiments showing that decreased accuracy was not a result of overfitting. Identity Mappings Consider a shallow network with some measure performance on a task. If we were to add additional layers to make this network deeper, but those layers were simply identity mappings, then we should expect an error no greater than the original shallow network. However, current solvers are unable to find such a solution in a reasonable amount of time on an equally deep network optimized from a random initialization.\nResidual Functions The main idea of this paper is to attempt to learn a residual function \\(\\mathcal{F}(\\mathbf{x}) := \\mathcal{H}(\\mathbf{x}) - \\mathbf{x}\\) of the desired mapping \\(\\mathcal{H}(\\mathbf{x})\\) rather than attempting to learn the mapping directly. The desired mapping then given by \\(\\mathcal{H}(\\mathbf{x}) = \\mathcal{F}(\\mathbf{x}) + \\mathbf{x}\\). If it were optimal to learn an identity mapping, the idea is that it would be simpler to learn by moving towards a 0 residual.\nFigure 15: Residual unit (He et al.) The function can be implemented into neural networks by using skip connections, as seen in the figure above. Adding these identity mappings does not require any additional parameters, as the input is simply passed to the end of the stack.\nArchitecture Complexity They compare a 34-layer plain network based on the VGG-19 architecture with a 34-layer residual network. They note that VGG-19 has more filters and higher complexity than their residual network. Specifically, VGG-19 requires 19.6 billion FLOPs compared to only 3.6 billion for their 34-layer residual network.\nFigure 16: Comparison of architectures and their complexity (He et al.) Results They evaluate how well the residual networks generalize when adding more layers. As mentioned in the introduction, typical models would see an increase in training error as the number of layers were increased.\nFigure 17: Training comparisons between plain and residual networks (He et al.) Their ensemble of models achieved 3.57% top-5 error on the ImageNet test set, achieving 1st place in the ILSVRC 2015 classification challenge. It additionally was adapted to other challenges and won first place on ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in both the ILSVRC and COCO 2015 competitions.\nUseful Resources https://www.songho.ca/dsp/convolution/convolution.html#convolution_2d https://github.com/vdumoulin/conv_arithmetic https://cs231n.github.io/convolutional-networks/ https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/ https://grzegorzgwardys.wordpress.com/2016/04/22/8/ References Dumoulin, Vincent, and Francesco Visin. 2018. “A Guide to Convolution Arithmetic for Deep Learning.” Arxiv:1603.07285 [Cs, Stat], January. http://arxiv.org/abs/1603.07285. He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “Deep Residual Learning for Image Recognition.” In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–78. Las Vegas, NV, USA: IEEE. https://doi.org/10.1109/CVPR.2016.90. Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. “ImageNet Classification with Deep Convolutional Neural Networks.” Communications of the Acm 60 (6): 84–90. https://doi.org/10.1145/3065386. LeCun, Yann, Bernhard Boser, John Denker, Donnie Henderson, R. Howard, Wayne Hubbard, and Lawrence Jackel. 1989. “Handwritten Digit Recognition with a Back-Propagation Network.” In Advances in Neural Information Processing Systems. Vol. 2. Morgan-Kaufmann. https://papers.nips.cc/paper/1989/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html. Simonyan, Karen, and Andrew Zisserman. 2015. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” Arxiv:1409.1556 [Cs], April. http://arxiv.org/abs/1409.1556. Springenberg, Jost Tobias, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. 2015. “Striving for Simplicity: The All Convolutional Net.” Arxiv:1412.6806 [Cs], April. http://arxiv.org/abs/1412.6806. ","date":1648875600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648875600,"objectID":"9924d8865e727ea2a7a88cf96aae9bff","permalink":"https://ajdillhoff.github.io/notes/convolutional_neural_networks/","publishdate":"2022-04-02T00:00:00-05:00","relpermalink":"/notes/convolutional_neural_networks/","section":"notes","summary":"Table of Contents Introduction Convolution Operator Properties of Convolutions Parameter Sharing Pooling Backwards Pass Example Neural Networks for Image Classification Useful Resources Key Concepts\nInvariance and Equivariance Definition Padding, Stride, Kernel size, dilation Purpose of multiple feature maps Receptive fields and hierarchies of features Downsampling, Upsampling, Examples in research Introduction Dense neural networks made up of linear layers and a chosen activation function are not practical for image data. Consider an image of size \\(224\\times224\\times3\\).","tags":["deep learning","computer vision"],"title":"Convolutional Neural Networks","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction What makes a model deep? Deep Networks Deep vs. Shallow Networks High Dimensional Structured Data Activation Functions Loss Functions A Typical Training Pipeline Useful Links Introduction Deep learning is a term that you\u0026rsquo;ve probably heard of a million times by now in different contexts. It is an umbrella term that encompasses techniques for computer vision, bioinformatics, natural language processing, and much more. It almost always involves a neural network of some kind that was trained on a large corpus of data.\nThe existence of the word \u0026ldquo;deep\u0026rdquo; implies a contrast to \u0026ldquo;shallow\u0026rdquo; learning. Some definition define a deep network as an artificial neural network with more than 1 layer. Another definition is that a deep model will include a hierarchy of features that are learned from the data. These features are learned as part of the optimization process as opposed to being manually engineered as is required in other machine learning techniques.\nIf you are not yet familiar with neural networks, follow the link to learn about their basics as they are the foundation of deep learning systems.\nWe will cover how to implement an array of deep learning models for different tasks. Different layers and activation functions will be explored as well as the effect of regularization. There will also be a focus on best practices for organizing a machine learning project.\nWhat makes a model deep? We begin by comparing shallow networks with deep networks. What defines a deep network? Is it as simple as crossing a threshold into \\(n\\) layers? As evidenced by (Zeiler and Fergus 2013) deeper networks allow for a more robust hierarchy of image features.\nThere is work by (Montúfar et al. 2014) which suggests that shallow networks require an exponential amount of nodes as compared to deeper networks. Additionally, there are many individual results which suggest that deeper networks provide better task generalization.\nAs we will later see when studying Convolutional Neural Networks, the optimization of such deep networks produces features that maximize the performance of a task. That is, the network is not only optimizing the overall performance of task, but it produces features from the data that may be useful in other contexts. This is particularly useful for transfer learning, where large pre-trained models can be used as starting points for novel tasks. The benefit being that a complete retraining of the model is not necessary.\nDeep Networks Like neural networks, deep networks are defined by the number of layers, nodes per layer, activation functions, and loss functions. We now review the forward and backward pass, providing more insight into the structure and usage of deep networks along the way.\nConsider a deep network with \\(L\\) layers. Layer \\(l\\) has \\(n_{l-1}\\) input connections and \\(n_l\\) output nodes and activation function \\(g^{(l)}\\). The final output is evaluated with some ground truth using a loss function \\(\\mathcal{L}\\).\nForward Pass \\begin{align*} \\mathbf{a}^{(l)} \u0026amp;= W^{(l)}\\mathbf{z}^{(l-1)} + \\mathbf{b}^{(l)}\\\\ \\mathbf{z}^{(l)} \u0026amp;= g^{(l)}(\\mathbf{a}^{(l)})\\\\ \\end{align*}\nThis is repeated from the input to the last layer. For the first layer \\(l=1\\), the input \\(\\mathbf{z}^{(0)} = \\mathbf{x}\\). In practice, the output \\(\\mathbf{a}^{(l)}\\) is cached since it is required for the backward pass. This prevents the values from needing to be computed twice.\nIt is also worth it to study the sizes of the matrices while performing a forward pass. For a layer \\(l\\), \\(W^{(l)} \\in \\mathbb{R}^{n_l \\times n_{l-1}}\\) and the input \\(\\mathbf{z}^{(l-1)} \\in \\mathbb{R}^{n_{l-1} \\times 1}\\). When training, it is common to perform batch gradient descent with batches of input of size \\(B\\). Then, \\(\\mathbf{z}^{(l-1)} \\in \\mathbb{R}^{n_{l-1}\\times B}\\) and \\(\\mathbf{a}^{(l)}, \\mathbf{b}^{(l)} \\in \\mathbb{R}^{n_l \\times B}\\).\nBackward Pass During the backward pass, the gradient is propagated from the last layer to the first. Each layer that contains trainable parameters must also compute the gradient of the network output with respect to the weights and biases. This can be done in a modular way, as shown next.\nConsider the last layer. The gradients with respect to the weights and biases are\n\\begin{align*} \\frac{d\\mathcal{L}}{dW^{(L)}} \u0026amp;= \\frac{d\\mathcal{L}}{d\\mathbf{z}^{(L)}} \\frac{d\\mathbf{z}^{(L)}}{d\\mathbf{a}^{(L)}} \\frac{d\\mathbf{a}^{(L)}}{dW^{(L)}}\\\\ \\frac{d\\mathcal{L}}{d\\mathbf{b}^{(L)}} \u0026amp;= \\frac{d\\mathcal{L}}{d\\mathbf{z}^{(L)}} \\frac{d\\mathbf{z}^{(L)}}{d\\mathbf{a}^{(L)}} \\frac{d\\mathbf{a}^{(L)}}{d\\mathbf{b}^{(L)}}. \\end{align*}\nTo see how the gradient continues to be propagated backward, compute the same thing for layer \\(L-1\\)\n\\begin{align*} \\frac{d\\mathcal{L}}{dW^{(L-1)}} \u0026amp;= \\frac{d\\mathcal{L}}{d\\mathbf{z}^{(L)}} \\frac{d\\mathbf{z}^{(L)}}{d\\mathbf{a}^{(L)}} \\frac{d\\mathbf{a}^{(L)}}{d\\mathbf{z}^{(L-1)}} \\frac{d\\mathbf{z}^{(L-1)}}{d\\mathbf{a}^{(L-1)}} \\frac{d\\mathbf{a}^{(L-1)}}{dW^{(L-1)}}\\\\ \\frac{d\\mathcal{L}}{d\\mathbf{b}^{(L-1)}} \u0026amp;= \\frac{d\\mathcal{L}}{d\\mathbf{z}^{(L)}} \\frac{d\\mathbf{z}^{(L)}}{d\\mathbf{a}^{(L)}} \\frac{d\\mathbf{a}^{(L)}}{d\\mathbf{z}^{(L-1)}} \\frac{d\\mathbf{z}^{(L-1)}}{d\\mathbf{a}^{(L-1)}} \\frac{d\\mathbf{a}^{(L-1)}}{d\\mathbf{b}^{(L-1)}}. \\end{align*}\nAs seen above, to continue propagating the gradient backward, each layer \\(l\\) must also compute\n\\[ \\frac{d\\mathbf{a}^{(l)}}{d\\mathbf{z}^{(l-1)}}. \\]\nTo summarize, every layer with trainable parameters will compute\n\\begin{align*} \\frac{d\\mathcal{L}}{dW^{(l)}} = \\frac{d\\mathbf{a}^{(l+1)}}{d\\mathbf{z}^{(l)}} \\frac{d\\mathbf{z}^{(l)}}{d\\mathbf{a}^{(l)}} \\frac{d\\mathbf{a}^{(l)}}{dW^{(l)}}\\\\ \\frac{d\\mathcal{L}}{d\\mathbf{b}^{(l)}} = \\frac{d\\mathbf{a}^{(l+1)}}{d\\mathbf{z}^{(l)}} \\frac{d\\mathbf{z}^{(l)}}{d\\mathbf{a}^{(l)}} \\frac{d\\mathbf{a}^{(l)}}{d\\mathbf{b}^{(l)}}. \\end{align*}\nThe term \\(\\frac{d\\mathbf{a}^{(l+1)}}{d\\mathbf{z}^{(l)}}\\) is the gradient that is propagated from layer \\(l+1\\).\nDeep vs. Shallow Networks As mentioned above, a shallow network can approximate any continuous function to arbitrary precision. If a deep network can represent the composition of two shallow networks, then it can also approximate any continuous function to arbitrary precision. Then why are deep networks better than shallow networks when both can approximate any function? There are a few compelling reasons as to why, starting with the capacity of the network and the number of linear regions it can represent per parameter.\nAs discussed in Understanding Deep Learning (Prince 2023), a shallow network with 1 input, 1 output, and \\(D \u0026gt; 2\\) hidden units can create up to \\(D + 1\\) linear regions using \\(3D+1\\) parameters. The \\(3D + 1\\) comes from the fact that the hidden layer requires \\(D\\) parameters for the weights with an extra \\(D\\) parameters for the bias terms. To convert from the hidden layer to the output layer, there are \\(D\\) parameters for the weights and 1 parameter for the bias term. The figure below shows the maximum number of linear regions as a function of the number of parameters for networks that map one input to one output.\nFigure 1: Maximum number of linear regions as a function of the number of parameters for networks that map one input to one output (Prince 2023). High Dimensional Structured Data For high dimensional structured data, such as images, deep networks are able to learn a hierarchy of features that are useful for the task at hand while requiring a significantly smaller number of parameters than a shallow network. Consider a \\(100\\times100\\) image used as input to a shallow network with 1 hidden layer. This would require \\(10,001\\) parameters to represent the weights and biases. If we instead use a deep network with with convolutional layers, we can use significantly fewer parameters. We will see this more closely when we study Convolutional Neural Networks.\nActivation Functions Sigmoid Function\n\\[ \\sigma(\\mathbf{x}) = \\frac{1}{1 + \\exp(-\\mathbf{x})} \\]\nDerivative\n\\[ \\sigma(\\mathbf{x})(1 - \\sigma(\\mathbf{x})) \\]\nFigure 2: Sigmoid non-linearity (Wikipedia) Loss Functions Loss functions are used to evaluate the performance of a model. In the context of gradient descent, their gradient with respect to the model parameters is used to update the parameters. Loss functions can be constructed using maximum likelihood estimation over a probability distribution or by using a distance metric between the model output and the ground truth. The table below from (Prince 2023) shows some common loss functions and their use cases.\nData Type Domain Distribution Use Univariate, continuous, unbounded \\(\\mathbb{R}\\) univariate normal Regression Univariate, discrete, binary \\(\\{0, 1\\}\\) Bernoulli Binary Classification Univariate, discrete, bounded \\(\\{0, 1\\}^K\\) Multinoulli Multiclass Classification A Typical Training Pipeline When training and evaluating models, especially on benchmark datasets, it is important to properly test their generalization performance. This test is crucial when comparing the efficacy of your ideas versus baseline evaluations or competing methods.\nTo ensure that your model is evaluated in a fair way, it is common to set aside a set of test data that is only used during the final comparison. This data is typically annotated so that some metric can be used.\nIt is true that the training data drives the parameter tuning during optimization. This is most commonly done with gradient descent. However, we will also change the hyperparamers such as learning rate, batch size, and data augmentation. In this case, we want to evaluate the relative performance of each change.\nIf we use the test set to do this, then we are necessarily using the test set for training. Our biases and intuitions about the model\u0026rsquo;s performance would be implicitly influenced by that set. To track our relative changes without using the test set, we can take a portion of the original training set and label it as our validation set.\nThe split between training, validation, and test data is relatively small. Most modern datasets are large, with millions of samples. Consider ImageNet, an image classification dataset with over 14 million samples. Taking 10,000 samples to serve as a validation set is only \\(~.07\\%\\) of the dataset.\nMost modern machine learning frameworks have an easy way to split the dataset. We can do this in PyTorch using torch.utils.data.random_split.\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size]) Useful Links Deep learning Bootcamp: Kaiming He (author of ResNet) References Montúfar, Guido, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. 2014. “On the Number of Linear Regions of Deep Neural Networks.” Arxiv:1402.1869 [Cs, Stat], June. http://arxiv.org/abs/1402.1869. Prince, Simon J.D. 2023. Understanding Deep Learning. MIT Press. https://udlbook.github.io/udlbook/. Zeiler, Matthew D., and Rob Fergus. 2013. “Visualizing and Understanding Convolutional Networks.” Arxiv:1311.2901 [Cs], November. http://arxiv.org/abs/1311.2901. ","date":1648530000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697932800,"objectID":"46433fcb0e55a5db6defcac8271e45b5","permalink":"https://ajdillhoff.github.io/notes/deep_learning/","publishdate":"2022-03-29T00:00:00-05:00","relpermalink":"/notes/deep_learning/","section":"notes","summary":"Table of Contents Introduction What makes a model deep? Deep Networks Deep vs. Shallow Networks High Dimensional Structured Data Activation Functions Loss Functions A Typical Training Pipeline Useful Links Introduction Deep learning is a term that you\u0026rsquo;ve probably heard of a million times by now in different contexts. It is an umbrella term that encompasses techniques for computer vision, bioinformatics, natural language processing, and much more. It almost always involves a neural network of some kind that was trained on a large corpus of data.","tags":["deep learning"],"title":"Deep Learning","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction AdaBoost Introduction Combining predictions from multiple sources is usually preferred to a single source. For example, a medical diagnosis would carry much more weight if it was the result of a consensus of several experts. This idea of prediction by consensus is a powerful way to improve classification and regression models. In fact, good performance of a committee of models can be achieved even if each individual model is conceptually very simple.\nBoosting is one such way of building a committee of models for classification or regression and is popularly implemented by an algorithm called AdaBoost.\nAdaBoost Given a dataset \\(\\{\\mathbf{x}_i\\}\\) and target variables \\(\\{\\mathbf{y}_i\\}\\), AdaBoost first initializes a set of weights corresponding to each data sample as \\(w_i = \\frac{1}{N}\\). At each step of the algorithm, a simple classifier, called a weak learner is fit to the data. The weights for each sample are adjusted based on the individual classifier\u0026rsquo;s performance. If the sample was misclassified, the relative weight for that sample is increased. After all classifiers have been fit, they are combined to form an ensemble model.\nThe Algorithm Initialize data weights \\({w_i}\\) as \\(w_i^{(1)} = \\frac{1}{n}\\) for \\(i = 1, \\dots, n\\).\nFit each weak learner \\(j\\) to the training data by minimizing the misclassification cost:\n\\[ \\sum_{i=1}^n w_i^{(j)} \\mathbb{1}(f_j(\\mathbf{x}_i) \\neq \\mathbf{y}_i) \\]\nCompute a weighted error rate\n\\[ \\epsilon_j = \\frac{\\sum_{i=1}^n w_i^{(j)} \\mathbb{1}(f_j(\\mathbf{x}_i) \\neq \\mathbf{y}_i)}{\\sum_{i=1}^n w_i^{(j)}} \\]\nUse the weighted error rate to compute a weight for each classifier such that misclassified samples are given higher weight:\n\\[ \\alpha_j = \\ln \\bigg\\{\\frac{1 - \\epsilon_j}{\\epsilon_j}\\bigg\\}. \\]\nUpdate the data weights for the next model in the sequence:\n\\[ w_i^{j+1} = w_i^{j} \\exp\\{\\alpha_j \\mathbb{1}(f_j(\\mathbf{x}_i \\neq \\mathbf{y}_i)\\}. \\]\nOnce all weak learners are trained, the final model predictions are given by\n\\[ Y_M(\\mathbf{x}) = \\text{sign} \\Bigg(\\sum_{j=1}^M \\alpha_j f_j(\\mathbf{x})\\Bigg). \\]\nWeak Learners The weak learners can be any classification or regression model. However, they are typically chosen to be very simple to account for training time. For example, a complex deep learning model would be a poor choice for a weak learner.\nOne example of a weak learner is a simple linear model like a Perceptron or decision stump. A standard implementation of AdaBoost uses a decision tree with depth 1, as observed in sklearn\u0026rsquo;s implementation.\nExample Let\u0026rsquo;s put this together and walk through the first few steps of training an AdaBoost model using a decision stump as the weak learner. We will use a very simple dataset to keep the values easy to compute by hand.\nInitial Data\nx1 x2 y weight 1 5 0 0.2 2 6 1 0.2 3 7 0 0.2 4 8 1 0.2 5 9 1 0.2 Weak Learner 1\nThe first learner is trained on the initial data and picks \\(x_1 = 2.5\\) as the split threshold. Input where \\(x_1 \\leq 2.5\\) is assigned to class 0 and all other samples are assigned class 1. The data with this learner\u0026rsquo;s predictions are shown below.\nx1 x2 y weight prediction 1 5 0 0.2 0 2 6 1 0.2 0 3 7 0 0.2 1 4 8 1 0.2 1 5 9 1 0.2 1 Error and weight\nThe error is simple enough to compute as all samples are currently weighted equally. Since two of the samples were misclassified, the error is the sum of their weights.\nTotal error\n\\(e_1 = 0.2 + 0.2 = 0.4\\).\nThe weight of the classifier can then be computed.\nClassifier weight\n\\(\\alpha_1 = \\frac{1}{2} \\ln \\big(\\frac{1 - e_1}{e_1}\\big) = 0.2027\\).\nThe weights of our data can now be updated using this value of \\(\\alpha_1\\). The weight of each example is updated by multiplying each correctly classifed sample by \\(\\exp\\{-\\alpha_1\\}\\) and each incorrectly classified sample by \\(\\exp\\{\\alpha\\}\\):\n\\[ w_i^{j+1} = w_i^{j} \\exp\\{\\alpha_j \\mathbb{1}(f_j(\\mathbf{x}_i \\neq \\mathbf{y}_i)\\}. \\]\nNOTE: You will notice that the equation above is different from the actual update rule that was applied to the weights in this example. In the original publication (TODO: reference Fruend), the weights are renormalized at the end of the loop. In this example, the normalization is combined with the update. In either case, the updated weights are shown below.\nx1 x2 y weight 1 5 0 0.167 2 6 1 0.250 3 7 0 0.250 4 8 1 0.167 5 9 1 0.167 Weak Learner 2\nThe algorithm now moves to the next weak learner, which classifies the data given a threshold of \\(x_1 = 3.5\\). Its predictions are shown below.\nx1 x2 y weight prediction 1 5 0 0.167 0 2 6 1 0.250 0 3 7 0 0.250 0 4 8 1 0.167 1 5 9 1 0.167 1 Only a single sample is misclassified, and the error is computed as before.\nTotal error\n\\(e_2 = 0.250\\)\nClassifier weight\n\\(\\alpha_2 = \\frac{1}{2} \\ln \\big(\\frac{1 - e_2}{e_2}\\big) = 0.5493\\)\nThe weights are updated for each sample, yielding the following data:\nx1 x2 y weight 1 5 0 0.111 2 6 1 0.500 3 7 0 0.167 4 8 1 0.111 5 9 1 0.111 The second sample has been misclassified twice at this point, leading to a relatively high weight. This will hopefully be addressed by the third learner.\nWeak Learner 3\nThe final weak learner splits the data on \\(x_2 = 6.5\\), yielding the following output for each sample.\nx1 x2 y weight prediction 1 5 0 0.111 0 2 6 1 0.500 0 3 7 0 0.167 1 4 8 1 0.111 1 5 9 1 0.111 1 Unfortunately, sample 2 is too tricky for any of our weak learners. The total error is shown below. Since this is a binary classification problem, the error suggests that our weak learner performs worse than random guessing.\nTotal error\n\\(e_3 = 0.667\\)\nClassifier weight\n\\(\\alpha_3 = \\frac{1}{2} \\ln \\big(\\frac{1 - e_3}{e_3}\\big) = -0.3473\\)\nThe negative value of the classifier weight suggests that its predictions will be reversed when evaluated. The updated weights of each data sample are given below.\nx1 x2 y weight 1 5 0 0.167 2 6 1 0.375 3 7 0 0.125 4 8 1 0.167 5 9 1 0.167 Final Classifier\nThe final classifier is a weighted vote of the three weak learners, with the weights being the classifier weights we calculated (0.2027, 0.5493, and -0.3473). The negative weight means that the third learner\u0026rsquo;s predictions are reversed.\n","date":1648011600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648011600,"objectID":"a74fb661122bb4b9319a36a6668cd31f","permalink":"https://ajdillhoff.github.io/notes/boosting/","publishdate":"2022-03-23T00:00:00-05:00","relpermalink":"/notes/boosting/","section":"notes","summary":"Table of Contents Introduction AdaBoost Introduction Combining predictions from multiple sources is usually preferred to a single source. For example, a medical diagnosis would carry much more weight if it was the result of a consensus of several experts. This idea of prediction by consensus is a powerful way to improve classification and regression models. In fact, good performance of a committee of models can be achieved even if each individual model is conceptually very simple.","tags":["machine learning"],"title":"Boosting","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Epipolar Geometry Calibration with Known Intrinsic Parameters and World Points Estimating Depth Introduction Binocular vision permits depth perception. It is an important part of many tasks such as robotic vision, pose estimation, and scene understanding. The goal of steropsis is to reconstruct a 3D representation of the world given correspondences between two or more cameras.\nThe process of computing these correspondences assumes two or more cameras with known intrinsic and extrinsic parameters. Methods exist to estimate the required transformation parameters using points based on matching image features. If some set of points which a fixed coordinate system is known, such as a calibration pattern, the problem becomes even simpler. Knowing the exact world point as it is projected in all image planes is essentially a ground truth.\nHartley and Zisserman address three primary questions when dealing with two views:\nCorrespondence geometry: How does a point in one view constraint the corresponding point in a second view? Camera geometry: How do we determine the cameras of both views given a set of corresponding image points? Scene geometry: If we know the cameras and have a set of corresponding points, how can we compute the depth? Epipolar Geometry Epipolar geometry is the backbone of stereopsis. We will first define what epipolar geometry is, how it is used in the stereo vision problem, and the core constraint that limits our search space of point correspondences. It is defined visually below.\nFigure 1: Overview of epipolar geometry for stereopsis (Source: Szeliski). Consider the point \\(\\mathbf{p}\\) whose projection is \\(\\mathbf{x}_0\\) with respect to camera \\(\\mathbf{c}_0\\) and \\(\\mathbf{x}_1\\) with respect to camera \\(\\mathbf{c}_1\\). All 5 of these points lie on the epipolar plane. Additionally, points \\(\\mathbf{e}_0\\) and \\(\\mathbf{e}_1\\) lie on the line defined by \\(\\mathbf{c}_0\\) and \\(\\mathbf{c}_1\\) as it intersects the image plane of each camera, respectively. These are called the epipoles. These are also the projections of the other camera centers.\nFundamental to establishing the correspondences between the two cameras is the epipolar constraint. If \\(\\mathbf{x}_0\\) and \\(\\mathbf{x}_1\\) represent projections of the same point, then \\(\\mathbf{x}_1\\) must like on the epipolar line \\(l_1\\) associated with \\(\\mathbf{x}_0\\) and vice versa.\nFigure 2: The epipolar constraint restricts the search space for matching correspondences. (Source: Szeliski) As seen in the figure above, potentional matches for \\(\\mathbf{x}_0\\) must lie on the epipolar line defined by \\(\\mathbf{e}_1\\) and \\(\\mathbf{x}_1\\).\nOur goal is to compute translation \\(\\mathbf{t}\\) and rotation \\(R\\). To find these, we start by mathematically defining the epipolar constraint. The epipolar plane is defined by the lines \\(\\overrightarrow{\\mathbf{c}_0 \\mathbf{p}}\\), \\(\\overrightarrow{\\mathbf{c}_1 \\mathbf{p}}\\), and \\(\\overrightarrow{\\mathbf{c}_0 \\mathbf{c}_1}\\). This can be written as\n\\[ \\overrightarrow{\\mathbf{c}_0 \\mathbf{p}} \\cdot [\\overrightarrow{\\mathbf{c}_0 \\mathbf{c}_1} \\times \\overrightarrow{\\mathbf{c}_1 \\mathbf{p}}] = 0. \\]\nWe do not know \\(\\mathbf{p}\\), but we do have \\(\\mathbf{x}_0\\) and \\(\\mathbf{x}_1\\), the projections of \\(\\mathbf{p}\\) onto the image plane of each respective camera. Assuming that both camera are calibrated, we have a set of known intrinsic matrices \\(\\mathbf{K}_j\\).\nAlthough we do not have the exact \\(z\\) value of the point, we do know the point with respect to the camera calculcated as\n\\[ \\mathbf{p}_0 = d_0 \\hat{\\mathbf{x}}_0, \\]\nwhere\n\\[ \\hat{\\mathbf{x}}_0 = \\mathbf{K}_0^{-1} \\mathbf{x}_0. \\]\nThe relationship between points \\(\\mathbf{p}_0\\) and \\(\\mathbf{p}_1\\) is\n\\[ d_1 \\hat{\\mathbf{x}}_1 = R(d_0 \\hat{\\mathbf{x}}_0) + \\mathbf{t}, \\]\nwhere \\(R\\) is a rotation matrix and \\(\\mathbf{t}\\) is an offset vector. These are the parameters that are solved from stereo calibration.\nSince the vectors \\(\\hat{\\mathbf{x}}_0\\), \\(\\hat{\\mathbf{x}}_1\\), and \\(\\mathbf{t}\\) are coplanar, the plane can be represented by a normal vector. That is, the vector that is orthogonal to all points in the plane. Such a vector can be calculated by taking the cross product of both sides of the above equation with \\(\\mathbf{t}\\):\n\\[ d_1 [\\mathbf{t}]_{\\times} \\hat{\\mathbf{x}}_1 = d_0 [\\mathbf{t}]_{\\times} R \\hat{\\mathbf{x}}_0, \\]\nwhere\n\\[ [\\mathbf{t}]_{\\times}=\\begin{bmatrix} 0 \u0026amp; -t_z \u0026amp; t_y\\\\ t_z \u0026amp; 0 \u0026amp; -t_x\\\\ -t_y \u0026amp; t_x \u0026amp; 0 \\end{bmatrix}. \\]\nIt is true that, since the normal vector is orthogonal to \\(\\hat{\\mathbf{x}}_0\\), \\(\\hat{\\mathbf{x}}_1\\), and \\(\\mathbf{t}\\), taking the dot product of any of these vectors and the normal vector yields 0:\n\\[ d_1 \\hat{\\mathbf{x}}_1^T [\\mathbf{t}]_{\\times} \\hat{\\mathbf{x}}_1 = d_0 \\hat{\\mathbf{x}}_1^T [\\mathbf{t}]_{\\times} R \\hat{\\mathbf{x}}_0 = 0. \\]\nWe have now established the epipolar constraint in terms of the rotation matrix \\(R\\) and translation \\(\\mathbf{t}\\). These are the parameters that relate the points between both cameras. This constraint is more compactly written as\n\\[ \\hat{\\mathbf{x}}_1^T E \\hat{\\mathbf{x}}_0 = 0, \\]\nwhere\n\\[ E = [\\mathbf{t}_{\\times}]R. \\]\n\\(E\\) is called the essential matrix which relates the projected points between the two cameras. Once we have the essential matrix, we can compute \\(\\mathbf{t}\\) and \\(R\\).\nCalibration with Known Intrinsic Parameters and World Points We first look at the simplest case of stereo calibration. The intrinsic parameters of both cameras have been computed using a standard calibration technique. Additionally, we have a fixed calibration pattern used to establish a correspondence between the world points and the points with respect to each camera.\nGiven \\(N\\) measured correspondences \\(\\{(\\mathbf{x}_{i0}, \\mathbf{x}_{i1})\\}\\), we can form a linear system with equations of the form\n\\begin{alignat*}{3} x_{i0}x_{i1}e_{00} \u0026amp; {}+{} \u0026amp; y_{i0}x_{i1}e_{01} \u0026amp; {}+{} \u0026amp; x_{i1}e_{02} \u0026amp; {}+{} \u0026amp;\\\\ x_{i0}y_{i1}e_{10} \u0026amp; {}+{} \u0026amp; y_{i0}y_{i1}e_{11} \u0026amp; {}+{} \u0026amp; y_{i1}e_{12} \u0026amp; {}+{} \u0026amp;\\\\ x_{i0}e_{20} \u0026amp; {}+{} \u0026amp; y_{i0}e_{21} \u0026amp; {}+{} \u0026amp; e_{22} \u0026amp; {}={} \u0026amp; 0 \\end{alignat*}\nGiven at least 8 equations corresponding to the 8 unknowns of \\(E\\), we can use SVD to solve for \\(E\\).\n\\[ E = [\\mathbf{t}]_{\\times}R = \\mathbf{U \\Sigma V^T} = \\begin{bmatrix} \\mathbf{u}_0 \u0026amp; \\mathbf{u}_1 \u0026amp; \\mathbf{t} \\end{bmatrix}\\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 1 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 1\\\\ \\end{bmatrix}\\begin{bmatrix} \\mathbf{v}_0^T\\\\ \\mathbf{v}_1^T\\\\ \\mathbf{v}_2^T\\\\ \\end{bmatrix} \\]\nEstimating Depth Given the intrinsic parameters and parameters relating both calibrated cameras, we can estimate the depth of a point that is seen by both cameras.\nWe know that \\(\\mathbf{x}_0 = K_0 \\hat{\\mathbf{x}}_0\\) and \\(\\mathbf{x}_1 = K_1 \\hat{\\mathbf{x}}_1\\). With the stereo calibration complete, we also know \\(A = [R\\quad \\mathbf{t}]\\) and\n\\[ \\hat{\\mathbf{x}}_0 = A\\hat{\\mathbf{x}}_1. \\]\nPlugging into the projection equation for the first camera yields\n\\[ \\mathbf{x}_0 = K_0 A\\hat{\\mathbf{x}}_1. \\]\nOur knowns are \\(\\mathbf{x}_0, \\mathbf{x}_1, K_0, K_1, \\text{ and } A\\). The only unknown is \\(\\hat{\\mathbf{x}}_1\\).\nWe are left with 2 equations\n\\begin{align*} \\mathbf{x}_0 \u0026amp;= K_0 A\\hat{\\mathbf{x}}_1\\\\ \\mathbf{x}_1 \u0026amp;= K_1 \\hat{\\mathbf{x}}_1. \\end{align*}\nIf we let \\(P = K_0 A\\), we write the \\(\\mathbf{x}_0 = \\begin{bmatrix}u_1\\\\v_1\\\\1\\end{bmatrix}\\) and\n\\begin{equation*} \\begin{bmatrix} u_1\\\\ v_1\\\\ 1 \\end{bmatrix}=\\begin{bmatrix} p_{11} \u0026amp; p_{12} \u0026amp; p_{13} \u0026amp; p_{14}\\\\ p_{21} \u0026amp; p_{22} \u0026amp; p_{23} \u0026amp; p_{24}\\\\ p_{31} \u0026amp; p_{32} \u0026amp; p_{33} \u0026amp; p_{34}\\\\ \\end{bmatrix}\\begin{bmatrix} X\\\\ Y\\\\ Z\\\\ W \\end{bmatrix}. \\end{equation*}\nThis gives two equations for \\(x_1\\) and \\(y_1\\), the measured 2D feature locations:\n\\begin{align*} x_1 \u0026amp;= \\frac{p_{11}X + p_{12}Y + p_{13}Z + p_{14}W}{p_{31}X + p_{32}Y + p_{33}Z + p_{34}W}\\\\ y_1 \u0026amp;= \\frac{p_{21}X + p_{22}Y + p_{23}Z + p_{24}W}{p_{31}X + p_{32}Y + p_{33}Z + p_{34}W}. \\end{align*}\nMultiplying both equations by the denominator yields a set of equations that can be solved via linear least squares or SVD.\n","date":1648011600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720569600,"objectID":"be1f78382e98a26c34a8c6047067316a","permalink":"https://ajdillhoff.github.io/notes/stereo_vision/","publishdate":"2022-03-23T00:00:00-05:00","relpermalink":"/notes/stereo_vision/","section":"notes","summary":"Table of Contents Introduction Epipolar Geometry Calibration with Known Intrinsic Parameters and World Points Estimating Depth Introduction Binocular vision permits depth perception. It is an important part of many tasks such as robotic vision, pose estimation, and scene understanding. The goal of steropsis is to reconstruct a 3D representation of the world given correspondences between two or more cameras.\nThe process of computing these correspondences assumes two or more cameras with known intrinsic and extrinsic parameters.","tags":["algorithms","computer science"],"title":"Stereo Vision","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Resources Introduction Example: Iris Dataset Growing a Tree Examining the Iris Classification Tree Pruning a Tree The Algorithm Resources https://www.kaggle.com/dmilla/introduction-to-decision-trees-titanic-dataset Introduction A decision tree, or Classification and Regression Trees (CART), is a model that recursively partitions the input space based on a collection of features. The partitions are split based on very simple binary choices. If yes, branch to the left; if no, branch to the right.\nFigure 1: Regression tree (left) and its piecewise constant surface (right) (Source: Machine Learning: A Probabilistic Perspective by Kevin P. Murphy). To compute the response, we represent each individual decision as a function \\(\\phi\\) and sum the responses:\n\\[ f(\\mathbf{x}) = \\mathbb{E}[y | \\mathbf{x}] = \\sum_{m=1}^M w_m \\mathbb{1} (\\mathbf{x} \\in R_m) = \\sum_{m=1}^M w_m \\phi(\\mathbf{x};\\mathbf{v}_m), \\]\nwhere \\(R_m\\) is the \\(m^{\\text{th}}\\) region, \\(w_m\\) is the mean response, and \\(\\mathbf{v}_m\\) is the choice of variable to split on along with its threshold value. Note that this is not a differentiable function due to the indicator function.\nExample: Iris Dataset To see this on real data, consider the Iris flower dataset. For example, we will look at a decision tree model that classifies each flower into either setosa, versicolor, or virginica.\nFigure 2: Our initial Iris classifier. We are given data about a new iris and want to classify it using this tree. Our sample has a sepal length of 6.1cm, a sepal width of 2.8cm, a petal length of 4.7cm, and a petal width of 1.2cm. The first decision considers the petal width. Our sample has a width of 1.2, so it continues down the right branch.\nThe second decision consideres petal width again. Since our sample does not have a width greater than 1.75, we continue down the left branch.\nAt this point, the model was optimized to now consider the petal length. Our length comes in at 4.7, just shy of going down the right path.\nWe now arrive at the last decision. Since our petal length is not greater than 1.65, the model classifies this sample as versicolor.\nGrowing a Tree To grow a tree, a decision needs to be made as to whether or not the current set of data can be split based on some feature. As such, there should be a reliable way of determining if a feature provided a good split. This is evaluated using a cost function and selecting the feature and value corresponding to the minimum cost:\n\\[ (j^*, t^*) = \\text{arg} \\min_{j\\in\\{1, \\dots, D\\}} \\min_{t \\in \\mathcal{T}_j} \\text{cost}(\\{\\mathbf{x}_i, y_i : x_{ij} \\leq t\\}) + \\text{cost}(\\{\\mathbf{x}_i, y_i : x_{ij} \u0026gt; t\\}). \\]\nIn words, this function finds a value \\(t\\) such that groups the data with the lowest cost. For a regression task, the cost function is typically defined as\n\\[ \\text{cost}(\\mathcal{D}) = \\sum_{i \\in \\mathcal{D}}(y_i - \\bar{y})^2, \\]\nwhere\n\\[ \\bar{y} = \\frac{1}{|\\mathcal{D}|}\\sum_{i \\in \\mathcal{D}} y_i. \\]\nSplits that result in clusters with high variance may still see a higher cost, even though they are the minimum.\nAs their alternative name implies, decision trees can also be used for classification. The splits are still based on features and threshold values at each branch. When a split is considered, a class-conditional probability is estimated for that data.\nSplitting the Data Given a set of data that makes it to node \\(i\\), denoted \\(\\mathcal{D}_i\\), a fitting procedure must select the feature and threshold value that minimizes the cost. If the feature is continuous, the range of search values is selected by sorting a list of unique values from the subset of data. For each unique value, the cost is computed by splitting the data into two groups based on the threshold value. The threshold value that minimizes the cost is selected.\nIn the case of categorical data, we would intuitively split the data into a set of data that contains the category and a set that does not. The cost is then computed for each category. The category that minimizes the cost is selected.\nGiven data satisfying \\(X_j \u0026lt; t\\), the class-conditional probability is\n\\[ \\hat{\\pi}_c = \\frac{1}{|\\mathcal{D}|}\\sum_{i \\in \\mathcal{D}} \\mathbb{1}(y_i = c). \\]\nError Functions The common error functions used for classification are misclassification rate, entropy, and Gini index. Misclassification rate is computed by summing the number of misclassifications:\n\\[ \\frac{1}{|\\mathcal{D}|} \\sum_{i \\in \\mathcal{D}} \\mathbb{1}(y_i \\neq \\hat{y}) = 1 - \\hat{\\pi}_{\\hat{y}}. \\]\nEntropy is computed as\n\\[ \\mathbb{H}(\\mathbb{\\hat{\\pi}}) = -\\sum_{c=1}^C \\hat{\\pi}_c \\log \\hat{\\pi}_c. \\]\nGini index computes the expected error rate.\n\\[ G = \\sum_{c=1}^C \\hat{\\pi}_c (1 - \\hat{\\pi}_c) = 1 - \\sum_{c=1}^C \\hat{\\pi}_c^2 \\]\nFigure 3: Impurity measured for binary classification (Source: Machine Learning: A Probabilistic Perspective by Kevin P. Murphy) Like entropy, it promotes an equal number of observations across all classes in a node. For small values of \\(\\hat{\\pi}\\), the error is smaller than that of entropy. If the dataset is imbalanced, entropy is typically favored as it penalizes imbalanced datasets more than Gini will. Both will favor splits that result in one node being pure.\nStopping Growth If left unchecked, the algorithm to grow a tree will continue until the data can no longer be split. In the trivial case, this will be when every data point represents a leaf node. In order to prevent overfitting, there are several criteria that are considered.\nDoes the split reduce the cost enough? It may be ideal to only split the data if the cost is reduced by some acceptable value. The reduction can be computed by\n\\[ \\Delta = \\text{cost}(\\mathcal{D}) - \\bigg(\\frac{|\\mathcal{D}_L|}{|\\mathcal{D}|}\\text{cost}(\\mathcal{D}_L) + \\frac{|\\mathcal{D}_R|}{|\\mathcal{D}|} \\text{cost}(\\mathcal{D}_R)\\bigg). \\]\nHas the tree reached some maximum depth? The depth of a tree is set as a hyperparameter. Later, when we look at an example, we will use cross validation to select the best depth parameter for our model.\nIs the distribution of the split pure? If either of the splits is fully made up of data with the same label, there is no need to split it any further.\nIs the split too small? A split that is too small may lead to overfitting.\nExamining the Iris Classification Tree How exactly does the earlier example model make its decision at each node? The full tree is shown below.\nFigure 4: A detailed view of our Iris classifier. The first split is visually very simple to intuit. Using petal width can perfectly split all setosa samples into a single leaf node.\nPruning a Tree Depending on the data, stopping growth based on measuring the relative decrease in error may not result in a model that performs well. Image a dataset that requires multiple features to provide a sufficient classification. If only one of the features is considered in isolation, it may provide no decrease in error. A practical example of this is the XOR problem. Splitting on \\(x_1\\) or \\(x_2\\) in isolation does not provide any indication about the true output. It is only when \\(x_1 \\neq x_2\\) does the output equal to 1.\nTo rectify this, a tree can be grown until it is completely full before pruning the branches that result in the smallest increase in error.\nThe Algorithm The general algorithm is shown in MATLAB below.\n% node = fitTree(node, D, depth) % Recursive function to learn a decision tree % Returns the index of the current node. % % node - The node index in obj.Nodes. % D - Indices to the current data. % depth - Current depth of the tree. function node = fitTree(obj, node, D, depth) % Determine best split for the data and return the split [j, t, dSplit, classDist] = obj.split(D, obj.Nodes(node).features); obj.Nodes(node).prediction = classDist; disp(classDist); % Use heuristic to determine if node is worth splitting if obj.splitNode(depth, classDist) == true % set the node test, the function that determines the branch obj.Nodes(node .test = {j, t}; newFeatures = obj.Nodes(node).features(obj.Nodes(node).features ~= j); % set the child nodes to the left and right splits obj.Nodes(node).children = zeros(size(dSplit, 1), 1); numNewNodes = size(dSplit, 1); for i = 1 : numNewNodes obj.Nodes(end + 1) = struct(\u0026#39;prediction\u0026#39;, 0, \u0026#39;test\u0026#39;, 0, ... \u0026#39;features\u0026#39;, newFeatures, \u0026#39;children\u0026#39;, 0, \u0026#39;parent\u0026#39;, node); obj.Nodes(node).children(i) = obj.fitTree(length(obj.Nodes), dSplit{i}, depth + 1); end end end ","date":1647579600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708560000,"objectID":"cb086cf2a1081a79001139440f6b6605","permalink":"https://ajdillhoff.github.io/notes/decision_trees/","publishdate":"2022-03-18T00:00:00-05:00","relpermalink":"/notes/decision_trees/","section":"notes","summary":"Table of Contents Resources Introduction Example: Iris Dataset Growing a Tree Examining the Iris Classification Tree Pruning a Tree The Algorithm Resources https://www.kaggle.com/dmilla/introduction-to-decision-trees-titanic-dataset Introduction A decision tree, or Classification and Regression Trees (CART), is a model that recursively partitions the input space based on a collection of features. The partitions are split based on very simple binary choices. If yes, branch to the left; if no, branch to the right.","tags":["machine learning"],"title":"Decision Trees","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Reading Outline Pinhole Model From World Space to Image Space Camera Parameters Estimating Camera Parameters Application: Camera Calibration Reading Chapters 1 and 7 (Forsyth and Ponce) https://www.scratchapixel.com/ https://docs.google.com/presentation/d/1RMyNQR9jGdJm64FCiuvoyJiL6r3H18jeNIyocIO9sfA/edit#slide=id.p http://vision.stanford.edu/teaching/cs131_fall1617/lectures/lecture8_camera_models_cs131_2016.pdf http://vlm1.uta.edu/~athitsos/courses/cse4310_spring2021/lectures/11_geometry.pdf Outline Pinhole model Coordinates of a pinhole model Perspective projections Homogeneous coordinates Computer graphics perspective Lenses Intrinsic and extrensic parameters From world to camera to image space Camera calibration Pinhole Model Imagine piercing a small hole into a plate and placing it in front of a black screen. The light that enters through the pinhole will show an inverted image against the back plane. If we place a virtual screen in front of the pinhole plate, we can project the image onto it. This is the basic idea behind a pinhole camera model.\nFigure 1: Pinhole camera model (Forsyth and Ponce). Virtual Camera Coordinates There are two sets of coordinates for each object in the pinhole camera model. First are the actual coordinates of the visible points as seen from the camera. For real images, these points are continuous. For virtual images, these are discrete. To produce an actual image, a set of 3D coordinates are projected onto the image plane. These 3D coordinates are with respect to the camera\u0026rsquo;s coordinate system.\nWe can represent the transformation from camera coordinates to image coordinates with a projection matrix. There are many properties of the camera that must be considered, such as the lens, resolution, etc. We will start with a simple projection matrix.\nFigure 2: Derivation of simple perspective projection. In the figure above, the camera is located at point \\(A\\) with its viewing plane defined by \\(BC\\). The \\(y\\) value of the world space coordinate is given by \\(DE\\). Notice that there are two right triangles (\\(\\Delta ABC\\) and \\(\\Delta ADE\\)). As such, the ratio between their sides is constant:\n\\[ \\frac{BC}{DE} = \\frac{AB}{AD}. \\]\nSolving for \\(y\u0026rsquo; = BC\\) yields \\(BC = \\frac{AB * DE}{AD}\\). A similar relationship follows for the \\(x\\) value. In a more common notation, this is written as\n\\begin{align*} x\u0026rsquo; \u0026amp;= \\frac{x}{z}\\\\ y\u0026rsquo; \u0026amp;= \\frac{y}{z} \\end{align*}\nThis can be represented as a transformation matrix and point vector by using homogeneous coordinates. These coordinate are to projective geometry as Cartesian coordinates are to Euclidean geometry. For a 3D point \\(\\mathbf{p} = (X, Y, Z)\\), its corresponding homogeneous coordinate is \\(\\mathbf{p}_w = (x, y, z, w)\\). The relationship between the original coordinate and its homogeneous coordinate is given by\n\\[ X = \\frac{x}{w},\\quad Y = \\frac{y}{w}, \\quad Z = \\frac{z}{w}. \\]\nWith homogeneous coordinates, we can easily project 3D points to an image plane. First, let the projection matrix be defined as\n\\[ M = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0\\\\ \\end{bmatrix} \\]\nand a general homogeneous coordinate is given by\n\\[ \\mathbf{p} = \\begin{bmatrix} x\\\\ y\\\\ z\\\\ w \\end{bmatrix}. \\]\nThen,\n\\[ M\\mathbf{p} = \\begin{bmatrix} x\\\\ y\\\\ z\\\\ z \\end{bmatrix} \\]\nand the perspective divide is applied to finish the transformation. That is, divide each component by \\(w\\) and dropping the last component.\n\\[ \\mathbf{p}\u0026rsquo; = \\begin{bmatrix} \\frac{x}{z}\\\\ \\frac{y}{z}\\\\ 1 \\end{bmatrix} \\]\nPhysical Camera Coordinates If we are simulating a physical camera, then the sensor should be behind the lens. In that case, the points will be inverted as compared to the virtual model.\nFigure 3: Visualization of the projection equation (Forsyth and Ponce). Remember that this is not taking optics into account. This is a simple projective model. The figure below shows the relationship between the virtual image plane and sensor image plane.\nFigure 4: Sensor plane versus virtual plane. To calculate the points projected to the sensor plane using our simple matrix from above, we only need to negate the \\(z\\) and \\(w\\) values:\n\\[ M = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0\\\\ \\end{bmatrix} \\]\nThen the transformed point becomes\n\\[ M\\mathbf{p} = \\begin{bmatrix} x\\\\ y\\\\ -z\\\\ -z \\end{bmatrix}. \\]\nApplying the perspective divide yields the 3D point \\((-\\frac{x}{z}, -\\frac{y}{z}, 1)\\).\nConsidering the Focal Length An important property that all cameras have is the focal length \\(f\\). That is the distance between the lens and the camera\u0026rsquo;s sensor. In computer graphics terms, the focal length may also refer to the distance between the virtual image plane and camera. This is then called the near plane instead.\nFor the following examples, we will follow OpenGL\u0026rsquo;s projective matrix in which the \\(x\\) and \\(y\\) values are scaled by a relationship between the focal length and image window size:\n\\[ \\frac{2n}{r - l}, \\]\nwhere \\(n\\) is the near plane distance (related to \\(f\\)), \\(r\\) is the value of the right side of the window, and \\(l\\) is the value of the left side of the window. For a square window of size \\([-1, 1]\\), this reduces to \\(n\\) (or \\(f\\)). Then the projection matrix is given as\n\\[ M = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; -\\frac{1}{f} \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0\\\\ \\end{bmatrix}. \\]\nWhy does \\(M(3, 3) = -\\frac{1}{f}\\)?\nThe projected point should be \\(f\\) units away from the lens. Projecting a point using the matrix above yields\n\\[ M\\mathbf{p} = \\begin{bmatrix} x\\\\ y\\\\ -\\frac{z}{f}\\\\ -z \\end{bmatrix}. \\]\nThen applying the perspective divide to compute the projected coordinate results in\n\\[ \\mathbf{p\u0026rsquo;} = \\begin{bmatrix} -\\frac{fx}{z}\\\\ -\\frac{fy}{z}\\\\ \\end{bmatrix}. \\]\nWe first computed the 3D homogeneous coordinate and then applied the relationship to 2D homogeneous coordinates using the resulting \\((x, y, z)\\) triplet. Applying the final divide which establishes the relationship between 2D Cartesian and homogeneous coordinates results in \\(\\mathbf{p\u0026rsquo;}\\).\nBe aware that computer graphics implementations may apply this process differently using a matrix that looks something like this\n\\[ M = \\begin{bmatrix} f \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; f \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0\\\\ \\end{bmatrix}. \\]\nIf we apply this to a point in 3D space using homogeneous coordinates, it results in the point\n\\[ M\\mathbf{p} = \\begin{bmatrix} fx\\\\ fy\\\\ -z\\\\ -z \\end{bmatrix}. \\]\nThen the resulting 3D point is\n\\[ \\mathbf{p\u0026rsquo;} = \\begin{bmatrix} -\\frac{fx}{z}\\\\ -\\frac{fy}{z}\\\\ 1\\\\ \\end{bmatrix}, \\]\nwhich gives us the same \\(x\\) and \\(y\\) values of the projected point as before.\nFigure 5: 3D points (green) projected onto the virtual plane (red) and sensor plane (blue). From World Space to Image Space The process of projecting points in camera space onto an image plane is fairly simple, but gets slightly more complicated when considering each entity from the world perspective. Objects in the world appear differently depending on the viewpoint of the camera. To model this mathematically, we need to be able to describe the points in 3 different spaces:\nWorld space Camera space Image space To fully describe the possibilities that our camera\u0026rsquo;s perspective can take, we need to define scaling, translation, and rotations. These allow us to describe the different positions and orientations are camera may be in with respect to the world around it.\nTranslation Consider the points of a pyramid observed by a virtual camera, as seen below.\nFigure 6: Points of a pyramid as seen from our virtual camera. If we move the camera away from the points, the will appear smaller (assuming a perspective projection). This movement is described by a translation with respect to the origin in world space. In the above figure, the camera is position at the origin. The figure below shows the camera moved down the $z$-axis by 2 units.\nFigure 7: Translating the camera backwards by 2 units. The project points appear smaller. This is described with a matrix as\n\\[ T = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; -2\\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1\\\\ \\end{bmatrix}. \\]\nIn general, a translation in 3D can be described with the following matrix\n\\[ T = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; T_x\\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; T_y\\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; T_z\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1\\\\ \\end{bmatrix}. \\]\nScaling Projected objects can appear larger or smaller depending on the focal length and field of view. Recall the perspective projection matrix from the previous section:\n\\[ P = \\begin{bmatrix} f \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; f \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0\\\\ \\end{bmatrix}. \\]\nThe projections we have seen so far are with a focal length of 1. We can make the objects in the image appear larger by increasing the focal length, as seen below.\nFigure 8: Focal length doubled. The projected image appears larger. Rotation Finally, we can rotate our camera about the \\(x\\), \\(y\\), or \\(z\\) axis. Following a right-handed coordinate system, these rotations are given by\n\\begin{align*} R_x(\\theta) \u0026amp;= \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; \\cos \\theta \u0026amp; -\\sin \\theta\\\\ 0 \u0026amp; \\sin \\theta \u0026amp; \\cos \\theta \\end{bmatrix}\\\\ R_y(\\theta) \u0026amp;= \\begin{bmatrix} \\cos \\theta \u0026amp; 0 \u0026amp; \\sin \\theta\\\\ 0 \u0026amp; 1 \u0026amp; 0\\\\ -\\sin \\theta \u0026amp; 0 \u0026amp; \\cos \\theta \\end{bmatrix}\\\\ R_z(\\theta) \u0026amp;= \\begin{bmatrix} \\cos \\theta \u0026amp; -\\sin \\theta \u0026amp; 0\\\\ \\sin \\theta \u0026amp; \\cos \\theta \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix}\\\\ \\end{align*}\nFigure 9: Rotating the camera about the y-axis. The points now appear on the right side of the image plane. Camera Parameters When relating points in the world to those of the image space, two sets of parameters are typically used: the intrinsic and extrinsic parameters of a camera. Intrinsic parameters relate the points in the camera\u0026rsquo;s reference frame to those of the image plane. Extrinsic parameters relate the camera\u0026rsquo;s coordinate system to a world coordinate system as well as describing its position and orientation.\nIntrinsic Parameters Which parameters are required to project a point to the image plane? The intrinsic parameters are those which convert the points on the physical sensor into a normalized image frame. We start with a basic projection matrix\n\\[ M = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0\\\\ \\end{bmatrix}. \\]\nA physical sensor may not have square pixels, so there are two additional scale parameters \\(k\\) and \\(l\\). Along with the focal length \\(f\\), the projected values become\n\\begin{align*} x \u0026amp;= kf\\frac{X}{Z} = kf\\hat{x}\\\\ y \u0026amp;= lf\\frac{Y}{Z} = lf\\hat{y}\\\\ \\end{align*}\nThe focal length and scale factors \\(k\\) and \\(l\\) are dependent and can be written as \\(\\alpha = kf\\) and \\(\\beta = lf\\).\nThe physical sensor has an origin at the corner whereas the normalized image frame has an origin at the center. To account for this offset, a translation parameter for both \\(x\\) and \\(y\\) is necessary. The pixel conversion is now represented as\n\\begin{align*} x \u0026amp;= \\alpha \\hat{x} + x_0\\\\ y \u0026amp;= \\beta \\hat{y} + y_0\\\\ \\end{align*}\nFinally, the sensor may not be perfectly center. There may also be additional defects that cause the sensor to be warped or skewed. This is error that can be accounted for by a rotation.\n\\begin{align*} x \u0026amp;= \\alpha \\hat{x} - \\alpha \\cot \\theta \\hat{y} + x_0\\\\ y \u0026amp;= \\frac{\\beta}{\\sin \\theta} \\hat{y} + y_0\\\\ \\end{align*}\nTo recap, the 5 intrinsic parameters are\n\\(\\alpha\\) - scale parameter for \\(x\\) \\(\\frac{\\beta}{\\sin \\theta}\\) - scale parameter for \\(y\\), also accounts for skew \\(-\\alpha \\cot \\theta\\) - skew coefficient between the \\(x\\) and \\(y\\) axis \\(x_0\\) - the principal point for \\(x\\) \\(y_0\\) - the principal point for \\(y\\) The third parameter \\(-\\alpha \\cot \\theta\\) is sometimes represented as \\(\\gamma\\) and will be set to 0 in most cases. A typical intrinsic matrix is then\n\\[ K = \\begin{bmatrix} \\alpha \u0026amp; \\gamma \u0026amp; x_0\\\\ 0 \u0026amp; \\beta \u0026amp; y_0\\\\ 0 \u0026amp; 0 \u0026amp; 1\\\\ \\end{bmatrix}. \\]\nExtrinsic Parameters Given a point with respect to the world frame, \\([\\mathbf{p}]_W\\), we are interested in a change of basis matrix which transforms the world space coordinate to the camera\u0026rsquo;s basis. This is represented by a change of basis matrix which converts a point in world space to the camera\u0026rsquo;s coordinate frame.\nThe matrix is defined as a rigid transformation involving a rotation and translation. The rotation components \\(R\\) are composed following the 3 rotation matrices described earlier. This only requires 3 parameters if using something like Euler angles. Euler angles represent the rotational angle around each axis by individual parameters \\(\\phi\\), \\(\\theta\\), and \\(\\psi\\).\n\\[ R = \\begin{bmatrix} r_{11} \u0026amp; r_{12} \u0026amp; r_{13}\\\\ r_{21} \u0026amp; r_{22} \u0026amp; r_{23}\\\\ r_{31} \u0026amp; r_{32} \u0026amp; r_{33}\\\\ \\end{bmatrix} \\]\nThe translational component is represented using an additional 3 parameters \\(t_x\\), \\(t_y\\), and \\(t_z\\). This can be represented as a matrix\n\\[ T = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; t_x\\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; t_y\\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; t_z\\\\ \\end{bmatrix}. \\]\nFor convenience, the extrinsic parameters can be represented as a single matrix\n\\[ M = \\begin{bmatrix} r_{11} \u0026amp; r_{12} \u0026amp; r_{13} \u0026amp; t_x\\\\ r_{21} \u0026amp; r_{22} \u0026amp; r_{23} \u0026amp; t_y\\\\ r_{31} \u0026amp; r_{32} \u0026amp; r_{33} \u0026amp; t_z\\\\ \\end{bmatrix} \\]\nThe full transformation from world space to image space is then represented as\n\\[ \\mathbf{p} = KM[\\mathbf{p}]_W, \\]\nwhere \\([\\mathbf{p}]_W = \\begin{bmatrix}x\\\\y\\\\z\\\\1\\end{bmatrix}\\).\nEstimating Camera Parameters There are many applications which require that we know the exact camera parameters, including augmented reality, inpainting techniques, and depth estimation. We now investigate how, given a fixed world coordinate system with known structure, we can approximate the camera parameters.\nConsider a set of \\(n\\) image-to-world point correspondences \\(\\mathbf{X}_i \\leftrightarrow \\mathbf{x}_i\\). The goal is to compute a homography \\(\\mathbf{H}\\) that relates the image points to the world points: \\(\\mathbf{x}_i = \\mathbf{H}\\mathbf{X}_i\\). This relationship can be written as \\(\\mathbf{x}_i \\times \\mathbf{H}\\mathbf{X}_i = 0\\) because the cross product of two vectors is zero if they are parallel. \\(H\\mathbf{x}_i\\) can be written as\n\\begin{pmatrix} \\mathbf{h}_1^T\\mathbf{x}_i\\\\ \\mathbf{h}_2^T\\mathbf{x}_i\\\\ \\mathbf{h}_3^T\\mathbf{x}_i\\\\ \\end{pmatrix}\nThen the cross product is given by\n\\begin{pmatrix} y_i\\mathbf{h}_3^T\\mathbf{X}_i - w_i\\mathbf{h}_2^T\\mathbf{X}_i\\\\ w_i\\mathbf{h}_1^T\\mathbf{X}_i - x_i\\mathbf{h}_3^T\\mathbf{X}_i\\\\ x_i\\mathbf{h}_2^T\\mathbf{X}_i - y_i\\mathbf{h}_1^T\\mathbf{X}_i\\\\ \\end{pmatrix}\nObserving that \\(\\mathbf{h}_j^T\\mathbf{X}_i = \\mathbf{X}_i^T\\mathbf{h}_j\\), we can write this as a homogeneous linear system.\n\\[ \\begin{bmatrix} \\mathbf{0}^T \u0026amp; -w_i\\mathbf{X}_i^T \u0026amp; y_i\\mathbf{X}_i^T\\\\ w_i\\mathbf{X}_i^T \u0026amp; \\mathbf{0}^T \u0026amp; -x_i\\mathbf{X}_i^T\\\\ -y_i\\mathbf{X}_i^T \u0026amp; x_i\\mathbf{X}_i^T \u0026amp; \\mathbf{0}^T\\\\ \\end{bmatrix} \\begin{pmatrix} \\mathbf{h}_1\\\\ \\mathbf{h}_2\\\\ \\mathbf{h}_3\\\\ \\end{pmatrix} = \\mathbf{0} \\]\nThis is a linear system whose third equation is linearly dependent on the first two, thus we only need to solve the first two equations.\n\\[ \\begin{bmatrix} \\mathbf{0}^T \u0026amp; -w_i\\mathbf{X}_i^T \u0026amp; y_i\\mathbf{X}_i^T\\\\ w_i\\mathbf{X}_i^T \u0026amp; \\mathbf{0}^T \u0026amp; -x_i\\mathbf{X}_i^T\\\\ \\end{bmatrix} \\begin{pmatrix} \\mathbf{h}_1\\\\ \\mathbf{h}_2\\\\ \\mathbf{h}_3\\\\ \\end{pmatrix} = \\mathbf{0} \\]\nIn practice, this system is denoted as \\(A_i \\mathbf{h} = \\mathbf{0}\\) where \\(A_i\\) is a \\(2 \\times 9\\) matrix and \\(\\mathbf{h}\\) is a \\(9 \\times 1\\) vector. This implies that we need at least 4 point correspondences to solve for \\(\\mathbf{h}\\). These correspondences can be used to create 4 matrices \\(A_i\\) which are then stacked to form a \\(8 \\times 9\\) matrix \\(A\\). But wait\u0026hellip; \\(A\\) is only rank 8! This is because the solution can only be determined up to a scale factor.\nIf we the matrix had rank 9, then the solution would be trivial. Remember, we\u0026rsquo;re working with a homogeneous linear system.\nRelationship to Camera Parameters Given the basic solution, let\u0026rsquo;s relate this to the camera parameters. Our goal is to estimate \\(P\\) which is a \\(3 \\times 4\\) matrix. This matrix is composed of the intrinsic matrix \\(K\\) and the extrinsic matrix \\(M\\). Since each row of \\(P\\) is a 4-vector, each \\(A_i\\) is a \\(2 \\times 12\\) matrix.\nAlgebraic versus Geometric Error The solution to the linear system minimizes the algebraic error. Even if the points are perfectly matched, the solution may not be accurate. This is because the solution is only determined up to a scale factor. The geometric error is minimized by using a non-linear optimization technique such as Levenberg-Marquardt.\nApplication: Camera Calibration Popular software solutions for calibrating cameras are provided by MATLAB and OpenCV. These are based off of work by Zhengyou Zhang. We will follow the original publication to explain the solutions to camera calibration. As such, the notation will change a bit.\nThe first goal is to relate the model points \\(\\mathbf{M}\\) to the image points \\(\\mathbf{m}\\) by a homography \\(\\mathbf{H}\\) such that\n\\[ s\\hat{\\mathbf{m}} = \\mathbf{H} \\hat{\\mathbf{M}}, \\]\nwhere \\(\\hat{\\mathbf{m}}\\) is the homogeneous coordinate of \\(\\mathbf{m}\\) (likewise for \\(\\mathbf{M}\\)) and \\(s\\) is a scale factor.\nTo relate this to the notation in the previous section,\n\\[ s\\tilde{\\mathbf{m}} = \\mathbf{H} \\tilde{\\mathbf{M}} \\equiv \\mathbf{p} = KM[\\mathbf{p}]_W. \\]\nIn the paper, \\(\\mathbf{H} = \\mathbf{A}[\\mathbf{R}\\quad \\mathbf{t}]\\), where\n\\[ \\mathbf{A} = \\begin{bmatrix} \\alpha \u0026amp; \\gamma \u0026amp; u_0\\\\ 0 \u0026amp; \\beta \u0026amp; v_0\\\\ 0 \u0026amp; 0 \u0026amp; 1\\\\ \\end{bmatrix}. \\]\n\\([\\mathbf{R}\\quad \\mathbf{t}]\\) is the matrix \\(M\\) from the previous section.\nEstimating the Homography If we have an image of the model plan, we can estimate \\(\\mathbf{H}\\) using maximum-likelihood estimation as follows. Assume that \\(\\mathbf{m}_i\\) is affected by Gaussian noise with zero mean and covariance \\(\\mathbf{\\Lambda}_{\\mathbf{m}_i}\\). The objective is then to minimize\n\\[ \\sum_i (\\mathbf{m}_i - \\hat{\\mathbf{m}_i})^T \\mathbf{\\Lambda_{\\mathbf{m}_i}^{-1}}(\\mathbf{m}_i - \\hat{\\mathbf{m}_i}), \\]\nwhere\n\\[ \\hat{\\mathbf{m}_i} = \\frac{1}{\\bar{\\mathbf{h}}_3^T \\mathbf{M}_i} \\begin{bmatrix} \\bar{\\mathbf{h}}_1^T \\mathbf{M}_i\\\\ \\bar{\\mathbf{h}}_2^T \\mathbf{M}_i\\\\ \\end{bmatrix}. \\]\n\\(\\bar{\\mathbf{h}}_i\\) represents the $i$th row of \\(\\mathbf{H}\\).\nThe approach now constructs a matrix of model points similar to the approach used by RANSAC. First, let \\(\\mathbf{x} = [\\bar{\\mathbf{h}}_1^T\\ \\bar{\\mathbf{h}}_2^T\\ \\bar{\\mathbf{h}}_3^T]\\). Recalling that \\(\\widetilde{\\mathbf{M}}^T \\in [X, Y, 1]^T\\),\n\\[ \\begin{bmatrix} \\widetilde{\\mathbf{M}}^T \u0026amp; \\mathbf{0}^T \u0026amp; -u \\widetilde{\\mathbf{M}}^T\\\\ \\mathbf{0}^T \u0026amp; \\widetilde{\\mathbf{M}}^T \u0026amp; -v \\widetilde{\\mathbf{M}}^T\\\\ \\end{bmatrix}\\mathbf{x} = \\mathbf{0}. \\]\nWriting this as \\(\\mathbf{L}\\mathbf{x} = \\mathbf{0}\\), where \\(\\mathbf{L} \\in \\mathbb{R}^{2n \\times 9}\\) with \\(n\\) being the number of points, the solution the eigenvector of \\(\\mathbf{L}^T\\mathbf{L}\\) corresponding to the smallest eigenvalue. This can be computed using Singular Value Decomposition without computing \\(\\mathbf{L}^T\\mathbf{L}\\) directly. Factorize \\(\\mathbf{L} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T\\) using SVD. Then the parameters that minimizes the objective is the column vector in \\(\\mathbf{V}\\) corresponding to the smallest eigenvalue.\nIt is noted that since the columns of the rotation matrix are orthonormal, the following constraints are observed:\n\\begin{align*} \\mathbf{h}_1^T \\mathbf{A}^{-T} \\mathbf{A}^{-1} \\mathbf{h}_2 \u0026amp;= 0\\\\ \\mathbf{h}_1^T \\mathbf{A}^{-T} \\mathbf{A}^{-1} \\mathbf{h}_1 \u0026amp;= \\mathbf{h}_2^T \\mathbf{A}^{-T} \\mathbf{A}^{-1} \\mathbf{h}_2\\\\ \\end{align*}\nSolving for the Camera Parameters With the homography \\(\\mathbf{H}\\) and constraints given in the last section, Zhang proposes a two step solution that first estimates the parameters using a closed form solution. These serve as a starting point for a non-linear optimization using Levenberg-Marquardt.\nFirst, let\n\\[ \\mathbf{B} = \\mathbf{A}^{-T} \\mathbf{A}^{-1} = \\begin{bmatrix} \\frac{1}{\\alpha^2} \u0026amp; -\\frac{\\gamma}{\\alpha^2\\beta} \u0026amp; \\frac{v_0\\gamma - u_0\\beta}{\\alpha^2 \\beta}\\\\ -\\frac{\\gamma}{\\alpha^2\\beta} \u0026amp; \\frac{\\gamma^2}{\\alpha^2\\beta^2} + \\frac{1}{\\beta^2} \u0026amp; -\\frac{\\gamma(v_0\\gamma-u_0\\beta)}{\\alpha^2\\beta^2}-\\frac{v_0}{\\beta^2}\\\\ \\frac{v_0\\gamma - u_0\\beta}{\\alpha^2 \\beta} \u0026amp; -\\frac{\\gamma(v_0\\gamma-u_0\\beta)}{\\alpha^2\\beta^2}-\\frac{v_0}{\\beta^2} \u0026amp; \\frac{(v_0\\gamma-u_0\\beta)^2}{\\alpha^2\\beta^2}+\\frac{v_0^2}{\\beta^2}+1 \\end{bmatrix}. \\]\nThis is a symmetric matrix which can be represented more compactly as\n\\[ \\mathbf{b} = [B_{11}\\ B_{12}\\ B_{22}\\ B_{13}\\ B_{23}\\ B_{33}]^T. \\]\nWe can then write \\(\\mathbf{h}_1^T \\mathbf{A}^{-T} \\mathbf{A}^{-1} \\mathbf{h}_2 = 0\\) as\n\\[ \\mathbf{h}_i^T \\mathbf{B} \\mathbf{h}_j = \\mathbf{v}_{ij}^T \\mathbf{b}, \\]\nwhere\n\\[ \\mathbf{v}_{ij} = [h_{i1}h_{j1}, h_{i1}h_{j2} + h_{i2}h_{j1}, h_{i2}h_{j2}, h_{i3}h_{j1} + h_{i1}h_{j3}, h_{i3}h_{j2}+h_{i2}h_{j3}, h_{i3}h_{j3}]^T. \\]\nThis permits us to write the constraints from the previous section as two homogeneous equations in \\(\\mathbf{b}\\)\n\\[ \\begin{bmatrix} \\mathbf{v}_{12}^T\\\\ (\\mathbf{v}_{11} - \\mathbf{v}_{22})^T \\end{bmatrix} \\mathbf{b} = \\mathbf{0}. \\]\nIf we compute the homography, as above, on \\(n\\) images, we end up with \\(n\\) equations like the one directly above. Stacking this yields\n\\[ \\mathbf{V}\\mathbf{b} = \\mathbf{0}, \\]\nwhere \\(\\mathbf{V} \\in \\mathbb{R}^{2n \\times 6}\\).\nThe initial set of parameters is then solved following the solution for each homography \\(\\mathbf{H}\\). With \\(\\mathbf{b}\\) estimated, the intrinsic parameters are\n\\begin{align*} v_0 \u0026amp;= (B_{12}B_{13} - B_{11}B_{23})/(B_{11}B_{22} - B_{12}^2)\\\\ \\lambda \u0026amp;= B_{33} - [B_{13}^2 + v_0(B_{12}B_{13} - B_{11}B_{23})]/B_{11}\\\\ \\alpha \u0026amp;= \\sqrt{\\lambda / B_{11}}\\\\ \\beta \u0026amp;= \\sqrt{\\lambda B_{11}/(B_{11}B_{22} - B_{12}^2)}\\\\ \\gamma \u0026amp;= -B_{12}\\alpha^2\\beta / \\lambda\\\\ u_0 \u0026amp;= \\gamma v_0 / \\alpha - B_{13} \\alpha^2 / \\lambda. \\end{align*}\nThe parameters \\(\\alpha, \\beta, \\gamma, u_0, v_0\\) make up our intrinsic parameter matrix \\(\\mathbf{A}\\). These can be used to compute the extrinsic parameters following\n\\begin{align*} \\lambda \u0026amp;= 1 / \\|\\mathbf{A}^{-1}\\mathbf{h}_1\\|\\\\ \\mathbf{r}_1 \u0026amp;= \\lambda \\mathbf{A}^{-1}\\mathbf{h}_1\\\\ \\mathbf{r}_2 \u0026amp;= \\lambda \\mathbf{A}^{-1}\\mathbf{h}_2\\\\ \\mathbf{r}_3 \u0026amp;= \\mathbf{r}_1 \\times \\mathbf{r}_2\\\\ \\mathbf{t} \u0026amp;= \\lambda \\mathbf{A}^{-1}\\mathbf{h}_3\\\\ \\end{align*}\nRefining the Estimates The approach in the last section is produced by minimizing algebraic distances. These values are further refined using maximum-likelihood estimation by way of a nonlinear optimizer. If there are \\(m\\) points on each of the \\(n\\) images, the objective function to minimize is given by\n\\[ \\sum_{i=1}^n \\sum_{j=1}^m \\|\\mathbf{m}_{ij} - \\hat{\\mathbf{m}}(\\mathbf{A}, \\mathbf{R}_i, \\mathbf{t}_i, \\mathbf{M}_j)\\|^2, \\]\nwhere \\(\\hat{\\mathbf{m}}(\\mathbf{A}, \\mathbf{R}_i, \\mathbf{t}_i, \\mathbf{M}_j)\\) is a projection of point \\(\\mathbf{M}_j\\) in image \\(i\\). This function is minimized using the Levenberg-Marquardt algorithm.\n","date":1646978400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720569600,"objectID":"223e4c4d7134644668c57b543af865b5","permalink":"https://ajdillhoff.github.io/notes/camera_models/","publishdate":"2022-03-11T00:00:00-06:00","relpermalink":"/notes/camera_models/","section":"notes","summary":"Table of Contents Reading Outline Pinhole Model From World Space to Image Space Camera Parameters Estimating Camera Parameters Application: Camera Calibration Reading Chapters 1 and 7 (Forsyth and Ponce) https://www.scratchapixel.com/ https://docs.google.com/presentation/d/1RMyNQR9jGdJm64FCiuvoyJiL6r3H18jeNIyocIO9sfA/edit#slide=id.p http://vision.stanford.edu/teaching/cs131_fall1617/lectures/lecture8_camera_models_cs131_2016.pdf http://vlm1.uta.edu/~athitsos/courses/cse4310_spring2021/lectures/11_geometry.pdf Outline Pinhole model Coordinates of a pinhole model Perspective projections Homogeneous coordinates Computer graphics perspective Lenses Intrinsic and extrensic parameters From world to camera to image space Camera calibration Pinhole Model Imagine piercing a small hole into a plate and placing it in front of a black screen.","tags":["machine learning"],"title":"Camera Models","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Tracking with Optical Flow Kalman Filters Introduction Tracking features and objects is required in many applications ranging from autonomous driving to security. Vision tracking systems are often used for live sports broadcasts to keep track of players, the ball, and other visual queues related to the game.\nFigure 1: Source: https://azbigmedia.com/lifestyle/ball-tracking-technology-changes-way-fans-consume-practice-sport-of-golf/ Naive tracking will detect an object per frame without any regard for prior information. More sophisticated trackers will consider the previous frame as a starting point to their search space. However, even these trackers many need to initialize after a certain amount of time if their estimate drifts too far away from the object\u0026rsquo;s actual location.\nExample of the importance of reliable tracking for driving assistance: https://youtu.be/NSDTZQdo6H8?t=898\nTracking with Optical Flow Image motion can be described as complex changes in image intensity from one time to another, displaced by \\(\\delta\\).\nThe displacement can be modeled as an affine motion field where the point is warped and translated by \\(d\\):\n\\[ \\delta = D\\mathbf{x} + \\mathbf{x}, \\]\nwhere\n\\[ D = \\begin{bmatrix} d_{xx} \u0026amp; d_{xy}\\\\ d_{yx} \u0026amp; d_{yy} \\end{bmatrix}. \\]\nWhen comparing features between an image at \\(t\\) and \\(t-1\\), the feature centered at \\(\\mathbf{x}\\) is transformed by\n\\[ \\textbf{I}_{t}(A\\mathbf{x} + \\mathbf{d}) = \\textbf{I}_{t-1}(\\mathbf{x}). \\]\nHere, \\(A = I_2 + D\\), where \\(I_2\\) is the \\(2 \\times 2\\) identity matrix. This addition will be explained later.\nSmaller variations between frames are less reliable for parameter estimation, so a pure translational model is better in these cases. That is\n\\[ \\delta = \\mathbf{d}. \\]\nComputing Image Motion Computing image motion then becomes a minimization problem.\n\\[ \\epsilon = \\int \\int_{W} \\big[\\mathbf{I}_{t}(A\\mathbf{x} + \\mathbf{d}) - \\mathbf{I}_{t-1}(\\mathbf{x})\\big]^2 w(\\mathbf{x}) d\\mathbf{x} \\]\nThis is used in a minimization problem which will minimize the dissimilarity between the tracked features between frames. The point is weighted by a function \\(w\\) over a window \\(W\\).\nMinimization of this error involves taking the derivative of \\(\\epsilon\\) with respect to the unknowns in \\(D\\) and displacement vector \\(\\mathbf{d}\\):\n\\begin{align*} \\frac{1}{2}\\frac{\\partial \\epsilon}{\\partial D} \u0026amp;= \\int \\int_W \\Big[\\mathbf{I}_t(A\\mathbf{x}+\\mathbf{d}) - \\mathbf{I}_{t-1}(\\mathbf{x})\\Big]\\mathbf{g}\\mathbf{x}^T w d\\mathbf{x} = 0\\\\ \\frac{1}{2}\\frac{\\partial \\epsilon}{\\partial \\mathbf{d}} \u0026amp;= \\int \\int_W \\Big[\\mathbf{I}_t(A\\mathbf{x}+\\mathbf{d}) - \\mathbf{I}_{t-1}(\\mathbf{x})\\Big]\\mathbf{g} w d\\mathbf{x} = 0\\\\ \\end{align*}\nwhere\n\\[ \\mathbf{g} = \\bigg(\\frac{\\partial \\mathbf{I}_t}{\\partial x}, \\frac{\\partial \\mathbf{I}_t}{\\partial y}\\bigg)^T \\]\nis this spatial gradient of the image intensity. We have computed these image gradients before!\nThe transformation of a point from \\(t\\) to \\(t+1\\) is complex because of the potential rotation, scaling, and translation that can occur. Specifically, \\(A\\mathbf{x} + \\mathbf{d}\\) is a nonlinear transformation. To solve this with less friction, the term can be approximated.\nTo linearlize \\(\\mathbf{I}_{t}\\), a Taylor series expansion of it can be used, taking just the linear term:\n\\[ \\mathbf{I}_t(A\\mathbf{x}+\\mathbf{d}) = \\mathbf{I}_t(\\mathbf{x})+\\mathbf{g}^T(\\mathbf{u}), \\]\nwhere\n\\[ \\mathbf{u} = D\\mathbf{x}+\\mathbf{d}. \\]\nThe authors argue that this approximation is reasonable assuming the motion in the images is small.\nPlugging this back into the derivatives above yields\n\\begin{align*} \\int \\int_W \\mathbf{g}\\mathbf{x}^T(\\mathbf{g}^T\\mathbf{u})w d\\mathbf{x} \u0026amp;= \\int \\int_W \\big[\\mathbf{I}_{t-1}(\\mathbf{x}) - \\mathbf{I}_t\\big]\\mathbf{g}\\mathbf{x}^T w d\\mathbf{x}\\\\ \\int \\int_W \\mathbf{g}(\\mathbf{g}^T\\mathbf{u})w d\\mathbf{x} \u0026amp;= \\int \\int_W \\big[\\mathbf{I}_{t-1}(\\mathbf{x}) - \\mathbf{I}_t\\big]\\mathbf{g} w d\\mathbf{x}\\\\ \\end{align*}\nThis is solved iteratively, following the Newton method, starting with the following values at \\(t=0\\):\n\\begin{align*} D_0 \u0026amp;= I\\\\ \\mathbf{d}_0 \u0026amp;= \\mathbf{0}\\\\ \\mathbf{I}_0 \u0026amp;= \\mathbf{I}(\\mathbf{x}). \\end{align*}\nAt step \\(i\\) the values are updated to\n\\begin{align*} D_i\\\\ \\mathbf{d}_i\\\\ \\mathbf{I}_i \u0026amp;= \\mathbf{I}_{i-1}(A_i \\mathbf{x} + \\mathbf{d}_i). \\end{align*}\nA More Compact Representation At this point, the authors convert this representation into a more compact form in which the unknowns in \\(D\\) and the values of \\(\\mathbf{d}\\) are separated from the function. We start with our current system of equations.\n\\begin{align*} \\int \\int_W \\mathbf{g}\\mathbf{x}^T(\\mathbf{g}^T\\mathbf{u})w d\\mathbf{x} \u0026amp;= \\int \\int_W \\big[\\mathbf{I}_{t-1}(\\mathbf{x}) - \\mathbf{I}_t(\\mathbf{x})\\big]\\mathbf{g}\\mathbf{x}^T w d\\mathbf{x}\\\\ \\int \\int_W \\mathbf{g}(\\mathbf{g}^T\\mathbf{u})w d\\mathbf{x} \u0026amp;= \\int \\int_W \\big[\\mathbf{I}_{t-1}(\\mathbf{x}) - \\mathbf{I}_t(\\mathbf{x})\\big]\\mathbf{g} w d\\mathbf{x}\\\\ \\end{align*}\nTo achieve this, the Kronecker product is used. This is a generalization of the outer product from vectors to matrices. For two matrices \\(A \\in \\mathbb{R}^{p \\times q}\\) and \\(B \\in \\mathbf{R}^{m \\times n}\\), \\(A \\otimes B\\) is a \\(p \\times q\\) block matrix\n\\[ A \\otimes B = \\begin{bmatrix} a_{11}B \u0026amp; \\cdots \u0026amp; a_{1q}\\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ a_{p1}B \u0026amp; \\cdots \u0026amp; a_{pq}B \\end{bmatrix}. \\]\nIt has two particularly useful properties for this problem:\n\\(A^T \\otimes B^T = (A \\otimes B)^T\\) \\(v(AXB) = (B^T \\otimes A)v(X)\\), where \\(v\\) is the vectorization operator. Using this product and the properties just listed, we can extract the unknowns \\(D\\) and \\(\\mathbf{d}\\) from the equations above.\nFirst, note that \\(\\mathbf{g}^T \\mathbf{u}\\) appear in both of the equations. These can then be rewritten as follows.\n\\begin{align*} \\mathbf{g}^T \\mathbf{u} \u0026amp;= \\mathbf{g}^T(D\\mathbf{x} + \\mathbf{d})\\\\ \u0026amp;= \\mathbf{g}^T D \\mathbf{x} + \\mathbf{g}^T \\mathbf{d}\\\\ \u0026amp;= v(\\mathbf{g}^T D \\mathbf{x}) + \\mathbf{g}^T \\mathbf{d} \u0026amp;\\text{ vectorization }\\\\ \u0026amp;= (\\mathbf{x}^T \\otimes \\mathbf{g}^T)v(D) + \\mathbf{g}^T \\mathbf{d} \u0026amp;\\text{ property 2 }\\\\ \u0026amp;= (\\mathbf{x} \\otimes \\mathbf{g})^T v(D) + \\mathbf{g}^T \\mathbf{d} \u0026amp;\\text{ property 1 } \\end{align*}\nLikewise, the term \\(\\mathbf{g}\\mathbf{x}^T\\) that appears on the right side of the first equation in our set to solve can be written as\n\\begin{align*} v(\\mathbf{g}\\mathbf{x}^T) \u0026amp;= v(\\mathbf{g}1\\mathbf{x}^T)\\\\ \u0026amp;= \\mathbf{x} \\otimes \\mathbf{g}. \\end{align*}\nPlugging this into the first equation to solve from above yields\n\\[ \\int \\int_W (\\mathbf{x} \\otimes \\mathbf{g})((\\mathbf{x} \\otimes \\mathbf{g})^T v(D) + \\mathbf{g}^T\\mathbf{d})w d\\mathbf{x} = \\int \\int_W \\big[\\mathbf{I}_{t-1}(\\mathbf{x}) - \\mathbf{I}_t(\\mathbf{x})\\big](\\mathbf{x} \\otimes \\mathbf{g}) w d\\mathbf{x}. \\]\nExpanding these terms out produces\n\\begin{align*} \\bigg(\\int \\int_W (\\mathbf{x} \\otimes \\mathbf{g})(\\mathbf{x} \\otimes \\mathbf{g})^T w d\\mathbf{x}\\bigg) v(D) + \\bigg(\\int \\int_W (\\mathbf{x} \\otimes \\mathbf{g}) \\mathbf{g}^T w d\\mathbf{x}\\bigg) \\mathbf{d}\\\\ = \\int \\int_W \\big[\\mathbf{I}_{t-1}(\\mathbf{x}) - \\mathbf{I}_t(\\mathbf{x})\\big](\\mathbf{x} \\otimes \\mathbf{g}) w d\\mathbf{x}. \\end{align*}\nThe authors further simplify this equation using the following variables:\n\\begin{align*} U(\\mathbf{x}) \u0026amp;= (\\mathbf{x} \\otimes \\mathbf{g})(\\mathbf{x} \\otimes \\mathbf{g})^T\\\\ V(\\mathbf{x}) \u0026amp;= (\\mathbf{x} \\otimes \\mathbf{g})\\mathbf{g}^T\\\\ \\mathbf{b}(\\mathbf{x}) \u0026amp;= \\big[\\mathbf{I}_{t-1}(\\mathbf{x}) - \\mathbf{I}_{t}(\\mathbf{x})\\big]v(\\mathbf{g}\\mathbf{x}^T). \\end{align*}\nThen the above equation can be written as\n\\[ \\bigg(\\int \\int_W U(\\mathbf{x}) w d\\mathbf{x}\\bigg) v(D) + \\bigg(\\int \\int_W V(\\mathbf{x}) w d\\mathbf{x}\\bigg) \\mathbf{d} = \\int \\int_W \\mathbf{b}(\\mathbf{x}) w d\\mathbf{x}. \\]\nTo write the second equation in a similar way, the authors introduce two additional variables\n\\begin{align*} Z(\\mathbf{x}) \u0026amp;= \\mathbf{g}\\mathbf{g}^T\\\\ \\mathbf{x}(\\mathbf{x}) \u0026amp;= \\big[\\mathbf{I}_{t-1}(\\mathbf{x}) - \\mathbf{I}_t(\\mathbf{x})\\big]\\mathbf{g}. \\end{align*}\nThen,\n\\[ \\bigg(\\int \\int_W V^T(\\mathbf{x}) w d\\mathbf{x}\\bigg) v(D) + \\bigg(\\int \\int_W Z(\\mathbf{x}) w d\\mathbf{x}\\bigg) \\mathbf{d} = \\int \\int_W \\mathbf{c}(\\mathbf{x}) w d\\mathbf{x}. \\]\nThese equations can be written in a simple form: \\(A\\mathbf{x} + B\\mathbf{y} = \\mathbf{z}\\). A symmetric block matrix \\(T \\in \\mathbb{R}^{6 \\times 6}\\) is then introduced:\n\\begin{align*} T \u0026amp;= \\int \\int_W \\begin{bmatrix} U \u0026amp; V\\\\ V^T \u0026amp; Z \\end{bmatrix} w d\\mathbf{x}\\\\ \u0026amp;= \\int \\int_W \\begin{bmatrix} x^2g_x^2 \u0026amp; x^2 g_x g_y \u0026amp; x y g_x^2 \u0026amp; x y g_x g_y \u0026amp; x g_x^2 \u0026amp; x g_x g_y\\\\ x^2 g_x g_y \u0026amp; x^2 g_y^2 \u0026amp; x y g_x g_y \u0026amp; x y g_y^2 \u0026amp; x g_x g_y \u0026amp; x g_y^2\\\\ x y g_x^2 \u0026amp; x y g_x g_y \u0026amp; y^2 g_x^2 \u0026amp; y^2 g_x g_y \u0026amp; y g_x^2 \u0026amp; y g_x g_y\\\\ x y g_x g_y \u0026amp; x y g_y^2 \u0026amp; y^2 g_x g_y \u0026amp; y^2 g_y^2 \u0026amp; y g_x g_y \u0026amp; y g_y^2\\\\ x g_x^2 \u0026amp; x g_x g_y \u0026amp; y g_x^2 \u0026amp; y g_x g_y \u0026amp; g_x^2 \u0026amp; g_x g_y\\\\ x g_x g_y \u0026amp; x g_y^2 \u0026amp; y g_x g_y \u0026amp; y g_y^2 \u0026amp; g_x g_y \u0026amp; g_y^2 \\end{bmatrix}w d \\mathbf{x}. \\end{align*}\nLook very closely at \\(Z\\). Does that remind you of anything? Harris corner detection!\nThe unknowns are vectorized as\n\\[ \\mathbf{z} = \\begin{bmatrix} v(D)\\\\ \\mathbf{d} \\end{bmatrix}. \\]\nThe product vector is defined as\n\\begin{align*} \\mathbf{a} \u0026amp;= \\int \\int_W \\begin{bmatrix} \\mathbf{b}\\\\ \\mathbf{c} \\end{bmatrix} w d \\mathbf{x}\\\\ \u0026amp;= \\int \\int_W \\Big[\\mathbf{I}_{t-1}(\\mathbf{x}) - \\mathbf{I}_t(\\mathbf{x})\\Big]\\begin{bmatrix} x g_x\\\\ x g_y\\\\ y g_x\\\\ y g_y\\\\ g_x\\\\ g_y \\end{bmatrix} w d \\mathbf{x}. \\end{align*}\nThus, the iterative solution requires solving the \\(6 \\times 6\\) linear system\n\\[ T \\mathbf{z} = \\mathbf{a}. \\]\nBack to Computing Image Motion With this more compact representation, the iterative solution is easier to achieve. The authors conveniently note that the deformation of the feature window between frames will be relatively small, so \\(D\\) could be set to 0 for tracking. This leads to the solution of a much smaller system for each time step:\n\\[ Z \\mathbf{d} = \\begin{bmatrix} g_x\\\\ g_y \\end{bmatrix}. \\]\nNote that this is only true for small steps between frames. It is also important to measure the dissimilarity between the feature at the initial frame as it changes over time with the iterative estimates. If it changes too much, the dissimilarity will be high, indicating that it is no longer a reliable feature to track.\nPicking the Best Feature Shi and Tomasi posit that the best feature is one that can be tracked well.\n\u0026ldquo;We can track a window from frame to frame if this system represents good measurements, and if it can be solved reliably.\u0026rdquo;\nThey analyze the basic equation \\(Z \\mathbf{d} = \\mathbf{e}\\), which is solved during tracking. If both eigenvalues of \\(Z\\) are large and do not differ by several orders of magnitude, the feature can be tracked reliably. That is, they accept a window if the eigenvalues of \\(Z\\) satisfy\n\\[ \\min(\\lambda_1, \\lambda_2) \u0026gt; \\lambda. \\]\nIn practice, \\(\\lambda\\) is determined by selecting a lower bound based on a region of uniform brightness in the image as well as an upper bound based on features such as corners. The selected value of \\(\\lambda\\) is somewhere in between.\nMeasuring dissimilarity To determine if a feature is still reliable over a longer time period, a measure of dissimilarity is used to measure the original feature versus its warped version at the current frame. Consider the sequence below over 21 frames.\nFigure 2: Source: Shi and Tomasi. Their method successfully tracks the speed limit sign, as seen below.\nFigure 3: Source: Shi and Tomasi. Figure 4: Source: Shi and Tomasi. They note the importance of using an affine deformation to track reliable features. The figure below plots the dissimilarity using translation versus the deformation matrix over time.\nFigure 5: Dissimilarity over time using translation (dashed) versus affine (solid) (Shi and Tomasi). They also present a case when the feature is lost to occlusion, thus the dissimilarity of both approaches increases greatly over time.\nFigure 6: Source: Shi and Tomasi. Figure 7: Source: Shi and Tomasi. Figure 8: Sign tracking (plusses) versus window tracking (circles) (Shi and Tomasi). Kalman Filters A Kalman filter is a linear dynamic model that models conditional probabilities following normal distributions. It is an simple and effective model for tracking motion even in the presence of noise from measurements. Kalman filters keep an estimate of the state and can update their estimates based on the given observations.\n\\(\\mathbf{X}_i\\) - State of object at step \\(i\\)\n\\(\\mathbf{Y}_i\\) - Measurement at step \\(i\\)\nThere are two primary tasks to deal with, the real-time tracking task and the offline smoothing task. We are more interested in the tracking task, so we will focus on that.\nTracking task: \\(P(X_k|Y_0, \\dots, Y_k)\\)\nSmoothing task: \\(P(X_k|X_0, \\dots, Y_n)\\)\nAssumptions:\n\\(P(Y_k|X_0, \\dots, X_N, Y_0, \\dots, Y_N) = P(Y_k|X_k)\\) \\(P(X_k|X_0, \\dots, X_{k-1}) = P(X_k|X_{k-1})\\) Prediction Task When tracking an object, we use the model to first predict a state and then update its current parameters given some measurement. We want to predict the state given measurements: \\(P(\\mathbf{X}_i|\\mathbf{Y}_0 = \\mathbf{y}_0, \\dots, \\mathbf{Y}_{k-1}=\\mathbf{y}_{k-1})\\).\nGiven the previous observations up to \\(k-1\\), what is our model\u0026rsquo;s estimate of the current state? This can help establish a search location if we were looking to narrow our detection algorithm.\nCorrection \\(P(\\mathbf{X}_i|\\mathbf{Y}_0 = \\mathbf{y}_0, \\dots, \\mathbf{Y}_{i}=\\mathbf{y}_{i})\\) is the cur rent distribution. This is the estimate given the actual observation at \\(i\\). Note that this observation could be given from a noisy measurement.\nLinear Dynamics Assuming linear models, the problem becomes much simpler. We can model the observations and state using normal distributions.\n\\[ \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu}, \\Sigma) \\]\nThe measurements themselves can be modeled as\n\\[ \\mathbf{y}_k \\sim \\mathcal{N}(\\mathcal{B}_k \\mathbf{x}_k, \\Sigma_k), \\]\nwhere \\(k\\) is the current step.\nIn other words, our model does not know the true position and velocity of objects at all times. If this were the case, tracking would be a trivial pursuit. This uncertainty is captured using Gaussian distributions. This is a reasonable assumption to make in many cases. It becomes more complex if we think about acceleration, but we could always model that as well.\nBy the way, position and velocity are correlated. If the velocity of the current object is 0, you would probably bet that its position in the next frame will be the same. A covariance matrix tracks this correlation between the variables. The problem is starting to take shape. We have input and observation variables as well as a covariance matrix that defines the relationship between our position and velocity.\nFor the future, the model will be represented as\n\\begin{align*} \\mathbf{x}_i \u0026amp;\\sim \\mathcal{N}(\\mathcal{D}_i \\mathbf{x}_{i-1}; \\Sigma_{d_i})\\\\ \\mathbf{y}_i \u0026amp;\\sim \\mathcal{N}(\\mathcal{M}_i \\mathbf{x}_i; \\Sigma_{m_i}). \\end{align*}\nThe mean parameter in both of these look a little strange; we usually see \\(\\mu\\) here. The matrix \\(\\mathcal{D}_i\\), for example, transforms the center of the Gaussian to an updated location. Consider a bird flying through the sky in a relatively straight pattern. Of course, there will be some variation due to external factors or the bird\u0026rsquo;s desire to move about. Modeling this motion with a Gaussian centered at the mean \\(\\mathbf{x}_{i-1}\\) would be insufficient.\nFor one, the likelihood that the bird would instantly change course is physically impossible. It could start to turn, but the resolution of our update steps would track this over a sequence of frames. Instead, we transform the value \\(\\mathbf{x}_{i-1}\\) based on our current perception of the bird\u0026rsquo;s movement dynamics. This translates the Gaussian to a new location. It also has the possibility of changing its shape.\nFor covering the algorithm, we will use notation following Forsyth and Ponce. \\(\\bar{\\mathbf{x}}_i^-\\) is the mean of \\(P(\\mathbf{x}_i|y_0, \\dots, y_{i-1})\\) and \\(\\bar{\\mathbf{x}}_i^+\\) is the mean of \\(P(\\mathbf{x}_i|y_0, \\dots, y_{i})\\). \\(\\Sigma_i^-\\) and \\(\\Sigma_i^+\\) are the covariances of those distributions.\nHowever, by making a convenient assumption about the observation model, we will mainly need to focus on the state model. That is, the matrix \\(\\mathcal{M}_i\\) is defined so that the mean is simply the state position at step \\(i\\).\nWhat does our state represent?\nIn a simple model, \\(\\mathbf{x} = \\begin{bmatrix}p_x\\\\p_y\\\\v_x\\\\v_y\\end{bmatrix}\\). That is the 2D position and velocity of the object being tracked. The corresponding covariance matrix is \\(\\mathbf{x}\\mathbf{x}^T\\).\nHow do we predict the position and velocity of the next time step?\n\\begin{align*} \\mathbf{p}_k \u0026amp;= \\mathbf{p}_{k-1} + \\Delta t \\mathbf{v}_{k-1}\\\\ \\mathbf{v}_k \u0026amp;= \\mathbf{v}_{k-1} \\end{align*}\nThis is making a simple, yet surprisingly effective, assumption that the velocity is constant. We can write this as a matrix vector product:\n\\[ \\mathbf{x}_k = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; \\Delta t \u0026amp; 0\\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; \\Delta t\\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\mathbf{x}_{k-1} \\]\nCompactly, the prediction for step \\(t\\) is \\(\\bar{\\mathbf{x}}_k^- = \\mathcal{D}_i \\bar{\\mathbf{x}}_{k-1}^+\\).\nSince we updated every point \\(\\mathbf{x}_{k-1}\\), we also need to make a prediction about the covariance matrix. This is also achieved by multiplying every point by \\(\\mathcal{D}_i\\)\n\\[ \\Sigma_{i}^- = \\mathcal{D}_i\\Sigma_{i-1}^+\\mathcal{D}_i^T \\]\nWhat if we want to add additional knowledge like acceleration?\nOur position prediction then follows\n\\begin{align*} \\mathbf{p}_i \u0026amp;= \\mathbf{p}_{i-1} + \\Delta t \\mathbf{v}_{i-1} + \\frac{1}{2} \\Delta t^2 \\mathbf{a}_{i-1}\\\\ \\mathbf{v}_i \u0026amp;= \\mathbf{v}_{i-1} + \\Delta t \\mathbf{a}_{i-1} \\end{align*}\nWe can simplify this and assume constant acceleration, then \\(\\mathbf{a}_i = \\mathbf{a}_{i-1}\\). The resulting update equation for \\(\\mathbf{x}_k\\) becomes\n\\[ \\mathbf{x}_k = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; \\Delta t \u0026amp; 0 \u0026amp; \\frac{1}{2}\\Delta t^2 \\mathbf{a} \u0026amp; 0\\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; \\Delta t \u0026amp; 0 \u0026amp; \\frac{1}{2}\\Delta t^2 \\mathbf{a}\\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; \\Delta t \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; \\Delta t\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1\\\\ \\end{bmatrix} \\mathbf{x}_{k-1} \\]\nHowever, if something like acceleration is a known control factor in our system, then it does not need to be part of the state. In this case, we could separate the update vector into\n\\[ \\mathbf{x}_k = \\mathcal{D}_i\\mathbf{x}_{k-1} + B_k \\mathbf{u}_{k}, \\]\nwhere \\(B_k\\) is the control matrix and \\(\\mathbf{u}_k\\) is the control vector.\nOne last consideration is that of uncertainty due to factors outside of our system. This can also be modeled using a Gaussian with zero mean and covariance \\(\\Sigma_d\\):\n\\[ \\xi_k \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma_d). \\]\nWith this added noise, the prediction step becomes\n\\begin{align*} \\bar{\\mathbf{x}}_k^- \u0026amp;= \\mathcal{D}_k\\bar{\\mathbf{x}}_{k-1}^+ + B_k \\mathbf{u}_k\\\\ \\Sigma_k^- \u0026amp;= \\mathcal{D}_k \\Sigma_{k-1}^+\\mathcal{D}_k^T + \\xi_k. \\end{align*}\nIn words, \\(\\bar{\\mathbf{x}}_k^-\\) is the prediction of our current state based on the previous best estimate with an added correction term based on known factors (acceleration). \\(\\Sigma_k^-\\) is the updated uncertainty based on the old uncertainty with added Gaussian noise to reflect unknown factors.\nMaking Corrections As we are tracking an object, we may have some way of getting measurements. This could be through an object detector or other physical sensor. The new measurement can refine our current set of parameters. Kalman filters work well even if the measurement is noisy.\nFigure 9: 1D Kalman Filter. Source: Forsyth and Ponce In the figure above, the initial prediction has a large amount of uncertainty. After updating with the current measurement, the uncertainty is reduced.\nThe goal here is to reconcile the uncertainty of our predicted state with the uncertainty of the measurement. That is, we want to know the distribution over the union of these two distributions. This is achieved by multiplying the Gaussians together.\n\\[ \\mathcal{N}(\\bar{\\mathbf{x}}_i^+, \\Sigma_i^+) = \\mathcal{N}(\\bar{\\mathbf{x}}_i^-, \\Sigma_i^-) * \\mathcal{N}(\\mathbf{y}_i, \\Sigma_{m_i}) \\]\nSolving for \\(\\bar{\\mathbf{x}}_i^+\\) and \\(\\Sigma_i^+\\) yields\n\\begin{align*} \\bar{\\mathbf{x}}_i^+ \u0026amp;= \\bar{\\mathbf{x}}_i^- + \\mathcal{K}_i(\\mathbf{y}_i - \\bar{\\mathbf{x}}_i^-)\\\\ \\Sigma_i^+ \u0026amp;= \\Sigma_i^- - \\mathcal{K}_i \\Sigma_i^-, \\end{align*}\nwhere\n\\[ \\mathcal{K}_i = \\Sigma_i^-(\\Sigma_i^- + \\Sigma_{m_i})^{-1}. \\]\n\\(\\mathcal{K}_i\\) is called the Kalman gain.\nLet\u0026rsquo;s combine the prediction and correction steps\nWe have a state distribution with mean and variance\n\\begin{align*} \\bar{\\mathbf{x}}_k^- \u0026amp;= \\mathcal{D}_k\\bar{\\mathbf{x}}_{k-1}^+ + B_k \\mathbf{u}_k\\\\ \\Sigma_k^- \u0026amp;= \\mathcal{D}_k \\Sigma_{k-1}^+\\mathcal{D}_k^T + \\xi_k \\end{align*}\nas well as an observation distribution with mean and variance \\(\\mathbf{y}_k\\) and \\(\\Sigma_{m_k}\\).\nPlugging these into the update equations yield\n\\begin{align*} \\bar{\\mathbf{x}}_i^+ \u0026amp;= \\mathcal{D}_k\\bar{\\mathbf{x}}_{k-1}^+ + \\mathcal{K}_i(\\mathbf{y}_i - \\mathcal{D}_k\\bar{\\mathbf{x}}_{k-1}^+)\\\\ \\Sigma_i^+ \u0026amp;= \\mathcal{D}_k \\Sigma_{k-1}^+\\mathcal{D}_k^T - \\mathcal{K}_i \\mathcal{D}_k \\Sigma_{k-1}^+\\mathcal{D}_k^T, \\end{align*}\nwhere\n\\[ \\mathcal{K}_i = \\mathcal{D}_k \\Sigma_{k-1}^+\\mathcal{D}_k^T(\\mathcal{D}_k \\Sigma_{k-1}^+\\mathcal{D}_k^T + \\Sigma_{m_i})^{-1}. \\]\n","date":1646632800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708905600,"objectID":"640d979f4ecdd3e8fc21d91460e20426","permalink":"https://ajdillhoff.github.io/notes/tracking/","publishdate":"2022-03-07T00:00:00-06:00","relpermalink":"/notes/tracking/","section":"notes","summary":"Table of Contents Introduction Tracking with Optical Flow Kalman Filters Introduction Tracking features and objects is required in many applications ranging from autonomous driving to security. Vision tracking systems are often used for live sports broadcasts to keep track of players, the ball, and other visual queues related to the game.\nFigure 1: Source: https://azbigmedia.com/lifestyle/ball-tracking-technology-changes-way-fans-consume-practice-sport-of-golf/ Naive tracking will detect an object per frame without any regard for prior information. More sophisticated trackers will consider the previous frame as a starting point to their search space.","tags":["computer vision"],"title":"Tracking","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Motion Features Computing Optical Flow Assumptions of Small Motion Applications Introduction Optical flow refers to the apparent motion in a 2D image. Optical flow methods estimate a motion field, which refers to the true motion of objects in 3D. If a fixed camera records a video of someone walking from the left side of the screen to the right, a difference of two consecutive frames reveals much about the apparent motion.\nMany different approaches to optical flow have been proposed from classical, algorithmic methods to deep learning-based methods. These notes will focus on the definition and classical methods, leaving the rest for future lectures.\nMotion Features Motion features are used a wide variety of tasks from compression, image segmentation, tracking, detection, video de-noising, and more. For example, human activity recognition methods can leverage motion features to identify complex actions. Along with traditional image-based features,\nConsider a sphere with Lambertian reflectance.\nIf the sphere is rotated:\nWhat does the motion field look like? What does the optical flow look like? If, instead, a point light is rotated around the sphere:\nWhat does the motion field look like? What does the optical flow look like? It is also possible to infer relative depth from video. As the sphere moves towards the camera, its apparent size will become larger. If we were to analyze the optical flow of such a sequence, we would see that the flow is radial as the sphere, projected as a circle, grows.\nFigure 1: From \u0026ldquo;Computer Vision - A Modern Approach\u0026rdquo; by Forsyth and Ponce At \\(t=1\\), the radius of the circle is given as \\(R\\). At \\(t=2\\), the radius is \\(r = f\\frac{R}{Z}\\), where \\(f\\) is some function of the motion and \\(Z\\) is the distance between the sphere and the camera. From this, we can also compute the speed at which the sphere is travelling towards the camera as \\(V=\\frac{dZ}{dt}\\). The apparent rate of growth as observed by the camera is \\(\\frac{dr}{dt} = -f\\frac{RV}{Z^2}\\). We can also determine the time to contact with the camera as \\(-\\frac{Z}{V}\\).\nFigure 2: Optical flow on different parts of the image as observed from a moving camera whose direction of focus is perpendicular to the white plane. Source: \u0026ldquo;Computer Vision - A Modern Approach\u0026rdquo; by Forsyth and Ponce In the figure above, the observer is moving at a constant rate to the left. The points in the image appear to translate towards the right edge of the frame. Points that are close to the camera appear to move faster than those that are farther away. This can be used to estimate the apparent depth between objects in a scene.\nComputing Optical Flow A popular assumption for optical flow, as discussed in (Horn and Schunck 1980), is that of brightness constancy. A local feature has the same image intensity in one frame as it does in the subsequent frame.\n\\[ I(x + u, y + v, t + 1) = I(x, y, t) \\]\nThe Aperture Problem Consider a small window in an image. As the subject in the image moves, the window will only capture the motion in the direction of the gradient. This is known as the aperture problem.\nFigure 3: The aperture problem (Murakami 2004). The problem is that only one direction of motion can be established, but there are many possible directions of motion. Mathematically, the system is underconstrained because there are two unknowns (\\(u\\) and \\(v\\)) for each pixel.\nLocal Constancy and Smoothness One way to address the aperture problem is to assume that the motion is constant within a local neighborhood (Lucas and Kanade, n.d.). Given a window of size \\(n \\times n\\), we have \\(n^2\\) equations for each pixel. This is a more constrained system and can be solved using the normal equations.\nAn additional assumption is that the motion is smooth across the image. This is known as the smoothness assumption. The energy function is then modified to include a term that penalizes the difference in motion between neighboring pixels (Horn and Schunck 1980).\nGiven these two constraints, we can formulate an objective function. First, the objective function assuming brightness constancy is given by\n\\[ E_D(\\mathbf{u}, \\mathbf{v}) = \\sum_{s}(I(x_s + u_s, y_s + v_s, t + 1) - I(x, y, t))^2. \\]\nAdding in the assumption of uniform motion in a local region yields the term\n\\[ E_S(\\mathbf{u}, \\mathbf{v}) = \\sum_{n\\in G(s)}(u_s - u_n)^2 + \\sum_{n \\in G(s)}(v_s - v_n)^2. \\]\nPutting these together, with a weighting term, yields\n\\[ E(\\mathbf{u}, \\mathbf{v}) = E_D + \\lambda E_S. \\]\nThis energy function \\(E_D\\) is minimized by differentiating and setting the equation to 0:\n\\[ I_x u + I_y v + I_t = 0. \\]\nTo be more specific, a Taylor series approximation of the original difference equation is used to derive the above equation. The Taylor series is truncated at the first order since the assumption is that the motion is small; the higher order terms are negligible. Taking the first order approximation allows us to compute the flow at sub-pixel accuracy. More importantly, it allows us to frame the problem as a system of linear equations. The entire energy to be minimized is then\n\\[ E_D(\\mathbf{u}, \\mathbf{v}) = \\sum_{s}(I_{x,s}u_s + I_{y,s}v_s + I_{t,s})^2 + \\lambda \\sum_{n\\in G(s)}(u_s - u_n)^2 + \\sum_{n \\in G(s)}(v_s - v_n)^2. \\]\nDifferentiating this and setting to 0 yields two equations in \\(u\\) and \\(v\\):\n\\begin{align*} \\sum_s{(I_{x,s}^2 u_s + I_{x,s}I_{y,s}v_s + I_{x,s}I_{t,s}) + \\lambda \\sum_{n \\in G(s)}(u_s - u_n)} \u0026amp;= 0\\\\ \\sum_s{(I_{x,s}I_{y,s} u_s + I_{y,s}^2v_s + I_{y,s}I_{t,s}) + \\lambda \\sum_{n \\in G(s)}(v_s - v_n)} \u0026amp;= 0\\\\ \\end{align*}\nNote that this is computed for every pixel in the image. This system is no longer underspecified because of the assumption that neighbors will exhibit the same flow. We now have 5 equations per pixel. In more recent works, larger neighborhood grids (\\(5 \\times 5\\)) are used. Then, we have 25 equations per pixel. Since this is a system of linear equations, it could be computed directly using the normal equations.\nHowever, Horn and Schunck did not have very fast computers in 1981. So, they introduced an iterative solution (Horn and Schunck 1980).\nAssumptions of Small Motion One of the core assumptions in early formulations of optical flow is that motion is very small (\u0026lt;1 pixel). In reality, some objects may move over 100 pixels within a single frame. A simple solution to this problem was proposed by Bergen et al. in 1992 (Bergen et al., n.d.). By creating an image pyramid over several resolutions, the assumption of small motion at each scale is still reasonable.\nFigure 4: Hierarchical motion estimation (Bergen et al.) At one scale, the warping parameters are estimated. Next, they are used to warp the image to match the one at \\(t-1\\). The warped image and true image at \\(t-1\\) are compared to refine the parameters. The refined parameters are then sent to the next scale layer.\nApplications Features for tracking Segmentation Optical mouse (used in early mice) Image stabilization Video compression Datasets (Sintel) \u0026hellip; and more References Bergen, James R, P Anandan, Keith J Hanna, and Rajesh Hingorani. n.d. “Hierarchical Model-Based Motion Estimation,” 16. Horn, Berthold K. P., and Brian G. Schunck. 1980. “Determining Optical Flow.” https://dspace.mit.edu/bitstream/handle/1721.1/6337/%EE%80%80AIM%EE%80%81-572.pdf?sequence=2. Lucas, Bruce D, and Takeo Kanade. n.d. “An Iterative Image Registration Technique with an Application to Stereo Vision,” 10. Murakami, Ikuya. 2004. “The aperture problem in egocentric motion,” 174–77. https://doi.org/https://doi-org.ezproxy.uta.edu/10.1016/j.tins.2004.01.009. ","date":1646546400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719187200,"objectID":"ac7b3648bffa8c22ef0116f850dca2b1","permalink":"https://ajdillhoff.github.io/notes/optical_flow/","publishdate":"2022-03-06T00:00:00-06:00","relpermalink":"/notes/optical_flow/","section":"notes","summary":"Table of Contents Introduction Motion Features Computing Optical Flow Assumptions of Small Motion Applications Introduction Optical flow refers to the apparent motion in a 2D image. Optical flow methods estimate a motion field, which refers to the true motion of objects in 3D. If a fixed camera records a video of someone walking from the left side of the screen to the right, a difference of two consecutive frames reveals much about the apparent motion.","tags":["computer vision"],"title":"Optical Flow","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Agglomerative Clustering K-Means Clustering Simple Linear Iterative Clustering (SLIC) Superpixels in Recent Work Introduction The goal of segmentation is fairly broad: group visual elements together. For any given task, the question is how are elements grouped? At the smallest level of an image, pixels can be grouped by color, intensity, or spatial proximity. Without a model of higher level objects, the pixel-based approach will break down at a large enough scale.\nSegmentation by thresholding works in cases where the boundaries between features are clearly defined. However, thresholding is not very robust to complex images with noise. Consider a simple image and its intensity histogram as noise is added.\nFigure 1: From left to right, a noiseless image with increasing amounts of Gaussian noise added. Source: Pearson Education, Inc. Even with some noise added, as seen in the middle image, thresholding is still relatively straightforward. Once enough noise is added, thresholding via pixel intensities will not work. A more sophisticated approach is needed in this case.\nClustering is a fairly intuitive way to think about segmentation. Instead of a fine-grained representation of an image as a collection of pixels, it is represented as groups or clusters that share some common features. The general process of clustering is simple. The image is represented as a collection of feature vectors (intensity, pixel color, etc.). Feature vectors are assigned to a single cluster. These clusters represent some segment of the image.\nWhen it comes to clustering methods, there are two main approaches: agglomerative and divisive. Simply, one is a bottom-up approach. The other is a top-down approach. After briefly introductin agglomerative clustering, we will explore specific implementations of segmentation using k-means clustering as well as segmentation using superpixels \u0026lt;\u0026amp;achantaSLICSuperpixelsCompared2012\u0026gt;.\nAgglomerative Clustering Agglomerative clustering methods start by assuming every element is a separate cluster. Elements are formed based on some local similarities. As these methods iterate, the number of clusters decreases. Deciding which elements to merge depends on inter-cluster distance. The exact choice of distance is dependent on the task. Some examples include:\nSingle-link clustering: The distance between the closest elements. Complete-link clustering: The maximum distance between an element of the first cluster and one of the second. Group average clustering: Average distance of elements in a cluster. How many clusters are or should be in a single image?\nThis is a difficult question to answer for many reasons. The answer will be largely dependent on the task at hand. It is a problem of learning the underlying generative process of the visual elements in the image. By defining the specific goal of segmentation (segment by color, shape, etc.), we are introducing a prior about the underlying generative processes which formed the image.\nFigure 2: 3D-PointCapsNet learns point segmentations on only 1% of the training data (Zhao et al.). There are approaches which attempt to segment objects in semi-supervised settings. As seen in Figure 1, Zhao et al. propose a part segmentation model for 3D objects which only utilizes 1-5% of the training part labels \u0026lt;\u0026amp;zhao3DPointCapsule2019\u0026gt;.\nFor example, if we divised an algorithm that would segment an image by color values, it might be able to segment the hand wearing a solid color glove relatively easily. If we wanted to segment the hand into its individual joints, we would have to introduce a visual prior such as asking the subject to wear a multicolored glove. We could also add prior information about the hand shape and joint configuration into the model itself.\nFigure 3: An image-based joint regression model predicts joint locations (left) along with a point cloud generated from the joint estimates (right). In the figure above, the kinematic hand model could be used to segment the hand by assigning points in the point cloud to the nearest joint as estimated by the model.\nOne way to visualize the cluster relationships is a dendrogram. Initially, each element is its own cluster. As the process evolves and clusters are merged based on some similarity, the hierarchy is updated to show how the connections are formed.\nFigure 4: Example output from scikit-image. K-Means Clustering K-Means Variant KNN Demo https://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html#sphx-glr-auto-examples-cluster-plot-color-quantization-py K-Means clustering is a popular machine learning method used in both supervised and unsupervised settings. It works by iteratively updating a set of centroids or means until some stopping criteria is achieved.\nTo use this with image segmentation, we start by treating our image features as vectors. In the RGB case, each pixel is a vector of 3 values. It starts out by initializing \\(k\\) clusters randomly with means \\(\\mathbf{m}_i\\). The next step is to compute the distance between the clusters and each point in the image. Points are assigned to the cluster that is closest.\n\\[ \\text{arg}\\min_{C} \\sum_{i=1}^k \\sum_{\\mathbf{z}\\in C_i}\\|\\mathbf{z} - \\mathbf{m}_i\\|^2, \\]\nwhere \\(C = \\{C_1, \\dots, C_k\\}\\) is the cluster set.\nK-Means uses Expectation Maximization to update its parameters. That is, it first computes the expected values given its current cluster centers before updating the cluster centers based on the new assignments. The standard algorithm is as follows:\nInitialize clusters - Randomly select \\(k\\) points as cluster centers \\(\\mathbf{m}_i\\). Assign samples to clusters - Assign each sample to the closest cluster center based on some distance metric. Update the means - Compute a new value for the cluster centers based on the assignments in the previous step. \\[ \\mathbf{m}_i = \\frac{1}{|C_i|}\\sum_{\\mathbf{z} \\in C_i}\\mathbf{z}, \\quad i = 1, \\dots, k \\] Test for convergence - Compute the distances between the means at time \\(t\\) and time \\(t - 1\\) as \\(E\\). Stop if the difference is less than some threshold: \\(E \\leq T\\). Figure 5: Image segmented using k-means with k=3. Source: Pearson Education, Inc. Simple Linear Iterative Clustering (SLIC) Simple Linear Iterative Clustering (SLIC) is widely used algorithm based on K-Means clustering for image segmentation \u0026lt;\u0026amp;achantaSLICSuperpixelsCompared2012\u0026gt;.\nAs discussed in the original paper, the authors state that SLIC h as two main advantages over traditional K-Means:\nThe search space for assigning points is reduced, leading to an increase in performance. By weighting the distance measure, color and spatial proximity are both considered when forming clusters. The algorithm itself is simple to understand and implement, as seen below.\nFigure 6: SLIC Algorithm (Achanta et al.) Initialization To keep the search space smaller, the individual search regions are spaced \\(S = \\sqrt{N/k}\\) pixels apart, where \\(N\\) is the number of pixels and \\(k\\) is the number of cluster centers.\nThe image itself is represented in CIELAB color space. This color space was chosen because it is perceputally uniform. That is, it is useful for detecting small differences in color.\nEach of the \\(k\\) pixel clusters is then defined as a superpixel consisting of the CIELAB color and position:\n\\[ C_i = [l_i\\ a_i\\ b_i\\ x_i\\ y_i]^T. \\]\nFor stability, the seed locations are moved to the lowest gradient position in a \\(3 \\times 3\\) neighborhood. If the superpixels are building locally distinct regions, it is better to avoid placing them on an edge (boundary) pixel.\nSearch Space and Distance The search space for a cluster center is a region \\(2S \\times 2S\\) around the cluster. Each pixel in this region is compared to the cluster center \\(C_k\\) using a distance measure \\(D\\).\nThe distance measure should consider both the spatial and color distances:\n\\begin{align*} d_c \u0026amp;= \\sqrt{(l_j - l_i)^2 + (a_j - a_i)^2 + (b_j - b_i)^2}\\\\ d_s \u0026amp;= \\sqrt{(x_j - x_i)^2 + (y_j - y_i)^2}\\\\ D\u0026rsquo; \u0026amp;= \\sqrt{\\Big(\\frac{d_c}{N_c}\\Big)^2 + \\Big(\\frac{d_s}{N_s}\\Big)^2} \\end{align*}\nThe individual distances should be normalized by their respective maximums since the range of CIELAB values is different from the variable maximum of \\(N_s\\), which is based on the image size. Here, \\(N_s\\) corresponds to the sampling size \\(\\sqrt{N/k}\\).\nThe authors found that normalizing this way was inconsistent since the color distances vary greatly from cluster to cluster. They turn this normalization into a hyperparameter constant \\(m\\) so that the user can control the importance between spatial and color proximity.\n\\[ D = \\sqrt{d_c^2 + \\Big(\\frac{d_s}{S}\\Big)^2 m^2} \\]\nA smaller \\(m\\) results in superpixels that adhere more to image boundaries, where a larger value promotes compact superpixels.\nResults Figure 7: Comparison of SLIC against other superpixel methods (Achanta et al.) Figure 8: Images segmented using a varying number of clusters (Achanta et al.) Superpixels in Recent Work Superpixels are useful for reducing the dimensionality of the feature space. Their applications include tracking, segmentation, and object detection. Methods that extract superpixels do not work out of the box with deep learning methods due to their non-differentiable formulation. Deep learning methods rely on gradient descent to optimize their parameters. This requires that the functions used in a deep network be differentiable.\nFigure 9: Superpixels optimized for semantic segmentation (Jampani et al.) Superpixel Sampling Networks, proposed by Jampani et al., introduce the first attempt at integrating superpixel extraction methods with deep learning models \u0026lt;\u0026amp;jampaniSuperpixelSamplingNetworks2018\u0026gt;. In this work, they adapt SLIC as a differentiable layer in a deep network which result in superpixels that are fine-tuned for specific tasks.\nFigure 10: Model diagram for SSN (Jampani et al.) The train their model on a semantic segmentation task which fine tunes the learned superpixels such that they adhere more closely to segmentation boundaries.\nFigure 11: Results on semantic segmentation (Jampani et al.) In a more recent work, Yang et al. propose a deep network that directly produces the superpixels as opposed to using a soft K-Means layer \u0026lt;\u0026amp;yangSuperpixelSegmentationFully2020\u0026gt;.\nFigure 12: Model comparison between Jampani et al. and Yang et al. (Yang et al.) Similar to SSN, they experiment on the Berkeley Image Segmentation Dataset. Their results are competitive with other deep learning-based approaches. The authors note that their method generalizes better in segmentation tasks by being robust to fine details and noise. Additionally, their model runs at 50 fps using 4 NVIDIA Titan Xp GPUs.\nFigure 13: Comparison of results on competing methods (Yang et al.) #print_bibliography: t\n","date":1645682400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645682400,"objectID":"be6e8c17fb4689ca9710b854eb6f0f7c","permalink":"https://ajdillhoff.github.io/notes/segmentation_via_clustering-2/","publishdate":"2022-02-24T00:00:00-06:00","relpermalink":"/notes/segmentation_via_clustering-2/","section":"notes","summary":"Table of Contents Introduction Agglomerative Clustering K-Means Clustering Simple Linear Iterative Clustering (SLIC) Superpixels in Recent Work Introduction The goal of segmentation is fairly broad: group visual elements together. For any given task, the question is how are elements grouped? At the smallest level of an image, pixels can be grouped by color, intensity, or spatial proximity. Without a model of higher level objects, the pixel-based approach will break down at a large enough scale.","tags":["gpgpu"],"title":"Segmentation via Clustering","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Agglomerative Clustering K-Means Clustering Simple Linear Iterative Clustering (SLIC) Superpixels in Recent Work Introduction The goal of segmentation is fairly broad: group visual elements together. For any given task, the question is how are elements grouped? At the smallest level of an image, pixels can be grouped by color, intensity, or spatial proximity. Without a model of higher level objects, the pixel-based approach will break down at a large enough scale.\nSegmentation by thresholding works in cases where the boundaries between features are clearly defined. However, thresholding is not very robust to complex images with noise. Consider a simple image and its intensity histogram as noise is added.\nFigure 1: From left to right, a noiseless image with increasing amounts of Gaussian noise added. Source: Pearson Education, Inc. Even with some noise added, as seen in the middle image, thresholding is still relatively straightforward. Once enough noise is added, thresholding via pixel intensities will not work. A more sophisticated approach is needed in this case.\nClustering is a fairly intuitive way to think about segmentation. Instead of a fine-grained representation of an image as a collection of pixels, it is represented as groups or clusters that share some common features. The general process of clustering is simple. The image is represented as a collection of feature vectors (intensity, pixel color, etc.). Feature vectors are assigned to a single cluster. These clusters represent some segment of the image.\nWhen it comes to clustering methods, there are two main approaches: agglomerative and divisive. Simply, one is a bottom-up approach. The other is a top-down approach. After briefly introductin agglomerative clustering, we will explore specific implementations of segmentation using k-means clustering as well as segmentation using superpixels (Achanta et al. 2012).\nAgglomerative Clustering Agglomerative clustering methods start by assuming every element is a separate cluster. Elements are formed based on some local similarities. As these methods iterate, the number of clusters decreases. Deciding which elements to merge depends on inter-cluster distance. The exact choice of distance is dependent on the task. Some examples include:\nSingle-link clustering: The distance between the closest elements. Complete-link clustering: The maximum distance between an element of the first cluster and one of the second. Group average clustering: Average distance of elements in a cluster. How many clusters are or should be in a single image?\nThis is a difficult question to answer for many reasons. The answer will be largely dependent on the task at hand. It is a problem of learning the underlying generative process of the visual elements in the image. By defining the specific goal of segmentation (segment by color, shape, etc.), we are introducing a prior about the underlying generative processes which formed the image.\nFigure 2: 3D-PointCapsNet learns point segmentations on only 1% of the training data (Zhao et al.). There are approaches which attempt to segment objects in semi-supervised settings. As seen in Figure 1, Zhao et al. propose a part segmentation model for 3D objects which only utilizes 1-5% of the training part labels (Zhao et al. 2019).\nFor example, if we divised an algorithm that would segment an image by color values, it might be able to segment the hand wearing a solid color glove relatively easily. If we wanted to segment the hand into its individual joints, we would have to introduce a visual prior such as asking the subject to wear a multicolored glove. We could also add prior information about the hand shape and joint configuration into the model itself.\nFigure 3: An image-based joint regression model predicts joint locations (left) along with a point cloud generated from the joint estimates (right). In the figure above, the kinematic hand model could be used to segment the hand by assigning points in the point cloud to the nearest joint as estimated by the model.\nOne way to visualize the cluster relationships is a dendrogram. Initially, each element is its own cluster. As the process evolves and clusters are merged based on some similarity, the hierarchy is updated to show how the connections are formed.\nFigure 4: Example output from scikit-image. K-Means Clustering K-Means Variant KNN Demo https://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html#sphx-glr-auto-examples-cluster-plot-color-quantization-py K-Means clustering is a popular machine learning method used in both supervised and unsupervised settings. It works by iteratively updating a set of centroids or means until some stopping criteria is achieved.\nTo use this with image segmentation, we start by treating our image features as vectors. In the RGB case, each pixel is a vector of 3 values. It starts out by initializing \\(k\\) clusters randomly with means \\(\\mathbf{m}_i\\). The next step is to compute the distance between the clusters and each point in the image. Points are assigned to the cluster that is closest.\n\\[ \\text{arg}\\min_{C} \\sum_{i=1}^k \\sum_{\\mathbf{z}\\in C_i}\\|\\mathbf{z} - \\mathbf{m}_i\\|^2, \\]\nwhere \\(C = \\{C_1, \\dots, C_k\\}\\) is the cluster set.\nK-Means uses Expectation Maximization to update its parameters. That is, it first computes the expected values given its current cluster centers before updating the cluster centers based on the new assignments. The standard algorithm is as follows:\nInitialize clusters - Randomly select \\(k\\) points as cluster centers \\(\\mathbf{m}_i\\). Assign samples to clusters - Assign each sample to the closest cluster center based on some distance metric. Update the means - Compute a new value for the cluster centers based on the assignments in the previous step. \\[ \\mathbf{m}_i = \\frac{1}{|C_i|}\\sum_{\\mathbf{z} \\in C_i}\\mathbf{z}, \\quad i = 1, \\dots, k \\] Test for convergence - Compute the distances between the means at time \\(t\\) and time \\(t - 1\\) as \\(E\\). Stop if the difference is less than some threshold: \\(E \\leq T\\). Figure 5: Image segmented using k-means with k=3. Source: Pearson Education, Inc. Simple Linear Iterative Clustering (SLIC) Simple Linear Iterative Clustering (SLIC) is widely used algorithm based on K-Means clustering for image segmentation (Achanta et al. 2012).\nAs discussed in the original paper, the authors state that SLIC h as two main advantages over traditional K-Means:\nThe search space for assigning points is reduced, leading to an increase in performance. By weighting the distance measure, color and spatial proximity are both considered when forming clusters. The algorithm itself is simple to understand and implement, as seen below.\nFigure 6: SLIC Algorithm (Achanta et al.) Initialization To keep the search space smaller, the individual search regions are spaced \\(S = \\sqrt{N/k}\\) pixels apart, where \\(N\\) is the number of pixels and \\(k\\) is the number of cluster centers.\nThe image itself is represented in CIELAB color space. This color space was chosen because it is perceputally uniform. That is, it is useful for detecting small differences in color.\nEach of the \\(k\\) pixel clusters is then defined as a superpixel consisting of the CIELAB color and position:\n\\[ C_i = [l_i\\ a_i\\ b_i\\ x_i\\ y_i]^T. \\]\nFor stability, the seed locations are moved to the lowest gradient position in a \\(3 \\times 3\\) neighborhood. If the superpixels are building locally distinct regions, it is better to avoid placing them on an edge (boundary) pixel.\nSearch Space and Distance The search space for a cluster center is a region \\(2S \\times 2S\\) around the cluster. Each pixel in this region is compared to the cluster center \\(C_k\\) using a distance measure \\(D\\).\nThe distance measure should consider both the spatial and color distances:\n\\begin{align*} d_c \u0026amp;= \\sqrt{(l_j - l_i)^2 + (a_j - a_i)^2 + (b_j - b_i)^2}\\\\ d_s \u0026amp;= \\sqrt{(x_j - x_i)^2 + (y_j - y_i)^2}\\\\ D\u0026rsquo; \u0026amp;= \\sqrt{\\Big(\\frac{d_c}{N_c}\\Big)^2 + \\Big(\\frac{d_s}{N_s}\\Big)^2} \\end{align*}\nThe individual distances should be normalized by their respective maximums since the range of CIELAB values is different from the variable maximum of \\(N_s\\), which is based on the image size. Here, \\(N_s\\) corresponds to the sampling size \\(\\sqrt{N/k}\\).\nThe authors found that normalizing this way was inconsistent since the color distances vary greatly from cluster to cluster. They turn this normalization into a hyperparameter constant \\(m\\) so that the user can control the importance between spatial and color proximity.\n\\[ D = \\sqrt{d_c^2 + \\Big(\\frac{d_s}{S}\\Big)^2 m^2} \\]\nA smaller \\(m\\) results in superpixels that adhere more to image boundaries, where a larger value promotes compact superpixels.\nResults Figure 7: Comparison of SLIC against other superpixel methods (Achanta et al.) Figure 8: Images segmented using a varying number of clusters (Achanta et al.) Superpixels in Recent Work Superpixels are useful for reducing the dimensionality of the feature space. Their applications include tracking, segmentation, and object detection. Methods that extract superpixels do not work out of the box with deep learning methods due to their non-differentiable formulation. Deep learning methods rely on gradient descent to optimize their parameters. This requires that the functions used in a deep network be differentiable.\nFigure 9: Superpixels optimized for semantic segmentation (Jampani et al.) Superpixel Sampling Networks, proposed by Jampani et al., introduce the first attempt at integrating superpixel extraction methods with deep learning models (Jampani et al. 2018). In this work, they adapt SLIC as a differentiable layer in a deep network which result in superpixels that are fine-tuned for specific tasks.\nFigure 10: Model diagram for SSN (Jampani et al.) The train their model on a semantic segmentation task which fine tunes the learned superpixels such that they adhere more closely to segmentation boundaries.\nFigure 11: Results on semantic segmentation (Jampani et al.) In a more recent work, Yang et al. propose a deep network that directly produces the superpixels as opposed to using a soft K-Means layer (Yang et al. 2020).\nFigure 12: Model comparison between Jampani et al. and Yang et al. (Yang et al.) Similar to SSN, they experiment on the Berkeley Image Segmentation Dataset. Their results are competitive with other deep learning-based approaches. The authors note that their method generalizes better in segmentation tasks by being robust to fine details and noise. Additionally, their model runs at 50 fps using 4 NVIDIA Titan Xp GPUs.\nFigure 13: Comparison of results on competing methods (Yang et al.) #print_bibliography: t\n","date":1645682400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645682400,"objectID":"bd891ab8e52cb43a9c10d142e3564982","permalink":"https://ajdillhoff.github.io/notes/segmentation_via_clustering/","publishdate":"2022-02-24T00:00:00-06:00","relpermalink":"/notes/segmentation_via_clustering/","section":"notes","summary":"Table of Contents Introduction Agglomerative Clustering K-Means Clustering Simple Linear Iterative Clustering (SLIC) Superpixels in Recent Work Introduction The goal of segmentation is fairly broad: group visual elements together. For any given task, the question is how are elements grouped? At the smallest level of an image, pixels can be grouped by color, intensity, or spatial proximity. Without a model of higher level objects, the pixel-based approach will break down at a large enough scale.","tags":["computer vision"],"title":"Segmentation via Clustering","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Resources Introduction Parametric Representation Motivation of the Fundamental Snake Equation External Force Energy Minimization Iterative Solution Applications Resources http://www.cs.ait.ac.th/~mdailey/cvreadings/Kass-Snakes.pdf\nhttps://www.spiedigitallibrary.org/conference-proceedings-of-spie/4322/0000/Statistical-models-of-appearance-for-medical-image-analysis-and-computer/10.1117/12.431093.pdf\nhttps://web.mat.upc.edu/toni.susin/files/SnakesAivru86c.pdf\nIntroduction Snakes, as named by Kass et al., is a spline curve that is minimized such that it moves towards distinct image features such as edges. The closed curve, or snake, can be thought of as a rubber band.\nFigure 1: Example of snake snapping to object. (Copyright 2018, 2008 Pearson Education, Inc.) When stretched out, the band has an internal potential energy that forces the band to close in around some rigid object which exerts force against the band\u0026rsquo;s internal energy. This method does not tout itself it be a fully autonomous way to segment interesting features. Instead, it is useful in semi-supervised settings where the user knows a general region of interest. The minimization of the snake will segment the desired object under reasonable settings.\nParametric Representation The contour is represented as a curve using a parametric representation:\n\\[ (x, y) = (g(s), h(s)). \\]\nThe argument \\(s\\) can be thought of as the trajectory of the curve.\nParametric representations are a natural choice for representing curves in computing due to their compact representation. For example, a circle is defined by \\(x^2 + y^2 = r^2\\). The individual values \\(x\\) and \\(y\\) are\n\\begin{align*} x \u0026amp;= r\\cos(s)\\\\ y \u0026amp;= r\\sin(s) \\end{align*}\nIt can then be shown that\n\\begin{align*} x^2 + y^2 \u0026amp;= r^2\\cos^2(s) + r^2\\sin^2(s)\\\\ \u0026amp;= r^2\\big(\\cos^2(s) + \\sin^2(s)\\big)\\\\ \u0026amp;= r^2. \\end{align*}\nAs a vector, we can represent \\((x, y)\\) as\n\\begin{equation*} \\mathbf{c}= \\begin{bmatrix} x\\\\ y \\end{bmatrix}= \\begin{bmatrix} x\\\\ \\pm \\sqrt{r^2 - x^2} \\end{bmatrix}. \\end{equation*}\nUsing the more efficient parametric representation, this vector is defined as\n\\begin{equation*} \\mathbf{c}= \\begin{bmatrix} r\\cos(s)\\\\ r\\sin(s) \\end{bmatrix}. \\end{equation*}\nAnother example using parametric representation is that of spline curves.\n\\begin{align*} x(t) \u0026amp;= a_xt^3 + b_xt^2 + c_xt + dx\\\\ y(t) \u0026amp;= a_yt^3 + b_yt^2 + c_yt + dy \\end{align*}\nThen, \\(\\mathbf{f}(t) = (x(t), y(t))\\).\nMotivation of the Fundamental Snake Equation Given a vector \\(\\mathbf{c}(s) = \\big(x(s), y(s)\\big)\\) normalized such that \\(0 \\leq s \\leq 1\\), an energy function is defined based on internal and external forces:\n\\[ E(\\mathbf{c}) = E_{int} + E_{ext}. \\]\nAs the snake is updated iteratively, its final position should be one such that the energy \\(E\\) is minimized.\nThe internal energy function is given as\n\\[ E_{int} = \\frac{\\alpha}{2}\\|\\mathbf{c}\u0026rsquo;(s)\\|^2 + \\frac{\\beta}{2}\\|\\mathbf{c}\u0026rsquo;\u0026rsquo;(s)\\|^2, \\]\nwhere the first-order term is controlled by \\(\\alpha\\) and the second-order term controlled by \\(\\beta\\). The first-order term gives the snake an elastic quality that shrinks towards a rigid object. The second-order term controls the siffness of the contour.\nExternal Force The external force \\(E_{ext}\\) is based on the magnitude of the image gradient:\n\\[ E_{img}(x, y) = \\|\\nabla f(x, y)\\|^2. \\]\nAdditionally, the gradient vectors are recorded. The combination of the two serve to represent a force field of the edge map.\nFigure 2: Force field using the edge map using normalized gradients. Source: Pearson Education, Inc. Energy Minimization The total energy of the snake is then\n\\[ E(\\mathbf{c}(s)) = \\int_0^1 \\frac{\\alpha}{2}\\|\\mathbf{c}\u0026rsquo;(s)\\|^2 ds + \\int_0^1 \\frac{\\beta}{2}\\|\\mathbf{c}\u0026rsquo;\u0026rsquo;(s)\\|^2 ds + \\int_0^1 E_{img}(\\mathbf{c}(s))ds. \\]\nTo find the minimum energy, we write the above equation as a function \\(F\\) and take its derivative with respect to s. The minimum energy must satisfy\n\\[ \\frac{\\partial}{\\partial s}\\Big(\\frac{\\partial F}{\\partial\\mathbf{c}\u0026rsquo;}\\Big) - \\frac{\\partial^2}{\\partial s^2} \\Big(\\frac{\\partial F}{\\partial \\mathbf{c}\u0026rsquo;\u0026rsquo;}\\Big) - \\frac{\\partial F}{\\partial \\mathbf{c}} = 0. \\]\nSolving for the partials above yields\n\\[ \\alpha \\mathbf{c}\u0026rsquo;\u0026rsquo; - \\beta \\mathbf{c}\u0026rsquo;\u0026rsquo;\u0026rsquo;\u0026rsquo; - \\nabla E_{img} = 0. \\]\nSince the derivative of energy is a force, and the external force of the object is against the internal force of the snake, we can write\n\\[ \\nabla E_{img} = -\\mathbf{F}. \\]\nUnder this perspective, the minimum energy is found when\n\\[ \\alpha \\mathbf{c}\u0026rsquo;\u0026rsquo;(s) - \\beta \\mathbf{c}\u0026rsquo;\u0026rsquo;\u0026rsquo;\u0026rsquo;(s) + \\mathbf{F}(\\mathbf{c}(s)) = 0. \\]\nIterative Solution Figure 3: From top left to bottom right: initial snake, 10 steps, 50, 100, 150, 200. Source: Pearson Education, Inc. To solve this as an iterative process over time \\(t\\), we write the force vector \\(\\mathbf{F}\\) in terms of its 2D components dependent on \\(t\\):\n\\[ \\mathbf{F}(\\mathbf{c}(s, t)) = \\mathbf{F}(x(s, t), y(s, t)) = \\begin{bmatrix} F_x (x(s, t), y(s, t))\\\\ F_y (x(s, t), y(s, t)) \\end{bmatrix}. \\]\nFor the internal energy components, we take the partial derivative of \\(\\mathbf{c}\\) with respect to time:\n\\[ \\frac{\\partial \\mathbf{c}(s, t)}{\\partial t} = \\begin{bmatrix} \\frac{\\partial x(s, t)}{\\partial t}\\\\ \\frac{\\partial y(s, t)}{\\partial t}\\\\ \\end{bmatrix}. \\]\nThese derivatives rely on second and fourth order derivatives. For \\(x(s, t)\\), this is\n\\[ \\frac{\\partial x(s, t)}{\\partial t} = \\alpha \\frac{\\partial^2 x(s, t)}{\\partial s^2} - \\beta \\frac{\\partial^4 x(s, t)}{\\partial s^4} + F_x(x(s, t), y(s, t)). \\]\nThe partial for \\(y\\) follows a similar formulation. These derivatives are approximated using finite differences. The second order derivative is approximated as\n\\[ \\frac{\\partial^2 x(s, t)}{\\partial s^2} = x\u0026rsquo;\u0026rsquo;(k, t) = x(k + 1, t) - 2x(k, t) + x(k-1, t), \\]\nand the fourth order derivative is approximated as\n\\[ \\frac{\\partial^4 x(s, t)}{\\partial s^4} = x\u0026rsquo;\u0026rsquo;\u0026rsquo;\u0026rsquo;(k, t) = x(k + 2, t) - 4x(k + 1, t) + 6x(k, t) - 4x(k-1, t) + x(k-2, t). \\]\nThe finite differences can be written in matrix form as a pentadiagonal banded matrix. Adding back the external force yields a much simpler equation:\n\\[ Dx + F_x(\\mathbf{x}(t), \\mathbf{y}(t)) = 0, \\]\nwhere\n\\[ D = \\alpha D_2 - \\beta D_4, \\]\nthe matrix of finite differences.\nSolving the above equation involves taking a finite step in time multiplied by the product of the negative time derivatives. To simplify this process further, an assumption is made that the external force remains constant over time.\n\\[ D\\mathbf{x}(t) + \\mathbf{F}_x(\\mathbf{x}(t-1, \\mathbf{y}(y-1)) = -\\gamma(\\mathbf{x}(t) - \\mathbf{x}(t-1)) \\]\nThis is solved using matrix inversion yielding the final update for time \\(t\\) as\n\\[ \\mathbf{x}(t) = A[\\mathbf{x}(t-1) + \\gamma \\mathbf{F}_x(\\mathbf{x}(t-1), \\mathbf{y}(t-1))], \\]\nwhere \\(A = [I - D]^{-1}\\).\nThe step for \\(\\mathbf{y}(t)\\) follows a similar formulation.\nFigure 4: Snake transition between time steps. Source: Pearson Education, Inc. Applications In the original paper, the authors show an application where an initial frame of video is initialize with a snak by hand to track the contours of a mouth. From that point, the shape automatically matches the subsequent frames.\nFigure 5: Snakes for motion tracking (Kass et al.) ","date":1645509600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645509600,"objectID":"295ed6bcc388ef1c9e91bbd786ee0d20","permalink":"https://ajdillhoff.github.io/notes/active_contours/","publishdate":"2022-02-22T00:00:00-06:00","relpermalink":"/notes/active_contours/","section":"notes","summary":"Table of Contents Resources Introduction Parametric Representation Motivation of the Fundamental Snake Equation External Force Energy Minimization Iterative Solution Applications Resources http://www.cs.ait.ac.th/~mdailey/cvreadings/Kass-Snakes.pdf\nhttps://www.spiedigitallibrary.org/conference-proceedings-of-spie/4322/0000/Statistical-models-of-appearance-for-medical-image-analysis-and-computer/10.1117/12.431093.pdf\nhttps://web.mat.upc.edu/toni.susin/files/SnakesAivru86c.pdf\nIntroduction Snakes, as named by Kass et al., is a spline curve that is minimized such that it moves towards distinct image features such as edges. The closed curve, or snake, can be thought of as a rubber band.\nFigure 1: Example of snake snapping to object.","tags":["gpgpu"],"title":"Active Contours","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction The Markov Assumption Definition Evaluation The Viterbi Algorithm Estimating Parameters Expectation Maximization Introduction This article is essentially a grok of a tutorial on HMMs by (RABINER 1989). It will be useful for the reader to reference the original paper.\nUp to this point, we have only explored \u0026ldquo;atomic\u0026rdquo; data points. That is, all of the information about a particular sample is encapsulated into one vector. Sequential data is easily represented by graphical models. This article introduces Hidden Markov Models, a powerful probabilistic graphical model used in many applications from gesture recognition to natural language processing.\nThere are many tasks for which we do not know the underlying process. However, we can observe samples that are produced from such processes. Music, gesture recognition, speech, text, etc. All of these have some underlying process which forms their outputs together into a hopefully coherent sequence. If we wish to make predictions about future samples given these sequences, we will need to make some guess about the underlying processes defining their output.\nThe Markov Assumption Markov models make a convenient assumption about sequential data. That is, all relevant information required for predicting future samples is captured in the current time step \\(t\\). Given a joint distribution over an input of \\(T\\) frames, \\(p(\\mathbf{x}_{1:T})\\), the Markov assumption allows us to represent it as\n\\[ p(\\mathbf{x}_{1:T}) = p(\\mathbf{x}_1)\\prod_{t=2}^T p(\\mathbf{x}_t|\\mathbf{x}_{t-1}) \\]\nDefinition A more complicated case is when we are attempting to model some unknown process that is responsible for the observations. In this case, an ordinary Markov chain is not sufficient. A hidden Markov model (HMM) is defined by a set \\(z_t \\in \\{1, \\dots, K\\}\\) of discrete hidden states and an observation model \\(p(\\mathbf{x}_i|z_t)\\). The joint probability distribution of this model is given by\n\\[ p(\\mathbf{z}, \\mathbf{x}) = p(\\mathbf{z})p(\\mathbf{x}|\\mathbf{z}) = \\Big(p(z_1)\\prod_{t=2}^Tp(z_t|z_{t-1})\\Big)\\Big(\\prod_{t=1}^Tp(\\mathbf{x}_t|z_t)\\Big). \\]\nFigure 1: The observations y are generated by the latent states x. Source: Wikipedia Although the states themselves are discrete, the observations may be continuous: \\(p(\\mathbf{x}|z_t, \\mathbf{\\theta})\\). If they are discrete, they can be modeled by an observation matrix \\(B\\). Continuous observations are typically modeled using a conditional Gaussian:\n\\[ p(\\mathbf{x}_t|z_t=k, \\theta) = \\mathcal{N}(\\mathbf{x}_t|\\mathbf{\\mu}_k,\\mathbf{\\Sigma}_k). \\]\nFollowing Rabiner, an HMM can be characterized by\nThe number of states in the model \\(N\\). The number of distinct observation symbols per state \\(M\\). The state probability distribution \\(A = \\{a_{ij}\\}\\), \\(a_{ij} = p(z_t=j | z_{t-1} = i)\\). The observation symbol probability distribution \\(B = \\{b_j(k)\\} = p(\\mathbf{x}_t = k|z_t = j)\\). An initial state distribution \\(\\mathbf{\\pi}_i = p(z_t = i)\\). Figure 2: HMM with observation probabilities and state transition probabilities. Source: Wikipedia The observation probability distribution is commonly modeled as a Gaussian, Mixture of Gaussians, or Multinomial distribution. Thus, the parameter estimates for those distributions follow the likelihood estimates for each respective distribution.\nIn his famous tutorial on HMMs, Rabiner addressed the three fundamental problems of HMMs:\nGiven an observation sequence and model parameters, how do we compute the probability of the observation sequence given the parameters (likelihood)? Given an observation sequence and model parameters, how do we choose a state sequence which is optimal (decoding)? How do we adjust the model parameters (learning)? HMMs are able to solve several different inference problems.\nFiltering computes \\(p(z_t | \\mathbf{x}_{1:t})\\). That is, we are computing this probability as new samples come in up to time \\(t\\). Smoothing is accomplished when we have all the data in the sequence. This is expressed as \\(p(z_t|\\mathbf{x}_{1:T})\\). Fixed lag smoothing allows for a trade off between accuracy and delay. It is useful in cases where we might not have the full sequence, but we wish to compute \\(p(z_{t-l}|\\mathbf{x}_{1:t})\\) for some \\(l \u0026gt; 0\\). Predictions are represented as \\(p(z_{t+h}|\\mathbf{x}_{1:t})\\), where \\(h \u0026gt; 0\\). MAP estimation yields the most probably state sequence \\(\\text{arg}\\max_{\\mathbf{z}_{1:T}}p(\\mathbf{z}_{1:T}|\\mathbf{x}_{1:T})\\). We can sample the posterior \\(p(\\mathbf{z}_{1:T}|\\mathbf{x}_{1:T})\\). We can also compute \\(p(\\mathbf{x}_{1:T})\\) by summing up over all hidden paths. This is useful for classification tasks. Evaluation We start by solving the first problem posited by (RABINER 1989).\nGiven an observation sequence and model parameters, how do we compute the probability of the observation sequence given the parameters? That is, given some model parameters \\(\\lambda = (A, B, \\pi)\\), compute \\(p(z_t|\\mathbf{x}_{1:t})\\).\nForwards Pass The forwards algorithm solves two problems of interest. First, we want to know how well our current parameters explain the observation sequence. That is, \\(p(\\mathbf{x}_{1:T}|\\lambda)\\).\nSecond, we want to compute \\(p(z_t | \\mathbf{x}_{1:t})\\). To compute these in an efficient way, a recursive strategy is adopted. Let the forward variable \\(\\alpha_t(i)\\) be defined as\n\\[ \\alpha_t(i) = p(\\mathbf{x}_{1:t}, z_t = i | \\lambda). \\]\nThe forwards algorithm is defined as 3 steps.\nInitialization:\n\\[ \\alpha_1(i) = \\pi_i b_i(\\mathbf{x}_1),\\quad 1 \\leq i \\leq N. \\]\nRecursion:\n\\[ \\alpha_{t+1}(j) = \\Big(\\sum_{i=1}^N \\alpha_t(i)a_{ij}\\Big)b_j(\\mathbf{x}_{t+1}),\\quad 1 \\leq t \\leq T - 1,\\quad 1 \\leq j \\leq N \\]\nTermination:\n\\[ p(\\mathbf{x}_{1:T}) = \\sum_{i=1}^N \\alpha_T(i). \\]\nThe recursive step is visualized as a lattice structure as seen below.\nFigure 3: From Rabiner 1989. With this step, we have a solution for the first problem. We can now calculate more efficiently the probability of our observations given the current model parameters. This along with the following backwards pass will be essential for updating our model parameters.\nThe forwards algorithm is also used to solve the filtering problem. To see how, consider \\(p(z_t | \\mathbf{x}_{1:t-1})\\) right before time \\(t\\).\n\\begin{equation*} p(z_t=j|\\mathbf{x}_{1:t-1}) = \\sum_i p(z_t=j|z_{t-1}=i)p(z_{t-1}=i|\\mathbf{x}_{1:t-1}) \\end{equation*}\nWhen we update for time \\(t\\), we have that\n\\begin{align*} p(z_t=j|\\mathbf{x}_{1:t}) \u0026amp;= p(z_t=j|\\mathbf{x}_t, \\mathbf{x}_{1:t})\\\\ \u0026amp;=\\frac{p(\\mathbf{x}_t|z_t=j, \\mathbf{x}_{1:t-1})p(z_t=j|\\mathbf{x}_{1:t-1})}{p(\\mathbf{x}_t|\\mathbf{x}_{t-1})} \\end{align*}\nHowever, \\(\\mathbf{x}_{1:t-1}\\) is conditionally independent given \\(z_t\\), so it becomes\n\\begin{equation*} p(z_t=j|\\mathbf{x}_{1:t})=\\frac{p(\\mathbf{x}_t|z_t=j)p(z_t=j|\\mathbf{x}_{1:t-1})}{p(\\mathbf{x}_t|\\mathbf{x}_{t-1})}. \\end{equation*}\nWriting out \\(p(z_t=j|\\mathbf{x}_{1:t-1})\\) fully yields\n\\begin{equation*} p(z_t=j|\\mathbf{x}_{1:t}) \\propto p(\\mathbf{x}_t|z_t=j)\\sum_i p(z_t=j|z_{t-1}=i)p(z_{t-1}=i|\\mathbf{x}_{1:t-1}). \\end{equation*}\nThis is the recursion step from above!\nThis can also be represented in terms of the \\(\\alpha\\) variables from above. To compute \\(p(z_t=i|\\mathbf{x}_{1:t})\\), we can use the definition of a conditional probability distribution:\n\\begin{align*} p(z_t=i|\\mathbf{x}_{1:t}) \u0026amp;= \\frac{p(z_t=i, \\mathbf{x}_{1:t})}{p(\\mathbf{x}_{1:t})}\\\\ \u0026amp;= \\frac{\\alpha_t(i)}{\\sum_{j=1}^N \\alpha_t(j)} \\end{align*}\nCompared to the complexity of the explicit representation, the forwards pass needs only \\(N^2T\\) calculations. As pointed out in (RABINER 1989), with 5 hidden states and an observation sequence of length 100, the forwards pass only needs around 3000 computations. A direct calculation would require \\(10^{72}\\).\nBackwards Pass When updating the parameters of our model, we will need to consider the entire observation sequence. The forward pass did not require the entire sequence. Instead, we can compute the probability of the observation up to some time \\(t\\). The backwards pass begins by defining the variable\n\\[ \\beta_t(i) = p(\\mathbf{x}_{t+1:T} | z_t = i). \\]\nWe can utilize a recursive process similar to the forwards algorithm with the following steps:\nInitialization:\n\\[ \\beta_T(i) = 1,\\quad 1 \\leq i \\leq N \\]\nRecursion:\n\\[ \\beta_t(i) = \\sum_{j=1}^N a_{ij}b_j(\\mathbf{x}_{t+1})\\beta_{t+1}(j),\\quad t = T-1,\\dots,1,\\quad 1 \\leq i \\leq N. \\]\nTermination:\n\\[ p(\\mathbf{x}_{1:T}) = \\sum_{j=1}^N \\pi_j b_j(x_1) \\beta_1(j) \\]\nThe complexity of the backwards algorithm is similar to that of the forwards: \\(N^2T\\).\nWith both the forward and backwards passes defined, we can compute the smoothing problem:\n\\[ p(z_t=i|\\mathbf{x}_{1:T}) = \\frac{\\alpha_t(i)\\beta_t(i)}{\\sum_{j=1}^N \\alpha_t(j)\\beta_t(j)} \\]\nThe Viterbi Algorithm With problem 1 out of the way, we turn our attention to problem 2.\nGiven an observation sequence and model parameters, how do we choose a state sequence which is optimal? \\[ \\mathbf{z}^* = \\text{arg}\\max_{\\mathbf{z}_{1:T}}p(\\mathbf{z}_{1:T}|\\mathbf{x}_{1:T}) \\]\nWith respect to the lattice diagram, this is equivalent to computing the shortest path. This is accomplished via the Viterbi algorithm, sometimes referred to as the max-sum algorithm. As with the forwards-backwards algorithm, the Viterbi algorithm takes on a recursive approach. It starts by defining an intermediate variable\n\\[ \\gamma_t(i) = p(z_t=i|\\mathbf{x}_{1:T}). \\]\nUsing the variables defined in the forwards-backwards algorithm, this can be expressed as\n\\[ \\gamma_t(i) = \\frac{\\alpha_t(i) \\beta_t(i)}{\\sum_{i=1}^N \\alpha_t(i) \\beta_t(i)}. \\]\nThis \\(\\gamma_t(i)\\), we can compute the most likely state at time \\(t\\):\n\\[ z_t^* = \\text{arg}\\max_{1\\leq i \\leq N} \\gamma_t(i), \\quad 1 \\leq t \\leq T. \\]\nOne problem with this approach alone is that the most likely state at a particular time \\(t\\) may not lead us to the most probable sequence of states. As stated above, we need to maximize \\(p(\\mathbf{z}_{1:T}|\\mathbf{x}_{1:T})\\). In order to tackle this efficiently, Viterbi employs a dynamic programming approach.\nInitialization\nStart with the best initial state out of all states given the observation at \\(t=1\\). Additionally, we want to record the index of each state through time so that the best path can be retraced.\n\\begin{align*} \\delta_1(i) \u0026amp;= \\pi_i b_i(\\mathbf{x}_1),\\quad 1 \\leq i \\leq N\\\\ \\psi_1(i) \u0026amp;= 0 \\end{align*}\nRecursion:\nThe quantity \\(\\delta_t(i)\\) represents the joint probability of state sequences and observations up to time \\(t\\) ending with state \\(z_t=i\\). Thus, the recursive step is to maximize the probability of the intermediate output for \\(t-1\\):\n\\[ \\delta_t(j) = \\max_{1 \\leq i \\leq N} (\\delta_{t-1}(i) a_{ij})b_j(\\mathbf{x}_t), \\quad 2 \\leq t \\leq T,\\quad 1 \\leq j \\leq N. \\]\nThe corresponding index for this step is recorded in the path matrix:\n\\[ \\psi_t(j) = \\text{arg}\\max_{1 \\leq i \\leq N} \\delta_{t-1}(i)a_{ij},\\quad 2 \\leq t \\leq T,\\quad 1 \\leq j \\leq N. \\]\nTermination\nThe last step of the Viterbi algorithm completes the calcuation of the joint probability of state sequences and observations.\n\\[ p^* = \\max_{1 \\leq i \\leq N} \\delta_T(i) \\]\n\\[ \\mathbf{z}_T^* = \\text{arg}\\max_{1 \\leq i \\leq N} \\delta_T(i) \\]\nPath Backtrace\nWith the state sequence matrix recorded along the way, we can retrace it to get the most probable sequence:\n\\[ z_t^* = \\psi_{t+1}(z_{t+1}^*),\\quad t = T-1, \\cdots, 1. \\]\nEstimating Parameters If the hidden states were fully observable, then updating our model parameters would be as straightforward as computing the maximum likelihood estimates for the model parameters \\(\\lambda = (A, B, \\pi)\\). For \\(A\\) and \\(\\pi\\), we first tally up the following counts:\n\\[ \\hat{a}_{ij} = \\frac{N_{ij}}{\\sum_j N_{ij}}, \\]\nthe number of times we expect to transition from \\(i\\) to \\(j\\) divided by the number of times we transition from \\(i\\) to any other state.\nFor \\(\\pi\\), we have\n\\[ \\hat{\\pi_i} = \\frac{N_i}{\\sum_i N_i}, \\]\nThe number of times we expect to start in state \\(i\\) divided by the number of times we start in any other state.\nEstimating the parameters for \\(B\\) depends on which distribution we are using for our observation probabilities. For a multinomial distribution, we would compute the number of times we are in state \\(j\\) and observe a symbol \\(k\\) divided by the number of times we are in state \\(j\\):\n\\[ \\hat{B}_{jk} = \\frac{N_{jk}}{N_k}, \\]\nwhere\n\\[ N_{jk} = \\sum_{i=1}^N \\sum_{t=1}^T \\mathbb{1} (z_{i, t}=j, x_{i, t}=k). \\]\nIf the observation probability follows a Gaussian distribution, the MLEs for \\(\\mu\\) and \\(\\mathbf{\\Sigma}\\) are\n\\[ \\hat{\\mathbf{\\mu}}_k = \\frac{\\bar{\\mathbf{x}}_k}{N_k},\\quad \\hat{\\mathbf{\\Sigma}}_k = \\frac{(\\bar{\\mathbf{x}}\\bar{\\mathbf{x}})_k^T - N_k \\hat{\\mathbf{\\mu}}_k\\hat{\\mathbf{\\mu}}_k^T}{N_k}, \\]\nwhere\n\\[ \\bar{\\mathbf{x}}_k = \\sum_{i=1}^N \\sum_{t=1}^T \\mathbb{1}(z_{i, t}=k)\\mathbf{x}_{i, t} \\]\nand\n\\[ (\\bar{\\mathbf{x}}\\bar{\\mathbf{x}})_k^T) = \\sum_{i=1}^N \\sum_{t=1}^T \\mathbb{1} (z_{i, t}=k)\\mathbf{x}_{i,k}\\mathbf{x}_{i,k}^T. \\]\nExpectation Maximization Of course, HMMs have hidden states which are not fully observable. Thus, we need to come up with another strategy for updating our parameters based on the observable data. The intuition behind this approach is as follows. We first start out by using our current parameters to estimate the missing data, making it complete. Initially, we may randomize our estimates if we have no good heuristic or guess as to what they should be.\nWith the completed data, we can update our current parameters. In other words, the expected values of the sufficient statistics can be derived now that the data has been filled in. A new set of parameters is found such that it maximizes the likelihood function with respect to the estimated data.\nE Step Following (RABINER 1989), we start with the joint probability of being in state \\(i\\) at time \\(t\\) and state \\(j\\) at time \\(t+1\\):\n\\[ \\xi_t(i, j) = p(z_t = i, z_{t+1} = j|\\mathbf{x}_{1:T}). \\]\nThis can be computed using the forwards-backwards algorithm:\n\\[ \\xi_t(i, j) = \\frac{\\alpha_t(i)a_{ij}b_j(\\mathbf{x}_{t+1})\\beta_{t+1}(j)}{\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_t(i)a_{ij}b_j(\\mathbf{x}_{t+1})\\beta_{t+1}(j)}. \\]\nThis can be related back to \\(\\gamma_t(i)\\) by summing over over \\(j\\):\n\\[ \\gamma_t(i) = \\sum_{j=1}^N \\xi_t(i, j). \\]\nHere, \\(\\gamma_t(i)\\) is the expected number of times we transition from \\(z = i\\). Summing over all \\(t\\) yields the expected transitions from \\(z_i\\) over all time steps:\n\\[ \\sum_{t=1}^{T-1} \\gamma_t(i). \\]\nSince \\(\\xi_t(i, j)\\) is the expected transition from \\(i\\) to \\(j\\) at time \\(t\\), we can compute the total number of transitions from \\(i\\) to \\(j\\) via\n\\[ \\sum_{t=1}^{T-1} \\xi_t(i, j). \\]\nM Step The previous E Step computed the expected values given the current parameter estimates. Now that the data is complete, we can update our parameter estimates. Starting with the transition probabilities, we must add the expected number of transitions from \\(i\\) to \\(j\\) and divide by the expected number of times we transition from \\(i\\). Using the parameters from the E Step, this can be written\n\\[ \\hat{a}_{ij} = \\frac{\\sum_{t=1}^{T-1}\\xi_t(i, j)}{\\sum_{t=1}^{T-1}\\gamma_t(i)}. \\]\nThe initial state probability at \\(t=1\\) is the number of times we expect to be in state \\(z=i\\) at \\(t=1\\):\n\\[ \\gamma_1(i). \\]\nFinally, the observation probability parameters are updated by considering the number of times we are in state \\(z=j\\) and observing \\(x=k\\) divided by the number of times we are in state \\(z=j\\). Note that this is for a multinomial probabiliy distribution:\n\\[ \\hat{b}_j(k) = \\frac{\\sum_{t=1, x_t = k}^T \\gamma_t(j)}{\\sum_{t=1}^T \\gamma_t(j)}. \\]\nThese formulas are derived from maximizing Baum\u0026rsquo;s auxiliary function\n\\[ Q(\\lambda, \\hat{\\lambda}) = \\sum_{Q} p(\\mathbf{z}|\\mathbf{x}, \\lambda) \\log p(\\mathbf{x}, \\mathbf{z}|\\hat{\\lambda}) \\]\nover \\(\\hat{\\lambda}\\). It has further been shown that maximizing this function leads to increased likelihood:\n\\[ \\max_{\\hat{\\lambda}} Q(\\lambda, \\hat{\\lambda}) \\implies p(\\mathbf{x}|\\hat{\\lambda}) \\geq p(\\mathbf{x}|\\lambda). \\]\nIf we have a Gaussian observation model, the values for \\(\\hat{b}_j(k)\\) are computed to accommodate the parameters of the distribution. These parameter estimates assume a Gaussian mixture model. Starting with \\(\\hat{\\mu}_{jk}\\), it can be estimated by dividing the expected value of observations belonging to Gaussian density \\(k\\) by the expected number of times we are in state \\(j\\) using the \\(k^{\\text{th}}\\) mixture component:\n\\[ \\hat{\\mathbf{\\mu}}_{jk} = \\frac{\\sum_{t=1}^T \\gamma_t(j, k)\\mathbf{x}_t}{\\sum_{t=1}^T \\gamma_t(j, k)}. \\]\nHere, \\(\\gamma_t(j, k)\\) is the probability of being in state \\(j\\) at time \\(t\\) with the \\(k^{\\text{th}}\\) mixture component accounting for \\(\\mathbf{x}_t\\):\n\\[ \\gamma_t(j, k) = \\frac{\\alpha_t(j)\\beta_t(j)}{\\sum_{j=1}^N \\alpha_t(j) \\beta_t(j)} \\frac{c_{jk}\\mathcal{N}(\\mathbf{x}_t, \\mu_{jk}, \\mathbf{\\Sigma}_{jk})}{\\sum_{m=1}^M c_{jm}\\mathcal{N}(\\mathbf{x}_t, \\mu_{jm}, \\mathbf{\\Sigma}_{jm})}. \\]\nThis method is proven to improve the parameters.\nEach iteration is guaranteed to improve the log-likelihood function. The process is guaranteed to converge. The convergence point is a fixed point of the likelihood function. These guarantees are similar to gradient ascent.\nReferences RABINER, LAWRENCE R. 1989. “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.” Proceedings of the Ieee 77 (2): 30. ","date":1645509600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689379200,"objectID":"24afc41ee651bea868216ff2c5e1f084","permalink":"https://ajdillhoff.github.io/notes/hidden_markov_models/","publishdate":"2022-02-22T00:00:00-06:00","relpermalink":"/notes/hidden_markov_models/","section":"notes","summary":"Table of Contents Introduction The Markov Assumption Definition Evaluation The Viterbi Algorithm Estimating Parameters Expectation Maximization Introduction This article is essentially a grok of a tutorial on HMMs by (RABINER 1989). It will be useful for the reader to reference the original paper.\nUp to this point, we have only explored \u0026ldquo;atomic\u0026rdquo; data points. That is, all of the information about a particular sample is encapsulated into one vector. Sequential data is easily represented by graphical models.","tags":["machine learning","graphical models"],"title":"Hidden Markov Models","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Resources Introduction Gestalt Theory Grouping Segmentation Methods Resources https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/ (Berkeley Segmentation Database) https://arxiv.org/abs/2105.15203v2 (SegFormer) https://arxiv.org/abs/1703.06870 (Mask R-CNN) https://github.com/sithu31296/semantic-segmentation (Collection of SOTA models) Introduction Feature extraction methods such as SIFT provide us with many distinct, low-level features that are useful for providing local descriptions images. We now \u0026ldquo;zoom out\u0026rdquo; and take a slightly higher level look at the next stage of image summarization. Our goal here is to take these low-level features and group, or fit, them together such that they represent a higher level feature. For example, from small patches representing color changes or edges, we may wish to build higher-level feature representing an eye, mouth, and nose.\nFigure 1: Capsule networks learn template components that make up handwritten images (Kosiorek et al.) The goal of image segmentation is to obtain a compact representation of distinct features in an image. We will see that this task is loosely defined and will vary from application to application. For example, in one task we may wish to segment individual people detected in an image. In another task, we may wish to segment the clothes they are wearing to identify certain fashion trends or make predictions about the time of year based on an image.\nFigure 2: Mask R-CNN results on COCO dataset (He et al.) Image segmentation is one of the oldest tasks in computer vision and, as such, encompasses many methods and techniques. The following list of segmentation categories is not exhaustive, but covers many notable approaches.\nRegion: Methods that group by regions and either grow, merge, or spread them fall into this category. A simple region-based approach would be to apply morphological operations on thresholded images. Boundary: Separating parts of the image based on edges, lines, or specific points. Clustering: Methods that utilize some form of clustering such as K-means or Gaussian models to group pixels fall into this category. Watershed: Groups pixels by treating the image as a topographical map. Consider a large pool with 4 local minima such that there are 4 tiny pools inside it. As you fill the entire pool up, eventually the water line from each of the 4 smaller pools will touch, creating a boundary between different regions. Graph-Based: Segmentation algorithms based on graph theory. Mean-Shift: Clustering methods fall into this category. Normalized Cuts: Quantifies the strength of connectedness between groups before separating weak connections. Each of these styles can be used for general segmentation or a well-defined downstream task. General segmentation may be used for unsupservised segmentation of an image for the purpose of creating superpixels (Achanta et al. 2012). A common downstream task is semantic segmentation, where distinct objects in an image are segmented. An even more difficult version of semantic segmentation is instance segmentation where multiple instances of objects are labelled separately.\nFigure 3: Comparison results from SegFormer paper (Xie et al.) Gestalt Theory Gestalt psychology provides a theory of perception emphasizing grouping patterns. Examining different ways to think about grouping sheds light on the open-endedness of segmentation in general.\nGrouping At the pixel-level, features such as edges will look similar regardless of the context. An edge segment from a car will look similar to that of a building at a small enough scale. As soon as those features are grouped together, their representation completely changes. A collection of edges becomes a square or a corner. The higher-level of grouping we have, the more that the collection of features begin to diverge from each other.\nConvNet Gradient Visualization\nConsider the famouse Muller-Lyer illusion seen below.\nFigure 4: From Forsyth and Ponce \u0026ldquo;Computer Vision - A Modern Approach\u0026rdquo; Looking at the details individually, it is easier to see that the lines are the same length. When considering the grouped features around them, they appear different.\nAnother view of segmentation considers what is the figure versus what is the ground, or background.\nFigure 5: From Forsyth and Ponce \u0026ldquo;Computer Vision - A Modern Approach\u0026rdquo; The Gestalt school of psychology posited that grouping was the key to visual understanding. Towards establishing a theory of segmentation, a set of factors was proposed:\nProximity - Group elements that are close together. Similarity - Elements that share some sort of measurable similarity are grouped. Common fate - Often tied to temporal features, elements with a similar trajectory are grouped. Common region - Elements enclosed in a region are part of the same group. This region could be arbitrary. Parallelism - Parallel elements are grouped. Closure - Lined or curves that are closed. Symmetry - Elements exhibiting some sort of symmetry. For example, a mirrored shaped. Continuity - Continuous curves are grouped. Familiar configuration - Lower level elements, when grouped together, form a higher level object. Figure 6: From Forsyth and Ponce \u0026ldquo;Computer Vision - A Modern Approach\u0026rdquo; Figure 7: From Forsyth and Ponce \u0026ldquo;Computer Vision - A Modern Approach\u0026rdquo; Figure 8: Sky and Water by M.C. Escher Intuiting applications for some of these rules is easier than others. For example, familiar configuration suggests that some familiar object can be identified by the sum of its parts. This is especially helpful for problems where the whole object is occluded.\nFigure 9: Parts of the hand are occluded either by the hand itself or some object (Mueller et al.) Common fate is a useful rule when considering tracking an object or group of objects over a series of frames. Even something a simple as frame differencing is an efficient preprocessing step to removing unrelated information.\nSegmentation Methods Active Contours Segmentation via Clustering References Achanta, Radhakrishna, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk. 2012. “SLIC Superpixels Compared to State-of-the-Art Superpixel Methods.” Ieee Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–82. https://doi.org/10.1109/TPAMI.2012.120. ","date":1645509600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718755200,"objectID":"786cd56c3c34a681a236676847376ccf","permalink":"https://ajdillhoff.github.io/notes/image_segmentation/","publishdate":"2022-02-22T00:00:00-06:00","relpermalink":"/notes/image_segmentation/","section":"notes","summary":"Table of Contents Resources Introduction Gestalt Theory Grouping Segmentation Methods Resources https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/ (Berkeley Segmentation Database) https://arxiv.org/abs/2105.15203v2 (SegFormer) https://arxiv.org/abs/1703.06870 (Mask R-CNN) https://github.com/sithu31296/semantic-segmentation (Collection of SOTA models) Introduction Feature extraction methods such as SIFT provide us with many distinct, low-level features that are useful for providing local descriptions images. We now \u0026ldquo;zoom out\u0026rdquo; and take a slightly higher level look at the next stage of image summarization. Our goal here is to take these low-level features and group, or fit, them together such that they represent a higher level feature.","tags":["computer vision"],"title":"Image Segmentation","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Algorithm Rectangle Detection based on a Windowed Hough Transform Introduction Fitting a model to a set of data by consensus, as in RANdom SAmple Consensus, produces a parameter estimate that is robust to outliers. A similar technique for detecting shapes in images is the Hough Transform. Originally it was designed for detecting simple lines, but it can be extended to detect arbitrary shapes.\nThe transform is computed given an edge image. Each edge pixel in the image casts a vote for a line given a set of parameters. This vote is added to an accumulator array which tallies the votes over all pixels for all parameter choices.\nAlgorithm An accumulator array holds the votes for each edge point. The indices to this array represent the line parameters \\((\\rho, \\theta)\\), where\n\\[ \\rho = x \\cos \\theta + y \\sin \\theta. \\]\nFigure 1: Parameterization of a line in a Hough transform. Source: Wikipedia In this parameterization, \\((\\rho, \\theta)\\) represents a vector that is normal to the line. For each edge pixel in the original image, a range of \\(\\theta\\) values are tested. The resolution of \\(\\theta\\) values used is set as a hyperparameter to the algorithm. A vote is cast for every single line within the resolution of \\(\\theta\\) by incrementing the accumulator array at entry \\((\\rho, \\theta)\\).\nFigure 2: Visualization of voting procedure. Source: Wikipedia After all edge pixels are evaluated, the accumulator array must be post processed to select the lines with the most agreement. Since the accumulator array is 2D, it can be visualized as seen below.\nFigure 3: Visualization of accumulator array. Source: Wikipedia Rectangle Detection based on a Windowed Hough Transform As mentioned in the introduction, Hough transforms can be extended to detect arbitrary shapes. In their 2004 publication, Jung and Schramm approach the problem of detecting rectangles in an image via Hough transforms. By analyzing the spatial relationship of peaks in a standard Hough transform, rectangles can reliably be detected.\nDetecting Rectangles via Hough peaks If we consider pixel located at the center of a rectangle, a few key symmetries can be observed.\nThe detected peaks from the Hough window will be in pairs, equidistant from the center. Two pairs will be separated by \\(90^{\\circ}\\) with respect to the \\(\\theta\\) axis. The distance between the peaks of a pair are the sides of the rectangle. Figure 4: Rectangle centered at the origin. Figure 5: The resulting Hough transform of the previous figure. The Algorithm Each pixel in the image is evaluated using a sliding window approach. Consider a pixel centered on \\((x_c, y_c)\\). The size of the window determines the maximum size of the rectangle that can be detected in a given window. The authors use a circular threshold with a minimum and maximum diameter \\(D_{min}\\) and \\(D_{max}\\). That is, the search region is a circle such that the smallest detectable rectangle has a side length of no less than \\(D_{min}\\) and a diagonal length of no more than \\(D_{max}\\). The figure below visualizes this region.\nFigure 6: Search region based on a circle. Source: Jung and Schramm 2004. With the search region defined, a hough transform for that region is computed. The paper mentions an optimization step when selecting the discretization steps \\(d_{\\theta}\\) and \\(d_{\\rho}\\). If the input image is large, the resulting Hough transform will also be large. They recommend picking \\(d_{\\theta} = \\frac{3 \\pi}{4 D_{max}}\\) and \\(d_{\\rho} = \\frac{3}{4}\\).\nThe Hough image is further processed in order to extra local extrema. These peaks should correspond to the lines of the rectangle. First, an enhanced image is created following\n\\[ C_{\\text{enh}}(\\rho, \\theta) = h w \\frac{C(\\rho, \\theta)^2}{\\int_{-h/2}^{h/2}\\int_{-w/2}^{h/2} C(\\rho + y, \\theta + x)dx dy}. \\]\nHow is such an integral computed? The integral is the \u0026ldquo;area under the curve\u0026rdquo; of a given signal. In this case, the accumulator image is a 2D signal for which a discrete approximation of the integral can be computed. This can be implemented via convolution. The local maxima of this enhanced image are those such that \\(C(\\rho, \\theta) \\geq T_C\\), where \\(T_C = 0.5D_{min}\\).\nFigure 7: Original Hough image (left) and enhanced (right) (Jung and Schramm 2004). With the detected peaks from the enhanced image, four peaks are selected with satisfy the symmetries of rectangles as listed above. This is shown as equation 3 in the paper:\n\\begin{align*} \u0026amp;\\Delta \\theta = |\\theta_i - \\theta_j| \u0026lt; T_{\\theta},\\\\ \u0026amp;\\Delta \\rho = |\\rho_i + \\rho_j| \u0026lt; T_{\\rho},\\\\ \u0026amp;|C(\\rho_{i}, \\theta_i) - C(\\rho_j, \\theta_j)| \u0026lt; T_{L}\\frac{C(\\rho_i, \\theta_i) + C(\\rho_j, \\theta_j)}{2}. \\end{align*}\n\\(T_{\\theta}\\) is used to threshold peaks corresponding to parallel lines (\\(\\theta_i \\approx \\theta_j\\)). \\(T_{\\rho}\\) is a threshold for symmetry (equal distance between lines and center). \\(T_L\\) ensures that line segments have approximately the same length.\nFor peaks that satisfy the equations above, an extended peak \\(P_k = (\\pm \\xi, \\alpha_k)\\) is formed, where\n\\[ \\alpha_k = \\frac{1}{2}(\\theta_i + \\theta_j) \\text{ and } \\xi_k = \\frac{1}{2}|\\rho_i - \\rho_j|. \\]\nFinally, a rectangle is detected if the pairs of lines are orthogonal. That is, if\n\\[ \\Delta \\alpha = ||\\alpha_k - \\alpha_l| - 90^{\\circ}| \u0026lt; T_{\\alpha}. \\]\nThe rectangle parameters are encoded by \\(\\alpha_k\\) being its orientation and \\(2\\xi_k\\) and \\(2\\xi_l\\) being its sides.\nThe final step in the paper is to remove duplicates since, depending on the threshold choices, multiple candidates for a rectangle may be detected. The intuition behind this step is to create an error measure that summarizes how well the symmetries defined by the conditions required for a rectangle are respected.\n\\[ E(P_k, P_l) = \\sqrt{a(\\Delta \\theta_k^2 + \\Delta \\theta_l^2 + \\Delta \\alpha^2) + b(\\Delta \\rho_k^2 + \\Delta \\rho_l^2)} \\]\nIn this error measure, \\(a\\) and \\(b\\) are used to weight the angular and distance errors differently since a change in 1 pixel would be more significant than a change of 1 degree.\nIf the difference in orientation for each line detected in the sides is greater, the orientation error increases. Likewise, the more that the pairs of lines stray from orthogonality, the greater the error becomes. For the distance measure, the more offset the lines are with respect to the center, the greater the error becomes. Thus, the rectangle that meets these criteria best will have the lowest error.\n","date":1645077600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707609600,"objectID":"a8be689a7c1c2065a2c3f81e571bafee","permalink":"https://ajdillhoff.github.io/notes/hough_transform/","publishdate":"2022-02-17T00:00:00-06:00","relpermalink":"/notes/hough_transform/","section":"notes","summary":"Table of Contents Introduction Algorithm Rectangle Detection based on a Windowed Hough Transform Introduction Fitting a model to a set of data by consensus, as in RANdom SAmple Consensus, produces a parameter estimate that is robust to outliers. A similar technique for detecting shapes in images is the Hough Transform. Originally it was designed for detecting simple lines, but it can be extended to detect arbitrary shapes.\nThe transform is computed given an edge image.","tags":null,"title":"Hough Transform","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Finding the Best Fit Model Introduction Unless our data is perfect, we will not be able to find parameters that fit the data in the presence of outliers. Consider fitting the data in the figure below using a least squares method.\nFigure 1: Points sample along a line with many outliers around it. Source: Wikipedia If we were to fit a naive least squares model, the outliers would surely produce parameters for a line that does not fit the most amount of data possible.\nConsider the figures below. In the first one, a least squares model is fit to points generated from a line. With the addition of just a single outlier, the model no longer fits the line.\nFigure 2: Least squares can easily fit a line with great accuracy. Figure 3: A single outlier leads to a bad fit for linear regression. Ideally, we want a model that is robust to outliers. That is, the model should be fit such that it matches the largest number of samples, or inliers. One such approach to this problem is RANdom SAmple Consensus (RANSAC).\nFigure 4: RANSAC fit to most inliers while ignoring the outliers. Source: Wikipedia The general process is as follows:\nRandomly select source samples and their matching targets. Fit a model to the data such that transforming the input by the model parameters yields a close approximation to the targets. Measure the error of how well ALL data fits and select the number of inliers with error less than \\(t\\). If the error is lower than the previous best error, fit a new model to these inliers. Figure 5: RANSAC fitting random samples and counting the number of inliers. Source: Wikipedia The algorithm can be found on the Wikipedia page.\nFinding the Best Fit Model When it comes to finding the parameters of a transformation matrix that converts points in one image to another, how do we solve for that matrix? We are looking for some \\(A\\) such that\n\\begin{equation*} A\\mathbf{x} = \\mathbf{\\hat{x}}. \\end{equation*}\nIn a perfect world, \\(\\mathbf{\\hat{x}}\\) will match the target point \\(\\mathbf{y}\\). In other words,\n\\(\\|\\mathbf{\\hat{x}} - \\mathbf{y}\\|_{2} = 0\\).\nFor an affine transformation, we would have some transformation matrix\n\\begin{equation*} A = \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\end{bmatrix}. \\end{equation*}\nThen we compute each component of \\(A\\mathbf{x}\\) as\n\\begin{align*} \\hat{x}_1 \u0026amp;= a_{11} * x_1 + a_{12} * x_2 + a_{13} * 1\\\\ \\hat{x}_2 \u0026amp;= a_{21} * x_1 + a_{22} * x_2 + a_{23} * 1\\\\ \\end{align*}\nWe can fit this using a least squares approach by the following construction.\n\\begin{equation*} \\begin{bmatrix} x_1^{(1)} \u0026amp; x_2^{(1)} \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; x_1^{(1)} \u0026amp; x_2^{(1)} \u0026amp; 1\\\\ \u0026amp;\u0026amp; \\vdots\\\\ x_1^{(n)} \u0026amp; x_2^{(n)} \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; x_1^{(n)} \u0026amp; x_2^{(n)} \u0026amp; 1\\\\ \\end{bmatrix} \\begin{bmatrix} a_{11}\\\\ a_{12}\\\\ a_{13}\\\\ a_{21}\\\\ a_{22}\\\\ a_{23}\\\\ \\end{bmatrix}= \\begin{bmatrix} \\hat{x}_1^{(1)}\\\\ \\hat{x}_2^{(1)}\\\\ \\vdots\\\\ \\hat{x}_1^{(n)}\\\\ \\hat{x}_2^{(n)}\\\\ \\end{bmatrix} \\end{equation*}\nWe can solve this analytically! Recall the normal equations:\n\\[ A^T A \\mathbf{x} = A^T \\mathbf{b}. \\]\nLet\u0026rsquo;s test this on a couple of images\u0026hellip;\nFigure 6: Two images taken with matching features shared between them. First, we use some feature detector such as SIFT to find keypoints in each image. Then, we can take a brute force approach to determine which keypoints match between them.\nFigure 7: We\u0026rsquo;ve got a lot of potential matches here. After running RANSAC, we end up with a model that fits the following inlier points.\nFigure 8: Many of the outliers were removed and we are left with the following matches. We can use the found transformation matrix to warp our source image to fit our destination image as seen below.\nFigure 9: Images stitched together\u0026hellip; not perfect! Finding a better transformation The transformation matrix was an affine transformation matrix. What we really want is a projective transformation! We can extend our approach to finding an affine matrix from earlier by remembering that projective transformations are completed following a perspective divide (usually denoted by \\(w\\)).\nInstead of a constant 1 in the third position of the affine vector, we have a value \\(w\\):\n\\begin{equation*} \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13}\\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23}\\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33}\\\\ \\end{bmatrix} \\begin{bmatrix} x\\\\ y\\\\ w \\end{bmatrix}= \\begin{bmatrix} \\hat{x}\\\\ \\hat{y}\\\\ \\hat{w} \\end{bmatrix}. \\end{equation*}\nDividing by \\(\\hat{w}\\) completes the perspective projection:\n\\begin{equation*} \\begin{bmatrix} \\frac{\\hat{x}}{\\hat{w}}\\\\ \\frac{\\hat{y}}{\\hat{w}}\\\\ 1 \\end{bmatrix}. \\end{equation*}\nAgain, we can write out the individual equation for each component as\n\\begin{align*} \\hat{x} \u0026amp;= (a_{11} * x + a_{12} * y + a_{13} * w) \\div (a_{31} * x + a_{32} * y + a_{33} * w)\\\\ \\hat{y} \u0026amp;= (a_{21} * x + a_{22} * y + a_{23} * w) \\div (a_{31} * x + a_{32} * y + a_{33} * w)\\\\ \\end{align*}\nWe may assume that \\(w = 1\\) for the original points (before transformation). Additionally, \\(a_{33}\\) is typically set to 1 when constructing a transformation matrix. These are safe enough assumptions to make considering that we will make many attempts at finding the best fitting parameters.\nSolving for \\(\\hat{x}\\) and \\(\\hat{y}\\) in terms of a linear combination of elements yields\n\\begin{align*} \\hat{x} \u0026amp;= a_{11} * x + a_{12} * y + a_{13} - \\hat{x} * a_{31} * x - \\hat{x} * a_{32} * y\\\\ \\hat{y} \u0026amp;= a_{21} * x + a_{22} * y + a_{23} - \\hat{y} * a_{31} * x - \\hat{y} * a_{32} * y\\\\ \\end{align*}\nWe can fit this using a least squares approach by the following construction.\n\\begin{equation*} \\begin{bmatrix} x_1^{(1)} \u0026amp; x_2^{(1)} \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; -x_1^{(1)}\\hat{x}_1^{(1)} \u0026amp; -x_2^{(1)}\\hat{x}_1^{(1)}\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; x_1^{(1)} \u0026amp; x_2^{(1)} \u0026amp; 1 \u0026amp; -x_1^{(1)}\\hat{x}_2^{(1)} \u0026amp; -x_2^{(1)}\\hat{x}_2^{(1)}\\\\ \u0026amp;\u0026amp; \\vdots\\\\ x_1^{(n)} \u0026amp; x_2^{(n)} \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; -x_1^{(n)}\\hat{x}_1^{(n)} \u0026amp; -x_2^{(n)}\\hat{x}_1^{(n)}\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; x_1^{(n)} \u0026amp; x_2^{(n)} \u0026amp; 1 \u0026amp; -x_1^{(n)}\\hat{x}_2^{(n)} \u0026amp; -x_2^{(n)}\\hat{x}_2^{(n)} \\end{bmatrix} \\begin{bmatrix} a_{11}\\\\ a_{12}\\\\ a_{13}\\\\ a_{21}\\\\ a_{22}\\\\ a_{23}\\\\ a_{31}\\\\ a_{32} \\end{bmatrix}= \\begin{bmatrix} \\hat{x}_1^{(1)}\\\\ \\hat{x}_2^{(1)}\\\\ \\vdots\\\\ \\hat{x}_1^{(n)}\\\\ \\hat{x}_2^{(n)} \\end{bmatrix}. \\end{equation*}\nWe can use the normal equations as before to solve for this system.\nThe figure below shows the final result of image stitching using a perspective projection instead of an affine matrix.\nFigure 10: Stitching using a perspective projection. ","date":1644904800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644904800,"objectID":"085825a3dddd7bf8c906c1b3a1eb9308","permalink":"https://ajdillhoff.github.io/notes/random_sample_consensus/","publishdate":"2022-02-15T00:00:00-06:00","relpermalink":"/notes/random_sample_consensus/","section":"notes","summary":"Table of Contents Introduction Finding the Best Fit Model Introduction Unless our data is perfect, we will not be able to find parameters that fit the data in the presence of outliers. Consider fitting the data in the figure below using a least squares method.\nFigure 1: Points sample along a line with many outliers around it. Source: Wikipedia If we were to fit a naive least squares model, the outliers would surely produce parameters for a line that does not fit the most amount of data possible.","tags":["machine learning"],"title":"RANdom SAmple Consensus","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":"Introduction Let\u0026rsquo;s take a simple constrained problem (from Nocedal and Wright).\n\\begin{align*} \\min \\quad \u0026amp; x_1 + x_2\\\\ \\textrm{s.t.} \\quad \u0026amp; x_1^2 + x_2^2 - 2 = 0 \\end{align*}\nThe set of possible solutions to this problem lie on the boundary of the circle defined by the constraint:\nFigure 1: Source: Nocedal and Wright If we let \\(g(\\mathbf{x}) = x_1^2 + x_2^2 - 2\\), then the gradient vector is \\((2x_1, 2x_2)\\)\nOur original function \\(f(\\mathbf{x}) = x_1 + x_2\\) has a gradient vector of \\((1, 1)\\).\nThe figure above visualizes these vectors at different points on the constraint boundary.\nNotice that the optimal solution \\(\\mathbf{x}^* = (-1, -1)\\) is at a point where \\(\\nabla g(\\mathbf{x}^*)\\) is parallel to \\(\\nabla f(\\mathbf{x}^*)\\). However, the gradients of the vectors are not equal. So there must be some scalar \\(\\lambda\\) such that \\(\\nabla f(\\mathbf{x}^*) = \\lambda \\nabla g(\\mathbf{x}^*)\\).\nThis scalar \\(\\lambda\\) is called a Lagrangian multiplier. We use this and introduce the Lagrangian function:\n\\begin{equation*} \\mathcal{L}(\\mathbf{x}, \\lambda) = f(\\mathbf{x}) - \\lambda g(\\mathbf{x}) \\end{equation*}\nThis yields a form for which we can analytically calculate the stationary points. That is,\n\\begin{equation*} \\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}^*, \\lambda^*) = 0. \\end{equation*}\nLagrangian Duality In general, the primal optimization problem is formulated as\n\\begin{align*} \\min_{w} \\quad \u0026amp; f(w)\\\\ \\textrm{s.t.} \\quad \u0026amp; g_i(w) \\leq 0, \\quad i = 1, \\dots, k\\\\ \u0026amp; h_i(w) = 0, \\quad i = 1, \\dots, l. \\end{align*}\nThe Lagrangian function is then\n\\[ L(w, \\alpha, \\beta) = f(w) + \\sum_{i=1}^k\\alpha_i g_i(w) + \\sum_{i=1}^l \\beta_i h_i(w). \\]\nAdditional Resources https://cs229.stanford.edu/notes2021fall/cs229-notes3.pdf ","date":1644040800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644040800,"objectID":"07ec05c44fcee043ca37fbb243255816","permalink":"https://ajdillhoff.github.io/notes/lagrangian_multipliers/","publishdate":"2022-02-05T00:00:00-06:00","relpermalink":"/notes/lagrangian_multipliers/","section":"notes","summary":"Introduction Let\u0026rsquo;s take a simple constrained problem (from Nocedal and Wright).\n\\begin{align*} \\min \\quad \u0026amp; x_1 + x_2\\\\ \\textrm{s.t.} \\quad \u0026amp; x_1^2 + x_2^2 - 2 = 0 \\end{align*}\nThe set of possible solutions to this problem lie on the boundary of the circle defined by the constraint:\nFigure 1: Source: Nocedal and Wright If we let \\(g(\\mathbf{x}) = x_1^2 + x_2^2 - 2\\), then the gradient vector is \\((2x_1, 2x_2)\\)","tags":["optimization"],"title":"Lagrangian Multipliers","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Computing Gradient Norms Nonmaxima Suppression Thresholding Connectivity Analysis Introduction Figure 1: Vertical derivative filter (left) and horizontal derivative filter (right). When image gradient filters are applied to an image, we can observe that the sample responses are very sensitive to noise and detail. For example, look at the surface at the back of ship near the drive cone. To resolve this, the image should be smoothed before differentiating it. Recall that the Gaussian filter smooths the area so that neighboring pixels are more similar than distant pixels.\nFigure 2: dx kernel applied to image blurred with Gaussian (sigma=1). This is closer to what we want, but the end goal is to create an image that shows distinct edges. We need to be clear about what an edge is. For now, we consider the images produced by convolving the \\(dx\\) or \\(dy\\) kernels as edge score images. They are only intermediate; we still need to make a final decision.\nIn this section, we will learn about the Canny Edge Detector. The general algorithm is as follows:\nSmooth the image using Gaussian blurring. Compute the gradient image via filtering. Most commonly, the Sobel operator is used. Filter out weaker edge score by selecting local pixels with the largest gradient change. Use double thresholding to separate strong, or definite, edge pixels from weak ones. Remove all weak pixels not connected to a strong pixel. Canny edge detection follows 3 objective criteria:\nEdges should be detected with a low error rate. The goal is to extract as many actual edges as possible. A detected edge should correspond to the center pixel of the edge in the original image. It should be robust to noise and only mark an edge pixel once. Smoothing the image can be done by applying a Gaussian blur. Next, we need to compute the gradient image.\nComputing Gradient Norms As we saw before, the derivative filters compute the direction of greatest change in the calculated direction. When combining the result of \\(dx\\) and \\(dy\\), we get the gradient of the pixel.\n\\[ \\nabla f(x, y) = \\Bigg[\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\\Bigg]. \\]\nCanny edge detection works by selecting local pixels with the largest gradient change. In order to do this, we need to compute the norm of the gradient. If we consider every pixel in the gradient image to be a vector indicating the direction of greatest change, the norm can be computed as\n\\[ \\|\\nabla f(x, y)\\| = \\sqrt{\\Big(\\frac{\\partial f}{\\partial x}\\Big)^2 + \\Big(\\frac{\\partial f}{\\partial y}\\Big)^2}. \\]\nAdditionally, we want the angle of direction of the gradient. This can be computed for each pixel as\n\\begin{equation*} \\theta = \\text{atan2}(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}). \\end{equation*}\nIn practice, this can be computed at the same time. There are also efficient implementations of atan2 which can generate an array of the same size of the original image containing the computed angles for each pixel.\nFigure 3: Gradient norm of image. The figure above shows the result of computing the gradient norms for each pixel. This representation is intuitive to interpret. The largest values are on the edges of the violin. The image produced by this step is still too fuzzy. These do not represent the final edges.\nNonmaxima Suppression The gradient norm image is helpful in showing all edge scores, but the egdes are still too thick and there are many disconnected edge scores detected. We can thin the edges by evaluating neighboring pixels. We will select only the local pixels which have the highest absolute gradient and suppress the others. This process is called nonmaxima suppression. There are two approaches to this problem. The first is approximates the closest gradient normal. The second uses interpolation to compute a more accurate value.\nIn the first approach, the number of discrete orientations for the edge normal are split into horizontal, vertical, \\(45^{\\circ}\\), and \\(-45^{\\circ}\\).\nFigure 4: Discretizing angles into 4 regions. For a given pixel, the gradient direction is discretized into one of the above four regions by selection the angle closest to the original angle given. Next, the gradient norm of the given pixel is compared to that of the pixels on either side of it following the same discretized direction. If one of the neighboring pixels has a higher gradient norm, the current pixel\u0026rsquo;s value is set to 0. The intuition here is that if it were an edge pixel, it would have the largest gradient norm along its given direction.\nThe result of applying this process on our gradient image is shown below.\nFigure 5: Gradient norm image after nonmaxima suppression is applied. Interpolation An alternative approach is to interpolate the gradient norm using the actual angle. Instead of discretizing it into one of four regions above, the original angle is used to compute the neighboring pixels in continuous space. This will, of course, produce invalid pixel locations. The gradient norm for the neighboring pixels follows the approach discussed in Sampling and Aliasing.\nFor example, if we are at pixel \\((5, 5)\\) with a gradient direction of \\(55^{\\circ}\\), then the neighboring pixels along that angle can be computed by first finding the vector following that direction. That is\n\\begin{align*} \\mathbf{p}_{\\text{offset}} \u0026amp;= \\begin{bmatrix} \\cos (55^{\\circ} \\cdot \\frac{\\pi}{180^{\\circ}})\\\\ \\sin (55^{\\circ} \\cdot \\frac{\\pi}{180^{\\circ}}) \\end{bmatrix}\\\\ \u0026amp;= \\begin{bmatrix} .5736\\\\ .8192 \\end{bmatrix} \\end{align*}\nThen the two neighboring pixels along this direction are \\(f(5 - .5373, 5 - .8192)\\) and \\(f(5 + .5373, 5 + .8192)\\). These are clearly not valid pixel locations. To compute the interpolated value, a weighted contribution from the closest 4 pixels are used for each of the two neighbors. For \\(f(4.4627, 4.1808)\\), these pixels are \\(\\{(4, 4), (5, 4), (4, 5), (5, 5)\\}\\). The interpolation weights for this pixel are computed as\n\\begin{align*} w_x \u0026amp;= 4.4627 - 4 = .4627\\\\ w_y \u0026amp;= 4.1808 - 4 = .1808 \\end{align*}\nThen the resulting pixel value is computed via bilinear interpolation:\n\\begin{align*} f(4.4627, 4.1808) \u0026amp;= (1 - w_x) \\cdot (1 - w_y) \\cdot f(4, 4)\\\\ \u0026amp;+ w_x \\cdot (1 - w_y) \\cdot f(5, 4)\\\\ \u0026amp;+ (1 - w_x) \\cdot w_y \\cdot f(4, 5)\\\\ \u0026amp;+ w_x \\cdot w_y \\cdot f(5, 5). \\end{align*}\nThresholding We now have an image of edge scores, but have not yet made a final determination on which pixels are actually edges. One approach to selecting the edge pixel is to use thresholding. That is, we suppress any pixel value that is lower than some parameter \\(T\\):\n\\[ f_{T}(x, y) = f(x, y) \\geq T. \\]\nHowever, this approach will still leave many false positives as well as edge segments that may be connected to strong edges. This issue is partly resolved via hysteresis thresholding. For this, we choose 2 threshold values: one for weak edges and another for strong edge scores. Using these scores, we can generate two images:\n\\begin{align*} f_{T_H}(x, y) \u0026amp;= f(x, y) \\geq T_{H}\\\\ f_{T_L}(x, y) \u0026amp;= f(x, y) \\geq T_{L} \\end{align*}\nWe can then eliminate the duplicate pixels in \\(f_{T_L}\\) by subtracting \\(f_{T_H}\\):\n\\[ f_{T_L} = f_{T_L} - f_{T_H}. \\]\nUsing the image processed via nonmaxima suppression from before, this generates the following images:\nFigure 6: Low threshold image (left) and high threshold image (right). Connectivity Analysis There must be a reason why we computed a lower threshold. There are weak edge pixels that may have been apart of a segment connected to strong pixels. In this case, we want to keep every weak pixel that is 8-connected to a strong pixel.\nFigure 7: The pixels surrounding the black pixel are 8-connected to it. Source: Wikipedia This can be accomplished with the following steps:\nLocate an edge pixel in the high threshold image. Mark all pixels in the weak image that are 8-connected to the current strong pixel as strong pixels. Repeat steps 1 and 2 for all strong pixels in the original high threshold image. Set all pixels in the weak image that were not marked to 0. Add the marked weak pixels to the strong image. Applying the procedure above given the weak and strong images from before yields the following result.\nFigure 8: Final edge image after connectivity analysis. ","date":1643695200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706400000,"objectID":"414934be6bf5e4329fbef86c5a20f69d","permalink":"https://ajdillhoff.github.io/notes/edge_detection/","publishdate":"2022-02-01T00:00:00-06:00","relpermalink":"/notes/edge_detection/","section":"notes","summary":"Table of Contents Introduction Computing Gradient Norms Nonmaxima Suppression Thresholding Connectivity Analysis Introduction Figure 1: Vertical derivative filter (left) and horizontal derivative filter (right). When image gradient filters are applied to an image, we can observe that the sample responses are very sensitive to noise and detail. For example, look at the surface at the back of ship near the drive cone. To resolve this, the image should be smoothed before differentiating it.","tags":["computer vision"],"title":"Edge Detection","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Resizing Sampling Resizing Aliasing arises through resampling an image How to resize - algorithm How to resolve aliasing Resizing an image, whether increase or decreasing the size, is a common image operation. In Linear Algebra, scaling is one of the transformations usually discussed, along with rotation and skew. Scaling is performed by creating a transformation matrix\n\\begin{equation*} M = \\begin{bmatrix} s \u0026amp; 0\\\\ 0 \u0026amp; s \\end{bmatrix}, \\end{equation*}\nwhere \\(s\\) is the scaling factor. This matrix can then be used to transform the location of each point via matrix multiplication.\nIs is that simple for digital images? Can we simply transform each pixel location of the image using \\(M\\)? There are a couple of steps missing when it comes to scaling digital images. First, \\(M\\) simply creates a mapping between the location in the original image and the corresponding output location in the scaled image. If we were to implement this in code, we would need to take the pixel\u0026rsquo;s value from the original image.\nA Simple example Take a \\(2 \\times 2\\) image whose pixel values are all the same color.\nFigure 1: 2 by 2 image whose values are the same. If we transform each pixel location of the image and copy that pixel\u0026rsquo;s value to the mapped location in the larger image, we would get something as seen in the figure below.\nFigure 2: The resulting scaled image. This image is exactly what we would expect. The resulting image is two times as large as the first. What pixel values should the new ones take on? This is a question of sampling.\nSampling Given \\(s = 2\\), the scaling matrix maps the original pixel locations to their new values:\n\\((0, 0) \\mapsto (0, 0)\\) \\((0, 1) \\mapsto (0, 2)\\) \\((1,0) \\mapsto (2, 0)\\) \\((1,1) \\mapsto (2, 2)\\) What values should be given to the unmapped values of the new image? There are several sampling strategies used in practice. Two of the most common approaches are nearest neighbor and bilinear sampling. Let\u0026rsquo;s start with the nearest neighbor approach.\nNearest Neighbor First, we let the pixel location in an image be the center of that pixel, as depicted below.\nFigure 3: A 2-by-2 image with pixel locations depicted as dots in the center. To establish a map between the pixel locations in the scaled image and that of the original image, we shrink the grid on the larger image and superimpose it over the smaller image.\nFigure 4: Pixel grid of larger image superimposed on original image. With nearest neighbor interpolation, the pixel value in the resized image corresponds to that of the nearest pixel in the original image. In the above figure, we can see that pixels \\((0, 0), (0, 1), (1, 0), \\text{ and } (1, 1)\\) in the resized image are closest to pixel \\((0, 0)\\) in the original image. Thus, they will take on that pixel\u0026rsquo;s value.\nLet\u0026rsquo;s compare both of these approaches on a real image. The first figure below shows the original image \\((16 \\times 16\\)). The following figure shows the image resized to \\((32 \\times 32)\\) with no interpolation and nearest neighbor interpolation, respectively.\nFigure 5: Original image. Figure 6: Image resized with no interpolation (left) and nearest neighbor (right). Bilinear Interpolation Bilinear interpolation is a slightly more sophisticated way of sampling which takes into account all neighboring pixels in the original image. The value of the pixel in the sampled image is a linear combination of the values of the neighbors of the corresponding pixel it is mapped to.\nConsider a \\(3 \\times 3\\) image upsampled to a \\(8 \\times 8\\) image. The figure below shows the original image with the coordinates of the upsampled image superimposed on it.\nFigure 7: 3-by-3 grid with 8-by-8 coordinates overlaid. How do we determine the coordinate map between the original and upscaled image? Solve a linear system.\nNote the extreme values of the image. That is, the smallest and largest coordinates. Since we stated previously that \\((0, 0)\\) refers to the coordinate in the middle of the pixel, the top-left of the image boundary for any image is \\((-0.5, -0.5)\\). The bottom-right corner for the smaller image is \\((2.5, 2.5)\\). The bottom-right corner for the resized image is \\((7.5, 7.5)\\). The equation that maps the top-left coordinates between the images is given by\n\\[ -\\frac{1}{2} = -\\frac{1}{2}a + b. \\]\nThe equation that maps the bottom-right coordinates between the images is given by\n\\[ \\frac{5}{2} = \\frac{15}{2}a + b. \\]\nThus, we have 2 equations 2 unknowns. Solving this yields \\(a = \\frac{3}{8}\\) and \\(b = -\\frac{5}{16}\\).\nWith the mapping solved, let\u0026rsquo;s compute the color value for pixel \\((3, 3)\\) in the upsampled image. Here, \\((3, 3) \\mapsto (\\frac{13}{16}, \\frac{13}{16})\\) in the original image. Our problem for this particular pixel is reduced to the following figure.\nFigure 8: Determining the pixel value for the mapped pixel using bilinear interpolation. We first interpolate between two pairs of pixels in the original image. That is, we find \\(p_1\\) and \\(p_2\\) in the following figure.\nFigure 9: Step 1: Interpolate the pixel values between two pixels for (p_1) and (p_2). Here, \\(p_2 = \\frac{3}{16}(255, 255, 255) + \\frac{13}{16}(128, 128, 128) \\approx (152, 152, 152)\\) and \\(p_1 = \\frac{3}{16}(255, 0, 0) + \\frac{13}{16}(255, 255, 255) \\approx (255, 207, 207)\\). Note that the contribution of the pixel depends on the weight on the other side the intermediate value \\(p_i\\). For example, if you think of \\(p_1\\) as a slider from the red pixel to the white pixel. The value to the left of the slider reflects the contribution of the pixel to the right, and vice versa.\nFigure 10: Computed values of (p_1) and (p_2). Finally, the value of the new pixel is a linear combination of \\(p_1\\) and \\(p_2\\). That is \\(p = \\frac{13}{16}(152, 152, 152) + \\frac{3}{16}(255, 207, 207) \\approx (171, 162, 162)\\).\nFigure 11: The final pixel value computed from (p_1) and (p_2). The following figure compares the original image with both types of interpolation discussed in this section.\nFigure 12: Original image (left), upscaled 2x with NN interpolation (middle), upscaled with bilinear interpolation (right). Aliasing Nearest neighbor interpolation often leads to images with aliasing. In general, aliasing occurs when two signals are sampled at such a frequency that they become indistinguishable from each other. Usually, images are smoothed prior to upsampling or downsampling in an effort to alleviate the effects of aliasing. The figure below shows a downsampled image using nearest neighbor interpolation.\nFigure 13: Image downsampled by 4x. Notice the \u0026ldquo;jaggies\u0026rdquo;, especially along straight lines. By blurring the image and using bilinear interpolation, the same image looks much smoother when downsized.\nFigure 14: Image downsampled by 4x using bilinear interpolation. ","date":1643522400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643522400,"objectID":"827f72b505b45f132e0d606c6c13fe4f","permalink":"https://ajdillhoff.github.io/notes/sampling/","publishdate":"2022-01-30T00:00:00-06:00","relpermalink":"/notes/sampling/","section":"notes","summary":"Table of Contents Resizing Sampling Resizing Aliasing arises through resampling an image How to resize - algorithm How to resolve aliasing Resizing an image, whether increase or decreasing the size, is a common image operation. In Linear Algebra, scaling is one of the transformations usually discussed, along with rotation and skew. Scaling is performed by creating a transformation matrix\n\\begin{equation*} M = \\begin{bmatrix} s \u0026amp; 0\\\\ 0 \u0026amp; s \\end{bmatrix}, \\end{equation*}","tags":["computer vision"],"title":"Sampling and Aliasing","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Topics The Human Eye Color Matching Color Physics Color Spaces HSV Color Space Topics What is color? How do we process color? What information does color contain? What can we infer from color? The Human Eye The eye acts as a camera, including a lens which focuses light onto a receptive surface. The cornea covers the lens which combine to make a compound lens. The lens itself is flexible to allow the eye to focus on objects of variable distance. The lens is attached to ciliary muscles which contract or expand to change the shape of the lens. This allows us to focus on near or far objects. As we age, the lens itself becomes hardened and does not transform back to a spherical shape when the ciliary muscles contract, resulting in farsightedness.\nThe pupil is a diaphragm that adjusts the amount of light that enters the eye in response to varying intensities. However, this isn\u0026rsquo;t the only mechanism available for this task. Our perception of the amount of light, luminance adaptation, occurs also in the retina and brain over a longer time period (usually several minutes).\nFigure 1: Source: Wikipedia The following image from Wikipedia shows the evolution of the eye from a simple region of photoreceptors to the current model we have today.\nFigure 2: Source: https://en.wikipedia.org/wiki/Eye Retina The primary purpose of the optics of the eye is to focus light onto the retina. This is a thin layer of nerve tissue that covers the inner surface of the eye. It consists of approximately 200 million layered cells. Half are photoreceptor cells and the other half recode photoreceptor outputs before passing them on towards the brain.\nFun fact: The retina is woven with blood vessels whose purpose is to nourish the retinal tissue. These blood vessels produce the red eye effect that is seen in flash photography.\nPhotoreceptor Cells Figure 3: Distribution of rods and cones. Source: Wikipedia Photorceptor cells can be classified into two types: rods and cones. There are about 100 million rods and 6 million cones. Rods dominate our low-light vision. They do not have the variety of photopigments necessary for color vision. Cones dominate our color vision. They can be separated into one of three classes of light receptors. These three classes contain a specific type of photopigment that is sensitive to different wavelengths of light.\nFigure 4: Source: https://handprint.com/HP/WCL/color1.html#3cones Rods can pool input together before sending it to the brain. Low-light perception is better through peripheral vision.\nMost of the cones are in the fovea. The high density of cones in this region means that the resolution is highest. Microsaccades - The eye is making tiny movements so that the eye is never fixated on a single point. This allows light to hit a greater number of photoreceptors.\nThe other segment of a photoreceptor cell lies the photopigment molecules. These act as transducers which convert light energy into a biological response. These molecules are made up of a light sensitive molecule called chromophore and a protein called opsin. Together, they are usually referred to as rhodopsin.\nFigure 5: Photopigment Molecules. Source: https://handprint.com/HP/WCL/color1.html Types of Photopigments Each photoreceptor can respond to light differently depending on the specific type of rhodopsin it is made up of.\nThe response of each type of cone has been measured and can is separated into short wavelength (S), medium wavelength (M), and long wavelength (L). Some resources incorrectly label these as B, G, and R receptors. This is not entirely accurate as there is much overlap between the medium and long wavelength receptors.\nMathematical Model How would we model a photoreceptor, mathematically? The response is dependent on how sensitive the receptor is to a specific wavelength along with the light arriving at the receptor.\n\\begin{equation*} p_k = \\int_{\\Lambda} \\sigma_k(\\lambda)E(\\lambda)d\\lambda \\end{equation*}\nColor Matching The theory that the visible colors are visible from the three primaries is called trichomatic theory. Trichromacy has been measured and observed.\nHow is it known that these 3 (or 4) types of photoreceptors absorb a specific wavelength? In other words, how do we know that color vision break down? There has not been a way to measure cone responses in living humans, but there is a way to measure them indirectly.\nJames Clerk Maxwell\u0026rsquo;s color matching experiments aim to do exactly that. In this experiment, participants are given a test color and a matching color. The goal is to add some amount of primary colors until the matching color is the same as the test color. There are some saturated test colors that cannot be matched in an additive manner (combining primaries). In these cases, the subjects are allowed to subtract some amount of a primary color from the test color until it matches the matching color.\nOver many different experiments involving many subjects, it was found that most subjects will match the test color with the same amount of primary weights. This provided confirmation that human vision is trichromatic. There were also carefully screen dichromats who lacked one class of photoreceptor.\nIs color matching reliable? Do we see all wavelengths the same? That is, do we have an equal number of photoreceptors for each range?\nThe fact that many independent observers came up with similar distributions of primary colors to match test colors gave rise to Grassman\u0026rsquo;s laws.\nThe result of these experiments reveals a function of cone sensitivities. Cones are unequally distributed across the retina. Visual angle and whether the light is detected in the central vs. peripheral region matters. The CIE 1931 RGB color matching functions show how much of each primary wavelength is required to match a particular target wavelength. The negative values mean that the primary was added to the test color in order to match.\nSource: https://en.wikipedia.org/wiki/CIE_1931_color_space\nActual cone sensitivy depends on light sensitivy. One of the most popular graphs of cone response is from Stockman \u0026amp; Sharpe (2000):\nThis graph is normalized to peak response and is not representative of the distribution of cones in the retina. If we weight the responses by the proportion of each class of cone in the retina, the graph looks like:\nThe fact that the 3 shapes are colored as blue, green, and red are misleading. There are far more photoreceptors that perceive green light than there are blue or red light. You can see this when comparing green text versus blue text. Reading the blue text may strain your eyes and appear blurry. This is because there are simply fewer receptors available for these wavelengths. Fewer such receptors also implies that the resolution is smaller.\nFigure 6: Fewer blue cones results in blue light appearing more blurry. Color Physics Light sources themselves can produce different colors at different wavelengths. The sky varies in color depending on the relative location of the sun. A surface is dependent on the color of the surface itself as well as the incident light shining towards it. A white surface with a green light shining on it will reflect green light. A green surface with white light will also reflect green. Further complexities such as atmosphere and dust can further complicate this.\nIn our world, both the sky and the sun are important light sources. They are often referred to as skylight and sunlight, respectively. Imagine a clear day with the sun at the highest point in the sky. The sun is seen as a yellow object with the surrounding sky being a rich blue. Compared to the zenith of the sky, the horizon is typically brighter. This is because air scatters the incident light. This can be modeled by assuming the sky emits a constant amount of exitant light per volume.\nWhy does the sun appear yellow and the sky appear blue? Longer wavelengths can travel farther before being scattered. This means that a ray of light travelling from the sun to the earth will scatter blue light before the other rays. Taking the blue light out of a ray of white light will leave a yellowish color. When the scattered blue light eventually hits the atmosphere of Earth, it is scattered once again and see as incident light to an observer. This gives the apperance of a blue sky.\nThe perceived color of an object can be computed by multiplying the incident illumination with the reflectance of the object.\nColor Temperature One quantity that is commonly used to describe a light source is temperature (measured in Kelvins). This is derived from the concept of the black body. That is, a body that does not reflect light. A heated black body emits radiation and the spectral power distribution of this radiation depends only on the temperature. The color temperature is the surface temperature of an ideal black body.\nAt lower temperature, the color is a red. As it increases, the color becomes whiter until reaching a light blue. See the table below:\nFigure 7: Colors corresponding to different temperatures. Source: Wikipedia Color Spaces How is color represented based on color matching? Can we accurately reproduce any color digitally?\nhttps://www.wnycstudios.org/podcasts/radiolab/episodes/211119-colors Interesting podcast on Color.\nThere are several color spaces (or color models) available. The most common is RGB. In this section, we will explore the most common color spaces.\nFigure 8: Comparison of several common color spaces. Source: https://en.wikipedia.org/wiki/ProPhoto_RGB_color_space CIE xy Color Space Verifying trichomatic theory meant that one should attempt to reproduce monochromatic (single wavelength) colors as a weighted mixture of primary colors. The Commision Internationale d\u0026rsquo;Eclairage did just that in the 1930s. They performed their own color matching experiments using red, blue, and green wavelengths as primary colors. Another benefit of this process is to set a standard in which colors can be reproduced.\nFigure 9: CIE xy color space. This shows the CIE xy space which was taken from CIE XYZ space. This 2D space is an intersection of the original XYZ space by the plane \\(X + Y + Z = 1\\). The coordinates are then\n\\begin{equation*} (x, y) = \\Big(\\frac{x}{x + y + z}, \\frac{y}{x + y + z}\\Big). \\end{equation*}\nThe border of this shape represents the wavevelength of the pure color. The colors in the middle represent some linear combination of those wavelengths.\nRGB Color Spaces RGB color spaces are the most commonly used in computer graphics. CIE also has their own RGB color space. It is derived from color matching experiments using 3 monochromatic primary colors.\nFigure 10: CIE RGB gamut on the CIE xy color space. RGB models are commonly represented as a cube, as seen in the figure below.\nFigure 11: Cube representation of an RGB color space. HSV Color Space Hue, Saturation, Value (HSV) provides an alternative representation of RGB. Hue represents the color. Given a fixed value for saturation and value, colors in HSV should appear as if they are receiving the same level of light.\nFigure 12: Hue values when saturation and value are fixed. Source: Wikipedia The CIE defines saturation as \u0026ldquo;the colourfulness of an area judged in proportion to its brightness.\u0026rdquo; The value of a pixel represents how bright the color is compared to black.\nFigure 13: HSV cylinder exemplifies the concepts of hue, saturation, and value. Source: Wikipedia Conversion from RGB We can convert images to HSV from RGB and vice versa. This article will only go through the steps of actually transforming it. To gain a better understanding of the conversion between HSV to RGB, check out the Wikipedia page.\nWe start by making sure that are input image is normalized such that the RGB values are in the range \\([0, 1]\\). If that\u0026rsquo;s the case, we can calculate the HSV value \\(V\\) as\n\\[ V = \\max(R, G, B). \\]\nWe can do this in one command in Python using numpy:\nV = np.max(img, axis=2) Saturation is computed based on another quantity called Chroma. It is simply computed as\n\\[ C = V - \\min(R,G,B). \\]\nIn Python, this is simply\nC = V - np.min(img, axis=2) Saturation is then computed as \\(S = \\frac{C}{V}\\). Note that this will be undefined if \\(V = 0\\). In practice, we can set \\(S = 0\\) if \\(V\\) is also 0.\nHue is commonly measured in degrees between \\([0, 360]\\). As seen in the figure below, it is the angle past the previous edge of the hexagon.\nFigure 14: Hue is the angle of the projected point with respect to the hexagon. 0 degrees is marked by the red edge. Source: Wikipedia The function for Hue can be written as a piecewise function, altered slightly to account for undefined values in practice:\n\\begin{equation*} H\u0026rsquo; = \\begin{cases} 0, \u0026amp; C = 0\\\\ \\frac{G - B}{C} \\text{ mod } 6, \u0026amp; \\text{if } V = R\\\\ \\frac{B - R}{C} + 2, \u0026amp; \\text{if } V = G\\\\ \\frac{R - G}{C} + 4, \u0026amp; \\text{if } V = B\\\\ \\end{cases} \\end{equation*}\nThen, \\(H = 60^{\\circ} \\times H\u0026rsquo;\\).\nConversion to RGB To go back to RGB, we take an HSV image with \\(H \\in [0^{\\circ}, 360^{\\circ}]\\), and \\(S, V \\in [0, 1]\\). The Chrome value is calcuated as\n\\[ C = V \\times S. \\]\nWe then divide up the Hue into one of 6 values:\n\\[ H\u0026rsquo; = \\frac{H}{60^{\\circ}}. \\]\nWith these intermediate value, we can calculate the corresponding point within the RGB cube that has the same hue and chroma as the current pixel value, with \\(X\\) being the second largest component of the color:\n\\[ X = C \\times (1 - |H\u0026rsquo; \\text{ mod } 2 - 1|) \\]\nand then\n\\begin{equation*} (R\u0026rsquo;, G\u0026rsquo;, B\u0026rsquo;) = \\begin{cases} (C, X, 0) \u0026amp; \\text{if } 0 \\leq H\u0026rsquo; \u0026lt; 1\\\\ (X, C, 0) \u0026amp; \\text{if } 1 \\leq H\u0026rsquo; \u0026lt; 2\\\\ (0, C, X) \u0026amp; \\text{if } 2 \\leq H\u0026rsquo; \u0026lt; 3\\\\ (0, X, C) \u0026amp; \\text{if } 3 \\leq H\u0026rsquo; \u0026lt; 4\\\\ (X, 0, C) \u0026amp; \\text{if } 4 \\leq H\u0026rsquo; \u0026lt; 5\\\\ (C, 0, X) \u0026amp; \\text{if } 5 \\leq H\u0026rsquo; \u0026lt; 6\\\\ \\end{cases} \\end{equation*}\nThe final RGB value can be calculated by adding the difference between the value and chroma to each pixel:\n\\[ m = V - C \\]\n\\[ (R, G, B) = (R\u0026rsquo; + m, G\u0026rsquo; + m, B\u0026rsquo; + m). \\]\nSome Examples Given an original image (below), we\u0026rsquo;ll view the output of changing the Hue, Saturation, and Value.\nFigure 15: Original image. Credit: The Expanse Reducing the Hue by 20 produces the following image:\nFigure 16: Image with Hue subtracted by 20 degrees. Credit: The Expanse Given our working knowledge of how hue is computed, this makes sense. The previous angle clearly pointed to lighter blue colors, reducing that by \\(20^{\\circ}\\) moves us towards the green edge.\nNow let\u0026rsquo;s take the original image and increase saturation by 0.3:\nFigure 17: Image with Saturation increased by 0.3. Credit: The Expanse All colors across the board look \u0026ldquo;richer\u0026rdquo; and \u0026ldquo;deeper\u0026rdquo;. This corresponds with the definition of the HSV cylinder.\nFinally, we\u0026rsquo;ll view an image in which the value was modified. Let\u0026rsquo;s increase the values by 0.2:\nFigure 18: Image with Values increased by 0.2. Credit: The Expanse This looks washed out. All of the pixels with 0 values were increased uniformly. Perhaps we could clamp that by setting all pixels with the given value change back to their original values.\n","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642831200,"objectID":"6362302a6a4c5b2ac83d14e7074f3e30","permalink":"https://ajdillhoff.github.io/notes/color/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/color/","section":"notes","summary":"Table of Contents Topics The Human Eye Color Matching Color Physics Color Spaces HSV Color Space Topics What is color? How do we process color? What information does color contain? What can we infer from color? The Human Eye The eye acts as a camera, including a lens which focuses light onto a receptive surface. The cornea covers the lens which combine to make a compound lens. The lens itself is flexible to allow the eye to focus on objects of variable distance.","tags":["computer vision"],"title":"Color","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Orientation Histograms Histogram of Oriented Gradients Introduction Key Questions\nWhy are these necessary? What limitations do they address that corner interest points cannot? Although the original HOG Paper came out after SIFT, it is much simpler to describe the process (Dalal and Triggs 2005). Histograms of Oriented Gradients are feature vectors that are generated by evaluating gradients within a local neighborhood of interest points.\nOrientation Histograms This approach depends on building orientation histograms. For each pixel in the original image, construct a histogram of gradient orientations of all pixels within a square window. The gradient orientations of each pixel are easily calculated following the approach used for Edge Detection. This transformation can be flattened into a single vector that is used to compare images via L2 distance or some other metric.\nFigure 1: Orientation histograms of hand images. The pseudocode to generate orientation histograms is shown below.\nlet w be the window_size let h be half the window_size let norms be the gradient norms of the input image for each pixel let angles be the computed orientations of the gradient vectors for each pixel for each pixel (i, j): create a histogram of orientations with b bins weight the orientations of the bins based on the gradient norm The histograms of each local feature are translation invariant. A histogram of gradient orientations for a feature in one image should be the same as one generated to a similar feature in another image.\nThis approach is not scale invariant.\nHistogram of Oriented Gradients Dalal and Triggs propose a feature extraction method based on orientation histograms (Dalal and Triggs 2005). In their work published at CVPR, they evaluate their features by training a SVM for pedestrian detection on a standard (at that time) benchmark. They evaluate on a person detector using the metric False Positives Per Window (FPPW). This can be calculated as num_fp / num_windows. This represents a tradeoff between the number of false positives and the number false negatives. Intuitively, lowering the threshold for detection will generate more false positives, but will also reduce the number of false negatives.\nComputing HoG Normalize for color and gamma values. Compute the gradient image. Extract a window of some size. Divide window into sub-grid Compute orientation histogram of each cell. Concatentate the four histograms. Normalize the feature vector. During the binning step, each pixel provides a weighted vote for the histogram based on the orientation of the gradient element it centered on. This vote is weighted based on a function of the gradient magnitude.\nIn the paper, they experiment with a wide range of different parameters. They show that optimal performance coincides with choosing 4 cells per window with 9 orientation bins.\nNormalization Schemes There were several normalizion schemes addressed in the paper. The normalization scheme picked based on lowest FPPW is L2-Hys:\nNormalize the concatenated vector. Clip values to 0.2 Normalize again. They also evaulated the features with L2, L1, and L1-sqrt.\nFigure 2: Evaluation of normalization approaches (Dalal and Triggs, 2005). References Dalal, N., and B. Triggs. 2005. “Histograms of Oriented Gradients for Human Detection.” In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05), 1:886–93 vol. 1. https://doi.org/10.1109/CVPR.2005.177. ","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706400000,"objectID":"9431ad211383388f296068dfc3552bc9","permalink":"https://ajdillhoff.github.io/notes/histogram_of_oriented_gradients/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/histogram_of_oriented_gradients/","section":"notes","summary":"Table of Contents Introduction Orientation Histograms Histogram of Oriented Gradients Introduction Key Questions\nWhy are these necessary? What limitations do they address that corner interest points cannot? Although the original HOG Paper came out after SIFT, it is much simpler to describe the process (Dalal and Triggs 2005). Histograms of Oriented Gradients are feature vectors that are generated by evaluating gradients within a local neighborhood of interest points.\nOrientation Histograms This approach depends on building orientation histograms.","tags":["computer vision"],"title":"Histogram of Oriented Gradients","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Detecting Corners Describing Image Patches Scale Invariance Introduction Why do we care about image features? One of the main goals of computer vision is understanding of some environment through visual perception. In order to summarize a visual object, we need some description of it. These descriptions can come in many forms, so we need to articulate some goals as to what we are ultimately looking for when describing an image.\nWhat makes an interesting feature in an image?\nSomething distinct Invariance properties (translation, rotation, scaling) Easy to compute Image features are the building blocks of many higher level applications such as\nStereo correspondence Image stitching Object recognition and detection Figure 1: Patches taken from two images from different perspectives. Some patches are more descriptive than others. Source: Szeliski Figure 2: Objects detected using YOLOv3. Source: Wikipedia. Figure 3: Image stitching result. The red lines show the seams at which the images are joined. Source: Wikipedia. Topics:\nCorners HOG SIFT Correlation with template PCA We have talked about Edge Detection, which produces an image of edge pixels given some raw input. Edges are certainly useful features, but are they distinct enough to produce consistent image features?\nConsider an image patch detected from three different primitives:\nPoints Edges Corners Figure 4: Aperture problem for patches detected from different primitives. Source: Szeliski. The above figure illustrates the aperture problem. Consider a flat surface without texture. If we generate a patch around any arbitrary point, it will have many correspondences with other patches. It may be obvious that picking any arbitrary point on a flat, single-colored surface would not be descriptive enough to match with anything useful.\nWhat about an edge? An edge is distinct based on its orientation. The middle image in the figure above shows that, while some ambiguity has been resolved, there are still a wide range of possible locations that it could be matched to. There could be many such edges found between images.\nThis brings us to a corner. A corner has two distinct gradient changes which make it a perfect candidate for interest point detection.\nDetecting Corners Paper Link\nCorners are a great choice for a feature. They are small, rotation and translation invariant, and can be computed simply from the gradient images we have computed before.\nThere are a few distinct interest points of a violin. Maybe we can use a corner detector to come up with patches which can be reproduced across different images.\nFigure 5: A replica violin. In a local window, a corner exhibits a large change in orientation. A flat surface has no orientation response at all. An edge only has an orientation in one direction.\nHow can we detect such changes in an image?\nThe sum of square differences (SSD). If we take some window and move it over some image, taking a the SSD at each point, we can produce an image of responses. SSD is defined as\n\\[ f(x, y) = \\sum_{(u, v) \\in W}\\big(I(x + u, y + v) - I(u, v)\\big)^2. \\]\nThis difference was previously used to evaluate discrete steps. Harris et al. note this as a limitation and instead aim to evaluate all possible small shifts about the origin of the shift. This is accomplished through analytic expansion of the term \\(I(x + u, y + v)\\).\nThrough Taylor expansion, this can be approximated as\n\\begin{align*} I(x + u, y + v) \u0026amp;= I(u, v) + x \\frac{\\partial}{\\partial x}I(x, y) + y \\frac{\\partial}{\\partial y}I(x, y) + O(x^2, y^2) \\\\ \u0026amp;\\approx I(u, v) + xI_x + yI_y \\end{align*}\nIn the above approximation, \\(O(x^2, y^2)\\) describes the upper bound of the behavior of the function. Through Taylor expansion, we could write the higher order terms. However, we only care about small shifts about the shift origin, so only a first order, or linear, approximation is sufficient.\nUsing this first order approximation, SSD can be written\n\\begin{align*} f(x, y) \u0026amp;\\approx \\sum_{(u, v) \\in W} w(u, v) \\big(I(u, v) + xI_x + yI_y - I(u,v)\\big)^2\\\\ \u0026amp;= \\sum_{(u, v) \\in W} w(u, v) \\big(xI_x + yI_y\\big)^2\\\\ \u0026amp;= \\sum_{(u, v) \\in W} w(u, v) \\big(x^2I_x^2 + 2xyI_xI_y + y^2I_y^2\\big) \\end{align*}\nThe term \\(x^2I_x^2 + 2xyI_xI_y + y^2I_y^2\\) is a linear combination and can be efficiently computed via matrix multiplication.\n\\begin{equation*} x^2I_x^2 + 2xyI_xI_y + y^2I_y^2 = \\begin{bmatrix} x \u0026amp; y \\end{bmatrix} \\begin{bmatrix} I_x^2 \u0026amp; I_x I_y \\\\ I_x I_y \u0026amp; I_y^2\\\\ \\end{bmatrix} \\begin{bmatrix} x\\\\ y \\end{bmatrix} \\end{equation*}\nWe can now rewrite the original SSD as follows.\n\\begin{equation*} f(x, y) \\approx \\begin{bmatrix} x \u0026amp; y \\end{bmatrix} M \\begin{bmatrix} x\\\\ y \\end{bmatrix}, \\end{equation*}\nwhere\n\\begin{equation*} M = \\sum_{(u, v) \\in W} w(u, v) H. \\end{equation*}\n\\begin{equation*} H = \\begin{bmatrix} I_x^2 \u0026amp; I_x I_y\\\\ I_x I_y \u0026amp; I_y^2 \\\\ \\end{bmatrix} \\end{equation*}\n\\(M\\) is then an autocorrelation matrix. The benefit of this formulation is that \\(M\\) is a symmetric matrix. If we remember our studies from linear algebra, we remember that there are some very important properties and characteristics of symmetric matrices.\nFigure 6: Gradient image (I_x^2). Figure 7: Gradient image (I_x I_y). Figure 8: Gradient image (I_y^2). Figure 9: Gradient change in both (x) and (y). Credit: David Jacobs First, lets consider a simple case of detecting the following corner. At this orientation, the changes in gradient are only in the vertical and horizontal directions. If we consider the matrix \\(M\\) from above, we would get the following result\n\\begin{equation*} M = \\begin{bmatrix} \\sum I_x^2 \u0026amp; \\sum I_x I_y\\\\ \\sum I_x I_y \u0026amp; \\sum I_y^2 \\end{bmatrix} = \\begin{bmatrix} \\lambda_1 \u0026amp; 0\\\\ 0 \u0026amp; \\lambda_2 \\end{bmatrix}. \\end{equation*}\nThe off-diagonal entries will be 0, by definition of the dot product and orthogonal vectors. The entries on the main diagonal will represent the eigenvalues of \\(M\\). If both entries on the main diagonal are large, this would indicate a large change in orientation within the window.\nWhat if the corner is not as ideal?\nFigure 10: Eigenvalue analysis of autocorrelation matrix. Source: Szeliski. Relationship to Eigenvalues\nRecall the Spectral Theorem for Symmetric Matrices, which states: A symmetric matrix \\(A \\in \\mathbb{R}^{n\\times n}\\) has the following properties:\nA has \\(n\\) real eigenvalues, counting multiplicities. The dimension of the eigenspace for each eigenvalue \\(\\lambda\\) equals the multiplicity of \\(\\lambda\\) as a root of the characteristic equation. The eigenspaces are mutually orthogonal, in the sense that eigenvectors corresponding to different eigenvalues are orthogonal. \\(A\\) is orthogonally diagonalizable. Symmetric matrices are orthogonally diagonalizable. Thus, a symmetric matrix \\(A\\) can be written as \\(A = PDP^{-1}\\), where the columns of \\(P\\) are the eigenvectors and \\(D\\) are the corresponding eigenvalues. Another perspective of this is that \\(A\\) is an ellipse with axis lengths determined by the eigenvalues (diagonal entries of \\(D\\)) rotated by \\(P\\).\nThe eigenvalues of \\(M\\) can be classified into different regions depending on if they are indicative of a flat region, edge, or corner.\nFigure 11: Classification of responses. Source: Harris (1988). Performing eigendecomposition seems cumbersome in this case. There must be a simpler way we could compute these responses.\nWe can then approximate this response!\n\\begin{align*} R \u0026amp;= \\det H - \\alpha \\cdot \\textrm{tr}(H)^2\\\\ \u0026amp;= I_x^2 \\cdot I_y^2 - (I_x I_y)^2 - \\alpha\\big(I_x^2 + I_y^2\\big)^2 \\end{align*}\nIf there is a corner, the gradient values will depict orthogonality. That is, the middle term in the equation above will be smaller. This results in a larger response.\nThe larger the value from the middle term, the less orthogonality is present. This results in a smaller response. In practice, we will see a negative response.\nIn practice, \\(\\alpha \\in [0.04, 0.06]\\).\nFigure 12: Response image (R). Window Selection What is the best window to choose when computing responses across an image? Harris et al. considered this in their original formulation when comparing to Moravec\u0026rsquo;s corner detection function. Using a flat window with uniform values produces a binary response when over interest points and 0 everywhere else. This can be written as\n\\begin{equation*} M = \\sum_{u, v} w(u, v) \\begin{bmatrix} I_x^2 \u0026amp; I_x I_y\\\\ I_x I_y \u0026amp; I_y^2 \\end{bmatrix} \\end{equation*}\nFigure 13: Uniform response window. Results in 1 for interest points inside the window, 0 otherwise. Credit: Fei-Fei Li Instead, Harris et al. propose using a circular Gaussian window which can be computed as\n\\begin{equation*} M = g(\\sigma) * \\begin{bmatrix} I_x^2 \u0026amp; I_x I_y\\\\ I_x I_y \u0026amp; I_y^2 \\end{bmatrix} \\end{equation*}\nFigure 14: Gaussian window response. Credit: Fei-Fei Li The Guassian window is easy enough to compute and has the added bonus of making the responses rotation invariant!\nNonmaxima Suppression (Again) We now have a response image in which each pixel gives an indication as to whether a corner has been detected. To thin out these hypotheses, we will need to suppress neighborhood values that are not maximal. Just like with Edge Detection, we will need to employ nonmaxima suppression.\nBefore applying that, we may choose to threshold the image to filter out points that are obviously not candidates.\nThe approach is quite simple here:\nSlide a \\(3 \\times 3\\) window across the image. If the center pixel is not the maximum value in the \\(3 \\times 3\\) window, set it to 0. Continue until all pixels are evaluated. Our final result is shown below. The detected corners are marked.\nFigure 15: Final output of corner detection. Describing Image Patches Given a list of interest points, we can start to build a collection of regions or patches surrounding the point which are useful for feature matching. The simple choice here is to take a fixed size patch surrounding an interest point and use it as a template. We can then compare that to other interest points in different images to see how well they score. There are many limitations to this naive approach that prevent it from working well in general. Even if the perspective of the object is the same in multiple images, slight changes is brightness could affect the matching scores greatly.\nAnother consideration is scale. If we have an interest point at one scale, will it be detected when that image is scaled by some factor \\(k\\)?\nFigure 16: Patch surrounding a similar interest point at different scales. Credit: Kristen Grauman, B. Liebe. However we choose to represent the information surrounding an interest point need to be robust to translation, scale, and orientation changes. It is important to note that, although the Harris corner detector is invariant to orientation, a naive image patch surrounding the interest point may not be.\nScale Invariance Is the Harris corner detector scale-invariant?\nNo! Consider the figure below.\nFigure 17: Harris corner detector is not scale invariant. Credit: Kristen Grauman The output from the detector will be different depending on scale. One solution would be to resize the image over several different scales and consolidate the detections. Doing this would produce many features and take much longer to compute.\nGiven some feature template that is centered on an interest point, we would expect that size of the patch scales with the scale change of the image itself. This property will drive the development of a scale-invariant method. We select the size of the patch by placing some dark blob with a light background (or vice versa) over the interest point and then selecting the size which provides the greatest response.\nLaplacian Filter A good choice for this is the Laplacian filter. The Laplacian of a 2D function is\n\\begin{equation*} (\\nabla^2 f)(x, y) = \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2}. \\end{equation*}\nThe Laplacian filter is created following the derivation in the two figures below.\nFigure 18: Deriving second partial derivative filters for x and y. Source: https://theailearner.com/2019/05/25/laplacian-of-gaussian-log/ Figure 19: Combining the x and y filters. Source: https://theailearner.com/2019/05/25/laplacian-of-gaussian-log/ It is also common to smooth the operator before use. This can be done by convolving with a Gaussian kernel:\n\\begin{equation*} K_{\\nabla^2} * G_{\\sigma}. \\end{equation*}\nThis is referred as the Laplacian of Gaussian filter.\nScale Space We can use the Laplacian of Gaussian to find the appropriate size of image patch for a given scale. This is achieved by computing a scale-space representation of an image. When we resize an image to make it smaller, there is a loss of information. Similarly, blurring the image causes a loss of information. The larger the \\(\\sigma\\) value for the Gaussian, the more information that is lost. Thus, we can quickly compute different scale-image representations by applying Gaussian blurring with a range of \\(\\sigma\\) values.\nFigure 20: Scale space representations. Source: Wikipedia As it turns out, blurring and resizing correspond with each other. This is calculated by applying a Gaussian blur to the image following:\n\\begin{equation*} L_f(\\sigma) = f(G_\\sigma * I). \\end{equation*}\nThis is the response of function \\(f\\) in scale-space \\(\\sigma\\).\nFigure 21: Selecting features at different scales. Credit: Kristen Grauman The size of the patch can be found by iterating through different values of \\(\\sigma\\), applying the Laplacian at each scale, and selecting the value of \\(\\sigma\\) which produced the greatest result.\nConsider a simple rounded square as depicted below.\nFigure 22: Simple rounded square. If we apply the LoG filter to select the scale which gives the greatest response for the region centered on the top left corner at the original scale, we produce the following graph.\nFigure 23: Responses of LoG at the top left corner from (sigma = 0) to (sigma = 8). If we scale the original image by 2 and apply the same analysis again, we get the following graph.\nFigure 24: Responses of LoG at original image size at the top left corner from (sigma = 0) to (sigma = 8). To summarize, using this method will allow us to select the appropriate scale at which our interest point provides the strongest response. However, the cost of this search is high. As we increase the size of the filters, the more work required for each convolution.\n","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706400000,"objectID":"ac2c64c22d328b427898fcae31172278","permalink":"https://ajdillhoff.github.io/notes/image_features/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/image_features/","section":"notes","summary":"Table of Contents Introduction Detecting Corners Describing Image Patches Scale Invariance Introduction Why do we care about image features? One of the main goals of computer vision is understanding of some environment through visual perception. In order to summarize a visual object, we need some description of it. These descriptions can come in many forms, so we need to articulate some goals as to what we are ultimately looking for when describing an image.","tags":["computer vision"],"title":"Image Features","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Dual Representation Relating Back to the Original Formulation Types of Kernels Constructing Kernels RBF maps to infinite-dimensional space Slides for these notes can be found here.\nIntroduction Notebook link: https://github.com/ajdillhoff/CSE6363/blob/main/svm/kernels.ipynb\nParametric models use training data to estimate a set of parameters that can then be used to perform inference on new data. An alternative approach uses nonparametric methods, meaning the function is estimated directly from the data instead of optimizing a set of parameters.\nOne possible downside to such an approach is that it becomes less efficient as the amount of training data increases. Additionally, the transformation into a feature space such that the data becomes linearly separable may be intractable. Consider sequential data such as text or audio. If each sample has a variable number of features, how do we account for this using standard linear models with a fixed number of parameters?\nThe situations described above can be overcome through the use of the kernel trick. We will see that, by computing a measure of similarity between samples in the feature space, we do not need to directly transform each individual sample to that space.\nA kernel function is defined as\n\\[ k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = \\phi(\\mathbf{x})^T \\phi(\\mathbf{x}\u0026rsquo;), \\]\nwhere \\(\\phi\\) is some function which transforms the input to a feature space.\nMethods that require part or all of the training data to make prediction will benefit from using kernel representations, especially when using high dimensional data. Instead of transforming the data into a high dimensional space which may be computationally intractable, a measure of similarity via the inner product is used. The inner product is not the projection into some space. Instead, it represents the outcome of that projection.\nIf the input vector takes on the form of scalar products, it can be represented as a kernel function.\nDual Representation The key to taking advantage of the kernel trick relies on reformulating our linear model into a dual representation. In this form, we will establish a dependence on the kernel function.\nThe following derivation of the dual representation for linear regression follows (Bishop). Consider the least squares loss with \\(L2\\) regularization, as we discussed with Linear Regression.\n\\[ J(\\mathbf{w}) = \\frac{1}{2}\\sum_{i=1}^n(\\mathbf{w}^T\\phi(\\mathbf{x}_i) - y_i)^2 + \\frac{\\lambda}{2} \\mathbf{w}^T \\mathbf{w} \\]\nHere, \\(\\phi\\) is a basis function that transforms the input. This could also be a simple identity function in which \\(\\phi(\\mathbf{x}) = \\mathbf{x}\\). To solve for \\(\\mathbf{w}\\), we take the gradient of \\(J(\\mathbf{w})\\) with respect to \\(\\mathbf{w}\\) and set it to 0.\n\\begin{align*} \\nabla_{\\mathbf{w}}J(\\mathbf{w}) \u0026amp;= \\sum_{i=1}^n(\\mathbf{w}^T\\phi(\\mathbf{x}_i) - y_i)\\phi(\\mathbf{x}_i) + \\lambda \\mathbf{w}\\\\ \\implies \\mathbf{w} \u0026amp;= -\\frac{1}{\\lambda}\\sum_{i=1}^n(\\mathbf{w}^T\\phi(\\mathbf{x}_i) - y_i)\\phi(\\mathbf{x}_i) \\end{align*}\nWe can formulate this as a matrix-vector product by letting\n\\begin{equation*} \\mathbf{\\Phi} = \\begin{bmatrix} \\phi(\\mathbf{x}_1)^T\\\\ \\vdots \\\\ \\phi(\\mathbf{x}_n)^T\\\\ \\end{bmatrix} \\text{ and } a_{i} = -\\frac{1}{\\lambda}(\\mathbf{w}^T\\phi(\\mathbf{x}_i) - y_i). \\end{equation*}\nThen, \\(\\mathbf{w} = \\mathbf{\\Phi}^T\\mathbf{a}\\), where \\(\\mathbf{a} = [a_1, \\dots, a_n]^T\\).\nThe dual representation is derived by reformulating \\(J(\\mathbf{w})\\) in terms of \\(\\mathbf{a}\\).\n\\begin{equation*} J(\\mathbf{a}) = \\frac{1}{2}\\mathbf{a}^T\\mathbf{\\Phi}\\mathbf{\\Phi}^T\\mathbf{\\Phi}\\mathbf{\\Phi}^T\\mathbf{a} - \\mathbf{a}^T\\mathbf{\\Phi}\\mathbf{\\Phi}^T\\mathbf{y} + \\frac{1}{2}\\mathbf{y}^T\\mathbf{y} + \\frac{\\lambda}{2} \\mathbf{a}^T\\mathbf{\\Phi}\\mathbf{\\Phi}^T\\mathbf{a}, \\end{equation*}\nwhere \\(\\mathbf{y} = [y_1, \\dots, y_n]\\).\nLooking at the products \\(\\mathbf{\\Phi}\\mathbf{\\Phi}^T\\), we see that these relate to our original kernel form: \\(\\phi(\\mathbf{x}_i)^T\\phi(\\mathbf{x}_j)\\). This product defines a Gram matrix \\(\\mathbf{K} = \\mathbf{\\Phi}\\mathbf{\\Phi}^T\\) whose elements are \\(k(\\mathbf{x}_i, \\mathbf{x}_j)\\). Thus, we can rewrite \\(J(\\mathbf{a})\\) as\n\\begin{equation*} J(\\mathbf{a}) = \\frac{1}{2}\\mathbf{a}^T\\mathbf{K}\\mathbf{K}\\mathbf{a} - \\mathbf{a}^T\\mathbf{K}\\mathbf{y} + \\frac{1}{2}\\mathbf{y}^T\\mathbf{y} + \\frac{\\lambda}{2}\\mathbf{a}^T\\mathbf{K}\\mathbf{a}. \\end{equation*}\nSolving for \\(\\mathbf{a}\\) can be done by computing the gradient of \\(J(\\mathbf{a})\\) with respect to \\(\\mathbf{a}\\) and setting the result to 0.\n\\begin{align*} \\nabla_\\mathbf{a}J(\\mathbf{a}) = \\mathbf{K}\\mathbf{K}\\mathbf{a} - \\mathbf{K}\\mathbf{y} + \\lambda \\mathbf{K}\\mathbf{a} \u0026amp;= 0\\\\ \\mathbf{K}\\mathbf{a} + \\lambda I\\mathbf{a} - \\mathbf{y} \u0026amp;= 0\\\\ (\\mathbf{K} + \\lambda I)\\mathbf{a} \u0026amp;= \\mathbf{y}\\\\ \\mathbf{a} \u0026amp;= (\\mathbf{K} + \\lambda I)^{-1} \\mathbf{y}. \\end{align*}\nWith \\(\\mathbf{a}\\) solved, we can complete the dual representation of our original linear regression model. Recall that\n\\begin{equation*} h(\\mathbf{x}; \\mathbf{w}) = \\mathbf{w}^T\\phi(\\mathbf{x}). \\end{equation*}\nIf we substitute \\(\\mathbf{w} = \\mathbf{\\Phi}^T\\mathbf{a}\\), we get\n\\begin{align*} f(\\mathbf{x};\\mathbf{a}) \u0026amp;= \\mathbf{a}^T\\mathbf{\\Phi}\\phi(\\mathbf{x})\\\\ \u0026amp;= \\Big[(\\mathbf{K} + \\lambda I)^{-1}\\mathbf{y})\\Big]^T\\mathbf{\\Phi}\\phi(\\mathbf{x}). \\end{align*}\nAgain, the kernel form is apparent in the product \\(\\mathbf{\\Phi}\\phi(\\mathbf{x})\\). If we let \\(k_i(\\mathbf{x}) = k(\\mathbf{x}_i,\\mathbf{x})\\) and\n\\begin{equation*} \\mathbf{k}(\\mathbf{x}) = \\begin{bmatrix} k_1(\\mathbf{x})\\\\ \\vdots \\\\ k_n(\\mathbf{x}) \\end{bmatrix}, \\end{equation*}\nwe can write the dual representation of our linear regression model as\n\\begin{equation*} f(\\mathbf{x}) = \\mathbf{k}(\\mathbf{x})^T(\\mathbf{K} + \\lambda \\mathbf{I})^{-1}\\mathbf{y}. \\end{equation*}\nRelating Back to the Original Formulation In this dual formulation, the solution for \\(\\mathbf{a}\\) can be expressed as a linear combination of elements \\(\\phi(\\mathbf{x})\\). From above, we see that\n\\[ a_i = -\\frac{1}{\\lambda}\\big(\\mathbf{w}^T\\phi(\\mathbf{x}_i) - y_i\\big). \\]\nExpanding this into individual coefficients yields\n\\begin{align*} a_i \u0026amp;= -\\frac{1}{\\lambda}\\big(w_1\\phi_1(\\mathbf{x}_i) + \\cdots + w_m \\phi_m(\\mathbf{x}_i) - y_i\\big)\\\\ \u0026amp;= -\\frac{w_1}{\\lambda}\\phi_1(\\mathbf{x}_i) - \\cdots - \\frac{w_m}{\\lambda} \\phi_m(\\mathbf{x}_i) + \\frac{y_i}{\\lambda}. \\end{align*}\nWe are close, but we still need to do something about the term \\(\\frac{y_i}{\\lambda}\\). For this, we can multiply both sides of our equation by a convenient 1. That is, we multiply by\n\\[ \\frac{\\phi_1(\\mathbf{x}_i) + \\cdots + \\phi_m(\\mathbf{x}_i)}{\\phi_1(\\mathbf{x}_i) + \\cdots + \\phi_m(\\mathbf{x}_i)}. \\]\nBy doing this and grouping the \\(\\phi_j\\) terms, we get\n\\begin{align*} \u0026amp;\\Big(\\frac{y_i}{\\lambda}\\cdot \\frac{1}{\\phi_1(\\mathbf{x}_i) + \\cdots + \\phi_m(\\mathbf{x}_i)} - \\frac{w_1}{\\lambda}\\Big)\\phi_1(\\mathbf{x}_i) + \\cdots\\\\ \u0026amp;+ \\Big(\\frac{y_i}{\\lambda}\\cdot \\frac{1}{\\phi_1(\\mathbf{x}_i) + \\cdots + \\phi_m(\\mathbf{x}_i)} - \\frac{w_m}{\\lambda}\\Big)\\phi_m(\\mathbf{x}_i). \\end{align*}\nWe can simplify this by introducing a term\n\\[ c_i = \\frac{y_i}{\\lambda}\\cdot \\frac{1}{\\phi_1(\\mathbf{x}_i) + \\cdots + \\phi_m(\\mathbf{x}_i)}. \\]\nThen the solution can be rewritten as\n\\[ \\Big(c_i - \\frac{w_1}{\\lambda}\\Big)\\phi_1(\\mathbf{x}_i) + \\cdots + \\Big(c_i - \\frac{w_m}{\\lambda}\\Big)\\phi_m(\\mathbf{x}_i). \\]\nWith this, we can step backwards using intermediate results in the previous section to get back to the original formulation of our linear regression model.\nTypes of Kernels There are several types of kernels that can be used to transform the input data depending on the problem. The simplest kernel is the identity kernel:\n\\[ k(\\mathbf{x}, \\mathbf{x\u0026rsquo;}) = \\mathbf{x}^T \\mathbf{x\u0026rsquo;}. \\]\nPolynomial Kernel A polynomial kernel is defined as\n\\[ k(\\mathbf{x}, \\mathbf{x\u0026rsquo;}) = (\\mathbf{x}^T\\mathbf{x\u0026rsquo;}+c)^d. \\]\nThis is a common choice for solving problems akin to polynomial regression. We can use this kernel to present a visual explanation of kernel functions. Consider the following dataset.\nFigure 1: Binary classification dataset that is not linearly separable. It is easy enough to see that this dataset could not be separated using a hyperplane in 2D. We could separate the two using some nonlinear decision boundary like a circle. If we could transform this into 3D space, we could come up with some features such that it is linearly separable in 3D. For example, let \\(\\phi(\\mathbf{x}) = (x_1^2, x_2^2, \\sqrt{2}x_1x_2)\\).\nTransforming all points and visualizing yields the figure below.\nFigure 2: Binary classification dataset transformed into a 3D feature space. From this perspective, we can clearly see that the data is linearly separable. The question remains: if we only have the original 2D features, how do we compare points in this 3D features space without explicitly transforming each point? The kernel function corresponding to the feature transform above is\n\\begin{align*} k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) \u0026amp;= (\\mathbf{x}^T\\mathbf{x}\u0026rsquo;)^2\\\\ \u0026amp;= (x_1x\u0026rsquo;_1 + x_2x\u0026rsquo;_2)^2\\\\ \u0026amp;= 2x_1x\u0026rsquo;_1x_2x\u0026rsquo;_2 + (x_1x\u0026rsquo;_1)^2 + (x_2x\u0026rsquo;_2)^2\\\\ \u0026amp;= \\phi(\\mathbf{x})^T \\phi(\\mathbf{x}\u0026rsquo;) \\end{align*}\nwhere\n\\[ \\phi(\\mathbf{x}) = \\begin{bmatrix} \\sqrt{2}x_1x_2\\\\ x_1^2\\\\ x_2^2 \\end{bmatrix}. \\]\nRadial Basis Function Kernel This kernel follows a Gaussian term and is commonly used with SVMs. It is defined as\n\\[ k(\\mathbf{x}, \\mathbf{x\u0026rsquo;}) = \\exp\\Big(-\\frac{\\|\\mathbf{x}-\\mathbf{x\u0026rsquo;}\\|^2}{2\\sigma^2}\\Big). \\]\nCosine Similarity Consider the problem of comparing text sequences for a document classification task. One approach is to compare the number of occurrences of each word. The idea is that documents that are similar will have a similar number of words that occur.\n\\[ k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = \\frac{\\mathbf{x}^T \\mathbf{x}\u0026rsquo;}{\\|\\mathbf{x}\\|_2 \\|\\mathbf{x}\u0026rsquo;\\|_2} \\]\nDocuments that are orthogonal, in the sense that the resulting cosine similarity is 0, are dissimilar. The similarity increases as the score approaches 1. There are several issues with this approach which are addressed by using the term frequence-inverse document frequency (TF-IDF) score.\nConstructing Kernels A valid kernel function must satisfy the following conditions:\nSymmetry: \\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = k(\\mathbf{x}\u0026rsquo;, \\mathbf{x})\\) Positive semi-definite: \\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) \\geq 0\\) If the feature space can be represented as a dot product, then it will satisfy the first condition by definition. The second condition can be shown by constructing a Gram matrix \\(\\mathbf{K}\\) and showing that it is positive semi-definite. A matrix \\(\\mathbf{K}\\) is positive semi-definite if and only if \\(\\mathbf{v}^T\\mathbf{K}\\mathbf{v} \\geq 0\\) for all \\(\\mathbf{v} \\in \\mathbb{R}^n\\).\nDirect Construction of a Kernel In this approach, we define a feature space \\(\\phi(\\mathbf{x})\\) and then compute the kernel function as\n\\[ k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = \\phi(\\mathbf{x})^T \\phi(\\mathbf{x}\u0026rsquo;). \\]\nThis is the approach used in the example from above. In that example, we used the kernel function \\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = (\\mathbf{x}^T\\mathbf{x}\u0026rsquo;)^2\\). For our 2D input, the feature space is \\(\\phi(\\mathbf{x}) = (x_1^2, x_2^2, \\sqrt{2}x_1x_2)\\). It is easy to see that the kernel function is the dot product of the feature space, and its kernel matrix is positive semi-definite.\nConstruction from other valid kernels As a more convenient approach, it is possible to construct complex kernels from known kernels. Given valid kernels \\(k_1(\\mathbf{x}, \\mathbf{x}\u0026rsquo;)\\) and \\(k_2(\\mathbf{x}, \\mathbf{x}\u0026rsquo;)\\), we can construct a new kernel \\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;)\\) using the following operations:\n\\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = ck_1(\\mathbf{x}, \\mathbf{x}\u0026rsquo;)\\) for \\(c \u0026gt; 0\\) \\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = f(\\mathbf{x})k_1(\\mathbf{x}, \\mathbf{x}\u0026rsquo;)f(\\mathbf{x}\u0026rsquo;)\\) for \\(f(\\mathbf{x})\\) \\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = k_1(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) + k_2(\\mathbf{x}, \\mathbf{x}\u0026rsquo;)\\) \\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = k_1(\\mathbf{x}, \\mathbf{x}\u0026rsquo;)k_2(\\mathbf{x}, \\mathbf{x}\u0026rsquo;)\\) \\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = \\exp(k_1(\\mathbf{x}, \\mathbf{x}\u0026rsquo;))\\) \\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = \\tanh(k_1(\\mathbf{x}, \\mathbf{x}\u0026rsquo;))\\) RBF maps to infinite-dimensional space It can be shown that the RBF kernel maps the input to an infinite-dimensional space. This is a result of the Taylor series expansion of the exponential function. The RBF kernel is defined as\n\\[ k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = \\exp\\Big(-\\frac{\\|\\mathbf{x}-\\mathbf{x\u0026rsquo;}\\|^2}{2\\sigma^2}\\Big). \\]\nThe Taylor series expansion of the exponential function is\n\\[ \\exp(x) = \\sum_{n=0}^\\infty \\frac{x^n}{n!}. \\]\nSubstituting the RBF kernel into the Taylor series expansion yields\n\\[ \\exp\\Big(-\\frac{\\|\\mathbf{x}-\\mathbf{x\u0026rsquo;}\\|^2}{2\\sigma^2}\\Big) = \\sum_{n=0}^\\infty \\frac{\\Big(-\\frac{\\|\\mathbf{x}-\\mathbf{x\u0026rsquo;}\\|^2}{2\\sigma^2}\\Big)^n}{n!}. \\]\nThis expansion can be viewed as an infnite sum of polynomial terms. A more formal proof of this result can be found here.\nThe benefit of this result is that it allows us to work in a high-dimensional space without explicitly transforming the input. This is especially useful when the input space is infinite-dimensional, such as with text data. It is also used to compare the similarity of documents without explicitly transforming the input into a high-dimensional space.\n","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706745600,"objectID":"83e1acfd29cf0148a76ae1f5d8926675","permalink":"https://ajdillhoff.github.io/notes/kernels/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/kernels/","section":"notes","summary":"Table of Contents Introduction Dual Representation Relating Back to the Original Formulation Types of Kernels Constructing Kernels RBF maps to infinite-dimensional space Slides for these notes can be found here.\nIntroduction Notebook link: https://github.com/ajdillhoff/CSE6363/blob/main/svm/kernels.ipynb\nParametric models use training data to estimate a set of parameters that can then be used to perform inference on new data. An alternative approach uses nonparametric methods, meaning the function is estimated directly from the data instead of optimizing a set of parameters.","tags":["machine learning"],"title":"Kernels","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Gaussian Class Conditional Densities Decision Boundaries Maximum Likelihood Estimation Quadratic Descriminant Analysis Example Slides for these notes can be found here.\nIntroduction This section covers classification from a probabilistic perspective. The discriminative approach involves a parameterized function which assigns each input vector \\(\\mathbf{x}\\) to a specific class. We will see that modeling the conditional probability distribution \\(p(C_k|\\mathbf{x})\\) grants us additional benefits while still fulfilling our original classification task.\nLet\u0026rsquo;s begin with a 2 class problem. To classify this with a generative model, we use the class-conditional densities \\(p(\\mathbf{x}|C_i)\\) and class priors \\(p(C_i)\\). The posterior probability for \\(C_1\\) can be written in the form of a sigmoid function:\n\\begin{align*} p(C_1|\\mathbf{x}) \u0026amp;= \\frac{p(\\mathbf{x}|C_1)p(C_1)}{p(\\mathbf{x}|C_1)p(C_1) + p(\\mathbf{x}|C_2)p(C_2)} \\end{align*}\nThen multiply the numerator and denominator by\n\\begin{equation*} \\frac{(p(\\mathbf{x}|C_1))^{-1}}{(p(\\mathbf{x}|C_1))^{-1}}, \\end{equation*}\nwhich yields\n\\begin{equation*} \\frac{1}{1 + \\frac{p(\\mathbf{x}|C_2)p(C_2)}{p(\\mathbf{x}|C_1)p(C_1)}}. \\end{equation*}\nNoting that \\(a = \\exp(\\ln(a))\\), we can rewrite further\n\\begin{equation*} \\frac{1}{1 + \\exp(-a)}, \\end{equation*}\nwhere \\(a = \\ln \\frac{p(\\mathbf{x}|C_1)p(C_1)}{p(\\mathbf{x}|C_2)p(C_2)}\\).\nWriting this distribution in the form of the sigmoid function is convenient as it is a natural choice for many other classification models. It also has a very simple derivative which is convenient for models optimized using gradient descent.\nGiven certain choices for the class conditional densities, the posterior probabilty distribution will be a linear function of the input features:\n\\begin{equation*} \\ln p(C_k|\\mathbf{x};\\theta) = \\mathbf{w}^T \\mathbf{x} + c, \\end{equation*}\nwhere \\(\\mathbf{w}\\) is a parameter vector based on the parameters of the chosen probability distribution, and \\(c\\) is a constant term that is not dependent on the parameters. As we will see, the resulting model will take an equivalent form to the discriminative approach.\nGaussian Class Conditional Densities Let\u0026rsquo;s assume that our class conditional densities \\(p(\\mathbf{x}|C_k)\\) are Gaussian. We will additionally assume that the covariance matrices between classes are shared. This will result in linear decision boundaries. Since the conditional densities are chosen to be Gaussian, the posterior is given by\n\\begin{equation*} p(C_k|\\mathbf{x};\\theta) \\propto \\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_c,\\Sigma), \\end{equation*}\nwhere \\(\\pi_k\\) is the prior probability of class \\(k\\). We choose to ignore the normalizing constant since it is not dependent on the class.\nThe class conditional density function for class \\(k\\) is given by\n\\begin{equation*} p(\\mathbf{x}|C_k;\\theta) = \\frac{1}{2\\pi^{D/2}}\\frac{1}{|\\Sigma|^{1/2}}\\exp\\Big(-\\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu}_k)^T \\Sigma^{-1} (\\mathbf{x} - \\mathbf{\\mu}_k)\\Big). \\end{equation*}\nNow that we have a concrete function to work with, let\u0026rsquo;s go back to the simple case of two classes and define \\(a = \\ln \\frac{p(\\mathbf{x}|C_1)p(C_1)}{p(\\mathbf{x}|C_2)p(C_2)}\\). First, we rewrite \\(a\\):\n\\begin{equation*} a = \\ln p(\\mathbf{x}|C_1) - \\ln p(\\mathbf{x}|C_2) + \\ln \\frac{p(C_1)}{p(C_2)}. \\end{equation*}\nThe log of the class conditional density for a Gaussian is\n\\begin{equation*} \\ln p(\\mathbf{x}|C_k;\\mathbf{\\mu}_k,\\Sigma) = -\\frac{D}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\Sigma|-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu}_k)^T \\Sigma^{-1} (\\mathbf{x}-\\mathbf{\\mu}_k). \\end{equation*}\nTo simplify the above result, we will group the terms that are not dependent on the class parameters since they are consant:\n\\begin{equation*} \\ln p(\\mathbf{x}|C_k;\\mathbf{\\mu}_k,\\Sigma) = -\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu}_k)^T \\Sigma^{-1} (\\mathbf{x}-\\mathbf{\\mu}_k) + c. \\end{equation*}\nObserving that this quantity takes on a quadratic form, we can rewrite the above as\n\\begin{equation*} \\ln p(\\mathbf{x}|C_k;\\mathbf{\\mu}_k,\\Sigma) = -\\frac{1}{2}\\mathbf{\\mu}_k\\Sigma^{-1}\\mathbf{\\mu}_k + \\mathbf{x}^T \\Sigma^{-1} \\mathbf{\\mu}_k -\\frac{1}{2}\\mathbf{x}^T \\Sigma^{-1}\\mathbf{x} + c. \\end{equation*}\nUsing this, we complete the definition of \\(a\\):\n\\begin{align*} a \u0026amp;= \\ln p(\\mathbf{x}|C_1) - \\ln p(\\mathbf{x}|C_2) + \\ln \\frac{p(C_1)}{p(C_2)}\\\\ \u0026amp;= -\\frac{1}{2}\\mathbf{\\mu}_1\\Sigma^{-1}\\mathbf{\\mu}_1 + \\mathbf{x}^T \\Sigma^{-1} \\mathbf{\\mu}_1 + \\frac{1}{2}\\mathbf{\\mu}_2\\Sigma^{-1}\\mathbf{\\mu}_2 - \\mathbf{x}^T \\Sigma^{-1} \\mathbf{\\mu}_2 + \\ln \\frac{p(C_1)}{p(C_2)}\\\\ \u0026amp;= \\mathbf{x}^T(\\Sigma^{-1}(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)) - \\frac{1}{2}\\mathbf{\\mu}_1\\Sigma^{-1}\\mathbf{\\mu}_1 + \\frac{1}{2}\\mathbf{\\mu}_2\\Sigma^{-1}\\mathbf{\\mu}_2 + \\ln \\frac{p(C_1)}{p(C_2)}\\\\ \u0026amp;= (\\Sigma^{-1}(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2))^T \\mathbf{x} - \\frac{1}{2}\\mathbf{\\mu}_1\\Sigma^{-1}\\mathbf{\\mu}_1 + \\frac{1}{2}\\mathbf{\\mu}_2\\Sigma^{-1}\\mathbf{\\mu}_2 + \\ln \\frac{p(C_1)}{p(C_2)}. \\end{align*}\nFinally, we define\n\\begin{equation*} \\mathbf{w} = \\Sigma^{-1}(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2) \\end{equation*}\nand\n\\begin{equation*} w_0 = - \\frac{1}{2}\\mathbf{\\mu}_1\\Sigma^{-1}\\mathbf{\\mu}_1 - \\frac{1}{2}\\mathbf{\\mu}_2\\Sigma^{-1}\\mathbf{\\mu}_2 + \\ln \\frac{p(C_1)}{p(C_2)}. \\end{equation*}\nThus, our posterior takes on the form\n\\begin{equation*} p(C_1|\\mathbf{x};\\theta) = \\sigma(\\mathbf{w}^T \\mathbf{x} + w_0). \\end{equation*}\nMultiple Classes What if we have more than 2 classes? Recall that a generative classifier is modeled as\n\\[ p(C_k|\\mathbf{x};\\mathbf{\\theta}) = \\frac{p(C_k|\\mathbf{\\theta})p(\\mathbf{x}|C_k, \\mathbf{\\theta})}{\\sum_{k\u0026rsquo;}p(C_{k\u0026rsquo;}|\\mathbf{\\theta})p(\\mathbf{x}|C_{k\u0026rsquo;}, \\mathbf{\\theta})}. \\]\nAs stated above, \\(\\mathbf{\\pi}_k = p(C_k|\\mathbf{\\theta})\\) and \\(p(\\mathbf{x}|C_k,\\mathbf{\\theta}) = \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_c,\\Sigma)\\).\nFor LDA, the covariance matrices are shared across all classes. This permits a simplification of the class posterior distribution \\(p(C_k|\\mathbf{x};\\mathbf{\\theta})\\):\n\\begin{align*} p(C_k|\\mathbf{x};\\mathbf{\\theta}) \u0026amp;\\propto \\mathbf{\\pi}_k \\exp\\big(\\mathbf{\\mu}_k^T \\mathbf{\\Sigma}^{-1}\\mathbf{x} - \\frac{1}{2}\\mathbf{x}^T\\mathbf{\\Sigma}^{-1}\\mathbf{x} - \\frac{1}{2}\\mathbf{\\mu}_k\\mathbf{\\Sigma}^{-1}\\mathbf{\\mu}_k\\big)\\\\ \u0026amp;= \\exp\\big(\\mathbf{\\mu}_k^T \\mathbf{\\Sigma}^{-1}\\mathbf{x} - \\frac{1}{2}\\mathbf{\\mu}_k\\mathbf{\\Sigma}^{-1}\\mathbf{\\mu}_k + \\log \\mathbf{\\pi}_k \\big) \\exp\\big(- \\frac{1}{2}\\mathbf{x}^T\\mathbf{\\Sigma}^{-1}\\mathbf{x}\\big). \\end{align*}\nThe term \\(\\exp\\big(- \\frac{1}{2}\\mathbf{x}^T\\mathbf{\\Sigma}^{-1}\\mathbf{x}\\big)\\) is placed aside since it is not dependent on the class \\(k\\). When divided by the sum per the definition of \\(p(C_k|\\mathbf{x};\\mathbf{\\theta})\\), it will equal to 1.\nUnder this formulation, we let\n\\begin{align*} \\mathbf{w}_k \u0026amp;= \\mathbf{\\Sigma}^{-1}\\mathbf{\\mu}_k\\\\ \\mathbf{b}_k \u0026amp;= -\\frac{1}{2}\\mathbf{\\mu}_k^T \\mathbf{\\Sigma}^{-1}\\mathbf{\\mu}_k + \\log \\mathbf{\\pi}_k. \\end{align*}\nThis lets us express \\(p(C_k|\\mathbf{x};\\mathbf{\\theta})\\) as the softmax function:\n\\(p(C_k|\\mathbf{x};\\mathbf{\\theta}) = \\frac{\\exp(\\mathbf{w}_k^T\\mathbf{x}+\\mathbf{b}_k)}{\\sum_{k\u0026rsquo;}\\exp(\\mathbf{w}_{k\u0026rsquo;}^T\\mathbf{x}+\\mathbf{b}_{k\u0026rsquo;})}\\).\nDecision Boundaries When using LDA, classifications can be made by choosing the class with the highest posterior probability. Geometrically, this decision boundary has a direct connection to logistic regression. The decision boundary is the set of points where the posterior probability of two classes is equal. This is the set of points where the linear discriminant function is equal to 0. This connection follows the derivation given by Kevin P. Murphy in his book Probabilistic Machine Learning: An Introduction (Murphy 2022).\nIn the previous section, the derivation for the posterior probability of class \\(C_k\\) was written in the form of the softmax function\n\\[ p(C_k|\\mathbf{x};\\mathbf{\\theta}) = \\frac{\\exp(\\mathbf{w}_k^T\\mathbf{x}+\\mathbf{b}_k)}{\\sum_{k\u0026rsquo;}\\exp(\\mathbf{w}_{k\u0026rsquo;}^T\\mathbf{x}+\\mathbf{b}_{k\u0026rsquo;})}. \\]\nIn the binary case, the posterior for class 1 is given by\n\\begin{align*} p(C_1|\\mathbf{x};\\mathbf{\\theta}) \u0026amp;= \\frac{\\exp(\\mathbf{w}_1^T\\mathbf{x}+\\mathbf{b}_1)}{\\exp(\\mathbf{w}_1^T\\mathbf{x}+\\mathbf{b}_1) + \\exp(\\mathbf{w}_2^T\\mathbf{x}+\\mathbf{b}_2)}\\\\ \u0026amp;= \\frac{1}{1 + \\exp((\\mathbf{w}_1 - \\mathbf{w}_2)^T\\mathbf{x}+(\\mathbf{b}_1 - \\mathbf{b}_2))}\\\\ \u0026amp;= \\sigma((\\mathbf{w}_1 - \\mathbf{w}_2)^T\\mathbf{x}+(\\mathbf{b}_1 - \\mathbf{b}_2)). \\end{align*}\nUsing the previous definition of \\(\\mathbf{b}_k\\), we can rewrite \\(\\mathbf{b}_1 - \\mathbf{b}_2\\) as\n\\begin{align*} \\mathbf{b}_1 - \\mathbf{b}_2 \u0026amp;= -\\frac{1}{2}\\mathbf{\\mu}_1^T \\mathbf{\\Sigma}^{-1}\\mathbf{\\mu}_1 + \\log \\mathbf{\\pi}_1 + \\frac{1}{2}\\mathbf{\\mu}_2^T \\mathbf{\\Sigma}^{-1}\\mathbf{\\mu}_2 - \\log \\mathbf{\\pi}_2\\\\ \u0026amp;= -\\frac{1}{2}(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)^T \\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu}_1 + \\mathbf{\\mu}_2) + \\log \\frac{\\mathbf{\\pi}_1}{\\mathbf{\\pi}_2}\\\\ \\end{align*}\nThis can be used to define a new weight vector \\(\\mathbf{w}\u0026rsquo;\\) and a point directly between the two class means \\(\\mathbf{x}_0\\):\n\\begin{align*} \\mathbf{w}\u0026rsquo; \u0026amp;= \\mathbf{\\Sigma}^{-1}(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)\\\\ \\mathbf{x}_0 \u0026amp;= \\frac{1}{2}(\\mathbf{\\mu}_1 + \\mathbf{\\mu}_2) - (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)\\frac{\\log \\frac{\\mathbf{\\pi}_1}{\\mathbf{\\pi}_2}}{(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)^T \\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)}. \\end{align*}\nWith these new terms defined, we have that \\(\\mathbf{w}\u0026rsquo;^T\\mathbf{x}_0 = -(\\mathbf{b}_1 - \\mathbf{b}_2)\\) and the posterior probability for class 1 can be written in the form of binary logistic regression:\n\\begin{equation*} p(C_1|\\mathbf{x};\\mathbf{\\theta}) = \\sigma(\\mathbf{w}\u0026rsquo;^T(\\mathbf{x} - \\mathbf{x}_0)). \\end{equation*}\nThe middle point between the two class means \\(\\mathbf{x}_0\\) is the point where the posterior probability of class 1 is 0.5. This is the decision boundary between the two classes. That is, if \\(\\mathbf{w}\u0026rsquo;^T\\mathbf{x} \u0026gt; \\mathbf{w}\u0026rsquo;^T\\mathbf{x}_0\\), then the posterior probability of class 1 is greater than \\(0.5\\) and the input vector \\(\\mathbf{x}\\) is classified as class 1.\nThe split between the class priors controls the location of the decision boundary. If the class priors are equal, then the decision boundary is the point directly between the two class means. If the class priors are not equal, then the decision boundary is shifted towards the class with the higher prior. The figure below visualizes this.\nFigure 1: Decision boundary between two classes (Murphy 2022). Maximum Likelihood Estimation Given our formulation in the previous section, we can estimate the parameters of the model via maximum likelihood estimation. Assuming \\(K\\) classes with Gaussian class conditional densities, the likelihood function is\n\\begin{equation*} p(\\mathbf{X}|\\mathbf{\\theta}) = \\prod_{i=1}^n \\mathcal{M}(y_i|\\mathbf{\\pi})\\prod_{k=1}^K \\mathcal{N}(\\mathbf{x}_i|\\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k)^{\\mathbb{1}(y_i=k)}. \\end{equation*}\nTaking the log of this function yields\n\\begin{equation*} \\ln p(\\mathbf{X}|\\mathbf{\\theta}) = \\Big[\\sum_{i=1}^n \\sum_{k=1}^K \\mathbb{1}(y_i=k)\\ln \\pi_k\\Big] + \\sum_{k=1}^K\\Big[\\sum_{i:y_i=c} \\ln \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k)\\Big]. \\end{equation*}\nGiven that this is a sum of two different components, we can optimize the multinomial parameter \\(\\mathbf{\\pi}\\) and the class Gaussian parameters \\((\\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k)\\) separately.\nClass Prior For multinomial distributions, the class prior parameter estimation \\(\\hat{\\pi}_k\\) is easily calculated by counting the number of samples belonging to class \\(k\\) and dividing it by the total number of samples.\n\\[ \\hat{\\pi}_k = \\frac{n_k}{n} \\]\nClass Gaussians The Gaussian parameters can be calculated as discussed during the probability review. The parameter estimates are\n\\begin{align*} \\hat{\\mathbf{u}}_k \u0026amp;= \\frac{1}{n_k}\\sum_{i:y_i=k}\\mathbf{x}_i\\\\ \\hat{\\Sigma}_k \u0026amp;= \\frac{1}{n_k}\\sum_{i:y_i=k}(\\mathbf{x}_i - \\hat{\\mathbf{\\mu}}_k)(\\mathbf{x}_i - \\hat{\\mathbf{\\mu}}_k)^T \\end{align*}\nThe Decision Boundary The decision boundary between two classes can be visualized at the point when \\(p(C_k|\\mathbf{x};\\theta) = 0.5\\).\nQuadratic Descriminant Analysis Linear Discriminant Analysis is a special case of Quadratic Discriminant Analysis (QDA) where the covariance matrices are shared across all classes. Assuming each class conditional density is Gaussian, the posterior probability is given by\n\\begin{equation*} p(C_k|\\mathbf{x};\\theta) \\propto \\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k). \\end{equation*}\nTaking the log of this function yields\n\\begin{equation*} \\ln p(C_k|\\mathbf{x};\\theta) = \\ln \\pi_k - \\frac{1}{2}\\ln |\\Sigma_k| - \\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu}_k)^T \\Sigma_k^{-1}(\\mathbf{x} - \\mathbf{\\mu}_k) + c. \\end{equation*}\nWith LDA, the term \\(\\frac{1}{2}\\ln |\\Sigma_k|\\) is constant across all classes, so we treat it as another constant. Since QDA considers a different covariance matrix for each class, we must keep this term in the equation.\nQuadratic Decision Boundary In the more general case of QDA, the decision boundary is quadratic, leading to a quadratic discriminant function. As shown above, the posterior probability function for LDA is linear in \\(\\mathbf{x}\\), which leads to a linear discriminant function.\nExample See here for an example using scikit-learn.\nReferences Murphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. MIT Press. ","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706572800,"objectID":"7b3b17d07ded3a7fc40ad7031aca0872","permalink":"https://ajdillhoff.github.io/notes/linear_discriminant_analysis/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/linear_discriminant_analysis/","section":"notes","summary":"Table of Contents Introduction Gaussian Class Conditional Densities Decision Boundaries Maximum Likelihood Estimation Quadratic Descriminant Analysis Example Slides for these notes can be found here.\nIntroduction This section covers classification from a probabilistic perspective. The discriminative approach involves a parameterized function which assigns each input vector \\(\\mathbf{x}\\) to a specific class. We will see that modeling the conditional probability distribution \\(p(C_k|\\mathbf{x})\\) grants us additional benefits while still fulfilling our original classification task.","tags":null,"title":"Linear Discriminant Analysis","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Smoothing Convolution Gaussian Filters Image Derivatives Introduction How do we detect specific patterns in images (eyes, nose, spots, etc.)? Weighted sums of pixel values. Smoothing When discussing resizing and interpolation, we saw how the choice of scale factor and rotation can produce aliasing in images. Typically, this effect is hidden using some sort of smoothing.\nLet\u0026rsquo;s first look at the case of downsampling an image to 10\\% of its original size. If we use nearest neighbor interpolation, the result is very blocky, as seen below.\nFigure 1: Rocinante cropped and scaled to 10% of the original size. Source: The Expanse Effectively, an entire block of pixels in the original image is being mapped to the nearest neighbor. We are losing a lot of information from the original image. The figure below shows an overlay of the low resolution grid over a patch of the high resolution image.\nFigure 2: The center dots show the selected pixel value for the downsampled grid. Computing the Average Instead of naively selecting one pixel to represent an entire block, we could compute the average pixel value of all pixels within that block. This is a simple as taking an equal contribution from each pixel in the block and dividing by the total number of pixels in the block. An example of such a block is shown below.\nFigure 3: A (3 times 3) averaging filter. By varying the size of this filter, we can effectively change the factor for which we smooth the aliasing.\nFigure 4: Original image smoothed with (9 times 9) average filter before downsampling to 10% of its original size. Convolution How can we apply the averaging filter to an image effectively? We slide the filter across the image starting at the first pixel in the first row and move row by row until all pixels have been computed.\nFigure 5: The takes input from all pixels under it and computes the average. Figure 6: Final portion of image after kernel is applied. This is performed by the convolution operation. It is defined as\n\\begin{equation*} g(x, y) = \\omega * f(x, y) = \\sum_{dx = -a}^a \\sum_{dy=-b}^b \\omega(dw, dy)f(x + dx, y + dy). \\end{equation*}\nThe range \\((-a, a)\\) represents the rows of the kernel and \\((-b, b)\\) the range of the columns of the kernel. The center of the kernel is at \\((dx = 0, dy = 0)\\).\nThis operation is one of, if not the most, important operations in computer vision. It is used to apply filters, but we will later see it as one of the guiding operators for feature extraction and deep learning methods.\nFigure 7: An image (f) convolved with kernel (h). Source: Szeliski Properties Commutativity \\(f * g = g * f\\)\nAssociativity \\(f * (g * h) = (f * g) * h\\)\nDistributivity \\(f * (g + h) = (f * g) + (f * h)\\)\nShift Invariant Linear Systems Convolution is a linear shift-invariant operator. That is, it obeys the following properties.\nSuperposition: The response of the sum of the input is the sum of the individual responses.\n\\[ R(f + g) = R(f) + R(g) \\]\nScaling: The response to a scaled input is equal to the scaled response of the same input.\n\\[ R(kf) = kR(f) \\]\nShift Invariance: The response to a translated input is equal to the translation of the response to the input.\nA system is linear if it satisfies both the superposition and scaling properties. Further, it is a shift-invariant linear system if it is linear and satisfies the shift-invariance property.\nIs the average box filter linear? Yes, it is applied with convolution which behaves the same everywhere.\nIs thresholding a linear system? No, it can be shown that \\(f(n, m) + g(n, m) \u0026gt; T\\), but \\(f(n, m) \u0026lt; T\\) and \\(g(n, m) \u0026lt; T\\).\nGaussian Filters Blurring an image using a box filter does not simulate realistic blurring as well as Gaussian filters do. The following figure exemplifies the difference between the two.\nFigure 8: The left image is blurred using a uniform average box filter. The right image is blurred with a Gaussian filter. The shortcomings of the average box filter can be seen when viewing the artifacting visible at edges, especially at corners.\nA Guassian kernel is defined as\n\\begin{equation*} G(x, y;\\sigma) = \\frac{1}{2\\pi \\sigma^2}\\exp\\Big(-\\frac{(x^2 + y^2)}{2 \\sigma^2}\\Big). \\end{equation*}\nIn effect, it enforces a greater contribution from neighbors near the pixel and a smaller contribution from distant pixels.\nImage Derivatives We can use convolution to approximate the partial derivative (or finite difference) of an image. Recall that\n\\begin{equation*} \\frac{\\partial f}{\\partial x} = \\lim\\limits_{\\epsilon \\rightarrow 0} \\frac{f(x + \\epsilon, y) - f(x, y)}{\\epsilon}. \\end{equation*}\nWe can estimate this as a finite difference\n\\begin{equation*} \\frac{\\partial h}{\\partial x} \\approx h_{i+1, j} - h_{i-1, j}. \\end{equation*}\nWhich can be applied via convolution given a kernal\n\\begin{equation*} \\mathcal{H} = \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 0\\\\ -1 \u0026amp; 0 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 0\\\\ \\end{bmatrix}. \\end{equation*}\nApplying both the horizontal and vertical derivative kernels to an image to a simple square shows the detection of horizontal and vertical edges.\nFigure 9: Horizontal (right) and vertical (middle) derivative kernels applied to the original image (left). The results of applying the derivative kernels are referred to as vertical edge and horizontal edge scores. Consider the middle image in the figure above. The left edge has pixel values of 255; the right edge has -255. In either case, a high absolute score reveals that there is an edge. These scores report the direction of greatest change. The 255 on the left edge indicates the direction is to the right, while the -255 score indicates the direction is to the left. All other instances of the image return a rate of change of 0. Let\u0026rsquo;s see how these filters perform on a more interesting image.\nFigure 10: Vertical derivative filter (left) and horizontal derivative filter (right). ","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642831200,"objectID":"0cc9be3eda94b1feb2ba9780ce74dbb9","permalink":"https://ajdillhoff.github.io/notes/linear_filters/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/linear_filters/","section":"notes","summary":"Table of Contents Introduction Smoothing Convolution Gaussian Filters Image Derivatives Introduction How do we detect specific patterns in images (eyes, nose, spots, etc.)? Weighted sums of pixel values. Smoothing When discussing resizing and interpolation, we saw how the choice of scale factor and rotation can produce aliasing in images. Typically, this effect is hidden using some sort of smoothing.\nLet\u0026rsquo;s first look at the case of downsampling an image to 10\\% of its original size.","tags":["computer vision"],"title":"Linear Filters","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Picking a Model Binary Classification Multiple Classes Slides for these notes are available here.\nIntroduction With Linear Regression we were able to fit a model to our data in order to make inferences on unseen data points. In the examples, both the input features and observation were continuous. With logistic regression, we will use similar models to classify the data points based on their input features. We start out with the simplest approach: we assume that the data is linearly separable and can be assigned one of \\(K\\) discrete classes.\nIn the binary case, the target variable will takes on either a 0 or 1. For \\(K \u0026gt; 2\\), we will use a \\(K\\) dimensional vector that has a 1 corresponding to the class encoding for that input and a 0 for all other positions. For example, if our possible target classes were \\(\\{\\text{car, truck, person}\\}\\), then a target vector for \\(\\text{person}\\) would be \\(\\mathbf{y} = [0, 0, 1]^T\\).\nThis article will stick to a discriminative approach to logistic regression. That is, we define a discriminant function which assigns each data input \\(\\mathbf{x}\\) to a class. For a probabilistic perspective, see Linear Discriminant Analysis.\nPicking a Model We will again start with a linear model \\(y = f(\\mathbf{x}; \\mathbf{w})\\). Unlike the model used with Linear Regression, ours will need to predict a discrete class label. The logistic model is often approached by introducing the odds of an event occurring:\n\\[ \\frac{p}{1-p}, \\]\nwhere \\(p\\) is the probability of the event happening. As \\(p\\) increases, the odds of it happening increase exponentially.\nOur input \\(p\\) represents the probability in range \\((0, 1)\\) which we want to map to the real number space. To approximate this, we apply the natural logarithm to the odds.\nThe logistic model assumes a linear relationship between the linear model \\(\\mathbf{w}^T\\mathbf{x}\\) and the logit function\n\\[ \\text{logit}(p) = \\ln \\frac{p}{1-p}. \\]\nThis function maps a value in range \\((0, 1)\\) to the space of real numbers. Under this assumption, we can write\n\\[ \\text{logit}(p) = \\mathbf{w}^T\\mathbf{x}. \\]\nThis assumption is reasonable because we ultimately want to predict the probability that an event occurs. The output should then be in the range of \\((0, 1)\\). If the logit function produces output in the range of real numbers, as does our linear model \\(\\mathbf{w}^T\\mathbf{x}\\), then we ultimately want a function that maps from the range of real numbers to to \\((0, 1)\\).\nWe can achieve this using the inverse of the logit function, the logistic sigmoid function. It is defined as\n\\begin{equation*} \\sigma(z) = \\frac{1}{1 + \\exp(-z)}, \\end{equation*}\nwhere \\(z = \\mathbf{w}^T\\mathbf{x}\\).\nThe reason for this choice becomes more clear when plotting the function, as seen below.\nFigure 1: The logistic sigmoid function. Source: Wikipedia The inputs on the \\(x\\) axis are clamped to values between 0 and 1. It is also called a squashing function because of this property. This form is also convenient and arises naturally in many probabilistic settings. With this nonlinear activation function, the form of our model becomes\n\\begin{equation*} f(\\mathbf{x};\\mathbf{w}) = h(\\mathbf{w}^T\\mathbf{x}), \\end{equation*}\nwhere \\(h\\) is our choice of activation function.\nThe logistic sigmoid function also has a convenient derivative, which is useful when solving for the model parameters via gradient descent.\n\\[ \\frac{d}{dx} = \\sigma(x)(1 - \\sigma(x)) \\]\nBinary Classification Consider a simple dataset with 2 features per data sample. Our goal is to classify the data as being one of two possible classes. For now, we\u0026rsquo;ll drop the activation function so that our model represents a line that separates both groups of data.\nFigure 2: Two groups of data that are very clearly linearly separable. In the binary case, we are approximating \\(p(C_1|\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x})\\). Then \\(p(C_2|\\mathbf{x}) = 1 - p(C_1| \\mathbf{x})\\).\nThe parameter vector \\(\\mathbf{w}\\) is orthogonal to the decision boundary that separates the two classes. The model output is such that \\(f(\\mathbf{x};\\mathbf{w}) = 0\\) when \\(\\mathbf{x}\\) lies on the decision boundary. If \\(f(\\mathbf{x};\\mathbf{w}) \\geq 0\\) then \\(\\mathbf{x}\\) is assigned to class 1. It is assigned to class 2 otherwise. Since we originally stated that the model should predict either a 0 or 1, we can use the model result as input to the Heaviside step function.\nFitting the Model via Maximum Likelihood Let \\(y_i \\in \\{0, 1\\}\\) be the target for binary classification and \\(\\hat{y}_i \\in (0, 1)\\) be the output of a logistic regression model. The likelihood function is\n\\[ p(\\mathbf{y}|\\mathbf{w}) = \\prod_{i=1}^n \\hat{y}_i^{y_i}(1 - \\hat{y}_i)^{1 - y_i}. \\]\nLet\u0026rsquo;s briefly take a look at \\(\\hat{y}_i^{y_i}(1 - \\hat{y}_i)^{1 - y_i}\\) to understand the output when the model correctly predicts the \\(i^{\\text{th}}\\) sample or not. Since the output is restricted within the range \\((0, 1)\\), the model will never produce 0 or 1.\nIf the target \\(y_i = 0\\), then we can evaluate the subexpression \\(1 - \\hat{y}_i\\). In this case, the likelihood increases as \\(\\hat{y}_i\\) decreases.\nIf the target \\(y_i = 1\\), then we evaluate the subexpression \\(\\hat{y}_i\\).\nWhen fitting this model, we want to define an error measure based on the above function. This is done by taking the negative logarithm of \\(p(\\mathbf{y}|\\mathbf{w})\\).\n\\[ E(\\mathbf{w}) = -\\ln p(\\mathbf{y}|\\mathbf{w}) = -\\sum_{i=1}^n y_i \\ln \\hat{y}_i + (1 - y_i) \\ln (1 - \\hat{y}_i) \\]\nThis function is commonly referred to as the cross-entropy function.\nIf we use this as an objective function for gradient descent with the understanding that \\(\\hat{y}_i = \\sigma(\\mathbf{w}^T \\mathbf{x})\\), then the gradient of the error function is\n\\[ \\nabla E(\\mathbf{w}) = \\sum_{i=1}^n (\\hat{y}_i - y_i)\\mathbf{x}_i. \\]\nThis results in a similar update rule as linear regression, even though the problem itself is different.\nMeasuring Classifier Performance How do we determine how well our model is performing?\nWe will use L1 loss because it works well with discrete outputs. L1 loss is defined as\n\\begin{equation*} L_1 = \\sum_{i}|\\hat{y}_i - y_i|, \\end{equation*}\nwhere \\(\\hat{y}_i\\) is the ground truth corresponding to \\(\\mathbf{x}_i\\) and \\(y_i\\) is the output of our model. We can further normalize this loss to bound it between 0 and 1. Either way, a loss of 0 will indicate 100% classification accuracy.\nMultiple Classes In multiclass logistic regression, we are dealing with target values that can take on one of \\(k\\) values \\(y \\in \\{1, 2, \\dots, k\\}\\). If our goal is to model the distribution over \\(K\\) classes, a multinomial distribution is the obvious choice. Let \\(p(y|\\mathbf{x};\\theta)\\) be a distribution over \\(K\\) numbers \\(w_1, \\dots, w_K\\) that sum to 1. Our parameterized model cannot be represented exactly by a multinomial distribution, so we will derive it so that it satisfies the same constraints.\nWe can start by introducing \\(K\\) parameter vectors \\(\\mathbf{w}_1, \\dots, \\mathbf{w}_K \\in \\mathbb{R}^{d}\\), where \\(d\\) is the number of input features. Then each vector \\(\\mathbf{w}_k^T \\mathbf{x}\\) represents \\(p(C_k | \\mathbf{x};\\mathbf{w}_k)\\). We need to squash each \\(\\mathbf{w}_k^T \\mathbf{x}\\) so that the output sums to 1.\nThis is accomplished via the softmax function:\n\\[ p(C_k|\\mathbf{x}) = \\frac{\\exp(\\mathbf{w}_k^T \\mathbf{x})}{\\sum_{j} \\exp(\\mathbf{w}_j^T \\mathbf{x})}. \\]\nMaximum Likelihood The target vector for each sample is \\(\\mathbf{y}_i \\in \\mathbb{R}^{k}\\). Likewise, the output vector \\(\\hat{\\mathbf{y}}_i\\) also has \\(k\\) elements.\nThe maximum likelihood function for the multiclass setting is given by\n\\[ p(\\mathbf{Y}|\\mathbf{W}) = \\prod_{i=1}^n \\prod_{k=1}^K p(C_k|\\mathbf{x}_i)^{y_{ik}} = \\prod_{i=1}^n \\prod_{k=1}^K \\hat{y}_{ik}^{y_{ik}}. \\]\n\\(\\mathbf{Y} \\in \\mathbb{R}^{n \\times K}\\) is a matrix of all target vectors in the data set. As with the binary case, we can take the negative logarithm of this function to produce an error function.\n\\[ E(\\mathbf{W}) = -\\ln p(\\mathbf{Y}|\\mathbf{W}) = -\\sum_{i=1}^n \\sum_{k=1}^K y_{ik} \\ln \\hat{y}_{ik} \\]\nThis is the cross-entropy function for multiclass classification.\nThe gradient of this function is given as\n\\[ \\nabla_{\\mathbf{w}_j}E(\\mathbf{W}) = \\sum_{i=1}^n (\\hat{y}_{ij} - y_{ij}) \\mathbf{x}_i. \\]\nPart of your first assignment will be to work through the derivation of this function. It is standard practice at this point, but it is highly valuable to understand how the result was produced.\n","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706572800,"objectID":"91f03ca8739733727b9c6b475ec22353","permalink":"https://ajdillhoff.github.io/notes/logistic_regression/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/logistic_regression/","section":"notes","summary":"Table of Contents Introduction Picking a Model Binary Classification Multiple Classes Slides for these notes are available here.\nIntroduction With Linear Regression we were able to fit a model to our data in order to make inferences on unseen data points. In the examples, both the input features and observation were continuous. With logistic regression, we will use similar models to classify the data points based on their input features.","tags":null,"title":"Logistic Regression","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Definition Maximum Likelihood Estimation Making a Decision Relation to Multinomial Logistic Regression MNIST Example Gaussian Formulation Slides for these notes can be found here.\nIntroduction To motivate naive Bayes classifiers, let\u0026rsquo;s look at slightly more complex data. The MNIST dataset was one of the standard benchmarks for computer vision classification algorithms for a long time. It remains useful for educational purposes. The dataset consists of 60,000 training images and 10,000 testing images of size \\(28 \\times 28\\). These images depict handwritten digits. For the purposes of this section, we will work with binary version of the images. This implies that each data sample has 784 binary features.\nWe will use the naive Bayes classifier to make an image classification model which predicts the class of digit given a new image. Each image will be represented by a vector \\(\\mathbf{x} \\in \\mathbb{R}^{784}\\). Modeling \\(p(\\mathbf{x}|C_k)\\) with a multinomial distribution would require \\(10^{784} - 1\\) parameters since there are 10 classes and 784 features.\nFigure 1: Samples of the MNIST training dataset. With the naive assumption that the features are independent conditioned on the class, the number model parameters becomes \\(10 \\times 784\\).\nDefinition A naive Bayes classifier makes the assumption that the features of the data are independent. That is, \\[ p(\\mathbf{x}|C_k, \\mathbf{\\theta}) = \\prod_{d=1}^D p(x_i|C_k, \\theta_{dk}), \\] where \\(\\mathbf{\\theta}_{dk}\\) are the parameters for the class conditional density for class \\(k\\) and feature \\(d\\). Using the MNIST dataset, \\(\\mathbf{\\theta}_{dk} \\in \\mathbb{R}^{784}\\). The posterior distribution is then\n\\begin{equation*} p(C_k|\\mathbf{x},\\mathbf{\\theta}) = \\frac{p(C_k|\\mathbf{\\pi})\\prod_{i=1}^Dp(x_i|C_k, \\mathbf{\\theta}_{dk})}{\\sum_{k\u0026rsquo;}p(C_{k\u0026rsquo;}|\\mathbf{\\pi})\\prod_{i=1}^Dp(x_i|C_{k\u0026rsquo;},\\mathbf{\\theta}_{dk\u0026rsquo;})}. \\end{equation*}\nIf we convert the input images to binary, the class conditional density \\(p(\\mathbf{x}|C_k, \\mathbf{\\theta})\\) takes on the Bernoulli pdf. That is,\n\\begin{equation*} p(\\mathbf{x}|C_k, \\mathbf{\\theta}) = \\prod_{i=1}^D\\text{Ber}(x_i|\\mathbf{\\theta}_{dk}). \\end{equation*}\nThe parameter \\(\\theta_{dk}\\) is the probability that the feature \\(x_i=1\\) given class \\(C_k\\).\nMaximum Likelihood Estimation Fitting a naive Bayes classifier is relatively simple using MLE. The likelihood is given by\n\\begin{equation*} p(\\mathbf{X}, \\mathbf{y}|\\mathbf{\\theta}) = \\prod_{n=1}^N \\mathcal{M}(y_n|\\mathbf{\\pi})\\prod_{d=1}^D\\prod_{k=1}^{K}p(x_{nd}|\\mathbf{\\theta}_{dk})^{\\mathbb{1}(y_n=k)}. \\end{equation*}\nTo derive the estimators, we first take the log of the likelihood:\n\\begin{equation*} \\ln p(\\mathbf{X}, \\mathbf{y}|\\mathbf{\\theta}) = \\Bigg[\\sum_{n=1}^N\\sum_{k=1}^K \\mathbb{1}(y_n = k)\\ln \\pi_k\\Bigg] + \\sum_{k=1}^K\\sum_{d=1}^D\\Bigg[\\sum_{n:y_n=k}\\ln p(x_{nd}|\\theta_{dk})\\Bigg]. \\end{equation*}\nThus, we have a term for the the multinomial and terms for the class-feature parameters. As with previous models that use a multinomial form, the parameter estimate for the first term is computed as\n\\begin{equation*} \\hat{\\pi}_k = \\frac{N_k}{N}. \\end{equation*}\nThe features used in our data are binary, so the parameter estimate for each \\(\\hat{\\theta}_{dk}\\) follows the Bernoulli distribution:\n\\begin{equation*} \\hat{\\theta}_{dk} = \\frac{N_{dk}}{N_{k}}. \\end{equation*}\nThat is, the number of times that feature \\(d\\) is in an example of class \\(k\\) divided by the total number of samples for class \\(k\\).\nMaking a Decision Given parameters \\(\\mathbf{\\theta}\\), how can we classify a given data sample?\n\\begin{equation*} \\text{arg}\\max_{k}p(y=k)\\prod_{i}p(x_i|y=k) \\end{equation*}\nRelation to Multinomial Logistic Regression Consider some data with discrete features having one of \\(K\\) states, then \\(x_{dk} = \\mathbb{1}(x_d=k)\\). The class conditional density, in this case, follows a multinomial distribution:\n\\[ p(y=c|\\mathbf{x}, \\mathbf{\\theta}) = \\prod_{d=1}^D \\prod_{k=1}^K \\theta_{dck}^{x_{dk}}. \\]\nWe can see a connection between naive Bayes and logistic regression when we evaluate the posterior over classes:\n\\begin{align*} p(y=c|\\mathbf{x}, \\mathbf{\\theta}) \u0026amp;= \\frac{p(y)p(\\mathbf{x}|y, \\mathbf{\\theta})}{p(\\mathbf{x})}\\\\ \u0026amp;= \\frac{\\pi_c \\prod_{d} \\prod_{k} \\theta_{dck}^{x_{dk}}}{\\sum_{c\u0026rsquo;}\\pi_{c\u0026rsquo;}\\prod_{d}\\prod_{k}\\theta_{dc\u0026rsquo;k}^{x_{dk}}} \\\\ \u0026amp;= \\frac{\\exp[\\log \\pi_c + \\sum_d \\sum_k x_{dk}\\log \\theta_{dck}]}{\\sum_{c\u0026rsquo;} \\exp[\\log \\pi_{c\u0026rsquo;} + \\sum_d \\sum_k x_{dk} \\log \\theta_{dc\u0026rsquo;k}]}. \\end{align*}\nThis has the same form as the softmax function:\n\\[ p(y=c|\\mathbf{x}, \\mathbf{\\theta}) = \\frac{e^{\\beta^{T}_c \\mathbf{x} + \\gamma_c}}{\\sum_{c\u0026rsquo;=1}^C e^{\\beta^{T}_{c\u0026rsquo;}\\mathbf{x} + \\gamma_{c\u0026rsquo;}}} \\]\nMNIST Example With the model definition and parameter estimates defined, we can fit and evaluate the model. Using scikit-learn, we fit a Bernoulli naive Bayes classifier on the MNIST training set: Naive Bayes.\nGaussian Formulation If our features are continuous, we would model them with univariate Gaussians.\n","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706745600,"objectID":"f86c9f09aae89628ddd3d235958176db","permalink":"https://ajdillhoff.github.io/notes/naive_bayes/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/naive_bayes/","section":"notes","summary":"Table of Contents Introduction Definition Maximum Likelihood Estimation Making a Decision Relation to Multinomial Logistic Regression MNIST Example Gaussian Formulation Slides for these notes can be found here.\nIntroduction To motivate naive Bayes classifiers, let\u0026rsquo;s look at slightly more complex data. The MNIST dataset was one of the standard benchmarks for computer vision classification algorithms for a long time. It remains useful for educational purposes. The dataset consists of 60,000 training images and 10,000 testing images of size \\(28 \\times 28\\).","tags":["machine learning"],"title":"Naive Bayes","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Resources Introduction Definition Forward Pass Activation Functions Multi-Class Classification Backpropagation Non-Convex Optimization Resources https://playground.tensorflow.org/ Introduction Previously, we studied the Perceptron and saw that while it made for a simple linear classifier, it is severely limited to problems that are already linearly separable. This limitation was resolved by introduding a hidden layer with multiple perceptron units, aptly named Multi-Layer Perceptrons.\nIn this series, we will explore the more general method of neural networks. We will see that even a network of only two layers can approximate any continuous functional mapping to arbitrary accuracy. Through a discussion about network architectures, activation functions, and backpropagation, we will understand and use neural networks to resolve a large number of both classification and regression tasks.\nDefinition We will take an abstract view of neural networks in which any formulation of a neural network defines a nonlinear mapping from an input space to some output space. This implies that our choice of activation function must be nonlinear. The function we create will be parameterized by some weight matrix \\(W\\). Thus, any neural network can be simply formulated as\n\\[ f(\\mathbf{x};W). \\]\nFigure 1: General neural network diagram. A neural network is in part defined by its layers, the number of nodes in each layer, the choice of activation function, and the choice of loss function.\nEach layer has a number of weights equal to the number of input nodes times the number of output nodes. This is commonly represented as a weight matrix \\(W\\).\nThe network produces output through the forward pass and computes the gradients with respect to that output in the backwards pass.\nForward Pass Computing the output is done in what is called the forward pass.\nOur neural network function takes in an input \\(\\mathbf{x} \\in \\mathbb{R}^D\\), where \\(D\\) is the number of features in our input space. Each output node \\(a_j\\) in a hidden layer \\(h_l\\) has a corresponding weight vector \\(\\mathbf{w}_j^{(l)}\\). The intermediate output of a hidden layer \\(h_l\\) is a linear combination of the weights and the input followed by some nonlinear function. Node \\(a_j\\) of a hidden layer is computed as\n\\[ a_j = \\sum_{i=1}^d w_{ji}^{(l)} x_{i} + w_{j0}^{(l)}. \\]\nAs with Linear Regression, we will prepend a constant 1 to our input so that the computation is simply\n\\[ a_{j} = \\sum_{i=0}^d w_{ji}^{(i)} x_i = \\mathbf{w}_j^T \\mathbf{x}. \\]\nThe final output of the hidden layer is \\(a_j\\) transformed by a nonlinear function \\(g\\) such that\n\\[ z_j = g(a_j). \\]\nWe can combine all weight vectors for each hidden layer node into a weight matrix \\(W \\in \\mathbb{R}^{n \\times d}\\), where \\(n\\) is the number of nodes in the layer and \\(d\\) is the number of input features such that\n\\begin{equation*} W = \\begin{bmatrix} \\mathbf{w}_1^T\\\\ \\vdots\\\\ \\mathbf{w}_n^T\\\\ \\end{bmatrix}. \\end{equation*}\nThen the output of the hidden layer can be computed as\n\\[ \\mathbf{a} = W\\mathbf{x}. \\]\nIf you instead wanted to separate the bias term, this would be\n\\[ \\mathbf{a} = W\\mathbf{x} + \\mathbf{b}. \\]\nUsing the notation to specify the individual layer, we can write the output of a full network. Let \\(W^{(l)} \\in \\mathbb{R}^{n_{l} \\times n_{l-1}}\\) be the weights for layer \\(l\\) which have \\(n_{l-1}\\) input connections and \\(n_{l}\\) output nodes. The activation function for layer \\(l\\) is given by \\(g^{(l)}\\).\nThe complete forward pass of the network is computed by repeating the following step for all layers:\n\\[ \\mathbf{z}^{(l)} = g^{(l)}(\\mathbf{a}^{(l-1)}), \\]\nwhere\n\\[ \\mathbf{a}^{(l-1)} = W^{(l-1)}\\mathbf{z}^{(l-1)} + \\mathbf{b}^{(l-1)}. \\]\nOnce all layers have been computed, then the output of the last layer, \\(\\hat{\\mathbf{y}}^{(L)}\\) is used as the final output of the model. For training, this is compared with some ground truth label \\(\\mathbf{y}\\) using a loss function \\(\\mathcal{L}\\):\n\\[ \\mathcal{L}(\\hat{\\mathbf{y}}, \\mathbf{y}). \\]\nXOR Example Consider the XOR problem. A single Perceptron was unable to solve that problem. However, adding a hidden layer and forming a multi-layer perceptron network allowed for a more complex decision boundary. Consider the network below and produce the output given all combinations of binary input: \\(\\{(0, 0), (0, 1), (1, 0), (1, 1)\\}\\).\nFigure 2: A network with 1 hidden layer that computes XOR. Source: https://athitsos.utasites.cloud/courses/cse4309_fall2021/lectures/09a_neural_networks.pdf Activation Functions Sigmoid Function \\[ g(x) = \\frac{1}{1 + e^{-x}} \\]\nThe logistic sigmoid function serves two purposes. First, it allows the output of the neuron to be interpreted as a posterior probability. Note that this is not actually a probability. Second, it is a continuous function for which the derivative can be computed:\n\\[ g\u0026rsquo;(x) = g(x)(1 - g(x)). \\]\nHyperbolic Tangent Function \\[ \\tanh x = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \\]\nThe hyperbolic tangent function maps input to a range of \\((-1, 1)\\).\nThe derivative is calculated as\n\\[ \\frac{d}{dx} \\tanh x = 1 - \\tanh^2 x. \\]\nFigure 3: Hyperbolic Tangent Function. Source: Wolfram Key Terms\nbias activation function Neurons fire after input reaches some threshold. Differential activation functions necessary for backpropagation. Multi-class learning How long to train? Weight decay How many layers versus how many nodes per layer? Training Data split (train/test/val) Multi-Class Classification Consider an output layer of a network with \\(k\\) nodes. Each of these nodes represents a decision node for a one-versus-all classifier. For a classification task, we have to think about whether or not the sum of squares loss function works.\nAs far as activation functions go, the logistic sigmoid function is a good way to produce some interpretation of probability. If we treat every output node as its own one versus all classifier, then a logistic sigmoid at the end of each one would indicate the \u0026ldquo;probability\u0026rdquo; that node \\(k\\) assigns class \\(k\\).\nHow do we formulate this in a neural network?\nThe number of nodes in the output layer will be \\(K\\), the number of classes. Since the output of each node produces a value in range \\((0, 1)\\), we want to construct a target value that works with this. Instead of assigning an integer to each class label (e.g. 1 for class 2, 2 for class 3, etc.), we will encode the target label as a \\(K\\) dimensional vector. For example, if our class label is for the class 1, then the corresponding target vector will be\n\\begin{equation*} \\mathbf{t} = \\begin{bmatrix} 1\\\\ 0\\\\ \\vdots\\\\ 0 \\end{bmatrix}. \\end{equation*}\nSince the output of our final layer is also a \\(K\\) dimensional vector, we can compare the two using some loss function.\nBackpropagation Given a series of linear layers with nonlinear activation functions, how can we update the weights across the entire network?\nThe short answer is through the chain rule of differentiation. Let\u0026rsquo;s explore this through an example.\nAfter constructing some series of hidden layers with an arbitrary number of nodes, we will pick an error function that provides a metric of how our network performs on a given regression or classification task. This loss is given by \\(\\mathcal{L}\\).\nNeural networks are traditionally trained using gradient descent. The goal is to optimize the weights such that they result in the lowest loss, or error. This is also why our choice of loss function is important.\n\\[ \\mathbf{W}^* = \\text{argmin}\\frac{1}{n}\\sum_{i=1}^n \\mathcal{L}(f(\\mathbf{x}^{(i)}; \\mathbf{W}), \\mathbf{y}^{(i)}) \\]\nWe first compute the gradients of the network with respect to the weights and biases. Then, we use those gradients to update our previous values for the weights and biases.\nA Simple Example We will first look at computing these gradients on a smaller network for binary classification with 1 hidden layer and 1 output layer. The loss function is defined using the binary cross-entropy function:\n\\[ \\mathcal{L}(\\hat{\\mathbf{y}}, \\mathbf{y}) = -\\mathbf{y}\\log \\hat{\\mathbf{y}} - (1 - \\mathbf{y}) \\log (1 - \\hat{\\mathbf{y}}) \\]\nThe network\u0026rsquo;s output is computed in sequence following\n\\begin{align*} \\mathbf{a}^{(1)} \u0026amp;= W^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)}\\\\ \\mathbf{z}^{(1)} \u0026amp;= g^{(1)}(\\mathbf{a}^{(1)})\\\\ \\mathbf{a}^{(2)} \u0026amp;= W^{(2)}\\mathbf{z}^{(1)} + \\mathbf{b}^{(2)}\\\\ \\mathbf{z}^{(2)} \u0026amp;= g^{(2)}(\\mathbf{a}^{(2)})\\\\ \\end{align*}\nThe goal is to compute the gradients for all weights and biases:\n\\[ \\frac{d\\mathcal{L}}{dW^{(1)}},\\quad \\frac{d\\mathcal{L}}{d\\mathbf{b}^{(1)}},\\quad \\frac{d\\mathcal{L}}{dW^{(2)}},\\quad \\frac{d\\mathcal{L}}{d\\mathbf{b}^{(2)}}. \\]\nStarting with the weights of the output layer:\n\\[ \\frac{d\\mathcal{L}}{dW^{(2)}} = \\frac{d\\mathcal{L}}{d\\mathbf{z}^{(2)}} \\frac{d\\mathbf{z}^{(2)}}{d\\mathbf{a}^{(2)}} \\frac{d\\mathbf{a}^{(2)}}{dW^{(2)}}. \\]\nThe first step is to compute the partial gradient of the loss function with respect to its input \\(\\hat{\\mathbf{y}} = \\mathbf{z}^{(2)}\\):\n\\[ \\frac{d\\mathcal{L}}{d\\mathbf{z}^{(2)}} = \\frac{\\mathbf{z}^{(2)} - \\mathbf{y}}{\\mathbf{z}^{(2)}(1 - \\mathbf{z}^{(2)})}. \\]\nNext, compute the gradient of the last layer\u0026rsquo;s activation function with respect to its input \\(\\mathbf{a}^{(2)}\\):\n\\[ \\frac{d\\mathbf{z}^{(2)}}{d\\mathbf{a}^{(2)}} = \\mathbf{z}^{(2)}(1 - \\mathbf{z}^{(2)}). \\]\nFinally, we compute \\(\\frac{d\\mathbf{a}^{(2)}}{dW^{(2)}}\\): \\[ \\frac{d\\mathbf{a}^{(2)}}{dW^{(2)}} = \\mathbf{z}^{(1)}. \\]\nPutting all of this together yields\n\\begin{align*} \\frac{d\\mathcal{L}}{dW^{(2)}} \u0026amp;= \\frac{\\mathbf{z}^{(2)} - \\mathbf{y}}{\\mathbf{z}^{(2)}(1 - \\mathbf{z}^{(2)})} * \\mathbf{z}^{(2)}(1 - \\mathbf{z}^{(2)}) * \\mathbf{z}^{(1)}\\\\ \u0026amp;= \\mathbf{z}^{(1)} (\\mathbf{z}^{(2)} - \\mathbf{y}). \\end{align*}\nNon-Convex Optimization Optimizing networks with non-linearities produces a non-convex landscape. Depending on our choice of optimization algorithm and initial starting point, the algorithm will most likely get \u0026ldquo;stuck\u0026rdquo; in some local minimum. Consider the figure below produced by (Li et al. 2017).\nFigure 4: Loss surface of ResNet-56 (Li et al.) References Li, Hao, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2017. “Visualizing the Loss Landscape of Neural Nets,” 11. ","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642831200,"objectID":"16c8d84377f70f04087cbe417e84cd5f","permalink":"https://ajdillhoff.github.io/notes/neural_networks/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/neural_networks/","section":"notes","summary":"Table of Contents Resources Introduction Definition Forward Pass Activation Functions Multi-Class Classification Backpropagation Non-Convex Optimization Resources https://playground.tensorflow.org/ Introduction Previously, we studied the Perceptron and saw that while it made for a simple linear classifier, it is severely limited to problems that are already linearly separable. This limitation was resolved by introduding a hidden layer with multiple perceptron units, aptly named Multi-Layer Perceptrons.\nIn this series, we will explore the more general method of neural networks.","tags":["machine learning"],"title":"Neural Networks","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction The Perceptron Learning Algorithm Limitations of Single-Layer Perceptrons Introduction A popular example of a Logistic Regression model is the perceptron. Proposed by Frank Rosenblatt in 1962, the perceptron is defined as a generalized linear model:\n\\begin{equation*} f(\\mathbf{w}^T\\mathbf{\\phi}(\\mathbf{x})), \\end{equation*}\nwhere \\(\\phi\\) is a basis function and \\(f\\) is a stepwise function with the form\n\\begin{equation*} f(a) = \\begin{cases} 1, a \\geq 0\\\\ -1, a \u0026lt; 0 \\end{cases} \\end{equation*}\nTo match this, the targets will take on a value of either 1 or -1.\nThe Perceptron Learning Algorithm Based on the stepwise function, the parameters \\(\\mathbf{w}\\) should lead to outputs above 0 for one class and outputs below 0 for the other. There is 0 error with a correct classification.\nThe original formulation does not work well with gradient based optimization methods due to the fact that the derivative of the stepwise function is 0 almost everyone. To get around this, the perceptron criterion is used:\n\\begin{equation*} E(\\mathbf{w}) = -\\sum_i \\mathbf{w}^T\\phi(\\mathbf{x}_i)\\hat{y}_i, \\end{equation*}\nwhere \\(\\hat{y}_i\\) is the target class (either 1 or -1).\nAn incorrect classification will minimize \\(\\mathbf{w}^T\\phi_i y_i\\). We can consider this loss only for misclassified patterns.\nUpdate Steps\nFor each input, evaluate \\(f(\\mathbf{w}^T\\phi(\\mathbf{x}_i))\\). For incorrect classifications Add \\(\\phi(\\mathbf{x}_i)\\) to \\(\\mathbf{w}\\) estimate for class 1 Subtract \\(\\phi(\\mathbf{x}_i)\\) from \\(\\mathbf{w}\\) for class 2. Does not necessarily get better each step, but guaranteed to converge.\nLimitations of Single-Layer Perceptrons Single layer perceptrons are limited to solving linearly separable patterns. As we have seen with a few datasets now, expecting our data to be linearly separable is wishful thinking. Minsky and Papert exposed this limitation in their book Perceptrons: an introduction to computational geometry.\nConsider the example XOR problem. It is a binary classification problem consisting of 4 data points. It is not linearly separable as seen in the figure below.\nFigure 1: XOR cannot be solved with a linear classifier. This is the result of using only a single Perceptron. What if we added another perceptron? A single perceptron computes \\(\\mathbf{w}^T + b\\). It is important to transform the first perceptron\u0026rsquo;s output using a non-linear activation function, otherwise the output would be similar to that of a logistic regression model. The updated \u0026ldquo;network\u0026rdquo; is shown below.\nFigure 2: A 2 layer perceptron for which each layer has a single node. The result is the same! The original input in 2D is transformed to a single dimensional output. This is then used as input to the second perceptron. The result is a linear decision boundary followed by another linear decision boundary. What if we used 2 perceptrons in the first layer? The idea is that using two linear decision boundaries in a single space would allow our model to create a more complex boundary. The updated network is shown below.\nFigure 3: A 2 layer perceptron for which the first layer has 2 nodes. This effectively solves the XOR problem! Since each node computes a linear combination of the input, we can visualize two decision boundaries with respect to the input space.\nFigure 4: Visualization of input space. Similarly, we can visualize how the data points are transformed by visualizing the space of the output layer.\nFigure 5: Output space ","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642831200,"objectID":"05741d6538d42b28c94ed3a24f23ee36","permalink":"https://ajdillhoff.github.io/notes/perceptron/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/perceptron/","section":"notes","summary":"Table of Contents Introduction The Perceptron Learning Algorithm Limitations of Single-Layer Perceptrons Introduction A popular example of a Logistic Regression model is the perceptron. Proposed by Frank Rosenblatt in 1962, the perceptron is defined as a generalized linear model:\n\\begin{equation*} f(\\mathbf{w}^T\\mathbf{\\phi}(\\mathbf{x})), \\end{equation*}\nwhere \\(\\phi\\) is a basis function and \\(f\\) is a stepwise function with the form\n\\begin{equation*} f(a) = \\begin{cases} 1, a \\geq 0\\\\ -1, a \u0026lt; 0 \\end{cases} \\end{equation*}","tags":["machine learning"],"title":"Perceptron","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Summary Maximum Variance Formulation Motivating Example Noise and Redundancy Covariance Matrix Summary If we have some measurements of data, but do not know the underlying dynamics, PCA can resolve this by producing a change of basis such that the dynamics are reflected upon the eigenvectors.\nMaximum Variance Formulation Although there are several derivations of PCA. I really like the approach of projecting the data onto a lower dimensional space in order to maximize the variance of the projected data.\nLet \\(\\mathbf{X}\\) be a dataset of \\(N\\) samples, each with \\(D\\) features. The goal of PCA is to project this data onto an $M$-dimensional space such that \\(M \u0026lt; D\\).\nRemember that the goal here is to maximize the variance of the projected data.\nHow do we project the data? Let\u0026rsquo;s say that we want to go from $D$-dimensional space to $M$-dimensional space where \\(M = 1\\). Let the vector \\(\\mathbf{u}\\) define this 1D space. If \\(\\mathbf{u}\\) is a unit vector, then the scalar projection of a data point \\(\\mathbf{x}\\) onto \\(\\mathbf{u}\\) is simply \\(\\mathbf{u} \\cdot \\mathbf{x}\\).\nSince we are maximizing variance, we need to subtract the mean sample from our data\n\\begin{equation*} \\mathbf{\\bar{x}} = \\frac{1}{N}\\sum_{n=1}^{N}\\mathbf{x}_n \\end{equation*}\nThen, the mean of the projected data is \\(\\mathbf{u} \\cdot \\mathbf{\\bar{x}}\\).\nWith the mean of the projected data, we can calculate the variance:\n\\begin{equation*} \\frac{1}{N}\\sum_{n=1}^{N}\\{\\mathbf{u}^T\\mathbf{x}_n - \\mathbf{u}^T\\mathbf{\\bar{x}}\\}^2 = \\mathbf{u}^T\\mathbf{S}\\mathbf{u} \\end{equation*}\nwhere\n\\begin{equation*} \\mathbf{S} = \\frac{1}{N}\\sum_{n=1}^{N}(\\mathbf{x}_n - \\mathbf{\\bar{x}})(\\mathbf{x}_n - \\mathbf{\\bar{x}})^T \\end{equation*}\nThus, if we are maximizing the variance of the projected data, then we are maximizing \\(\\mathbf{u}^T\\mathbf{S}\\mathbf{u}\\)!\nSo this is an optimization problem, but there is one minor issue to deal with: if \\(\\mathbf{u}\\) is not constrained, then we scale it to infinity while maximizing the function.\nBefore, we stated that \\(\\mathbf{u}\\) is a unit vector. Thus, the constraint is that \\(\\mathbf{u} \\cdot \\mathbf{u} = 1\\).\nAfter reviewing Lagrangian multipliers\u0026hellip;\nTo enforce this constraint, we can use a lagrangian multiplier:\n\\begin{equation*} \\mathcal{L}(\\mathbf{u}, \\lambda) = \\mathbf{u}^T\\mathbf{S}\\mathbf{u} + \\lambda(1 - \\mathbf{u}^T\\mathbf{u}). \\end{equation*}\nLet\u0026rsquo;s see what happens when we compute the stationary points (critical points) of the given Lagrangian function.\n\\begin{equation*} \\nabla_{\\mathbf{u}}\\mathcal{L}(\\mathbf{u}, \\lambda) = \\mathbf{S}\\mathbf{u} - \\lambda \\mathbf{u} = 0 \\end{equation*}\nThis implies that\n\\begin{equation*} \\mathbf{S}\\mathbf{u} = \\lambda \\mathbf{u} \\end{equation*}\nThat particular equation means that \\(\\mathbf{u}\\) is an eigenvector of \\(\\mathbf{S}\\) with \\(\\lambda\\) being the corresponding eigenvalue. Since \\(\\mathbf{u}\\) is a unit vector, we can conveniently left-multiply both sides of that equation by \\(\\mathbf{u}^T\\), resulting in:\n\\begin{equation*} \\mathbf{u}^T\\mathbf{S}\\mathbf{u} = \\lambda \\end{equation*}\nWhat does this mean?\nThat means that the variance is maximized when \\(\\mathbf{u}\\) is the eigenvector corresponding to the largest eigenvalue \\(\\lambda\\).\nWe can repeat this process to find the direction (eigenvector) corresponding to the second largest variance by considering eigenvectors that are orthogonal to the first one. This is where an orthonormal eigenbasis comes in handy.\nMotivating Example Consider a frictionless, massless spring that produces dynamics in a single direction.\nFigure 1: Toy model of spring with ball observed from 3 perspectives. We can clearly understand that the spring will only move in a single direction. That movement reflects the underlying dynamics of this data. To understand how PCA can be useful in this situation, let\u0026rsquo;s pretend that we do not know the underlying dynamics. Instead, we observe that the data seems to go back and forth along a single axis. We observe the data over time from 3 different perspectives given by the cameras in the above figure.\nFrom the perspective of the observer, we are recording some observations in an effort to understand which dimensions are the most salient at representing the underlying mechanics. From the above figure, we know that the most important dimension in this system is that of the labeled x-axis.\nHow would we figure this out if we did not already know that?\nEach camera has its own coordinate system (basis). If each camera gives us a 2D location of the ball relative to that camera\u0026rsquo;s basis, then each sample in time gives us a 6D vector of locations.\nEquivalently, every time sample is a vector that lies in an $m$-dimensional vector space spanned by an orthonormal basis.\nIs it possible to find another basis that best expresses the data?\nMathemtically, is there some matrix \\(P\\) that changes our original data \\(X\\) into a new representation \\(Y\\)?\n\\(PX = Y\\)\nNoise and Redundancy When observing real data, we will have to account for noisy measurements. Noise can come from a wide variety of sources. Being able to reduce it or filter it out is vital to understanding the underlying system.\nNoise is an arbitrary measurement and means nothing without some measurement of a signal. Thus, we typically measure the amount of noise in our system using a Signal-to-Noise Ratio (SNR). This assumes we have some idea of what our signal is. This is usually given based on the nature of whatever problem we are investigating. In the toy example, we know that the spring largely moves in a single dimension. That is the signal we expect to observe.\nFor arguments sake, imagine we that the recordings over time from a single camera plot the following data:\nFrom our advantageous position of knowing the true nature of the problem, we understand there really should be no noise. However, let\u0026rsquo;s say that our camera has some noise in interpreting the precise location of the ball at any given time. In this case, our SNR is quite high, which is good! Ideally, it would be a straight line.\nThere is a second factor to consider: the fact that we are taking measurements from multiple sensors means that there may be some redundancy among the data collected from them. If we were to discover features that have high redundancy, we could be confident in concluding that they are highly correlated.\nCovariance Matrix Let \\(X\\) be a an \\(m \\times n\\) matrix of \\(n\\) observations with \\(m\\) features per observation.\nWe can produce a covariance matrix of the features via \\(S_{X} = \\frac{1}{n-1}XX^{T}\\).\nThis gives us a measurement of the correlations between all pairs of measurements.\nIf we want to reduce redunancy between separate measurements (those in the off-diagonal of the matrix), we would then want to diagonalize this matrix. In terms of the equation \\(PX=Y\\), this has the effective of finding a new covariance matrix \\(S_{Y}\\) that is diagonal. This means that each value in the off-diagonal of \\(S_{Y}\\) is 0.\nPCA has a convenient assumption: the change of basis matrix \\(P\\) is orthonormal. Why is this convenient? PCA can then select the normalized direction in the feature space for which the variance in the data is maximized. This is called the first principal component. Because we assume \\(P\\) is orthonormal, the subsequent principal components must be orthogonal to the previously discovered components.\nOne more thing If \\(P\\) is not orthonormal, then we can simply scale our eigenvectors to maximize variance.\n","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642831200,"objectID":"cddf33f5490eea81f22d826d26808a07","permalink":"https://ajdillhoff.github.io/notes/principal_component_analysis/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/principal_component_analysis/","section":"notes","summary":"Table of Contents Summary Maximum Variance Formulation Motivating Example Noise and Redundancy Covariance Matrix Summary If we have some measurements of data, but do not know the underlying dynamics, PCA can resolve this by producing a change of basis such that the dynamics are reflected upon the eigenvectors.\nMaximum Variance Formulation Although there are several derivations of PCA. I really like the approach of projecting the data onto a lower dimensional space in order to maximize the variance of the projected data.","tags":["dimensionality reduction","machine learning"],"title":"Principal Component Analysis","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction A Simple Example Probability Distributions Conditional Probability Rules of Probability Random Variables Continuous Variables Moments of a Distribution Slides for these notes are available here.\nIntroduction Probability theory provides a consistent framework for the quantification and manipulation of uncertainty. It allows us to make the best decisions given the limited information we may have. Many tasks, models, and evaluation metrics that we will explore in this course are either based on, or are inspired by, probability theory.\nA Simple Example Scenario: There are two cookie jars, a blue one for cookies with oatmeal raisin cookies and a red one for chocolate chip cookies. The jar with oatmeal raisin cookies has 8 cookies in it. The chocolate chip jar has 10 cookies. Some monster took 2 of the chocolate chip cookies and placed them in the oatmeal raisin jar and placed 1 of the oatmeal raisin cookies in the chocolate chip jar. Thus, the oatmeal raisin jar has 2 chocolate chip and 7 oatmeal raisin. The chocolate chip jar has 8 chocolate chip and 1 oatmeal raisin.\nLet\u0026rsquo;s say that we pick the chocolate chip jar 80% of the time and the oatmeal raisin jar 20% of the time. For a given jar, the cookies inside are all equally likely to be picked. We can assign this probability to random variables:\n\\(J\\) - The type of jar, either blue \\(b\\) or red \\(r\\). \\(C\\) - The type of cookie, either oatmeal \\(o\\) or chocolate chip \\(c\\). We can define the probability of picking a particular jar:\n\\(p(J=b) = 0.2\\) \\(p(J = r) = 0.8\\) Notice that their sum is 1.0. These probabilities can be estimated empirically given an observer recording the events. We may also define the probabilities of picking a particular type of cookie.\n\\(p(C = o)\\) \\(p(C = c)\\) For each jar, the probabilities of picking the cookies must sum to 1. The tables below show the individual probabilities of picking each type of cookie from each jar. Since we can observe the actual quantities, we can define the probabilities empirically.\nChocolate Chip Oatmeal Raisin Blue Jar 2 / 9 = 0.222 7 / 9 = 0.778 Chocolate Chip Oatmeal Raisin Red Jar 8 / 9 = 0.889 1 / 9 = 0.111 Given these quantities, we can ask slightly more complicated questions such as \u0026ldquo;what is the probability that I will select the red jar AND take a chocolate chip cookie?\u0026rdquo; This is expressed as a joint probability distribution, written as \\(p(J = r, C = c)\\). It is defined based on two events:\nthe prior probability of picking the red jar, the conditional probability of picking a chocolate chip cookie conditioned on the event that the red jar was picked. \\begin{equation*} p(J=r, C=c) = p(C=c | J=r) p(J = r) \\end{equation*}\nThis is also referred to as the product rule.\nWe already know \\(p(J=r) = 0.8\\). From the table above, we can see that \\(p(C=c|J=r) = 0.889\\). Thus, \\(p(C=c,J=r) = 0.8 * 0.889 = 0.711\\).\nIf we knew nothing about the contents of the jar or the prior probabilities of selecting a jar, we could measure the joint probability empirically. This would simply be the number of times we select the red jar AND a chocolate chip cookie divided by total number of trials. For best results, perform an infinite number of trials.\nIf instead we wanted to measure the conditional probability \\(p(C=c|J=r)\\), we would simply take the number of times a chocolate chip cookie is taken from the red jar and divide by the total number of times the red jar was selected.\nWe can construct a joint probability table given the joint probabilities of all the events listed.\nChocolate Chip Oatmeal Raisin Red Jar 0.711 0.089 Blue Jar 0.044 0.156 If you summed each row and further took the sum of the sum of rows, you would get 1. Likewise, the sum of the sum of columns would equal 1.\nSumming the columns for each row yields the prior probability of selecting each type of jar. Similarly, summing the rows for each column gives the prior probability of selecting that type of cookie. This is referred to as the marginal probability or sum rule, which is computed by summing out the other variables in the joint distribution. For example,\n\\begin{equation*} p(x_i) = \\sum_j p(x_i, y_j) \\end{equation*}\nEmpirically, this is computed as the number of times event \\(x_i\\) occurs out of ALL trials.\nAlthough the joint probabilities \\(p(X, Y)\\) and \\(p(Y, X)\\) would be written slightly differently, they are equal. With this in mind, we can set them equal to each other to derive Bayes\u0026rsquo; rule:\n\\begin{align*} p(X, Y) \u0026amp;= p(Y, X)\\\\ p(X|Y)p(Y) \u0026amp;= p(Y|X)p(X)\\\\ p(X|Y) \u0026amp;= \\frac{p(Y|X)p(X)}{p(Y)} \\end{align*}\nIn this context, \\(p(X|Y)\\) is referred to as the posterior probability of event \\(X\\) conditioned on the fact that we know event \\(Y\\) has occurred. On the right, \\(p(X)\\) is the prior probability of event \\(X\\) in the absence of any additional evidence.\nTwo variables are independent, then\n\\[ p(X, Y) = p(X)p(Y) \\]\nIf two variables are conditionally independent given a third event, then\n\\[ p(X, Y|Z) = P(X|Z)P(Y|Z) \\]\nProbability Distributions Events come from a space of possible outcomes.\n\\begin{equation*} \\Omega = {1, 2, 3, 4, 5, 6} \\end{equation*}\nA measureable event is one for which we can assign a probability.\nAn event space must satisfy the following:\nIt contains the empty event \\(\\emptyset\\) and trivial event \\(\\Omega\\) It is closed under union It is closed under complementation: if \\(\\alpha \\in S\\), so is \\(\\Omega - \\alpha\\) Statement 2 implies difference and intersection A probability distribution \\(P\\) over \\((\\Omega, S)\\) maps events \\(S\\) to real values and satisfies:\n\\(P(\\alpha) \\geq 0\\) for all \\(\\alpha \\in S\\) \\(P(\\Omega) = 1\\) If \\(\\alpha,\\beta \\in S\\) and \\(\\alpha \\cap \\beta = \\emptyset\\), then \\(P(\\alpha \\cup \\beta) = P(\\alpha)+P(\\beta)\\) \\(P(\\emptyset) = 0\\) \\(P(\\alpha \\cup \\beta) = P(\\alpha) + P(\\beta) - P(\\alpha \\cap \\beta)\\) Conditional Probability Defined as\n\\begin{equation*} P(\\beta | \\alpha) = \\frac{P(\\alpha \\cap \\beta)}{P(\\alpha)} \\end{equation*}\nThe more that \\(\\alpha\\) and \\(\\beta\\) relate, the higher the probability.\nThe Chain Rule of Probability \\begin{equation*} P(\\alpha \\cap \\beta) = P(\\alpha) P(\\beta | \\alpha) \\end{equation*}\nGenerally\u0026hellip;\n\\begin{equation*} P(\\alpha_1 \\cap \\dotsb \\cap \\alpha_k) = P(\\alpha_1)P(\\alpha_2 | \\alpha_1) \\dotsm P(\\alpha_k | \\alpha_1 \\cap \\dotsb \\cap \\alpha_{k-1}) \\end{equation*}\nBayes\u0026rsquo; Rule \\begin{equation*} P(\\alpha | \\beta) = \\frac{P(\\beta | \\alpha)P(\\alpha)}{P(\\beta)} \\end{equation*}\nComputes the inverse conditional probability.\nA general conditional version of Baye\u0026rsquo;s rule:\n\\begin{equation*} P(\\alpha | \\beta \\cap \\gamma) = \\frac{P(\\beta | \\alpha \\cap \\gamma)P(\\alpha | \\gamma)}{P(\\beta | \\gamma)} \\end{equation*}\nExample: TB Tests A common example for introduction Bayes\u0026rsquo; rule is that of the test that gives 95% accuracy. The naive assumption here is that if you receive a positive result with no prior information, then there is a 95% chance you have the infection. This is wrong because that value is conditioned on already being infected.\nRules of Probability Sum Rule: \\(p(X) = \\sum_{Y}p(X, Y)\\)\nProduct Rule: \\(p(X, Y) = p(Y|X)p(X)\\)\nRandom Variables Allows for compact notation when talking about an event. It can also be represented as a function:\n\\begin{equation*} f_{\\text{Grade}} \\end{equation*}\nmaps each person in \\(\\Omega\\) to a grade value.\nRandom variables are commonly either categorical or real numbers.\nThe multinoulli distribution is one over \\(k \u0026gt; 2\\) categorical random variables. If \\(k = 2\\), the distribution is called the Bernoulli or binomial distribution.\nThe marginal distribution is one over a single random variable \\(X\\).\nA joint distribution is one over a set of random variables.\nThe marginal can be computed from a joint distribution.\n\\begin{equation*} P(x) = \\sum_{y}P(x, y) \\end{equation*}\nContinuous Variables The introductory example looked at events that take on discrete values. That is, we either selected a cookie or did not. Most of the problems we will deal with in this course involve continuous values. In this case, we are concerned with intervals that the values may take on. If we consider a small differential of our random variable \\(x\\) as \\(\\delta x\\), we can compute the probability density \\(p(x)\\).\nFigure 1: PDF (p(x)) and CDF (P(x)). Source: Bishop With this differential \\(\\delta x\\), we can compute the probability that \\(x\\) lies on some interval \\((a, b)\\):\n\\begin{equation*} p(a \\leq x \\leq b) = \\int_{a}^{b} p(x) dx \\end{equation*}\nAs with discrete probability distributions, the probability density must sum to 1 and cannot take a negative value. That is\n\\begin{align*} p(x) \u0026amp;\\geq 0\\\\ \\int_{-\\infty}^{\\infty}p(x)dx \u0026amp;= 1 \\end{align*}\nIn the plot above, \\(p(x)\\) is the probability density function (pdf) and \\(P(x)\\) is the cumulative distribution function (cdf). It is possible for a pdf to have a value greater than 1, as long as integrals over any interval are less than or equal to 1.\nThe cumulative distribution function \\(P(x)\\) is the probability that \\(x\\) lies in the interval \\((-\\infty, z)\\), given by\n\\begin{equation*} P(z) = \\int_{\\infty}^{z} p(x)dx. \\end{equation*}\nNote that the derivative of the cdf is equal to the pdf.\nThe product rule for continuous probability distributions takes on the same form as that of discrete distributions. The sum rule is written in terms of integration:\n\\begin{equation*} p(x) = \\int p(x, y)dy. \\end{equation*}\nMoments of a Distribution A moment of a function describes a quantitative measurement related to its graph. With respect to probability densities, the $k$th moment of \\(p(x)\\) is defined as \\(\\mathbb{E}[x^k]\\). The first moment is the mean of the distribution, the second moment is the variance, and the third moment is the skewness.\nThree extremely important statistics for any probability distribution are the average, variance, and covariance.\nExpectation The average of a function \\(f(x)\\) under a probability distribution \\(p(x)\\) is referred to as the expectation of \\(f(x)\\), written as \\(\\mathbb{E}[f]\\). The expectation for discrete and continuous distributions are\n\\begin{align*} \\mathbb{E}[f] \u0026amp;= \\sum_x p(x)f(x) \\text{ and}\\\\ \\mathbb{E}[f] \u0026amp;= \\int p(x)f(x)dx, \\end{align*}\nrespectively.\nFigure 2: Expectation of rolling a d6 over ~1800 trials converges to 3.5. Source: Seeing Theory The mean value for a discrete and continuous probability distribution is define as\n\\begin{align*} \\mathbb{E}[f] \u0026amp;= \\sum_x p(x)x \\text{ and}\\\\ \\mathbb{E}[f] \u0026amp;= \\int_{-\\infty}^{\\infty} p(x)xdx, \\end{align*}\nrespectively.\nEmpirically, we can approximate this quantity given \\(N\\) samples as\n\\begin{equation*} \\mathbb{E}[f] \\approx \\frac{1}{N}\\sum_{i=1}^{N}f(x_i). \\end{equation*}\nVariance The variance of a function \\(f(x)\\) under a probability distribution \\(p(x)\\) measures how much variability is in \\(f(x)\\) around the expected value \\(\\mathbb{E}[f(x)]\\) and is defined by\n\\begin{align*} \\text{var}[f] \u0026amp;= \\mathbb{E}[(f(x) - \\mathbb{E}[f(x)])^2]\\\\ \u0026amp;= \\mathbb{E}[f(x)^2] - \\mathbb{E}[f(x)]^2. \\end{align*}\nFigure 3: Variance of drawing cars with values 1-10 100 trials converges to 8.79. True variance is 8.25. Source: Seeing Theory Covariance The covariance of two random variables \\(x\\) and \\(y\\) provides a measure of dependence between the two variables. This implies that the covariance between two independent variables is 0.\n\\begin{align*} \\text{cov}[\\mathbf{x},\\mathbf{y}] \u0026amp;= \\mathbf{E}_{\\mathbf{x},\\mathbf{y}}[\\{\\mathbf{x} - \\mathbb{E}[\\mathbf{x}]\\}\\{\\mathbf{y}^T - \\mathbb{E}[\\mathbf{y}^T]\\}]\\\\ \u0026amp;= \\mathbb{E}_{\\mathbf{x},\\mathbf{y}}[\\mathbf{x}\\mathbf{y}^T] - \\mathbb{E}[\\mathbf{x}]\\mathbb{E}[\\mathbf{y}^T]. \\end{align*}\nFigure 4: Plot of 2D data with negative covariance. Source: Wikipedia Figure 5: Plot of 2D data with approximately 0 covariance. Source: Wikipedia Figure 6: Plot of data with positive covariance. Source: Wikipedia Correlation The correlation between two random variables \\(x\\) and \\(y\\) relates to their covariance, but it is normalized to lie between -1 and 1.\n\\begin{equation*} \\text{corr}[\\mathbf{x},\\mathbf{y}] = \\frac{\\text{cov}[\\mathbf{x},\\mathbf{y}]}{\\sqrt{\\text{var}[\\mathbf{x}]\\text{var}[\\mathbf{y}]}} \\end{equation*}\nThe correlation between two variables will equal 1 if there is a linear relationship between them. We can then view the correlation as providing a measurement of linearity.\nFigure 7: Sets of points with their correlation coefficients. Source: Wikipedia Limitations of Moments Summary statistics can be useful but do not tell the whole story of your data. When possible, it is always better to visualize the data. An example of this is the Anscombosaurus, derived from the Anscombe\u0026rsquo;s quartet. The quartet consists of four datasets that have nearly identical summary statistics but are visually distinct. A modern version, called the Datasaurus Dozen, consists of 12 datasets that have the same summary statistics but are visually distinct.\nFigure 8: Datasaurus Dozen (source: Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing) ","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706572800,"objectID":"03c02884d03e4e29be21f5ab9a6ade33","permalink":"https://ajdillhoff.github.io/notes/probability_theory/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/probability_theory/","section":"notes","summary":"Table of Contents Introduction A Simple Example Probability Distributions Conditional Probability Rules of Probability Random Variables Continuous Variables Moments of a Distribution Slides for these notes are available here.\nIntroduction Probability theory provides a consistent framework for the quantification and manipulation of uncertainty. It allows us to make the best decisions given the limited information we may have. Many tasks, models, and evaluation metrics that we will explore in this course are either based on, or are inspired by, probability theory.","tags":null,"title":"Probability Theory","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Overfitting Penalizing Weights Dataset Augmentation Early Stopping Dropout Slides for these notes are available here.\nIntroduction Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error. - Goodfellow et al.\nRegularization comes in many forms. Some techniques may add an additional penalty to the loss function. Others, such as data augmentation, add artificial variation to the data. In all cases, regularization aims to improve the generalization performance by preventing the model from overfitting.\nOverfitting What happens when the complexity of our chosen model fits the data too well? Take a look at the following plot of data. The red curve is the true underlying function that generated the data. The blue line represents a polynomial of degree 9 fit via linear regression. It is first necessary to understand what is happening.\nFigure 1: A polynomial of degree 11 (blue) fit to data generated following the red line. The model with more parameters is able to fit some the noisy data slightly better. Does this necessarily mean it will perform better on new samples? No, it will usually perform worse. This is referred to as overfitting. Overfitting can be identified as the model trains. When the testing loss continues to decrease while the validation loss increases, the model is probably overfitting. It is also evident from looking at the weights.\nIdentifying the Cause The goal of training is to modify the weights such that they minimize the loss function. Models with more parameters have the capacity to fit more of their training data. Given the presence of noise, this is not a good thing. A very low loss on the training set may not translate to good performance on the validation set.\nLooking at weights of the trained model is a good way of detecting overfitting. From the model above, the mean of the absolute value of the weights is \\(11.1\\). Left unchecked, the weights will take on whatever values necessary to meet the objective function.\nPenalizing Weights The most common form of regularization is to penalize the weights from taking on a high value. That is, we define a penalty term \\(E(\\mathbf{w})\\) that is added to the loss. The higher the weight values, the higher the total loss. Thus, optimization will also include minimizing the absolute values of the weights. A simple choice for \\(E(\\mathbf{w})\\), especially in the context of least squares, is \\(L2\\) regularzation:\n\\[ E(\\mathbf{w}) = \\frac{\\lambda}{2}||\\mathbf{w}||^2 = \\frac{\\lambda}{2}\\mathbf{w}^T \\mathbf{w}. \\]\nAdded to the sum-of-squares error for least squares, the final loss becomes\n\\[ J(\\mathbf{w}) = \\frac{1}{2}\\sum_{i=1}^n(h(\\mathbf{x}_i;\\mathbf{w}) - \\mathbf{y}_i)^2 + \\frac{\\lambda}{2} \\mathbf{w}^T \\mathbf{w}. \\]\nThis choice of regularization also has the benefit of being in a form that can be minimized in closed form via the normal equations. Taking the gradient of \\(J(\\mathbf{w})\\) above with respect to 0 and solving for \\(\\mathbf{w}\\) yields\n\\[ \\mathbf{w} = (\\lambda I + \\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}, \\]\nwhere \\(\\lambda\\) is a regularization hyperparameter.\nApplying this regularization term to the model above with \\(\\lambda=1\\) yields the model shown below.\nFigure 2: Least squares model fit with (L2) regularization ((lambda = 1)). Inspecting the weights as before, we can see that the mean of the absolute values of \\(\\mathbf{w}\\) is \\(0.0938\\).\nEvaluating on the Testing Data To see which model generalizes better, we set aside some samples from the original dataset to use as testing.\nWith regularization, the model error on the test set is \\(1.8\\). Without regularization, the model error on the test set is \\(2.2\\).\nDataset Augmentation The same data augmentation techniques should be applied on both methods being compared. Getting a better result on a benchmark because of data augmentation does not mean the method was better suited for the task. By controlling these factors, a fair comparison can be made.\nThere are many forms of augmentation available for image tasks in particular. Rotating, translating, and scaling images are the most common. Additionally applying random crops can further augment the dataset.\nThe original dataset may only include samples of a class that have similar lighting. Color jitter is an effective way of including a broader range of hue or brightness and usually leads to a model that is robust to such changes.\nIt is important to make sure that the crops still contain enough information to properly classify it. Common forms of data augmentation are available through APIs like torchvision.\nEarly Stopping If the validation loss begins to increase while the training loss continues to decrease, this is a clear indication that the model is beginning to overfit the training data. Stopping the model in this case is the best way to prevent this. Frameworks like PyTorch Lightning include features to checkpoing the models based on best validation loss and stop the model whenever the validation loss begins to diverge.\nDropout Dropout is a regularization method introduced by \u0026lt;\u0026amp;srivastavaDropoutSimpleWay2014\u0026gt; which is motivated by ensemble methods. Ensembles of models are regularized by the fact that many different models are trained on random permutations of the dataset with varying parameters and initializations. Using an ensemble of networks is a powerful way of increasing generalization performance. However, it requires much more compute due to the fact that several models must be trained.\nTraining a single network with dropout approximates training several models in an ensemble. It works by randomly removing a node from the network during a forward/backward pass. The node is not truly removed. Instead, its output during the forward and backward passes is ignored via a binary mask.\nWhen training a network with dropout, it will generally take longer for the model to converge to a solution. Intuitively, this is because a different subnetwork is being used for each pass.\n","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706486400,"objectID":"4be961e39b7e422bb57adc26e82744ea","permalink":"https://ajdillhoff.github.io/notes/regularization/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/regularization/","section":"notes","summary":"Table of Contents Introduction Overfitting Penalizing Weights Dataset Augmentation Early Stopping Dropout Slides for these notes are available here.\nIntroduction Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error. - Goodfellow et al.\nRegularization comes in many forms. Some techniques may add an additional penalty to the loss function. Others, such as data augmentation, add artificial variation to the data.","tags":["machine learning"],"title":"Regularization","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Difference of Gaussians Keypoint Localization Orientation Assignment Descriptor Formation Introduction https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\nGeneral approach to computing SIFT features:\nScale-space Extrema Detection Keypoint localization Orientation Assignment Generate keypoint descriptors Difference of Gaussians This same technique for detecting interesting points in a scale-invariant way can be approximated by taking the Difference of Gaussians. Consider the figure below.\nFigure 1: Comparison of DoG and Laplacian. Credit: Fei-Fei Li. By taking the difference of images smoothed by a Gaussian with different values of \\(\\sigma\\), the resulting pixel values correspond to areas with high gradient norms in the less blurry version.\nLet \\(I_{\\sigma_1}\\) be the image blurred with a smaller value of \\(\\sigma\\) and \\(I_{\\sigma_2}\\) be the image blurred with a larger value. Then \\(D(I_{\\sigma_1}, I_{\\sigma_2}) = I_{\\sigma_2} - I_{\\sigma_1}\\). If a region in \\(I_{\\sigma_1}\\) is locally flat, it will also be in flat in \\(I_{\\sigma_2}\\). The difference will be relatively small for that region. If there are abrupt changes in a local region within \\(I_{\\sigma_1}\\), they will be smoothed in \\(I_{\\sigma_2}\\). Therefore, the difference \\(D(I_{\\sigma_1}, I_{\\sigma_2})\\) will be higher for that region.\nFigure 2: The Royal Concertgebouw in Amsterdam. Figure 3: Difference of Gaussian between the original image blurred with (sigma = 0.5) and (sigma=1.5). When building SIFT features, the extremum are selected by comparing 3 DoG images. These are selected by evaluating each pixel to 26 of its neighbors in the current scale space and neighboring DoG spaces as visualized below.\nFigure 4: Finding extrema of pixel (i, j) in a neighborhood of 26 values (Lowe 2004). To build the DoG pyramid, the authors propose that images are separated by a constant factor \\(k\\) in scale space. Each octave of scale space is divided such that the scalespace doubles every \\(s\\) samples.\nStarting with \\(\\sigma = 0.5\\), if we choose \\(s=3\\) then the fourth sample will be at \\(\\sigma = 1\\), the seventh at \\(\\sigma=2\\), and so on. To make sure the DoG images cover the full range of an octave, \\(s + 3\\) images need to be created per octave.\nWhy \\(s + 3\\)?\nEach octave should evaluate local extrema for \\(s\\) scales. To evaluate this for scale \\(\\sigma_s\\), we need the DoG for scales \\(\\sigma_{s-1}\\) and \\(\\sigma_{s+1}\\). This would require 4 Gaussians images to compute. The figure below represents the stack for \\(s=2\\).\nFigure 5: DOG figure (Lowe 2004). How is the value of \\(s\\) determined?\nIn the paper, the authors perform a repeatability test to determine if the keypoints would be localized even with random augmentations. The process is as follows:\nRandomly augment an input image with noise, color jitter, scale, rotation, etc. Compute keypoints using using the extrema detection. Compare detected keypoints with known keypoints from original samples. The authors found that using \\(s = 3\\) provided the highest percentage of repeatability in their experiments.\nFigure 6: Measuring repeatability of keypoint detections versus # of scales sampled per octave (Lowe 2004). Keypoint Localization Given the candidate keypoints selected by picking out local extrema, they pool of responses can further be refined by removing points that are sensitive to noise or located along an edge. They borrow the same approach used in the Harris corner detector to select more robust interest points in corners.\nFigure 7: Refinement of candidate keypoints by filtering those sensitive to noise (c) and those representing ambiguity along edges (d) (Lowe 2004). Orientation Assignment Given a keypoint, an orientation histogram is generated. The authors use 36 bins to cover a 360 degree range for orientations. Similar to Histogram of Oriented Gradients, the orientations are weighted by their gradient magnitudes (Dalal and Triggs 2005). Additionally, a Gaussian-weighted circular patch is applied, centered on the keypoint, to further weight the responses. This means that points farther away from the center contribute less to the overall feature vector.\nIn order to make the keypoint rotation invariant, the dominant orientation is determined. If there are orientations that are within 80% of the highest orientation peak, multiple keypoints will be created using those orientations as well.\nOrientations in this window are rotated by the dominant gradient so that all directions are with respect to the dominant orientation. This is a more efficient alternative to rotating the entire image by that orientation.\nDescriptor Formation Figure 8: Keypoint descriptor generation (Lowe 2004). In the paper, the authors generate keypoints using a \\(16 \\times 16\\) window from which \\(4 \\times 4\\) descriptors are generated following the descriptions above. Through experimentation, each \\(4 \\times 4\\) descriptor uses 8 orientations, resulting in a feature vector \\(\\mathbf{x} \\in \\mathbb{R}^{128}\\).\nDifferent levels of contrast will product edges with higher gradient magnitudes. To account for this, the final feature vector is normalized using the \\(L2\\) hysteresis approach used in Harris corner detection.\nReferences Dalal, N., and B. Triggs. 2005. “Histograms of Oriented Gradients for Human Detection.” In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05), 1:886–93 vol. 1. https://doi.org/10.1109/CVPR.2005.177. Lowe, David G. 2004. “Distinctive Image Features from Scale-Invariant Keypoints.” International Journal of Computer Vision 60 (2): 91–110. https://doi.org/10.1023/B:VISI.0000029664.99615.94. ","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706400000,"objectID":"d0ee980feb00beaf92debc461f4179ad","permalink":"https://ajdillhoff.github.io/notes/scale_invariant_feature_transforms/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/scale_invariant_feature_transforms/","section":"notes","summary":"Table of Contents Introduction Difference of Gaussians Keypoint Localization Orientation Assignment Descriptor Formation Introduction https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\nGeneral approach to computing SIFT features:\nScale-space Extrema Detection Keypoint localization Orientation Assignment Generate keypoint descriptors Difference of Gaussians This same technique for detecting interesting points in a scale-invariant way can be approximated by taking the Difference of Gaussians. Consider the figure below.\nFigure 1: Comparison of DoG and Laplacian. Credit: Fei-Fei Li. By taking the difference of images smoothed by a Gaussian with different values of \\(\\sigma\\), the resulting pixel values correspond to areas with high gradient norms in the less blurry version.","tags":["computer vision"],"title":"Scale Invariant Feature Transforms","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Maximum Margin Classifier Formulation Overlapping Class Distributions Multiclass SVM Additional Resources Introduction Support Vector Machines are a class of supervised learning methods primarily used for classification. Although they can be formulated for regression and outlier detection as well. Instead of optimizing a set of parameters which compress or summarize the training set, they use a small subset of the training data to compute the decision function.\nThey rely on the data being linearly separable, so feature transformations are critical for problems in which the original representation of the data is not linearly separable.\nMaximum Margin Classifier Let\u0026rsquo;s start with a simple classification model as we studied with Logistic Regression. That is, we have\n\\[ f(\\mathbf{x}) = \\mathbf{w}^T\\phi(\\mathbf{x}), \\]\nwhere \\(\\phi(\\mathbf{x})\\) is a function which transforms our original input into some new feature space. The transformed input is assumed to be linearly separable so that a decision boundary can be computed. In the original logistic regression problem, a decision boundary was found through optimization. For linearly separable data, there are an infinite number of decision boundaries that satisfy the problem.\nWhat about the quality of the decision boundary?\nIs one decision boundary better than the other?\nFormulation Given a training set \\(\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\}\\) with labels \\(\\{y_1, \\dots, y_n\\}\\), where \\(y_i \\in \\{-1, 1\\}\\), we construct a linear model which classifies an input sample depending on the sign of the output.\nOur decision rule for classification, given some input \\(\\mathbf{x}\\), is\n\\begin{equation*} f(\\mathbf{x}) = \\begin{cases} 1\\text{ if }\\mathbf{w}^T\\mathbf{x} + b \\geq 0\\\\ -1\\text{ if }\\mathbf{w}^T\\mathbf{x} + b \u0026lt; 0 \\end{cases} \\end{equation*}\nHow large should the margin be?\nIn the original formulation of Logistic Regression, we saw that the parameter vector \\(\\mathbf{w}\\) described the normal to the decision boundary. The distance between a given point \\(\\mathbf{x}\\) and the decision boundary is given by\n\\[ \\frac{y_if(\\mathbf{x})}{||\\mathbf{w}||}. \\]\nWe can frame this as an optimization problem: come up with a value for \\(\\mathbf{w}\\) that maximizes the margin.\n\\[ \\text{arg max}_{\\mathbf{w}, b} \\frac{1}{\\|\\mathbf{w}\\|}\\min_{i} y_i (\\mathbf{w}^T\\phi(\\mathbf{x}_i) + b) \\]\nWe can arbitrarily scale the parameters, so we add an additional constraint that any point that lies on the boundary of the margin satisfies\n\\[ y_i(\\mathbf{w}^T\\mathbf{x} + b) = 1. \\]\nUnder this constraint, we have that all samples satisfy\n\\[ y_i(\\mathbf{w}^T\\mathbf{x} + b) \\geq 1. \\]\nThat is, all positive samples with target \\(1\\) will produce at least a \\(1\\), yielding a value greater than or equal to 1. All negative samples with target \\(-1\\) will produce at most a \\(-1\\), yielding a value greater than or equal to 1.\nAnother way of writing this is\n\\begin{equation*} f(\\mathbf{x}) = \\begin{cases} 1\\text{ if }\\mathbf{w}^T\\mathbf{x}_{+} + b \\geq 1\\\\ -1\\text{ if }\\mathbf{w}^T\\mathbf{x}_{-} + b \\leq -1, \\end{cases} \\end{equation*}\nwhere \\(\\mathbf{x}_+\\) is a positive sample and \\(\\mathbf{x}_-\\) is a negative sample. The decision rule can then be written as\n\\[ y_i(\\mathbf{w}^T\\mathbf{x} + b) - 1 \\geq 0. \\]\nThis implies that the only samples that would yield an output of 0 are those that lie directly on the margins of the decision boundary.\nGiven this constraint of \\(y_i(\\mathbf{w}^T\\mathbf{x} + b) - 1 = 0\\), we can derive our optimization objective.\nThe margin can be computed via the training data. To do this, consider two data points which lie on their respective boundaries, one positive and one negative, and compute the distance between them: \\(\\mathbf{x}_+ - \\mathbf{x}_-\\). This distance with respect to our decision boundary, defined by \\(\\mathbf{w}\\), is given by\n\\[ (\\mathbf{x}_+ - \\mathbf{x}_-) \\cdot \\frac{\\mathbf{w}}{||\\mathbf{w}||}. \\]\nFor clarity, we can rewrite this as\n\\begin{equation*} \\frac{1}{||\\mathbf{w}||}(\\mathbf{x}_{+} \\cdot \\mathbf{w} - \\mathbf{x}_{-} \\cdot \\mathbf{w}). \\end{equation*}\nIf we substitute the sample values into the equality constraint above, we can simplify this form. For the positive sample, we have \\(\\mathbf{w}^T\\mathbf{x} = 1 - b\\). For the negative sample, we get \\(\\mathbf{w}^T\\mathbf{x} = -1 - b\\). The equation above then becomes\n\\begin{equation*} \\frac{1}{||\\mathbf{w}||}(1 - b - (-1 - b)) = \\frac{2}{||\\mathbf{w}||}. \\end{equation*}\nThus, our objective is to maximize \\(\\frac{2}{||\\mathbf{w}||}\\) which is equivalent to minimizing \\(\\frac{1}{2}||\\mathbf{w}||^2\\) subject to the constraints \\(y_i(\\mathbf{w}^T\\mathbf{x}+b)\\geq 1\\). This is a constrainted optimization problem. As discussed previously, we can simplify such problems by introducing Lagrangian Multipliers. Doing this produces the dual representation of our optimization objection:\n\\begin{equation*} L = \\frac{1}{2}||\\mathbf{w}||^2 - \\sum_{i=1}^n \\alpha_i \\big(y_i(\\mathbf{w}^T\\mathbf{x}_i + b) - 1\\big). \\end{equation*}\nTo solve for \\(\\mathbf{w}\\) we compute \\(\\frac{\\partial}{\\partial \\mathbf{w}}L\\).\n\\begin{equation*} \\frac{\\partial}{\\partial \\mathbf{w}}L = \\mathbf{w} - \\sum_{i=1}^n \\alpha_i y_i \\mathbf{x}_i. \\end{equation*}\nSetting this to 0 yields\n\\begin{equation*} \\mathbf{w} = \\sum_{i=1}^n \\alpha_i y_i \\mathbf{x}_i. \\end{equation*}\nDoing the same for the other parameter \\(b\\) yields\n\\[ 0 = \\sum_{i=1}^n \\alpha_i y_i. \\]\nWe can now simplify our objective function by substituting these results into it:\n\\begin{align*} L \u0026amp;= \\frac{1}{2}\\Big(\\sum_{i=1}^n \\alpha_i y_i \\mathbf{x}_i\\Big)^2 - \\sum_{i=1}^n \\alpha_i\\Big(y_i\\big((\\sum_{i=1}^n\\alpha_i y_i \\mathbf{x}_i)^T\\mathbf{x}_i + b \\big) - 1 \\Big)\\\\ \u0026amp;= \\frac{1}{2}\\Big(\\sum_{i=1}^n \\alpha_i y_i \\mathbf{x}_i\\Big)^2 - \\Big(\\sum_{i=1}^n \\alpha_i y_i \\mathbf{x}_i \\Big)^2 - \\sum_{i=1}^n \\alpha_i y_i b + \\sum_{i=1}^n \\alpha_i\\\\ \u0026amp;= -\\frac{1}{2} \\Big(\\sum_{i=1}^n \\alpha_i y_i \\mathbf{x}_i \\Big)^2 + \\sum_{i=1}^n \\alpha_i\\\\ \u0026amp;= \\sum_{i=1}^n \\alpha_i - \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i \\cdot \\mathbf{x}_j \\end{align*}\nThus, the objective is dependent on the inner product of samples \\(\\mathbf{x}_i\\) and \\(\\mathbf{x}_j\\). If these were representations in some complex feature space, our problem would remain computationally inefficient. However, we can take advantage of Kernels for this.\nNote that, in most cases, \\(\\alpha_i\\) will be 0 since we only consider support vectors. That is, the points that lie on the margins of the decision boundary.\nOverlapping Class Distributions The above formulation is fine and works with datasets that have no overlap in feature space. That is, they are completely linearly separable. However, it is not always the case that they will be.\nTo account for misclassifications while still maximizing a the margin between datasets, we introduce a penalty value for points that are misclassified. As long as there aren\u0026rsquo;t too many misclassifications, this penalty will stay relatively low while still allowing us to come up with an optimal solution.\nThis penalty comes in the form of a slack variable \\(\\xi_i \\geq 0\\) for each sample that is \\(0\\) for points that are on or inside the correct margin and \\(\\xi_i = |y_i - f(\\mathbf{x})|\\) for others. If the point is misclassified, its slack variable will be \\(\\xi_i \u0026gt; 1\\).\nMulticlass SVM Similar to our simple Logistic Regression method, SVMs are binary classifiers by default. We can take a similar approach to extending them to multiple classes, but there are downsides to each approach.\nThe \u0026ldquo;one-vs-all\u0026rdquo; approach entails building \\(|K|\\) classifiers and choose the classifier which predicts the input with the greatest margin.\nThe \u0026ldquo;one-vs-one\u0026rdquo; approach involves building \\(|K|\\cdot\\frac{|K| - 1}{2}\\) classifiers. In this case, training each classifer will be more tractable since the amount of data required for each one is less. For example, you would have a model for class 1 vs 2, class 1 vs 3, \u0026hellip;, class 1 vs \\(K\\). Then repeat for class 2: 2 vs 3, 2 vs 4, \u0026hellip;, 2 vs \\(|K|\\), and so on.\nA third approach is to construct several models using a feature vector dependent on both the data and class label. When given a new input, the model computes\n\\[ y = \\text{arg}\\max_{y\u0026rsquo;}\\mathbf{w}^T\\phi(\\mathbf{x},y\u0026rsquo;). \\]\nThe margin for this classifier is the distance between the correct class and the closest data point of any other class.\nAdditional Resources https://web.mit.edu/6.034/wwwbob/svm.pdf https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf ","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719273600,"objectID":"0072e15bf08a567be6de4867b7b022a6","permalink":"https://ajdillhoff.github.io/notes/support_vector_machine/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/support_vector_machine/","section":"notes","summary":"Table of Contents Introduction Maximum Margin Classifier Formulation Overlapping Class Distributions Multiclass SVM Additional Resources Introduction Support Vector Machines are a class of supervised learning methods primarily used for classification. Although they can be formulated for regression and outlier detection as well. Instead of optimizing a set of parameters which compress or summarize the training set, they use a small subset of the training data to compute the decision function.","tags":["machine learning"],"title":"Support Vector Machine","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Probabilistic Interpretation Solving with Normal Equations Another Approach to Normal Equations Fitting Polynomials Linear Basis Functions Slides for these notes are available here.\nIntroduction Given a dataset of observations \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\), where \\(n\\) is the number of samples and \\(d\\) represents the number of features per sample, and corresponding target values \\(\\mathbf{Y} \\in \\mathbb{R}^n\\), create a simple prediction model which predicts the target value \\(\\mathbf{y}\\) given a new observation \\(\\mathbf{x}\\). The classic example in this case is a linear model, a function that is a linear combination of the input features and some weights \\(\\mathbf{w}\\).\nFigure 1: Plot of univariate data where the (x) values are features and (y) are observations. The generated data is plotted above along with the underlying true function that was used to generate it. If we already know what the true function is, our job is done. Suppose that we only have the data points (in blue). How do we go about modelling it? It is reasonable to first visualize the data and observe that it does follow a linear pattern. Thus, a linear model would be a decent model to choose.\nIf the data followed a curve, we may decide to fit a polynomial. We will look at an example of that later on. For now, let\u0026rsquo;s formalize all of the information that we have.\n\\((\\mathbf{x}, \\mathbf{y})\\) - Data points from the original dataset. Generally, \\(\\mathbf{x}\\) is a vector of features and \\(\\mathbf{y}\\) is the target vector. In our simple dataset above, these are both scalar values. \\(\\mathbf{w} = (w_0, w_1)\\) - Our model parameters. Comparing to the equation \\(y = mx + b\\), \\(w_0\\) is our bias term and \\(w_1\\) is our slope parameter. Making Predictions Given \\(\\mathbf{w}\\), we can make a prediction for a new data sample \\(\\mathbf{x} = x_1\\).\n\\[ h(\\mathbf{x}; \\mathbf{w}) = w_0 + w_1 x_1 \\]\nNote that the bias term is always added to the result. We can simplify this into a more general form by appending a constant 1 (s.t. \\(x_0 = 1\\)) to each of our samples such that \\(\\mathbf{x} = (1, x_1, \u0026hellip;, x_d)\\). Then, the general linear model becomes\n\\[ h(\\mathbf{x}; \\mathbf{w}) = \\sum_{i=0}^{d} w_i x_i = \\mathbf{w}^T \\mathbf{x}. \\]\nIf our data happened to have more than 1 feature, it would be easy enough to model it appropriately using this notation.\nDetermining Fitness If we really wanted to, we could fit our model by plotting it and manually adjusting the weights until our model looked acceptable by some qualitative standard. Fortunately we won\u0026rsquo;t be doing that. Instead, we will use a quantitative measurement that provides a metric of how well our current parameters fit the data.\nFor this, we use a cost function or loss function. The most common one to use for this type of model is the least-squares function:\n\\[ J(\\mathbf{w}) = \\frac{1}{2}\\sum_{i=1}^{n}(h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i})^2. \\]\nStochastic Gradient Descent Depending on the random initialization of parameters, our error varies greatly. We can observe that no matter what the chose parameters are, there is no possible way we can achieve an error of 0. The best we can do is minimize this error:\n\\[ \\min_{\\mathbf{w}} J(\\mathbf{w}). \\]\nFor this, we rely on stochastic gradient descent. The basic idea is as follows:\nBegin with an initial guess for \\(\\mathbf{w}\\). Compare the prediction for sample \\(\\mathbf{x}^{(i)}\\) with its target \\(\\mathbf{y}^{(i)}\\). Update \\(\\mathbf{w}\\) based on the comparison in part 2. Repeat steps 2 and 3 on the dataset until the loss has converged. Steps 1, 3, and 4 are easy enough. What about step 2? How can we possibly know how to modify \\(\\mathbf{w}\\) such that \\(J(\\mathbf{w})\\) will decrease? By computing the gradient \\(\\frac{d}{d\\mathbf{w}}J(\\mathbf{w})\\)! How will we know when we have arrived at a minima? When \\(\\nabla J(\\mathbf{w}) = 0\\).\n\\begin{align*} \\frac{d}{d\\mathbf{w}}J(\\mathbf{w}) \u0026amp;= \\frac{d}{d\\mathbf{w}}\\frac{1}{2}(h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i})^2\\\\ \u0026amp;= 2 \\cdot \\frac{1}{2}(h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i}) \\cdot \\frac{d}{d\\mathbf{w}} (h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i})\\\\ \u0026amp;= (h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i}) \\cdot \\frac{d}{d\\mathbf{w}} (\\mathbf{w}^T \\mathbf{x}_{i} - \\mathbf{y}_{i})\\\\ \u0026amp;= (h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i}) \\mathbf{x}_{i} \\end{align*}\nThe gradient represents the direction of greatest change for a function evaluated With this gradient, we can use an update rule to modify the previous parameter vector \\(\\mathbf{w}\\):\n\\[ \\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\alpha \\sum_{i=1}^{n} (h(\\mathbf{x}_{i};\\mathbf{w}_{t}) - \\mathbf{y}_{i}) \\mathbf{x}_{i}. \\]\nHere, \\(\\alpha\\) is an update hyperparameter that allows us to control how big or small of a step our weights can take with each update. In general, a smaller value will be more likely to get stuck in local minima. However, too large of a value may never converge to any minima.\nAnother convenience of this approach is that it is possible to update the weights based on a single sample, batch of samples, or the entire dataset. This sequential process makes optimization using very large dataset feasible.\nProbabilistic Interpretation \u0026ldquo;Probability theory is nothing but common sense reduced to calculation.\u0026rdquo; - Pierre-Simon Laplace\nRecall Bayes\u0026rsquo; theorem:\n\\[ p(\\mathbf{w}|\\mathbf{X}) = \\frac{p(\\mathbf{X}|\\mathbf{w})p(\\mathbf{w})}{p(\\mathbf{X})}. \\]\nThat is, the posterior probability of the weights conditioned on the observered data \\(\\mathbf{X}\\) is equal to the likelihood of the observed data given the times the prior distribution. This base notation doesn\u0026rsquo;t line up well with our problem. For our problem, we have observations \\(\\mathbf{Y}\\) which are dependent on the input features \\(\\mathbf{X}\\):\n\\[ p(\\mathbf{w}|\\mathbf{X}, \\mathbf{Y}) = \\frac{p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}) p(\\mathbf{w}|\\mathbf{X})}{p(\\mathbf{Y}|\\mathbf{X})}, \\]\nwhere \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) and \\(\\mathbf{Y} \\in \\mathbb{R}^n\\).\nThe choice of least squares also has statistical motivations. As discussed previously, we are making a reasonable assumption that there is some relationship between the features of the data and the observed output. This is typically modeled assume\n\\[ \\hat{\\mathbf{Y}} = f(\\mathbf{X}) + \\epsilon. \\]\nHere, \\(\\epsilon\\) is a random error term that is independent of \\(\\mathbf{X}\\) and has 0 mean. This term represents any random noise that occurs either naturally or from sampling. It also includes any effects that are not properly captured by \\(f\\). Rearranging the terms of this equation to solve for \\(\\epsilon\\) allows us to define the discrepencies in the model:\n\\[ \\mathbf{\\epsilon}_i = h(\\mathbf{x}_{i}; \\mathbf{w}) - \\mathbf{y}_{i}. \\]\nIf we assume that these discrepancies are independent and identically distributed with variance \\(\\sigma^2\\) and Gaussian PDF \\(f\\), the likelihood of observations \\(\\mathbf{y}^{(i)}\\) given parameters \\(\\mathbf{w}\\) is\n\\[ p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}, \\sigma) = \\prod_{i=1}^{n} f(\\epsilon_i; \\sigma), \\]\nwhere\n\\[ f(\\epsilon_i; \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\Big(-\\frac{\\epsilon^2}{2\\sigma^2}\\Big). \\]\nThis new parameter changes our original distribution function to\n\\[ p(\\mathbf{w}|\\mathbf{X}, \\mathbf{Y}, \\sigma) = \\frac{p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}, \\sigma) p(\\mathbf{w}|\\mathbf{X}, \\sigma)}{p(\\mathbf{Y}|\\mathbf{X}, \\sigma)}. \\]\nTwo things to note before moving on. First, the prior \\(p(\\mathbf{Y}|\\mathbf{X}, \\sigma)\\) is a normalizing constant to ensure that the posterior is a valid probability distribution. Second, if we assume that all value for \\(\\mathbf{w}\\) are equally likely, then \\(p(\\mathbf{w}|\\mathbf{x}, \\sigma)\\) also becomes constant. This is a convenient assumption which implies that maximizing the posterior is equivalent to maximizing the likelihood function.\nWith that out of the way, we can focus solely on the likelihood function. Expanding out the gaussian PDF \\(f\\) yields\n\\[ p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}, \\sigma) = -\\frac{n}{\\sqrt{2\\pi\\sigma^2}}\\exp\\Big(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i})^2\\Big). \\]\nWe can see that maximizing \\(p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}, \\sigma)\\) is the same as minimizing the sum of squares. In practice, we use the negative log of the likelihood function since the negative logarithm is monotonically decreasing.\nSolving with Normal Equations You may have studied the normal equations when you took Linear Algebra. The normal equations are motivated by finding approximate solutions to \\(A\\mathbf{x} = \\mathbf{b}\\). Most of the earlier part of linear algebra courses focus on finding exact solutions by solving systems of equations using Gaussian elimination (row reduction). Approximate solutions can be found by projecting the observed data points \\(\\mathbf{b}\\) onto the column space of \\(A\\) and solving \\(A \\mathbf{x} = \\hat{\\mathbf{b}}\\), where \\(\\hat{\\mathbf{b}} = \\text{proj}_{\\text{Col} A}\\mathbf{b}\\). Then, \\(\\mathbf{b} - \\hat{\\mathbf{b}}\\) represents a vector orthogonal to \\(\\text{Col}A\\).\nIt is helpful to keep in mind what \\(A\\), \\(\\mathbf{x}\\), and \\(\\mathbf{b}\\) represent. \\(A\\) and \\(\\mathbf{b}\\) are the input and output values. If we were trying to predict home prices based on size, each row of \\(A\\) would represent the size of a different house. In \\(\\mathbf{b}\\) we would record the corresponding prices. We are trying to solve for \\(\\mathbf{x}\\), which is a vector that relates the input to the output.\nFigure 2: The plane represents every linear combination of the columns of A. Since each column vector of \\(A\\) is orthogonal to \\(\\mathbf{b} - \\hat{\\mathbf{b}}\\), the dot product between them should be 0. Rewriting this, we get\n\\begin{aligned} A^T(\\mathbf{b} - A\\mathbf{x}) \u0026amp;= \\mathbf{0}\\\\ A^T \\mathbf{b} - A^T A \\mathbf{x} \u0026amp;= \\mathbf{0}. \\end{aligned}\nThis means that each least-squares solution of \\(A\\mathbf{x} = \\mathbf{b}\\) satisfies\n\\[ A^T A \\mathbf{x} = A^T \\mathbf{b}. \\]\nExample Let\u0026rsquo;s take our univariate problem of \\((\\mathbf{x}, \\mathbf{y})\\) pairs. To use the normal equations to solve the least squares problem, we first change the notation just a bit as not confuse our data points and our parameters:\n\\[ \\mathbf{X}^T \\mathbf{X} \\beta = \\mathbf{X}^T \\mathbf{y} \\]\nCreate the design matrix \\(\\mathbf{X}\\) where each row represents the the \\(\\mathbf{x}\\) values. Recall that even though we only have 1 feature for \\(\\mathbf{x}\\), we append the bias constant as \\(x_0 = 1\\) to account for the bias parameter. \\(\\mathbf{X}\\) is then\n\\begin{equation*} \\mathbf{X} = \\begin{bmatrix} x_0^{(0)} \u0026amp; x_1^{(0)}\\\\ x_0^{(1)} \u0026amp; x_1^{(1)}\\\\ \\vdots \u0026amp; \\vdots \\\\ x_0^{(n)} \u0026amp; x_1^{(n)} \\end{bmatrix}. \\end{equation*}\nThe parameter vector is\n\\begin{equation*} \\beta = \\begin{bmatrix} \\beta_0\\\\ \\beta_1 \\end{bmatrix}. \\end{equation*}\nThe observed values are packed into \\(\\mathbf{y}\\). We can then solve for \\(\\beta\\) using any standard solver:\n\\[ \\beta = (\\mathbf{X}^T \\mathbf{X})^{-1}X^T \\mathbf{y}. \\]\nRank-Deficient matrices In the event that the matrix \\(\\mathbf{X}^T \\mathbf{X}\\) is singular, then its inverse cannot be computed. This implies that one or more of the features is a linear combination of the others.\nThis can be detected by checking the rank of \\(\\mathbf{X}^T \\mathbf{X}\\) before attempting to compute the inverse. You can also determine which features are redundant via Gaussian elimination. The columns in the reduced matrix that do not have a pivot entry are redundant.\nAnother Approach to Normal Equations We can arrive at the normal equations by starting at the probabilistic perspective. Recall the likelihood function\n\\[ p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}, \\sigma) = -\\frac{n}{\\sqrt{2\\pi\\sigma^2}}\\exp\\Big(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i})^2\\Big). \\]\nTaking the natural log of this function yields\n\\[ \\ln p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}, \\sigma) = - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(h(\\mathbf{x}_{i}; \\mathbf{w}) - \\mathbf{y}_{i})^2 - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{n}{2}\\ln(2\\pi). \\]\nAs mentioned before, maximizing the likelihood function is equivalent to minimizing the sum-of-squares function. Thus, we must find the critical point of the likelihood function by computing the gradient (w.r.t. \\(\\mathbf{w}\\)) and solving for 0:\n\\begin{align*} \\nabla \\ln p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}, \\sigma) \u0026amp;= \\sum_{i=1}^{n}(\\mathbf{w}^T\\mathbf{x}_{i} - \\mathbf{y}_{i})\\mathbf{x}_{i}^{T}\\\\ \u0026amp;= \\mathbf{w}^T \\sum_{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^T - \\sum_{i=1}^{n}\\mathbf{y}_{i}\\mathbf{x}_{i}^{T}\\\\ \\end{align*}\nNoting that \\(\\sum_{i=1}^{n}\\mathbf{x}_i \\mathbf{x}_i^T\\) is simply matrix multiplication, we can use\n\\begin{equation*} \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1^T\\\\ \\vdots\\\\ \\mathbf{x}_n^T\\\\ \\end{bmatrix}. \\end{equation*}\nThen, \\(\\sum_{i=1}^{n}\\mathbf{x}_i \\mathbf{x}_i^T = \\mathbf{X}^T \\mathbf{X}\\), \\(\\sum_{i=1}^{n}\\mathbf{y}_i \\mathbf{x}_i^T = \\mathbf{Y}^T \\mathbf{X}\\), and\n\\[ \\nabla \\ln p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}, \\sigma) = \\mathbf{w}^T \\mathbf{X}^T \\mathbf{X} - \\mathbf{Y}^T \\mathbf{X}. \\]\nSince we are finding the maximum likelihood, we set \\(\\nabla \\ln p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}, \\sigma) = 0\\) and solve for \\(\\mathbf{w}\\):\n\\[ \\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{Y}. \\]\nThus, we arrive again at the normal equations and can solve this using a linear solver.\nFitting Polynomials Not every dataset can be modeled using a simple line. Data can be exponential or logarithmic in nature. We may also look to use splines to model more complex data.\nFigure 3: Data generated from a nonlinear function with added noise. The dataset above was generated from the function as seen in red. Using a simple linear model (blue) does not fit the data well. For cases such as this, we can fit a polynomial to the data by changing our input data.\nThe simple dataset above has 100 paired samples \\((x_i, y_i)\\). There is only a single feature \\(x_i\\) for each sample. It is trivial to determine that the shape of the data follows a cubic function. One solution would be to raise each input to the power of 3. This results in the function (blue) below.\nFigure 4: Solution from raising each input to the power of 3. To fit this data, we need to add more features to our input. Along with the original \\(x_i\\) features, we will also add \\(x_i^2\\) and \\(x_i^3\\). Our data is then 3 dimensional. The figure below shows the least squares fit using the modified data (blue).\nFigure 5: Least squares fit using a polynomial model (blue). A demo of this can be found here.\nLinear Basis Functions Linear models are linear in their inputs. This formulation is simple, producing models with limited representation. Linear models can be extended as a linear combination of fixed nonlinear functions of the original features. In the previous section, was saw that they could easily be extended to fit polynomial functions.\nWe now consider creating a model that transforms the original input using one or more nonlinear functions. This type of model is called a linear basis function model.\n\\[ h(\\mathbf{x};\\mathbf{w}) = \\sum_{j=1}^{m} w_j\\phi_j(\\mathbf{x}) \\]\nCommon basis functions are the sigmoid, Gaussian, or exponential function. If we choose the \\(\\sin\\) function as a basis function, we can more closely fit our dataset using the least squares approach.\nFigure 6: A linear basis function model using the sin function as the choice of basis. ","date":1641967200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706572800,"objectID":"267d7f34a9ee73d8fc77237de6f23b2a","permalink":"https://ajdillhoff.github.io/notes/linear_regression/","publishdate":"2022-01-12T00:00:00-06:00","relpermalink":"/notes/linear_regression/","section":"notes","summary":"Table of Contents Introduction Probabilistic Interpretation Solving with Normal Equations Another Approach to Normal Equations Fitting Polynomials Linear Basis Functions Slides for these notes are available here.\nIntroduction Given a dataset of observations \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\), where \\(n\\) is the number of samples and \\(d\\) represents the number of features per sample, and corresponding target values \\(\\mathbf{Y} \\in \\mathbb{R}^n\\), create a simple prediction model which predicts the target value \\(\\mathbf{y}\\) given a new observation \\(\\mathbf{x}\\).","tags":["machine learning"],"title":"Linear Regression","type":"notes"},{"authors":[],"categories":[],"content":"Welcome to Slides academia\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne **Two** Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://ajdillhoff.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using academia's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"NVIDIA Visual Profiler Quickstart Guide NVIDIA Visual Profiler is installed on both the GPU machines and the workstations. The following guide will show you how to use the NVIDIA Visual Profiler to profile your CUDA code. For more details, please refer to the official documentation.\nGenerate a profiling report for your kernel First, a profiling report must be generated on the machine with a GPU. The following code should replace the current code in benchmark.sh for Lab 3. Note that this is specific to the lab in ERB 125. If you\u0026rsquo;re running on your own machine with a GPU more recent than Pascal, it is highly recommended that you profile your code with NSight Compute instead.\nThe given script will first load CUDA Toolkit 11.5, which is compatible with the version installed on the workstations. It will then compile the code and run the benchmark with a 1024x1024x1024 matrix. The nvprof command will generate a profiling report in the form of a .nvvp file, which can be opened with NVIDIA Visual Profiler.\n#!/bin/bash #SBATCH --export=/usr/local/cuda-11.5/bin #SBATCH --gres=gpu:1 module load cuda/11.5 make benchmark nvprof --analysis-metrics --export-profile matmul_benchmark.nvvp -f ./build/main/benchmark 1024 1024 1024 module unload cuda/11.5 Open the profiling report After running the script, you should have a file called matmul_benchmark.nvvp. Copy this file from the GPU machine to your local workstation first. You can open this file with the following command:\nnvvp matmul_benchmark.nvvp We will review the metrics reported from the report in class. You can use the provided guided analysis to get a feel for the output metrics and how to interpret them.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"af194e83989dc3e8996dbe246554a633","permalink":"https://ajdillhoff.github.io/notes/visual_profiler_quick_guide/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/visual_profiler_quick_guide/","section":"notes","summary":"NVIDIA Visual Profiler Quickstart Guide NVIDIA Visual Profiler is installed on both the GPU machines and the workstations. The following guide will show you how to use the NVIDIA Visual Profiler to profile your CUDA code. For more details, please refer to the official documentation.\nGenerate a profiling report for your kernel First, a profiling report must be generated on the machine with a GPU. The following code should replace the current code in benchmark.","tags":null,"title":"","type":"notes"}]