[{"authors":["admin"],"categories":null,"content":"I love building things and helping others build things they are passionate about. I also love distilling complex topics, clearing the path towards knowledge.\n","date":1700114400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1700114400,"objectID":"05ebe7d8c837c5d6c5fe1186ef4f04cb","permalink":"https://ajdillhoff.github.io/authors/alex-dillhoff/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/alex-dillhoff/","section":"authors","summary":"I love building things and helping others build things they are passionate about. I also love distilling complex topics, clearing the path towards knowledge.","tags":null,"title":"Alex Dillhoff","type":"authors"},{"authors":["admin"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nIt is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum. The point of using Lorem Ipsum. distracted by the readable content of a page.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://ajdillhoff.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nIt is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum.","tags":null,"title":"Alex Dillhoff","type":"authors"},{"authors":null,"categories":null,"content":"This is the course page for CSE 1325 - Object-Oriented Programming in Summer 2023. Here you’ll find slides and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Xuan Wang (xuan (dot) wang2 (at) mavs (dot) uta (dot) edu) TAs across all sections are eligible to help you with assignments and other coding questions. Click here to view the TA lab schedule.\nOffice Hours Office hours are facilitated via Teams. If these times do not work for you, e-mail me to schedule an appointment.\nMonday, Wednesday, and Friday from 2PM - 3PM Tuesday and Thursday from 4:30PM - 5:30PM ","date":1689811200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1689811200,"objectID":"14223e1c38e3bdf11fdefa5e5bcfedc9","permalink":"https://ajdillhoff.github.io/courses/archive/summer2023/cse1325/","publishdate":"2023-07-20T00:00:00Z","relpermalink":"/courses/archive/summer2023/cse1325/","section":"courses","summary":"Introduction to object oriented programming using Java.","tags":null,"title":"CSE 1325 - Object Oriented Programming","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for CSE1310 - Introduction to Computers \u0026amp; Programming in Summer 2023. Here you\u0026rsquo;ll find slides and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Sachit Satyal (sxs3757 (at) mavs (dot) uta (dot) edu) Office Hours MoWeFri 2PM - 3PM and TuTh 4:30PM - 5:30PM via Teams (or by appointment).\n","date":1689552000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1689552000,"objectID":"7f035aaf21d8190ebddfaa8902ca6ab9","permalink":"https://ajdillhoff.github.io/courses/archive/summer2023/cse1310/","publishdate":"2023-07-17T00:00:00Z","relpermalink":"/courses/archive/summer2023/cse1310/","section":"courses","summary":"Introductory programming course using C.","tags":null,"title":"CSE 1310 - Introduction to Computers \u0026 Programming","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for DASC 5300 - Foundations of Computing in Fall 2023. Here you\u0026rsquo;ll find class notes and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Keshav Bansal (kxb5527 (at) mavs (dot) uta (dot) edu) Office Hours All office hours are conducted via Teams by default. Please contact me for an appointment if you wish to meet in person.\nAlex Dillhoff\nMoWeFri 12PM - 1PM TuTh 11AM - 12PM Keshav Bansal\nTuTh 4:30PM - 6:30PM (via Teams) ","date":1692230400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1692230400,"objectID":"6442b8bea2c82cc2d71fa01ca1518907","permalink":"https://ajdillhoff.github.io/courses/dasc5300/","publishdate":"2023-08-17T00:00:00Z","relpermalink":"/courses/dasc5300/","section":"courses","summary":"Basics of programming, data structures, and algorithms. Introduction to databases and operating systems.","tags":null,"title":"DASC 5300 - Foundations of Computing","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for CSE 1320 - Intermediate Programming in Fall 2023. Here you\u0026rsquo;ll find class notes and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Section 001: Sama Nikanfar (sxn8789 (at) mavs (dot) uta (dot) edu) Section 006: Rishabh Mediratta (rxm5684 (at) mavs (dot) uta (dot) edu) Office Hours All office hours are conducted via Teams by default. Please contact me for an appointment if you wish to meet in person.\nMoWeFri 12PM - 1PM TuTh 11AM - 12PM ","date":1692057600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1692057600,"objectID":"3f9f77222cb7f888e29ef038f2b4d2ea","permalink":"https://ajdillhoff.github.io/courses/cse1320/","publishdate":"2023-08-15T00:00:00Z","relpermalink":"/courses/cse1320/","section":"courses","summary":"Advanced programming concepts in C paired with introductory data structures and algorithms.","tags":null,"title":"CSE 1320 - Intermediate Programming","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for CSE 3380 - Linear Algebra in Fall 2023. Here you\u0026rsquo;ll find class notes and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Section 001: Yanjun Lyu (yxl9168 (at) mavs (dot) uta (dot) edu) Section 002: Roman Strukov (rxs6055 (at) mavs (dot) uta (dot) edu) Office Hours All office hours are conducted via Teams by default. Please contact me for an appointment if you wish to meet in person.\nMoWeFri 12PM - 1PM TuTh 11AM - 12PM ","date":1692057600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1692057600,"objectID":"466db5f73b7caf0e6f93b18a35a50e8f","permalink":"https://ajdillhoff.github.io/courses/cse3380/","publishdate":"2023-08-15T00:00:00Z","relpermalink":"/courses/cse3380/","section":"courses","summary":"Linear Algebra theory combined with applications in Engineering and Computer Science.","tags":null,"title":"CSE 3380 - Linear Algebra for CSE","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for CSE 6363 - Machine Learning in Fall 2023. Here you\u0026rsquo;ll find class notes and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Greeshma Jayanth (gxj4507 (at) mavs (dot) uta (dot) edu) Office Hours All office hours are conducted via Teams by default. Please contact me for an appointment if you wish to meet in person.\nMoWeFri 12PM - 1PM TuTh 11AM - 12PM ","date":1692057600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1692057600,"objectID":"8d9b4ecfa5ebcb8f21c39196f962b138","permalink":"https://ajdillhoff.github.io/courses/cse6363/","publishdate":"2023-08-15T00:00:00Z","relpermalink":"/courses/cse6363/","section":"courses","summary":"Graduate-level machine learning topics covering the foundations up to modern publications.","tags":null,"title":"CSE 6363 - Machine Learning","type":"docs"},{"authors":null,"categories":null,"content":"This is the course page for CSE 6363 - Machine Learning in Summer 2023. Here you\u0026rsquo;ll find class notes and other helpful resources. All assignments and announcements will be posted on Canvas.\nTeaching Assistants Yanjun Lyu (yxl9168 (at) mavs (dot) uta (dot) edu) Xiao Shi (xiao (dot) shi (at) mavs (dot) uta (dot) edu) Office Hours All office hours are conducted via Teams by default. Please contact me for an appointment if you wish to meet in person.\nMoWeFri 2PM - 3PM TuTh 4:30PM - 5:30PM ","date":1689724800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1689724800,"objectID":"68c00b7cac3dc2a82e18e821e2a58551","permalink":"https://ajdillhoff.github.io/courses/archive/summer2023/cse6363/","publishdate":"2023-07-19T00:00:00Z","relpermalink":"/courses/archive/summer2023/cse6363/","section":"courses","summary":"Graduate-level machine learning topics covering the foundations up to modern publications.","tags":null,"title":"CSE 6363 - Machine Learning","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials August 21 Course Introduction, Intro. to Python Introduction Introduction to Python August 23 Control Flow, Quiz 1 Control Flow August 28 Git Introduction to Git August 30 Comprehensions, Functions, File I/O Comprehensions September 6 OOP in Python, Quiz 2 Object-Oriented Programming in Python Example: Data Loaders in Python September 11 Additional Packages NumPy: Basics NumPy: Shape Manipulation NumPy: Copies and Views September 13 Exam 1 Review Python Review Questions September 18 Exam 1 September 20 CANCELLED September 25 Introduction to Algorithms Introduction to Algorithms September 27 Complexity Analysis Complexity Analysis October 2 Introduction to Data Structures Introduction to Data Structures October 4 Stacks, Queues, and Linked Lists Stacks and Queues October 9 Hash Maps, Quiz 3 Hash Maps October 11 Binary Search Trees Binary Search Trees October 16 Red-Black Trees Red-Black Trees October 18 Introduction to Graphs Intro. to Graphs October 23 Minimum Spanning Trees, Quiz 4 Minimum Spanning Trees October 25 Single-Source Shortest Paths Single-Source Shortest Paths October 30 Introduction to Databases Introduction to Databases November 1 Structured Query Language Structured Query Language November 6 Exam 2 Review November 8 Exam 2 November 13 Advanced SQL Structured Query Language November 15 Distributed Databases Distributed Databases November 20 NOSQL, Quiz 5 NOSQL ","date":1692226800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692226800,"objectID":"68daa7a892cda01548842bb6a7d95303","permalink":"https://ajdillhoff.github.io/courses/dasc5300/schedule/","publishdate":"2023-08-17T00:00:00+01:00","relpermalink":"/courses/dasc5300/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials August 21 Course Introduction, Intro. to Python Introduction Introduction to Python August 23 Control Flow, Quiz 1 Control Flow August 28 Git Introduction to Git August 30 Comprehensions, Functions, File I/O Comprehensions September 6 OOP in Python, Quiz 2 Object-Oriented Programming in Python Example: Data Loaders in Python September 11 Additional Packages NumPy: Basics NumPy: Shape Manipulation NumPy: Copies and Views September 13 Exam 1 Review Python Review Questions September 18 Exam 1 September 20 CANCELLED September 25 Introduction to Algorithms Introduction to Algorithms September 27 Complexity Analysis Complexity Analysis October 2 Introduction to Data Structures Introduction to Data Structures October 4 Stacks, Queues, and Linked Lists Stacks and Queues October 9 Hash Maps, Quiz 3 Hash Maps October 11 Binary Search Trees Binary Search Trees October 16 Red-Black Trees Red-Black Trees October 18 Introduction to Graphs Intro.","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials Week 1 Course Introduction, CSE 1310 Review Introduction Introduction to C Number Systems Week 2 CSE 1310 Review Arrays Loops File I/O Week 3 Pointers and Structs Structs Pointers Week 4 Function Pointers and Dynamic Memory Allocation Pointers Dynamic Memory Allocation Week 5 Introduction to Data Structures Stacks \u0026amp; Queues Linked Lists Week 6 Exam 1 Week 7 Data Structures, Continued Binary Search Trees Hash Maps Week 8 Makefiles and Macros Makefiles Macros ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"38ef58e35b80ba0eb94ccc54eaee91ae","permalink":"https://ajdillhoff.github.io/courses/cse1320/schedule/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/cse1320/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials Week 1 Course Introduction, CSE 1310 Review Introduction Introduction to C Number Systems Week 2 CSE 1310 Review Arrays Loops File I/O Week 3 Pointers and Structs Structs Pointers Week 4 Function Pointers and Dynamic Memory Allocation Pointers Dynamic Memory Allocation Week 5 Introduction to Data Structures Stacks \u0026amp; Queues Linked Lists Week 6 Exam 1 Week 7 Data Structures, Continued Binary Search Trees Hash Maps Week 8 Makefiles and Macros Makefiles Macros ","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials Week 1 Course Introduction, Systems of Linear Equations, Solution Sets 1.1, 1.2, and 1.5 Week 2 Vectors, Linear Combinations, Matrix Operations 1.3, 1.4, 2.1, 2.2 Week 3 Matrix Inverse, Block Matrices, and Determinants 2.4, 3.1, 3.2 Week 4 Determinants, Vector Spaces, and Linear Independence 3.2, 4.1, 4.2, 4.3 Week 5 Exam 1 Week 6 Linear Independence, Bases, and Linear Transformations 1.8, 4.3 Week 7 Computer Graphics, Python Crash Course 2.7 Week 8 Determinants and Volume, Coordinate Systems, and Dimensionality 3.3, 4.4, 4.5 ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"f39e709c62e7ca2fb9a35610ba67dfe5","permalink":"https://ajdillhoff.github.io/courses/cse3380/schedule/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/cse3380/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials Week 1 Course Introduction, Systems of Linear Equations, Solution Sets 1.1, 1.2, and 1.5 Week 2 Vectors, Linear Combinations, Matrix Operations 1.3, 1.4, 2.1, 2.2 Week 3 Matrix Inverse, Block Matrices, and Determinants 2.4, 3.1, 3.2 Week 4 Determinants, Vector Spaces, and Linear Independence 3.2, 4.1, 4.2, 4.3 Week 5 Exam 1 Week 6 Linear Independence, Bases, and Linear Transformations 1.","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials August 21 Course Introduction August 23 Supervised Learning Linear Regression August 25 Supervised Learning Linear Regression August 28 Supervised Learning Logistic Regression August 30 Python Review, Probability Theory Probability Theory September 1 Linear Discriminant Analysis, Regularization Linear Discriminant Analysis Regularization September 6 Naive Bayes Classifiers Naive Bayes Classifiers September 8 Kernels Kernels September 11 Support Vector Machines Support Vector Machines September 13 Support Vector Machines Support Vector Machines September 15 Sequential Minimal Optimization Sequential Minimal Optimization September 18 Perceptrons Perceptron September 20 Artificial Neural Networks Artificial Neural Networks September 22 Backpropagation Artificial Neural Networks September 25 Bias-Variance Tradeoff, Model Selection, and Cross-Validation Bias-Variance Tradeoff September 27 Hidden Markov Models Hidden Markov Models September 29 Hidden Markov Models Hidden Markov Models October 4 Hidden Markov Models Hidden Markov Models October 6 Decision Trees Decision Trees October 9 Decision Trees Decision Trees October 11 Clustering October 13 Principal Component Analysis Principal Component Analysis October 16 Principal Component Analysis Principal Component Analysis October 18 Boosting Boosting October 20 Boosting Boosting October 23 Introduction to Deep Learning Deep Learning October 25 Convolutional Neural Networks Convolutional Neural Networks October 27 Convolutional Neural Networks Convolutional Neural Networks October 30 Optimization for Deep Learning Optimization for Deep Learning November 1 Recurrent Neural Networks Recurrent Neural Networks November 3 Transformers Transformers November 6 Markov Decision Processes Markov Decision Processes November 8 Markov Decision Processes Markov Decision Processes November 10 Reinforcement Learning Reinforcement Learning November 13 Policy Gradient Methods Policy Gradient Methods November 15 Natural Language Processing Natural Language Processing November 17 Recent History of LLMs November 20 CANCELLED ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"2a3448e80b3ddf3d6fc98a1c6bc12913","permalink":"https://ajdillhoff.github.io/courses/cse6363/schedule/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/cse6363/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials August 21 Course Introduction August 23 Supervised Learning Linear Regression August 25 Supervised Learning Linear Regression August 28 Supervised Learning Logistic Regression August 30 Python Review, Probability Theory Probability Theory September 1 Linear Discriminant Analysis, Regularization Linear Discriminant Analysis Regularization September 6 Naive Bayes Classifiers Naive Bayes Classifiers September 8 Kernels Kernels September 11 Support Vector Machines Support Vector Machines September 13 Support Vector Machines Support Vector Machines September 15 Sequential Minimal Optimization Sequential Minimal Optimization September 18 Perceptrons Perceptron September 20 Artificial Neural Networks Artificial Neural Networks September 22 Backpropagation Artificial Neural Networks September 25 Bias-Variance Tradeoff, Model Selection, and Cross-Validation Bias-Variance Tradeoff September 27 Hidden Markov Models Hidden Markov Models September 29 Hidden Markov Models Hidden Markov Models October 4 Hidden Markov Models Hidden Markov Models October 6 Decision Trees Decision Trees October 9 Decision Trees Decision Trees October 11 Clustering October 13 Principal Component Analysis Principal Component Analysis October 16 Principal Component Analysis Principal Component Analysis October 18 Boosting Boosting October 20 Boosting Boosting October 23 Introduction to Deep Learning Deep Learning October 25 Convolutional Neural Networks Convolutional Neural Networks October 27 Convolutional Neural Networks Convolutional Neural Networks October 30 Optimization for Deep Learning Optimization for Deep Learning November 1 Recurrent Neural Networks Recurrent Neural Networks November 3 Transformers Transformers November 6 Markov Decision Processes Markov Decision Processes November 8 Markov Decision Processes Markov Decision Processes November 10 Reinforcement Learning Reinforcement Learning November 13 Policy Gradient Methods Policy Gradient Methods November 15 Natural Language Processing Natural Language Processing November 17 Recent History of LLMs November 20 CANCELLED ","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials June 6 Course Introduction Introduction June 8 Introduction to Java, Classes and Objects Intro. to Java Classes and Objects June 13 Classes and Objects, Inheritance Classes and Objects Inheritance June 15 CANCELLED, Quiz 1 June 20 Inheritance, UML \u0026amp; Documentation Inheritance UML \u0026amp; Documentation June 22 Interfaces, Quiz 2 Interfaces June 27 Interfaces, Midterm Exam Review Interfaces June 29 Midterm Exam July 4 US Independence Day Holiday July 6 Exception Handling, ArrayList, Quiz 3 Exception Handling ArrayList July 11 Generic Programming Generics July 13 Collections Collections July 18 GUI Programming GUI Programming July 20 GUI Programming GUI Programming July 25 Multithreading, Lambda Functions Multithreading Lambdas July 27 Introduction to C\u0026#43;\u0026#43;, Classes in C\u0026#43;\u0026#43; C\u0026#43;\u0026#43; Basics C\u0026#43;\u0026#43; Classes ","date":1689811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689811200,"objectID":"8d290555026c6d1afd0214382e8849da","permalink":"https://ajdillhoff.github.io/courses/archive/summer2023/cse1325/schedule/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/archive/summer2023/cse1325/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials June 6 Course Introduction Introduction June 8 Introduction to Java, Classes and Objects Intro. to Java Classes and Objects June 13 Classes and Objects, Inheritance Classes and Objects Inheritance June 15 CANCELLED, Quiz 1 June 20 Inheritance, UML \u0026amp; Documentation Inheritance UML \u0026amp; Documentation June 22 Interfaces, Quiz 2 Interfaces June 27 Interfaces, Midterm Exam Review Interfaces June 29 Midterm Exam July 4 US Independence Day Holiday July 6 Exception Handling, ArrayList, Quiz 3 Exception Handling ArrayList July 11 Generic Programming Generics July 13 Collections Collections July 18 GUI Programming GUI Programming July 20 GUI Programming GUI Programming July 25 Multithreading, Lambda Functions Multithreading Lambdas July 27 Introduction to C\u0026#43;\u0026#43;, Classes in C\u0026#43;\u0026#43; C\u0026#43;\u0026#43; Basics C\u0026#43;\u0026#43; Classes ","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials June 5 Course Introduction, Supervised Learning Linear Regression June 7 Supervised Learning, Python Review Logistic Regression June 12 Probability Theory, Linear Discriminant Analysis Probability Theory, LDA June 14 Naive Bayes, Overfitting \u0026amp; Regularization in Linear Models, Kernels Naive Bayes, Regularization, Kernels June 19 Juneteenth Holiday June 21 Support Vector Machines Support Vector Machines June 26 Support Vector Machines, Sequential Minimal Optimization SVM, SMO June 28 Neural Networks Neural Networks July 3 US Independence Day Extra July 5 Introduction to PyTorch, Bias-Variance Tradeoff, Model Selection, Regularization Bias and Variance July 10 Hidden Markov Models Hidden Markov Models July 12 Decision Trees Decision Trees July 17 HMM Example, DT Example, Boosting Boosting July 19 Gradient Boosting, Principal Component Analysis Gradient Boosting, Principal Component Analysis July 24 Deep Learning, Convolutional Neural Networks, Sequential Models Deep Learning, CNNs July 26 Transformers, Markov Decision Processes Transformers, Markov Decision Processes ","date":1689721200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689721200,"objectID":"4bd30469545bd8dfeb55aca56843be33","permalink":"https://ajdillhoff.github.io/courses/archive/summer2023/cse6363/schedule/","publishdate":"2023-07-19T00:00:00+01:00","relpermalink":"/courses/archive/summer2023/cse6363/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials June 5 Course Introduction, Supervised Learning Linear Regression June 7 Supervised Learning, Python Review Logistic Regression June 12 Probability Theory, Linear Discriminant Analysis Probability Theory, LDA June 14 Naive Bayes, Overfitting \u0026amp; Regularization in Linear Models, Kernels Naive Bayes, Regularization, Kernels June 19 Juneteenth Holiday June 21 Support Vector Machines Support Vector Machines June 26 Support Vector Machines, Sequential Minimal Optimization SVM, SMO June 28 Neural Networks Neural Networks July 3 US Independence Day Extra July 5 Introduction to PyTorch, Bias-Variance Tradeoff, Model Selection, Regularization Bias and Variance July 10 Hidden Markov Models Hidden Markov Models July 12 Decision Trees Decision Trees July 17 HMM Example, DT Example, Boosting Boosting July 19 Gradient Boosting, Principal Component Analysis Gradient Boosting, Principal Component Analysis July 24 Deep Learning, Convolutional Neural Networks, Sequential Models Deep Learning, CNNs July 26 Transformers, Markov Decision Processes Transformers, Markov Decision Processes ","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"This schedule is tentative and may change.\nDate Topic Materials June 6 Course Introduction Introduction June 8 Introduction to C Introduction to C June 13 UNIX, Code Formatting, Input/Output Examples UNIX June 15 CANCELLED June 20 Control Structures Basic Control June 22 Loops, Data Types \u0026amp; Conversions Loops June 27 Arrays, Midterm Exam Review Arrays June 29 Midterm Exam July 4 US Independence Day Holiday July 6 Arrays Arrays July 11 Number Systems Number Systems July 13 Functions Functions July 18 Benchmark Review, Debugging July 20 File I/O File I/O July 25 Binary Files, Software Development Lifecycle File I/O, SDLC July 27 Software Development Lifecycle SDLC ","date":1689548400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689548400,"objectID":"7e2fb4d0c832da5f8dc0c9e32aba5abc","permalink":"https://ajdillhoff.github.io/courses/archive/summer2023/cse1310/schedule/","publishdate":"2023-07-17T00:00:00+01:00","relpermalink":"/courses/archive/summer2023/cse1310/schedule/","section":"courses","summary":"This schedule is tentative and may change.\nDate Topic Materials June 6 Course Introduction Introduction June 8 Introduction to C Introduction to C June 13 UNIX, Code Formatting, Input/Output Examples UNIX June 15 CANCELLED June 20 Control Structures Basic Control June 22 Loops, Data Types \u0026amp; Conversions Loops June 27 Arrays, Midterm Exam Review Arrays June 29 Midterm Exam July 4 US Independence Day Holiday July 6 Arrays Arrays July 11 Number Systems Number Systems July 13 Functions Functions July 18 Benchmark Review, Debugging July 20 File I/O File I/O July 25 Binary Files, Software Development Lifecycle File I/O, SDLC July 27 Software Development Lifecycle SDLC ","tags":null,"title":"Schedule","type":"docs"},{"authors":null,"categories":null,"content":"Books W. Foster and L. Foster, C By Discovery (4th ed.), Scott/Jones, 2004. Stephen Prata, C Primer Plus, Addison-Wesley, 2014. Resources Submitting Assignments using GitHub Using GCC with MinGW CSE 13xx SharePoint C/C++ Reference The Linux Command Line Git Cheat Sheet Programming Practice C Etudes Project Euler C Exercises (W3 Schools) edabit Challenges exercism ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"2725e611ea00afbe5fb5ba573d44a12b","permalink":"https://ajdillhoff.github.io/courses/cse1320/materials/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/cse1320/materials/","section":"courses","summary":"Books W. Foster and L. Foster, C By Discovery (4th ed.), Scott/Jones, 2004. Stephen Prata, C Primer Plus, Addison-Wesley, 2014. Resources Submitting Assignments using GitHub Using GCC with MinGW CSE 13xx SharePoint C/C++ Reference The Linux Command Line Git Cheat Sheet Programming Practice C Etudes Project Euler C Exercises (W3 Schools) edabit Challenges exercism ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books David Lay, Stephen Lay, and Judi McDonald, Linear Algebra and Its Applications (5th ed.), Pearson, 2016. Sheldon Axler, Linear Algebra Done Right, Springer International Publishing, 2014. Resources 3blue1brown - Essence of Linear Algebra Python Programming Language ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"d45ca47b9deb7212407ff8e66c8749bd","permalink":"https://ajdillhoff.github.io/courses/cse3380/materials/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/cse3380/materials/","section":"courses","summary":"Books David Lay, Stephen Lay, and Judi McDonald, Linear Algebra and Its Applications (5th ed.), Pearson, 2016. Sheldon Axler, Linear Algebra Done Right, Springer International Publishing, 2014. Resources 3blue1brown - Essence of Linear Algebra Python Programming Language ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006. Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer, 2009. Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, An Introduction to Statistical Learning, Springer, 2023. Kevin Murphy, Probabilistic Machine Learning: An Introduction, MIT Press, 2022. Resources Python Programming Language PyTorch Anaconda HuggingFace ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"32a4e7f841ca201e054e6091dbff81c5","permalink":"https://ajdillhoff.github.io/courses/cse6363/materials/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/cse6363/materials/","section":"courses","summary":"Books Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006. Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer, 2009. Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, An Introduction to Statistical Learning, Springer, 2023. Kevin Murphy, Probabilistic Machine Learning: An Introduction, MIT Press, 2022. Resources Python Programming Language PyTorch Anaconda HuggingFace ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books Charles, Severance, Python for Everybody. Al Sweigart, Automate the Boring Stuff with Python. Michael Sipser, Introduction to the Theory of Computation, Cengage Learning, 2012. Ramez Elmasri, Sham Navathe, Fundamentals of Database Systems, Pearson, 2016. Cormen, Leiserson, Rivest, Stein, Introduction to Algorithms, MIT Press, 2009. Resources Python Programming Language Anaconda SQL Programming Practice exercism.io leetcode.com ","date":1692054000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692054000,"objectID":"03fbac83d564f70718ccec0081f6d319","permalink":"https://ajdillhoff.github.io/courses/dasc5300/materials/","publishdate":"2023-08-15T00:00:00+01:00","relpermalink":"/courses/dasc5300/materials/","section":"courses","summary":"Books Charles, Severance, Python for Everybody. Al Sweigart, Automate the Boring Stuff with Python. Michael Sipser, Introduction to the Theory of Computation, Cengage Learning, 2012. Ramez Elmasri, Sham Navathe, Fundamentals of Database Systems, Pearson, 2016. Cormen, Leiserson, Rivest, Stein, Introduction to Algorithms, MIT Press, 2009. Resources Python Programming Language Anaconda SQL Programming Practice exercism.io leetcode.com ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books Horstmann, Cay S. Core Java Volume 1\u0026ndash;Fundamentals (11th ed.), Pearson, 2018. Morelli, Ralph and Walde, Ralph, Java, Java, Java Resources Submitting Assignments using GitHub CSE 13xx SharePoint Java Documentation Programming Practice Project Euler Java (W3 Schools) edabit Challenges exercism - Coding Exercises ","date":1689811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689811200,"objectID":"27374616a4ab70fbfa9e08144168d429","permalink":"https://ajdillhoff.github.io/courses/archive/summer2023/cse1325/materials/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/archive/summer2023/cse1325/materials/","section":"courses","summary":"Books Horstmann, Cay S. Core Java Volume 1\u0026ndash;Fundamentals (11th ed.), Pearson, 2018. Morelli, Ralph and Walde, Ralph, Java, Java, Java Resources Submitting Assignments using GitHub CSE 13xx SharePoint Java Documentation Programming Practice Project Euler Java (W3 Schools) edabit Challenges exercism - Coding Exercises ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006. Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer, 2009. Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, An Introduction to Statistical Learning, Springer, 2023. Kevin Murphy, Probabilistic Machine Learning: An Introduction, MIT Press, 2022. Resources Python Programming Language PyTorch Anaconda HuggingFace ","date":1689721200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689721200,"objectID":"20c52c4e821b648e67d9cf8f23b1cd94","permalink":"https://ajdillhoff.github.io/courses/archive/summer2023/cse6363/materials/","publishdate":"2023-07-19T00:00:00+01:00","relpermalink":"/courses/archive/summer2023/cse6363/materials/","section":"courses","summary":"Books Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006. Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer, 2009. Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, An Introduction to Statistical Learning, Springer, 2023. Kevin Murphy, Probabilistic Machine Learning: An Introduction, MIT Press, 2022. Resources Python Programming Language PyTorch Anaconda HuggingFace ","tags":null,"title":"Course Materials","type":"docs"},{"authors":null,"categories":null,"content":"Books L. Foster and W. Foster, C By Discovery (4th ed.), Pearson, 2004. Stephen Prata, C Primer Plus, Pearson, 2013. Bjarne Stroustrup, The C++ Programming Language (4th ed.), Addison-Wesley, 2013. Resources Submitting Assignments using GitHub CSE 13xx SharePoint C/C++ Reference The Linux Command Line Git Cheat Sheet Using GCC with MinGW Using WSL with VS Code GDB Cheat Sheet Programming Practice C Etudes Project Euler C Exercises (W3 Schools) C++ Exercises (W3 Schools) edabit Challenges exercism - Coding Exercises ","date":1689548400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689548400,"objectID":"ef694aacb53f50f69a7fffa6b72ec821","permalink":"https://ajdillhoff.github.io/courses/archive/summer2023/cse1310/materials/","publishdate":"2023-07-17T00:00:00+01:00","relpermalink":"/courses/archive/summer2023/cse1310/materials/","section":"courses","summary":"Books L. Foster and W. Foster, C By Discovery (4th ed.), Pearson, 2004. Stephen Prata, C Primer Plus, Pearson, 2013. Bjarne Stroustrup, The C++ Programming Language (4th ed.), Addison-Wesley, 2013. Resources Submitting Assignments using GitHub CSE 13xx SharePoint C/C++ Reference The Linux Command Line Git Cheat Sheet Using GCC with MinGW Using WSL with VS Code GDB Cheat Sheet Programming Practice C Etudes Project Euler C Exercises (W3 Schools) C++ Exercises (W3 Schools) edabit Challenges exercism - Coding Exercises ","tags":null,"title":"Course Materials","type":"docs"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Unsupervised Pre-training From GPT to GPT2 These notes provide an overview of pre-training large language models like GPT and Llama.\nUnsupervised Pre-training Let\u0026rsquo;s start by reviewing the pre-training procedure detailed in the GPT paper (Radford et al. 2020). The Generative in Generative Pre-Training reveals much about how the network can be trained without direct supervision. It is analogous to how you might have studied definitions as a kid: create some flash cards with the term on the front and the definition on the back. Given the context of the word, you try and recite the definition. For a pre-training language model, it is given a series of tokens and is tasked with generating the next token in the sequence. Since we have access to the original documents, we can easily determine if it was correct.\nGiven a sequence of tokens \\(\\mathcal{X} = \\{x_1, x_2, \\ldots, x_n\\}\\), the model is trained to predict the next token \\(x_{n+1}\\) in the sequence. The model is trained to maximize the log-likelihood of the next token:\n\\[\\mathcal{L}(\\mathcal{X}) = \\sum_{i=1}^{n} \\log p(x_{i+1} \\mid x_{i-k}, \\ldots, x_i)\\]\nwhere \\(k\\) is the size of the context window.\nLarge language models are typically based on the Transformers model. The original model was trained for language translation. Depending on the task, different variants are employed. For GPT models, a decoder-only architecture is used, as see below.\nFigure 1: Decoder-only diagram from (Vaswani et al. 2017). The entire input pipeline for GPT can be expressed rather simply. First, the tokenized input is passed through an embedding layer \\(W_{e}\\). Embedding layers map the tokenized input into a lower-dimensional vector representation. A positional embedding matrix of the same size as \\(\\mathcal{X} W_{e}\\) is added in order to preserve the order of the tokens.\nThe embedded data \\(h_0\\) is then passed through \\(n\\) transformer blocks. The output of this is passed through the softmax function in order to produce an output distribution over target tokens.\nFrom GPT to GPT2 GPT2 is a larger version of GPT, with an increased context size of 1024 tokens and a vocabulary of 50,257 vocabulary. In this paper, they posit that a system should be able to perform many tasks on the same input. For example, we may want our models to summarize complex texts as well as provide answers to specific questions we have about the content. Instead of training multiple separate models to perform these tasks individually, the model should be able to adapt to these tasks based on the context. In short, it should model \\(p(output \\mid input, task)\\) instead of \\(p(output \\mid input)\\).\nReferences Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2020. “Improving Language Understanding by Generative Pre-Training,” 12. Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need,” 11. ","date":1700114400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700114400,"objectID":"49a23a5b57d87ec8a4f3138e9d80013d","permalink":"https://ajdillhoff.github.io/notes/pretraining_large_language_models/","publishdate":"2023-11-16T00:00:00-06:00","relpermalink":"/notes/pretraining_large_language_models/","section":"notes","summary":"Table of Contents Unsupervised Pre-training From GPT to GPT2 These notes provide an overview of pre-training large language models like GPT and Llama.\nUnsupervised Pre-training Let\u0026rsquo;s start by reviewing the pre-training procedure detailed in the GPT paper (Radford et al. 2020). The Generative in Generative Pre-Training reveals much about how the network can be trained without direct supervision. It is analogous to how you might have studied definitions as a kid: create some flash cards with the term on the front and the definition on the back.","tags":["large language models","machine learning"],"title":"Pretraining Large Language Models","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Overview Data Fragmentation Data Replication Data Concurrency Distributed systems excel at partitioning large problems into smaller chunks that can be processed in parallel. This requires parallel thinking instead of serial thinking. Many algorithms and solutions that run serially may be easier to adapt to parallel applications than others.\nDistributed solutions are the natural next step to scaling up a system. In the context of databases, the main challenges related to distribution, replication, distributed transactions, distributed metadata management, and distributed query processing.\nOverview According to Elmasri and Navathe (Elmasri and Navathe 2015), a distributed database should satisfy at least the following conditions:\ndatabase nodes should be connected by a network, the information on each node should be logically related, and each node does not necessarily need to be identicaly in terms of data, hardware, and software. Transparency Transparency is the concept of hiding the complex details of a distributed database from the user. There are several types of transparency:\nDistribution transparency - the user does not need to know how the data is distributed across the nodes. This could refer to the location of the data, the replication of the data, or the fragmentation of the data. Replication transparency - data may be stored in multiple nodes. This type of transparency improves availability by allowing the system to continue operating even if a node goes down. Fragmentation transparency - data is either horizontally or vertically fragmented across nodes. Horizontal fragmentation, also called sharding, refers to decomposing tuples of a table into multiple systems. For example, we could horizontally fragment our Character table based on the class_id. Vertical fragmentation refers to decomposing the columns of a table into multiple systems. For example, we could vertically fragment our Character table into a Character table and a CharacterStats table. Availability and Reliability Having more than one point of failure means that a distributed database is more reliable than a centralized database. With technologies like replication, the availability of the database also increases.\nScalability Scalability in a database that is distributed over multiple nodes can be categorized into two types:\nHorizontal scalability - adding more nodes to the system. Vertical scalability - adding more resources to the nodes. A centralized database can only support vertical scalability. If it goes down or is fragmented from a portion of a broader network, the data is no longer accessible. In a distributed system, the nodes can be partitioned into smaller networks that can still operate independently depending on the type of failure. This is called partition tolerance.\nAutonomy Autonomy refers to the ability of a node to operate independently of other nodes. This is important for distributed systems because it allows for the system to continue operating even if a node goes down.\nDesign autonomy - Data model usage and transaction managament are independent of other nodes. Communication autonomy - Nodes can communicate with each other without the need for a central coordinator. Execution autonomy - Nodes can execute transactions independently of other nodes. While this type of autonomy leads to more availability and higher performance, it can also create problems with consistency since nodes may not be able to agree on the order of operations. Data Fragmentation As mentioned at the beginning of these notes, breaking up a problem into smaller chunks is the key to parallelism. In the context of databases, this means figuring out which nodes have which portions of the data. We will discuss fragmentation under the assumption that no data replication is being used.\nHorizontal Fragmentation (Sharding) Imagine a scenario in which we shard our Users table based on the geographic location of their IP address. If we have 3 nodes in (west coast, central, east coast), then we can separate our table into 3 tables, one for each region. This is called horizontal fragmentation or sharding. The main advantage of sharding is that it allows us to scale horizontally. The main disadvantage is that it makes it more difficult to perform queries that require data from multiple regions.\nVertical Fragmentation Vertical fragmentation can make sense when we have a table with a large number of columns. For example, we could vertically fragment our Users table into a Users table and a UserStats table. When vertically fragmenting data, there should be a common attribute between the two tables. In this case, the user_id would be the common attribute.\nData Replication Data replication is the process of storing the same data in multiple nodes. There are obvious tradeoffs when it comes to selecting a replication strategy. First, let\u0026rsquo;s consider the extreme cases. If no replication is used, then the system is more consistent since there is only one copy of the data. The availability suffers, however, since there is only a single copy of the data.\nIf the data is replicated to every single node, then the availability and performance of the system increases. However, the consistency of the system suffers since there are multiple copies of the data that need to be kept in sync. Picking a replication strategy will largely depend on the needs of the application. Deciding how this data is fragmented is the process of data distribution.\nExample The following example is from Elmasri and Navathe (Elmasri and Navathe 2015). In this example, a company has three nodes for each of its departments. Node 2 stores data for the Research department and Node 3 stores data for the Administration department. The idea behind this is that the EMPLOYEE and PROJECT information for each department will be frequently accessed by that department. This would be more efficient than having to access the data from a centralized database. Node 1 is located at the company\u0026rsquo;s headquarters and includes data for all departments.\nThe data in the DEPARTMENT table is horizontally fragmented using the department number Dnumber. Since there are foreign key relationships in EMPLOYEE, PROJECT, and DEPT_LOCATIONS, they are also fragmented. This is a special type of fragmentation called derived fragmentation. These are easier to fragment since they have a direct foreign key relationship.\nA more difficulty decision comes with the WORKS_ON table. It does not have an attribute that indicates which department each tuple belongs to. The authors choose to fragment based on the department that the employee works for. This is further fragmented based on the department that controls the projects that the employee is working on.\nFigure 1: Fragmentation of WORKS_ON table for department 5. \u0026lt;@elmasri_fundamentals_2015\u0026gt; In the figure above, all of the fragments include employees of the research department. The first fragment includes employees that work on projects controlled by the research department. The second fragment includes employees that work on projects controlled by the administration department. The third fragment includes employees that work on projects controlled by headquarters.\nData Concurrency Distributed systems that employ data replication or allow for multiple users to access the same data at the same time need to be concerned with data concurrency. This is the process of ensuring that the data remains consistent when multiple users are accessing the same data at the same time. Several problems can occur in a DDBMS, such as\ninconsistency between multiple copies of the data, failure of a node, network outages that sever the connection between nodes, failure of a transaction that is applied to multiple nodes, and deadlocks between transactions. Concurrency Control Many control solutions for distributed systems are based on the idea of a centralized locking authority. This authority is responsible for granting locks to transactions that request them. The authority is also responsible for granting access to data that is locked by other transactions. When an object is locked, it cannot be accessed by other transactions.\nIn this case, the central authority may be a distinguished copy of the data. All requests to lock or unlock are sent to that copy.\nPrimary Site Technique All locks are kept at a primary site. This site is responsible for granting locks to transactions that request them. The primary site is also responsible for granting access to data that is locked by other transactions. This is a simple technique that is easy to implement. However, it is not very scalable since all requests must go through the primary site. Note that this does not prevent transactions with read locks from accessing any copy of the item. If a transaction has a write lock, the primary site must update all copies of the data before releasing the lock.\nPrimary Site with Backup If the primary site fails in the first approach, the system effectively becomes unavailable. To prevent this, we can have a backup primary site that takes over if the primary site fails. This is a simple solution that is easy to implement. If the primary site fails in this case, a backup takes over and becomes the new primary. A new backup is chosen so that the system can continue to operate. One downside to this approach is that locks must be recorded at both the primary and backup sites.\nPrimary Copy Technique Lock coordination is distributed among various sites. Distinguished copies for different items are distributed to different sites. A failure at one site would only affect the transactions that are accessing its distinguished copies. Other items not on the site would remain functional. In the case of a failure, the sites that are still running can choose a new coordinator based on some strategy. One such strategy is to have all running sites vote on a new coordinator. The site with the most votes becomes the new coordinator.\nReferences Elmasri, Ramez, and Shamkant B. Navathe. 2015. Fundamentals of Database Systems. 7th ed. Pearson. https://www.pearson.com/en-us/subject-catalog/p/fundamentals-of-database-systems/P200000003546/9780137502523. ","date":1699941600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699941600,"objectID":"3ab12ea9ee4c2922449f269c1fb4640f","permalink":"https://ajdillhoff.github.io/notes/distributed_databases/","publishdate":"2023-11-14T00:00:00-06:00","relpermalink":"/notes/distributed_databases/","section":"notes","summary":"Table of Contents Overview Data Fragmentation Data Replication Data Concurrency Distributed systems excel at partitioning large problems into smaller chunks that can be processed in parallel. This requires parallel thinking instead of serial thinking. Many algorithms and solutions that run serially may be easier to adapt to parallel applications than others.\nDistributed solutions are the natural next step to scaling up a system. In the context of databases, the main challenges related to distribution, replication, distributed transactions, distributed metadata management, and distributed query processing.","tags":["computer science","databases"],"title":"Distributed Databases","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents NOSQL Characteristics for Distributed Systems NOSQL Data Models CAP Theorem Document-Based NOSQL Systems Key-Value NOSQL Systems Column-Based NOSQL Systems Graph-Based NOSQL Systems NOSQL refers to Not Only SQL. A NOSQL system is commonly a distributed one that focuses on semi-structured data storage, high performance, availability, replication and scalability. These type of systems developed to meet the needs of large-scale internet applications where a traditional SQL database could not.\nConsider an application like Amazon which manages a high volume of data and user requests. The application needs to be able to store and retrieve this data quickly and reliably. They created their own database system called DynamoDB which is a key-value store. DynamoDB has been used for many applications that require high performance and availability such as video streaming through services like Disney+.\nThe data that is used in these systems does not usually fit the mold of a traditional SQL database. For example, a relational database might store an object by disassembling it into its components and storing each component in a separate table. This is not ideal for a system that needs to store and retrieve data quickly. A NOSQL system will store the object as a whole and retrieve it as a whole.\nNOSQL Characteristics for Distributed Systems Given the nature of the applications that utilize NOSQL systems, the most important characteristic is high availability. Of course, performance is also important given the number of users that expect the service to remain responsive at all times.\nScalability NOSQL systems typically aim for horizontal scalability. The applications that use these systems are expected to grow rapidly and the system needs to be able to handle the increased load. This sort of dynamic scaling means that implementations should not rely on a fixed number of nodes.\nFor example, during the holiday season, Amazon will need to rapidly scale up their infrastructure to handle the increased load. Cloud technologies are capable of doing this automatically, but the database system needs to be able to handle the increased load as well.\nAvailability NOSQL systems are expected to be highly available. This means that the system should be able to handle failures and continue to operate. Data is typically replicated over multiple nodes. However, this replication comes with increased complexity for writing data. To deal with this, many NOSQL systems implement a relaxed version called eventual consistency.\nReplication Models There are two main replication models for NOSQL systems: primary-replica and primary-primary. In primary-replica replication, only one copy is the primary for which all write operations are applied. The write is propagated asynchronously to the replicas.\nIn primary-primary replication, all copies are equal and can accept write operations. This is more complex to implement, but it allows for better performance and availability. If multiple users write to the same object, the system needs to be able to handle the conflict through a reconciliation process.\nSharding Depending on the application, a NOSQL collection could have millions of documents. These may need to be accessed simultaneously by a large number fo users. Sharding is a technique that allows the data to be distributed across multiple nodes. In this way, multiple nodes can work in parallel to handle the load. This has an added benefit of ensuring that no single node is overloaded.\nHigh-Performance Data Access In a distributed system with millions upon millions of objects distributed across many nodes, how do you find the object you are looking for? NOSQL systems typically use a hash-based approach to find the object. This is done by hashing the key of the object and using the hash to determine which node the object is stored on. This is a very fast operation and allows for the system to scale to millions of objects.\nAnother solution is called range partitioning in which the location is determined based on a range of key values. Each node would handle a different partition of the keys.\nOther Characteristics NOSQL systems do not require a schema. This means that the data does not need to be structured in a specific way. This is useful for applications that need to store a variety of data types. For example, a social media application might need to store user profiles, posts, comments, etc. These are all different types of data that would not fit well into a relational database. Instead of a schema, a language for describing the data is used. A common language is JSON.\nGiven the common application of NOSQL systems, a complex query language is not required. Many of the requests are in the form of a simple read or write operation. This allows for the system to be optimized for these operations. These operations are typically provided by an API and are called CRUD operations (Create, Read, Update, and Delete). Without the full power of SQL, complex operations such as JOIN or CONSTRAINTS must be handled by the application.\nNOSQL Data Models There are four main data models used by NOSQL systems: key-value, column, document, and graph. Each of these models has its own advantages and disadvantages. The model that is chosen depends on the application and the type of data that is being stored.\nKey-Value The key-value model is the simplest of the four. It is essentially a hash table where the key is used to retrieve the value. The value can be any type of data. This model is very fast and can scale to millions of objects.\nColumn Tables are partitioned by columns into column families. Each column family is stored in its own files.\nDocument Documents are stored in collections. Each document is stored as a JSON object. This model is very flexible and can store a variety of data types. It is also very fast and can scale to millions of objects. The documents are typically queried using their document ID, but other indices can be created to speed up queries.\nGraph Graphs are used to represent relationships between objects. Each object is represented as a node and the relationships are represented as edges. This model is useful for applications that need to represent complex relationships between objects.\nCAP Theorem The CAP theorem states that a distributed system can only guarantee two of the following three properties: consistency, availability, and partition tolerance. Consistency means that all nodes see the same data at the same time. Availability means that every request receives a response. Partition tolerance means that the system continues to operate despite network failures.\nDocument-Based NOSQL Systems In document-based NOSQL systems, the data is self-describing as there is no need for a schema. These sytems store documents which are essentially JSON objects. The documents are stored in collections which are similar to tables in a relational database. The documents are retrieved using their document ID.\nMongoDB MongoDB is a document-based NOSQL database that is flexibile, scalable, and high-performance. It stores data in a JSON-like format called BSON (Binary JSON). Inidividual documents are stored in a collection. No schema is needed to begin storing data. The python code below will create a new collection for our RPG Users with a simple command in pymongo:\ndb[\u0026#39;users\u0026#39;] This will create a new collection named users with the default settings. If you want to specify additional options, call the create_collection function. Common parameters include determining of a collection is capped by the storage size and maximum number of documents.\nRepresenting Data Whenever a new item is inserted to a colletion, a unique ObjectId is created and indexed. If the ID of a document should match a user-defined protocol, it can be set manually. Since there is no schema to specify a relationship, document relationships can be created by including the ~ObjectId~s of objects you wish to reference in your data.\nThere are multiple ways to represent relationships between documents. Consider a Character that holds multiple items in an Inventory. The items could be referenced as an array of Item objects within the Character object itself. Alternatively, the Character could hold an array of ObjectId~s that reference the ~Item objects in the Inventory collection. A third approach would have each Item reference the Character that owns it. The best approach depends on the application and the type of queries that will be performed.\nCRUD Operations CRUD stands for Create, Read, Update, and Delete. Single or multiple documents can be implemented with the insert function. In pymongo, you can use either Collections.insert_one or Collections.insert_many. The insert_one function takes a single document as an argument and returns the ObjectId of the inserted document. The insert_many function takes a list of documents as an argument and returns a list of ~ObjectId~s.\ndb[\u0026#39;users\u0026#39;].insert_one({\u0026#39;name\u0026#39;: \u0026#39;Naomi\u0026#39;, \u0026#39;age\u0026#39;: 25}) db[\u0026#39;users\u0026#39;].insert_many([{\u0026#39;name\u0026#39;: \u0026#39;Naomi\u0026#39;, \u0026#39;age\u0026#39;: 25}, {\u0026#39;name\u0026#39;: \u0026#39;James\u0026#39;, \u0026#39;age\u0026#39;: 30}]) Reading objects is done with the find function. There are several variants of this available in pymongo.\nfind_one returns a single document that matches the query. find returns a cursor that can be iterated over to retrieve all documents that match the query. find_one_and_delete returns a single document that matches the query and deletes it. find_one_and_replace returns a single document that matches the query and replaces it with the specified document. find_one_and_update returns a single document that matches the query and updates it with the specified document. val = db[\u0026#39;users\u0026#39;].find_one({\u0026#39;name\u0026#39;: \u0026#39;Naomi\u0026#39;}) # Print the document print(val) # Print the name print(val[\u0026#39;name\u0026#39;]) Updating documents is done with the update function. We saw an updated combined with find above, but pymongo also implements update_one and update_many. The update_one function takes a query and an update document as arguments. The update_many function takes a query and an update document as arguments. Both functions return a UpdateResult object that contains information about the operation.\ndb[\u0026#39;users\u0026#39;].update_one({\u0026#39;name\u0026#39;: \u0026#39;Naomi\u0026#39;}, {\u0026#39;$set\u0026#39;: {\u0026#39;age\u0026#39;: 26}}) Deleting documents is done with the delete_one and delete_many functions. Both functions take a query as an argument and return a DeleteResult object that contains information about the operation.\ndb[\u0026#39;users\u0026#39;].delete_one({\u0026#39;name\u0026#39;: \u0026#39;Naomi\u0026#39;}) Characteristics MongoDB uses a two-phase commit method to ensure transaction atomicity and consistency. In the first phase of the process, a coordinator sends a message to all nodes to prepare for the transaction. Each node then responds with an acknowledgement. If all nodes respond with an acknowledgement, the coordinator sends a commit message to all nodes. If any node fails to respond with an acknowledgement, the coordinator sends a message to roll back the transaction.\nFor data replication, a variation on the primary-replica model is used. A primary node is chosen with at least one replica. More nodes can be added at the cost of increased time for writes. The total number of nodes for a replica set is at least 3, so if only a primary and one replica are used, an arbiter must be chosen to break ties. In fact, any replica set with an even number of nodes must have an arbiter.\nAll write operations mus be performed on the primary copy before being propagated to the replicas. Users can determine the read preference for their application. The default is to read from the primary copy, but users can choose to read from the nearest copy or a specific copy. If a copy other than the primary is chosen for the read preference, it is not guaranteed that the user will get the lastest version of the data.\nSharding We previously discussed that having all of the data in a single collection can lead to performance issues. Sharding is a technique that allows the data to be distributed across multiple nodes. This allows for multiple nodes to work in parallel to handle the load. Sharding splits the data into disjoint partitions which can then be stored on different nodes.\nThe partitions can be determined via hash partitioning or range partitioning. In either case, a document field must be chosen to determine the partition. This partition field is called the shard key. It must exist in every document and be indexed.\nWhen using sharding on MongoDB, a query router keeps tracks of which nodes contain which shards. The actual query is then routed to the node containing the shard. In the event that a query is sent to a node that does not contain the shard, the query router will forward the query to all nodes.\nKey-Value NOSQL Systems Key-value systems use a simple data model and typically do not have a query language. The data is stored as a key-value pair. The key is used to retrieve the value. The value can be any type of data. This model is very fast and can scale to millions of objects. Popular key-value stores include DynamoDB, Voldemort, Redis, and Cassandra. We will briefly discuss each of them below.\nDynamoDB DynamoDB was developed by Amazon to meet the needs of their large-scale internet applications. It is a key-value store that is highly available and scalable. It is also a managed service which means that Amazon handles the scaling and replication for you. It uses tables, items, and attributes without the need for a schema. The table itself holds multiple items which are self-describing. That is, the items have (attribute, value) pairs.\nTables must have primary keys which can be either a single attribute or pair of attributes. For single attributes, DynamoDB will build a hash index on this attribute. For pairs of attribute, a hash and range primary key is used. The primary key is the pair of attributes and the hash index is built on the first attribute. This allows for fast retrieval of items based on the first attribute. The second attribute can be used to sort the items for which the first attribute is the same.\nVoldemort Voldemort is a distributed key-value store based on DynamoDB and developed by LinkedIn and Microsoft. The distribution of data is handled via consistent hashing. Since Voldemort is based on DynamoDB, many of the characteristics described below also apply to DynamoDB.\nOperations Like DynamoDB, key-value pairs are the primary data structure. These are kept in a data store. Three basic operations are implemented: get, put, and delete. Data is stored as a byte array.\nFormatted Data Voldemort supports multiple formats for the data. The default format is a byte array, but other formats such as JSON and Protocol Buffers are supported. It provides default serializers for these formats, but users can also implement their own. As long as a Serializer class is implemented, it can be used to serialize and deserialize data.\nConsistent Hashing Voldemort distributes data based on a hash function that is applied to each key. The range of values on which the key is mapped corresponds to a node. The figure below shows an example of 7 regions being mapped to 3 nodes (Elmasri and Navathe 2015).\nFigure 1: Consistent hashing in Voldemort. Consistent hashing naturally permits data replication and horizontal scaling. As new nodes are added, only a small subset of the data needs to be rehashed to the new node. Replicas are created by mapping the key to multiple nodes.\nConsistency Concurrent writes are allowed which means there can exist multiple versions of the same key at different nodes. Consistency occurs when an item is read. If the system can reconcile the different versions of the key to a single value, it will pass that final value on. Otherwise, multiple versions may be sent to the application to be resolved.\nRedis Redis is an in-memory key-value store. This implies that is basic operations perform very quickly. However, it is not well suited for general purpose applications that require high volumes of data. A typical use-case for Redis would be caching, session management, or real-time analytics.\nFor example, Twitter uses Redis to drive their timeline feature. The posts are indexed using an ID and stored in Redis. When a user requests their timeline, the IDs are retrieved from Redis as a chain of IDs.\nCassandra Cassandra can be used as a wide-column database (discussed below) or key-value database. It was originally developed at Facebook to handle large amounts of data across multiple commodity servers. It implements the Cassandra Query Language (CQL) which is similar to SQL. The data it partitioned similarly to other NOSQL datastores in that data is distributed in partitions across multiple nodes. CQL does not support cross-partition queries.\nColumn-Based NOSQL Systems The largest differentiator of a column-based system and key-value system is the way the key is defined. A popular implementation of this type of system is known as BigTable which was developed by Google. It uses the Google File System (GFS) to store data. There is an open source equivalent named Apache Hbase which we will focus on below.\nHbase organizes data using namespaces, tables, column families, column qualifiers, columns, rows, and data cells. A column is identified by a family and qualifier. It can store multiple versions of the same data, differentiating each version using a timestamp. Each data cell is identified by a unique key. Tables are associated with column families. When loading data, the column qualifiers must be specified.\nNew column qualifiers can be created as needed, producing new rows of data. However, application developers must keep track of which qualifiers belnog to which family. This is a form of vertical partitioning. Since the columns belong to the same column family, they are stored in the same file.\nCells are reference by their key which is a combination of the row key, column family, column qualifier, and timestamp. For relational semantics, namespaces are used to define a collection of tables.\nHbase divides tables into regions which hold a range of row keys into the table. It is for this reason that they keys must be sortable lexicographically. Each region has a number of stores for which a column family is assigned. These regions of data are assigned to nodes in the cluster. To manage splitting and merging of regions, a primary server is used.\nGraph-Based NOSQL Systems The last category of NOSQL databases discussed in these notes are Graph Databases. These databases are used to represent relationships between objects. Each object is represented as a node and the relationships are represented as edges. This model is useful for applications that need to represent complex relationships between objects. A popular implementation of this type of system is known as Neo4j.\nNodes and relationships can have a unique collection of properties to describe them. Nodes are labeled, and nodes with the same label are grouped into collections for querying. Relationship types are useful for grouping relationships based on a common property.\nPaths specify a traversal of a subgraph. They are used to specify a query and consist of nodes and relationships. The subgraph is used as a pattern to find other subgraphs that match the pattern. The query can be further refined by specifying constraints on the nodes and relationships.\nReferences Elmasri, Ramez, and Shamkant B. Navathe. 2015. Fundamentals of Database Systems. 7th ed. Pearson. https://www.pearson.com/en-us/subject-catalog/p/fundamentals-of-database-systems/P200000003546/9780137502523. ","date":1699941600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699941600,"objectID":"df47404d31f56f5ae51156a3555ba788","permalink":"https://ajdillhoff.github.io/notes/nosql/","publishdate":"2023-11-14T00:00:00-06:00","relpermalink":"/notes/nosql/","section":"notes","summary":"Table of Contents NOSQL Characteristics for Distributed Systems NOSQL Data Models CAP Theorem Document-Based NOSQL Systems Key-Value NOSQL Systems Column-Based NOSQL Systems Graph-Based NOSQL Systems NOSQL refers to Not Only SQL. A NOSQL system is commonly a distributed one that focuses on semi-structured data storage, high performance, availability, replication and scalability. These type of systems developed to meet the needs of large-scale internet applications where a traditional SQL database could not.","tags":["computer science","databases"],"title":"NOSQL","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":"When we had full knowledge of the states, we could use Markov Decision Processes to find the optimal policy. When this assumption breaks down, we need to come up with our best approximation. This is not a far stretch from how we might handle new scenarios in our own lives. When we begin a new task, we are certainly not experts. We may learn from a teacher or set off to explore on our own. As we practice and churn out the seemingly endless variations of our endeavour, we begin to develop a sense of what works and what doesn\u0026rsquo;t. We may not be able to articulate the exact rules that we follow, but we can certainly tell when we are doing well or poorly.\nIn lieu of a conscious agent with human intelligence, we can approximate the policy using a gradient-based approach. We will use the gradient of the expected reward with respect to the policy parameters to update the policy. Methods that use this approach are called policy gradient methods.\n","date":1699768800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699768800,"objectID":"a2be0420fc1b40d3ff70917fe51205d0","permalink":"https://ajdillhoff.github.io/notes/policy_gradient_methods/","publishdate":"2023-11-12T00:00:00-06:00","relpermalink":"/notes/policy_gradient_methods/","section":"notes","summary":"When we had full knowledge of the states, we could use Markov Decision Processes to find the optimal policy. When this assumption breaks down, we need to come up with our best approximation. This is not a far stretch from how we might handle new scenarios in our own lives. When we begin a new task, we are certainly not experts. We may learn from a teacher or set off to explore on our own.","tags":["reinforcement learning"],"title":"Policy Gradient Methods","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents History and Development Schemas Data Types Creation Constraints Retrieving Data Modifying Data Nested Queries Joined Tables Aggregate Functions Grouping WITH Clause Modifying Tables Summary History and Development Structured Query Language (SQL) is a database language for managing data in a relation DBMS. Its original inception was based on a paper by Edgar F. Codd in 1970 titled A Relational Model of Data for Large Shared Data Banks (Codd 1970). Two employees working at IBM in the 1970s, Donald D. Chamberlin and Raymond F. Boyce, developed the first version of SQL in 1974 (Chamberlin and Boyce 1974).\nThe first official standard of SQL was SQL-86, or SQL1, which was published in 1986 by the American National Standards Institute (ANSI). The following table shows the release dates of major SQL standards along with a brief description of the changes made in each version.\nStandard Name Description SQL-86 SQL1 First official standard of SQL SQL-89 SQL2 Added support for integrity constraints, views, and assertions SQL-92 SQL2 Added support for triggers, recursive queries, and support for procedural programming SQL:1999 SQL3 Added support for object-relational features SQL:2003 SQL3 Added support for XML, window functions, and support for regular expressions SQL:2006 SQL3 Added more XML storage features and XQuery support SQL:2008 SQL3 Added support for TRUNCATE TABLE and enhanced MERGE statements SQL:2011 SQL3 Added support for temporal data SQL:2016 SQL3 Added support for JSON SQL:2023 SQL3 Added support for Propery Graph Queries and new JSON features Schemas In our Introduction to Databases we discussed the concept of a schema as a definition of the structure of a database. In SQL, a schema is a collection of database objects, such as tables, views, and indexes. A schema is owned by a database user and has the same name as the user. A database user can own multiple schemas, and a schema can be owned by multiple users. A schema can also be owned by a role, which is a collection of users. A role can own multiple schemas, and a schema can be owned by multiple roles.\nThere are several practical reasons for which we would want to create multiple schemas. For example, a database might be used by both a Human Resources and Healthcare Management application. Creating two separate schemas would ensure that data for each application is kept secure from unauthorized users. Multiple schemas are also used for testing and development processes. Large structural changes to an application may require a new scheme to be created. New features can be developed in the new schema while the old schema is still being used by the application.\nThe following command creates a new schema named MedApp and assigns it to the user MedAdmin.\nCREATE SCHEMA MedApp AUTHORIZATION MedAdmin; Data Types SQL supports a wide variety of data types. The following table shows the most common data types supported by SQL.\nData Type Description CHAR(n) Fixed-length character string. The maximum length is n characters. VARCHAR(n) Variable-length character string. The maximum length is n characters. INT Integer value. The maximum value is 2^31 - 1. SMALLINT Integer value. The maximum value is 2^15 - 1. DECIMAL(i,j) Fixed-point number. The maximum precision is 38 digits. The maximum scale is 38 digits. NUMERIC(i,j) Fixed-point number. The maximum precision is 38 digits. The maximum scale is 38 digits. REAL Floating-point number. The maximum precision is 6 digits. DOUBLE Floating-point number. The maximum precision is 15 digits. DATE Date value. The range is 1000-01-01 to 9999-12-31. TIME Time value. The range is 00:00:00 to 23:59:59. TIMESTAMP Date and time value. The range is 1000-01-01 00:00:00 to 9999-12-31 23:59:59. CLOB(n) Specifies columns with large text values. Maximum length specified in kilobytes (K), megabytes (M), or gigabytes (G) BIT(n) Fixed-length bit string. BIT VARYING(n) Variable-length bit string. BLOB(n) Binary Large Object - used for images, video, and other large items. Creation Creating schemas, databases, and tables is done with the CREATE command. The following command creates a new database named RPG.\nCREATE DATABASE RPG; When creating a new table, we must specify the name of the table and the attributes of the table. The following command creates a new table named Users with four attributes.\nCREATE TABLE Users ( user_id INT, username VARCHAR(50), email VARCHAR(50), created_at TIMESTAMP ); Constraints Constraints allow us to add rules to our database that ensure the integrity of our data. There are several types of constraints that can be added to a table. For example, if a user is deleted, we may want to delete all of the user\u0026rsquo;s posts as well. This can be accomplished by adding a CASCADE constraint to the DELETE statement. We can also set a default value to each attribute. Constraints such as CHECK and UNIQUE can be added to ensure that the data is valid and unique. The following table shows the most common constraints supported by SQL.\nConstraint Description NOT NULL Ensures that a column cannot have a NULL value. UNIQUE Ensures that all values in a column are unique. PRIMARY KEY A combination of a NOT NULL and UNIQUE. FOREIGN KEY Ensures that values in a column match values in another table\u0026rsquo;s column. CHECK Ensures that all values in a column satisfy a specific condition. DEFAULT Sets a default value for a column when no value is specified. INDEX Used to create and retrieve data from the database very quickly. AUTO INCREMENT Automatically generates a unique number when a new record is inserted into a table. When creating the Users table above, we may want to ensure that the user_id attribute is unique. We can do this by adding a UNIQUE constraint to the user_id attribute. It is also possible to have it auto increment so that we do not have to specify a value for it when inserting a new user.\nCREATE TABLE Users ( user_id INT UNIQUE AUTO_INCREMENT, username VARCHAR(50), email VARCHAR(50), created_at TIMESTAMP ); The following command creates a new table named Characters with a VARCHAR attribute named Name which is set to NOT NULL.\nCREATE TABLE Characters ( Name VARCHAR(50) NOT NULL ); Constraints can also be added after the initial attribute declaration. When creating the Characters table, if we want to state that the user_id field should be a foreign key, we can add a FOREIGN KEY constraint to the user_id attribute.\nCREATE TABLE Characters ( id INT UNIQUE AUTO_INCREMENT, Name VARCHAR(50) NOT NULL, user_id INT, CONSTRAINT fk_user_id FOREIGN KEY (user_id) REFERENCES Users(user_id) ); The constraint is given the name fk_user_id and is added to the user_id attribute. The FOREIGN KEY constraint states that the user_id attribute references the user_id attribute in the Users table.\nRetrieving Data Retrieving data from an SQL database is done with an SFW query, SELECT-FROM-WHERE.\nSELECT \u0026lt;attribute list\u0026gt; FROM \u0026lt;table list\u0026gt; WHERE \u0026lt;condition\u0026gt; For example, we can get the experience and level of a character named Atticus from the Characters table with the following query.\nSELECT experience, level FROM Characters WHERE Name = \u0026#39;Atticus\u0026#39;; The attributes we retrieve in a query are referred to as the projection attributes. This query SELECT~s a ~Character from all rows of the Character table which satisfy the selection condition of the WHERE clause. We can also query the e-mail addresses of all users who have a character that is a human.\nSELECT email FROM Users, Characters, Races WHERE Users.user_id = Characters.user_id AND Characters.race_id = Races.id AND Races.name = \u0026#39;Human\u0026#39;; The WHERE clause in this example is an example of a join condition since it combines attributes from multiple tables. Note that there are two tables which have a user_id attribute, so we must differentiate them by prepending the table name before the attribute name. This is how ambiguities are solved in SQL.\nYou can also use the AS keyword to shorthand the table names in your query. The previous query can be rewritten as\nSELECT U.username FROM Users AS U, Characters AS C, Races AS R WHERE U.user_id = C.user_id AND R.id = C.race_id AND R.name = \u0026#39;Human\u0026#39;; Duplicate Return Values The previous query returns the names of all users who have a Human character. If a user has multiple characters that are Human, it will return their name multiple times. If we are instead only interested in the names of users who have a Human character, we can use the DISTINCT keyword to remove duplicate values.\nSELECT DISTINCT U.username FROM Users AS U, Characters AS C, Races AS R WHERE U.user_id = C.user_id AND R.id = C.race_id AND R.name = \u0026#39;Human\u0026#39;; Tables as Sets SQL uses some set operations from set theory. It supports the UNION, set difference EXCEPT, and set intersection INTERSECT operations. The following query returns the names of all users who have a Human character or a Gnome character.\n(SELECT DISTINCT U.username FROM Users as U, Characters as C, Races as R WHERE U.user_id = C.user_id AND C.race_id = R.id AND R.name = \u0026#39;Human\u0026#39;) UNION (SELECT DISTINCT U.username FROM Users as U, Characters as C, Races as R WHERE U.user_id = C.user_id AND C.race_id = R.id AND R.name = \u0026#39;Gnome\u0026#39;); If we wanted to find the users who had both a Human character and a Gnome character, we could use the INTERSECT operator instead.\n(SELECT DISTINCT U.username FROM Users as U, Characters as C, Races as R WHERE U.user_id = C.user_id AND C.race_id = R.id AND R.name = \u0026#39;Human\u0026#39;) INTERSECT (SELECT DISTINCT U.username FROM Users as U, Characters as C, Races as R WHERE U.user_id = C.user_id AND C.race_id = R.id AND R.name = \u0026#39;Gnome\u0026#39;); We can also use the EXCEPT operator to find the users who have a Human character but not a Gnome character.\n(SELECT DISTINCT U.username FROM Users as U, Characters as C, Races as R WHERE U.user_id = C.user_id AND C.race_id = R.id AND R.name = \u0026#39;Human\u0026#39;) EXCEPT (SELECT DISTINCT U.username FROM Users as U, Characters as C, Races as R WHERE U.user_id = C.user_id AND C.race_id = R.id AND R.name = \u0026#39;Gnome\u0026#39;); Pattern Matching SQL supports pattern matching with the LIKE operator. The LIKE operator is used in the WHERE clause to search for a specified pattern in a column. This is different from equality operators since it allows us to search for patterns rather than exact matches. The following table shows the most common wildcards used in SQL.\nWildcard Description % Matches any string of zero or more characters. _ Matches any single character. [] Matches any single character within the brackets. [^] Matches any single character not within the brackets. The following query returns the names of all simple items in the Items table. These can be found based on their description, since the term simple is not explicitly mentioned in the name.\nSELECT name FROM Items WHERE description LIKE \u0026#39;%simple%\u0026#39;; We can also query based on arithmetic ranges. For example, we might be interested in the items that are less than 100 gold.\nSELECT name FROM Items WHERE value \u0026lt; 100; Ordering SQL allows us to order the results of our query with the ORDER BY clause. The following query returns the names of all items in the Items table ordered by their value.\nSELECT name FROM Items ORDER BY value; We can also order by multiple attributes. The following query returns the names of all items in the Items table ordered by their value and then their name.\nSELECT name FROM Items ORDER BY value, name; Modifying Data Inserting Data We previously saw an example of inserting new data. Let\u0026rsquo;s insert a new user account to our table. If we are inserting a value for every attribute, we can omit the attribute list.\nINSERT INTO Users VALUES (7, \u0026#39;Alex\u0026#39;, \u0026#39;alex.dillhoff@uta.edu\u0026#39;, \u0026#39;2023-10-31 15:26:17\u0026#39;); If we are only inserting values for some attributes, we must specify the attribute list.\nINSERT INTO Users (user_id, username, email) VALUES (7, \u0026#39;Alex\u0026#39;, \u0026#39;alex.dillhoff@uta.edu\u0026#39;); If we attempt to leave out a value for an attribute that is not nullable, we will get an error. While working on our database, we may have realized that some of these important attributes should always be specified. We can add a NOT NULL constraint to these attributes to ensure that they are always specified. We will look at ways of modifying tables in the next section.\nUpdating Data Updating data is a common task and is easily supported by the UPDATE command. In an RPG, players will use items, gain experience, and level up. All of these will require modifications to existing tables. For example, if we wish to update the experience of a character, we can use the following query.\nUPDATE Characters SET experience = experience + 100 WHERE name = \u0026#39;Atticus\u0026#39;; Deleting Data Deleting a tuple or several tuples is straightforward in SQL. The following query deletes the user with the user_id of 7 from the Users table.\nDELETE FROM Users WHERE user_id = 7; If we want to delete all tuples from a table, we can use the TRUNCATE TABLE command. This command is faster than deleting all tuples with the DELETE command since it does not log each deletion. However, it cannot be used if the table is referenced by a foreign key constraint.\nTRUNCATE TABLE Users; When deleting tuples from a database, it\u0026rsquo;s important to consider any foreign key constraints that the table may have. If we delete a tuple from a table that is referenced by a foreign key constraint, we may end up with orphaned tuples. For example, if we delete a user from the Users table, we may end up with a character that has no user. We can avoid this by adding a CASCADE constraint to the DELETE statement. This will delete all tuples that reference the tuple we are deleting.\nNested Queries Nested queries allow us to make more complex queries on subsets of data returned from an original query. A nested query can be placed in any of the SELECT, FROM, or WHERE clauses. The query that uses the results of the nested query is called the outer query.\nWhat if we wanted a list of users who had at least 1 character whose class was the least represented class across all characters? We could first make a query to identify which class is the least represented before using that to find the users who have a character with that class.\nSELECT username FROM Users WHERE user_id IN (SELECT user_id FROM Characters WHERE class_id = (SELECT class_id FROM Characters GROUP BY class_id ORDER BY COUNT(*) ASC LIMIT 1)); The innert-most query returns the class_id of the least represented class by counting the number of characters in each class and ordering them in ascending order. The middle query returns the user_id of all characters whose class is the least represented class. The outer query returns the username of all users who have a character with the least represented class.\nPay attention to the second WHERE clause that uses the = operator instead of IN. This is because the nested query returns a single value and single tuple. If we used the IN operator, we would get an error since MySQL does not support the LIMIT and IN/ALL/ANY/SOME operators together.\nCorrelated Nested Queries Some nested queries would have to execute for each tuple in the outer query. Consider the following query which returns the name and ID of all Human characters.\nSELECT C.id, C.name FROM Characters as C WHERE C.race_id IN (SELECT R.id FROM Races as R WHERE R.name = \u0026#39;Human\u0026#39;); This query is an example of a correlated nested query since the inner query is dependent on the outer query. The inner query must be executed for each tuple in the outer query. This can be inefficient if the outer query returns a large number of tuples.\nSince this query uses only an IN operator, we can rewrite it as a single block query.\nSELECT C.id, C.name FROM Characters as C, Races as R WHERE C.race_id = R.id AND R.name = \u0026#39;Human\u0026#39;; Let\u0026rsquo;s revisit an earlier query to introduce the EXISTS operator. If we want to query the user names and IDs of all users who have an elf character, we can use the following query.\nSELECT U.id, U.username FROM Users as U WHERE EXISTS (SELECT * FROM Characters as C, Races as R WHERE C.user_id = U.id AND C.race_id = R.id AND R.name = \u0026#39;Elf\u0026#39;); We can also use the NOT EXISTS operator to find the users who do not have an elf character. This works opposite to the EXISTS operator.\nJoined Tables Joined tables are the result of a query that combines rows from two or more tables. The syntax itself may be easier to understand as compared to the nested queries written above. The following query returns the names of all users who have a Human character.\nSELECT U.username, C.name FROM (Users as U JOIN Characters as C ON U.user_id = C.user_id), Races as R WHERE C.race_id = R.id AND R.name = \u0026#39;Human\u0026#39;; Here, the JOIN function is used to combine both tables on the condition that the user_id of the Users table is equal to the user_id of the Characters table. The WHERE clause is used to filter the results to only those that have a Human character.\nThis type of join is also referred to as an inner join since it only returns rows that satisfy the join condition. There are several other types of joins that we can use to combine tables. If we wanted to return the same information along with all of the users who do not have a Human character, we can use a left outer join.\nSELECT U.username, C.name FROM (Users as U LEFT OUTER JOIN (Characters as C JOIN Races as R ON C.race_id = R.id AND R.name = \u0026#39;Human\u0026#39;) ON U.user_id = C.user_id); Aggregate Functions Aggregate functions are used to perform calculations on a set of values and return a single value. The following table shows the most common aggregate functions supported by SQL.\nFunction Description AVG() Returns the average value of a numeric column. COUNT() Returns the number of rows that match a specified criteria. MAX() Returns the maximum value of a column. MIN() Returns the minimum value of a column. SUM() Returns the sum of all values in a column. These can be used in the SELECT clause. The following query returns the average level of all characters.\nSELECT AVG(level) FROM Characters; These statistics can be used with more complex queries. The following query returns the name of the user with the highest level character.\nSELECT U.username FROM Users as U, Characters as C WHERE U.user_id = C.user_id AND C.level = (SELECT MAX(level) FROM Characters); In the next example, the query returns the tuple of the highest level Human character.\nSELECT C.* FROM (Characters as C JOIN Races as R ON C.race_id = R.id AND R.name = \u0026#39;Human\u0026#39;) WHERE C.level = (SELECT MAX(level) FROM Characters); Grouping As we just saw, aggregate functions permit some preliminary data analysis. We can take this further using grouping. For example, we calculated above the average level of all characters. What if we wanted to compute the average level of each class? We can use the GROUP BY clause to group the tuples by class.\nSELECT C.class_id, AVG(C.level) FROM Characters as C GROUP BY C.class_id; In the next example, we will run a similar query except we will also include the number of distinct users who have a character of that class.\nSELECT C.class_id, AVG(C.level), COUNT(DISTINCT C.user_id) FROM Characters as C GROUP BY C.class_id; WITH Clause Let\u0026rsquo;s say we wanted to get the actual class name along with the users that had a character with the least represented class. We can use a nested query in the FROM clause to get the class name.\nSELECT username, Classes.name FROM Users, Classes WHERE user_id IN (SELECT user_id FROM Characters WHERE class_id = (SELECT class_id FROM Characters GROUP BY class_id ORDER BY COUNT(*) ASC LIMIT 1)) AND Classes.id = (SELECT class_id FROM Characters GROUP BY class_id ORDER BY COUNT(*) ASC LIMIT 1); This query might immediately come across as inefficient since we are making the same query multiple times. If you agree, your intuition would be right. We can use the WITH clause to make this query more efficient.\nWITH LeastUsedClass AS ( SELECT class_id FROM Characters GROUP BY class_id ORDER BY COUNT(*) ASC LIMIT 1 ) SELECT U.username, C.name FROM Users U JOIN Characters CH ON U.user_id = CH.user_id JOIN Classes C ON C.id = CH.class_id WHERE CH.class_id = (SELECT class_id FROM LeastUsedClass); Modifying Tables Modifying databases used in production is inevitable. Typically, you will modify an offline test version before deploying it, but the process is the same. The ALTER TABLE command is used to modify tables. The following table shows the most common modifications that can be made to a table.\nCommand Description ADD Adds a new column to the table. DROP Deletes a column from the table. MODIFY Changes the data type of a column. RENAME Changes the name of a column. When we originally created the Users table, we did not specify a NOT NULL constraint for the username attribute. We can add this constraint with the following command.\nALTER TABLE Users MODIFY username VARCHAR(50) NOT NULL; Another mistake that was made was with the name of the ID column. We can rename this column with the following command.\nALTER TABLE Users RENAME COLUMN user_id TO id; Scenario: Updating Foreign Keys Let\u0026rsquo;s say we want to delete anything related to a user if that user is deleted from the Users table. That means all characters and inventories associated with those characters should be deleted. Currently, attempting to delete a user will fail with the following error:\nSQL Error [1451] [23000]: Cannot delete or update a parent row: a foreign key constraint fails (`rpg`.`Inventory`, CONSTRAINT `Iventory_ibfk_1` FOREIGN KEY (`character_id`) REFERENCES `Characters` (`id`)) It looks like the foreign key also has a typo. Let\u0026rsquo;s recreate this foreign key so that it will delete all characters and inventories associated with a user. First we drop the current key.\nALTER TABLE Inventory DROP FOREIGN KEY Iventory_ibfk_1; Then we add a new one.\nALTER TABLE Inventory ADD CONSTRAINT Inventory_ibfk_1 FOREIGN KEY (character_id) REFERENCES Characters(id) ON DELETE CASCADE; Summary A typical SQL query consists of a SELECT clause, a FROM clause, and a WHERE clause. The SELECT clause specifies the attributes to be returned. The FROM clause specifies the tables to be queried. The WHERE clause specifies the conditions that must be satisfied for a tuple to be returned.\nAs we saw in the examples above, there are up to 6 clauses that can be used in a query. The following table shows the clauses that can be used in a query and the order in which they must appear.\nClause Description SELECT Specifies the attributes to be returned. FROM Specifies the tables to be queried. WHERE Specifies the conditions that must be satisfied for a tuple to be returned. GROUP BY Groups the tuples by a specified attribute. HAVING Specifies the conditions that must be satisfied for a group to be returned. ORDER BY Specifies the order in which the tuples are returned. References Chamberlin, Donald D., and Raymond F. Boyce. 1974. “SEQUEL: A Structured English Query Language.” In Proceedings of the 1974 ACM SIGFIDET (Now SIGMOD) Workshop on Data Description, Access and Control, 249–64. SIGFIDET ’74. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/800296.811515. Codd, E. F. 1970. “A Relational Model of Data for Large Shared Data Banks.” Communications of the Acm 13 (6): 377–87. https://doi.org/10.1145/362384.362685. ","date":1698642000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698642000,"objectID":"3cd2d98d654a4ed902465704df9083fb","permalink":"https://ajdillhoff.github.io/notes/structured_query_language/","publishdate":"2023-10-30T00:00:00-05:00","relpermalink":"/notes/structured_query_language/","section":"notes","summary":"Table of Contents History and Development Schemas Data Types Creation Constraints Retrieving Data Modifying Data Nested Queries Joined Tables Aggregate Functions Grouping WITH Clause Modifying Tables Summary History and Development Structured Query Language (SQL) is a database language for managing data in a relation DBMS. Its original inception was based on a paper by Edgar F. Codd in 1970 titled A Relational Model of Data for Large Shared Data Banks (Codd 1970).","tags":["computer science","databases"],"title":"Structured Query Language","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents An Online RPG Database From Schema to Database Database Management Systems Creating our RPG Database Recommended Reading: Chapters 1 and 2 from (Elmasri and Navathe 2015)\nDatabases allow us to store, retrieve, and edit different types of data. They should be scalable, secure, and reliable. They should also be able to handle concurrent access and be able to recover from failures. There are multiple types of databases that are optimized for different use cases. Tabular data, for example, is typically stored in a relational database. Large format data such as images, videos, and audio are typically stored in a non-relational database.\nCreating, deploying, and maintaining databases is facilitated through a database management system (DBMS). A DBMS is a software system that allows us to interact with a database. It provides an interface for us to create, read, update, and delete data. It also provides a way for us to define the structure of our data and the relationships between different pieces of data. Examples of DBMSs include MySQL, PostgreSQL, and MongoDB.\nOnce a database is deployed, we can interact with it a number of ways. Most DBMSs include a client which allows us to interact with the database through a command line interface. We can also interact with the database through a programming language such as Python or Java.\nIt is important to emphasize that a database is not the same thing as a file system. A file system is a way to store data on a disk, whereas a database is a way to store data in a file system. File systems are good at managing unstructured data with little regard to the relationships inherit in the data itself. What if multiple people working on the same document try to save their changes at the same time? What if a user tries to delete a file that is currently being used by another user? These are problems that a file system is not designed to handle.\nAn Online RPG Database To introduce some foundational terms and concepts of databases, let\u0026rsquo;s design and create a database for an online RPG. In this game, users can create accounts, make multiple characters, store items for their characters, and embark on quests to level up their characters. Even from this simple description, we can start separating our data into different entities and relationships. Each logical entity in our game will be represented by a table in our database. The attributes of each table will be represented by columns in our database. For this database, we will need at least the following tables:\nUsers Characters Items Inventory Quests We may add or modify these depending on the finer details. If you are not familiar with online RPG games, don\u0026rsquo;t worry. We will be sure to include the necessities to get us started. Let\u0026rsquo;s start with the first table, Users.\nUsers A User represents an online account that is unique to each person who plays the game. It should contain the username, email, and date that it was created, which we will call created_at. This is enough information for now. Using this, we can create our first table. There is one more attribute that wasn\u0026rsquo;t explicitly mentioned. Each User in our table should have a unique identifier. This is called a primary key. We will use a sequentially increasing number starting at 1 for our primary key. This is a common practice, but it is not the only way to do it. We will call this column user_id. The full table is showing below.\nUsers\nuser_id: primary key username email created_at Characters It is common for users to have multiple characters so they can experience the full range of our game. This table will have more attributes than the Users table since there are a wide range of stats that our characters can have, such as their name, level, experience, and health. We will also need to know which user each character belongs to. We can do this by adding a column called user_id which will be a foreign key to the Users table. This will allow us to link each character to the user that created it. The full table is shown below.\nCharacters\ncharacter_id: primary key user_id: foreign key name level experience health created_at Items As our user\u0026rsquo;s play, they will collect items such as weapons, armor, and potions. As our game evolves, our game designers will add more items to the game. A table for our items is shown below.\nItems\nitem_id: primary key name value Inventory Our users will need a way to store their items. We can do this by creating a table called Inventory. This table will have a foreign key to the Characters table so we can link each item to the character that owns it. It will also have a foreign key to the Items table so we can link each item to the item that it represents. We will also need to know how many of each item our users have. We can do this by adding a column called quantity. The full table is shown below.\nInventory\ninventory_id: primary key character_id: foreign key item_id: foreign key quantity Quests No RPG would be complete without quests that our player\u0026rsquo;s could embark upon. The Quests table will have a name, description, and a reward. In the case of multiple rewards, we can create a separate table called QuestRewards that will have a foreign key to the Quests table and a foreign key to the Items table. This will allow us to link each quest to the items that it rewards. This means that the Quests table does not need an explicit reference to the reward item. We can look those up separately. The full table is shown below.\nQuests\nquest_id: primary key name description reward_experience min_level QuestRewards\nquest_reward_id: primary key quest_id: foreign key item_id: foreign key A Few Extras There are a few more tables we should add to round out our characters. Most RPGs allow the users to create characters of different races, such as a human, orc, or elf, as well as the characters class, which defines what sort of abilities the character will have.\nRace\nrace_id: primary key name Class\nclass_id: primary key name With the addition of these two tables, let\u0026rsquo;s add foreign keys to our original Characters table. We will add a race_id and a class_id. The full table is shown below.\nCharacters\ncharacter_id: primary key user_id: foreign key name level experience health race_id: foreign key class_id: foreign key created_at That\u0026rsquo;s it! We have all the tables we need to get us started. All tables with example data are shown below. You\u0026rsquo;ll notice that each of the primary IDs in the tables below have been renamed to id. Besides giving us extra room to display the table, the primary key is always unique to the table, so we don\u0026rsquo;t need to include the table name in the column name.\nUsers\n\\begin{array}{|r|l|l|l|} \\hline \\text{id} \u0026amp; \\text{username} \u0026amp; \\text{email} \u0026amp; \\text{created_at} \\\\ \\hline 1 \u0026amp; \\text{Naomi} \u0026amp; \\text{player1@example.com} \u0026amp; \\text{2023-01-01 10:00:00} \\\\ 2 \u0026amp; \\text{Clarissa} \u0026amp; \\text{player2@example.com} \u0026amp; \\text{2023-01-02 11:00:00} \\\\ 3 \u0026amp; \\text{Avasarala} \u0026amp; \\text{player3@example.com} \u0026amp; \\text{2023-01-03 12:00:00} \\\\ \\hline \\end{array}\nCharacters\n\\begin{array}{|r|r|l|r|r|r|r|r|l|} \\hline \\text{id} \u0026amp; \\text{user_id} \u0026amp; \\text{name} \u0026amp; \\text{class_id} \u0026amp; \\text{race_id} \u0026amp; \\text{level} \u0026amp; \\text{experience} \u0026amp; \\text{health} \u0026amp; \\text{created_at} \\\\ \\hline 1 \u0026amp; 1 \u0026amp; \\text{Atticus} \u0026amp; 1 \u0026amp; 1 \u0026amp; 10 \u0026amp; 1000 \u0026amp; 100 \u0026amp; \\text{2023-01-01 10:10:00} \\\\ 2 \u0026amp; 1 \u0026amp; \\text{Bobbie} \u0026amp; 2 \u0026amp; 2 \u0026amp; 15 \u0026amp; 1500 \u0026amp; 200 \u0026amp; \\text{2023-01-01 10:20:00} \\\\ 3 \u0026amp; 2 \u0026amp; \\text{Raimi} \u0026amp; 3 \u0026amp; 3 \u0026amp; 8 \u0026amp; 800 \u0026amp; 90 \u0026amp; \\text{2023-01-02 11:10:00} \\\\ 4 \u0026amp; 3 \u0026amp; \\text{Beef} \u0026amp; 4 \u0026amp; 4 \u0026amp; 12 \u0026amp; 1200 \u0026amp; 110 \u0026amp; \\text{2023-01-03 12:10:00} \\\\ 5 \u0026amp; 2 \u0026amp; \\text{Demon} \u0026amp; 4 \u0026amp; 4 \u0026amp; 12 \u0026amp; 1200 \u0026amp; 110 \u0026amp; \\text{2023-01-05 12:10:00} \\\\ \\hline \\end{array}\nItems\n\\begin{array}{|r|l|r|r|} \\hline \\text{id} \u0026amp; \\text{name} \u0026amp; \\text{value} \\\\ \\hline 1 \u0026amp; \\text{Sword} \u0026amp; 100 \\\\ 2 \u0026amp; \\text{Shield} \u0026amp; 150 \\\\ 3 \u0026amp; \\text{Staff} \u0026amp; 200 \\\\ 4 \u0026amp; \\text{Bow} \u0026amp; 250 \\\\ \\hline \\end{array}\nInventory\n\\begin{array}{|r|r|r|r|} \\hline \\text{id} \u0026amp; \\text{character_id} \u0026amp; \\text{item_id} \u0026amp; \\text{quantity} \\\\ \\hline 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \\\\ 2 \u0026amp; 2 \u0026amp; 2 \u0026amp; 1 \\\\ 3 \u0026amp; 3 \u0026amp; 3 \u0026amp; 1 \\\\ 4 \u0026amp; 4 \u0026amp; 4 \u0026amp; 1 \\\\ \\hline \\end{array}\nQuests\n\\begin{array}{|r|l|l|r|l|r|} \\hline \\text{id} \u0026amp; \\text{name} \u0026amp; \\text{description} \u0026amp; \\text{reward_experience} \u0026amp; \\text{min_level} \\\\ \\hline 1 \u0026amp; \\text{Linken\u0026rsquo;s Sword} \u0026amp; \\text{Desc1} \u0026amp; 100 \u0026amp; 5 \\\\ 2 \u0026amp; \\text{Mankrik\u0026rsquo;s Wife} \u0026amp; \\text{Desc2} \u0026amp; 200 \u0026amp; 10 \\\\ 3 \u0026amp; \\text{The Hermit} \u0026amp; \\text{Desc3} \u0026amp; 300 \u0026amp; 15 \\\\ 4 \u0026amp; \\text{The Great Masquerade} \u0026amp; \\text{Desc4} \u0026amp; 400 \u0026amp; 20 \\\\ \\hline \\end{array}\nQuestRewards\n\\begin{array}{|r|r|r|} \\hline \\text{id} \u0026amp; \\text{quest_id} \u0026amp; \\text{item_id} \\\\ \\hline 1 \u0026amp; 1 \u0026amp; 1 \\\\ 2 \u0026amp; 2 \u0026amp; 2 \\\\ 3 \u0026amp; 3 \u0026amp; 3 \\\\ 4 \u0026amp; 4 \u0026amp; 4 \\\\ \\hline \\end{array}\nRaces\n\\begin{array}{|r|l|} \\hline \\text{race_id} \u0026amp; \\text{name} \\\\ \\hline 1 \u0026amp; \\text{Human} \\\\ 2 \u0026amp; \\text{Elf} \\\\ 3 \u0026amp; \\text{Dwarf} \\\\ 4 \u0026amp; \\text{Orc} \\\\ \\hline \\end{array}\nClasses\n\\begin{array}{|r|l|} \\hline \\text{class_id} \u0026amp; \\text{name} \\\\ \\hline 1 \u0026amp; \\text{Warrior} \\\\ 2 \u0026amp; \\text{Mage} \\\\ 3 \u0026amp; \\text{Rogue} \\\\ 4 \u0026amp; \\text{Paladin} \\\\ \\hline \\end{array}\nFrom Schema to Database What we did in the previous example is created a database schema based on our entities. A schema does not represent the entire picture of our data model. Relationships and other constraints are not represented in the schema. The data model itself defines the structure of a database, including data types, relationships, constraints, and a set of operations for performing basic functions like retrieving and updating data.\nThe Three-Schema Architecture The three-schema architecture is a way to separate the different aspects of a database. The three schemas are the external schema, the conceptual schema, and the internal schema. The internal schema describes how the data is stored on disk. Unless we are working on the backend of the database, we typically do not need to worry about the internal level. The external schema describes how the data is viewed by the user. This is the level that we interact with when we use a DBMS. The conceptual schema is the middle layer that describes the logical structure of the data. This is the level that we are working with when we create a schema.\nUnder this architecture, we can modify the internal schema without affecting the external schema. This is important because it allows us to change the way that the data is stored without affecting the applications that use it. We can also modify the external schema without affecting the internal schema. This allows us to change the way that the data is viewed without affecting the applications that use it. This concept of data independence is one of the most important features of a DBMS.\nDatabase Management Systems With our database defined, we can use it to make queries about the records that it stores. How we access that database depends on the DBMS that we are using. The database itself is can be modified and changed without affecting the applications that use it. We can also create multiple views of our data dynamically. For example, we can create a view that shows all of the items that a user has in their inventory, or show all of the characters that belong to a specific user. This is all done without modifying the underlying data. This is a powerful feature of databases that allows us to create complex applications that can be easily modified and updated.\nA transaction is a set of operations that are performed on a database. Transactions are typically used to ensure that the database is in a consistent state. For example, if we want to transfer money from one account to another, we need to make sure that the money is removed from one account and added to the other. If we fail to do this, we could end up with money that is neither in the original account nor the destination account. Transactions allow us to perform these operations in a way that guarantees that the database is in a consistent state.\nA DBMS must ensure transactional properties such as isolation, which ensures that each transaction executes in isolation from others, and atomicity, which ensures that either all operations in a transaction are executed or none are.\nDBMS Languages A DBMS provides a way for us to interact with the database. Depending on the level of abstraction and the DBMS itself, a specific language is used to perform basic operations on the database. The most common languages are data definition languages (DDLs) and data manipulation languages (DMLs). A DDL is used to define the structure of the database, such as creating tables and defining relationships between them. A DML is used to perform operations on the data itself, such as inserting, updating, and deleting records.\nA common query language called Structured Query Language (SQL) defines both DDLs and DMLs. For example, to create our User table from above, we can use the following SQL statement:\nCREATE TABLE Users ( user_id INT PRIMARY KEY, username VARCHAR(255), email VARCHAR(255), created_at DATETIME ); Note that we must specify a type for each attribute in our table. SQL also provides a DML, we can use to insert records into our table:\nINSERT INTO Users (user_id, username, email, created_at) VALUES (1, \u0026#39;Naomi\u0026#39;, \u0026#39;player1@example.com\u0026#39;, \u0026#39;2023-01-01 10:00:00\u0026#39;); DBMS Interfaces A DBMS provides an interface for us to interact with the database. This interface can be a command line interface, a graphical user interface, or a programming language interface. Other interfaces using natural language or voice can also be found in the wild. With the rapid advancement of machine learning, these interfaces are becoming more and more common. Here is an example of a chatbot that can be used to query a database.\nCreating our RPG Database For this example, we will be using MySQL. We only want to make sure that we have MySQL installed and are able to interface with the command line. You can find a thorough installation guide here. Once it is installed and configured, start the MySQL server and log in using the following command:\nmysql -u root -p You should be prompted for a password. If you have not set a password, you can leave it blank. Once you are logged in, you should see a prompt that looks like this:\nmysql\u0026gt; Let\u0026rsquo;s create a database for our RPG. We can do this with the following command:\nCREATE DATABASE rpg; We can verify that the database was created by listing all of the databases on the server:\nSHOW DATABASES; You should see the rpg database in the list. We can now use this database to create our tables. We can do this with the following command:\nUSE rpg; This will tell MySQL to use the rpg database for all subsequent commands. We can now create our Users table:\nCREATE TABLE Users ( user_id INT PRIMARY KEY, username VARCHAR(255), email VARCHAR(255), created_at DATETIME ); We can verify that the table was created by listing all of the tables in the database:\nSHOW TABLES; You should see the Users table in the list. We can now insert some data into the table:\nINSERT INTO Users (user_id, username, email, created_at) VALUES (1, \u0026#39;Naomi\u0026#39;, \u0026#39;player1@example.com\u0026#39;, \u0026#39;2023-01-01 10:00:00\u0026#39;), (2, \u0026#39;Clarissa\u0026#39;, \u0026#39;player2@example.com\u0026#39;, \u0026#39;2023-01-02 11:00:00\u0026#39;), (3, \u0026#39;Avasarala\u0026#39;, \u0026#39;player3@example.com\u0026#39;, \u0026#39;2023-01-03 12:00:00\u0026#39;); We can verify that the data was inserted by querying the table:\nSELECT * FROM Users; You should see the data that we inserted in the table. We can now create the rest of our tables:\nCREATE TABLE Characters ( character_id INT PRIMARY KEY, user_id INT, name VARCHAR(255), level INT, experience INT, health INT, created_at DATETIME ); CREATE TABLE Items ( item_id INT PRIMARY KEY, name VARCHAR(255), value INT ); CREATE TABLE Inventory ( inventory_id INT PRIMARY KEY, character_id INT, item_id INT, quantity INT ); CREATE TABLE Quests ( quest_id INT PRIMARY KEY, name VARCHAR(255), description VARCHAR(255), reward_experience INT, min_level INT ); CREATE TABLE QuestRewards ( quest_reward_id INT PRIMARY KEY, quest_id INT, item_id INT ); Try creating the tables for the Races and Classes yourself. Once you are done, you can insert some data into the tables. Use the samples from above or create your own. Once you are done, you can query the tables to verify that the data was inserted correctly.\nReferences Elmasri, Ramez, and Shamkant B. Navathe. 2015. Fundamentals of Database Systems. 7th ed. Pearson. https://www.pearson.com/en-us/subject-catalog/p/fundamentals-of-database-systems/P200000003546/9780137502523. ","date":1698469200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698469200,"objectID":"85830aa0149b307a38bed69c592b16a7","permalink":"https://ajdillhoff.github.io/notes/introduction_to_databases/","publishdate":"2023-10-28T00:00:00-05:00","relpermalink":"/notes/introduction_to_databases/","section":"notes","summary":"Table of Contents An Online RPG Database From Schema to Database Database Management Systems Creating our RPG Database Recommended Reading: Chapters 1 and 2 from (Elmasri and Navathe 2015)\nDatabases allow us to store, retrieve, and edit different types of data. They should be scalable, secure, and reliable. They should also be able to handle concurrent access and be able to recover from failures. There are multiple types of databases that are optimized for different use cases.","tags":["computer science","databases"],"title":"Introduction to Databases","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Definition Finding the Minimum Spanning Tree Kruskal\u0026rsquo;s Algorithm Prim\u0026rsquo;s Algorithm Minimum spanning trees are undirected graphs that connect all of the vertices such that there are no redundant edges and the total weight is minimized. They are useful for finding the shortest path between two points in a graph. Useful application of MSTs include\nnetwork design: it is useful to know the least expensive path with respect to either latency or resource cost for telecommunications networks, transportation networks, or electrical grids. approximation algorithms: MSTs can be used to approximate the solution to the traveling salesman problem. clustering: MSTs can be used to cluster data points in a graph. image segmentation: MSTs can be used to segment images into smaller regions. Definition Let \\(G\\) be a connected, undirected graph with edges \\(E\\), vertices \\(V\\), and edge weights \\(w\\). A minimum spanning tree is a subset \\(T \\subseteq E\\) that connects all of the vertices such that the total weight is minimized. The original graph \\(G\\) is shown below.\nFigure 1: An undirected graph with redundant edges. The minimum spanning tree of the above graph is show below. All of the redundant edges have been removed, but there is still a path between each pair of nodes.\nFigure 2: The minimum spanning tree of (G). As described in Introduction to Algorithms there are two greedy algorithms for finding the minimum spanning tree of a graph (Cormen et al. 2022). These notes will review both of these, but first let\u0026rsquo;s look at a general algorithm for finding the minimum spanning tree of a graph.\nFinding the Minimum Spanning Tree The general algorithm for finding the minimum spanning tree of a graph grows a set of edges \\(T\\) from an empty set. At each step, the algorithm adds the edge with the smallest weight that does not create a cycle. The algorithm terminates when \\(T\\) is a complete tree.\nT = {} while T is not a spaning tree find the edge e with the smallest weight that does not create a cycle T = T union {e} Each edge \\(e\\) that is added must result in a tree that is a subset of the minimum spanning tree. The challenge of this algorithm is actually finding such an edge. How would we know such an edge if we saw it? We first need to define a few properties which will shine light on this.\nA cut of a graph \\(G\\) is a partition of the vertices \\(V\\) into two disjoint sets \\(S\\) and \\(V - S\\). An edge \\(e\\) crosses the cut if one of its endpoints is in \\(S\\) and the other is in \\(V - S\\). If no edge in a given set \\(E\\) crosses the cut, then that cut respects \\(E\\). An edge that is the minimum weight edge that crosses a cut is called a light edge. With these definitions, we can now formally define how to find a safe edge, which is an edge that can be added to the current set of edges \\(T\\) without creating a cycle.\nTheorem 21.1 (Cormen et al. 2022)\nLet \\(G = (V, E)\\) be a connected, undirected graph with a real-valued weight function \\(w\\) defined on \\(E\\). Let \\(A\\) be a subset of \\(E\\) that is included in some minimum spanning tree for \\(G\\), let \\((S, V - S)\\) be any cut of \\(G\\) that respects \\(A\\), and let \\(e\\) be a light edge crossing \\((S, V - S)\\). Then, edge \\(e\\) is safe for \\(A\\).\nFigure 3: Visual proof of Theorem 21.1 (Cormen et al. 2022). Proof\nThe two sets in the figure above represent vertices in \\(S\\) (orange) and vertices in \\(V - S\\) (tan). \\(T\\) is the original MST depicted in the figure. The dotted line is the new edge \\((u, v)\\) to consider. \\(A\\) is a subset of edges in \\(T\\) represented by the blue lines. If the safe edge \\((u, v)\\) is already in the original MST \\(T\\), then we are done.\nThe vertices \\(u\\) and \\(v\\) lie on opposite sides of the cut. The edge \\((u, v)\\) would introduce a cycle since there is already a path from \\(u\\) to \\(v\\) in \\(T\\) that crosses the cut via \\((x, y)\\). Since both \\((u, v)\\) and \\((x, y)\\) are light edges that cross the cut, then it must be that \\(w(u, v) \\leq w(x, y)\\).\nLet \\(T\u0026rsquo;\\) be the minimum spanning tree with \\((x, y)\\) replaced by \\((u, v)\\). That is \\(T\u0026rsquo; = T - \\{(x, y)\\} \\cup \\{(u, v)\\}\\). Since \\(T\\) is a minimum spanning tree, then \\(w(T) \\leq w(T\u0026rsquo;)\\). Since \\(w(T) = w(T\u0026rsquo;)\\), then \\(T\u0026rsquo;\\) is also a minimum spanning tree. Therefore, \\((u, v)\\) is safe for \\(A\\).\nCorollary 21.2 (Cormen et al. 2022)\nWe can also view this in terms of connected components, which are subsets of vertices that are connected by a path. If \\(C\\) and \\(C\u0026rsquo;\\) are two connected components in \\(T\\) and \\((u, v)\\) is a light edge connecting \\(C\\) and \\(C\u0026rsquo;\\), then \\((u, v)\\) is safe for \\(T\\).\nThe figure below shows a graph with two individual components. If the edge \\((u, v)\\) is a light edge, then it is safe to add it to the set of edges \\(T\\).\nFigure 4: Two connected components from a graph (left). Adding a safe edge (right). Kruskal\u0026rsquo;s Algorithm The first solution to the minimum spanning tree that we will study is called Kruskal\u0026rsquo;s algorithm. This algorithm grows a forest of trees from an empty set. At each step, the algorithm adds the lightest edge that does not create a cycle. The algorithm terminates when the forest is a single tree. This can be viewed as an agglomerative clustering algorithm. The algorithm starts with each vertex in its own cluster. At each step, the algorithm merges the two clusters that are closest together. The algorithm terminates when there is only one cluster.\nThe algorithm is given below (Cormen et al. 2022).\nKruskal\u0026rsquo;s Algorithm\nA = {} for each vertex v in G.V MAKE-SET(v) sort the edges of G.E into nondecreasing order by weight w for each edge (u, v) in G.E, taken in nondecreasing order by weight if FIND-SET(u) != FIND-SET(v) A = A union {(u, v)} UNION(u, v) return A A step-by-step example of an implementation in Python is available here.\nAnalysis The running time is dependent on how the disjoint-set of vertices is implemented. In the best known case, a disjoint-set-forest implementation should be used (Cormen et al. 2022). Creating a list of edges takes \\(O(E)\\) time. Sorting the edges takes \\(O(E \\log E)\\) time. The for loop iterates over each edge, which is \\(O(E)\\). All disjoin-set operations take \\(O((V + E)\\alpha(V))\\) time. Since the graph is connected, \\(E \\geq V - 1\\), so the total running time is \\(O(E \\log E + E + E \\alpha(V)) = O(E \\log E + E \\alpha(V)) = O(E \\log V)\\).\nPrim\u0026rsquo;s Algorithm The second solution starts at an arbitrary vertex in a set \\(A\\) and adds a new vertex to \\(A\\) in a greedy fashion. To efficiently select a new edge to add, Prim\u0026rsquo;s algorithm uses a priority queue to keep track of the lightest edge that crosses the cut. The algorithm terminates when \\(A\\) is a complete tree. The full algorithm is given below. We will step through it in more detail after.\nPrim\u0026rsquo;s Algorithm\nA = {} for each vertex v in G.V key[v] = infinity pi[v] = NIL key[r] = 0 Q = G.V while Q is not empty u = EXTRACT-MIN(Q) A = A union {u} for each vertex v in G.Adj[u] if v in Q and w(u, v) \u0026lt; key[v] pi[v] = u key[v] = w(u, v) You might look at this and wonder how the MST is represented. Prim\u0026rsquo;s algorithm implicitly maintains the set \\(A = \\{(v, v.\\pi) : v \\in V - \\{r\\} - Q\\}\\). When the while loop terminates, \\(A = \\{(v, v.\\pi) : v \\in V - \\{r\\}\\}\\), since the queue is empty. The critical part of this is to understand how the algorith changes the key values.\nA step-by-step example of an implementation in Python is available here.\nAnalysis Prim\u0026rsquo;s algorithm uses a priority queue to keep track of the lightest edge that crosses the cut. If the priority queue is implemented as a min-heap, which has a worst-case running time of \\(O(\\log V)\\) for both EXTRACT-MIN and DECREASE-KEY. The algorithm calls EXTRACT-MIN once for each vertex, which is \\(O(V \\log V)\\). The algorithm calls DECREASE-KEY once for each edge, which is \\(O(E \\log V)\\). The total running time is \\(O(V \\log V + E \\log V) = O(E \\log V)\\).\nReferences Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. Introduction to Algorithms. 4th ed. MIT Press. http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/. ","date":1697864400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697864400,"objectID":"afb8da22c3362523bbd30b8c2f572af6","permalink":"https://ajdillhoff.github.io/notes/minimum_spanning_trees/","publishdate":"2023-10-21T00:00:00-05:00","relpermalink":"/notes/minimum_spanning_trees/","section":"notes","summary":"Table of Contents Definition Finding the Minimum Spanning Tree Kruskal\u0026rsquo;s Algorithm Prim\u0026rsquo;s Algorithm Minimum spanning trees are undirected graphs that connect all of the vertices such that there are no redundant edges and the total weight is minimized. They are useful for finding the shortest path between two points in a graph. Useful application of MSTs include\nnetwork design: it is useful to know the least expensive path with respect to either latency or resource cost for telecommunications networks, transportation networks, or electrical grids.","tags":["computer science","algorithms","graphs","minimum spanning trees"],"title":"Minimum Spanning Trees","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Definition Bellman-Ford Dijkstra\u0026rsquo;s Algorithm When you hear the term shortest path, you may think of the shortest physical distance between your current location and wherever it is you\u0026rsquo;re going. Finding the most optimal route via GPS is one of the most widely used mobile applications. Physical paths are not the only types we may wish to find a shortest path for. Other examples include:\nNetwork Routing: To improve network performance, it is critical to know the shortest path from one system to another in terms of latency. Puzzle Solving: For puzzles such as a Rubik\u0026rsquo;s cube, the vertices could represents states of the cube and edges could correspond to a single move. Robotics: Shortest paths in terms of robotics have a lot to do with physical distances, but it could also relate the completing a task efficiently. These notes will cover classical single-source shortest path algorithms, but first we must formally define the problem.\nDefinition Given a weighted, directed graph \\(G = (V, E)\\) with weight function \\(w: E \\rightarrow \\mathbb{R}\\), a source vertex \\(s \\in V\\), and a destination vertex \\(t \\in V\\), find the shortest path from \\(s\\) to \\(t\\). The weight of a path is defined as the sum of the weights of its edges:\n\\[ w(p) = \\sum_{e \\in p} w(e). \\]\nThe shortest-path weight between two vertices \\(u\\) and \\(v\\) is given by\n\\[ \\delta(u, v) = \\begin{cases} \\min_{p \\in P(u, v)} w(p) \u0026amp; \\text{if } P(u, v) \\neq \\emptyset \\\\ \\infty \u0026amp; \\text{otherwise} \\end{cases} \\]\nwhere \\(P(u, v)\\) is the set of all paths from \\(u\\) to \\(v\\). The shortest-path weight from \\(s\\) to \\(t\\) is given by \\(\\delta(s, t)\\).\nShortest-path algorithms rely on an optimal substructure property that is defined by Lemma 22.1 (Cormen et al. 2022).\nLemma 22.1\nGiven a weighted, directed graph \\(G = (V,E)\\) with weight function \\(w: E \\rightarrow \\mathbb{R}\\), let \\(p = \\langle v_0, v_1, \\dots, v_k \\rangle\\) be a shortest path from vertex \\(v_0\\) to vertex \\(v_k\\). For any \\(i\\) and \\(j\\) such that \\(0 \\leq i \\leq j \\leq k\\), let \\(p_{ij} = \\langle v_i, v_{i+1}, \\dots, v_j \\rangle\\) be the subpath of \\(p\\) from vertex \\(v_i\\) to vertex \\(v_j\\). Then, \\(p_{ij}\\) is a shortest path from \\(v_i\\) to \\(v_j\\).\nIt is also important to note here that a shortest path should contain no cycles. Some shortest-path algorithms require that the edge weights be strictly positive. For those that do not, they may have some mechanism for detecting negative-weight cycles. In any case, a cycle of any kind cannot be included in a shortest path. This is because if a cycle were included, we could simply traverse the cycle as many times as we wanted to reduce the weight of the path. For positive-weight cycles, if a shortest path included a cycle, then surely we could remove the cycle to get a lower weight.\nAs we build a shortest path, we need to keep track of which vertices lead us from the source to the destination. Some algorithms maintain this by keeping a predecessor attribute for each vertex in the path. Solutions such as the Viterbi algorithm keep an array of indices that correspond to the vertices in the path. In any case, we will need to keep track of the vertices in the path as we build it.\nRelaxation There is one more important property to define before discussing specific algorithms: relaxation. Relaxing an edge \\((u, v)\\) is to test whether going through vertex \\(u\\) improves the shortest path to \\(v\\). If so, we update the shortest-path estimate and predecessor of \\(v\\) to reflect the new shortest path. Relaxation requires that we maintain the shortest-path estimate and processor for each vertex. This is initialized as follows.\ndef initialize_single_source(G, s): for v in G.V: v.d = float(\u0026#39;inf\u0026#39;) v.pi = None s.d = 0 When the values are changed, we say that the vertex has been relaxed. Relaxing an edge \\((u, v)\\) is done as follows.\ndef relax(u, v, w): if v.d \u0026gt; u.d + w(u, v): v.d = u.d + w(u, v) v.pi = u Properties Relaxation has the following properties.\nIf the shortest-path estimate of a vertex is not \\(\\infty\\), then it is always an upper bound on the weight of a shortest path from the source to that vertex.\nThe shortest-path estimate of a vertex will either stay the same or decrease as the algorithm progresses.\nOnce a vertex\u0026rsquo;s shortest-path estimate is finalized, it will never change.\nThe shortest-path estimate of a vertex is always greater than or equal to the actual shortest-path weight.\nAfter \\(i\\) iterations of relaxing on all \\((u, v)\\), if the shortest path to \\(v\\) has \\(i\\) edges, then \\(v.d = \\delta(s, v)\\).\nFollowing Introduction to Algorithms, we will first discuss the Bellman-Ford algorithm, which has a higher runtime but works with graphs that have negative edge weights. Then, we will discuss Dijkstra\u0026rsquo;s algorithm, which has a lower runtime but only works with graphs that have non-negative edge weights.\nBellman-Ford The Bellman-Ford algorithm is a dynamic programming algorithm that solves the single-source shortest-paths problem in the general case in which edge weights may be negative. If a negative-weight cycle is reachable from the source, then the algorithm will report its existence. Otherwise, it will report the shortest-path weights and predecessors. It works by relaxing edges, decreasing the shortest-path estimate on the weight of a shortest path from \\(s\\) to each vertex \\(v\\) until it reaches the shortest-path weight.\ndef bellman_ford(G, w, s): initialize_single_source(G, s) for i in range(1, len(G.V)): for (u, v) in G.E: relax(u, v, w) for (u, v) in G.E: if v.d \u0026gt; u.d + w(u, v): return False return True Example In the figure below, graph (a) shows the original graph before iterating over the edges. Graphs (b)-(e) show the result of looping over both edges originating from \\(s\\). Depending on the implementation, the first iteration of the vertices would result directly in graph (c). You can find a Python implementation of this example here.\nFigure 1: Step-by-step execution of Bellman-Ford on a graph with negative-weight edges (Cormen et al. 2022). Analysis Using an adjacency list representation, the runtime of Bellman-Ford is \\(O(V^2 + VE)\\). The initialization takes \\(\\Theta(V)\\). Each of the \\(|V| - 1\\) iterations over the edges takes \\(\\Theta(V + E)\\), and the final check for negative-weight cycles takes \\(\\Theta(V + E)\\). If the number of edges and vertices is such that the number of vertices are a lower bound on the edges, then the runtime is \\(O(VE)\\).\nDijkstra\u0026rsquo;s Algorithm Dijkstra\u0026rsquo;s algorithm also solves the single-source shortest path problem on a weighted, directed graph \\(G = (V,E)\\) but requires nonnegative weights on all edges. It works in a breadth-first manner. A minimum priority queue is utilized to keep track of the vertices that have not been visited based on their current minimum shortest-path estimate. The algorithm works by relaxing edges, decreasing the shortest-path estimate on the weight of a shortest path from \\(s\\) to each vertex \\(v\\) until it reaches the shortest-path weight.\ndef dijkstra(G, w, s): initialize_single_source(G, s) S = [] Q = G.V while Q: u = extract_min(Q) S.append(u) for v in G.adj[u]: prev_d = v.d relax(u, v, w) if v.d \u0026lt; prev_d: decrease_key(Q, v) Example A Python example of the figure below is available here.\nFigure 2: A step-by-step execution of Dijkstra\u0026rsquo;s algorithm on a graph with non-negative edge weights (Cormen et al. 2022). Analysis See Chapter 22 of Introduction to Algorithms for a detailed analysis of Dijkstra\u0026rsquo;s algorithm. Inserting the nodes and then extracting them from the queue yields \\(O(V \\log V)\\). After extracting a node, its edges are iterated with a possible update to the queue. This takes \\(O(E \\log V)\\). The total runtime is \\(O((V + E) \\log V)\\).\nReferences Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. Introduction to Algorithms. 4th ed. MIT Press. http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/. ","date":1697864400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697864400,"objectID":"4d280a181eeff8feebb6a316b1e284c6","permalink":"https://ajdillhoff.github.io/notes/single_source_shortest_paths/","publishdate":"2023-10-21T00:00:00-05:00","relpermalink":"/notes/single_source_shortest_paths/","section":"notes","summary":"Table of Contents Definition Bellman-Ford Dijkstra\u0026rsquo;s Algorithm When you hear the term shortest path, you may think of the shortest physical distance between your current location and wherever it is you\u0026rsquo;re going. Finding the most optimal route via GPS is one of the most widely used mobile applications. Physical paths are not the only types we may wish to find a shortest path for. Other examples include:\nNetwork Routing: To improve network performance, it is critical to know the shortest path from one system to another in terms of latency.","tags":["computer science","algorithms","graphs","shortest path"],"title":"Single-Source Shortest Paths","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents What are Graphs? Graph Traversal Algorithms What are Graphs? A graph is a data structure that is used to represent pairwise relationships between objects. Graphs are used in many applications, such as social networks, maps, and routing algorithms. These notes accompany the series of lectures on graphs for my Foundations of Computing course at the University of Texas - Arlington.\nDefinitions A directed graph \\(G\\) is represented as a pair \\((V, E)\\) of a set of vertices \\(V\\) and edges \\(E\\). Edges are represented as ordered pairs.\nAn undirected graph \\(G\\) is represented as a pair \\((V, E)\\) of a set of vertices \\(V\\) and edges \\(E\\). The edges are represented as unordered pairs, as it does not matter which direction the edge is going.\nLet \\((u, v)\\) be an edge in a graph \\(G\\). If \\(G\\) is a directed graph, then the edge is incident from \\(u\\) and is incident to \\(v\\). In this case, \\(v\\) is also adjacent to \\(u\\). If \\(G\\) is an undirected graph, then the edge is incident on \\(u\\) and \\(v\\). For undirected graphs, the adjacency relation is symmetric.\nThe degree is a graph is the number of edges incident on a vertex. For directed graphs, the in-degree is the number of edges incident to a vertex, and the out-degree is the number of edges incident from a vertex.\nA path from a vertex \\(u\\) to another vertex \\(v\\) is a sequence of edges that starts at \\(u\\) and ends at \\(v\\). This definition can include duplicates. A simple path is a path that does not repeat any vertices. A cycle is a path that starts and ends at the same vertex. If a path exists from \\(u\\) to \\(v\\), then \\(u\\) is reachable from \\(v\\).\nA connected graph is a graph where there is a path between every pair of vertices. A strongly connected graph is a directed graph where there is a path between every pair of vertices. The connected components of a graph are the subgraphs in which each pair of nodes is connected by a path. In image processing, connected-component labeling is used to find regions of connected pixels in a binary image.\nLet \\(G = (V, E)\\) and \\(G\u0026rsquo; = (V\u0026rsquo;, E\u0026rsquo;)\\). \\(G\\) and \\(G\u0026rsquo;\\) are isomorphic if there is a bijection between their vertices such that \\((u, v) \\in E\\) if and only if \\((f(u), f(v)) \\in E\u0026rsquo;\\).\nA complete graph is an undirected graph in which every pair of vertices is adjacent. A bipartite graph is an undirected graph in which the vertices can be partitioned into two sets such that every edge connects a vertex in one set to a vertex in the other set.\nA multi-graph is a graph that allows multiple edges between the same pair of vertices. These are commonly in social network analysis, where multiple edges between two people can represent different types of relationships.\nTODO: Add figures demonstrating the above definitions\nRepresentations Graphs can be represented in many different ways. The most common representations are adjacency lists and adjacency matrices. Adjacency lists are more space-efficient for sparse graphs, while adjacency matrices are more space-efficient for dense graphs. Adjacency lists are also more efficient for finding the neighbors of a vertex, while adjacency matrices are more efficient for checking if an edge exists between two vertices.\nExample: Adjacency Matrix and Reachability Consider the graph in the figure below.\nFigure 1: A directed graph The adjacency matrix for this graph is:\n\\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ \\end{bmatrix}\nThe rows and columns represent the vertices in the graph. The value at row \\(i\\) and column \\(j\\) is 1 if there is an edge from vertex \\(i\\) to vertex \\(j\\). Otherwise, the value is 0. Let \\(A\\) be the adjacency matrix for a graph \\(G\\). The matrix \\(A^k\\) represents the number of paths of length \\(k\\) between each pair of vertices. For example, \\(A^2\\) for the above graph is:\n\\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ \\end{bmatrix}\nThe value at row \\(i\\) and column \\(j\\) is the number of paths of length 2 from vertex \\(i\\) to vertex \\(j\\). For example, is a path from vertex 0 to vertex 6 via 0 -\u0026gt; 3 -\u0026gt; 6.\nGraph Traversal Algorithms Breadth First Search We previously studied breadth-first search in the context of binary search trees. The algorithm is the same when applied on general graphs, but our perspective is slightly different now. The function studied before did not use node coloring. Let\u0026rsquo;s investigate the algorithm given by Cormen et al. in Introduction to Algorithms.\nThe algorithm adds a color to each node to keep track of its state. The colors are:\nWHITE: The node has not been discovered yet. GRAY: The node has been discovered, but not all of its neighbors have been discovered. BLACK: The node has been discovered, and all of its neighbors have been discovered. First, every vertex is painted white and the distance is set to \\(\\infty\\). The first node s is immediately set to have 0 distance. The queue then starts with s. While there are any grey vertices, dequeue the next available node and add its adjacent vertices to the queue. The distance of each adjacent vertex is set to the distance of the current vertex plus one. Once all of its neighbors have been discovered, the current vertex is painted black.\nThe algorithm and an example run from Cormen et al. are shown below.\ndef bfs(G, s): for u in G.V: u.color = WHITE u.d = inf u.pi = None s.color = GRAY s.d = 0 s.pi = None Q = Queue() Q.enqueue(s) while not Q.empty(): u = Q.dequeue() for v in G.adj[u]: if v.color == WHITE: v.color = GRAY v.d = u.d + 1 v.pi = u Q.enqueue(v) u.color = BLACK Figure 2: Breadth First Search from Cormen et al. Analysis The running time of BFS is \\(O(V + E)\\), where \\(V\\) is the number of vertices and \\(E\\) is the number of edges. Each vertex is queued and dequeued once, so the queue operations take \\(O(V)\\) time. Each edge is examined once, so the edge operations take \\(O(E)\\) time. The total running time is \\(O(V + E)\\).\nDepth First Search Like the BFS algorithm presented in Introduction to Algorithms by Cormen et al., the DFS algorithm also uses colors to keep track of the state of each node. The colors are similar to the BFS algorithm, but the meaning is slightly different:\nWHITE: The node has not been discovered yet. GRAY: The node has been visited for the first time. BLACK: The adjacency list of the node has been examined completely. First, all vertices are colored white. The time is set to 0. The function dfs_visit is called on each vertex. The function is defined as follows:\ndef dfs_visit(G, u): time += 1 u.d = time u.color = GRAY for v in G.adj[u]: if v.color == WHITE: v.pi = u dfs_visit(G, v) u.color = BLACK time += 1 u.f = time When a node is discovered via dfs_visit, the time is recorded and the color is changed to gray. The start and finish times are useful in understanding the structure of the graph. After all of the node\u0026rsquo;s neighbors have been discovered, the color is changed to black and the finish time is recorded. That is, the depth from the current node must be fully explored before it is considered finished. The figure below shows the progress of DFS on a directed graph.\nFigure 3: Depth First Search from Cormen et al. ","date":1697518800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697518800,"objectID":"6602c9f998120efe72b5ce6f5f8f62d1","permalink":"https://ajdillhoff.github.io/notes/introduction_to_graph_theory/","publishdate":"2023-10-17T00:00:00-05:00","relpermalink":"/notes/introduction_to_graph_theory/","section":"notes","summary":"Table of Contents What are Graphs? Graph Traversal Algorithms What are Graphs? A graph is a data structure that is used to represent pairwise relationships between objects. Graphs are used in many applications, such as social networks, maps, and routing algorithms. These notes accompany the series of lectures on graphs for my Foundations of Computing course at the University of Texas - Arlington.\nDefinitions A directed graph \\(G\\) is represented as a pair \\((V, E)\\) of a set of vertices \\(V\\) and edges \\(E\\).","tags":["computer science","data structures"],"title":"Introduction to Graph Theory","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Definition Operations Exercises Red-Black Trees are modified Binary Search Trees that maintain a balanced structure in order to guarantee that operations like search, insert, and delete run in \\(O(\\log n)\\) time.\nDefinition A red-black tree is a binary search tree with the following properties:\nEvery node is either red or black. The root is black. Every NULL leaf is black. If a node is red, then both its children are black. For each node, all simple paths from the node to descendant leaves contain the same number of black nodes. Figure 1: Red-Black Tree from CLRS Chapter 13. The only structural addition we need to make over a BST is the addition of a color attribute to each node. This attribute can be either RED or BLACK.\nProperty 5 implies that the black-height of a tree is an important property. This property is used to prove that the height of a red-black tree with \\(n\\) internal nodes is at most \\(2 \\log(n + 1)\\).\nOperations Rotate If a is balanced, then searching for a node takes \\(O(\\log n)\\) time. However, if the tree is unbalanced, then searching can take \\(O(n)\\) time. When items are inserted or deleted from a tree, it can become unbalanced. Without any way to correct for this, a BST is less desirable unless the data will not change.\nWhen nodes are inserted or deleted into a red-black tree, the rotation operation is used in functions that maintain the red-black properties. This ensures that the tree remains balanced and that operations like search, insert, and delete run in \\(O(\\log n)\\) time. The figure below shows the two types of rotations that can be performed on a red-black tree.\nFigure 2: Rotations in a red-black tree (CLRS Figure 13.2). Python implementations of both left and right rotations are given below.\ndef left_rotate(self, x): y = x.right x.right = y.left if y.left != self.nil: y.left.p = x y.p = x.p if x.p == self.nil: self.root = y elif x == x.p.left: x.p.left = y else: x.p.right = y y.left = x x.p = y def right_rotate(self, y): x = y.left y.left = x.right if x.right != self.nil: x.right.p = y x.p = y.p if y.p == self.nil: self.root = x elif y == y.p.left: y.p.left = x else: y.p.right = x x.right = y y.p = x Cormen et al. Figure 13.3 (below) shows the result of performing a left rotation on node \\(x\\).\nFigure 3: Left rotation on node (x) (CLRS Figure 13.3). A rotation only changes pointer assignments, so it takes \\(O(1)\\) time.\nInsert The insert operation in a red-black tree starts off identically to the insert operation in a BST. The new node is inserted into the tree as a leaf node. Since the NULL leaf nodes must be black by definition, the added node is colored red. The function in Python is shown below.\ndef insert(self, z): y = self.nil x = self.root while x != self.nil: y = x if z.key \u0026lt; x.key: x = x.left else: x = x.right z.p = y if y == self.nil: self.root = z elif z.key \u0026lt; y.key: y.left = z else: y.right = z z.left = self.nil z.right = self.nil z.color = RED self.insert_fixup(z) By adding the node and setting its color to red, we have possibly violated properties 2 and 4. Property 2 is violated if z is the root. Property 4 is violated if the parent of the new node is also red. The final line of the function calls insert_fixup to restore the red-black properties. It is defined as follows.\ndef insert_fixup(self, z): while z.p.color == RED: if z.p == z.p.p.left: y = z.p.p.right if y.color == RED: z.p.color = BLACK y.color = BLACK z.p.p.color = RED z = z.p.p else: if z == z.p.right: z = z.p self.left_rotate(z) z.p.color = BLACK z.p.p.color = RED self.right_rotate(z.p.p) else: y = z.p.p.left if y.color == RED: z.p.color = BLACK y.color = BLACK z.p.p.color = RED z = z.p.p else: if z == z.p.left: z = z.p self.right_rotate(z) z.p.color = BLACK z.p.p.color = RED self.left_rotate(z.p.p) self.root.color = BLACK Case 1 Inside the while loop, the first and second conditions are symmetric. One considers the case where z\u0026rsquo;s parent is a left child, and the other considers the case where z\u0026rsquo;s parent is a right child. Further, if z\u0026rsquo;s parent is a left child, then we start by setting y to z\u0026rsquo;s aunt. Let\u0026rsquo;s investigate the first if statement, where y is RED. In this case, both z\u0026rsquo;s parent and aunt are RED. We can fix this by setting both to BLACK and setting z\u0026rsquo;s grandparent to RED. This may violate property 2, so we set z to its grandparent and repeat the loop.\nif y.color == RED: z.p.color = BLACK y.color = BLACK z.p.p.color = RED z = z.p.p Case 2 If y is BLACK, then we need to consider the case where z is a right child. In this case, we set z to its parent and perform a left rotation. This automatically results in the third case, where z is a left child.\nif z == z.p.right: z = z.p self.left_rotate(z) Case 3 If z is a left child, then we set z\u0026rsquo;s parent to BLACK and its grandparent to RED. Then we perform a right rotation on the grandparent.\nz.p.color = BLACK z.p.p.color = RED self.right_rotate(z.p.p) Figure 13.4 from Cormen et al. demonstrates the addition of a node to a red-black tree. The node is inserted as a leaf node and colored red. Then insert_fixup is called to restore the red-black properties.\nFigure 4: Inserting a node into a red-black tree (CLRS Figure 13.4). The insert operation takes \\(O(\\log n)\\) time since it performs a constant number of rotations.\nDelete Like the delete operation of a BST, the delete operation of a RBT uses a transplant operation to replace the deleted node with its child. The transplant operation is defined as follows.\ndef transplant(self, u, v): if u.p == self.nil: self.root = v elif u == u.p.left: u.p.left = v else: u.p.right = v v.p = u.p The full delete operation follows a similar structure to that of its BST counterpart. There are a few distinct differences based on the color of the node being deleted. The function begins as follows.\ndef delete(self, z): y = z y_original_color = y.color The first line sets y to the node to be deleted. The second line saves the color of y. This is necessary because y will be replaced by another node, and we need to know the color of the replacement node. The first two conditionals check if z has any children. If there is right child, then the z is replaced by the left child. If there is a left child, then z is replaced by the right child. If z has no children, then z is replaced by NULL.\nif z.left == None: x = z.right self.transplant(z, z.right) elif z.right == None: x = z.left self.transplant(z, z.left) If z has two children, then we find the successor of z and set y to it. The successor is the node with the smallest key in the right subtree of z. The successor is guaranteed to have at most one child, so we can use the code above to replace y with its child. Then we replace z with y.\nelse: y = self.minimum(z.right) y_original_color = y.color x = y.right if y != z.right: # y is farther down the tree self.transplant(y, y.right) y.right = z.right y.right.p = y else: x.p = y self.transplant(z, y) y.left = z.left y.left.p = y y.color = z.color The procedure kept track of y_original_color to see if any violations occurred. This would happen if y was originally BLACK because the transplant operation, or the deletion itself, could have violated the red-black properties. If y_original_color is BLACK, then we call delete_fixup to restore the properties.\nDelete Fixup If the node being deleted is BLACK, then the following scenarios can occur. If y is the root and a RED child of y becomes the new root, property 2 is violated. Let x be a RED child of y, if a new parent of x is RED, then property 4 is violated. Lastly, removing y may have caused a violation of property 5, since any path containing y has 1 less BLACK node in it.\nCorrecting violation 5 can be done by transferring the BLACK property from y to x, the node that moves into y\u0026rsquo;s original position. This requires us to allow nodes to take on multiple counts of colors. That is, if x was already BLACK, it becomes double BLACK. If it was RED, it becomes RED-AND-BLACK. There is a good reason to this extension, as it will help us decide which case of delete_fixup to use.\nThe delete_fixup function will restore violations of properties 1, 2, and 4. It is called after the delete operation, and it takes a single argument, x, which is the node that replaced the deleted node. It performs a series of rotations and color changes to restore the violated properties.\nLet\u0026rsquo;s look at the delete_fixup function from the ground up. It is a little more complex than insert_fixup because it has to handle the case where the node being deleted is BLACK. In total, there are 4 distinct cases per side. Like insert_fixup, it is enough to understand the first half, as the second is symmetric. The function begins as follows, where x is a left child.\nCase 1 def delete_fixup(self, x): while x != self.root and x.color == BLACK: if x == x.p.left: w = x.p.right if w.color == RED: w.color = BLACK x.p.color = RED self.left_rotate(x.p) w = x.p.right In the first case, x\u0026rsquo;s sibling w is RED. If this is true, then w must have two BLACK subnodes. The colors of w and x\u0026rsquo;s parent are then switched, and a left rotation is performed on x\u0026rsquo;s parent. The result of case 1 converts to one of cases 2, 3, or 4. The figure below shows the result of the first case.\nFigure 5: Case 1 of delete_fixup (CLRS Figure 13.7). Case 2 if w.left.color == BLACK and w.right.color == BLACK: w.color = RED x = x.p If both of w\u0026rsquo;s subnodes are BLACK and both w and x are also black (actually, x is doubly BLACK), then there is an extra BLACK node on the path from w to the leaves. The colors of both x and w are switched, which leaves x with a single BLACK count and w as RED. The extra BLACK property is transferred to x\u0026rsquo;s parent. The figure below shows the result of the second case.\nFigure 6: Case 2 of delete_fixup (CLRS Figure 13.7). Case 3 else: if w.right.color == BLACK: w.left.color = BLACK w.color = RED self.right_rotate(w) w = x.p.right If w is BLACK, its left child is RED, and its right child is BLACK, then the colors of w and its left child are switched. Then a right rotation is performed on w. This rotation moves the BLACK node to w\u0026rsquo;s position, which is now the new sibling of x. This leads directly to case 4. A visualization of case 3 is shown below.\nFigure 7: Case 3 of delete_fixup (CLRS Figure 13.7). Case 4 w.color = x.p.color x.p.color = BLACK w.right.color = BLACK self.left_rotate(x.p) x = self.root At this point, w is BLACK and its right child is RED. Also remember that x still holds an extra BLACK count. This last case performs color changes and a left rotation which remedy the extra BLACK count. The figure below shows the result of case 4.\nFigure 8: Case 4 of delete_fixup (CLRS Figure 13.7). The full delete_fixup function is shown below.\ndef delete_fixup(self, x): while x != self.root and x.color == BLACK: if x == x.p.left: w = x.p.right if w.color == RED: w.color = BLACK x.p.color = RED self.left_rotate(x.p) w = x.p.right if w.left.color == BLACK and w.right.color == BLACK: w.color = RED x = x.p else: if w.right.color == BLACK: w.left.color = BLACK w.color = RED self.right_rotate(w) w = x.p.right w.color = x.p.color x.p.color = BLACK w.right.color = BLACK self.left_rotate(x.p) x = self.root else: w = x.p.left if w.color == RED: w.color = BLACK x.p.color = RED self.right_rotate(x.p) w = x.p.left if w.right.color == BLACK and w.left.color == BLACK: w.color = RED x = x.p else: if w.left.color == BLACK: w.right.color = BLACK w.color = RED self.left_rotate(w) w = x.p.left w.color = x.p.color x.p.color = BLACK w.left.color = BLACK self.right_rotate(x.p) x = self.root x.color = BLACK The delete operation takes \\(O(\\log n)\\) time since it performs a constant number of rotations. The delete_fixup operation also takes \\(O(\\log n)\\) time since it performs a constant number of color changes and at most 3 rotations. Case 2 of delete_fixup could move the violation up the tree, but this would happen no more than \\(O(\\log n)\\) times. In total, the delete operation takes \\(O(\\log n)\\) time.\nExercises Create a red-black tree class in Python that supports the operations discussed in these notes. Using the created class from exercise 1, implement a Hash Map class that uses a red-black tree for collision resolution via chaining. ","date":1697346000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697346000,"objectID":"48726e9a212db00c135bdb0404fea1cc","permalink":"https://ajdillhoff.github.io/notes/red_black_trees/","publishdate":"2023-10-15T00:00:00-05:00","relpermalink":"/notes/red_black_trees/","section":"notes","summary":"Table of Contents Definition Operations Exercises Red-Black Trees are modified Binary Search Trees that maintain a balanced structure in order to guarantee that operations like search, insert, and delete run in \\(O(\\log n)\\) time.\nDefinition A red-black tree is a binary search tree with the following properties:\nEvery node is either red or black. The root is black. Every NULL leaf is black. If a node is red, then both its children are black.","tags":["computer science","data structures"],"title":"Red-Black Trees","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Binary Search Trees Operations Analysis A $n$-ary tree is a graph-based data structure in which each node has up to \\(n\\) subnodes. It is supported by the following operations (not exclusive):\nSearch Insert Delete Tree-based data structures are defined by the following properties.\nThe size of a tree \\(T\\) is determined by the total number of nodes in \\(T\\). The root of a tree \\(T\\) is the starting point of \\(T\\). A leaf node of a tree \\(T\\) is a node that has no sub-nodes. The height of a tree is determined by the length of the shortest path between the root of \\(T\\) and the lowest leaf node of \\(T\\). If we limit the number of subnodes each node may have to 2, the structure becomes known as a binary tree. Limiting the structure in this way is of interest to us because of the efficiency benefits seen in operations applied to binary trees. If we narrow the scope of these trees further, we can define a binary search tree whose search operation, as the name might suggest, runs in \\(\\Theta(lg n)\\) worst-case time.\nBinary Search Trees A binary search tree is a regular binary tree with references to the left, right, and parent nodes, defined by the following property:\nLet \\(x\\) be a node in a binary search tree. If \\(y\\) is a node in the left subtree of \\(x\\), then \\(y.key \\leq x.key\\). If \\(y\\) is a node in the right subtree of \\(x\\), then \\(y.key \\geq x.key\\).\nUnder this definition, operations such as search, insert, and delete can be performed in \\(\\Theta(lg n)\\) worst-case time assuming that the tree is balanced. Later, we will explore a variant of the binary search tree that guarantees a balanced tree.\nA tree node implemented in Python might look like this:\nclass Node: def __init__(self, key): self.key = key self.left = None self.right = None self.parent = None Operations Traversals Like any other graph-based structure, a tree can be traversed using either depth-first or breadth-first search. Only an inorder depth-first search is of interest for a binary search tree, as we will see below. Consider the given tree in the figure below. Performing an inorder traversal on this tree yields the keys in sorted order from smallest to largest.\ndef inorder_tree_walk(x): if x is not None: inorder_tree_walk(x.left) print(x.key) inorder_tree_walk(x.right) Traversing the entire tree takes \\(\\Theta(n)\\) time, as each node must be visited once. Searching a tree, however, only takes \\(\\Theta(lg n)\\) time. The search algorithm is defined recursively as follows:\ndef tree_search(x, k): if x is None or k == x.key: return x if k \u0026lt; x.key: return tree_search(x.left, k) else: return tree_search(x.right, k) Consider the balanced tree shown in the figure below. If we search for the key 15, notice that after the first comparison with the root, the search space goes from 15 nodes to 7 nodes. After the second comparison, the search space goes from 7 nodes to 3 nodes. After the third comparison, the search space goes from 3 nodes to 1 node. This is the essence of binary search, and it is why the search operation runs in \\(\\Theta(lg n)\\) time.\nFigure 1: A balanced binary search tree Minimum In a BST, the minimum value is the leftmost node. Finding the minimum is as easy as traversing down the left branch until a leaf node is reached.\ndef tree_minimum(x): while x.left is not None: x = x.left return x Maximum In a BST, the maximum value is the rightmost node. Finding the maximum is as easy as traversing down the right branch until a leaf node is reached.\ndef tree_maximum(x): while x.right is not None: x = x.right return x Successor The successor and predecessor operations are useful for the delete operation defined below. The successor of a node \\(x\\) is the node with the smallest key greater than \\(x.key\\). If \\(x\\) has a right subtree, then the successor of \\(x\\) is the minimum of the right subtree. If \\(x\\) has no right subtree, then the successor of \\(x\\) is the lowest ancestor of \\(x\\) whose left child is also an ancestor of \\(x\\).\ndef tree_successor(x): if x.right is not None: return tree_minimum(x.right) y = x.parent while y is not None and x == y.right: x = y y = y.parent return y Predecessor The predecessor of a node \\(x\\) is the node with the largest key less than \\(x.key\\). If \\(x\\) has a left subtree, then the predecessor of \\(x\\) is the maximum of the left subtree. If \\(x\\) has no left subtree, then the predecessor of \\(x\\) is the lowest ancestor of \\(x\\) whose right child is also an ancestor of \\(x\\).\ndef tree_predecessor(x): if x.left is not None: return tree_maximum(x.left) y = x.parent while y is not None and x == y.left: x = y y = y.parent return y Insert Inserting an item into a binary search tree follows the same logic as traversal. Starting at the root, the key is compared to see if it is greater than the root\u0026rsquo;s key. If so, recursively traverse down the right branch. If not, recursively traverse down the left branch. This process continues until a leaf node is reached, at which point the new node is inserted as a child of the leaf node.\nThis process will not necessarily result in a balanced tree. In fact, if the keys are inserted in sorted order, the tree will be a linked list. This is the worst-case scenario for a binary search tree, as the search operation will then run in \\(\\Theta(n)\\) time.\nThe full algorithm is given below.\ndef tree_insert(T, z): y = None x = T.root while x is not None: y = x if z.key \u0026lt; x.key: x = x.left else: x = x.right z.parent = y if y is None: T.root = z elif z.key \u0026lt; y.key: y.left = z else: y.right = z Delete Deleting a node is not a straightforward as insert. Depending on the structure of the subtree, one of three cases must be considered.\nIf \\(z\\) has no subnodes, simply remove \\(z\\) from the tree. If \\(z\\) has one subnode, replace \\(z\\) with its subnode. If \\(z\\) has two subnodes, replace \\(z\\) with its successor. It is a bit more complicated than this, as we explore below. In case 3, node \\(z\\) has both a left and right subnode. The first step is to find the successor of \\(z\\), \\(y\\). Since \\(z\\) has 2 subnodes, its successor has no left subnode (convince yourself of this). Likewise, its predecessor has no right subnode. If \\(y\\) is the right subnode of \\(z\\), replace \\(z\\) by \\(y\\).\nIf \\(y\\) is not the right subnode of \\(z\\), it is somewhere further down the right branch. In this case, replace \\(y\\) by its right subnode before replacing \\(z\\) by \\(y\\). The figure below shows the removal of node 12 from the tree in the figure above.\nFigure 2: Deleting node 12 from the tree Even though only 1 node was moved (13 to 12\u0026rsquo;s old position), the process of deleting a node actually involves transplanting a subtree to a new position. This is defined algorithmically as follows:\ndef transplant(T, u, v): if u.parent is None: T.root = v elif u == u.parent.left: u.parent.left = v else: u.parent.right = v if v is not None: v.parent = u.parent In the code above, u is the node to be replaced, and v is the node to replace it. Updating v\u0026rsquo;s left and right subnodes are done in the calling function tree_delete, as seen below.\ndef tree_delete(T, z): if z.left is None: # Case 1 and 2 transplant(T, z, z.right) elif z.right is None: # Also case 1 and 2 transplant(T, z, z.left) else: # Case 3 y = tree_minimum(z.right) # get successor if y != z.right: transplant(T, y, y.right) y.right = z.right y.right.parent = y transplant(T, z, y) y.left = z.left y.left.parent = y Analysis Insert, delete, and search all run in \\(\\Theta(h)\\) time, where \\(h\\) is the height of the tree. If the tree is balanced, \\(h = \\Theta(lg n)\\), and all operations run in \\(\\Theta(lg n)\\) time. If the tree is not balanced, \\(h = \\Theta(n)\\), and all operations run in \\(\\Theta(n)\\) time.\n","date":1696914000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696914000,"objectID":"ced91f60d0a798ea462f41c5e80c4bc3","permalink":"https://ajdillhoff.github.io/notes/binary_search_trees/","publishdate":"2023-10-10T00:00:00-05:00","relpermalink":"/notes/binary_search_trees/","section":"notes","summary":"Table of Contents Binary Search Trees Operations Analysis A $n$-ary tree is a graph-based data structure in which each node has up to \\(n\\) subnodes. It is supported by the following operations (not exclusive):\nSearch Insert Delete Tree-based data structures are defined by the following properties.\nThe size of a tree \\(T\\) is determined by the total number of nodes in \\(T\\). The root of a tree \\(T\\) is the starting point of \\(T\\).","tags":["computer science","data structures","algorithms"],"title":"Binary Search Trees","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction to Data Structures Review: Pointers Arrays Matrices Multi-Dimensional Arrays Stacks Queues Introduction to Data Structures Data structures are fundamental concepts in computer science that allow us to organize and store data in a way that enables efficient access and modification. They are essential building blocks for creating efficient and sophisticated computer programs and databases. Different types of data structures include arrays, linked lists, stacks, queues, trees, graphs, and many more, each serving a specific purpose and suited to specific applications.\nUnderstanding data structures is therefore important because they are used in almost every software application. For instance, social media applications use data structures to store user information and their connections, while search engines use them to index and retrieve web pages quickly. The choice of the right data structure significantly impacts the performance, scalability, and resource utilization of software applications.\nHaving a strong grasp of data structures and their properties is critical for anyone working with software or data in general. Through studying the benefits and limitations of each data structure, you will be equipped to analyze the efficacy of existing systems as well as make the right choice when developing new ones.\nWhy Do We Need So Many? There are many data structures available because no single dataset works best for all cases. Each data structure has its unique characteristics, advantages, and disadvantages. These differences can often be evaluated quantitatively, providing rigorous backing when selecting the appropriate one for the task at hand.\nFor example, arrays are excellent when the size of the data is known and constant, but they are not efficient when it comes to frequent insertions and deletions. Linked lists, on the other hand, allow for efficient insertions and deletions but are not as quick as arrays when it comes to accessing elements. Trees are invaluable when we need to maintain a sorted list of data and perform quick searches, insertions, and deletions, while hash tables are optimal for scenarios where we need to perform fast lookups.\nReview: Pointers What are Pointers? Pointers are variables in programming that store the memory address of another variable. They are a powerful feature in many programming languages, including C and C++, allowing programmers to directly access memory locations and manipulate data efficiently. Pointers are crucial for implementing dynamic data structures like linked lists, trees, and graphs.\nIn Python, pointers are not exposed explicitly as in languages like C, but references, which are similar to pointers, are used to hold the memory address of objects. Understanding the concept of pointers and references is essential for managing memory effectively and avoiding issues like memory leaks and dangling pointers in languages that allow direct memory manipulation. Even if we are not dealing with pointers directly, studying them is beneficial for understanding algorithms and data structures in general.\nHow are They Represented? In languages like C, pointers are represented using the asterisk (*) symbol, and the address operator (\u0026amp;) is used to retrieve the memory address of a variable. For example, int *p; declares a pointer to an integer, and p = \u0026amp;x; assigns the address of the variable x to the pointer p.\nIn Python, pointers are not explicitly represented, but references to objects are used to achieve similar functionality. For instance, when a list is assigned to a new variable, the new variable holds a reference to the same list object, not a copy of the list. Any modifications made through one variable are reflected in the other.\nArrays How are Arrays Represented in Memory? Arrays are fundamental data structures that store elements of the same type in contiguous memory locations. The elements can be accessed randomly by indexing into the array. In memory, an array is represented as a block of memory cells, each holding an element of the array placed side by side. The size of each cell is determined by the size of the array\u0026rsquo;s element type.\nThe base address of the array is the memory address of the first element (index 0), and it is used, along with the index and the size of each element, to calculate the address of any element in the array. For example, if the base address is `B`, the size of each element is `S`, and the index of the element is `i`, the address of the element can be calculated as `B + (i * S)`.\nFigure 1: Memory layout of an integer array of size 8. How Many Bytes Does Each Element Use? The number of bytes used by each element in an array depends on the data type of the elements. For example, in most systems, an int uses 4 bytes, a char uses 1 byte, and a double uses 8 bytes. When an array is declared, the total memory allocated for the array is the product of the number of elements and the size of each element.\nIn Python, the sys module can be used to find the size of an object in bytes. However, Python’s dynamic typing and object-oriented nature mean that the size of an array element can vary significantly, as each element is an object that can have additional overhead and can hold references to other objects.\nSince Python does not expose pointers explicitly, we can safely program efficient programs without worrying about making common mistakes related to pointers and memory management.\nHow are Arrays Indexed? Arrays are indexed using a zero-based indexing system, where the first element is accessed using index 0, the second element with index 1, and so on. To access an element at a specific index, the address of the element is calculated using the base address of the array, the size of each element, and the index of the element.\nIn languages that support pointers, the address of an element in an array can be calculated using pointer arithmetic. If p is a pointer to the base address of the array, and i is the index of the element, the address of the element can be calculated as p + i, where i is automatically scaled by the size of the array\u0026rsquo;s element type.\nMatrices Fixed-sized Arrays vs. Ragged Arrays Matrices are two-dimensional arrays that can be represented using fixed-sized arrays or ragged arrays. A fixed-sized array is a regular matrix where each row has the same number of columns, and it is represented in memory as a contiguous block. It allows for efficient access to elements using row and column indices but can waste memory if the matrix is sparse.\nA ragged array, on the other hand, is an irregular matrix where each row can have a different number of columns. It is represented using an array of pointers, where each pointer points to a one-dimensional array representing a row of the matrix. Ragged arrays are more memory-efficient for sparse matrices but can be more complex to manage and traverse.\nChoosing between fixed-sized and ragged arrays depends on the requirements of the application, the characteristics of the matrix, and the trade-offs between memory efficiency and complexity. Understanding the differences between the two representations is crucial for implementing matrices effectively and optimizing memory usage.\nFlat Indexing, Back and Forth Flat indexing is a technique used to represent a two-dimensional array or matrix using a one-dimensional array. In this representation, the elements of the matrix are stored in a single array in row-major or column-major order, and the two-dimensional indices (row and column) are mapped to a single index in the one-dimensional array.\nFor a matrix with M rows and N columns, the mapping from two-dimensional indices to a one-dimensional index in row-major order is done using the formula index = (row * N) + column, and in column-major order using the formula index = (column * M) + row. Flat indexing allows for efficient memory utilization and easy serialization of matrices but requires conversion between one-dimensional and two-dimensional indices.\nPython Example of Matrices In Python, matrices can be represented using lists of lists, where each inner list represents a row of the matrix. Elements can be accessed and modified using two indices, one for the row and one for the column. For example, to create a 2x3 matrix and access the element in the second row and third column, you can do the following:\nmatrix = [[1, 2, 3], [4, 5, 6]] element = matrix[1][2] # access value 6 NumPy also provides support for multi-dimensional arrays and matrices along with a host of functions to perform operations on them. Using Numpy, you can create a matrix and access its elements as follows:\nimport numpy as np matrix = np.array([[1, 2, 3], [4, 5, 6]]) element = matrix[1, 2] # access value 6 Multi-Dimensional Arrays Introduction to Multi-Dimensional Arrays Multi-dimensional arrays are arrays of more than 1 dimension. They are used to represent complex data structures like matrices, tensors, and tables. They are crucial for various applications, including scientific computing, image processing, and machine learning, where data is often multi-dimensional.\nIn a multi-dimensional array, each element is identified by multiple indices, one for each dimension of the array. For example, in a two-dimensional array representing a matrix, each element is identified by two indices representing the row and column of the element. The number of dimensions and the size of each dimension determine the structure and capacity of the array.\nDifferent languages have different approaches to handling multi-dimensional arrays. C, for example, arranges the memory of any array contiguously. Java and Python use jagged arrays, where the row lengths can differ in size. The data in each row might be contiguous, but the entire array is not.\nNumPy Arrays and Their Representation A NumPy array is represented in memory as a contiguous block, and it allows for efficient access and manipulation of elements using multiple indices. The shape of the array, represented as a tuple of integers, determines the number of dimensions and the size of each dimension of the array.\nTo determine how to index the contiguous stream of values represented as an $n$-dimensional array, each np.array specifies the strides for each dimension. Consider the following array with shape (2, 3, 4) and data type int32:\nimport numpy as np x = np.array([[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]], [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]], dtype=np.int32) The strides of the array are (48, 16, 4), which means that to move to the next depth, we need to move 48 bytes, to move to the next row, we need to move 16 bytes, and to move to the next column, we need to move 4 bytes. The strides are calculated based on the size of each element and the shape of the array.\nPython Example with NumPy Here’s an example of how to create a two-dimensional array (matrix) using Numpy and how to access its elements:\nimport numpy as np # Creating a 2x3 Numpy array array = np.array([[1, 2, 3], [4, 5, 6]]) # Accessing the element in the second row and third column element = array[1, 2] # element will be 6 NumPy also provides various functions to perform operations on arrays, such as reshaping, transposing, and aggregating. For example, to calculate the sum of all elements in the array, you can use the np.sum function:\ntotal = np.sum(array) # total will be 21 Stacks How are Stacks Represented with an Array? A stack is a linear data structure that follows the Last In First Out (LIFO) principle, meaning the last element added to the stack is the first one to be removed. Stacks can be easily implemented using arrays, where elements are added and removed from one end of the array, known as the top of the stack.\nWhen representing a stack with an array, one must keep track of the index of the top of the stack. Elements are added to the stack by placing them at the position indicated by the top index and then incrementing the top index. Elements are removed from the stack by decrementing the top index and then accessing the element at that position.\nDifference Between Using the Beginning or the End of the Array as the Top When implementing a stack using an array, one can choose to use either the beginning or the end of the array as the top of the stack. The choice affects the implementation of the push and pop operations and the way the top index is managed.\nIf the beginning of the array is used as the top, elements are added and removed from the first position of the array, and the other elements must be shifted to make room or fill the gap. This can lead to higher time complexity for push and pop operations. If the end of the array is used as the top, elements are added and removed from the last position of the array, allowing for constant-time push and pop operations without the need to shift other elements.\nPython Example of Stack with Array Here’s an example of how to implement a stack using a Python list, with the end of the list as the top of the stack:\nstack = [] # Initializing an empty stack stack.append(1) # Pushing an element onto the stack stack.append(2) # Pushing another element onto the stack top_element = stack.pop() # Popping the top element from the stack, top_element will be 2 This implementation allows for efficient and simple push and pop operations, with constant time complexity. However, the size of the stack is limited by the available memory, and care must be taken to handle underflow and overflow conditions.\nUnderstanding how to implement and use stacks is crucial for solving problems that involve reversing, balancing, or processing data in a LIFO manner. Stacks are a versatile and fundamental data structure used in various applications, including expression evaluation, syntax parsing, and undo mechanisms.\nQueues How are Queues Represented with an Array? A queue is a linear data structure that follows the First In First Out (FIFO) principle, meaning the first element added to the queue is the first one to be removed. Queues can be implemented using arrays, where elements are added at the rear and removed from the front.\nWhen representing a queue with an array, two indices are maintained, one for the front and one for the rear of the queue. Elements are enqueued by placing them at the position indicated by the rear index and then incrementing the rear index. Elements are dequeued by accessing the element at the front index and then incrementing the front index.\nDownside to Using One Side of the Array as the Front and the Other as the Rear When implementing a queue using an array, using one side of the array as the front and the other as the rear can lead to inefficient use of space. Once elements are dequeued from the front, the space they occupied cannot be reused, and overflow can occur even if there is free space at the front of the array.\nTo overcome this limitation, a circular queue can be implemented, where the front and rear indices wrap around to the beginning of the array when they reach the end. This allows for efficient use of space and avoids overflow as long as there is free space in the array. However, it requires more complex index management and can be harder to implement correctly.\nMore Efficient Approach by Using a Reference to a Head and Tail A more efficient approach to implementing a queue is to use a linked list, where each element holds a reference to the next element in the queue. This allows for dynamic resizing of the queue and efficient enqueue and dequeue operations, without the need for complex index management or wasted space.\nIn this approach, two pointers are maintained, one for the head (front) and one for the tail (rear) of the queue. Elements are enqueued by adding them at the position pointed to by the tail pointer and updating the tail pointer to the new element. Elements are dequeued by accessing the element pointed to by the head pointer and updating the head pointer to the next element.\nThis same approach can be done by using indices for the head and tail. The data itself is \u0026ldquo;circular\u0026rdquo; in the sense that the indices wrap around to the beginning of the array when they reach the end. This allows for efficient use of space and avoids overflow as long as there is free space in the array.\nPython Example of a Queue using an Array Here’s an example of how to implement a simple queue using a Python list, with the front at the beginning of the list and the rear at the end of the list:\nfrom collections import deque queue = deque() # Initializing an empty queue queue.append(1) # Enqueueing an element queue.append(2) # Enqueueing another element front_element = queue.popleft() # Dequeueing the front element from the queue, front_element will be 1 This implementation uses the deque class from the collections module, which allows for efficient appending and popping from both ends of the list. It provides a simple and versatile way to implement a queue in Python, with dynamic resizing and constant-time enqueue and dequeue operations.\nUnderstanding how to implement and use queues is essential for solving problems that involve processing data in a FIFO manner. Queues are a fundamental and versatile data structure used in various applications, including task scheduling, order processing, and breadth-first search.\n","date":1696136400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696136400,"objectID":"c43adb26bf0e538cd185ff8baec88e1f","permalink":"https://ajdillhoff.github.io/notes/introduction_to_data_structures/","publishdate":"2023-10-01T00:00:00-05:00","relpermalink":"/notes/introduction_to_data_structures/","section":"notes","summary":"Table of Contents Introduction to Data Structures Review: Pointers Arrays Matrices Multi-Dimensional Arrays Stacks Queues Introduction to Data Structures Data structures are fundamental concepts in computer science that allow us to organize and store data in a way that enables efficient access and modification. They are essential building blocks for creating efficient and sophisticated computer programs and databases. Different types of data structures include arrays, linked lists, stacks, queues, trees, graphs, and many more, each serving a specific purpose and suited to specific applications.","tags":["computer science","data structures"],"title":"Introduction to Data Structures","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Singly-Linked Lists Doubly-Linked Lists Operations Exercises A linked list is a dynamic and aggregate data structure made up of a collection of nodes. The nodes of a linked list can store any data type and are not enforced to contain the same data type. A basic node structure may be defined as\nstruct node { void *data; struct node *next; }; Singly-Linked Lists A singly-linked list, the most basic form, has a reference to the first node in the list, called the head, and a single link between each node.\nFigure 1: Diagram of a linked list with 3 nodes. The top sections contain data and the bottom sections contain pointers to the next node. The definition of a linked list node allows the list to grow dynamically. Nodes can be added at any time to any position in the node, as long as a reference to the node before the new one is known. The last node in a list points to NULL.\nDoubly-Linked Lists More commonly, linked lists are doubly-linked in that there is a link to the next node and a link to the previous node. This allows for more flexibility in traversing the list, but requires more memory to store the extra link. A standard implementation will also keep a reference to both the head and the tail of the list. This permits efficient insertion and deletion at both ends of the list.\nOperations Insertion A new node can be inserted either at the beginning, the end, or somewhere in between. Inserting at the beginning or end is a constant time operation, but inserting in the middle requires traversing the list to find the correct position. To insert at the beginning, the new node\u0026rsquo;s next reference is updated to the old head and the head is updated to the new node.\ndef insert_at_beginning(head, data): new_node = Node(data) new_node.next = head head = new_node To insert at the end without a reference to the tail, the list must be traversed to find the last node. The new node\u0026rsquo;s next reference is set to NULL and the last node\u0026rsquo;s next reference is set to the new node.\ndef insert_at_end(head, data): new_node = Node(data) new_node.next = None if head is None: head = new_node else: last_node = head while last_node.next is not None: last_node = last_node.next last_node.next = new_node To insert at the end with a reference to the tail, the new node\u0026rsquo;s next reference is set to NULL and the tail\u0026rsquo;s next reference is set to the new node. The tail is then updated to the new node.\ndef insert_at_end(head, tail, data): new_node = Node(data) new_node.next = None if head is None: head = new_node tail = new_node else: tail.next = new_node tail = new_node To insert in the middle, the list must be traversed to find the correct position. The new node\u0026rsquo;s next reference is set to the next node and the previous node\u0026rsquo;s next reference is set to the new node.\ndef insert_in_middle(head, data, position): new_node = Node(data) if head is None: head = new_node else: current_node = head for i in range(position - 1): current_node = current_node.next new_node.next = current_node.next current_node.next = new_node Searching Searching a linked list is a linear time operation. The list is traversed until the desired node is found or the end of the list is reached.\ndef search(head, data): current_node = head while current_node is not None: if current_node.data == data: return current_node current_node = current_node.next return None Deletion Deletion is similar to insertion. A node can be deleted from the beginning, the end, or somewhere in between. Deleting at the beginning or end is a constant time operation, but deleting in the middle requires traversing the list to find the correct position. To delete at the beginning, the head is updated to the second node and the first node is deleted.\ndef delete_at_beginning(head): if head is not None: head = head.next To delete at the end without a reference to the tail, the list must be traversed to find the last node. The second to last node\u0026rsquo;s next reference is set to NULL and the last node is deleted.\ndef delete_at_end(head): if head is not None: if head.next is None: head = None else: last_node = head while last_node.next.next is not None: last_node = last_node.next last_node.next = None To delete at the end with a reference to the tail, the tail is updated to the second to last node and the last node is deleted.\ndef delete_at_end(head, tail): if head is not None: if head.next is None: head = None tail = None else: last_node = head while last_node.next.next is not None: last_node = last_node.next last_node.next = None tail = last_node To delete in the middle, the list must be traversed to find the correct position. The previous node\u0026rsquo;s next reference is set to the next node and the current node is deleted.\ndef delete_in_middle(head, position): if head is not None: current_node = head for i in range(position - 1): current_node = current_node.next current_node.next = current_node.next.next Exercises Reverse a singly-linked list in linear time and constant space. Implement a queue using a linked list. ","date":1696136400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696136400,"objectID":"4503865aa3a005b9f5b525528459b074","permalink":"https://ajdillhoff.github.io/notes/linked_lists/","publishdate":"2023-10-01T00:00:00-05:00","relpermalink":"/notes/linked_lists/","section":"notes","summary":"Table of Contents Singly-Linked Lists Doubly-Linked Lists Operations Exercises A linked list is a dynamic and aggregate data structure made up of a collection of nodes. The nodes of a linked list can store any data type and are not enforced to contain the same data type. A basic node structure may be defined as\nstruct node { void *data; struct node *next; }; Singly-Linked Lists A singly-linked list, the most basic form, has a reference to the first node in the list, called the head, and a single link between each node.","tags":["computer science","data structures"],"title":"Linked Lists","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents The notation of complexity analysis Formal Definition of Asymptotic Notation Common Functions The notation of complexity analysis $O$-notation $O$-notation, often referred to as \u0026ldquo;Big Oh\u0026rdquo; notation, describes an upper bound on the behavior of a function. It really means that the function will not grow faster than the a given rate. This rate is typically the highest-order term in the function, and is often referred to as the \u0026ldquo;dominant term\u0026rdquo; or \u0026ldquo;dominant function\u0026rdquo;.\nFor example, the function \\(f(n) = 3n^2 + 2n + 1\\) has a dominant term of \\(n^2\\), and so we would say that \\(f(n) = O(n^2)\\). We could also accurately describe \\(f(n)\\) as \\(O(n^3)\\) since it technically does not grow at a faster rate than \\(n^3\\), but this is less common as it misleads the reader into thinking that the function is bounded at \\(n^3\\).\n$Ω$-notation $Ω$-notation is used to describe the lower bound on the asymptotic behavior of a function. Specifically, it means that the function grows at least as fast as the given rate. The function \\(f(n) = 3n^2 + 2n + 1\\) grows at least as fast as \\(n^2\\), so we would say that \\(f(n) = \\Omega(n^2)\\). It does not grow as fast as \\(n^3\\), however.\nJust like $O$-notation, we can abuse this definition and say that something that grows at least as fast as \\(n^2\\) also grows as fast as \\(n\\). This would lead the reader to believe that the function is bounded at \\(n\\), which is not true. For this reason, we typically use the tightest bound possible.\n$Θ$-notation $Θ$-notation gives a tightly bound characterization of a function\u0026rsquo;s behavior. It gives the rate of growth within a constant factor bounded above as well as constant factor bounded below.\nTo show that a function is \\(\\Theta(f(n))\\), we must show that it is both \\(O(f(n))\\) and \\(\\Omega(f(n))\\). Taking our example from above, the function \\(f(n) = 3n^2 + 2n + 1\\) is \\(\\Theta(n^2)\\).\nExample: Insertion Sort Let\u0026rsquo;s put this notation to work and characterize the running time of insertion sort. We\u0026rsquo;ll start by writing out the pseudocode for the algorithm:\ndef insertion_sort(A): for i in range(1, len(A)): key = A[i] j = i - 1 while i \u0026gt;= 0 and A[j] \u0026gt; key: A[j + 1] = A[j] j = j - 1 A[j + 1] = key From our Introduction to Algorithms lecture, we already know that the outer loop runs \\((n-1)\\) times (although the loop is checked \\(n\\) times). This is not dependent on the order of the \\(n\\) inputs either. The inner loop is dependent on the values of our input. It could run anywhere between 0 and \\(i-1\\) times. In the worst case, we saw that it would run \\(n-1\\) times as well. With this, we concluded that the running time of insertion sort is \\(O(n^2)\\). Since this was derived for the worst-case, it is reasonable to say that insertion sort is \\(O(n^2)\\) for all cases.\nThe key to the number of operations that the inner loop takes is A[j + 1] = A[j], or the number of times a value is shifted to the right. Given an input of \\(n\\) elements in the worst-case scenario, we can split the input into 3 partitions where the largest \\(\\lfloor\\frac{n}{4}\\rfloor\\) values are in the first partition. The second partition has size \\(\\lceil\\frac{n}{2}\\rceil\\), and the last partition has size \\(\\lfloor\\frac{n}{4}\\rfloor\\). By using the floor and ceiling functions, we can accommodate for odd values of \\(n\\).\nWhen the array is finally sorted, the largest \\(\\lfloor\\frac{n}{4}\\rfloor\\) values will be in the last partition. That means that they would have passed through the middle \\(\\lceil\\frac{n}{2}\\rceil\\) values one at a time. Therefore, we can state that the worst-case is proportional to\n\\[ \\left(\\left\\lfloor\\frac{n}{4}\\right\\rfloor\\right)\\left(\\left\\lceil\\frac{n}{2}\\right\\rceil\\right) \\leq \\frac{n^2}{8}. \\]\nThis is \\(\\Omega(n^2)\\), so we can conclude that insertion sort is \\(\\Theta(n^2)\\).\nBonus Example: Selection Sort Use a similar analysis to show that the worst-case for selection sort is \\(\\Theta(n^2)\\). As a reminder, selection sort is defined as\ndef selection_sort(A): for i in range(0, len(A)-1): min_j = i for j in range(i + 1, len(A)): if A[j] \u0026lt; A[min_j]: min_j = j A[i], A[min_j] = A[min_j], A[i] We have already observed that the outer loop iterates \\(n-1\\) times. Even in the best case, the inner loop runs proportional to \\(n\\) times. This is sufficient to conclude that the running time is \\(O(n^2)\\) for all cases.\nFor showing that the worst case is \\(\\Omega(n^2)\\), we could use the same argument as insertion sort. However, that isn\u0026rsquo;t necessary. In any case, the inner loop will run proportional to \\(n\\) times. It is not dependent on any specific arrangement of the input as selection sort is. Therefore, we can conclude that the worst-case is \\(\\Omega(n^2)\\), and so selection sort is \\(\\Theta(n^2)\\).\nFormal Definition of Asymptotic Notation Now that we have established some understanding of the notation, let\u0026rsquo;s define it formally. We typically use functions whose domains are over the set of natural or real numbers.\n$O$-notation We previously established that $O$-notation described as asymptotic upper bound. It was briefly mentioned that this bound holds within a constant factor, which we will now define more thoroughly. For a function \\(g(n)\\), \\(O(g(n)) = \\{f(n) : \\exists c \u0026gt; 0, n_0 \u0026gt; 0 \\text{ such that } 0 \\leq f(n) \\leq cg(n) \\text{ for all } n \\geq n_0\\}\\). It might make more sense to visualize this definition.\nFigure 1: Visualization of $O$-notation (source: Cormen et al.) Notice that the function \\(f(n)\\) is bounded above by \\(cg(n)\\) for all \\(n \\geq n_0\\) in the figure above.\nLet\u0026rsquo;s put this definition to the test with an example. Given a function \\(f(n) = 3n^2 + 200n + 1000\\), show that \\(f(n) = O(n^2)\\). The goal is to find positive constants \\(c\\) and \\(n_0\\) such that \\(3n^2 + 200n + 1000 \\leq cn^2\\) for all \\(n \\geq n_0\\). Dividing both sides by \\(n^2\\) yields\n\\[ 3 + \\frac{200}{n} + \\frac{1000}{n^2} \\leq c. \\]\nThis equation has many possible solutions. Let\u0026rsquo;s choose \\(n_0 = 2\\), then\n\\[ 3 + \\frac{200}{2} + \\frac{1000}{2^2} = 3 + 100 + 250 = 353 \\leq c. \\]\nTherefore, we can conclude that \\(f(n) = O(n^2)\\).\n$Ω$-notation The notation used to describe an asymptotic lower bound is formally defined as \\(\\Omega(g(n)) = \\{f(n) : \\exists c \u0026gt; 0, n_0 \u0026gt; 0 \\text{ such that } 0 \\leq cg(n) \\leq f(n) \\text{ for all } n \\geq n_0\\}\\). Again, it is helpful to visualize this.\nFigure 2: Visualization of $Ω$-notation (source: Cormen et al.) Notice that the function \\(f(n)\\) is bounded below by \\(cg(n)\\) for all \\(n \\geq n_0\\) in the figure above.\nLet\u0026rsquo;s revisit our function from above and show that \\(f(n) = \\Omega(n^2)\\). The goal is to find positive constants \\(c\\) and \\(n_0\\) such that \\(3n^2 + 200n + 1000 \\geq cn^2\\) for all \\(n \\geq n_0\\). Dividing both sides by \\(n^2\\) yields\n\\[ 3 + \\frac{200}{n} + \\frac{1000}{n^2} \\geq c. \\]\nThis holds when \\(c = 3\\) and \\(n_0\\) is any positive integer. To see this, think about what happens to this function as \\(n\\) approaches infinity. The first term will always be 3, and the second and third terms will approach 0. Therefore, we can conclude that \\(f(n) = \\Omega(n^2)\\).\n$Θ$-notation Lastly, the notation used for an asymptotically tight bound is \\(\\Theta(g(n)) = \\{f(n) : \\exists c_1, c_2 \u0026gt; 0, n_0 \u0026gt; 0 \\text{ such that } 0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n) \\text{ for all } n \\geq n_0\\}\\).\nFigure 3: Visualization of $Θ$-notation (source: Cormen et al.) We had mentioned previously that if \\(f(n) = \\Omega(g(n))\\) and \\(f(n) = O(g(n))\\), then \\(f(n) = \\Theta(g(n))\\). This is formalized in the following theorem, as stated in Cormen et al.\nFor any two functions \\(f(n)\\) and \\(g(n)\\), we have \\(f(n) = \\Theta(g(n))\\) if and only if \\(f(n) = O(g(n))\\) and \\(f(n) = \\Omega(g(n))\\).\nFunction Properties The following properties are useful when analyzing the asymptotic behavior of functions.\nTransitivity If \\(f(n) = O(g(n))\\) and \\(g(n) = O(h(n))\\), then \\(f(n) = O(h(n))\\). If \\(f(n) = \\Omega(g(n))\\) and \\(g(n) = \\Omega(h(n))\\), then \\(f(n) = \\Omega(h(n))\\). If \\(f(n) = \\Theta(g(n))\\) and \\(g(n) = \\Theta(h(n))\\), then \\(f(n) = \\Theta(h(n))\\). Reflexivity \\(f(n) = O(f(n))\\) \\(f(n) = \\Omega(f(n))\\) \\(f(n) = \\Theta(f(n))\\) Symmetry \\(f(n) = \\Theta(g(n))\\) if and only if \\(g(n) = \\Theta(f(n))\\). Transpose Symmetry \\(f(n) = O(g(n))\\) if and only if \\(g(n) = \\Omega(f(n))\\). Common Functions The functions used to describe both time and space complexity are visualized below.\nFigure 4: Common functions used in complexity analysis (source: Wikipedia) ","date":1695618000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695618000,"objectID":"2e6bf41f4deb3ffa0071eeebb242a1e0","permalink":"https://ajdillhoff.github.io/notes/complexity_analysis/","publishdate":"2023-09-25T00:00:00-05:00","relpermalink":"/notes/complexity_analysis/","section":"notes","summary":"Table of Contents The notation of complexity analysis Formal Definition of Asymptotic Notation Common Functions The notation of complexity analysis $O$-notation $O$-notation, often referred to as \u0026ldquo;Big Oh\u0026rdquo; notation, describes an upper bound on the behavior of a function. It really means that the function will not grow faster than the a given rate. This rate is typically the highest-order term in the function, and is often referred to as the \u0026ldquo;dominant term\u0026rdquo; or \u0026ldquo;dominant function\u0026rdquo;.","tags":["computer science","algorithms","complexity analysis"],"title":"Complexity Analysis","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction to Algorithms Insertion Sort Example: Sorting Numbers Worst-Case Analysis Best-Case Analysis Rate of Growth Example: Analysis of Selection Sort Introduction to Algorithms One of the major goals of computer science is to solve important problems. In order to do that, we must be able to express those solutions both mathematically and in a way that can be executed by a computer. Further, those solutions need to be aware of the resources that are available to them. It does us no good to come up with a solution that could never be run by current hardware or executed in a reasonable amount of time.\nThere are of course other considerations besides runtime. How much memory does the solution require? Does it require a lot of data to be stored on disk? What about distributed solutions that can be run on multiple machines? Some solutions can be so complex, that we must also consider their environmental impact. For example, Meta\u0026rsquo;s Llama 2 large language models required 3,311,616 combined GPU hours to train. They report that their total carbon emissions from training were 539 tons of CO2 equivalent (Touvron et al. 2023).\nWe begin our algorithmic journey by studying a simple sorting algorithm, insertion sort. First, we need to formally define the problem of sorting. Given a sequence of \\(n\\) objects \\(A = \\langle a_1, a_2, \\ldots, a_n \\rangle\\), we want to rearrange the elements such that \\(a_1\u0026rsquo; \\leq a_2\u0026rsquo; \\leq \\ldots \\leq a_n\u0026rsquo;\\). We will assume that the elements are comparable, meaning that we can use the operators \\(\u0026lt;\\) and \\(\u0026gt;\\) to compare them. Some sets, such as the set of all real numbers, have a natural ordering. A useful programming language would provide the required comparison operators. For other types of elements, such as strings, this may not be the case. For example, how would you compare the strings \u0026ldquo;apple\u0026rdquo; and \u0026ldquo;banana\u0026rdquo;? In these cases, we will need to define our own comparison operators. Either way, we will assume that the comparison operators are available to us.\nThis example follows the one given in Chapter 2 of Cormen et al. (2009).\nInsertion Sort Insertion sort is defined as\ndef insertion_sort(A): for i in range(1, len(A)): key = A[i] j = i - 1 while j \u0026gt;= 0 and A[j] \u0026gt; key: A[j + 1] = A[j] j = j - 1 A[j + 1] = key Example: Sorting Numbers TODO: Add a step-by-step example of sorting a list of numbers.\nWorst-Case Analysis Given the definition from above, we can compute \\(T(n)\\), the running time of the algorithm on an input of size \\(n\\). To do this, we need to sum the products of the cost of each statement and the number of times each statement is executed.\nAt first glance, the first statement for i in range(1, len(A)) appears to execute \\(n-1\\) times since it starts at 1 and only goes up to, but not including, \\(n\\). Remember that the for statement must be checked to see if it should exit, so the test is executed one more time than the number of iterations. Therefore, the first statement is executed \\(n\\) times. If we say that the cost to execute each check is \\(c_1\\), then the total cost of the first statement is \\(c_1 n\\).\nWith the exception of the while loop, the statement inside the for loop is executed once per iteration. The cost of executing statement \\(i\\) is \\(c_i\\). Therefore, the total cost of the second statement is \\(c_2 n\\). The costs are updated in the code below.\ndef insertion_sort(A): for i in range(1, len(A)): # c_1 n key = A[i] # c_2 n j = i - 1 # c_3 n while j \u0026gt;= 0 and A[j] \u0026gt; key: A[j + 1] = A[j] j = j - 1 A[j + 1] = key # c_7 n For the while loop, we can denote the number of times it runs by \\(t_i\\), where \\(i\\) is the iteration of the for loop. If the while condition check costs \\(c_4\\) and is executed \\(t_i\\) times for each for loop iteration, the total cost is given as \\(c_4 \\sum_{i=1}^{n-1} t_i\\).\nThe statement inside the while loop are executed 1 fewer times than the number of times the condition check is executed. Therefore, the total cost of the statements inside the while loop is \\(c_5 \\sum_{i=1}^{n-1} (t_i - 1) + c_5 \\sum_{i=1}^{n-1} (t_i - 1)\\). The cost of the while loop is updated in the code below.\ndef insertion_sort(A): for i in range(1, len(A)): # c_1 * n key = A[i] # c_2 * (n-1) j = i - 1 # c_3 * (n-1) while j \u0026gt;= 0 and A[j] \u0026gt; key: # c_4 * [t_i for i in range(1, len(A))] A[j + 1] = A[j] # c_5 * [t_i - 1 for i in range(1, len(A))] j = j - 1 # c_6 * [t_i - 1 for i in range(1, len(A))] A[j + 1] = key # c_7 * (n-1) To get the total running time \\(T(n)\\), we sum up all of the costs.\n\\begin{align} T(n) \u0026amp;= c_1 n + c_2 (n-1) + c_3 (n-1) + c_4 \\sum_{i=1}^{n-1} t_i + c_5 \\sum_{i=1}^{n-1} (t_i - 1) + c_6 \\sum_{i=1}^{n-1} (t_i - 1) + c_7 (n-1) \\\\ \\end{align}\nThis analysis is a good start, but it doesn\u0026rsquo;t paint the whole picture. The number of actual executions will depend on the input that is given. For example, what if the input is already sorted, or given in reverse order? It is common to express the worst-case runtime for a particular algorithm. For insertion sort, that is when the input is in reverse order. In this case, each element \\(A[i]\\) is compared to every other element in the sorted subarray. This means that \\(t_i = i\\) for every iteration of the for loop. Therefore, the worst-case runtime is given as\n\\begin{align} T(n) \u0026amp;= c_1 n + c_2 (n-1) + c_3 (n-1) + c_4 \\sum_{i=1}^{n-1} i + c_5 \\sum_{i=1}^{n-1} (i - 1) + c_6 \\sum_{i=1}^{n-1} (i - 1) + c_7 (n-1) \\\\ \\end{align}\nTo express this runtime solely in terms of \\(n\\), we can use the fact that \\(\\sum_{i=1}^{n-1} i = (\\sum_{i=0}^{n-1} i) - 1 = \\frac{n(n-1)}{2} - 1\\) and \\(\\sum_{i=1}^{n-1} (i - 1) = \\sum_{i=0}^{n-2} i = \\frac{n(n-1)}{2}\\). This gives us\n\\begin{align} T(n) \u0026amp;= c_1 n + c_2 (n-1) + c_3 (n-1) + c_4 \\left(\\frac{n(n-1)}{2} - 1\\right)\\\\ \u0026amp;+ c_5 \\left(\\frac{n(n-1)}{2}\\right) + c_6 \\left(\\frac{n(n-1)}{2}\\right) + c_7 (n-1) \\\\ \u0026amp;= \\left(\\frac{c_4}{2} + \\frac{c_5}{2} + \\frac{c_6}{2}\\right)n^2 + \\left(c_1 + c_2 + c_3 + \\frac{c_4}{2} - \\frac{c_5}{2} - \\frac{c_6}{2} + c_7\\right)n - (c_2 + c_3 + c_4 + c_7) \\\\ \\end{align}\nWith the appropriate choice of constants, we can express this as a quadratic function \\(an^2 + bn + c\\).\nBest-Case Analysis The best-case runtime for insertion sort is when the input is already sorted. In this case, the while check is executed only once per iteration of the for loop. That is, \\(t_i = 1\\) for every iteration of the for loop. Therefore, the best-case runtime is given as\n\\begin{align} T(n) \u0026amp;= c_1 n + c_2 (n-1) + c_3 (n-1) + c_4 (n-1) + c_7 (n-1) \\\\ \u0026amp;= (c_1 + c_2 + c_3 + c_4 + c_7)n - (c_2 + c_3 + c_4 + c_7) \\\\ \\end{align}\nLet \\(a = c_1 + c_2 + c_3 + c_4 + c_7\\) and $b = -(c_2 + c_3 + c_4 + c_7)$Then the best-case runtime is given as \\(an + b\\), a linear function of \\(n\\).\nRate of Growth We can simplify how we express the runtime of both these cases by considering only the highest-order term. Consider the worst-case, \\(T(n) = an^2 + bn + c\\). As \\(n\\) grows, the term \\(an^2\\) will dominate the runtime, rendering the others insignificant by comparison. This simplification is typically expressed using \\(\\Theta\\) notation. For the worst-case, we say that \\(T(n) = \\Theta(n^2)\\). It is a compact way of stating that the runtime is proportional to \\(n^2\\) for large values of \\(n\\).\nExample: Analysis of Selection Sort Based on the analysis above, let\u0026rsquo;s check our understanding and see if we can characterize the runtime of another sorting algorithm, selection sort. Selection sort is defined as\ndef selection_sort(A): for i in range(0, len(A) - 1): min_index = i for j in range(i + 1, len(A)): if A[j] \u0026lt; A[min_index]: min_index = j A[i], A[min_index] = A[min_index], A[i] The first statement for i in range(0, len(A) - 1) will be evaluated \\(n\\) times. With the exception of the inner for loop, the rest of the statements in the scope of the first for loop are executed once per iteration. Their costs are \\(c_2\\) and \\(c_6\\), respectively.\nThe inner for loop will be checked \\(n-i\\) times for each iteration of the outer for loop. The cost of the condition check is \\(c_3\\). The cost of the statements inside the for loop are \\(c_4\\) and \\(c_5\\). The if check is evaluated for every iteration of the inner loop, but the statements inside the if are only executed when the condition is true. We can denote this as \\(t_i\\), the number of times the if condition is true for each iteration of the inner for loop. The cost of the inner loop is given as\n\\begin{align} c_3 \\sum_{i=1}^{n-1} (n-i) + c_4 \\sum_{i=0}^{n-1} (n-i-1) + c_5 \\sum_{i=0}^{n-1} t_i\\\\ \\end{align}\nCombining this with the cost of the outer for loop, we get\n\\begin{align} T(n) \u0026amp;= c_1 n + c_2 (n-1) + c_6 (n-1) + c_3 \\sum_{i=0}^{n-1} (n-i) + c_4 \\sum_{i=0}^{n-1} (n-i-1) + c_5 \\sum_{i=0}^{n-1} t_i\\\\ \\end{align}\nReferences Touvron, Hugo, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, et al. 2023. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” arXiv. https://doi.org/10.48550/arXiv.2307.09288. ","date":1695099600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695099600,"objectID":"ae729bf0a8e3739ea8dbfc7bb807a7db","permalink":"https://ajdillhoff.github.io/notes/introduction_to_complexity_analysis/","publishdate":"2023-09-19T00:00:00-05:00","relpermalink":"/notes/introduction_to_complexity_analysis/","section":"notes","summary":"Table of Contents Introduction to Algorithms Insertion Sort Example: Sorting Numbers Worst-Case Analysis Best-Case Analysis Rate of Growth Example: Analysis of Selection Sort Introduction to Algorithms One of the major goals of computer science is to solve important problems. In order to do that, we must be able to express those solutions both mathematically and in a way that can be executed by a computer. Further, those solutions need to be aware of the resources that are available to them.","tags":["computer science","algorithms"],"title":"Introduction to Algorithms","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":"Python Review Questions The following questions are meant to help you review introductory concepts in Python. They are based on the Python 3 Tutorial and Python 3 Documentation and were written to accompany a 5 lecture series on Python.\nThere are three types of questions:\nVerify the Code: Determine the output of a code snippet. Fill in the Code: Fill in the code to complete a code snippet. Create the Function: Create a function that satisfies the given requirements. Verify the Code Operations on Strings\nprint(\u0026#34;Hello\u0026#34; + \u0026#34; \u0026#34; + \u0026#34;World\u0026#34; * 2) What will the output be?\nSolution Hello World World Explanation: The + operator concatenates strings, and the * operator repeats strings.\nLooping Over Lists\nfor i in [1, 2, 3]: print(i * 2) What will the output be?\nSolution 2 4 6 Explanation: The for loop iterates over the elements of the list.\nMutability of Strings\ns = \u0026#34;hello\u0026#34; s[0] = \u0026#34;H\u0026#34; Is this code valid? If not, why?\nSolution This code is not valid. Strings are immutable, so you cannot change their elements.\nStatic Methods vs. Instance Methods\nclass MyClass: def instance_method(self): return \u0026#39;instance method\u0026#39; @staticmethod def static_method(): return \u0026#39;static method\u0026#39; obj = MyClass() print(obj.instance_method()) print(MyClass.static_method()) What will the output be?\nSolution instance method static method Explanation: Instance methods are called on an instance of a class, whereas static methods are called on the class itself.\nyield Keyword Example\ndef my_gen(): yield \u0026#34;A\u0026#34; yield \u0026#34;B\u0026#34; gen = my_gen() print(next(gen)) print(next(gen)) print(next(gen)) What will the output be?\nSolution A B Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; StopIteration Explanation: The yield keyword is used to create generators. Generators are iterators that can be iterated over only once.\nType Validation\nx = 42 print(isinstance(x, int)) What will the output be?\nSolution True Explanation: The isinstance function checks if an object is of a certain type.\ninput Always Returns a str\nx = input(\u0026#34;Enter a number: \u0026#34;) print(type(x) is int) If the user enters 42, what will the output be?\nSolution False Explanation: The input function always returns a str, even if the user enters a number.\nSearch in List Example\nmy_list = [1, 2, 3, 4, 5] print(3 in my_list) What will the output be?\nSolution True Explanation: The in operator checks if an element is in a list.\nFormatted Printing\nname = \u0026#34;Alice\u0026#34; age = 30 print(f\u0026#34;{name} is {age} years old.\u0026#34;) What will the output be?\nSolution Alice is 30 years old. Explanation: The f prefix allows you to use formatted strings.\nEquality Versus is\nx = [1, 2, 3] y = [1, 2, 3] print(x == y) print(x is y) What will the output be?\nSolution True False Explanation: The == operator checks if two objects are equal, whereas the is operator checks if two objects are the same object.\nCSV Line to List Using split\nline = \u0026#34;apple,banana,cherry\u0026#34; fruits = line.split(\u0026#39;,\u0026#39;) print(fruits) What will the output be?\nSolution [\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;cherry\u0026#39;] Explanation: The split method splits a string into a list of strings using a delimiter.\nDeep vs. Shallow Copy\nimport copy a = [1, [2, 3]] b = copy.copy(a) c = copy.deepcopy(a) a[1][0] = 99 print(b) print(c) What will the output be?\nSolution [1, [99, 3]] [1, [2, 3]] Explanation: The copy function creates a shallow copy, whereas the deepcopy function creates a deep copy.\nAccess Class Variable vs. Instance Variable\nclass Dog: kind = \u0026#39;canine\u0026#39; def __init__(self, name): self.name = name d = Dog(\u0026#39;Fido\u0026#39;) print(d.kind) print(d.name) What will the output be?\nSolution canine Fido Explanation: The kind variable is a class variable, whereas the name variable is an instance variable.\nAccessing global Variable\nx = 10 def foo(): global x x += 5 print(x) foo() What will the output be?\nSolution 15 Explanation: The global keyword allows you to access a global variable inside a function.\nFill in the Code Looping Over 2D Lists\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] # Fill in the code to print each element in the 2D list. Solution for row in matrix: for element in row: print(element) Explanation: The outer loop iterates over the rows, and the inner loop iterates over the elements in each row.\nStatic Methods vs. Instance Methods\nclass Calculator: @staticmethod def add(a, b): return a + b # Fill in the code to create an instance method that multiplies two numbers. Solution class Calculator: @staticmethod def add(a, b): return a + b def multiply(self, a, b): return a * b Explanation: Instance methods take self as the first argument.\nList Comprehension for 2D List\n# Fill in the code to create a 2D list with list comprehension. # The 2D list should contain rows from 0 to 4 and columns from 0 to 4, # where each element is the sum of its row and column index. Solution matrix = [[row + col for col in range(5)] for row in range(5)] Explanation: The outer loop iterates over the rows, and the inner loop iterates over the columns.\nDictionary Add, Iterating Over Keys and Values\nmy_dict = {\u0026#39;apple\u0026#39;: 1, \u0026#39;banana\u0026#39;: 2} # Fill in the code to add a key-value pair (\u0026#39;cherry\u0026#39;, 3) to my_dict and print all keys and values. Solution my_dict[\u0026#39;cherry\u0026#39;] = 3 for key in my_dict: print(key, my_dict[key]) Explanation: The for loop iterates over the keys of the dictionary.\nAdd List to Existing List (Zip vs. Extend)\nlist1 = [1, 2, 3] list2 = [4, 5, 6] # Fill in the code to append the elements of list2 to list1. Solution list1.extend(list2) Explanation: The extend method appends the elements of a list to another list.\nOverride Special Method So Class Can Be Sorted\nclass Person: def __init__(self, name, age): self.name = name self.age = age # Fill in the code to make instances of this class sortable by age. Solution class Person: def __init__(self, name, age): self.name = name self.age = age def __lt__(self, other): return self.age \u0026lt; other.age Explanation: The __lt__ method overrides the \u0026lt; operator. Either this method or the __gt__ method must be defined for the class to be sortable.\nCreate the Function Reading Input From the User\nnumbers = read_numbers_from_user(5) print(\u0026#34;The sum is:\u0026#34;, sum(numbers)) Create the function read_numbers_from_user that takes an integer ( n ) and reads ( n ) numbers from the user.\nSolution def read_numbers_from_user(n): numbers = [] for i in range(n): numbers.append(int(input(\u0026#34;Enter a number: \u0026#34;))) return numbers Explanation: The input function reads a string from the user. The int function converts a string to an integer.\nMatch Statement vs. If-Elif-Else\nprint(match_fruit_color(\u0026#34;apple\u0026#34;)) Create the function match_fruit_color that takes a fruit name and returns its color using a match statement.\nSolution def match_fruit_color(fruit): match fruit: case \u0026#34;apple\u0026#34;: return \u0026#34;red\u0026#34; case \u0026#34;banana\u0026#34;: return \u0026#34;yellow\u0026#34; case \u0026#34;cherry\u0026#34;: return \u0026#34;red\u0026#34; case _: return \u0026#34;unknown\u0026#34; Explanation: The match statement is used to compare a value against a number of patterns. It is similar to the switch statement in other languages.\nFormatted Printing\nprint_formatted_string(\u0026#34;John\u0026#34;, 25) Create the function print_formatted_string that takes a name and an age and prints them in a formatted string.\nSolution def print_formatted_string(name, age): print(f\u0026#34;{name} is {age} years old.\u0026#34;) Explanation: The f prefix allows you to use formatted strings.\nType Validation\nprint(is_valid_number(\u0026#34;42\u0026#34;)) Create the function is_valid_number that takes a string and returns True if it can be converted to an integer or a float, otherwise False.\nSolution def is_valid_number(s): try: float(s) return True except ValueError: return False Explanation: The try statement allows you to handle exceptions. The float function converts a string to a float. If a string could be converted to a float, it can also be converted to an int.\nAccessing global Variable\nincrement_global_x() print(x) Create the function increment_global_x that increments the global variable ( x ) by 1.\nSolution def increment_global_x(): global x x += 1 Explanation: The global keyword allows you to access a global variable inside a function.\n","date":1694494800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694494800,"objectID":"4be737f5d2b79e20db4eee0b2e491ba7","permalink":"https://ajdillhoff.github.io/notes/python_exam_review/","publishdate":"2023-09-12T00:00:00-05:00","relpermalink":"/notes/python_exam_review/","section":"notes","summary":"Python Review Questions The following questions are meant to help you review introductory concepts in Python. They are based on the Python 3 Tutorial and Python 3 Documentation and were written to accompany a 5 lecture series on Python.\nThere are three types of questions:\nVerify the Code: Determine the output of a code snippet. Fill in the Code: Fill in the code to complete a code snippet. Create the Function: Create a function that satisfies the given requirements.","tags":["python"],"title":"Python Review Questions","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" NumPy Quickstart This notebook is a quick introduction to NumPy. It is an interactive version of the NumPy Quickstart Tutorial.\nAll credits go to the original authors of the tutorial © Copyright 2008-2023, NumPy Developers.\nThe Basics NumPy’s main object is the homogeneous multidimensional array. It is a table of elements (usually numbers), all of the same type, indexed by a tuple of non-negative integers. In NumPy dimensions are called axes.\nFor example, the array for the coordinates of a point in 3D space, [1, 2, 1], has one axis. That axis has 3 elements in it, so we say it has a length of 3. In the example pictured below, the array has 2 axes. The first axis has a length of 2, the second axis has a length of 3.\n[[ 1., 0., 0.], [ 0., 1., 2.]] NumPy’s array class is called ndarray. It is also known by the alias array. Note that numpy.array is not the same as the Standard Python Library class array.array, which only handles one-dimensional arrays and offers less functionality. The more important attributes of an ndarray object are:\nndarray.ndim\nthe number of axes (dimensions) of the array.\nndarray.shape\nthe dimensions of the array. This is a tuple of integers indicating the size of the array in each dimension. For a matrix with n rows and m columns, shape will be (n,m). The length of the shape tuple is therefore the number of axes, ndim.\nndarray.size\nthe total number of elements of the array. This is equal to the product of the elements of shape.\nndarray.dtype\nan object describing the type of the elements in the array. One can create or specify dtype’s using standard Python types. Additionally NumPy provides types of its own. numpy.int32, numpy.int16, and numpy.float64 are some examples.\nndarray.itemsize\nthe size in bytes of each element of the array. For example, an array of elements of type float64 has itemsize 8 (=64/8), while one of type complex32 has itemsize 4 (=32/8). It is equivalent to ndarray.dtype.itemsize.\nndarray.data\nthe buffer containing the actual elements of the array. Normally, we won’t need to use this attribute because we will access the elements in an array using indexing facilities.\nAn example import numpy as np a = np.arange(15).reshape(3, 5) print(\u0026#34;a = \\n{}\u0026#34;.format(a)) print(\u0026#34;a.shape = {}\u0026#34;.format(a.shape)) print(\u0026#34;a.ndim = {}\u0026#34;.format(a.ndim)) print(\u0026#34;a.dtype.name = {}\u0026#34;.format(a.dtype.name)) print(\u0026#34;a.itemsize = {}\u0026#34;.format(a.itemsize)) print(\u0026#34;a.size = {}\u0026#34;.format(a.size)) print(\u0026#34;type(a) = {}\u0026#34;.format(type(a))) a = [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14]] a.shape = (3, 5) a.ndim = 2 a.dtype.name = int64 a.itemsize = 8 a.size = 15 type(a) = \u0026lt;class 'numpy.ndarray'\u0026gt; Array Creation There are several ways to create arrays.\nFor example, you can create an array from a regular Python list or tuple using the array function. The type of the resulting array is deduced from the type of the elements in the sequences.\na = np.array([2,3,4]) print(\u0026#34;a = {}\u0026#34;.format(a)) print(\u0026#34;a.dtype = {}\u0026#34;.format(a.dtype)) b = np.array([1.2, 3.5, 5.1]) print(\u0026#34;b = {}\u0026#34;.format(b)) print(\u0026#34;b.dtype = {}\u0026#34;.format(b.dtype)) a = [2 3 4] a.dtype = int64 b = [1.2 3.5 5.1] b.dtype = float64 A frequent error consists in calling array with multiple numeric arguments, rather than providing a single list of numbers as an argument.\na = np.array(1, 2, 3, 4) # WRONG --------------------------------------------------------------------------- TypeError Traceback (most recent call last) /home/alex/dev/teaching/python-examples/numpy/quickstart.ipynb Cell 6 line 1 ----\u0026gt; \u0026lt;a href='vscode-notebook-cell:/home/alex/dev/teaching/python-examples/numpy/quickstart.ipynb#X14sZmlsZQ%3D%3D?line=0'\u0026gt;1\u0026lt;/a\u0026gt; a = np.array(1, 2, 3, 4) TypeError: array() takes from 1 to 2 positional arguments but 4 were given a = np.array([1, 2, 3, 4]) # RIGHT array transforms sequences of sequences into two-dimensional arrays, sequences of sequences of sequences into three-dimensional arrays, and so on.\nb = np.array([(1.5, 2, 3), (4, 5, 6)]) print(\u0026#34;b =\\n{}\u0026#34;.format(b)) b = [[1.5 2. 3. ] [4. 5. 6. ]] The type of the array can also be explicitly specified at creation time:\nc = np.array([[1, 2], [3, 4]], dtype=complex) print(\u0026#34;c =\\n{}\u0026#34;.format(c)) c = [[1.+0.j 2.+0.j] [3.+0.j 4.+0.j]] Often, the elements of an array are originally unknown, but its size is known. Hence, NumPy offers several functions to create arrays with initial placeholder content. These minimize the necessity of growing arrays, an expensive operation.\nThe function zeros creates an array full of zeros, the function ones creates an array full of ones, and the function empty creates an array whose initial content is random and depends on the state of the memory. By default, the dtype of the created array is float64.\nprint(\u0026#34;np.zeros((3, 4)) =\\n{}\u0026#34;.format(np.zeros((3, 4)))) print(\u0026#34;np.ones((2, 3, 4), dtype=np.int16) =\\n{}\u0026#34;.format(np.ones((2, 3, 4), dtype=np.int16))) print(\u0026#34;np.empty((2, 3)) =\\n{}\u0026#34;.format(np.empty((2, 3)))) np.zeros((3, 4)) = [[0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.]] np.ones((2, 3, 4), dtype=np.int16) = [[[1 1 1 1] [1 1 1 1] [1 1 1 1]] [[1 1 1 1] [1 1 1 1] [1 1 1 1]]] np.empty((2, 3)) = [[1.39069238e-309 1.39069238e-309 1.39069238e-309] [1.39069238e-309 1.39069238e-309 1.39069238e-309]] To create sequences of numbers, NumPy provides the arange function which is analogous to the Python built-in range, but returns an array.\nprint(\u0026#34;np.arange(10, 30, 5) = {}\u0026#34;.format(np.arange(10, 30, 5))) print(\u0026#34;np.arange(0, 2, 0.3) = {}\u0026#34;.format(np.arange(0, 2, 0.3))) # it accepts float arguments np.arange(10, 30, 5) = [10 15 20 25] np.arange(0, 2, 0.3) = [0. 0.3 0.6 0.9 1.2 1.5 1.8] When arange is used with floating point arguments, it is generally not possible to predict the number of elements obtained, due to the finite floating point precision. For this reason, it is usually better to use the function linspace that receives as an argument the number of elements that we want, instead of the step:\nfrom numpy import pi print(\u0026#34;np.linspace(0, 2, 9) = {}\u0026#34;.format(np.linspace(0, 2, 9))) # 9 numbers from 0 to 2 x = np.linspace(0, 2*pi, 100) # useful to evaluate function at lots of points f = np.sin(x) np.linspace(0, 2, 9) = [0. 0.25 0.5 0.75 1. 1.25 1.5 1.75 2. ] Print Arrays When you print an array, NumPy displays it in a similar way to nested lists, but with the following layout:\nthe last axis is printed from left to right, the second-to-last is printed from top to bottom, the rest are also printed from top to bottom, with each slice separated from the next by an empty line. One-dimensional arrays are then printed as rows, bidimensionals as matrices and tridimensionals as lists of matrices.\na = np.arange(6) # 1d array print(\u0026#34;a =\\n{}\u0026#34;.format(a)) b = np.arange(12).reshape(4, 3) # 2d array print(\u0026#34;b =\\n{}\u0026#34;.format(b)) c = np.arange(24).reshape(2, 3, 4) # 3d array print(\u0026#34;c =\\n{}\u0026#34;.format(c)) a = [0 1 2 3 4 5] b = [[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]] c = [[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] If an array is too large to be printed, NumPy automatically skips the central part of the array and only prints the corners:\nprint(\u0026#34;np.arange(10000) = {}\u0026#34;.format(np.arange(10000))) print(\u0026#34;np.arange(10000).reshape(100, 100) =\\n{}\u0026#34;.format(np.arange(10000).reshape(100, 100))) np.arange(10000) = [ 0 1 2 ... 9997 9998 9999] np.arange(10000).reshape(100, 100) = [[ 0 1 2 ... 97 98 99] [ 100 101 102 ... 197 198 199] [ 200 201 202 ... 297 298 299] ... [9700 9701 9702 ... 9797 9798 9799] [9800 9801 9802 ... 9897 9898 9899] [9900 9901 9902 ... 9997 9998 9999]] To disable this behaviour and force NumPy to print the entire array, you can change the printing options using set_printoptions.\nimport sys np.set_printoptions(threshold=sys.maxsize) # force NumPy to print the entire array Basic Operations Arithmetic operators on arrays apply elementwise. A new array is created and filled with the result.\na = np.array([20, 30, 40, 50]) b = np.arange(4) print(\u0026#34;b =\\n{}\u0026#34;.format(b)) c = a - b print(\u0026#34;c =\\n{}\u0026#34;.format(c)) print(\u0026#34;b**2 =\\n{}\u0026#34;.format(b**2)) print(\u0026#34;10*np.sin(a) =\\n{}\u0026#34;.format(10*np.sin(a))) print(\u0026#34;a \u0026lt; 35 =\\n{}\u0026#34;.format(a \u0026lt; 35)) b = [0 1 2 3] c = [20 29 38 47] b**2 = [0 1 4 9] 10*np.sin(a) = [ 9.12945251 -9.88031624 7.4511316 -2.62374854] a \u0026lt; 35 = [ True True False False] Unlike in many matrix languages, the product operator * operates elementwise in NumPy arrays. The matrix product can be performed using the @ operator (in python \u0026gt;=3.5) or the dot function or method:\nA = np.array([[1, 1], [0, 1]]) B = np.array([[2, 0], [3, 4]]) print(\u0026#34;A * B =\\n{}\u0026#34;.format(A * B)) # elementwise product print(\u0026#34;A @ B =\\n{}\u0026#34;.format(A @ B)) # matrix product print(\u0026#34;A.dot(B) =\\n{}\u0026#34;.format(A.dot(B))) # another matrix product A * B = [[2 0] [0 4]] A @ B = [[5 4] [3 4]] A.dot(B) = [[5 4] [3 4]] Some operations, such as += and *=, act in place to modify an existing array rather than create a new one.\nrg = np.random.default_rng(1) # create instance of default random number generator a = np.ones((2, 3), dtype=int) b = rg.random((2, 3)) a *= 3 print(\u0026#34;a =\\n{}\u0026#34;.format(a)) b += a print(\u0026#34;b =\\n{}\u0026#34;.format(b)) a += b # b is not automatically converted to integer type a = [[3 3 3] [3 3 3]] b = [[3.51182162 3.9504637 3.14415961] [3.94864945 3.31183145 3.42332645]] --------------------------------------------------------------------------- UFuncTypeError Traceback (most recent call last) /home/alex/dev/teaching/python-examples/numpy/quickstart.ipynb Cell 29 line 1 \u0026lt;a href='vscode-notebook-cell:/home/alex/dev/teaching/python-examples/numpy/quickstart.ipynb#Y110sZmlsZQ%3D%3D?line=7'\u0026gt;8\u0026lt;/a\u0026gt; b += a \u0026lt;a href='vscode-notebook-cell:/home/alex/dev/teaching/python-examples/numpy/quickstart.ipynb#Y110sZmlsZQ%3D%3D?line=9'\u0026gt;10\u0026lt;/a\u0026gt; print(\u0026quot;b =\\n{}\u0026quot;.format(b)) ---\u0026gt; \u0026lt;a href='vscode-notebook-cell:/home/alex/dev/teaching/python-examples/numpy/quickstart.ipynb#Y110sZmlsZQ%3D%3D?line=11'\u0026gt;12\u0026lt;/a\u0026gt; a += b UFuncTypeError: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind' When operating with arrays of different types, the type of the resulting array corresponds to the more general or precise one (a behavior known as upcasting).\na = np.ones(3, dtype=np.int32) b = np.linspace(0, pi, 3) print(\u0026#34;b.dtype.name = {}\u0026#34;.format(b.dtype.name)) c = a + b print(\u0026#34;c =\\n{}\u0026#34;.format(c)) print(\u0026#34;c.dtype.name = {}\u0026#34;.format(c.dtype.name)) d = np.exp(c*1j) print(\u0026#34;d =\\n{}\u0026#34;.format(d)) print(\u0026#34;d.dtype.name = {}\u0026#34;.format(d.dtype.name)) b.dtype.name = float64 c = [1. 2.57079633 4.14159265] c.dtype.name = float64 d = [ 0.54030231+0.84147098j -0.84147098+0.54030231j -0.54030231-0.84147098j] d.dtype.name = complex128 Many unary operations, such as computing the sum of all the elements in the array, are implemented as methods of the ndarray class.\na = rg.random((2, 3)) print(\u0026#34;a =\\n{}\u0026#34;.format(a)) print(\u0026#34;a.sum() = {}\u0026#34;.format(a.sum())) print(\u0026#34;a.min() = {}\u0026#34;.format(a.min())) print(\u0026#34;a.max() = {}\u0026#34;.format(a.max())) a = [[0.82770259 0.40919914 0.54959369] [0.02755911 0.75351311 0.53814331]] a.sum() = 3.1057109529998157 a.min() = 0.027559113243068367 a.max() = 0.8277025938204418 By default, these operations apply to the array as though it were a list of numbers, regardless of its shape. However, by specifying the axis parameter you can apply an operation along the specified axis of an array:\nb = np.arange(12).reshape(3, 4) print(\u0026#34;b =\\n{}\u0026#34;.format(b)) print(\u0026#34;b.sum(axis=0) = {}\u0026#34;.format(b.sum(axis=0))) # sum of each column print(\u0026#34;b.min(axis=1) = {}\u0026#34;.format(b.min(axis=1))) # min of each row print(\u0026#34;b.cumsum(axis=1) =\\n{}\u0026#34;.format(b.cumsum(axis=1))) # cumulative sum along each row b = [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] b.sum(axis=0) = [12 15 18 21] b.min(axis=1) = [0 4 8] b.cumsum(axis=1) = [[ 0 1 3 6] [ 4 9 15 22] [ 8 17 27 38]] Universal Functions NumPy provides familiar mathematical functions such as sin, cos, and exp. In NumPy, these are called “universal functions”(ufunc). Within NumPy, these functions operate elementwise on an array, producing an array as output.\nB = np.arange(3) print(\u0026#34;B = {}\u0026#34;.format(B)) print(\u0026#34;np.exp(B) = {}\u0026#34;.format(np.exp(B))) print(\u0026#34;np.sqrt(B) = {}\u0026#34;.format(np.sqrt(B))) C = np.array([2., -1., 4.]) print(\u0026#34;np.add(B, C) = {}\u0026#34;.format(np.add(B, C))) B = [0 1 2] np.exp(B) = [1. 2.71828183 7.3890561 ] np.sqrt(B) = [0. 1. 1.41421356] np.add(B, C) = [2. 0. 6.] Indexing, Slicing and Iterating One-dimensional arrays can be indexed, sliced and iterated over, much like lists and other Python sequences.\na = np.arange(10)**3 print(\u0026#34;a = {}\u0026#34;.format(a)) print(\u0026#34;a[2] = {}\u0026#34;.format(a[2])) print(\u0026#34;a[2:5] = {}\u0026#34;.format(a[2:5])) a[:6:2] = -1000 # equivalent to a[0:6:2] = -1000 # from start to position 6, exclusive, set every 2nd element to -1000 print(\u0026#34;a = {}\u0026#34;.format(a)) print(\u0026#34;a[ : :-1] = {}\u0026#34;.format(a[ : :-1])) # reversed a for i in a: print(i**(1/3.)) a = [ 0 1 8 27 64 125 216 343 512 729] a[2] = 8 a[2:5] = [ 8 27 64] a = [-1000 1 -1000 27 -1000 125 216 343 512 729] a[ : :-1] = [ 729 512 343 216 125 -1000 27 -1000 1 -1000] nan 1.0 nan 3.0 nan 4.999999999999999 5.999999999999999 6.999999999999999 7.999999999999999 8.999999999999998 /tmp/ipykernel_15700/4153428083.py:14: RuntimeWarning: invalid value encountered in power print(i**(1/3.)) Multidimensional arrays can have one index per axis. These indices are given in a tuple separated by commas:\ndef f(x, y): return 10 * x + y b = np.fromfunction(f, (5, 4), dtype=int) print(\u0026#34;b =\\n{}\u0026#34;.format(b)) print(\u0026#34;b[2, 3] = {}\u0026#34;.format(b[2, 3])) print(\u0026#34;b[0:5, 1] = {}\u0026#34;.format(b[0:5, 1])) # each row in the second column of b print(\u0026#34;b[ : , 1] = {}\u0026#34;.format(b[ : , 1])) # equivalent to the previous example print(\u0026#34;b[1:3, : ] =\\n{}\u0026#34;.format(b[1:3, : ])) # each column in the second and third row of b b = [[ 0 1 2 3] [10 11 12 13] [20 21 22 23] [30 31 32 33] [40 41 42 43]] b[2, 3] = 23 b[0:5, 1] = [ 1 11 21 31 41] b[ : , 1] = [ 1 11 21 31 41] b[1:3, : ] = [[10 11 12 13] [20 21 22 23]] When fewer indices are provided than the number of axes, the missing indices are considered complete slices:\nprint(\u0026#34;b[-1] = {}\u0026#34;.format(b[-1])) # the last row. Equivalent to b[-1, : ] b[-1] = [40 41 42 43] The expression within brackets in b[i] is treated as an i followed by as many instances of : as needed to represent the remaining axes. NumPy also allows you to write this using dots as b[i,...].\nThe dots (...) represent as many colons as needed to produce a complete indexing tuple. For example, if x is an array with 5 axes, then\nx[1,2,...] is equivalent to x[1,2,:,:,:], x[...,3] to x[:,:,:,:,3] and x[4,...,5,:] to x[4,:,:,5,:]. c = np.array([[[ 0, 1, 2], # a 3D array (two stacked 2D arrays) [ 10, 12, 13]], [[100,101,102], [110,112,113]]]) print(\u0026#34;c.shape = {}\u0026#34;.format(c.shape)) print(\u0026#34;c[1,...] =\\n{}\u0026#34;.format(c[1,...])) # same as c[1,:,:] or c[1] print(\u0026#34;c[...,2] =\\n{}\u0026#34;.format(c[...,2])) # same as c[:,:,2] c.shape = (2, 2, 3) c[1,...] = [[100 101 102] [110 112 113]] c[...,2] = [[ 2 13] [102 113]] Iterating over multidimensional arrays is done with respect to the first axis:\nfor row in b: print(row) [0 1 2 3] [10 11 12 13] [20 21 22 23] [30 31 32 33] [40 41 42 43] ","date":1694322000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694322000,"objectID":"731b4582036527f840208a4d0fa0a672","permalink":"https://ajdillhoff.github.io/notes/numpy_basics/","publishdate":"2023-09-10T00:00:00-05:00","relpermalink":"/notes/numpy_basics/","section":"notes","summary":"NumPy Quickstart This notebook is a quick introduction to NumPy. It is an interactive version of the NumPy Quickstart Tutorial.\nAll credits go to the original authors of the tutorial © Copyright 2008-2023, NumPy Developers.\nThe Basics NumPy’s main object is the homogeneous multidimensional array. It is a table of elements (usually numbers), all of the same type, indexed by a tuple of non-negative integers. In NumPy dimensions are called axes.","tags":["python"],"title":"NumPy: Basics","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" NumPy Quickstart This notebook is a quick introduction to NumPy. It is an interactive version of the NumPy Quickstart Tutorial.\nAll credits go to the original authors of the tutorial © Copyright 2008-2023, NumPy Developers.\nCopies and Views When operating and manipulating arrays, their data is sometimes copied into a new array and sometimes not. This is often a source of confusion for beginners. There are three cases:\nNo Copy at All Simple assignments make no copy of array objects or of their data.\nimport numpy as np a = np.array([[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]) b = a # no new object is created print(b is a) # a and b are two names for the same ndarray object True Python passes mutable objects as references, so function calls make no copy.\ndef f(x): print(id(x)) print(id(a)) # id is a unique identifier of an object f(a) # a is passed to the function under the name x 139846954913200 139846954913200 View or Shallow Copy Different array objects can share the same data. The view method creates a new array object that looks at the same data.\nc = a.view() # c is a view of the data owned by a print(\u0026#34;c is a = {}\u0026#34;.format(c is a)) print(\u0026#34;c.base is a = {}\u0026#34;.format(c.base is a)) # c is a view of the data owned by a print(\u0026#34;c.flags.owndata = {}\u0026#34;.format(c.flags.owndata)) # c does not own the data c = c.reshape((2, 6)) # a\u0026#39;s shape doesn\u0026#39;t change print(\u0026#34;a.shape = {}\u0026#34;.format(a.shape)) c[0, 4] = 1234 # a\u0026#39;s data changes print(\u0026#34;a =\\n{}\u0026#34;.format(a)) c is a = False c.base is a = True c.flags.owndata = False a.shape = (3, 4) a = [[ 0 1 2 3] [1234 5 6 7] [ 8 9 10 11]] Slicing an array returns a view of it:\ns = a[:, 1:3] s[:] = 10 # s[:] is a view of s. Note the difference between s=10 and s[:]=10 print(\u0026#34;a =\\n{}\u0026#34;.format(a)) a = [[ 0 10 10 3] [1234 10 10 7] [ 8 10 10 11]] Deep Copy The copy method makes a complete copy of the array and its data.\nd = a.copy() # a new array object with new data is created print(\u0026#34;d is a = {}\u0026#34;.format(d is a)) print(\u0026#34;d.base is a = {}\u0026#34;.format(d.base is a)) # d doesn\u0026#39;t share anything with a d[0, 0] = 9999 print(\u0026#34;a =\\n{}\u0026#34;.format(a)) d is a = False d.base is a = False a = [[ 0 10 10 3] [1234 10 10 7] [ 8 10 10 11]] Sometimes copy should be called after slicing if the original array is not required anymore. For example, suppose a is a huge intermediate result and the final result b only contains a small fraction of a, a deep copy should be made when constructing b with slicing:\na = np.arange(int(1e8)) b = a[:100].copy() del a # the memory of ``a`` can be released. If b = a[:100] is used instead, a is referenced by b and will persist in memory even if del a is executed.\nFunctions and Methods Overview See Routines for the full list of routines available in NumPy.\n","date":1694322000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694322000,"objectID":"ee88950d9dda019a9121368dfb64ca91","permalink":"https://ajdillhoff.github.io/notes/numpy_copies_and_views/","publishdate":"2023-09-10T00:00:00-05:00","relpermalink":"/notes/numpy_copies_and_views/","section":"notes","summary":"NumPy Quickstart This notebook is a quick introduction to NumPy. It is an interactive version of the NumPy Quickstart Tutorial.\nAll credits go to the original authors of the tutorial © Copyright 2008-2023, NumPy Developers.\nCopies and Views When operating and manipulating arrays, their data is sometimes copied into a new array and sometimes not. This is often a source of confusion for beginners. There are three cases:\nNo Copy at All Simple assignments make no copy of array objects or of their data.","tags":["python"],"title":"NumPy: Copies and Views","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" # `NumPy` Quickstart This notebook is a quick introduction to NumPy. It is an interactive version of the NumPy Quickstart Tutorial.\nAll credits go to the original authors of the tutorial © Copyright 2008-2023, NumPy Developers.\nShape Manipulation Changing the shape of an array An array has a shape given by the number of elements along each axis:\nimport numpy as np a = np.floor(10 * np.random.random((3,4))) print(\u0026#34;a = \\n{}\u0026#34;.format(a)) print(\u0026#34;a.shape = {}\u0026#34;.format(a.shape)) a = [[6. 7. 2. 1.] [5. 9. 4. 9.] [3. 5. 3. 2.]] a.shape = (3, 4) The shape of an array can be changed with various commands. Note that the following three commands all return a modified array, but do not change the original array:\nprint(\u0026#34;a.ravel() = {}\u0026#34;.format(a.ravel())) # flatten the array print(\u0026#34;a.reshape(6,2) = \\n{}\u0026#34;.format(a.reshape(6,2))) # reshape the array print(\u0026#34;a.T = \\n{}\u0026#34;.format(a.T)) # transpose the array print(\u0026#34;a.T.shape = {}\u0026#34;.format(a.T.shape)) a.ravel() = [6. 7. 2. 1. 5. 9. 4. 9. 3. 5. 3. 2.] a.reshape(6,2) = [[6. 7.] [2. 1.] [5. 9.] [4. 9.] [3. 5.] [3. 2.]] a.T = [[6. 5. 3.] [7. 9. 5.] [2. 4. 3.] [1. 9. 2.]] a.T.shape = (4, 3) The order of the elements in the array resulting from ravel is normally “C-style”, that is, the rightmost index “changes the fastest”, so the element after a[0,0] is a[0,1]. If the array is reshaped to some other shape, again the array is treated as “C-style”. NumPy normally creates arrays stored in this order, so ravel will usually not need to copy its argument, but if the array was made by taking slices of another array or created with unusual options, it may need to be copied. The functions ravel and reshape can also be instructed, using an optional argument, to use FORTRAN-style arrays, in which the leftmost index changes the fastest.\nThe reshape function returns its argument with a modified shape, whereas the ndarray.resize method modifies the array itself:\na.resize((2,6)) # resize the array print(\u0026#34;a.resize((2,6)) = \\n{}\u0026#34;.format(a)) a.resize((2,6)) = [[6. 7. 2. 1. 5. 9.] [4. 9. 3. 5. 3. 2.]] If a dimension is given as -1 in a reshaping operation, the other dimensions are automatically calculated:\nprint(\u0026#34;a.reshape(3, -1) = \\n{}\u0026#34;.format(a.reshape(3, -1))) a.reshape(3, -1) = [[6. 7. 2. 1.] [5. 9. 4. 9.] [3. 5. 3. 2.]] Stacking together different arrays Several arrays can be stacked together along different axes:\na = np.floor(10 * np.random.random((2,2))) print(\u0026#34;a = \\n{}\u0026#34;.format(a)) b = np.floor(10 * np.random.random((2,2))) print(\u0026#34;b = \\n{}\u0026#34;.format(b)) print(\u0026#34;np.vstack((a,b)) = \\n{}\u0026#34;.format(np.vstack((a,b)))) print(\u0026#34;np.hstack((a,b)) = \\n{}\u0026#34;.format(np.hstack((a,b)))) a = [[8. 0.] [5. 1.]] b = [[7. 7.] [0. 1.]] np.vstack((a,b)) = [[8. 0.] [5. 1.] [7. 7.] [0. 1.]] np.hstack((a,b)) = [[8. 0. 7. 7.] [5. 1. 0. 1.]] The function column_stack stacks 1D arrays as columns into a 2D array. It is equivalent to hstack only for 2D arrays:\nfrom numpy import newaxis print(\u0026#34;np.column_stack((a,b)) = \\n{}\u0026#34;.format(np.column_stack((a,b)))) # with 2D arrays a = np.array([4.,2.]) b = np.array([3.,8.]) print(\u0026#34;np.column_stack((a,b)) = \\n{}\u0026#34;.format(np.column_stack((a,b)))) # returns a 2D array print(\u0026#34;np.hstack((a,b)) = \\n{}\u0026#34;.format(np.hstack((a,b)))) # returns a 1D array print(\u0026#34;a[:,newaxis] = \\n{}\u0026#34;.format(a[:,newaxis])) # view a as a 2D column vector print(\u0026#34;np.column_stack((a[:,newaxis],b[:,newaxis])) = \\n{}\u0026#34;.format(np.column_stack((a[:,newaxis],b[:,newaxis])))) print(\u0026#34;np.hstack((a[:,newaxis],b[:,newaxis])) = \\n{}\u0026#34;.format(np.hstack((a[:,newaxis],b[:,newaxis])))) np.column_stack((a,b)) = [[4. 3.] [2. 8.]] np.column_stack((a,b)) = [[4. 3.] [2. 8.]] np.hstack((a,b)) = [4. 2. 3. 8.] a[:,newaxis] = [[4.] [2.]] np.column_stack((a[:,newaxis],b[:,newaxis])) = [[4. 3.] [2. 8.]] np.hstack((a[:,newaxis],b[:,newaxis])) = [[4. 3.] [2. 8.]] In general, for arrays of with more than two dimensions, hstack stacks along their second axes, vstack stacks along their first axes, and concatenate allows for an optional arguments giving the number of the axis along which the concatenation should happen.\nNote\nIn complex cases, r_ and c_ are useful for creating arrays by stacking numbers along one axis. They allow the use of range literals :\nprint(\u0026#34;np.r_[1:4,0,4] = \\n{}\u0026#34;.format(np.r_[1:4,0,4])) # concatenate along the first axis np.r_[1:4,0,4] = [1 2 3 0 4] When used with arrays as arguments, r_ and c_ are similar to vstack and hstack in their default behavior, but allow for an optional argument giving the number of the axis along which to concatenate.\nSplitting one array into several smaller ones Using hsplit, you can split an array along its horizontal axis, either by specifying the number of equally shaped arrays to return, or by specifying the columns after which the division should occur:\na = np.floor(10 * np.random.random((2,12))) print(\u0026#34;a = \\n{}\u0026#34;.format(a)) # split a into 3 print(\u0026#34;np.hsplit(a,3) = \\n{}\u0026#34;.format(np.hsplit(a,3))) # split a after the third and the fourth column print(\u0026#34;np.hsplit(a,(3,4)) = \\n{}\u0026#34;.format(np.hsplit(a,(3,4)))) a = [[7. 0. 8. 1. 4. 7. 8. 1. 1. 8. 1. 2.] [6. 7. 7. 5. 7. 5. 0. 7. 7. 4. 6. 1.]] np.hsplit(a,3) = [array([[7., 0., 8., 1.], [6., 7., 7., 5.]]), array([[4., 7., 8., 1.], [7., 5., 0., 7.]]), array([[1., 8., 1., 2.], [7., 4., 6., 1.]])] np.hsplit(a,(3,4)) = [array([[7., 0., 8.], [6., 7., 7.]]), array([[1.], [5.]]), array([[4., 7., 8., 1., 1., 8., 1., 2.], [7., 5., 0., 7., 7., 4., 6., 1.]])] vsplit splits along the vertical axis, and array_split allows one to specify along which axis to split.\n","date":1694322000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694322000,"objectID":"9a635cd092ed235fef43b647c1c954e6","permalink":"https://ajdillhoff.github.io/notes/numpy_shape_manipulation/","publishdate":"2023-09-10T00:00:00-05:00","relpermalink":"/notes/numpy_shape_manipulation/","section":"notes","summary":"# `NumPy` Quickstart This notebook is a quick introduction to NumPy. It is an interactive version of the NumPy Quickstart Tutorial.\nAll credits go to the original authors of the tutorial © Copyright 2008-2023, NumPy Developers.\nShape Manipulation Changing the shape of an array An array has a shape given by the number of elements along each axis:\nimport numpy as np a = np.floor(10 * np.random.random((3,4))) print(\u0026#34;a = \\n{}\u0026#34;.","tags":["python"],"title":"NumPy: Shape Manipulation","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Loading Data for ML Applications In this notebook, we will implement code to load data for ML applications. Following the approach used by PyTorch, we will implement a Dataset class and a DataLoader class. The Dataset class will be used to load the data and the DataLoader class will be used to iterate over the data in batches.\nWe will test it on a simple image dataset.\nThe Dataset Class First, we need some way of representing how each individual item will be pre-processed as the dataloader iterates over the data. We will do this by creating a Dataset class. Since this class represents multiple items in our dataset, we will need to define the following special methods:\n__len__: returns the length of the dataset __getitem__: returns the item at a given index The Intel Image Classification Dataset We will use the Intel Image Classification Dataset from Kaggle. This dataset contains images of natural scenes around the world. The dataset contains 25,000 images of size 150x150 distributed under 6 categories. The dataset is divided into a training set and a test set. The training set contains 14k images. The images are organized under folder representing each category. When initializing our dataset, it should iterate through the folders to enumerate all the images and their corresponding labels.\nimport os class IntelDataset: def __init__(self, data_path): self.data_path = data_path self.classes, self.class_to_idx = self._find_classes(self.data_path) self.samples = self._make_dataset(self.data_path, self.class_to_idx) def __len__(self): return len(self.samples) def __getitem__(self, idx): raise NotImplementedError def _find_classes(self, path): \u0026#34;\u0026#34;\u0026#34;Summarized from torchvision.datasets.ImageFolder\u0026#34;\u0026#34;\u0026#34; classes = [d.name for d in os.scandir(path) if d.is_dir()] classes.sort() class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)} return classes, class_to_idx def _make_dataset(self, dir, class_to_idx): \u0026#34;\u0026#34;\u0026#34;Summarized from torchvision.datasets.ImageFolder\u0026#34;\u0026#34;\u0026#34; images = [] dir = os.path.expanduser(dir) for target in sorted(os.listdir(dir)): d = os.path.join(dir, target) if not os.path.isdir(d): continue for root, _, fnames in sorted(os.walk(d)): for fname in sorted(fnames): path = os.path.join(root, fname) item = (path, class_to_idx[target]) images.append(item) return images Here is the first iteration of our dataset. Let\u0026rsquo;s test it on the Intel Image Classification dataset available here: https://www.kaggle.com/datasets/puneet6060/intel-image-classification.\nWe will use the training samples to test our Dataset class. There are 14,000 images in the training set over 6 categories. Each category is contained in its own subfolder.\ndataset = IntelDataset(data_path=\u0026#39;data/seg_train\u0026#39;) print(dataset.classes) print(dataset.class_to_idx) print(len(dataset)) # Sample the first 5 items for i in range(5): print(dataset.samples[i]) ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street'] {'buildings': 0, 'forest': 1, 'glacier': 2, 'mountain': 3, 'sea': 4, 'street': 5} 14034 ('data/seg_train/buildings/0.jpg', 0) ('data/seg_train/buildings/10006.jpg', 0) ('data/seg_train/buildings/1001.jpg', 0) ('data/seg_train/buildings/10014.jpg', 0) ('data/seg_train/buildings/10018.jpg', 0) Loading the Images Now that we can successfully initialize the dataset, we can implement __getitem__ to load the actual image and return a tuple of the image and its label. We will use the PIL library to load the image and convert it to a numpy array.\nimport os import numpy as np from PIL import Image import matplotlib.pyplot as plt class IntelDataset: def __init__(self, data_path): self.data_path = data_path self.classes, self.class_to_idx = self._find_classes(self.data_path) self.samples = self._make_dataset(self.data_path, self.class_to_idx) def __len__(self): return len(self.samples) def __getitem__(self, idx): path, target = self.samples[idx] image = Image.open(path).convert(\u0026#39;RGB\u0026#39;) image = np.array(image) return image, target def _find_classes(self, path): \u0026#34;\u0026#34;\u0026#34;Summarized from torchvision.datasets.ImageFolder\u0026#34;\u0026#34;\u0026#34; classes = [d.name for d in os.scandir(path) if d.is_dir()] classes.sort() class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)} return classes, class_to_idx def _make_dataset(self, dir, class_to_idx): \u0026#34;\u0026#34;\u0026#34;Summarized from torchvision.datasets.ImageFolder\u0026#34;\u0026#34;\u0026#34; images = [] dir = os.path.expanduser(dir) for target in sorted(os.listdir(dir)): d = os.path.join(dir, target) if not os.path.isdir(d): continue for root, _, fnames in sorted(os.walk(d)): for fname in sorted(fnames): path = os.path.join(root, fname) item = (path, class_to_idx[target]) images.append(item) return images # Sample 9 random images and display them with their labels dataset = IntelDataset(data_path=\u0026#39;data/seg_train\u0026#39;) fig, axes = plt.subplots(3, 3, figsize=(10, 10)) for i in range(3): for j in range(3): idx = np.random.randint(len(dataset)) image, target = dataset[idx] axes[i, j].imshow(image) axes[i, j].set_title(dataset.classes[target]) axes[i, j].axis(\u0026#39;off\u0026#39;) plt.show() The DataLoader Class Now that we have a Dataset to manage loading each individual item, we need to create a DataLoader that is responsible for iterating over the dataset in batches.\nclass DataLoader: def __init__(self, dataset, batch_size, shuffle=False): self.dataset = dataset self.batch_size = batch_size self.shuffle = shuffle def __len__(self): return len(self.dataset) // self.batch_size def __iter__(self): if self.shuffle: indices = np.random.permutation(len(self.dataset)) else: indices = np.arange(len(self.dataset)) for i in range(len(self)): batch_indices = indices[i*self.batch_size:(i+1)*self.batch_size] batch = [self.dataset[idx] for idx in batch_indices] images, targets = zip(*batch) images = np.stack(images, axis=0) targets = np.stack(targets, axis=0) yield images, targets In this example, the __iter__ uses Python generators to yield a batch of data instead of overriding the __next__ method.\nLet\u0026rsquo;s test this with our dataset.\ndataset = IntelDataset(data_path=\u0026#39;data/seg_train\u0026#39;) dataloader = DataLoader(dataset, batch_size=32, shuffle=False) # Print the first 5 batches for i, (images, targets) in enumerate(dataloader): print(f\u0026#39;Batch {i} images shape: {images.shape}\u0026#39;) print(f\u0026#39;Batch {i} targets shape: {targets.shape}\u0026#39;) if i == 5: break Batch 0 images shape: (32, 150, 150, 3) Batch 0 targets shape: (32,) Batch 1 images shape: (32, 150, 150, 3) Batch 1 targets shape: (32,) Batch 2 images shape: (32, 150, 150, 3) Batch 2 targets shape: (32,) Batch 3 images shape: (32, 150, 150, 3) Batch 3 targets shape: (32,) Batch 4 images shape: (32, 150, 150, 3) Batch 4 targets shape: (32,) Batch 5 images shape: (32, 150, 150, 3) Batch 5 targets shape: (32,) Cleaning the Data Our data loader failed because there are some images that are not $150 \\times 150$. We need to figure out exactly what to do with those images. We can either remove them or resize them so that they are all the same size. We will resize them to $150 \\times 150$. This will be easiest to do in the __getitem__ method of our Dataset class.\nimport os import numpy as np from PIL import Image import matplotlib.pyplot as plt class IntelDataset: def __init__(self, data_path): self.data_path = data_path self.classes, self.class_to_idx = self._find_classes(self.data_path) self.samples = self._make_dataset(self.data_path, self.class_to_idx) def __len__(self): return len(self.samples) def __getitem__(self, idx): path, target = self.samples[idx] image = Image.open(path).convert(\u0026#39;RGB\u0026#39;) # Resize if the image is not 150x150 if image.size != (150, 150): image = image.resize((150, 150)) image = np.array(image) return image, target def _find_classes(self, path): \u0026#34;\u0026#34;\u0026#34;Summarized from torchvision.datasets.ImageFolder\u0026#34;\u0026#34;\u0026#34; classes = [d.name for d in os.scandir(path) if d.is_dir()] classes.sort() class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)} return classes, class_to_idx def _make_dataset(self, dir, class_to_idx): \u0026#34;\u0026#34;\u0026#34;Summarized from torchvision.datasets.ImageFolder\u0026#34;\u0026#34;\u0026#34; images = [] dir = os.path.expanduser(dir) for target in sorted(os.listdir(dir)): d = os.path.join(dir, target) if not os.path.isdir(d): continue for root, _, fnames in sorted(os.walk(d)): for fname in sorted(fnames): path = os.path.join(root, fname) item = (path, class_to_idx[target]) images.append(item) return images dataset = IntelDataset(data_path=\u0026#39;data/seg_train\u0026#39;) dataloader = DataLoader(dataset, batch_size=32, shuffle=False) # Verify that our fix works! # Only print the first 5 batches for i, (images, targets) in enumerate(dataloader): print(f\u0026#39;Batch {i} images shape: {images.shape}\u0026#39;) print(f\u0026#39;Batch {i} targets shape: {targets.shape}\u0026#39;) if i == 5: break Batch 0 images shape: (32, 150, 150, 3) Batch 0 targets shape: (32,) Batch 1 images shape: (32, 150, 150, 3) Batch 1 targets shape: (32,) Batch 2 images shape: (32, 150, 150, 3) Batch 2 targets shape: (32,) Batch 3 images shape: (32, 150, 150, 3) Batch 3 targets shape: (32,) Batch 4 images shape: (32, 150, 150, 3) Batch 4 targets shape: (32,) Batch 5 images shape: (32, 150, 150, 3) Batch 5 targets shape: (32,) ","date":1693803600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693803600,"objectID":"2c3a8997dfc68ddc5c5418e348822690","permalink":"https://ajdillhoff.github.io/notes/dataloader/","publishdate":"2023-09-04T00:00:00-05:00","relpermalink":"/notes/dataloader/","section":"notes","summary":"Loading Data for ML Applications In this notebook, we will implement code to load data for ML applications. Following the approach used by PyTorch, we will implement a Dataset class and a DataLoader class. The Dataset class will be used to load the data and the DataLoader class will be used to iterate over the data in batches.\nWe will test it on a simple image dataset.\nThe Dataset Class First, we need some way of representing how each individual item will be pre-processed as the dataloader iterates over the data.","tags":["python","examples"],"title":"Example: Writing a Data Loader in Python","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Classes When a class is defined, a namespace is created for it. All assignments to local variables are part of this namespace. The code below defines a class, creates an instance of the class, and calls a method on the instance.\nclass Shape(): \u0026#34;\u0026#34;\u0026#34;Represents any shape.\u0026#34;\u0026#34;\u0026#34; def __init__(self, color): self.color = color self.orientation = 0.0 def rotate(self, angle): self.orientation += angle s = Shape(\u0026#34;red\u0026#34;) s.rotate(45.0) print(s.orientation) 45.0 Class and Instance Variables The class above has two instance variables, color and orientation. These variables are accessed using the self keyword. The self keyword is used to access instance variables and methods.\nClasses can also have class variables that are accessible, and shared, by all instances of the class. Let\u0026rsquo;s add a class variable to the Shape class.\nPrivate Variables Python does not have a formal mechanism for describing a private variable. You can still create them using naming conventions. A common approach to creating private variables is to prefix each identifier with double underscores. If we wanted to make the orientation variable private, we would rename it to __orientation, for example.\nclass Shape(): \u0026#34;\u0026#34;\u0026#34;Represents any shape.\u0026#34;\u0026#34;\u0026#34; max_area = 100.0 def __init__(self, color): self.color = color self.orientation = 0.0 def rotate(self, angle): self.orientation += angle s = Shape(\u0026#34;red\u0026#34;) s.rotate(45.0) r = Shape(\u0026#34;blue\u0026#34;) print(s.orientation) print(\u0026#34;Maximum area for a shape:\u0026#34;, Shape.max_area) 45.0 Maximum area for a shape: 100.0 Special Methods We already saw one special method, __init__(), that serves as our constructor for a class. There are several others that are useful for customizing our classes. They are\n__str__(): called when str() is called on an instance of the class __repr__(): called when repr() is called on an instance of the class __len__(): called when len() is called on an instance of the class __add__(): called when + is used on two instances of the class __eq__(): called when == is used on two instances of the class __lt__(): called when \u0026lt; is used on two instances of the class __gt__(): called when \u0026gt; is used on two instances of the class __le__(): called when \u0026lt;= is used on two instances of the class __ge__(): called when \u0026gt;= is used on two instances of the class __ne__(): called when != is used on two instances of the class __hash__(): called when hash() is called on an instance of the class __bool__(): called when bool() is called on an instance of the class Let\u0026rsquo;s modify the Shape class to add a few of these methods. We will also add an area attribute so that we can override the comparison operators.\nclass Shape(): \u0026#34;\u0026#34;\u0026#34;Represents any shape.\u0026#34;\u0026#34;\u0026#34; max_area = 100.0 def __init__(self, color, area): self.color = color self.orientation = 0.0 self.area = area def rotate(self, angle): self.orientation += angle def __eq__(self, other): return self.area == other.area def __lt__(self, other): return self.area \u0026lt; other.area def __gt__(self, other): return self.area \u0026gt; other.area def __le__(self, other): return self.area \u0026lt;= other.area def __ge__(self, other): return self.area \u0026gt;= other.area def __ne__(self, other): return self.area != other.area def __str__(self): return \u0026#34;Shape, color: {0}, area: {1}\u0026#34;.format(self.color, self.area) s1 = Shape(\u0026#34;red\u0026#34;, 10.0) s2 = Shape(\u0026#34;blue\u0026#34;, 20.0) print(\u0026#34;s1 == s2:\u0026#34;, s1 == s2) print(\u0026#34;s1 != s2:\u0026#34;, s1 != s2) print(\u0026#34;s1 \u0026lt; s2:\u0026#34;, s1 \u0026lt; s2) print(\u0026#34;s1 \u0026gt; s2:\u0026#34;, s1 \u0026gt; s2) print(\u0026#34;s1 \u0026lt;= s2:\u0026#34;, s1 \u0026lt;= s2) print(\u0026#34;s1 \u0026gt;= s2:\u0026#34;, s1 \u0026gt;= s2) print(s1) print(s2) s1 == s2: False s1 != s2: True s1 \u0026lt; s2: True s1 \u0026gt; s2: False s1 \u0026lt;= s2: True s1 \u0026gt;= s2: False Shape, color: red, area: 10.0 Shape, color: blue, area: 20.0 Since we have defined the \u0026lt; operator, list.sort() can sort our shapes. If the __lt__() operator was not defined, list.sort() would use the __gt__() operator. If neither are defined, attemping to sort would result in an error. Let\u0026rsquo;s add a few more and verify this.\nimport random colors = [\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;green\u0026#34;, \u0026#34;yellow\u0026#34;, \u0026#34;black\u0026#34;, \u0026#34;white\u0026#34;] # Generate 10 shapes with random colors and areas shapes = [] for i in range(10): color = random.choice(colors) area = random.uniform(0.0, 100.0) shapes.append(Shape(color, area)) # Print the shapes, sorted by area for shape in sorted(shapes): print(shape) Shape, color: red, area: 13.697816464863 Shape, color: white, area: 42.56718610585648 Shape, color: white, area: 47.443134198872464 Shape, color: white, area: 53.85070279838825 Shape, color: yellow, area: 66.78631236791435 Shape, color: green, area: 70.55065950752918 Shape, color: blue, area: 73.19818592952365 Shape, color: white, area: 74.03228452807117 Shape, color: black, area: 86.72544463003362 Shape, color: red, area: 94.59245601130148 Inheritance Inheritance allows us to create a specialized version of another class. Generally, this means that our specialized class has access to the methods and instance variables of the parent class. Let\u0026rsquo;s create a Circle and Square that inherit from shape. Their areas will be calculated based on their properties.\nimport math class Shape(): \u0026#34;\u0026#34;\u0026#34;Represents any shape.\u0026#34;\u0026#34;\u0026#34; max_area = 100.0 def __init__(self, color): self.color = color self.orientation = 0.0 def rotate(self, angle): self.orientation += angle def __eq__(self, other): return self.area == other.area def __lt__(self, other): return self.area \u0026lt; other.area def __gt__(self, other): return self.area \u0026gt; other.area def __le__(self, other): return self.area \u0026lt;= other.area def __ge__(self, other): return self.area \u0026gt;= other.area def __ne__(self, other): return self.area != other.area def __str__(self): return \u0026#34;Shape, color: {0}, area: {1}\u0026#34;.format(self.color, self.area) class Circle(Shape): \u0026#34;\u0026#34;\u0026#34;Represents a circle.\u0026#34;\u0026#34;\u0026#34; def __init__(self, color, radius): Shape.__init__(self, color) self.radius = radius self.area = self.get_area() def __str__(self): return \u0026#34;Circle, color: {0}, area: {1}, radius: {2}\u0026#34;.format(self.color, self.area, self.radius) def get_area(self): return 2 * math.pi * self.radius ** 2 class Rectangle(Shape): \u0026#34;\u0026#34;\u0026#34;Represents a rectangle.\u0026#34;\u0026#34;\u0026#34; def __init__(self, color, width, height): Shape.__init__(self, color) self.width = width self.height = height self.area = self.get_area() def __str__(self): return \u0026#34;Rectangle, color: {0}, area: {1}, width: {2}, height: {3}\u0026#34;.format(self.color, self.area, self.width, self.height) def get_area(self): return self.width * self.height shape_classes = [Rectangle, Circle] colors = [\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;green\u0026#34;, \u0026#34;yellow\u0026#34;, \u0026#34;black\u0026#34;, \u0026#34;white\u0026#34;] # Generate 10 shapes with random colors and areas shapes = [] for i in range(10): color = random.choice(colors) shape_class = random.choice(shape_classes) if shape_class == Rectangle: width = random.uniform(0.0, math.sqrt(Shape.max_area)) height = random.uniform(0.0, math.sqrt(Shape.max_area)) shape = Rectangle(color, width, height) else: radius = random.uniform(0.0, math.sqrt(Shape.max_area / (2 * math.pi))) shape = Circle(color, radius) shapes.append(shape) # Print the shapes, sorted by area for shape in sorted(shapes): print(shape) Circle, color: blue, area: 0.5110628296555956, radius: 0.2851984845159934 Rectangle, color: yellow, area: 0.6186484099066036, width: 0.2740689854907352, height: 2.25727259433927 Circle, color: white, area: 5.867493292628993, radius: 0.9663542627217231 Circle, color: green, area: 18.28116762721217, radius: 1.7057368476298893 Rectangle, color: blue, area: 20.114023003818147, width: 5.075190544004695, height: 3.963205485472614 Rectangle, color: blue, area: 23.171307446004665, width: 2.586024349725701, height: 8.960204666465124 Circle, color: red, area: 42.43901187799147, radius: 2.598918721375873 Rectangle, color: white, area: 45.198912747710224, width: 9.793600740960953, height: 4.615147578833736 Circle, color: yellow, area: 47.991651978311594, radius: 2.7637128359318064 Rectangle, color: green, area: 83.54726788281138, width: 9.643204828660805, height: 8.66384872739595 global and nonlocal keywords The global keyword is used to declare an identifier that can be used for the entire code block. This is useful when we want to use a variable in a function that is defined outside of the function.\nx = 1 def f(): global x # global keyword is used to access a global variable from a function x = 2 f() print(x) 2 The nonlocal keyword is used to declare an identifier that is defined in the nearest enclosing scope. This is useful when we want to use a variable in a nested function that is defined outside of the nested function.\ndef f(): x = 1 def g(): nonlocal x x = 2 g() print(x) f() 2 The following example from the official Python docs shows the relationship between global, local, and nonlocal variables.\ndef scope_test(): def do_local(): spam = \u0026#34;local spam\u0026#34; def do_nonlocal(): nonlocal spam spam = \u0026#34;nonlocal spam\u0026#34; def do_global(): global spam spam = \u0026#34;global spam\u0026#34; spam = \u0026#34;test spam\u0026#34; do_local() print(\u0026#34;After local assignment:\u0026#34;, spam) do_nonlocal() print(\u0026#34;After nonlocal assignment:\u0026#34;, spam) do_global() print(\u0026#34;After global assignment:\u0026#34;, spam) scope_test() print(\u0026#34;In global scope:\u0026#34;, spam) After local assignment: test spam After nonlocal assignment: nonlocal spam After global assignment: nonlocal spam In global scope: global spam The unexpected result here is that spam is still equal to nonlocal even though it was changed in do_global by declaring global spam. When declaring something as nonlocal, the variable must already exist in the enclosing namespace. The declaration of global spam created a new instance of spam in the global namespace.\nThe example below shows how local, nonlocal, and global variables work in the context of classes.\nclass User: \u0026#34;\u0026#34;\u0026#34;Represents a user.\u0026#34;\u0026#34;\u0026#34; def __init__(self, id, name, password): self.id = id self.name = name self.password = password self.domain = \u0026#34;unknown\u0026#34; def __str__(self): return \u0026#34;User: {0}, id: {1}\u0026#34;.format(self.name, self.id) def global_login(self): global domain self.domain = domain # def nonlocal_login(self): # nonlocal domain # domain = \u0026#34;compuserve.net\u0026#34; def nonlocal_login(self): domain = \u0026#34;compuserve.net\u0026#34; def set_domain(): nonlocal domain self.domain = domain set_domain() def local_login(self): self.domain = \u0026#34;tx.rr.com\u0026#34; domain = \u0026#34;gmail.com\u0026#34; u = User(1, \u0026#34;John\u0026#34;, \u0026#34;password\u0026#34;) u.global_login() print(u.domain) u.nonlocal_login() print(u.domain) u.local_login() print(u.domain) gmail.com compuserve.net tx.rr.com Data Classes Sometimes in our work, we may want to represent a simple class consisting only of attributes, similar to a struct in C. Python provides a way to do this using the dataclass decorator. The dataclass decorator will automatically generate a constructor, __repr__(), and __eq__() method for us. The follow example shows how to implement such a class.\nfrom dataclasses import dataclass @dataclass class Product: \u0026#34;\u0026#34;\u0026#34;Represents a product.\u0026#34;\u0026#34;\u0026#34; id: int name: str price: float quantity: int = 0 def __str__(self): return \u0026#34;Product: {0}, id: {1}\u0026#34;.format(self.name, self.id) # Let\u0026#39;s create a list of graphics cards and list them products = [] products.append(Product(1, \u0026#34;GeForce RTX 2080 Ti\u0026#34;, 1200.0)) products.append(Product(2, \u0026#34;GeForce RTX 2080\u0026#34;, 800.0)) products.append(Product(3, \u0026#34;GeForce RTX 2070\u0026#34;, 600.0)) products.append(Product(4, \u0026#34;GeForce RTX 2060\u0026#34;, 350.0)) products.append(Product(5, \u0026#34;GeForce GTX 1660 Ti\u0026#34;, 275.0)) products.append(Product(6, \u0026#34;GeForce GTX 1660\u0026#34;, 200.0)) products.append(Product(7, \u0026#34;GeForce GTX 1650\u0026#34;, 150.0)) products.append(Product(8, \u0026#34;GeForce GTX 1080 Ti\u0026#34;, 800.0)) products.append(Product(9, \u0026#34;GeForce GTX 1080\u0026#34;, 500.0)) products.append(Product(10, \u0026#34;GeForce GTX 1070 Ti\u0026#34;, 450.0)) for product in products: print(product) Product: GeForce RTX 2080 Ti, id: 1 Product: GeForce RTX 2080, id: 2 Product: GeForce RTX 2070, id: 3 Product: GeForce RTX 2060, id: 4 Product: GeForce GTX 1660 Ti, id: 5 Product: GeForce GTX 1660, id: 6 Product: GeForce GTX 1650, id: 7 Product: GeForce GTX 1080 Ti, id: 8 Product: GeForce GTX 1080, id: 9 Product: GeForce GTX 1070 Ti, id: 10 Iterators We have used iterator objects, like a list, in previous examples. When defining our own custom classes, we can also define them as iterators. To do this, we need to implement the __iter__() and __next__() methods. The __iter__() method should return the iterator object itself. The __next__() method should return the next item in the sequence. When there are no more items in the sequence, __next__() should raise a StopIteration exception.\nThis will be useful for our final example, where we will implement a dataloader for a machine learning application. To illusterate iterators with a simpler example, we need to justify the need to implement our own iterator. If we create something simple that iterates over a simple list of objects, why not just use the list itself?\nLet\u0026rsquo;s create a class that represents a 3D object. A 3D object has a list of vertices and a list of faces. Each face is a list of indices into the list of vertices. We will create a class that represents a 3D object. Our iterator for this class will iterator over the faces of the object, returning the vertices that make up each face.\nclass Model: \u0026#34;\u0026#34;\u0026#34;Represents a 3D model.\u0026#34;\u0026#34;\u0026#34; def __init__(self, vertices, faces): self.vertices = vertices self.faces = faces self.index = 0 def __str__(self): return \u0026#34;{} vertices, {} faces\u0026#34;.format(len(self.vertices), len(self.faces)) def __len__(self): return len(self.faces) def __iter__(self): return self def __next__(self): if self.index \u0026gt;= len(self.faces): raise StopIteration face = self.faces[self.index] vertices = [] for vertex_index in face: vertices.append(self.vertices[vertex_index]) self.index += 1 return vertices def __getitem__(self, key): vertices = [] for vertex_index in self.faces[key]: vertices.append(self.vertices[vertex_index]) return vertices # Create a cube model vertices = [(0.0, 0.0, 0.0), (1.0, 0.0, 0.0), (1.0, 1.0, 0.0), (0.0, 1.0, 0.0), (0.0, 0.0, 1.0), (1.0, 0.0, 1.0), (1.0, 1.0, 1.0), (0.0, 1.0, 1.0)] faces = [(0, 1, 2, 3), (1, 5, 6, 2), (5, 4, 7, 6), (4, 0, 3, 7), (3, 2, 6, 7), (4, 5, 1, 0)] cube = Model(vertices, faces) # Iterator over the model for face in cube: print(\u0026#34;Face {}: {}\u0026#34;.format(cube.index, face)) Face 1: [(0.0, 0.0, 0.0), (1.0, 0.0, 0.0), (1.0, 1.0, 0.0), (0.0, 1.0, 0.0)] Face 2: [(1.0, 0.0, 0.0), (1.0, 0.0, 1.0), (1.0, 1.0, 1.0), (1.0, 1.0, 0.0)] Face 3: [(1.0, 0.0, 1.0), (0.0, 0.0, 1.0), (0.0, 1.0, 1.0), (1.0, 1.0, 1.0)] Face 4: [(0.0, 0.0, 1.0), (0.0, 0.0, 0.0), (0.0, 1.0, 0.0), (0.0, 1.0, 1.0)] Face 5: [(0.0, 1.0, 0.0), (1.0, 1.0, 0.0), (1.0, 1.0, 1.0), (0.0, 1.0, 1.0)] Face 6: [(0.0, 0.0, 1.0), (1.0, 0.0, 1.0), (1.0, 0.0, 0.0), (0.0, 0.0, 0.0)] Generators Generators provide a much cleaner way to implement iterators. Instead of implementing the __iter__() and __next__() methods, we can use the yield keyword. The yield keyword is used to return a value from a generator. The generator will remember its place in the sequence and return the next value when next() is called on it.\nSince our class already included __getitem__(), we can use the yield keyword to implement our iterator.\ndef get_next_face(model): for face in range(len(model)): yield model[face] # Generator function for face in get_next_face(cube): print(face) [(0.0, 0.0, 0.0), (1.0, 0.0, 0.0), (1.0, 1.0, 0.0), (0.0, 1.0, 0.0)] [(1.0, 0.0, 0.0), (1.0, 0.0, 1.0), (1.0, 1.0, 1.0), (1.0, 1.0, 0.0)] [(1.0, 0.0, 1.0), (0.0, 0.0, 1.0), (0.0, 1.0, 1.0), (1.0, 1.0, 1.0)] [(0.0, 0.0, 1.0), (0.0, 0.0, 0.0), (0.0, 1.0, 0.0), (0.0, 1.0, 1.0)] [(0.0, 1.0, 0.0), (1.0, 1.0, 0.0), (1.0, 1.0, 1.0), (0.0, 1.0, 1.0)] [(0.0, 0.0, 1.0), (1.0, 0.0, 1.0), (1.0, 0.0, 0.0), (0.0, 0.0, 0.0)] ","date":1693803600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693803600,"objectID":"26577078c16291b5ad773dc839b174fc","permalink":"https://ajdillhoff.github.io/notes/oop/","publishdate":"2023-09-04T00:00:00-05:00","relpermalink":"/notes/oop/","section":"notes","summary":"Classes When a class is defined, a namespace is created for it. All assignments to local variables are part of this namespace. The code below defines a class, creates an instance of the class, and calls a method on the instance.\nclass Shape(): \u0026#34;\u0026#34;\u0026#34;Represents any shape.\u0026#34;\u0026#34;\u0026#34; def __init__(self, color): self.color = color self.orientation = 0.0 def rotate(self, angle): self.orientation += angle s = Shape(\u0026#34;red\u0026#34;) s.rotate(45.0) print(s.orientation) 45.0 Class and Instance Variables The class above has two instance variables, color and orientation.","tags":["python"],"title":"Object-Oriented Programming with Python","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" File I/O in Python This notebook will cover the following topics:\nOpening and closing files Reading and writing text files Reading and writing binary files Using the with statement Using the pickle module Serializing objects with pickle Reading and writing JSON files Opening and closing files Let\u0026rsquo;s start with the basics. To open a file, we use the built-in open() function. The open() function takes two arguments: the name of the file to open and the mode in which to open it. The mode can be one of the following:\n'r' - open for reading (default) 'w' - open for writing, truncating the file first 'x' - open for exclusive creation, failing if the file already exists 'a' - open for writing, appending to the end of the file if it exists 'b' - binary mode 't' - text mode (default) '+' - open a disk file for updating (reading and writing) To make this tutorial more interesting, let\u0026rsquo;s create a purpose for our code. Our goal is to create a logging system for players and their rolls. Every time a player rolls, we will add it to our log. We will also add a timestamp to each roll. We will store the log in a file called log.txt.\nimport random import datetime # First, we\u0026#39;ll open a file fp = open(\u0026#34;log.txt\u0026#34;, \u0026#34;w\u0026#34;) # Let\u0026#39;s create a list of players players = [\u0026#34;Naomi\u0026#34;, \u0026#34;James\u0026#34;, \u0026#34;Amos\u0026#34;, \u0026#34;Bobbie\u0026#34;] # Let\u0026#39;s roll a d20 for each player and write it in the log, including a timestamp for player in players: roll = random.randint(1, 20) timestamp = datetime.datetime.now() fp.write(f\u0026#34;{timestamp} - {player} rolled a {roll}\\n\u0026#34;) # Finally, let\u0026#39;s close the file fp.close() It isn\u0026rsquo;t recommended to open and close the file manually like this. Instead, we will use the with statement. This will ensure that the file is closed properly even if an exception is raised.\nAdditionally, it is possible that calling write without using with will not write to the file immediately. Instead, it will be buffered and written later. Using with will ensure that the file is written to immediately.\nimport datetime with open(\u0026#39;log.txt\u0026#39;, \u0026#39;a\u0026#39;) as f: f.write(f\u0026#39;{datetime.datetime.now()}: {player} rolled {roll}\\n\u0026#39;) Reading and writing binary files For the next example, we will write the user\u0026rsquo;s rolls as binary. Each user will get their own file so that we can easily read their rolls later.\nfor player in players: rolls = [random.randint(1, 20) for _ in range(5)] with open(\u0026#34;rolls/\u0026#34; + player + \u0026#34;.bin\u0026#34;, \u0026#34;wb\u0026#34;) as fp: fp.write(bytes(rolls)) # To verify, let\u0026#39;s open the files and print the contents for player in players: with open(\u0026#34;rolls/\u0026#34; + player + \u0026#34;.bin\u0026#34;, \u0026#34;rb\u0026#34;) as fp: print(player, list(fp.read())) Naomi [2, 13, 18, 8, 16] James [7, 10, 17, 15, 15] Amos [19, 9, 11, 6, 3] Bobbie [16, 5, 18, 13, 1] Reading from CSV files Python has a built-in module for reading and writing CSV files. CSV stands for comma-separated values. It is a common format for storing tabular data.\n# Load and parse CSV import csv # Store the data in a list of dictionaries data = [] with open(\u0026#34;../data/musicnet_metadata.csv\u0026#34;, \u0026#34;r\u0026#34;) as fp: reader = csv.reader(fp) keys = next(reader) for row in reader: data.append(dict(zip(keys, row))) # Let\u0026#39;s print the first 5 rows for row in data[:5]: print(row) # Count the number of Violin pieces are in the dataset count = 0 for row in data: if \u0026#34;Violin\u0026#34; in row[\u0026#34;ensemble\u0026#34;]: count += 1 print(f\u0026#34;There are {count} Violin pieces in the dataset\u0026#34;) {'id': '1727', 'composer': 'Schubert', 'composition': 'Piano Quintet in A major', 'movement': '2. Andante', 'ensemble': 'Piano Quintet', 'source': 'European Archive', 'transcriber': 'http://tirolmusic.blogspot.com/', 'catalog_name': 'OP114', 'seconds': '447'} {'id': '1728', 'composer': 'Schubert', 'composition': 'Piano Quintet in A major', 'movement': '3. Scherzo: Presto', 'ensemble': 'Piano Quintet', 'source': 'European Archive', 'transcriber': 'http://tirolmusic.blogspot.com/', 'catalog_name': 'OP114', 'seconds': '251'} {'id': '1729', 'composer': 'Schubert', 'composition': 'Piano Quintet in A major', 'movement': '4. Andantino - Allegretto', 'ensemble': 'Piano Quintet', 'source': 'European Archive', 'transcriber': 'http://tirolmusic.blogspot.com/', 'catalog_name': 'OP114', 'seconds': '444'} {'id': '1730', 'composer': 'Schubert', 'composition': 'Piano Quintet in A major', 'movement': '5. Allegro giusto', 'ensemble': 'Piano Quintet', 'source': 'European Archive', 'transcriber': 'http://tirolmusic.blogspot.com/', 'catalog_name': 'OP114', 'seconds': '368'} {'id': '1733', 'composer': 'Schubert', 'composition': 'Piano Sonata in A major', 'movement': '2. Andantino', 'ensemble': 'Solo Piano', 'source': 'Museopen', 'transcriber': 'Segundo G. Yogore', 'catalog_name': 'D959', 'seconds': '546'} There are 35 Violin pieces in the dataset Application: List all works in the dataset by Bach for Solo Violin Now that we have our data loaded. Let\u0026rsquo;s use list comprehensions to print out all the works by Bach for solo violin. Our formatted table should show the following columns:\nid composition name seconds # Filter all lines that are written by Bach bach = [row for row in data if \u0026#34;Bach\u0026#34; in row[\u0026#34;composer\u0026#34;] and \u0026#34;Solo Violin\u0026#34; in row[\u0026#34;ensemble\u0026#34;]] # Print the number of Bach pieces print(f\u0026#34;There are {len(bach)} Bach pieces in the dataset\u0026#34;) # Print them all out for row in bach: print(row) There are 9 Bach pieces in the dataset {'id': '2186', 'composer': 'Bach', 'composition': 'Violin Partita No 3 in E major', 'movement': '1. Preludio', 'ensemble': 'Solo Violin', 'source': 'Oliver Colbentston', 'transcriber': 'suzumidi', 'catalog_name': 'BWV1006', 'seconds': '214'} {'id': '2191', 'composer': 'Bach', 'composition': 'Violin Partita No 3 in E major', 'movement': '6. Bourree', 'ensemble': 'Solo Violin', 'source': 'Oliver Colbentston', 'transcriber': 'suzumidi', 'catalog_name': 'BWV1006', 'seconds': '102'} {'id': '2241', 'composer': 'Bach', 'composition': 'Violin Sonata No 1 in G minor', 'movement': '1. Adagio', 'ensemble': 'Solo Violin', 'source': 'European Archive', 'transcriber': 'suzumidi', 'catalog_name': 'BWV1001', 'seconds': '242'} {'id': '2242', 'composer': 'Bach', 'composition': 'Violin Sonata No 1 in G minor', 'movement': '2. Fuga', 'ensemble': 'Solo Violin', 'source': 'European Archive', 'transcriber': 'suzumidi', 'catalog_name': 'BWV1001', 'seconds': '312'} {'id': '2243', 'composer': 'Bach', 'composition': 'Violin Sonata No 1 in G minor', 'movement': '3. Siciliana', 'ensemble': 'Solo Violin', 'source': 'European Archive', 'transcriber': 'suzumidi', 'catalog_name': 'BWV1001', 'seconds': '193'} {'id': '2244', 'composer': 'Bach', 'composition': 'Violin Sonata No 1 in G minor', 'movement': '4. Presto', 'ensemble': 'Solo Violin', 'source': 'European Archive', 'transcriber': 'suzumidi', 'catalog_name': 'BWV1001', 'seconds': '214'} {'id': '2288', 'composer': 'Bach', 'composition': 'Violin Partita No 1 in B minor', 'movement': '2. Corrente', 'ensemble': 'Solo Violin', 'source': 'John Garner', 'transcriber': 'suzumidi', 'catalog_name': 'BWV1002', 'seconds': '191'} {'id': '2289', 'composer': 'Bach', 'composition': 'Violin Partita No 1 in B minor', 'movement': '3. Sarabande', 'ensemble': 'Solo Violin', 'source': 'John Garner', 'transcriber': 'suzumidi', 'catalog_name': 'BWV1002', 'seconds': '203'} {'id': '2659', 'composer': 'Bach', 'composition': 'Violin Partita No 1 in B minor', 'movement': '6. Double', 'ensemble': 'Solo Violin', 'source': 'John Garner', 'transcriber': 'suzumidi', 'catalog_name': 'BWV1002', 'seconds': '108'} It looks like this dataset is missing quite a few pieces. Bach wrote 6 sonatas and partitas for solo violin. We only have 3 of them in this dataset. Of the 3 that are included, partitas 1 and 3 are incomplete as well.\nBased on the order of the ids, it looks like this data should be present. Let\u0026rsquo;s see if we can fill some of this in.\n# The times are based on Hilary Hahn\u0026#39;s recordings missing_pieces = [ { \u0026#34;id\u0026#34;: \u0026#34;2679\u0026#34;, \u0026#34;composer\u0026#34;: \u0026#34;Bach\u0026#34;, \u0026#34;composition\u0026#34;: \u0026#34;Violin Partita No 3 in E major\u0026#34;, \u0026#34;movement\u0026#34;: \u0026#34;2. Loure\u0026#34;, \u0026#34;ensemble\u0026#34;: \u0026#34;Solo Violin\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;DASC 5300 Fall 2023\u0026#34;, \u0026#34;transcriber\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;catalog_name\u0026#34;: \u0026#34;BWV 1006\u0026#34;, \u0026#34;seconds\u0026#34;: \u0026#34;287\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;2680\u0026#34;, \u0026#34;composer\u0026#34;: \u0026#34;Bach\u0026#34;, \u0026#34;composition\u0026#34;: \u0026#34;Violin Partita No 3 in E major\u0026#34;, \u0026#34;movement\u0026#34;: \u0026#34;3. Gavotte en Rondeau\u0026#34;, \u0026#34;ensemble\u0026#34;: \u0026#34;Solo Violin\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;DASC 5300 Fall 2023\u0026#34;, \u0026#34;transcriber\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;catalog_name\u0026#34;: \u0026#34;BWV 1006\u0026#34;, \u0026#34;seconds\u0026#34;: \u0026#34;196\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;2681\u0026#34;, \u0026#34;composer\u0026#34;: \u0026#34;Bach\u0026#34;, \u0026#34;composition\u0026#34;: \u0026#34;Violin Partita No 3 in E major\u0026#34;, \u0026#34;movement\u0026#34;: \u0026#34;4. Menuet I\u0026#34;, \u0026#34;ensemble\u0026#34;: \u0026#34;Solo Violin\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;DASC 5300 Fall 2023\u0026#34;, \u0026#34;transcriber\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;catalog_name\u0026#34;: \u0026#34;BWV 1006\u0026#34;, \u0026#34;seconds\u0026#34;: \u0026#34;113\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;2682\u0026#34;, \u0026#34;composer\u0026#34;: \u0026#34;Bach\u0026#34;, \u0026#34;composition\u0026#34;: \u0026#34;Violin Partita No 3 in E major\u0026#34;, \u0026#34;movement\u0026#34;: \u0026#34;5. Menuet II\u0026#34;, \u0026#34;ensemble\u0026#34;: \u0026#34;Solo Violin\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;DASC 5300 Fall 2023\u0026#34;, \u0026#34;transcriber\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;catalog_name\u0026#34;: \u0026#34;BWV 1006\u0026#34;, \u0026#34;seconds\u0026#34;: \u0026#34;183\u0026#34; }, ] Formatting Our Output Viewing raw lines of a dictionary or CSV file is less than ideal. Let\u0026rsquo;s format our output to make it easier to read. We will format the filtered Bach data. Since we know that every piece was written by him, we don\u0026rsquo;t need to show that column.\n# Let\u0026#39;s add the missing pieces to our dataset data.extend(missing_pieces) # Refilter the Bach pieces bach = [row for row in data if \u0026#34;Bach\u0026#34; in row[\u0026#34;composer\u0026#34;] and \u0026#34;Solo Violin\u0026#34; in row[\u0026#34;ensemble\u0026#34;]] print(f\u0026#34;There are {len(bach)} Bach pieces in the dataset\u0026#34;) # Print them all out, sorted by composition then movement composition_width = max(len(row[\u0026#34;composition\u0026#34;]) for row in bach) movement_width = max(len(row[\u0026#34;movement\u0026#34;]) for row in bach) id_width = max(len(row[\u0026#34;id\u0026#34;]) for row in bach) print(\u0026#34;{:\u0026lt;{}} | {:\u0026lt;{}} | {:\u0026lt;{}}\u0026#34;.format(\u0026#34;Composition\u0026#34;, composition_width, \u0026#34;Movement\u0026#34;, movement_width, \u0026#34;ID\u0026#34;, id_width)) print(\u0026#34;-\u0026#34; * (composition_width + movement_width + id_width + 6)) for row in sorted(bach, key=lambda x: (x[\u0026#34;composition\u0026#34;], x[\u0026#34;movement\u0026#34;])): print(\u0026#34;{:\u0026lt;{}} | {:\u0026lt;{}} | {:\u0026lt;{}}\u0026#34;.format(row[\u0026#34;composition\u0026#34;], composition_width, row[\u0026#34;movement\u0026#34;], movement_width, row[\u0026#34;id\u0026#34;], id_width)) There are 13 Bach pieces in the dataset Composition | Movement | ID ------------------------------------------------------------- Violin Partita No 1 in B minor | 2. Corrente | 2288 Violin Partita No 1 in B minor | 3. Sarabande | 2289 Violin Partita No 1 in B minor | 6. Double | 2659 Violin Partita No 3 in E major | 1. Preludio | 2186 Violin Partita No 3 in E major | 2. Loure | 2679 Violin Partita No 3 in E major | 3. Gavotte en Rondeau | 2680 Violin Partita No 3 in E major | 4. Menuet I | 2681 Violin Partita No 3 in E major | 5. Menuet II | 2682 Violin Partita No 3 in E major | 6. Bourree | 2191 Violin Sonata No 1 in G minor | 1. Adagio | 2241 Violin Sonata No 1 in G minor | 2. Fuga | 2242 Violin Sonata No 1 in G minor | 3. Siciliana | 2243 Violin Sonata No 1 in G minor | 4. Presto | 2244 # Now that it looks good, let\u0026#39;s write the updated dataset to a new file with open(\u0026#34;../data/musicnet_metadata_updated.csv\u0026#34;, \u0026#34;w\u0026#34;) as fp: writer = csv.DictWriter(fp, fieldnames=keys) writer.writeheader() writer.writerows(data) Reading and Writing JSON JSON stands for JavaScript Object Notation. It is a common format for storing and transmitting data. It is often used for web APIs. Python has a built-in module for reading and writing JSON files.\nLet\u0026rsquo;s open the CSV file from the previous example and write it to JSON.\n# Load and parse CSV import csv import json # Store the data in a list of dictionaries data = [] with open(\u0026#34;../data/musicnet_metadata.csv\u0026#34;, \u0026#34;r\u0026#34;) as fp: reader = csv.reader(fp) keys = next(reader) for row in reader: data.append(dict(zip(keys, row))) # Write the data to a JSON file with open(\u0026#34;musicnet_metadata.json\u0026#34;, \u0026#34;w\u0026#34;) as fp: json.dump(data, fp) Reading and Writing pickle Files Python has a built-in module for reading and writing pickle files. pickle is a binary format for serializing Python objects. It is not human-readable, but it is very useful for storing and transmitting data. Note that pickle is not secure. It is possible to create malicious pickle files that can execute arbitrary code when loaded. Also, other languages cannot natively read pickle files. However, there are usually libraries available for reading pickle files in other languages.\nLet\u0026rsquo;s start by writing the list from the previous example to a pickle file.\n# Write the data to a pickle file import pickle with open(\u0026#34;musicnet_metadata.pkl\u0026#34;, \u0026#34;wb\u0026#34;) as fp: pickle.dump(data, fp) # Get the file size of the pickle file we just wrote import os print(os.path.getsize(\u0026#34;musicnet_metadata.pkl\u0026#34;)) 52353 Serializing Class Objects Serializing objects is simple with Python. Let\u0026rsquo;s create a simple class to represent the attributes of our dataset. We can then convert our previous list of dictionary data to a list of objects. Finally, we can serialize the list of objects to a pickle file.\n# Create a class for our data class Piece: def __init__(self, data): self.id = data[\u0026#34;id\u0026#34;] self.composer = data[\u0026#34;composer\u0026#34;] self.composition = data[\u0026#34;composition\u0026#34;] self.movement = data[\u0026#34;movement\u0026#34;] self.ensemble = data[\u0026#34;ensemble\u0026#34;] self.source = data[\u0026#34;source\u0026#34;] self.transcriber = data[\u0026#34;transcriber\u0026#34;] self.catalog_name = data[\u0026#34;catalog_name\u0026#34;] self.seconds = data[\u0026#34;seconds\u0026#34;] def __repr__(self): return f\u0026#34;\u0026lt;Piece {self.id}: {self.composer} - {self.composition}\u0026gt;\u0026#34; def __str__(self): return f\u0026#34;{self.composer} - {self.composition}\u0026#34; # Convert our data into a list of objects and write it to a pickle file pieces = [Piece(d) for d in data] with open(\u0026#34;musicnet_metadata.pkl\u0026#34;, \u0026#34;wb\u0026#34;) as fp: pickle.dump(pieces, fp) # Print the size of this new file print(os.path.getsize(\u0026#34;musicnet_metadata.pkl\u0026#34;)) 54352 Bonus: Write the class objects to JSON We can\u0026rsquo;t write the class objects to JSON directly. We need to convert them to a dictionary first. We can do this by implementing the __dict__ method. Without explicitly defining __dict__, it will return the default dictionary for the class. However, we can override it to return a custom dictionary. The default version will include all of the class attributes. We can use this to our advantage to convert the class to a dictionary.\n# Bonus: Save the class data as JSON import json # Write the list of class objects to a JSON file with open(\u0026#34;musicnet_metadata.json\u0026#34;, \u0026#34;w\u0026#34;) as fp: json.dump(pieces, fp, default=lambda o: o.__dict__) ","date":1693371600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693371600,"objectID":"eee3603d2b9f6cba33b4e8b622a97af9","permalink":"https://ajdillhoff.github.io/notes/file_io/","publishdate":"2023-08-30T00:00:00-05:00","relpermalink":"/notes/file_io/","section":"notes","summary":"File I/O in Python This notebook will cover the following topics:\nOpening and closing files Reading and writing text files Reading and writing binary files Using the with statement Using the pickle module Serializing objects with pickle Reading and writing JSON files Opening and closing files Let\u0026rsquo;s start with the basics. To open a file, we use the built-in open() function. The open() function takes two arguments: the name of the file to open and the mode in which to open it.","tags":["python"],"title":"Python File I/O","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Functions We learned the importance of functions early on in mathematics. It is a compact way of represented a complex process dependent on a set of variables. In programming, functions are used to encapsulate a set of instructions that are used repeatedly. Functions are also used to make code more readable and easier to debug.\nIn this notebook, we will look at a few important built-in functions in Python as well as how to define our own functions.\nBuilt-in Functions Python has a good number of built-in functions that cover a general range of tasks.\n# Find the max of a list of numbers max_val = max(1, 2, 3) print(f\u0026#34;The max value is {max_val}\u0026#34;) # Find the min of a list of numbers min_val = min(1, 2, 3) print(f\u0026#34;The min value is {min_val}\u0026#34;) # Get the length of a string text = input(\u0026#34;Enter some text: \u0026#34;) print(f\u0026#34;The length of \\\u0026#34;{text}\\\u0026#34; is {len(text)}\u0026#34;) The max value is 3 The min value is 1 The length of \u0026quot;test\u0026quot; is 4 The len function returns the length of a string, list, or other iterable object. Since these functions are built-in, we should treat them as keywords. However, we can still overwrite them if we want to.\nType Conversion Functions Python has built-in functions to convert between data types. These are int, float, str, bool, and list. Converting between incompatible types will result in an error.\n# Try entering a number and text val = input(\u0026#34;Enter a number: \u0026#34;) val_int = int(val) print(f\u0026#34;The value of {val} is {val_int}\u0026#34;) # Converting an integer type to float will truncate the decimal value float_val = 3.14 int_val = int(float_val) print(f\u0026#34;The value of {float_val} is {int_val}\u0026#34;) # Converting a number to a string also has its uses big_number = 184759372934 big_number_str = str(big_number) print(f\u0026#34;There are {len(big_number_str)} digits in {big_number_str}\u0026#34;) The value of 10 is 10 The value of 3.14 is 3 There are 12 digits in 184759372934 Math Functions Python has many more functions that are available through named modules. For example, the math module contains many useful functions for mathematical operations. To use these functions, we need to import the module first.\nimport math signal_power = 10 noise_power = 5 ratio = signal_power / noise_power decibels = 10 * math.log10(ratio) print(f\u0026#34;The decibel value is {decibels}\u0026#34;) # The math module also has a function to convert from radians to degrees radians = 0.7 degrees = math.degrees(radians) print(f\u0026#34;{radians} radians is {degrees} degrees\u0026#34;) # The math module also has a function to convert from degrees to radians degrees = 45 radians = math.radians(degrees) print(f\u0026#34;{degrees} degrees is {radians} radians\u0026#34;) The decibel value is 3.010299956639812 0.7 radians is 40.10704565915762 degrees 45 degrees is 0.7853981633974483 radians Example: Getting the number of digits without len We can use the math.log10 function to get the number of digits in a number.\nbig_number = 184759372934 big_number_log = math.log10(big_number) print(f\u0026#34;The log of {big_number} is {big_number_log}\u0026#34;) # We can use this to get the number of digits in a number big_number_log_int = int(big_number_log) print(f\u0026#34;There are {big_number_log_int + 1} digits in {big_number}\u0026#34;) The log of 184759372934 is 11.266606479598726 There are 12 digits in 184759372934 Random Numbers The random module contains functions for generating random numbers. The random function returns a random number between 0 and 1. The randint function returns a random integer between two numbers. Other functions in the random module include randrange, choice, choices, shuffle, and sample.\nimport random # Generate 10 random numbers between 0 and 1 for i in range(10): print(random.random()) # Generate 10 random integers between 4 and 10 for i in range(10): print(random.randint(4, 10)) # Randomly select a value from a list outcomes = [\u0026#34;rock\u0026#34;, \u0026#34;paper\u0026#34;, \u0026#34;scissors\u0026#34;] print(random.choice(outcomes)) 0.5791115192498043 0.25376108559783617 0.3024994224981705 0.09072083021401978 0.42037740159252324 0.6617409553852582 0.4379309412534801 0.2475132325137145 0.8452657960508129 0.9268148237541722 4 5 7 5 7 10 8 6 7 4 scissors Third-party modules such as numpy and scipy contain many more functions for generating random numbers.\nimport numpy as np # Generate 10 random numbers between 0 and 1 numbers = np.random.rand(10) print(numbers) # Sample 10 random numbers from a normal distribution numbers = np.random.randn(10) print(numbers) [0.47426298 0.22698408 0.42633457 0.97584666 0.68835279 0.57067918 0.56958053 0.86689698 0.54039587 0.59397872] [ 0.97783258 -0.4043045 0.05416324 -2.21162364 -0.60327265 -0.39077797 -0.83294774 0.56285811 -0.28047169 -0.60044315] Defining Functions We can define our own functions using the def keyword. The syntax for defining a function is as follows:\ndef function_name(parameters): # function body return value Python functions can have multiple parameters and return multiple values. The return keyword is optional. If it is not used, the function will return None.\nimport math def calculate_stats(numbers): \u0026#34;\u0026#34;\u0026#34;Calculate the mean and standard deviation of a list of numbers\u0026#34;\u0026#34;\u0026#34; mean = sum(numbers) / len(numbers) variance = sum((x - mean) ** 2 for x in numbers) / len(numbers) std_dev = math.sqrt(variance) return mean, std_dev # Calculate the mean and standard deviation of a list of numbers numbers = [1, 2, 3, 4, 5] mean, std_dev = calculate_stats(numbers) print(f\u0026#34;The mean is {mean} and the standard deviation is {std_dev}\u0026#34;) The mean is 3.0 and the standard deviation is 1.4142135623730951 Default Argument Values Python functions can have default values for their arguments. This allows us to call the function without specifying the value for that argument. If we do not specify a value for an argument, the default value will be used.\n# To demonstrate default argument values, let\u0026#39;s make a function that # allows the user to select the axis along which to calculate the mean def calculate_mean(numbers, axis=0): \u0026#34;\u0026#34;\u0026#34;Calculates the mean of a 2D array along a given axis\u0026#34;\u0026#34;\u0026#34; # Check that the input is a 2D python list if not isinstance(numbers, list) or not isinstance(numbers[0], list): raise ValueError(\u0026#34;Input must be a 2D list\u0026#34;) if axis == 0: # Here, zip(*numbers) will return a list of tuples, # where each tuple is a column of our 2D list. return [sum(col) / len(col) for col in zip(*numbers)] elif axis == 1: return [sum(row) / len(row) for row in numbers] else: raise ValueError(\u0026#34;Invalid axis value\u0026#34;) # Calculate the mean along the rows numbers = [[1, 2, 3], [4, 5, 6]] mean = calculate_mean(numbers, axis=1) print(\u0026#34;Mean of each row:\u0026#34;, mean) # Calculate the mean along the columns numbers = [[1, 2, 3], [4, 5, 6]] mean = calculate_mean(numbers, axis=0) print(\u0026#34;Mean of each column\u0026#34;, mean) Mean of each row: [2.0, 5.0] Mean of each column [2.5, 3.5, 4.5] Keyword Arguments Python functions can also have keyword arguments. This allows us to specify the name of the argument when calling the function. This is useful when a function has many arguments and we only want to specify a few of them.\ndef matrix_max(matrix, axis=0, return_indices=False): \u0026#34;\u0026#34;\u0026#34;Calculates the maximum of a 2D array along a given axis\u0026#34;\u0026#34;\u0026#34; # Check that the input is a 2D python list if not isinstance(matrix, list) or not isinstance(matrix[0], list): raise ValueError(\u0026#34;Input must be a 2D list\u0026#34;) if axis == 0: # Here, zip(*matrix) will return a list of tuples, # where each tuple is a column of our 2D list. max_vals = [max(col) for col in zip(*matrix)] if return_indices: max_indices = [col.index(max(col)) for col in zip(*matrix)] return max_vals, max_indices else: return max_vals elif axis == 1: max_vals = [max(row) for row in matrix] if return_indices: max_indices = [row.index(max(row)) for row in matrix] return max_vals, max_indices else: return max_vals else: raise ValueError(\u0026#34;Invalid axis value\u0026#34;) # Calculate the max along the rows numbers = [[1, 2, 3], [4, 5, 6]] max_vals = matrix_max(numbers, axis=1) print(\u0026#34;Max of each row:\u0026#34;, max_vals) # Calculate the max along the columns numbers = [[1, 2, 3], [4, 5, 6]] max_vals = matrix_max(numbers, axis=0) print(\u0026#34;Max of each column\u0026#34;, max_vals) # Calculate the max along the rows and return the indices numbers = [[1, 2, 3], [4, 5, 6]] max_vals, max_indices = matrix_max(numbers, axis=1, return_indices=True) print(\u0026#34;Max of each row:\u0026#34;, max_vals) print(\u0026#34;Indices of max values:\u0026#34;, max_indices) Max of each row: [3, 6] Max of each column [4, 5, 6] Max of each row: [3, 6] Indices of max values: [2, 2] List Unpacking and Variable Arguments Python functions can have variable arguments. This allows us to pass in a variable number of arguments to a function. The * operator is used to unpack a list or tuple into separate arguments.\n# The range function expects up to 3 arguments: start, stop, and step # We can use list unpacking to put our arguments in a list args = [1, 10, 2] for i in range(*args): print(i) 1 3 5 7 9 # We can create a function that support multiple arguments as well as keyword arguments def print_args(*args, **kwargs): print(\u0026#34;Positional arguments:\u0026#34;, args) print(\u0026#34;Keyword arguments:\u0026#34;, kwargs) print_args(1, 2, 3, a=4, b=5) Positional arguments: (1, 2, 3) Keyword arguments: {'a': 4, 'b': 5} ","date":1693371600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693371600,"objectID":"84951135ebd9fe6eddadf2a6c8d8f3a5","permalink":"https://ajdillhoff.github.io/notes/functions/","publishdate":"2023-08-30T00:00:00-05:00","relpermalink":"/notes/functions/","section":"notes","summary":"Functions We learned the importance of functions early on in mathematics. It is a compact way of represented a complex process dependent on a set of variables. In programming, functions are used to encapsulate a set of instructions that are used repeatedly. Functions are also used to make code more readable and easier to debug.\nIn this notebook, we will look at a few important built-in functions in Python as well as how to define our own functions.","tags":["python"],"title":"Python Functions","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents What is Version Control? What is Git? What is a Repository? Configuring Git Creating a Repository Staging Files Committing Changes Ignoring Files Branching Merging Remotes Cloning an Existing Repository Summary What is Version Control? Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later. This is useful not just for team projects, for for individual projects as well.\nWith version control, you can:\nrevert files back to a previous state revert the entire project back to a previous state compare changes over time see who last modified something that might be causing a problem who introduced an issue and when and much more When tracking changes to a project over time, the simplest approach is one that you might recognize if you\u0026rsquo;ve ever worked on an essay for class. Imagine you\u0026rsquo;ve just finished the first draft of an assignment. You decide to save this document as essay_first_draft.docx. After working on it a bit more, you choose to save the updated copy to a new file so that you can compare the initial and final draft. This one is then named essay_first_draft_COMPLETE.docx. You end up reading some new information and realize you missed a key requirement of the assignment. After adding in the new information you save it as essay_first_draft_COMPLETE_v2.docx. After many such iterations you end up with a collection of ill-named files.\nMaybe you\u0026rsquo;ve never done this yourself, but this example actually depicts a version control system. Luckily for us, there have been many improvements to this naive method. A more ideal choice, especially in a team environment, would be a Centralized VCS. Project files would be hosted on a server that keeps track of the different changes. Team members can download the latest versions, modify them, and update the server once they are done.\nFigure 1: Centralized VCS (source) This is a welcome improvement over the naive version, but it still has its downsides. What if the server or connection goes down? There are many scenarios that would lead to a catastrophic loss of data. For important projects, you would not want to keep all of your eggs in once basket. A more ideal solution would be a Distributed VCS, this is what Git is.\nFigure 2: Distributed VCS (source) In a DVCS, every user has a complete copy of the project. If the server goes down, or a connection is lost, you can still work on the project. Since every user has a complete copy of the project, there is no single point of failure. Another huge advantage is speed. The operations are performed locally. It is only when you want to share your changes that you need to connect to the server. This means that you can commit changes, create branches, and perform other operations without an internet connection.\nWhat is Git? There are two primary ways of thinking about versioning in general: snapshots and differences. The first starts with your original files and records each change as a delta between the latest version and the previous.\nFigure 3: Difference-based Version Control (source) The second starts with your original files and records each change as a snapshot of the entire project. Files that have not changed will not be duplicated. Instead, Git will create a reference to the previous version of the file. This is the approach that Git uses, and it comes with a great benefit that we will see when we get to branching.\nFigure 4: Snapshot-based Version Control (source) What is a Repository? A repository is a collection of files and folders that are tracked by Git. It is the project folder that you will be working in. You can create a repository from scratch, or you can clone an existing repository. We will cover examples of both during class. Cloning is the process of copying an existing repository to your local machine. We will start with the first approach: creating a repository from scratch.\nBefore starting, it is important to at least know the three major states of Git. Files can be modified, staged, or committed.\nA modified file has been changed locally, but has not been committed to the repository. A staged file is a modified file that has been marked to be included in the next commit. A committed file is a staged file that has been saved to the repository. Figure 5: The main sections of Git. (source) The figure above depicts the three major sections of working with a Git repository. Each repository has a .git directory that contains all of the information about the project. The working directory is the root directory where the latest versions of the files exist. Once modifications are made, these changes are sent to the staging area. This is where you can choose which changes to include in the next commit. Once you are happy with the changes, you can commit them to the repository. This will save the changes to the .git directory.\nConfiguring Git Once you have installed Git, there are a few important configuration options to get started. If you have already been using Git, you can skip this section. If you are using Git for the first time, you will need to set your name and email address. This information will be used to identify you as the author of the commits that you make.\ngit config --global user.name \u0026#34;Naomi Nagata\u0026#34; git config --global user.email \u0026#34;naomi@rocinante.exp\u0026#34; If you have already used a service like GitHub, note that this name and email does not need to match the one you used to log into that service.\nYou can view your current configuration at any time by running the following command:\ngit config --list --show-origin Creating a Repository For this example, our first project will be a Python program that resizes images to a specified width. This is to ensure that the aspect ratio is maintained.\nNow that we have Git installed and configured, we can create our first repository. First, create a new directory for the project. I will use pyresize in this document. Then, navigate to that directory and run the following command:\nmkdir pyresize \u0026amp;\u0026amp; cd pyresize git init You may see the following warning when creating a new repository:\nhint: Using \u0026#39;master\u0026#39; as the name for the initial branch. This default branch name hint: is subject to change. To configure the initial branch name to use in all hint: of your new repositories, which will suppress this warning, call: hint: hint: git config --global init.defaultBranch \u0026lt;name\u0026gt; hint: hint: Names commonly chosen instead of \u0026#39;master\u0026#39; are \u0026#39;main\u0026#39;, \u0026#39;trunk\u0026#39; and hint: \u0026#39;development\u0026#39;. The just-created branch can be renamed via this command: hint: hint: git branch -m \u0026lt;name\u0026gt; Initialized empty Git repository in /home/alex/dev/pyresize/.git/ Let\u0026rsquo;s first set the default branch name to main.\ngit config --global init.defaultBranch main Next, we will change the name of the current branch to main. We could also delete the .git directory and start over, but this is a good opportunity to learn how to rename a branch.\ngit branch -m main You can view the status of your repository at any time by using the git status command. This will show you the current branch, the files that have been modified, and the files that have been staged. Our newly created repository looks like this:\n$ git status On branch main No commits yet nothing to commit (create/copy files and use \u0026#34;git add\u0026#34; to track) Staging Files Let\u0026rsquo;s create our first file and add it to the repository. We will create a file called pyresize.py that contains the following code:\nimport sys from PIL import Image def resize_image(image_path, width): image = Image.open(image_path) wpercent = (width / float(image.size[0])) hsize = int((float(image.size[1]) * float(wpercent))) image = image.resize((width, hsize), Image.LANCZOS) image.save(image_path) if __name__ == \u0026#34;__main__\u0026#34;: resize_image(sys.argv[1], int(sys.argv[2])) At this point, we have a local change that our repository is not aware of. We can see this by running the git status command again.\n$ git status On branch main No commits yet Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) pyresize.py nothing added to commit but untracked files present (use \u0026#34;git add\u0026#34; to track) Let\u0026rsquo;s add our file with git add pyresize.py and check the status again.\n$ git add pyresize.py $ git status On branch main No commits yet Changes to be committed: (use \u0026#34;git rm --cached \u0026lt;file\u0026gt;...\u0026#34; to unstage) new file: pyresize.py Committing Changes Finally, we will commit this change via git commit. There are a few things to note about this command. If you haven\u0026rsquo;t configured your default editor, it might be set to something like nano or vim by default. If you are not familiar with these editors, you can set your default editor to something else by running the following command:\ngit config --global core.editor \u0026#34;code --wait\u0026#34; This will set your default editor to Visual Studio Code. Obviously, this should be installed on your system if you are using it. If you are using a different editor, you can replace code with the command that you would use to open a file in that editor. For example, if you are using vim, you would use vim. The --wait flag above will wait for the editor to close before continuing. This is important for Git to know when you are done writing your commit message. Note that not every application supports this flag.\nOnce you have set your default editor, you can run git commit to open the editor and write your commit message. The first line should be a short description of the change. The following lines should be a more detailed description of the change. You can see an example below:\nAdded our first file. # Please enter the commit message for your changes. Lines starting # with \u0026#39;#\u0026#39; will be ignored, and an empty message aborts the commit. # # On branch main # # Initial commit # # Changes to be committed: #\tnew file: pyresize.py # Once we save this message, the commit will be complete. You can view the commit history by running git log. This will show you the commit hash, the author, the date, and the commit message. You can also commit changes and add a message in one command.\ngit commit -m \u0026#34;Added our first file.\u0026#34; We do not yet have a remote repository to push to, so we will save that for later. For now, we will continue to work locally. Let\u0026rsquo;s add an image to our repository so that we can test it. I am going to use the UTA Logo for this example. You can download this and variations from the UTA Branding Resources page.\nFigure 6: UTA Logo Create a new imgs folder and add your image(s) to it. We can then add the directory along with all of its contents using git add imgs. You can see the status of your repository by running git status again. Let\u0026rsquo;s go ahead and commit these changes.\ngit commit -am \u0026#34;Added the UTA logo.\u0026#34; Making a Change For this project, we don\u0026rsquo;t really need to have a bunch of test images. It is sufficient to have one or two. The name of our image folder should probably change to reflect its purpose. Let\u0026rsquo;s start by renaming imgs to test_imgs. We can do this with the mv command in bash. Our repository will now look like this:\n$ git status On branch main Changes not staged for commit: (use \u0026#34;git add/rm \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) deleted: imgs/uta.png Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) test_imgs/ no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) This might be a tad unexpected. We renamed the folder, but Git is telling us that we deleted a file. This is because Git is tracking the file imgs/uta.png. When we renamed the folder, Git no longer knew where to find the file. We can fix this by running git rm imgs/uta.png. This will remove the file from the repository. We can then add the new folder with git add test_imgs. However, if we simply use git add test_imgs, Git will not know that we renamed the folder. We can fix this by using the -A flag. This will tell Git to add all changes, including renames. Our repository will now look like this:\n$ git status On branch main Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) renamed: imgs/uta.png -\u0026gt; test_imgs/uta.png Go ahead and commit these changes.\nIgnoring Files There are certain files and directories that will end up in our project folder that we do not want to track. For example, we may want to resize images and save them in a local output directory. However, we do not want to track any of these images. We can have Git remember what we want or do not want using an ignore file. This file will contain a list of files and directories that we want to ignore. Let\u0026rsquo;s create a .gitignore file and add the following line:\noutput/ Make sure you add and commit the .gitignore file.\nBranching Let\u0026rsquo;s go ahead and test our program by resizing one of our images. I\u0026rsquo;m going to to resize uta.png to have a width of 500 pixels using the following command.\npython pyresize.py test_imgs/uta.png 500 Our program doesn\u0026rsquo;t support creating a new file when resizing. Instead, it resizes the file and overwrites the original. We should add this feature and modify it without messing up our current code base. This is where branching comes in. We can create a new branch that will contain our new feature. We can then test it and merge it back into the main branch once we are happy with it.\nEvery commit that we make is a snapshot of the entire project up to that point. There is a unique identifier attached to each commit. If we want to work on a specific bug or new feature without affecting the current code base, we can create a branch to track those changes independently of the other branches. The main benefits are that we can potentially break the code base without affecting the production-ready code. We can also work on multiple features at the same time without affecting each other.\nLet\u0026rsquo;s create a new branch called output_write. We can do this with the git branch command.\ngit branch output_write Figure 7: The result of creating a new branch named testing. (source) This only creates a new branch, but we are still on the main branch. We can see this by running git branch. The current branch will be highlighted with an asterisk. The current branch is pointed to by the HEAD pointer. We can switch to the new branch using git checkout.\ngit checkout output_write Figure 8: The HEAD pointer after switching to a new branch. (source) Modifying the Code Now that we are on the output_write branch, we can modify the code without affecting the main branch. Let\u0026rsquo;s modify our original function to take in an additional argument: the output path. We can then use this path to save the resized image to a new file. Our new code will look like this:\nimport sys from PIL import Image def resize_image(image_path, width, output_path): image = Image.open(image_path) wpercent = (width / float(image.size[0])) hsize = int((float(image.size[1]) * float(wpercent))) image = image.resize((width, hsize), Image.LANCZOS) image.save(output_path) if __name__ == \u0026#34;__main__\u0026#34;: resize_image(sys.argv[1], int(sys.argv[2]), sys.argv[3]) Go ahead and commit these changes. Since we have already moved the HEAD pointer to the new branch, this change will not affect our main branch. The figure below is analagous to this scenario.\nFigure 9: The result of committing changes to a new branch. (source) Let\u0026rsquo;s test our program once more by resizing an image and saving it to the output directory. We can do this with the following command:\npython pyresize.py test_imgs/uta.png 500 output/uta.png Notice that when you run git status after resizing the image and saving to the output directory, it does not report any changes. Our ignore file is working as intended!\nMerging Now that we have completed our new feature and tested it, we should merge these changes back to the main branch. We can do this with the git merge command. First, we need to switch back to the main branch.\ngit checkout main We can then merge the output_write branch into the main branch.\ngit merge output_write This will merge the changes from the output_write branch into the main branch. If there are any conflicts, Git will let you know and you can resolve them manually. Once the merge is complete, you can delete the output_write branch.\ngit branch -d output_write Figure 10: The result of merging a branch into the master branch. (source) The figure above shows the history of a repository in which a branch named iss53 was created, modified with new commits, and eventually merged back into the master branch.\nRemotes We have now covered the basics of using Git locally. Eventually, we will want our changes to be backed up on a remote server. This will allow us to collaborate with others and work on our projects from multiple machines. There are many services that provide this functionality. We will use GitHub for this example, but the process is similar for other services.\nCreating a Repository First, we need to create a new repository on GitHub. You can do this by clicking the New button on the GitHub homepage. I am only going to add a short description of this program. Go ahead and click Create repository.\nGitHub supports both SSH and HTTPS. I already have an SSH key set up. If you haven\u0026rsquo;t configured one yet, check out Adding a new SSH key to your GitHub account for instructions on how to do so. You can also use HTTPS, this requires a personal access token. More information can be found at Creating a personal access token.\nOnce created, we will have the option to use either our HTTPS or SSH URL. Mine is git@github.com:ajdillhoff/pyresize.git. We can add this as a remote repository using the git remote add command.\ngit remote add origin git@github.com:ajdillhoff/pyresize.git Pushing to a Remote Now that we have a remote repository, we can push our changes to it. We can do this with the git push command. However, we need to specify the remote repository and the branch that we want to push. We can do this with the following command:\ngit push -u origin main That\u0026rsquo;s it! Our changes are now backed up on GitHub. We can view our repository by navigating to the URL in our browser. We can also view the commit history by clicking the Commits link.\nCloning an Existing Repository If our repository already exists on GitHub, we can clone it to our local machine. This will create a new directory with the same name as the repository. We can do this with the git clone command. Let\u0026rsquo;s clone the pyresize repository that we just created.\ngit clone git@github.com:ajdillhoff/pyresize.git You can clone using either the HTTPS or SSH URLs. Make sure you have the appropriate key or access token to do so.\nPulling from a Remote Now that we have cloned the repository, we can make changes and push them to the remote. However, if someone else makes changes to the remote repository, we will need to pull those changes to our local repository. We can do this with the git pull command.\ngit pull origin main Git will always require that we are up-to-date with the remote before we can push our changes. If someone else has made changes to the remote, we will need to pull those changes before we can push our own. This is to prevent conflicts.\nSummary We have covered the basics of using Git. We have created a repository, staged and committed changes, created branches, merged branches, and pushed our changes to a remote repository. There are many more features that we have not covered, but this should be enough to get you started. If you are interested in learning more, check out the Git Book.\nCommand Reference\nCommand Description git init Create a new repository git config Configure Git git status View the status of your repository git add Add files to the staging area git commit Commit changes to the repository git branch Create, list, or delete branches git checkout Switch branches or restore working tree files git merge Join two or more development histories together git remote Manage set of tracked repositories git push Update remote refs along with associated objects git clone Clone a repository into a new directory git pull Fetch from and integrate with another repository or a local branch git log Show commit logs git rm Remove files from the working tree and from the index git mv Move or rename a file, a directory, or a symlink git branch -d Delete a branch git remote add Add a remote repository ","date":1693026000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693026000,"objectID":"dad86f5b79754b913113ce9031015069","permalink":"https://ajdillhoff.github.io/notes/introduction_to_git/","publishdate":"2023-08-26T00:00:00-05:00","relpermalink":"/notes/introduction_to_git/","section":"notes","summary":"Table of Contents What is Version Control? What is Git? What is a Repository? Configuring Git Creating a Repository Staging Files Committing Changes Ignoring Files Branching Merging Remotes Cloning an Existing Repository Summary What is Version Control? Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later. This is useful not just for team projects, for for individual projects as well.","tags":["git","programming"],"title":"Introduction to Git","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Lists and List Comprehensions List comprehensions provide a concise way to create lists, and are often faster than using a for-loop. They are inspired by set-builder notation in mathematics.\nThis notebook demonstrates common list functions as well as the syntax and basic usage of list comprehensions.\nnames = [\u0026#34;Naomi\u0026#34;, \u0026#34;James\u0026#34;, \u0026#34;Amos\u0026#34;, \u0026#34;Bobbie\u0026#34;] # Append \u0026#34;Miller\u0026#34; to the end of the list names.append(\u0026#34;Miller\u0026#34;) # This can also be accomplished using the + operator or the extend() method # These are similar, where `+=` is shorthand for `extend` names.extend([\u0026#34;Chrisjen\u0026#34;]) names += [\u0026#34;Alex\u0026#34;] # `extend()` works on any iterable names.extend([\u0026#34;Fred\u0026#34;, \u0026#34;Dawes\u0026#34;, \u0026#34;Ashford\u0026#34;]) # This can also be done using slicing # names[len(names):] = [\u0026#34;Fred\u0026#34;, \u0026#34;Dawes\u0026#34;, \u0026#34;Ashford\u0026#34;] # Insert \u0026#34;Holden\u0026#34; at the beginning of the list names.insert(0, \u0026#34;Holden\u0026#34;) # We can easily remove items names.remove(\u0026#34;Alex\u0026#34;) # We can also remove items by value names.remove(\u0026#34;Ashford\u0026#34;) # We can also remove items by index. This will return the removed item eros_passenger = names.pop(5) print(eros_passenger + \u0026#34; is on his way to Venus\u0026#34;) # If your list has duplicates, you can count the number of times a value appears print(\u0026#34;Naomi\u0026#34; + \u0026#34; appears \u0026#34; + str(names.count(\u0026#34;Naomi\u0026#34;)) + \u0026#34; time(s)\u0026#34;) # Reversing a list is easy names.reverse() # We can also create a shallow copy of a list names_copy = names.copy() Miller is on his way to Venus Naomi appears 1 time(s) Sorting A list can be sorted by calling the sort function. The list is sorted in-place, meaning that the original list is modified. Since a list can contain any type of object, the objects must be comparable to each other. For example, a list of strings can be sorted alphabetically, but a list of strings and integers cannot be sorted.\nIn cases with mixed types, a custom key function can be passed to the sort function. The key function is called on each element of the list, and the return value is used to sort the list. For example, to sort a list of strings and integers by the length of the string, the key function would transform the integers into strings.\n\u0026gt;\u0026gt;\u0026gt; l = [\u0026#39;abc\u0026#39;, 1, \u0026#39;ab\u0026#39;] \u0026gt;\u0026gt;\u0026gt; l.sort(key=str) \u0026gt;\u0026gt;\u0026gt; l [1, \u0026#39;abc\u0026#39;, \u0026#39;ab\u0026#39;] # Sorting is a very common operation, and Python gives us some level of control over how the items are sorted. # The default is to sort in ascending order # Sort the list in ascending order names.sort() print(names) # Sort the list in descending order names.sort(reverse=True) print(names) # We can also sort by a key function, such as the length of each name names.sort(key=len) print(names) ['Amos', 'Bobbie', 'Chrisjen', 'Dawes', 'Fred', 'Holden', 'James', 'Naomi'] ['Naomi', 'James', 'Holden', 'Fred', 'Dawes', 'Chrisjen', 'Bobbie', 'Amos'] ['Fred', 'Amos', 'Naomi', 'James', 'Dawes', 'Holden', 'Bobbie', 'Chrisjen'] Let\u0026rsquo;s introduce a slightly more complex scenario in which each person in the list rolls a 20-sided die. We\u0026rsquo;ll use the random module to generate a random number between 1 and 20 for each person. Using those values, we can then sort the list by the roll of the die, in descending order.\nimport random # Roll a 20-sided die for each person using a for loop # rolls = [] # for name in names: # rolls.append(random.randint(1, 20)) # The above code is commented out by default because it can be written more succinctly using a list comprehension rolls = [random.randint(1, 20) for _ in names] print(names) names.index(\u0026#39;Fred\u0026#39;) # Sort the names by the roll of the die, in descending order # First, create a function that returns the roll based on the name def get_roll(name): # print(names) # reveals an empty list! return rolls[names.index(name)] # names.sort(key=get_roll, reverse=True) # The above will not work because `names` does not exist within the scope of the function. # We can instead combine the rolls with the names using the zip() function names_and_rolls = list(zip(names, rolls)) names_and_rolls.sort(key=lambda roll: roll[1], reverse=True) print(names_and_rolls) ['Fred', 'Amos', 'Naomi', 'James', 'Dawes', 'Holden', 'Bobbie', 'Chrisjen'] [('Dawes', 19), ('Fred', 18), ('Amos', 12), ('Naomi', 11), ('Bobbie', 10), ('Holden', 8), ('Chrisjen', 7), ('James', 5)] List Comprehensions We can also create nested list comprehensions, which is equivalent to nested for loops. For example, let\u0026rsquo;s create a 3x4 matrix using a nested list comprehension.\nNested list comprehension Advanced list comprehension # Nested list comprehension # Create a 2D list where each row represents the top 5 rolls for each person top_rolls = [[random.randint(1, 20) for _ in range(5)] for _ in names] print(top_rolls) # We can create a similar 2D list where each row is a tuple of the name and the top 5 rolls top_rolls = [(name, [random.randint(1, 20) for _ in range(5)]) for name in names] print(top_rolls) # If we wrote this with loops, it would look like this: top_rolls = [] for name in names: rolls = [] for _ in range(5): rolls.append(random.randint(1, 20)) top_rolls.append((name, rolls)) [[7, 1, 11, 15, 14], [3, 14, 15, 10, 9], [7, 16, 17, 15, 19], [12, 6, 10, 17, 19], [10, 4, 7, 17, 11], [16, 1, 14, 2, 12], [3, 10, 2, 12, 13], [7, 17, 16, 3, 6]] [('Fred', [15, 7, 1, 3, 16]), ('Amos', [7, 15, 9, 4, 6]), ('Naomi', [19, 7, 12, 5, 6]), ('James', [6, 11, 4, 10, 17]), ('Dawes', [13, 5, 15, 19, 8]), ('Holden', [14, 18, 16, 5, 15]), ('Bobbie', [12, 15, 8, 18, 20]), ('Chrisjen', [18, 16, 19, 10, 11])] Performance of list comprehensions versus for loops A strong argument for list comprehensions is that they are more elegant and easier to read. However, they are also faster than for loops. Let\u0026rsquo;s compare the performance of list comprehensions and for loops.\nTwo common benchmarks to test their performance are to append numbers to a list and to square numbers. Let\u0026rsquo;s compare the performance of list comprehensions and for loops for these two tasks.\nimport timeit # Benchmark #1: Append vs. List Comprehension # Append def for_append(): names = [] for i in range(1000000): names.append(i) # print the average out of 10 runs (in milliseconds) print(\u0026#34;Append: \u0026#34; + str(timeit.timeit(for_append, number=10) * 1000)) # List Comprehension def list_comprehension(): names = [i for i in range(1000000)] # print the average out of 10 runs (in milliseconds) print(\u0026#34;Append: \u0026#34; + str(timeit.timeit(for_append, number=10) * 1000)) Append: 371.44755799999984 Append: 351.0218179999356 # Benchmark 2: Squaring Numbers # For loop def for_loop(): squares = [] for i in range(1000000): squares.append(i**2) # print the average out of 10 runs (in milliseconds) print(\u0026#34;For Loop: \u0026#34; + str(timeit.timeit(for_loop, number=10) * 1000)) # List Comprehension def list_comprehension(): squares = [i**2 for i in range(1000000)] # print the average out of 10 runs (in milliseconds) print(\u0026#34;List Comprehension: \u0026#34; + str(timeit.timeit(list_comprehension, number=10) * 1000)) For Loop: 593.4785219997138 List Comprehension: 572.2285600004398 ","date":1693026000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693026000,"objectID":"59bb38b9c22df96f8411c343f872cd6b","permalink":"https://ajdillhoff.github.io/notes/list_comprehensions/","publishdate":"2023-08-26T00:00:00-05:00","relpermalink":"/notes/list_comprehensions/","section":"notes","summary":"Lists and List Comprehensions List comprehensions provide a concise way to create lists, and are often faster than using a for-loop. They are inspired by set-builder notation in mathematics.\nThis notebook demonstrates common list functions as well as the syntax and basic usage of list comprehensions.\nnames = [\u0026#34;Naomi\u0026#34;, \u0026#34;James\u0026#34;, \u0026#34;Amos\u0026#34;, \u0026#34;Bobbie\u0026#34;] # Append \u0026#34;Miller\u0026#34; to the end of the list names.append(\u0026#34;Miller\u0026#34;) # This can also be accomplished using the + operator or the extend() method # These are similar, where `+=` is shorthand for `extend` names.","tags":["python"],"title":"Python List Comprehensions","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Control Flow Control flow allows us to build programs that react to some pre-determined condition. For example, what happens when a user logs in with the correct credentials? What if they don\u0026rsquo;t give valid credentials?\nThis notebook covers the basic tools to writing conditional statements in Python. It follows Chapter 3 in Python for Everyone by Charles Severance along with my own examples.\nBoolean Expressions A boolean expression evaluates to either True or False. This type of expression would be used to check status codes or to check if a user has entered the correct password, for example.\nprint(\u0026#34;1==1 is {}\u0026#34;.format(1==1)) print(\u0026#34;1==2 is {}\u0026#34;.format(1==2)) 1==1 is True 1==2 is False Operators like == are called relational operators and they compare two operands and return either True or False. Other relational operators include:\nx != y # x is not equal to y x \u0026gt; y # x is greater than y x \u0026lt; y # x is less than y x \u0026gt;= y # x is greater than or equal to y x \u0026lt;= y # x is less than or equal to y x is y # x is the same as y x is not y # x is not the same as y The last two operators, is and is not, are used to check if two variables are referencing the same object. The fact that the operands must be objects is important here. You should avoid comparing a value with a variable. Python will let you do this, but it will also output a warning.\nx = 5 y = 5 x is y # True x is 5 # True, but not recommended x is not 5 # False, but not recommended x = 5 y = 10 x is 5 \u0026lt;\u0026gt;:3: SyntaxWarning: \u0026quot;is\u0026quot; with a literal. Did you mean \u0026quot;==\u0026quot;? \u0026lt;\u0026gt;:3: SyntaxWarning: \u0026quot;is\u0026quot; with a literal. Did you mean \u0026quot;==\u0026quot;? /var/folders/vd/wbzsx0g538nfr96xq81fp7k40000gn/T/ipykernel_24914/759655086.py:3: SyntaxWarning: \u0026quot;is\u0026quot; with a literal. Did you mean \u0026quot;==\u0026quot;? x is 5 True Python includes three logical operators that are verbose compared to other languages.\nx and y # True if both x and y are True x or y # True if either x or y are True not x # True if x is False # Example: FizzBuzz # Consider two possible solutions to the FizzBuzz problem # Solution 1 n = 15 if not n % 3 and not n % 5: print(\u0026#34;FizzBuzz\u0026#34;) # Solution 2 n = 15 if n % 3 == 0: print(\u0026#34;Fizz\u0026#34;) if n % 5 == 0: print(\u0026#34;Buzz\u0026#34;) FizzBuzz Fizz Buzz In the second solution, the output was separated to two separate lines since the print function automatically adds a newline. We can change this behavior by adding a second argument to the print function.\nn = 15 if n % 3 == 0: print(\u0026#34;Fizz\u0026#34;, end=\u0026#34;\u0026#34;) if n % 5 == 0: print(\u0026#34;Buzz\u0026#34;) FizzBuzz Conditional Execution We have already used a key conditional execution tool: the if statement. The if statement allows us to execute a block of code if a condition is met. The general syntax is:\nif condition: # code to execute if condition is True Also note that Python is particular about indentation. The code that is executed if the condition is met must be indented. The standard is to use four spaces for each level of indentation.\nWe can also chain conditional statements together using elif and else. The elif statement is short for \u0026ldquo;else if\u0026rdquo; and allows us to check another condition if the previous condition was not met. The else statement is used to execute code if none of the previous conditions were met. The general syntax is:\nif condition: # code to execute if condition is True elif condition: # code to execute if the first condition is False and this condition is True else: # code to execute if all other conditions are False Switch Statements Until version 3.10, Python did not have a switch statement. This is a conditional statement that allows us to check a variable against a series of values.\nWith version 3.10 comes the match statement. This statement is similar to the switch statement in other languages. The general syntax is:\nmatch variable: case value1: # code to execute if variable == value1 case value2: # code to execute if variable == value2 case value3: # code to execute if variable == value3 case _: # code to execute if none of the previous conditions were met language = input(\u0026#34;What is your favorite programming language? \u0026#34;) match language: case \u0026#34;Python\u0026#34;: print(\u0026#34;You\u0026#39;re in the right place.\u0026#34;) case \u0026#34;Java\u0026#34;: print(\u0026#34;Do you despise C++ as much as the creator of Java?\u0026#34;) case \u0026#34;C++\u0026#34;: print(\u0026#34;You probably like game development.\u0026#34;) case \u0026#34;C\u0026#34;: print(\u0026#34;Speed is your thing.\u0026#34;) case _: print(\u0026#34;You like something else!\u0026#34;) You like something else! Unlike other languages that implement a switch statement, Python\u0026rsquo;s match statement does not have a break statement. We can still utilize fall-through behavior by including multiple values in a single case separated by |.\nmatch variable: case value1 | value2: # code to execute if variable == value1 or variable == value2 case value3: # code to execute if variable == value3 case _: # code to execute if none of the previous conditions were met language = input(\u0026#34;What is your favorite programming language? \u0026#34;) match language: case \u0026#34;Python\u0026#34; | \u0026#34;python\u0026#34;: print(\u0026#34;You\u0026#39;re in the right place.\u0026#34;) case \u0026#34;Java\u0026#34;: print(\u0026#34;Do you despise C++ as much as the creator of Java?\u0026#34;) case \u0026#34;C++\u0026#34;: print(\u0026#34;You probably like game development.\u0026#34;) case \u0026#34;C\u0026#34;: print(\u0026#34;Speed is your thing.\u0026#34;) case _: print(\u0026#34;You like something else!\u0026#34;) You like something else! Iterations Iterations allow us to execute a block of code multiple times. This is useful for iterating over a list of items or for executing a block of code until a condition is met.\nPython supports both a while loop and a for loop. The while loop will execute a block of code until a condition is met. The for loop will iterate over a sequence of items.\nFor Loops As opposed to something like C, Python\u0026rsquo;s for loop is more like a foreach loop. The for loop will iterate over a sequence of items. The general syntax is:\nfor item in sequence: # code to execute for each item in the sequence It is commonly used with the range function to iterate over a sequence of numbers. The range function takes three arguments: start, stop, and step. The start argument is the first number in the sequence. The stop argument is the last number in the sequence. The step argument is the amount to increment the sequence by. The step argument is optional and defaults to 1. The stop argument is required. The start argument is optional and defaults to 0.\nfor i in range(5): print(i) While Loops The while loop will execute a block of code until a condition is met. The general syntax is:\nwhile condition: # code to execute while condition is True Lists Lists are a sequence of values. They are similar to arrays in other languages. The values in a list are called elements or items. Lists are mutable, meaning that we can change the values in a list. Lists are also ordered, meaning that the order of the elements in a list is important.\nWe can create a list by separating the elements with commas and surrounding the list with square brackets.\nnumbers = [1, 2, 3, 4, 5] numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9] names = [\u0026#34;Naomi\u0026#34;, \u0026#34;Bobbie\u0026#34;, \u0026#34;James\u0026#34;, \u0026#34;Amos\u0026#34;, \u0026#34;Chrisjen\u0026#34;, \u0026#34;Alex\u0026#34;, \u0026#34;Clarissa\u0026#34;] names_and_numbers = [\u0026#34;Naomi\u0026#34;, 5, \u0026#34;Bobbie\u0026#34;, 7, \u0026#34;James\u0026#34;, 9, \u0026#34;Amos\u0026#34;, 11, \u0026#34;Chrisjen\u0026#34;, 13, \u0026#34;Alex\u0026#34;, 15, \u0026#34;Clarissa\u0026#34;, 17] # We can even include lists in our lists nested_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] Iterating Over Lists We can iterate over a list using a for loop. The for loop will iterate over each element in the list. The general syntax is:\nfor item in list: # code to execute for each item in the list If the list contains tuples, we can use tuple unpacking to assign the values in the tuple to multiple variables.\nnumbers = [(1, 2), (3, 4), (5, 6)] for x, y in numbers: print(x, y) Combining Lists with zip The zip function allows us to combine two lists into a single list of tuples. The first element in the first list will be paired with the first element in the second list, the second element in the first list will be paired with the second element in the second list, and so on. The general syntax is:\nuser_ids = [1, 2, 3] usernames = [\u0026#39;alice\u0026#39;, \u0026#39;bob\u0026#39;, \u0026#39;charlie\u0026#39;] users = zip(user_ids, usernames) user_ids = [1, 2, 3, 4, 5] user_names = [\u0026#34;Naomi\u0026#34;, \u0026#34;Bobbie\u0026#34;, \u0026#34;James\u0026#34;, \u0026#34;Amos\u0026#34;, \u0026#34;Chrisjen\u0026#34;] # We can combine these lists into a single list of tuples user_ids_and_names = zip(user_ids, user_names) # We can also convert the zip object into a list user_ids_and_names = list(user_ids_and_names) for users in user_ids_and_names: print(users) (1, 'Naomi') (2, 'Bobbie') (3, 'James') (4, 'Amos') (5, 'Chrisjen') ","date":1692680400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692680400,"objectID":"6870d25c01cc5ff4afe7ab4ce0e783a2","permalink":"https://ajdillhoff.github.io/notes/control_flow_in_python/","publishdate":"2023-08-22T00:00:00-05:00","relpermalink":"/notes/control_flow_in_python/","section":"notes","summary":"Control Flow Control flow allows us to build programs that react to some pre-determined condition. For example, what happens when a user logs in with the correct credentials? What if they don\u0026rsquo;t give valid credentials?\nThis notebook covers the basic tools to writing conditional statements in Python. It follows Chapter 3 in Python for Everyone by Charles Severance along with my own examples.\nBoolean Expressions A boolean expression evaluates to either True or False.","tags":["python"],"title":"Control Flow in Python","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Programming with Python Variables, Values, and Data Types Basic Operators Statements and Expressions Basic I/O Commenting Code These notes are focused on introducing programming with Python for those without a technical background.\nIntroduction The official website provides the following description of Python.\nPython is an interpreted, interactive, object-oriented programming language. It incorporates modules, exceptions, dynamic typing, very high level dynamic data types, and classes. It supports multiple programming paradigms beyond object-oriented programming, such as procedural and functional programming. Python combines remarkable power with very clear syntax. It has interfaces to many system calls and libraries, as well as to various window systems, and is extensible in C or C++. It is also usable as an extension language for applications that need a programmable interface. Finally, Python is portable: it runs on many Unix variants including Linux and macOS, and on Windows.\nIf you have never studied any programming languages before, much of this description will be useless to you. There is no context for the types of programming (object-oriented, functional, etc.), and the fact that is extensible in C or C++ may mean absolutely nothing.\nWhy Python? Given that this course is for prospective data science practitioners, and the fact that we have less than a month to cover a programming language, Python is a natural choice. It is widely used in the field of Machine Learning and is gaining more and more ground over R (or so I think) for statistics. There are many third-party libraries for data analysis, visualization, and just about any other data science application we can think of.\nHow to use these notes These notes are organized to follow each major topic in Python. They will also follow the free online book Python for Everybody. These particular lecture notes will start with Chapter 2: Variables, expressions, and statements. It is highly recommended that you run the examples on your own machine, and you are encouraged to make changes. Try to break the code and fix it again. Have it output something different and change the program\u0026rsquo;s purpose entirely.\nA Python notebook will accompany each lecture and will be accessible on my GitHub page. I will also include code snippets directly in this article to highlight a particular example or point.\nProgramming is Hard Before we dive into the language itself, there are a few things that are important to keep in mind. First, programming is hard. It is a skill that requires practice. The tools that you use to program are constantly evolving to keep up with the use-cases of the day. A processor is a complex calculator which means we have to be extremely explicit about the instructions we provide. If you are picking this up for the first time, remember to be patient and be kind to yourself. You will be able to work on big projects that are important to you, but we all have to start somewhere.\nResources are Finite Another important thing to remember is that we are working with limited resources. There is only so much memory and storage space that we can access. These notes are not meant to accompany a full course on hardware architectures, so we will use the following diagram to visualize this point.\nFigure 1: Figure 1.3 from Python for Everybody. The closer the memory is to the CPU, the quicker it can be accessed. Most of the algorithms and data structures we will study in this course will be used with data in main memory. The trade off is that memory that is closer to the CPU is more expensive and has reduced capacity when compared with secondary memory. When we start working with larger sources of data, we will need to adapt our solutions to work with memory that is not directly accessible through a local machine. For now, keep this picture in mind as we dive into Python.\nProgramming with Python Programming languages provide the following features:\na way to write instructions (syntax, statements, expressions) a way to execute a complex series of instructions (functions) a way to store the results of computations and represent data (variables, data structures) They mostly differ in how the language is written, the syntax. Consider the following snippet of Python code:\nusername = \u0026#34;test\u0026#34; password = \u0026#34;securePass1\u0026#34; user_logged_in = False if login(username, password) == True: user_logged_in = True Even if you have never seen any Python code before, you could probably figure out what this block of code is doing. First, 3 variables are defined which store the username, password, and a flag that represents whether or not the user is logged into the system.\nThe control statement if is declared to evaluate if an expression is true. This expression calls a function named login and passes the user\u0026rsquo;s credentials as its arguments. We can assume that the call to login is doing something like validating the user\u0026rsquo;s information and registering their request with a server. If the user was successfully logged in, the function will return True. In this case, we can updated our variable user_logged_in to reflect this.\nVariables, Values, and Data Types The first 3 lines in the example above are variable initializations. The first line is username = \u0026quot;test\u0026quot; which instructs our machines to create a new variable named username and assign it the value \u0026quot;test\u0026quot;. All variables require memory to store their values. When a variable is created, our machine will assign it an address so that it knows where to access that variable\u0026rsquo;s value. This concept is rather simple: in order for something to exist, there must be space for it. As Python developers, we will rarely think about where and how these values are being stored.\nMost languages have rules about what names we can give to variables, and Python is no exception. A variable can use any combination of letters, numbers, and underscores, as long as it does not start with a number and is not the same as a reserved word.\nReserved words in Python\nand continue finally is raise as def for lambda return assert del from None True async elif global nonlocal try await else if not while break except import or with class False in pass yield Data Types Different values are represented differently depending on their type. An integer can be represented in binary in a very straightforward manner. 10 in base 10 is represented as 1010 in binary, for example. Characters in a string like \u0026quot;securePass1\u0026quot; are represented using an encoding such as ASCII or Unicode. Real numbers are typically represented as floating-point types using the IEEE 754 Standard for Floating-Point Arithmetic.\nWhen we create a variable in Python, we do not need to explicitly declare what type that variable it is. That is what makes Python dynamically typed. Instead, it will infer the type based on the value. We can always ask Python how it is representing each variable, as seen in the following code.\n\u0026gt;\u0026gt;\u0026gt; type(\u0026#34;securePass1\u0026#34;) \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; \u0026gt;\u0026gt;\u0026gt; type(10) \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; \u0026gt;\u0026gt;\u0026gt; type(3.14) \u0026lt;class \u0026#39;float\u0026#39;\u0026gt; Basic Operators A programming language would be pretty useless if it did not offer some way to do basic arithmetic. Python supports the following arithmetic operators: +, -, *, /, //, ** and %. You may instantly recognize the first 4, but the last 3 may not be so familiar. Let us start with //, integer division.\nAn integer data type cannot represent decimal values. So what happens if you try to execute something like 1 / 2? We recognize this to be 0.5, but that is not the case with every programming language. In C, for example, 1 and 2 are treated as integer types by default. If you attempt to evaluate 1 / 2, the result is 0 since there is no way to store the decimal information. Essentially, the decimal portion of the result is truncated.\nPython is a little more forgiving. 1 / 2 evaluates to 0.5, as we may expect. However, if you want to perform division between these operands as if they were both integers, you can use //. Indeed, 1 // 2 evalutes to 0 in Python.\nThe ** operator is more straightforward: it raises the left-hand operand to whatever value is provided on the right side. For example, 2**4 evaluates to 16.\nFinally, the modulus operator % provides the remainder of integer division. So something like 5 % 2 would return 1.\nStatements and Expressions A statement is anything that can be executed. This could be a simple variable assignment or a function call.\nusername = \u0026#34;user1\u0026#34; print(username == \u0026#34;user1\u0026#34;) An expression is a statement that evaluates into some result. This could be the result of a function call, an assignment, or a complex computation.\ny = x**2 get_user_by_id(10) Basic I/O Python provides functions to read input from the user\u0026rsquo;s keyboard as well as print information back to the terminal using input and print. When input is evaluated, it will wait for the user to press Enter before processing the input. If you would like to provide a text prompt to the user before entering, you can pass the prompt as a string.\ntext = input(\u0026#34;Enter some text: \u0026#34;) print(text) An example run of this program may look like the following.\nEnter some text: OK here it is OK here it is Notice that the result of the input function is the actual data entered by the user. This is immediately assigned to the text variable. We can easily print out the value of text by passing it as an argument to the print function.\nFormatted Output We can work with more detailed output using formatted strings, as demonstrated in the following example.\npi = 3.14159265359 print(f\u0026#34;The value of pi is approximately {pi:.3f}.\u0026#34;) Output\nThe value of pi is approximately 3.141. More details about the different ways of using formatting strings is documented here.\nCommenting Code The last topic of this introduction is about commenting. Communicating the purpose of your program is not only important when working with others, but you will find it to be extremely helpful as you build larger and larger projects. It is a common trap to dive into an idea with absolute focus, quickly hacking away as your program takes shape. This sort of approach is like a house of cards. As soon as your attention is diverted, it takes time to build that model up in your head again.\nFigure 2: Daily experiences with programming. Writing down your program\u0026rsquo;s purpose and design while documenting its function is paramount for a product that is both robust and maintainable. The simplest way to communicate ideas is to leave comments in the code itself. There are two ways to leave basic comments in Python: single-line and multi-line. The code below demonstrates both.\n\u0026#34;\u0026#34;\u0026#34; This small code example shows how to comment in Python. By the way, this is a multi-line comment. \u0026#34;\u0026#34;\u0026#34; a = \u0026#34;user1\u0026#34; # stores the user\u0026#39;s name As see above, multi-line comments are wrapped in \u0026quot;\u0026quot;\u0026quot;. These are typically reserved for things like function documentation (more on that later). Single-line comments start with # and can be placed on the same line as a statement.\nThere is a third type of commenting called self-commenting. The same example above will motivate this type of commenting. There is nothing invalid about the statement a = \u0026quot;user1\u0026quot;. It defines a variable named a whose value is the string \u0026quot;user1\u0026quot;. However, if there wasn\u0026rsquo;t a comment on the same line describing its purpose, it might not be so clear. There is an easier way to communicate this without commenting at all. We could instead write something like username = \u0026quot;user1\u0026quot;. The variable name itself resolves any ambiguity about its purpose and obviates the need for an additional comment.\n","date":1692507600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692507600,"objectID":"d7999fc245dac666c793fcc58759b001","permalink":"https://ajdillhoff.github.io/notes/introduction_to_python/","publishdate":"2023-08-20T00:00:00-05:00","relpermalink":"/notes/introduction_to_python/","section":"notes","summary":"Table of Contents Introduction Programming with Python Variables, Values, and Data Types Basic Operators Statements and Expressions Basic I/O Commenting Code These notes are focused on introducing programming with Python for those without a technical background.\nIntroduction The official website provides the following description of Python.\nPython is an interpreted, interactive, object-oriented programming language. It incorporates modules, exceptions, dynamic typing, very high level dynamic data types, and classes. It supports multiple programming paradigms beyond object-oriented programming, such as procedural and functional programming.","tags":["python"],"title":"Introduction to Python","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Key Terms Defining Goals Policies and Values Bellman Equations Optimality Optimizing the Policy Key Terms Agent: The learner or decision maker. Environment: The world that the agent can interact with. State: A representation of the agent and environment. Action: The agent can take an action in the environment. Reward: Given to the agent based on actions taken. Goal: Maximize rewards earned over time.\nAt time \\(t\\), the agent observes the state of the environment \\(S_t \\in \\mathcal{S}\\) and can select an action \\(A_t \\in \\mathcal{A}(s)\\), where \\(\\mathcal{A}(s)\\) suggests that the available actions are dependent on the current state. At time \\(t + 1\\), the agent receives a reward \\(R_{t+1} \\in \\mathcal{R}\\).\nFigure 1: The agent-environment in a Markov decision process (Credit: Sutton \u0026amp; Barto). To improve its knowledge about an environment or increase its performance on a task, an agent must first be able to interpret or make sense of that environment in some way. Second, there must be a well defined goal. For an agent playing Super Mario, for example, the goal would be to complete each level while maximizing the score. Third, the agent must be able to interact with its environment by taking actions. If the Mario-playing agent could not move Mario around, it would never be able to improve. If the agent makes a decision which leads to Mario\u0026rsquo;s untimely demise, it would update its knowledge of the world so that it would tend towards a more favorable action. These three requirements: sensations, actions, and goals, are encapsulated by Markov Decision Processes.\nA Markov decision process is defined by\n\\(\\mathcal{S}\\) - a set of states, \\(\\mathcal{A}\\) - a set of actions, \\(\\mathcal{R}\\) - a set of rewards, \\(P\\) - the transition probability function to determine transition between states, \\(\\gamma\\) - discount factor for future rewards. At time \\(t\\), an agent in state \\(S_t\\) selects an action \\(A_t\\). At \\(t+1\\), it receives a reward \\(R_{t+1}\\) based on that action.\nIn a finite MDP, the states, actions, and rewards have a finite number of elements. Random variables \\(R_t\\) and \\(S_t\\) have discrete probability distributions dependent on the preceding state and action.\n\\[ p(s\u0026rsquo;, r|s, a) = P\\{S_t = s\u0026rsquo;, R_t = r|S_{t-1} = s, A_{t-1} = a\\} \\]\nIf we want the state transition probabilities, we can sum over the above distribution:\n\\[ p(s\u0026rsquo;|s, a) = P\\{S_t = s\u0026rsquo;|S_{t-1} = s, A_{t-1}=a\\} = \\sum_{r\\in\\mathcal{R}}p(s\u0026rsquo;, r|s, a). \\]\nThe reward function \\(r\\) gives the expected next reward given some state and action:\n\\[ r(s, a) = \\mathbb{E}[R_{t+1}|S_t=s, A_t=a] = \\sum_{r}r \\sum_{s\u0026rsquo;}p(s\u0026rsquo;, r|s, a). \\]\nDefining Goals In reinforcement learning, the goal is encoded in the form of a reward signal. The agent sets out to maximize the total amount of reward it receives over an episode. An episode is defined dependent on the problem context and ends in a terminal state. It could be a round of game, a single play, or the result of moving a robot. Typically, the rewards come as a single scalar value at teach time step. This implies that an agent might take an action that results in a negative reward if it is optimal in the long run.\nFormally, the expected return includes a discount factor that allows us to control the trade-off between short-term and long-term rewards:\n\\[ G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}, \\]\nwhere \\(0 \\leq \\gamma \\leq 1\\). This can be written in terms of the expected return itself as well:\n\\[ G_t = R_{t+1} + \\gamma G_{t+1}. \\]\nPolicies and Values Two important concepts that help our agent make decisions are the policy and value functions. A policy, typically denoted by \\(\\pi\\), maps states to actions. Such a function can be deterministic, \\(\\pi(s) = a\\), or stochastic, \\(\\pi(a|s)\\).\nThe value of a particular state under a policy \\(\\pi\\) is defined as\n\\[ v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t | S_t = s] = \\mathbb{E}_{\\pi}\\Bigg[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}\\Bigg|S_t=s\\Bigg]. \\]\nWe also must define the value of taking an action \\(a\\) in state \\(s\\) following policy \\(\\pi\\):\n\\[ q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_t|S_t=s, A_t=a] = \\mathbb{E}_{\\pi}\\Bigg[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}\\Bigg|S_t=s, A_t=a\\Bigg]. \\]\nThis function defines the expected return of following a particular policy and starting in state \\(s\\). Both the state-value and action-value functions can be updated as a result of the agent\u0026rsquo;s experience. How it is updated is method-dependent. Certain methods will also dictate how the policy itself can be updated.\nBellman Equations The recursive relationship between the value of a state and its future states can be represented using Bellman equations. In RL, we are interested in the equations for both the state-value and action-value. Given the diagram of an MDP, we can see that they are related to each other. To make the following equations easier to understand, it is important to remember the flow of a Markov decision process:\ntake an action, arrive at a state and sense the reward, consult the policy for the next action. With that in mind, let\u0026rsquo;s look at the state-value function first. This function considers the expected value of starting in a state \\(s\\) and following policy \\(\\pi\\). In other words, we must consider all possible actions and their future rewards.\n\\begin{align*} v_{\\pi}(s) \u0026amp;= \\mathbb{E}_{\\pi}[G_t | S_t = s]\\\\ \u0026amp;= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t = s]\\\\ \u0026amp;= \\sum_{a} \\pi(a|s) \\sum_{s\u0026rsquo;}\\sum_{r}p(s\u0026rsquo;, r|s, a)\\Big[r + \\gamma \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t = s]\\Big]\\\\ \u0026amp;= \\sum_{a} \\pi(a|s) \\sum_{s\u0026rsquo;, r}p(s\u0026rsquo;, r|s, a)\\Big[r + \\gamma v_{\\pi}(s\u0026rsquo;)\\Big]\\\\ \u0026amp;= \\sum_{a} \\pi(a | s)\\big[r(s, a) + \\gamma \\sum_{s\u0026rsquo;}p(s\u0026rsquo;|s, a)v_{\\pi}(s\u0026rsquo;)\\big]\\\\ \\end{align*}\nThe first sum over actions considers all possible actions. This is followed by a transition to possible states \\(s\u0026rsquo;\\) conditioned on taking each action multiplied by the expected value of being at the new state.\nThe action-value function follows a similar derivation:\n\\begin{align*} q_{\\pi}(s, a) \u0026amp;= \\mathbb{E}_{\\pi}[G_t|S_t = s, A_t = a]\\\\ \u0026amp;= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t = s, A_t = a]\\\\ \u0026amp;= r(s, a) + \\sum_{s\u0026rsquo;}p(s\u0026rsquo;|s, a) v_{\\pi}(s\u0026rsquo;) \\end{align*}\nThere is a very similar looking set of terms in the state-value function above, and we should expect that! If we want to evaluate the current state, we need to look ahead at the possible actions and their resulting rewards. Similarly, evaluating the current action requires us to look head at the value of future states.\nLet\u0026rsquo;s expand \\(q_{\\pi}(s,a)\\) once more so that it is written in terms of itself.\n\\begin{align*} q_{\\pi}(s, a) \u0026amp;= r(s, a) + \\gamma \\sum_{s\u0026rsquo;}p(s\u0026rsquo;|s, a) v_{\\pi}(s\u0026rsquo;)\\\\ \u0026amp;= r(s, a) + \\gamma \\sum_{s\u0026rsquo;}p(s\u0026rsquo;|s, a) \\sum_{a\u0026rsquo;} \\pi(s\u0026rsquo;, a\u0026rsquo;) \\big[r(s\u0026rsquo;, a\u0026rsquo;) + \\gamma \\sum_{s\u0026rsquo;\u0026rsquo;} p(s\u0026rsquo;\u0026rsquo;|s\u0026rsquo;, a\u0026rsquo;)v_{\\pi}(s\u0026rsquo;\u0026rsquo;)]\\\\ \u0026amp;= r(s, a) + \\gamma \\sum_{s\u0026rsquo;}p(s\u0026rsquo;|s, a) \\sum_{a\u0026rsquo;} \\pi(s\u0026rsquo;, a\u0026rsquo;) q_{\\pi}(s\u0026rsquo;, a\u0026rsquo;) \\end{align*}\nOptimality To solve a reinforcement learning problem, we are interested in finding the policy \\(\\pi_{*}\\) whose expected return is greater than all other possible policies over all states. An optimal policy will use an *optimal state-value function and optimal action-value function:\n\\begin{align*} v_{*}(s) \u0026amp;= \\max_{\\pi}v_{\\pi}(s)\\\\ q_{*}(s, a) \u0026amp;= \\max_{\\pi}q_{\\pi}(s, a). \\end{align*}\nThe optimal state-value function would select the best possible action instead of summing over all possibley actions starting in state \\(s\\):\n\\begin{align*} v_{*}(s) \u0026amp;= \\max_{a} q_{\\pi_*}(s, a)\\\\ \u0026amp;= \\max_{a}\\big[r(s, a) + \\gamma \\sum_{s\u0026rsquo;} p(s\u0026rsquo;|s, a) v_{*}(s\u0026rsquo;)\\big] \\end{align*}\nSimilarly, the optimal action-value function selects the best possible action from the next state \\(s\u0026rsquo;\\):\n\\[ q_{*}(s) = r(s, a) + \\gamma \\sum_{s\u0026rsquo;} p(s\u0026rsquo;|s, a) \\max_{a} q_{*}(s\u0026rsquo;, a\u0026rsquo;). \\]\nOptimizing the Policy For smaller problems with reasonably small state and action spaces, we can use Dynamic Programming to compute the optimal policy. These methods quickly become intractable as the complexity of our problem increases. As is common in machine learning, we would resort to approximation methods for complex spaces.\n\u0026ldquo;In fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.\u0026rdquo;\n\u0026ndash; Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction\nImagine if you had a set policy that dictated the actions you would take from work to home. In this example, assume the policy is not an optimal policy. One day, you decide to take a left at a particular intersection rather than going forward. After that, you follow your policy as described. If this decision ultimately resulted in you arriving home sooner, you would probably update your policy to always take that left. This intuition describes a result of the policy improvement theorem.\nLet \\(\\pi\\) and \\(\\pi\u0026rsquo;\\) be two deterministic policies where\n\\[ q_{\\pi}(s, \\pi\u0026rsquo;(s)) \\geq v_{\\pi}(s),\\quad \\forall s. \\]\n","date":1690174800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690174800,"objectID":"29ea88dc65c270723dd6982dc5081bf4","permalink":"https://ajdillhoff.github.io/notes/markov_decision_processes/","publishdate":"2023-07-24T00:00:00-05:00","relpermalink":"/notes/markov_decision_processes/","section":"notes","summary":"Table of Contents Key Terms Defining Goals Policies and Values Bellman Equations Optimality Optimizing the Policy Key Terms Agent: The learner or decision maker. Environment: The world that the agent can interact with. State: A representation of the agent and environment. Action: The agent can take an action in the environment. Reward: Given to the agent based on actions taken. Goal: Maximize rewards earned over time.\nAt time \\(t\\), the agent observes the state of the environment \\(S_t \\in \\mathcal{S}\\) and can select an action \\(A_t \\in \\mathcal{A}(s)\\), where \\(\\mathcal{A}(s)\\) suggests that the available actions are dependent on the current state.","tags":["reinforcement learning"],"title":"Markov Decision Processes","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":"Notes from (Friedman 2001) Many machine learning methods are parameterized functions that are optimized using some numerical optimization techniques, notably steepest-descent.\nInitial learner is a stump, subsequent learners are trees with depth as some power of 2 (commonly).\nNumerical optimization in function space \\[ g_m(\\mathbf{x}) = E_y\\Big[\\frac{\\partial L(y, F(\\mathbf{x}))}{\\partial F(\\mathbf{x})}|\\mathbf{x}\\Big]_{F(\\mathbf{x})=F_{m-1}(\\mathbf{x})} \\] The optimal step size found by solving\n\\[ \\rho_m = \\mathop{\\arg \\min}_{\\rho} E_{y,\\mathbf{x}}L(y,F_{m-1}(\\mathbf{x})-\\rho g_m(\\mathbf{x})) \\] Then the function \\(m\\) is updated: \\[ f_m(\\mathbf{x}) = -\\rho_m g_m(\\mathbf{x}) \\]\nWalking through it\u0026hellip;\nMake an initial guess with \\(f_0(\\mathbf{x})\\)\nEvaluate \\(L(y, f_0(\\mathbf{x}))\\)\nImprove model by boosting \\(f_1(\\mathbf{x}) = -\\rho_1 g_1(\\mathbf{x})\\), where \\[ g_1(\\mathbf{x}) = \\frac{\\partial L(y, f_0(\\mathbf{x}))}{\\partial f_0(\\mathbf{x})}. \\] This implies that \\(f_1\\) is predicting the gradient of the previous function.\nIf the model is nonparametric, the expected value of the function conditioned on the input cannot be estimated accurately because we cannot sample the entire distribution of \\(\\mathbf{x}\\). The author\u0026rsquo;s note that \u0026ldquo;\u0026hellip;even if it could, one would like to estimate \\(F^*(\\mathbf{x})\\) at \\(\\mathbf{x}\\) values other than the training sample points.\u0026rdquo;\nSmoothness is imposed by approximating the function with a parametric model. I think this means that the distribution is approximated as well.\n\\begin{equation} (\\beta_m, \\mathbf{a}_m) = \\mathop{\\arg \\min}_{\\beta, \\mathbf{a}}\\sum_{i=1}^N L(y_i, F_{m-1}(\\mathbf{x}_i) + \\beta h(\\mathbf{x}_i; \\mathbf{a})) \\end{equation}\nWhat if a solution to the above equation is difficult to obtain? Instead, view \\(\\beta_m h(\\mathbf{x};\\mathbf{a}_m)\\) as the best greedy step toward \\(F^*(\\mathbf{x})\\), under the constraint that the step direction, in this case \\(h(\\mathbf{x};\\mathbf{a}_m)\\), is a member of the class of functions \\(h(\\mathbf{x};\\mathbf{a})\\). The negative gradient can be evaluated at each data point: \\[ -g_m(\\mathbf{x}_i) = -\\frac{\\partial L(y_i, F_{m-1}(\\mathbf{x}_i))}{\\partial F_{m-1}(\\mathbf{x}_i)}. \\]\nThis gradient is evaluated at every data point. However, we cannot generalize to new values not in our dataset. The proposed solution comes via \\(\\mathbf{h}_m = \\{h(\\mathbf{x}_i;\\mathbf{a}_m)\\}_{1}^N\\) \u0026ldquo;most parallel to\u0026rdquo; \\(-\\mathbf{g}_m \\in \\mathbb{R}^N\\).\nAs long as we can compute a derivative for the original loss function, our subsequent boosting problems are solved via least-squared error: \\[ \\mathbf{a}_m = \\mathop{\\arg \\min}_{\\mathbf{a}, \\beta} \\sum_{i=1}^N \\Big(-g_m(\\mathbf{x}_i)-\\beta h(\\mathbf{x}_i;\\mathbf{a})\\Big)^2 \\]\nFigure 1: Original generic algorithm from (Friedman 2001). Check out a basic implementation in Python here.\nReferences Friedman, Jerome H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” The Annals of Statistics 29 (5): 1189–1232. https://www.jstor.org/stable/2699986. ","date":1689570000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689570000,"objectID":"f575924412ebf4cfa533e035a22bcd88","permalink":"https://ajdillhoff.github.io/notes/gradient_boosting/","publishdate":"2023-07-17T00:00:00-05:00","relpermalink":"/notes/gradient_boosting/","section":"notes","summary":"Notes from (Friedman 2001) Many machine learning methods are parameterized functions that are optimized using some numerical optimization techniques, notably steepest-descent.\nInitial learner is a stump, subsequent learners are trees with depth as some power of 2 (commonly).\nNumerical optimization in function space \\[ g_m(\\mathbf{x}) = E_y\\Big[\\frac{\\partial L(y, F(\\mathbf{x}))}{\\partial F(\\mathbf{x})}|\\mathbf{x}\\Big]_{F(\\mathbf{x})=F_{m-1}(\\mathbf{x})} \\] The optimal step size found by solving\n\\[ \\rho_m = \\mathop{\\arg \\min}_{\\rho} E_{y,\\mathbf{x}}L(y,F_{m-1}(\\mathbf{x})-\\rho g_m(\\mathbf{x})) \\] Then the function \\(m\\) is updated: \\[ f_m(\\mathbf{x}) = -\\rho_m g_m(\\mathbf{x}) \\]","tags":["machine learning"],"title":"Gradient Boosting","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":"Introduction Hidden Markov Models provide a way of modeling the dynamics of sequential information. They have been used for speech recognition, part-of-speech tagging, machine translation, handwriting recognition, and, as we will see in this article, gesture recognition.\nConsider a somewhat practical use-case: you are going to throw a party with a meticulously curated playlist. You would rather not let anyone have the remote as it might get lost, and letting anyone interrupt the playlist with their own selections may derail the entire event. However, you still want to give your guests the ability to control the volume and skip back and forth between tracks in the playlist. We will also assume that guests will use change tracks and control the volume responsibly.\nThe solution to this problem is to implement a gesture recognition system to identify simple hand motions. In this case, we only have to model 4 separate gestures: VolumeUp, VolumeDown, PrevTrack, NextTrack. Since the motions are temporal in nature, we can model each gesture using Hidden Markov Models. First, we need to cover a bit of background on what a Hidden Markov Model actually is.\nBackground First, introduce Markov Chains Then the Markov assumption At the core of our problem, we want to model a distribution over a sequence of states. Consider a sequence of only 3 states \\(p(x_1, x_2, x_3)\\). The full computation of this can be done using the chain rule of probability:\n\\[ p(x_1, x_2, x_3) = p(x_1) p(x_2 | x_1) p(x_3 | x_1, x_2). \\]\nIf the random variables of our problem are not conditionally independent, the complexity of calculating this is exponential in the number of random variables.\nThe Markov in Hidden Markov Models addresses this complexity. The Markov Assumption states that the probability of an event at time \\(t\\) is conditioned only on the previously observed event: \\(p(x_t | x_{t-1})\\). This is compactly represented with a graphical model, as seen in figure TODO.\nTODO: Figure of basic Markov Chain\nThe hidden qualifier comes from the fact that the data we wish to model was generated from some underlying process that is not directly observable. A classic example for HMMs uses the weather. Imagine you had a log which had the number of water bottles a person had drank per day over the entire year. To make the problem slightly more difficult, the log entries were not associated with a date. It is reasonable to say that the amount of water a person drinks is influenced by how hot or cold it is on a particular day. So, the hidden state in this case is the weather: hot or cold. We can model this with an HMM by establishing that the amount of water (observed state) is conditioned on the weather (hidden state). Figure TODO shows this HMM graphically.\nFigure 1: An HMM with 4 states and 2 observation symbols (y_1) or (y_2). Formally, a Hidden Markov Model is defined by\nThe number of hidden states \\(N\\). A transition probability matrix \\(A \\in \\mathbb{R}^{N \\times N}\\), where \\(a_{ij} = p(z_t = j | z_{t-1} = i)\\). An observation symbol probability distribution \\(B = \\{b_j(k)\\} = p(\\mathbf{x}_t = k | z_t = j)\\). An initial state distribution \\(\\pi_i = p(z_t = i)\\). The trainable parameters of our model are \\(\\lambda = (A, B, \\pi)\\).\nFunctions of an HMM Given the basic definition of what an HMM is, how can we train the parameters defined in \\(\\lambda\\). If we somehow already knew the parameters, how can we extract useful information from the model? Depending on our task, we can use HMMs to answer many important questions:\nFiltering computes \\(p(z_t | \\mathbf{x}_{1:t})\\). That is, we are computing this probability as new samples come in up to time \\(t\\). Smoothing is accomplished when we have all the data in the sequence. This is expressed as \\(p(z_t|\\mathbf{x}_{1:T})\\). Fixed lag smoothing allows for a trade off between accuracy and delay. It is useful in cases where we might not have the full sequence, but we wish to compute \\(p(z_{t-l}|\\mathbf{x}_{1:t})\\) for some \\(l \u0026gt; 0\\). Predictions are represented as \\(p(z_{t+h}|\\mathbf{x}_{1:t})\\), where \\(h \u0026gt; 0\\). MAP estimation yields the most probably state sequence \\(\\text{arg}\\max_{\\mathbf{z}_{1:T}}p(\\mathbf{z}_{1:T}|\\mathbf{x}_{1:T})\\). We can sample the posterior \\(p(\\mathbf{z}_{1:T}|\\mathbf{x}_{1:T})\\). We can also compute \\(p(\\mathbf{x}_{1:T})\\) by summing up over all hidden paths. This is useful for classification tasks. Of course not all of these functions make sense for every possible task, more on that later. This article is not meant to be an exhaustive resource for all HMM functions; we will only look at the tasks necessary to train and use HMMs for isolated gesture recognition TODO: offer additional reading suggestions.\nData Processing As far as the efficacy of our model goes, how we process the data is the most important. Our system will start with a camera that records our guests performing one of the four simple motions. For simplicity, let\u0026rsquo;s pretend that the camera has an onboard chip that detects the 2D centroids of the left hand for each frame. That helps a lot, but there is still the problem of isolating a group of frames based on when the user wanted to start and finish the command. Assuming we have a solution for both of these problems, we still need to take into account that users will gesture at different speeds. Since all of these problems are challenging in their own right, we will assume the computer vision fairy has taken care of this for us.\nEach gesture in our dataset consists of 30 \\((x, y)\\) locations of the center of the left hand with respect to image coordinates. Even with this simplified data, we have another problem: different users may gesture from different locations. The hand locations for one user performing the VolumeUp gesture may be vastly different from another. This isn\u0026rsquo;t too bad to deal with. We could normalize or training data by subtracting the location of the hand in the first frame from the gesture. That way every input would start at \\((0, 0)\\). We can simplify this even further by using relative motion states.\nRelative Motion States Relative motion states discretize our data, thus simplifying the input space. The idea is quite simple: if the hand moved to the right relative to the previous frame, we assign \\(x = 1\\) for that frame. If it moved to the left, assign \\(x = -1\\). If it didn\u0026rsquo;t move at all, or did not move a significant amount, assign \\(x = 0\\). We apply similar rules for the \\(y\\) locations as well. The TODO: figure below shows the relative motion grid.\nBesides greatly simplifying our input space, meaning we can use a simple categorical distribution to model these observations, we no longer have to worry about the discrepency between where each user performed the gesture.\nModeling a Gesture Our system will consist of 4 HMM models to model the dynamics of each gesture. To determine which gesture was performed, we will given our input sequence to each one and have it compute \\(p(\\mathbf{x}_{1:T}; \\lambda_i)\\), the probability of the observation given the parameters of model \\(i\\). Whichever model gives the high probability wins.\nTODO\nDescribe EM at a high level, show the breakdown of probabilities that need to be known Go into forward-backwards Go back to EM and plug them in Training: Expectation-Maximization If we cannot observe the hidden states directly, how are we supposed to update the model parameters \\(\\lambda = (A, B, \\pi)\\)? We may not have all of the information, but we do have some information. We can use that to fill in the missing values with what we would expect them to be given what we already know. Then, we can update our parameters using those expected values. This is accomplished through a two-stage algorithm called Expectation-Maximization. Those familiar with k-Nearest Neighbors should already be familiar with this process.\nUpdating with Perfect Information It is useful to know how we would update our parameters assuming we had perfect information. If the hidden states were fully observable, then updating our model parameters would be as straightforward as computing the maximum likelihood estimates. For \\(A\\) and \\(\\pi\\), we first tally up the following counts:\n\\[ \\hat{a}_{ij} = \\frac{N_{ij}}{\\sum_j N_{ij}}, \\]\nthe number of times we expect to transition from \\(i\\) to \\(j\\) divided by the number of times we transition from \\(i\\) to any other state. Put simply, this computes the expected transitions from \\(i\\) to \\(j\\) normalized by all the times we expect to start in state \\(i\\).\nFor \\(\\pi\\), we have\n\\[ \\hat{\\pi_i} = \\frac{N_i}{\\sum_i N_i}, \\]\nthe number of times we expect to start in state \\(i\\) divided by the number of times we start in any other state.\nEstimating the parameters for \\(B\\) depends on which distribution we are using for our observation probabilities. For a multinomial distribution, we would compute the number of times we are in state \\(j\\) and observe a symbol \\(k\\) divided by the number of times we are in state \\(j\\):\n\\[ \\hat{b}_{jk} = \\frac{N_{jk}}{N_k}, \\]\nwhere\n\\[ N_{jk} = \\sum_{i=1}^N \\sum_{t=1}^T \\mathbb{1} (z_{i, t}=j, x_{i, t}=k). \\]\nIt is also common to model our emission probability using a Normal distribution. We can even use a parameterized model like a neural network. TODO: provide links to examples of these\nUpdating with Missing Information Now to the real problem: fill in our missing information using our observable data and the current parameter estimates. There are two important statistics that we need to compute, called the sufficient statistics.\nThe expected number of transitions from \\(i\\) to \\(j\\). The expected number of times we are transitioning from \\(i\\) to any other state. Both of these can be computed starting with the same probability conditioned on our observable data:\n\\[ p(z_t = i, z_{t+1} = j|\\mathbf{x}_{1:T}). \\]\nForwards-Backwards Algorithm Computing joint distribution can be very computationally expensive. Fortunately for us, the Markov assumption along with operations on graphs open the door to a dynamic programming approach named the Forward-Backward algorithm.\nThe Forwards-Backwards Algorithm, also known as the Baum-Welch algorithm, provides an effective solution to computing the joint described above. In fact, there are many useful distributions that can be computed with this algorithm such as the filtering and smoothing tasks.\nForward Probability The forward probability, often denoted as \\(\\alpha\\), represents the probability of ending up at a particular hidden state \\(i\\) at time \\(t\\) having seen the observations up to that time:\n\\[ \\alpha_t(i) = p(z_t = i, \\mathbf{x}_{1:t} | \\lambda). \\]\nThis value is computed recursively starting from \\(t=1\\) and going forwards to \\(t=T\\).\nInitialization\nFor \\(t=1\\), we calculate:\n\\[ \\alpha_1(i) = \\pi_i b_i(x_1),\\quad 1 \\leq i \\leq N, \\]\nwhere \\(\\pi_i\\) is the initial probability of state \\(i\\) and \\(b_i(x_1)\\) is the emission probability of the first observation \\(x_1\\) given that we are in state \\(i\\).\nRecursion\nAfter that, we calculate the remaining \\(\\alpha_t(i)\\) as follows:\n\\[ \\alpha_{t+1}(j) = b_j(x_{t+1}) \\sum_{i=1}^{N} \\alpha_{t}(i)a_{ij}, \\]\nwhere \\(N\\) is the number of hidden states, and \\(a_{ij}\\) is the transition probability from state \\(i\\) to state \\(j\\).\nBackward Probability The backward probability, denoted as \\(\\beta\\), gives the probability of observing the remaining observations from time \\(t+1\\) to \\(T\\) given that we are in state \\(i\\) at time \\(t\\):\n\\[ \\beta_t(i) = p(\\mathbf{x}_{t+1:T} | z_t = i, \\lambda). \\]\nAgain, this is calculated recursively but this time starting from \\(t=T\\) and going backwards to \\(t=1\\).\nInitialization\nFor \\(t=T\\), we initialize:\n\\[ \\beta_T(i) = 1, \\forall i. \\]\nRecursion\nThen we calculate the remaining \\(\\beta_t(i)\\) as:\n\\[ \\beta_{t}(i) = \\sum_{j=1}^{N} a_{ij}b_j(x_{t+1})\\beta_{t+1}(j). \\]\nCalculating the Sufficient Statistics With these two sets of probabilities, we can calculate the two required sufficient statistics as follows:\nThe expected number of transitions from \\(i\\) to \\(j\\): \\[ \\frac{\\sum_{t=1}^{T-1} \\alpha_t(i) a_{ij} b_j(x_{t+1}) \\beta_{t+1}(j)}{P(X|\\lambda)} \\]\nThe expected number of times we are transitioning from \\(i\\) to any other state: \\[ \\frac{\\sum_{t=1}^{T-1} \\alpha_t(i) \\beta_t(i)}{P(X|\\lambda)} \\]\nWhere \\(P(X|\\lambda)\\) is the total probability of the observations, calculated as:\n\\[ P(X|\\lambda) = \\sum_{i=1}^{N} \\alpha_T(i) \\]\nHow does this give us \\(p(z_t = i, z_{t+1} = j|\\mathbf{x}_{1:T})\\)? To understand how the variables of the Forwards-Backwards algorithm relate to the original probabilities, we can express the term \\(p(z_t = i, z_{t+1} = j|\\mathbf{x}_{1:T})\\) in terms of the original probability distributions in the HMM:\n\\(\\pi_i\\) - the probability of starting in state \\(i\\), \\(a_{ij}\\) - the probability of transitioning from state \\(i\\) to state \\(j\\), \\(b_j(x_t)\\) - the probability that state \\(j\\) will emit observation \\(x_t\\). The joint probability \\(p(z_t = i, z_{t+1} = j, \\mathbf{x}_{1:T})\\) would represent the probability of being in state \\(i\\) at time \\(t\\), moving to state \\(j\\) at time \\(t+1\\), and observing the sequence of emissions \\(\\mathbf{x}_{1:T}\\). This can be factored as follows due to the Markov property:\n\\[ p(z_t = i, z_{t+1} = j, \\mathbf{x}_{1:T}) = p(\\mathbf{x}_{1:t}, z_t = i)p(z_{t+1} = j| z_t = i)p(\\mathbf{x}_{t+1:T} | z_{t+1} = j, \\mathbf{x}_{1:t}). \\]\nUsing our definitions of \\(\\alpha\\) and \\(\\beta\\), we can rewrite this in terms of our HMM quantities:\n\\[ p(z_t = i, z_{t+1} = j, \\mathbf{x}_{1:T}) = \\alpha_t(i)a_{ij}b_j(x_{t+1})\\beta_{t+1}(j). \\]\nHere, \\(\\alpha_t(i)\\) represents \\(p(\\mathbf{x}_{1:t}, z_t = i)\\), the joint probability of the observations until time \\(t\\) and being in state \\(i\\) at time \\(t\\), and \\(\\beta_{t+1}(j)\\) represents \\(p(\\mathbf{x}_{t+1:T} | z_{t+1} = j)\\), the probability of the observations from time \\(t+1\\) to \\(T\\) given we\u0026rsquo;re in state \\(j\\) at time \\(t+1\\).\nThen, to obtain \\(p(z_t = i, z_{t+1} = j|\\mathbf{x}_{1:T})\\), we divide by \\(p(\\mathbf{x}_{1:T})\\) to normalize the probabilities, which is the sum over all states of \\(\\alpha_T(i)\\), or equivalently, the sum over all states of \\(\\beta_1(i)\\pi_i b_i(x_1)\\).\nThis gives us:\n\\[ p(z_t = i, z_{t+1} = j|\\mathbf{x}_{1:T}) = \\frac{\\alpha_t(i)a_{ij}b_j(x_{t+1})\\beta_{t+1}(j)}{\\sum_{i=1}^{N}\\alpha_T(i)}. \\]\nThis is the same expression as before, but broken down in terms of the original HMM quantities and the forward and backward variables. This can also be explained through graph properties and operations. See Sargur Srihari\u0026rsquo;s excellent lecture slides for more details.\nImplementation in Python Conclusion ","date":1689397200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689397200,"objectID":"f274eff17c529462e71940e1c4965e60","permalink":"https://ajdillhoff.github.io/articles/intro_to_hmms/","publishdate":"2023-07-15T00:00:00-05:00","relpermalink":"/articles/intro_to_hmms/","section":"articles","summary":"Hidden Markov Models provide a way of modeling the dynamics of sequential information. They have been used for speech recognition, part-of-speech tagging, machine translation, handwriting recognition, and, as we will see in this article, gesture recognition.","tags":["machine learning"],"title":"An Introduction to Hidden Markov Models for Gesture Recognition","type":"articles"},{"authors":null,"categories":null,"content":" Table of Contents Topics Introduction Definition Markov Decision Processes RL vs. MDP Passive RL Resources Topics What is reinforcement learning? Examples Finite Markov Decision Processes Passive vs. Active Methods Adaptive Dynamic Programming Monte Carlo Methods Temporal-Different Learning Q-Learning Function Approximation Deep Q-Learning Policy and Value Iteration Case Studies Introduction When placed in a new environment, we orient and learn about it by interacting with it. When given a novel task, we may not be able to explicitly describe the task or even perform well at it, but we can learn a lot about it through trial and error.\nImage if you were to be thrown into an alien world teaming with unknown life forms. You would not be able to identify these life forms, but you would be able to learn about their behaviors, shape, surface anatomy, and other attributes based on perception alone. Simply learning about the structure of the environment is the task of unsupervised learning.\nIn supervised machine learning, there is typically some objective function that is minimized based on ground truth or target variables that are known ahead of time. In settings like the ones depicted above, there is no form of supervision. Instead, a representation of the environment is learned from one\u0026rsquo;s experience and sensory input.\nReinforcement learning maps environment input to actions. For example, an agent trained to play Chess will evaluate the current board and make a decision based on experience. Whether or not that decision was beneficial may not immediately be known. If it results in winning against an opponent, the model would strengthen its knowledge of that particular play. The concept of trial-and-error and the fact that an agent may not know if the correct decision was made until later are two of the most important concepts of reinforcement learning.\nThere are many challenges present in reinforcement learning. The agents consider the entire environment and attempt to maximize some reward value based on a well defined goal. There is then a trade off between exploiting actions that are known to give a positive reward and exploring new actions that may lead to a better payoff.\nReinforcement learning is the study of goal-seeking agents that can sense some aspect of their environment and can perform actions that directly affect that environment. An agent is not always a robot, although it is a popular use case. Taking from recent popularity, agents such as AlphaGo exist within a game playing environment where their goal is win.\nExamples Gaming: Agents for traditional board games such as Chess and Go have been implemented, surpassing even the best players in the world. Video games are also a popular environment for constructing complex agents. Developers can use RL to construct sophisticated characters in the game that behave in complex way as well as react more realistically towards the player. DeepMind introduced a powerful agent for StarCaft II which beat some of the best human players in the world. OpenAI debuted OpenAI Five at the 2019 Dota 2 World Championships, defeating the championship team back-to-back.\nOpenAI released OpenAI gym, now an open source project named Gymnasium, which is a library for developing and comparing reinforcement learning algorithms. One of recent interest is RLGym, a Rocket League agent.\nDefinition A reinforcement learning system is identified by the environment and an agent acting in that environment. An agent\u0026rsquo;s behavior in the environment is defined by a policy. A policy usually describes a set of rules in response to the current state of the environment.\nIn order to improve on some task, a reward signal is defined. Over time, an agent should maximize the reward. In general, an action leading to a favorable outcome should present a high reward value.\nA value function describes the estimated total reward over the long run given the agent\u0026rsquo;s current state. Taking an action that results in an immediate reward may lead to a lower payoff in the long run. This can be predicted from the value function.\nAdditionally, a model of the environment will include prior knowledge about that environment. This allows the agent to act optimally over a longer sequence of states.\nMarkov Decision Processes RL vs. MDP Reinforcement learning does not assume the transition model or the reward function.\nPassive RL The policy is fixed while the transition model and reward function are learned over time. The goal is to compute the utility of each state.\nAdaptive Dynamic Programming \\(R(s)\\) and \\(p(s\u0026rsquo;|s,a)\\) can be updated at each step based solely on the observations. Using these observations, the utility of each state can be estimated following policy evaluation (TODO: link algorithm for policy evaluation).\nResources https://web.stanford.edu/class/cs234/modules.html https://lilianweng.github.io/posts/2018-02-19-rl-overview/ ","date":1689138000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689138000,"objectID":"ecba1c8a3e4b25ff5f3fb7fdc2a0f549","permalink":"https://ajdillhoff.github.io/notes/reinforcement_learning/","publishdate":"2023-07-12T00:00:00-05:00","relpermalink":"/notes/reinforcement_learning/","section":"notes","summary":"Table of Contents Topics Introduction Definition Markov Decision Processes RL vs. MDP Passive RL Resources Topics What is reinforcement learning? Examples Finite Markov Decision Processes Passive vs. Active Methods Adaptive Dynamic Programming Monte Carlo Methods Temporal-Different Learning Q-Learning Function Approximation Deep Q-Learning Policy and Value Iteration Case Studies Introduction When placed in a new environment, we orient and learn about it by interacting with it. When given a novel task, we may not be able to explicitly describe the task or even perform well at it, but we can learn a lot about it through trial and error.","tags":null,"title":"Reinforcement Learning","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Generalization Bias Variance Bias-Variance Tradeoff Generalization When fitting machine learning models to data, we want them to generalize well to the distribution that we have sampled from. We can measure a model\u0026rsquo;s ability to generalize by evaluating it on previously unseen data that is sampled from the same distribution as the training set. However, we often do not know the true underlying distribution. So we must fit the models to empirical distributions derived from observed data.\nMeasuring bias and variance is crucial for determining the quality of a model. Bias refers to the difference between the average prediction of a model and the correct value we are trying to predict. A model with high bias oversimplifies the problem and leads to high error on both training and test data. Variance refers to the sensitivity of a model to fluctuations in the training set. High variance suggests that the model\u0026rsquo;s performance changes significantly when it is fit on different samplings of the training data, which can lead to overfitting.\nTo achieve good generalization, it is essential to find a balance between bias and variance, minimizing the total error. This can be done by selecting appropriate model complexity and using regularization techniques to prevent overfitting or underfitting. Additionally, model validation techniques, such as hold-out validation and cross-validation, can be employed to assess a model\u0026rsquo;s ability to generalize to unseen data.\nBias Consider fitting a simple linear model to nonlinear data. The model will not be able to generalize well, regardless of the size of the training set. In fact, it would also exhibit poor performance when evaluated on the training set as well. When a model has not learned the patterns in the training data and is likewise unable to generalize to new data, it is known as underfitting. In this case, such a model has high bias.\nFigure 1: Regardless of the dataset sampled, a linear model exhibits high bias. Variance Variance is described in terms of the model fitting procedure and the training data. In terms of data, variance measures dispersion. It could also be interpreted as a measure of diversity. Sets with low variance contain samples that are close to the mean, and sampling from such a set would produce rather consistent data points.\nIn terms of model fitting, a model that fits the training data well but not the test data describes overfitting. This is because the training data is only an empirical sample of the true underlying distribution. A different sampling of the distribution may yield a set that more closely resembles the test set. Due to the variance of the underlying distribution, our model overfits the patterns that exist in the training set.\nFigure 2: A 5th degree polynomial trained on 3 different samplings of the distribution. Bias-Variance Tradeoff If a model is not complex enough to capture the underlying distribution, it will perform poorly on both the training and test sets. Indeed, the model has low bias. If the model is too complex, it will exhibit low bias and high variance, overfitting the training set while failing to generalize well to unseen data. The solution then is to find a tradeoff between bias and variance with respect to the model complexity.\n","date":1688446800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688446800,"objectID":"957c85788398153f94cf803baeab4aad","permalink":"https://ajdillhoff.github.io/notes/bias_and_variance/","publishdate":"2023-07-04T00:00:00-05:00","relpermalink":"/notes/bias_and_variance/","section":"notes","summary":"Table of Contents Generalization Bias Variance Bias-Variance Tradeoff Generalization When fitting machine learning models to data, we want them to generalize well to the distribution that we have sampled from. We can measure a model\u0026rsquo;s ability to generalize by evaluating it on previously unseen data that is sampled from the same distribution as the training set. However, we often do not know the true underlying distribution. So we must fit the models to empirical distributions derived from observed data.","tags":["machine learning"],"title":"Bias and Variance","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Text Preprocessing Tasks Models Perplexity Introduction Text Preprocessing Character-level tokenization Word-level tokenization Subword tokenization Stopwords Batching Padding Unsupervised Pre-Training Autoregression BERT loss Tasks Text Classification Named Entity Recognition Question Answering Summarization Translation Text Generation Text Preprocessing Text preprocessing is an essential step in NLP that involves cleaning and transforming unstructured text data to prepare it for analysis. Some common text preprocessing techniques include:\nExpanding contractions (e.g., \u0026ldquo;don\u0026rsquo;t\u0026rdquo; to \u0026ldquo;do not\u0026rdquo;) [7] Lowercasing text[7] Removing punctuations[7] Removing words and digits containing digits[7] Removing stopwords (common words that do not carry much meaning) [7] Rephrasing text[7] Stemming and Lemmatization (reducing words to their root forms) [7] Common Tokenizers Tokenization is the process of breaking a stream of textual data into words, terms, sentences, symbols, or other meaningful elements called tokens. Some common tokenizers used in NLP include:\nWhitespace Tokenizer: Splits text based on whitespace characters (e.g., spaces, tabs, and newlines) [2]. NLTK Tokenizer: A popular Python library that provides various tokenization functions, including word and sentence tokenization[1]. SpaCy Tokenizer: Another popular Python library for NLP that offers a fast and efficient tokenizer, which can handle large documents and is customizable[5]. WordPiece Tokenizer: A subword tokenizer used in models like BERT, which breaks text into smaller subword units to handle out-of-vocabulary words more effectively[3]. Byte Pair Encoding (BPE) Tokenizer: A subword tokenizer that iteratively merges the most frequent character pairs in the text, resulting in a vocabulary of subword units[12]. SentencePiece Tokenizer: A library that provides both BPE and unigram-based subword tokenization, which can handle multiple languages and does not rely on whitespace for tokenization[6]. These tokenizers differ in the way they split text into tokens and handle language-specific considerations, such as handling out-of-vocabulary words, dealing with punctuation, and managing whitespace characters. The choice of tokenizer depends on the specific NLP task and the characteristics of the text data being processed.\nCitations: [1] https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/ [2] https://neptune.ai/blog/tokenization-in-nlp [3] https://towardsdatascience.com/comparing-transformer-tokenizers-686307856955 [4] https://www.analyticsvidhya.com/blog/2021/09/essential-text-pre-processing-techniques-for-nlp/ [5] https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/ [6] https://www.reddit.com/r/MachineLearning/comments/rprmq3/d_sentencepiece_wordpiece_bpe_which_tokenizer_is/ [7] https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/ [8] https://towardsdatascience.com/top-5-word-tokenizers-that-every-nlp-data-scientist-should-know-45cc31f8e8b9 [9] https://www.projectpro.io/recipes/explain-difference-between-word-tokenizer [10] https://www.telusinternational.com/insights/ai-data/article/what-is-text-mining [11] https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4 [12] https://towardsdatascience.com/a-comprehensive-guide-to-subword-tokenisers-4bbd3bad9a7c [13] https://towardsdatascience.com/text-preprocessing-in-natural-language-processing-using-python-6113ff5decd8 [14] https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/ [15] https://docs.tamr.com/new/docs/tokenizers-and-similarity-functions [16] https://pitt.libguides.com/textmining/preprocessing [17] https://medium.com/@ajay_khanna/tokenization-techniques-in-natural-language-processing-67bb22088c75 [18] https://datascience.stackexchange.com/questions/75304/bpe-vs-wordpiece-tokenization-when-to-use-which [19] https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing [20] https://www.tokenex.com/blog/ab-what-is-nlp-natural-language-processing-tokenization/ [21] https://hungsblog.de/en/technology/learnings/difference-between-the-tokenizer-and-the-pretrainedtokenizer-class/ [22] https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html [23] https://medium.com/nlplanet/two-minutes-nlp-a-taxonomy-of-tokenization-methods-60e330aacad3 [24] https://www.geeksforgeeks.org/text-preprocessing-in-python-set-1/ [25] https://medium.com/@utkarsh.kant/tokenization-a-complete-guide-3f2dd56c0682 [26] https://stackoverflow.com/questions/380455/looking-for-a-clear-definition-of-what-a-tokenizer-parser-and-lexers-are [27] https://blog.floydhub.com/tokenization-nlp/ [28] https://medium.com/product-ai/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908 [29] https://pub.towardsai.net/in-depth-tokenization-methods-of-14-nlp-libraries-with-python-example-297ecdd14c1 [30] https://datascience.stackexchange.com/questions/88680/what-is-the-difference-between-countvectorizer-and-tokenizer-or-are-they-the [31] https://www.freecodecamp.org/news/train-algorithms-from-scratch-with-hugging-face/ [32] https://exchange.scale.com/public/blogs/preprocessing-techniques-in-nlp-a-guide [33] https://huggingface.co/docs/transformers/tokenizer_summary [34] https://blog.octanove.org/guide-to-subword-tokenization/ [35] https://www.enjoyalgorithms.com/blog/text-data-pre-processing-techniques-in-ml/ [36] https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/\nTasks Text Classification Commonly seen in the form of sentiment analysis, where the objective is to classify whether some input text is positive or negative. Document classification, in which documents are identified by their content, is also useful.\nNamed Entity Recognition Extract important nouns from a body of text.\nQuestion Answering Summarization Translation Text Generation Generate text from a prompt. This could be in the form of a simple question or some initial dialog. This is also seen in tools like GitHub Co-Pilot to generate code based on contextual code in the same project.\nModels Discuss GPT2\nPerplexity A measure of confidence of a language model. A naive model may predict a word by randomly selecting any of the \\(N\\) words in its vocabulary. As the model is optimized and the distribution of possible sequences is acquired, the perplexity decreases.\n","date":1682226000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682226000,"objectID":"c130cc8d69fc0079b385a484f01820d6","permalink":"https://ajdillhoff.github.io/notes/natural_language_processing/","publishdate":"2023-04-23T00:00:00-05:00","relpermalink":"/notes/natural_language_processing/","section":"notes","summary":"Table of Contents Introduction Text Preprocessing Tasks Models Perplexity Introduction Text Preprocessing Character-level tokenization Word-level tokenization Subword tokenization Stopwords Batching Padding Unsupervised Pre-Training Autoregression BERT loss Tasks Text Classification Named Entity Recognition Question Answering Summarization Translation Text Generation Text Preprocessing Text preprocessing is an essential step in NLP that involves cleaning and transforming unstructured text data to prepare it for analysis. Some common text preprocessing techniques include:\nExpanding contractions (e.","tags":["deep learning","NLP"],"title":"Natural Language Processing","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Definition Attention Key-value Store Scaled Dot Product Attention Multi-Head Attention Encoder-Decoder Architecture Encoder Decoder Usage Resources Introduction The story of Transformers begins with \u0026ldquo;Attention Is All You Need\u0026rdquo; (Vaswani et al., n.d.). In this seminal work, the authors describe the current landscape of sequential models, their shortcomings, and the novel ideas that result in their successful application.\nTheir first point highlights a fundamental flaw in how Recurrent Neural Networks process sequential data: their output is a function of the previous time step. Given the hindsight of 2022, where large language models are crossing the trillion parameter milestone, a model requiring recurrent computation dependent on previous time steps without the possibility of parallelization would be virtually intractable.\nThe second observation refers to attention mechanisms, a useful addition to sequential models that enable long-range dependencies focused on specific contextual information. When added to translation models, attention allows the model to focus on particular words (Bahdanau, Cho, and Bengio 2016).\nThe Transformer architecture considers the entire sequence using only attention mechanisms. There are no recurrence computations in the model, allowing for higher efficiency through parallelization.\nDefinition The original architecture consists of an encoder and decoder, each containing one or more attention mechanisms. Not every type of model uses both encoders and decoders. This is discussed later [TODO: discuss model types]. Before diving into the architecture itself, it is important to understand what an attention mechanism is and how it functions.\nAttention Attention mechanisms produce relationships between sequences. When we look at an image of a dog running in a field with the intent of figuring out what the dog is doing in the picture, we pay greater attention to the dog and look at contextual cues in the image that might inform us of their task. This is an automatic process which allows us to efficiently process information.\nAttention mechanisms follow the same concept. Consider a machine translation task in which a sentence in English is translated to French. Certain words between the input and output will have stronger correlations than others.\nSoft Attention Use of context vector that is dependent on a sequence of annotations. These contain information about the input sequence with a focus on the parts surrounding the $i$-th word.\n\\[ c_i = \\sum_{j=1}^{T_x}\\alpha_{ij}h_j \\]\nWhat is \\(\\alpha_{ij}\\) and how is it computed? This comes from an alignment model which assigns a score reflecting how well the inputs around position \\(j\\) and output at position \\(i\\) match, given by\n\\[ e_{ij} = a(s_{i-1}, h_j), \\]\nwhere \\(a\\) is a feed-forward neural network and \\(h_j\\) is an annotation produced by the hidden layer of a BRNN. These scores are passed to the softmax function so that \\(\\alpha_{ij}\\) represents the weight of annotation \\(h_j\\):\n\\[ \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp (e_{ik})}. \\]\nThis weight reflects how important \\(h_j\\) is at deciding the next state \\(s_i\\) and generating \\(y_i\\).\nSoft vs. Hard Attention This mechanism was also described in the context of visual attention as \u0026ldquo;soft\u0026rdquo; attention (Xu et al. 2016). The authors also describe an alternative version they call \u0026ldquo;hard\u0026rdquo; attention. Instead of providing a probability of where the model should look, hard attention provides a single location that is sampled from a multinoulli distribution parameterized by \\(\\alpha_i\\).\n\\[ p(s_{t,i} = 1 | s_{j\u0026lt;t}, \\mathbf{a}) = \\alpha_{t,i} \\]\nHere, \\(s_{t,i}\\) represents the location \\(i\\) at time \\(t\\), \\(s_{j\u0026lt;t}\\) are the location variables prior to \\(t\\), and \\(\\mathbf{a}\\) is an image feature vector.\nFigure 1: Hard attention for \u0026ldquo;A man and a woman playing frisbee in a field.\u0026rdquo; (Xu et al.) Figure 2: Soft attention for \u0026ldquo;A woman is throwing a frisbee in a park.\u0026rdquo; (Xu et al.) The two figures above show the difference between soft and hard attention. Hard attention, while faster at inference time, is non-differentiable and requires more complex methods to train (TODO: cite Luong).\nSelf-Attention Self attention is particularly useful for determining the relationship between different parts of an input sequence. The figure below demonstrates self-attention given an input sentence (Cheng, Dong, and Lapata 2016).\nFigure 3: Line thickness indicates stronger self-attention (Cheng et al.). How aligned the two vectors are. Cross Attention TODO\nKey-value Store Query, key, and value come from the same input (self-attention).\nCheck query against all possible keys in the dictionary. They have the same size. The value is the result stored there, not necessarily the same size. Each item in the sequence will generate a query, key, and value.\nThe attention vector is a function of they keys and the query.\nHidden representation is a function of the values and the attention vector.\nThe Transformer paper talks about queries, keys, and values. This idea comes from retrieval systems. If you are searching for something (a video, book, song, etc.), you present a system your query. That system will compare your query against the keys in its database. If there is a key that matches your query, the value is returned.\n\\[ att(q, \\mathbf{k}, \\mathbf{v}) = \\sum_i v_i f(q, k_i), \\] where \\(f\\) is a similarity function.\nThis is an interesting and convenient representation of attention. To implement this idea, we need some measure of similarity. Why not orthogonality? Two vectors that are orthogonal produce a scalar value of 0. The maximum value two vectors will produce as a result of the dot product occurs when the two vectors have the exact same direction. This is convenient because the dot product is simple and efficient and we are already performing these calculations in our deep networks in the form of matrix multiplication.\nScaled Dot Product Attention Figure 4: Scaled dot-product attention ((Vaswani et al., n.d.)) Each query vector is multiplied with each key using the dot product. This is implemented more efficiently via matrix multiplication. A few other things are added here to control the output. The first is scaling.\nMulti-Head Attention A single attention head can transform the input into a single representation. Is this analagous to using a single convolutional filter? The benefit of having multiple filters is to create multiple possible representations from the same input.\nEncoder-Decoder Architecture The original architecture of a transformer was defined in the context of sequence transduction tasks, where both the input and output are sequences. The most common task of this type is machine translation.\nEncoder The encoder layer takes an input sequence \\(\\{\\mathbf{x}_t\\}_{t=0}^T\\) and transforms it into another sequence \\(\\{\\mathbf{z}_t\\}_{t=0}^T\\).\nWhat is \\(\\mathbf{z}_t\\)?\nHow is it used? Input as key and value into second multi-head attention layer of the decoder.\nCould you create an encoder only model? Yes. Suitable for classification tasks \u0026ndash; classify the representation produced by the encoder. How does this representation relate to understanding?\nIt\u0026rsquo;s a transformation to another representation.\nGenerated representation also considers the context of other parts of the same sequence (bi-directional).\nDecoder Generates an output sequence.\nDecoder-only models? Suitable for text generation.\nWhat does the input represent?\nWhat does the output represent?\nWhat if we don\u0026rsquo;t use an encoder, what information is added in lieu of the encoder output?\n\u0026nbsp;Model\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Examples\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;Tasks\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;Encoder\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;ALBERT,\u0026nbsp;BERT,\u0026nbsp;DistilBERT,\nELECTRA,\u0026nbsp;RoBERTa\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;Sentence\u0026nbsp;classification,\u0026nbsp;named\u0026nbsp;entity\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\nrecognition,\u0026nbsp;extractive\u0026nbsp;question\u0026nbsp;answering\u0026nbsp; \u0026nbsp;Decoder\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;CTRL,\u0026nbsp;GPT,\u0026nbsp;GPT-2,\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\nTransformer\u0026nbsp;XL\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;Text\u0026nbsp;generation\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;Encoder-decoder\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;BART,\u0026nbsp;T5,\u0026nbsp;Marian,\u0026nbsp;mBART\u0026nbsp;\u0026nbsp;\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp;Summarization,\u0026nbsp;translation,\u0026nbsp;generative\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\nquestion\u0026nbsp;answering\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Usage TODO\nResources https://twitter.com/labmlai/status/1543159412940242945?s=20\u0026t=EDu5FzDWl92EqnJlWvfAxA https://en.wikipedia.org/wiki/Transduction_(machine_learning) https://www.apronus.com/math/transformer-language-model-definition https://lilianweng.github.io/posts/2018-06-24-attention/ http://nlp.seas.harvard.edu/annotated-transformer/ ","date":1667710800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667710800,"objectID":"0a389e4db0e6fb95e6eb83d1b1358c6d","permalink":"https://ajdillhoff.github.io/notes/transformers/","publishdate":"2022-11-06T00:00:00-05:00","relpermalink":"/notes/transformers/","section":"notes","summary":"Table of Contents Introduction Definition Attention Key-value Store Scaled Dot Product Attention Multi-Head Attention Encoder-Decoder Architecture Encoder Decoder Usage Resources Introduction The story of Transformers begins with \u0026ldquo;Attention Is All You Need\u0026rdquo; (Vaswani et al., n.d.). In this seminal work, the authors describe the current landscape of sequential models, their shortcomings, and the novel ideas that result in their successful application.\nTheir first point highlights a fundamental flaw in how Recurrent Neural Networks process sequential data: their output is a function of the previous time step.","tags":["deep learning","llms"],"title":"Transformers","type":"notes"},{"authors":null,"categories":null,"content":" Table of Contents Introduction Cloning a Repository Adding a new file Committing Changes Pushing your local changes to the remote repository Introduction This article walks through the steps needed to complete Assignment 0. For this course, we only need to use 5 commands. Although it is not required for this course, it is highly recommended that you learn the basics of git. The documentation page provided by git-scm is extremely helpful. It includes links to a free book on git as well as a cheat sheet with the most common git commands.\nCloning a Repository After accepting the assignment, you are provided with a link to your own private repository. To work with it on your local machine, you will first need to clone it. To clone it, you will need to authenticate that you are allowed to work with that repository. This is done by either SSH or HTTPS.\nIf you want to add and use an SSH key to authenticate, follow the instructions listed here. This article will walk through authentication via HTTPS. To clone your repository, you will need the HTTPS link. This is the same as the link to your actual repository. You can also view it by clicking on the green Code button and selecting HTTPS.\nFigure 1: Viewing the repository link. In a terminal window, run the command git clone [link], where [link] is the URL you copied for your respository. You should be prompted to enter your GitHub username and password. If you use your regular account password, you will see something similar to the output below\nFigure 2: Attempting to authenticate using your GitHub password. As stated in the error message, GitHub does not support using your password to authenticate via HTTPS. The link that is provided in the error message directs your to an article explaining this decision. The article links to another article on how to set up a personal access token (direct link). Create a personal access token as described in the direct link. This will be used in place of your regular password, so make sure you keep it somewhere safe.\nWhen creating the access token, be sure to at least select the repo scope. This will ensure that your access token is authorized to clone your repository.\nFigure 3: Selecting scopes. With the generated access token, you can successfully clone your repository via HTTPS. As a reminder, entering text into a password prompt in terminal will not show the characters you are typing. Do not worry! It is still reading the input.\nFigure 4: Successfully cloning the repository. Adding a new file Now that the repository is cloned, we can begin working with it locally. The assignment requires us to create a program which will print the lines of CSV to the terminal window. This requires opening data.csv, looping through its contents, and printing each line as our program reads it.\nStart by creating a new file named read_csv.c using your favorite code editor. It does not matter which editor you use. It only matters that the file you create is in the repository folder. This assignment is really about making sure you can use git properly, so a solution has been given below.\n#include \u0026lt;stdio.h\u0026gt; #define BUF_SIZE 128 int main() { char buffer[BUF_SIZE] = { 0 }; FILE *fp = fopen(\u0026#34;data.csv\u0026#34;, \u0026#34;r\u0026#34;); if (fp == NULL) return 1; while (fgets(buffer, BUF_SIZE, fp)) { printf(\u0026#34;%s\u0026#34;, buffer); } return 0; } Once we have finished editing the code, we can add it to our local repository. To check the current status of changes in our repo, use git status.\nFigure 5: Checking the status. We have created a file named read_csv.c, but it is not currently tracked by our repository. To track this file, we need to add it via git add read_csv.c. After adding the file, we can see that our local repo\u0026rsquo;s status has changed.\nFigure 6: Status after adding a file. Now that the file is being tracked, any modifications we make to it will be recorded. Git keeps snapshots of each state the file is in. We can always review a history of our file\u0026rsquo;s changes and see how the code has developed over the lifetime of a project. In that status output above, our repository detects changes to read_csv.c.\nCommitting Changes If we are happy with the changes, we can commit them to the repository using git commit. A commit should be accompanied by a message explaining what was changed. This will be very useful later on when you need to review what changes were made and why. Since this code fulfills the requirements of the assignment, let\u0026rsquo;s form our message that way. We can commit with a message with the following command.\ngit commit -m \u0026#34;Completed assignment.\u0026#34; If you have other tracked files with changes that needed to be committed, you can use the flag -a to add them with your commit.\nFigure 7: Committing the changes. Pushing your local changes to the remote repository When you clone a repository, you get a full copy of that repository including all of its data. If the server that is hosting your project crashes, you will still have a full copy of the repository on your local machine. To synchronize your changes with the local repository, you can push the local files with git push.\nFigure 8: Pushing local changes to the remote repo. If we view the website for our repository, it shows our changes.\nFigure 9: Repository website after pushing local changes. That is all that is needed for submitting your assignments. If the code is on your remote repository, than it will be considered as your submission.\n","date":1662181200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662181200,"objectID":"ceaf5b8b3a982a3798fd6d2d5ff2d578","permalink":"https://ajdillhoff.github.io/notes/submitting_assignments_using_github/","publishdate":"2022-09-03T00:00:00-05:00","relpermalink":"/notes/submitting_assignments_using_github/","section":"notes","summary":"Table of Contents Introduction Cloning a Repository Adding a new file Committing Changes Pushing your local changes to the remote repository Introduction This article walks through the steps needed to complete Assignment 0. For this course, we only need to use 5 commands. Although it is not required for this course, it is highly recommended that you learn the basics of git. The documentation page provided by git-scm is extremely helpful.","tags":null,"title":"Submitting Assignments using GitHub","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Box Constraints Updating the Lagrangians The Algorithm Implementation Introduction Paper link: https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/\nSequential Minimal Optimization (SMO) is an algorithm to solve the SVM Quadratic Programming (QP) problem efficiently. Developed by John Platt at Microsoft Research, SMO deals with the constraints of the SVM objective by breaking it down into a smaller optimization problem at each step.\nThe two key components of SMO are\nan analytic method to solving for two Lagrange multipliers at a time and a heuristic for choosing which multipliers to optimize. The original objective is to maximize the margin between the nearest positive and negative examples. For the linear case, if the output is given as\n\\[ u = \\mathbf{w}^T \\mathbf{x} - b, \\]\nwhere \\(\\mathbf{w}\\) is the normal vector to the hyperplane separating the classes, then the margin is given as\n\\[ m = \\frac{1}{\\|w\\|_2}. \\]\nMaximizing this margin yielded the primal optimization problem\n\\begin{align*} \\min_{\\mathbf{w},b} \\frac{1}{2} \\|\\mathbf{w}\\|^2\\\\ \\textrm{s.t.} \\quad \u0026amp; y_i(\\mathbf{w}^T \\mathbf{x} - b) \\geq 1, \\forall i\\\\ \\end{align*}\nThe dual form of the objective function for a Support Vector Machine is\n\\[ \\min_{\\vec\\alpha} \\Psi(\\vec{\\alpha}) = \\min_{\\vec{\\alpha}} \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)\\alpha_i\\alpha_j - \\sum_{i=1}^N \\alpha_i \\]\nwith inequality constraints\n\\[ \\alpha_i \\geq 0, \\forall i, \\]\nand a linear equality constraint\n\\[ \\sum_{i=1}^N y_i \\alpha_i = 0. \\]\nFor a linear SVM, the output is dependent on a weight vector \\(\\mathbf{w}\\) and threshold \\(b\\):\n\\[ \\mathbf{w} = \\sum_{i=1}^N y_i \\alpha_i \\mathbf{x}_i, \\quad b = \\mathbf{w}^T \\mathbf{x}_k - y_k. \\]\nThe threshold is also dependent on the weight vector? The weight vector \\(\\mathbf{w}\\) is computed using the training data. The threshold is only dependent on non-zero support vectors, \\(\\alpha_k \u0026gt; 0\\).\nOverlapping Distributions Slack variables were introduced to allow misclassifications at the cost of a linear penalty. This is useful for datasets that are not linearly separable. In practice, this is accomplished with a slight modification of the original objective function:\n\\begin{align*} \\min_{\\mathbf{w},b} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^N \\xi_i\\\\ \\textrm{s.t.} \\quad \u0026amp; y_i(\\mathbf{w}^T \\mathbf{x} - b) \\geq 1 - \\xi_i, \\forall i\\\\ \\end{align*}\nThe convenience of this formulation is that the parameters \\(\\xi_i\\) do not appear in the dual formulation at all. The only added constraint is\n\\[ 0 \\leq \\alpha_i \\leq C, \\forall i. \\]\nThis is referred to as the box constraint for reasons we shall see shortly.\nBox Constraints The smallest optimization step that SMO solves is that of two variables. Given the constraints above, the solution lies on a diagonal line \\(\\sum_{i=1}^N y_i \\alpha_i = 0\\) bounded within a box \\(0 \\leq \\alpha_i \\leq C, \\forall i\\).\nIsolating for two samples with alphas \\(\\alpha_1\\) and \\(\\alpha_2\\), the constraint \\(\\sum_{i=1}^n y_i \\alpha_i = 0\\) suggests that\n\\[ y_1 \\alpha_1 + y_2 \\alpha_2 = w. \\]\nWe first consider the case when \\(y_1 \\neq y_2\\). Let \\(y_1 = 1\\) and \\(y_2 = -1\\), then \\(a_1 - a_2 = w\\). As \\(\\alpha_1\\) increases, \\(\\alpha_2\\) must also increase to satisfy the constraint.\nFigure 1: Equality constraint for case 1 (from Platt\u0026rsquo;s SMO paper). The other case is when \\(y_1 = y_2\\), then \\(\\alpha_1 + \\alpha_2 = w\\). As \\(\\alpha_1\\) is increased, \\(\\alpha_2\\) is decreased to satisfy the constraint.\nFigure 2: Box constraint for samples of the same class (from Platt\u0026rsquo;s SMO paper). Updating the Lagrangians SMO solves for only two Lagrange multipliers at a time. Solving for only 1 at a time would be impossible under the constraint \\(\\sum_{i=1}^N y_i \\alpha_i = 0\\). The first step is to compute \\(\\alpha_2\\) and constrain it between the ends of the diagonal line segment from the box constraints.\nIf \\(y_1 \\neq y_2\\), then the following bounds are applied to \\(\\alpha_2\\):\n\\begin{equation*} L = \\max(0, \\alpha_2 - \\alpha_1), \\quad H = \\min(C, C + \\alpha_2 - \\alpha_1) \\end{equation*}\notherwise, the bounds are computed as:\n\\begin{equation*} L = \\max(0, \\alpha_2 + \\alpha_1 - C), \\quad H = \\min(C, \\alpha_2 + \\alpha_1) \\end{equation*}\nUpdating the actual parameter is done following the update rule of gradient descent:\n\\[ \\alpha_2^{\\text{new}} = \\alpha_2 + \\frac{y_2(E_1 - E_2)}{\\eta}. \\]\nHow do we arrive at this update rule?\nSecond Derivative of the Objective Function Here, \\(\\eta\\) represents the step size and direction. It is computed from the second derivative of the objective function along the diagonal line. To see that this is the case, consider the original objective function\n\\begin{align*} \\min_{\\mathbf{\\alpha}} \\quad \u0026amp; \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j) \\mathbf{\\alpha}_1 \\mathbf{\\alpha}_2 - \\sum_{i=1}^N \\alpha_i\\\\ \\textrm{s.t.} \\quad \u0026amp; 0 \\leq \\alpha_i \\leq C, \\forall i\\\\ \u0026amp; \\sum_{i=1}^N y_i \\alpha_i = 0\\\\ \\end{align*}\nSince we are optimizing with respect to only 2 Lagrangian multipliers at a time, we can write the Lagrangian function as\n\\[ \\frac{1}{2} y_1^2 K_{11} \\alpha_1^2 + \\frac{1}{2} y_2^2 K_{22} \\alpha_2^2 + y_1 \\alpha_1 \\sum_{j=3}^N y_j \\alpha_j K_{1j} + y_2 \\alpha_2 \\sum_{j=3}^N y_j \\alpha_j K_{2j} - \\alpha_1 - \\alpha_2 + \\sum_{j=3}^N \\alpha_j \\]\nWe are only optimizing with respect to \\(\\alpha_1\\) and \\(\\alpha_2\\), the next step is to extract those terms from the sum. This is simplified further by noting that \\(\\sum_{j=3}^N y_j \\alpha_j K_{ij}\\) looks very similar to the output of an SVM:\n\\[ u = \\sum_{j=1}^N y_j \\alpha_j K(\\mathbf{x}_j, \\mathbf{x}) - b. \\]\nThis allows us to introduce a variable \\(v_i\\) based on \\(u_i\\), the output of an SVM given sample \\(\\mathbf{x}_i\\):\n\\[ v_i = \\sum_{j=3}^N y_j \\alpha_j K_{ij} = u_i + b - y_1 \\alpha_1 K_{1i} - y_2 \\alpha_2 K_{2i}. \\]\nThe objective function is then written as\n\\[ \\frac{1}{2} y_1^2 K_{11} \\alpha_1^2 + \\frac{1}{2} y_2^2 K_{22} \\alpha_2^2 + y_1 \\alpha_1 v_1 + y_2 \\alpha_2 v_2 - \\alpha_1 - \\alpha_2 + \\sum_{j=3}^N \\alpha_j. \\]\nNote that the trailing sum \\(\\sum_{j=3}^N \\alpha_j\\) is treated as a constant since those values are not considered when optimizing for \\(\\alpha_1\\) and \\(\\alpha_2\\).\nGiven the box constraints from above, we must update \\(\\alpha_1\\) and \\(\\alpha_2\\) such that\n\\[ \\alpha_1 + s \\alpha_2 = \\alpha_1^* + s \\alpha_2^* = w. \\]\nThis linear relationship allows us to express the objective function in terms of α_2:\n\\[ \\Psi = \\frac{1}{2} y_1^2 K_{11} (w - s \\alpha_2)^2 + \\frac{1}{2} y_2^2 K_{22} \\alpha_2^2 + y_1 (w - s \\alpha_2) v_1 + y_2 \\alpha_2 v_2 - \\alpha_1 - \\alpha_2 + \\sum_{j=3}^N \\alpha_j. \\]\nThe extremum of the function is given by the first derivative with respect to \\(\\alpha_2\\):\n\\[ \\frac{d\\Psi}{d\\alpha_2} = -sK_{11}(w - s\\alpha_2) + K_{22}\\alpha_2 - K_{12}\\alpha_2 + s K_{12} (w - s \\alpha_2) - y_2 v_2 + s + y_2 v_2 - 1 = 0. \\]\nIn most cases, the second derivative will be positive. The minimum of \\(\\alpha_2\\) is where\n\\begin{align*} \\alpha_2 (K_{11} + K_{22} - 2 K_{12}) \u0026amp;= s(K_{11} - K_{12})w + y_2(v_1 - v_2) + 1 - s\\\\ \u0026amp;= s(K_{11} - K_{12})(s\\alpha_2^*+\\alpha_1^*)\\\\ \u0026amp;+ y_2(u_1-u_2+y_1\\alpha_1^*(K_{12} - K_{11}) + y_2 \\alpha_2^* (K_{22} - K_{21})) + y_2^2 - s\\\\ \u0026amp;= \\alpha_2^*(K_{11}+K_{22} - 2K_{12}) + y_2(u_1 - u_2 + y_2 - y_1). \\end{align*}\nIf we let \\(E_1 = u_1 - y_1\\), \\(E_2 = u_2 - y_2\\), and \\(\\eta = K_{11} + K_{22} - 2K_{12}\\), then\n\\[ \\alpha_2^{\\text{new}} = \\alpha_2 + \\frac{y_2(E_1 - E_2)}{\\eta}. \\]\nThe Algorithm Sequential Minimal Optimization (SMO) solves the SVM problem which usually requires a Quadratic Programming (QP) solution. It does this by breaking down the larger optimization problem into a small and simple form: solving for two Lagrangians. Solving for one would not be possible without violating KKT conditions. There are two components to Sequential Minimal Optimization: the first is how the Lagrangians are selected and the second is the actual optimization step.\nChoosing the First Lagrangian The algorithm first determines which samples in the dataset violate the given KKT conditions. Only those violating the conditions are eligible for optimization. Additionally, samples that are not on the bounds are selected (those with \\(\\alpha_i \\neq 0\\) and \\(\\alpha_i \\neq C\\)). This continues through the dataset until no sample violates the KKT constraints within \\(\\epsilon\\).\nAs a last step, SMO searches the entire dataset to look for any bound samples that violate KKT conditions.\nChoosing the Second Lagrangian The second Lagrangian is chosen to maximize the size of the step taken during joint optimization. Noting that the step size is based on\n\\[ \\alpha_2^{\\text{new}} = \\alpha_2 + \\frac{y_2(E_1 - E_2)}{\\eta}, \\]\nit is approximated by computing \\(|E_1 - E_2|\\).\nIf positive progress cannot be made given the choice of Lagrangian, SMO will begin iterating through non-bound examples. If no eligible candidates are found in the non-bound samples, the entire dataset is searched.\nUpdating the Parameters With the second derivative of the objective function, we can take an optimization step along the diagonal line. To ensure that this step adheres to the box constraints defined above, the new value of \\(\\alpha_2\\) is clipped:\n\\begin{equation*} \\alpha_2^{\\text{new,clipped}} = \\begin{cases} H \u0026amp;\\text{if} \\quad \\alpha_2^{\\text{new}} \\geq H;\\\\ \\alpha_2^{\\text{new}} \u0026amp;\\text{if} \\quad L \u0026lt; \\alpha_2^{\\text{new}} \u0026lt; H;\\\\ L \u0026amp;\\text{if} \\quad \\alpha_2^{\\text{new}} \\geq L.\\\\ \\end{cases} \\end{equation*}\nWith the new value of \\(\\alpha_2\\), \\(\\alpha_1\\) is computed such that the original KKT condition is preserved:\n\\[ \\alpha_1^{\\text{new}} = \\alpha_1 + s(\\alpha_2 - \\alpha_2^{\\text{new,clipped}}), \\]\nwhere \\(s = y_1y_2\\).\nPoints that are beyond the margin are given an alpha of 0: \\(\\alpha_i = 0\\). Points that are on the margin satisfy \\(0 \u0026lt; \\alpha_i \u0026lt; C\\). These are the support vectors. Points inside the margin satisfy \\(\\alpha_i = C\\).\nLinear SVMs In the case of linear SVMs, the parameters can be stored as a single weight vector\n\\[ \\mathbf{w}^{\\text{new}} = \\mathbf{w} + y_1 (\\alpha_1^{\\text{new}} - \\alpha_1)\\mathbf{x}_1 + y_2(\\alpha_2^{\\text{new,clipped}} - \\alpha_2)\\mathbf{x}_2. \\]\nThe output of a linear SVM is computed as\n\\[ u = \\mathbf{w}^T \\mathbf{x} - b. \\]\nNonlinear SVMs In the nonlinear case, the output of the model is computed as\n\\[ u = \\sum_{i=1}^N y_i \\alpha_i K(\\mathbf{x}_i, \\mathbf{x}) - b. \\]\nImplementation An implementation of SMO in Python is available at https://github.com/ajdillhoff/CSE6363/blob/main/svm/smo.ipynb\n","date":1656910800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656910800,"objectID":"d82d40256130c8b03c7cfd6f8e1a5a3b","permalink":"https://ajdillhoff.github.io/notes/sequential_minimal_optimization/","publishdate":"2022-07-04T00:00:00-05:00","relpermalink":"/notes/sequential_minimal_optimization/","section":"notes","summary":"Table of Contents Introduction Box Constraints Updating the Lagrangians The Algorithm Implementation Introduction Paper link: https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/\nSequential Minimal Optimization (SMO) is an algorithm to solve the SVM Quadratic Programming (QP) problem efficiently. Developed by John Platt at Microsoft Research, SMO deals with the constraints of the SVM objective by breaking it down into a smaller optimization problem at each step.\nThe two key components of SMO are\nan analytic method to solving for two Lagrange multipliers at a time and a heuristic for choosing which multipliers to optimize.","tags":["machine learning"],"title":"Sequential Minimal Optimization","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Binary Classification Plotting a Decision Boundary Multiple Classes Sensitivity to Outliers Introduction The notebook for this lesson can be found here.\nWith linear regression, we were able to fit a model to our data in order to make inferences on unseen data points. In the examples, both the input features and observation were continuous. With discriminant functions, we will use similar models to classify the data points based on their input features. We start out with the simplest approach: we assume that the data is linearly separable and can be assigned one of \\(K\\) discrete classes.\nFor classification with linear discriminant functions, we will use a \\(K\\) dimensional vector that has a 1 corresponding to the class encoding for that input and a 0 for all other positions. For example, if our possible target classes were \\(\\{\\text{car, truck, person}\\}\\), then a target vector for \\(\\text{person}\\) would be \\(\\mathbf{y} = [0, 0, 1]^T\\).\nThis article will stick to a discriminative approach to classification. That is, we define a discriminant function which assigns each data input \\(\\mathbf{x}\\) to a class. For a probabilistic perspective, see Linear Discriminant Analysis.\nWe will again start with a linear model \\(y = f(\\mathbf{x}; \\mathbf{w})\\). Unlike the model used with linear regression, ours will need to predict a discrete class label. In other words, we need to predict a vector with a 1 corresponding to the class encoding.\nBinary Classification Consider a simple dataset with 2 features per data sample. Our goal is to classify the data as being one of two possible classes. This only requires a single function which classifies the sample as being in class 0 if \\(f(\\mathbf{x};\\mathbf{w}) \\geq 0\\) and class 1 otherwise.\nFigure 1: Two groups of data that are very clearly linearly separable. The model output is such that \\(f(\\mathbf{x};\\mathbf{w}) = [1, 0]\\) when \\(\\mathbf{x}\\) is predicted as class 1. If \\(f(\\mathbf{x};\\mathbf{w}) = [0, 1]\\) then \\(\\mathbf{x}\\) is assigned to class 2. In practice, the actual output will not be a one-hot vector. There will be some values in all positions of the vector.\nFor example, a model trained on a binary classification task outputs the following vector given a randomly selected input sample:\n\\[ [0.1224, 0.8776] \\]\nA class would be assigned by taking the argmax of this output vector. That is, the model predicts that this sample belongs to class 1.\nMeasuring Classifier Performance L1 loss can be used to measure classifier performance for linear discriminant function models.\n\\[ E = \\sum_{i=1}^N \\sum_{j=1}^M |\\hat{y}_{ij} - y_{ij}| \\]\nPlotting a Decision Boundary In the case of binary classification, a sample is predicted as class 1 if the output vector has the highest value at index 0. Otherwise, it is classified as class 2. If we were to plot the decision regions, we would see that the boundary is at the point when the output for both classes is equal.\nFigure 2: Binary classification with decision regions shown. Multiple Classes Extending this to multiple classes is as easy as encoding the classes in a one-hot vector whose length equals the number of classes. The parameters of the model can be obtained using gradient descent, the normal equations, or any other method that optimizes the least squares criterion.\nThe figure below shows an example of a linear discriminant function model fit to a dataset with 3 classes.\nFigure 3: Multiclass classification using linear discriminant functions. Sensitivity to Outliers One major flaw with least squares models is their sensitivity to outliers in the data. Consider the dataset shown below.\nFigure 4: Linearly separable dataset This dataset is clearly linearly separable. This will be no problem for our linear classifier, as seen below.\nFigure 5: Linear classifier fit to data using least squares. This dataset has a convenient property that the samples from each class are tightly clustered. What happens if our data is slightly more diverse?\nFigure 6: 2-class dataset in which one class is not as tightly clustered as the other. In the dataset above, we can still clearly see that it should be linearly separable. Unfortunately, our least squares model will be very sensitive to the 20 points at the top left of the plot. Training a linear discriminant function using least squares results in the following decision boundary.\nFigure 7: The model misclassifies samples that should be linearly separable. If we determine that a linear classifier is adequate for a given dataset, we may wish to use a slightly more robust model such as Logistic Regression instead of linear discriminant functions.\n","date":1654578000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654578000,"objectID":"4eeb3b49e53888a547d60e92fc258530","permalink":"https://ajdillhoff.github.io/notes/discriminant_functions/","publishdate":"2022-06-07T00:00:00-05:00","relpermalink":"/notes/discriminant_functions/","section":"notes","summary":"Table of Contents Introduction Binary Classification Plotting a Decision Boundary Multiple Classes Sensitivity to Outliers Introduction The notebook for this lesson can be found here.\nWith linear regression, we were able to fit a model to our data in order to make inferences on unseen data points. In the examples, both the input features and observation were continuous. With discriminant functions, we will use similar models to classify the data points based on their input features.","tags":null,"title":"Discriminant Functions","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":"The recurrent nature of RNNs means that gradients get smaller and smaller as the timesteps increase. This is known as the vanishing gradient problem. One of the first popular solutions to this problem is called Long Short-Term Memory, a recurrent network architecture by Hochreiter and Schmidhuber.\nAn LSTM is made up of memory blocks as opposed to simple hidden units. Each block is differentiable and contains a memory cell along with 3 gates: the input, output, and forget gates. These components allow the blocks to maintain some history of information over longer range dependencies.\nFigure 1: LSTM memory block with a single cell (adapted from Andrew Ng\u0026rsquo;s diagram). The original LSTM only had an input and output gate. Forget gates were added in 2000 by Gers et al. to control the amount of context that could be reset, if the task called for it. Peephole connections were proposed by Gers et al. in 2002. These are weights that combine the previous cell state to the gates in order to learn tasks that require precise timing.\nBy controlling when information can either enter or leave the memory cell, LSTM blocks are able to maintain more historical context than RNNs.\nForward Pass The equations listed here follow notation and description from Alex Graves\u0026rsquo; thesis.\nThe weight from unit \\(i\\) to \\(j\\) is given as \\(w_{ij}\\). The input to unit \\(j\\) at time \\(t\\) is \\(a_j^t\\) and the result of its activation function is \\(b_j^t\\). Let \\(\\psi\\), \\(\\phi\\), and \\(\\omega\\) be the input, forget, and output gates. A memory cell is denoted by \\(c \\in C\\), where \\(C\\) is the set of cells in the network. The activation, or state, of a given cell \\(c\\) at time \\(t\\) is \\(s_c^t\\). The output of each gate passes through an activation function \\(f\\), while the input and output activation functions of a memory block are given by \\(g\\) and \\(h\\).\nThe forward pass for the input gates is\n\\[ a_{\\psi}^t = \\sum_{i=1}^I w_{i\\psi}x_i^t + \\sum_{h=1}^H w_{h\\psi}b_h^{t-1} + \\sum_{c=1}^C w_{c\\psi}s_c^{t-1}. \\]\nThe output of the forget gates is\n\\[ a_{\\phi}^t = \\sum_{i=1}^I w_{i\\phi}x_i^t + \\sum_{h=1}^H w_{h\\phi}b_h^{t-1} + \\sum_{c=1}^C w_{c\\phi}s_c^{t-1}. \\]\nThe output of the output gates is\n\\[ a_{\\omega}^t = \\sum_{i=1}^I w_{i\\omega}x_i^t + \\sum_{h=1}^H w_{h\\omega}b_h^{t-1} + \\sum_{c=1}^C w_{c\\omega}s_c^{t-1}. \\]\nEach of the outputs above is passed through an activation function \\(f\\).\nThe output of each cell is computed as\n\\[ a_c^t = \\sum_{i=1}^I w_{ic}x_i^t + \\sum_{i=1}^H w_{hc}b_h^{t-1} \\]\nand the internal state is updated via\n\\[ s_c^t = b_{\\phi}^t s_c^{t-1} + b_{\\psi}^t g(a_c^t). \\]\nThe state update considers the state at the previous timestep multiplied by the output of the forget gate. That is, it controls how much of the current memory to keep.\nThe final cell output is given as\n\\[ b_c^t = b_{\\omega}^t h(s_c^t). \\]\n","date":1649739600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649739600,"objectID":"4db6081bc11780061ee225406efb2939","permalink":"https://ajdillhoff.github.io/notes/long_short_term_memory/","publishdate":"2022-04-12T00:00:00-05:00","relpermalink":"/notes/long_short_term_memory/","section":"notes","summary":"The recurrent nature of RNNs means that gradients get smaller and smaller as the timesteps increase. This is known as the vanishing gradient problem. One of the first popular solutions to this problem is called Long Short-Term Memory, a recurrent network architecture by Hochreiter and Schmidhuber.\nAn LSTM is made up of memory blocks as opposed to simple hidden units. Each block is differentiable and contains a memory cell along with 3 gates: the input, output, and forget gates.","tags":["deep learning"],"title":"Long Short-Term Memory","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Definition Bidirectional Recurrent Neural Networks References Introduction Neural networks are an effective tool for regression and classification tasks, but they do not consider the dependencies of information over time. Many tasks have implicit information that is dependent on input that may have already been processed or may not be seen until the future.\nRecurrent Neural Networks (RNN) consider the historical context of time-series data. Bi-directional Recurrent Neural Networks (BRNN) consider both historical and future context. This is necessary for tasks like language tanslation.\nParameter sharing across different parts of the model is key for sequence models. Different instances of a particular feature may appear at different time steps.\n\u0026ldquo;I see Naomi there.\u0026rdquo; and \u0026ldquo;Naomi is right there\u0026rdquo; both convey that Naomi is present, but we would not require the model to have separate parameters just because the word position is different between the two.\nRecurrent connections provide a memory of sorts. This enables important contextual information to be \u0026ldquo;remembered\u0026rdquo; throughout time. These models are not without their limitations. When trained with gradient descent, the gradient information passed throughout multiple time steps can become insignificant. There are several ways to address the vanishing gradient problem which are explored in alternative models such as Long Short-Term Memory and Transformers.\nDefinition The definition of RNNs start with that of Neural Networks. One layer of an RNN has some number of hidden units that transforms the input into an intermediate representation. In addition to transforming the input, another set of parameters is used to transform the hidden context over time. The difference is that the hidden layer is shared over time, as seen in the equation below.\n\\[ \\mathbf{h}^{(t)} = f(\\mathbf{h}^{(t-1)}, \\mathbf{x}^{(t)}; \\mathbf{\\theta}) \\]\nFigure 1: Computation graph of an RNN (By fdeloche - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=60109157) In the computation graph above, a recurrent network has three weight matrices associated with its forward pass. An input weight matrix \\(U \\in \\mathbb{R}^{H \\times D}\\) processes the features for each frame of the input sequence. The hidden layer has a weight matrix \\(V \\in \\mathbb{R}^{H \\times H}\\), where \\(H\\) is the number of hidden nodes. The output layer will have a weight matrix \\(W \\in \\mathbb{R}^{O \\times H}\\).\nForwards Pass To understand the computation graph of an RNN, consider an input of length \\(T\\) with \\(D\\) features. That is, each input sample is a sequence of features. This could be represented as encoded video data, text data, or any other sequence signals. To compute the output of a hidden layer \\(\\mathbf{h}\\) at time \\(t\\), take a linear combination of all input feature \\(x_i^t\\) at time \\(t\\) in addition to the output of the previous hidden layer and then add the linear combination of output activations for each node in the hidden layer:\n\\[ a_h^t = \\sum_{d=1}^D w_{dh} x_d^t + \\sum_{h\u0026rsquo;=1}^H w_{h\u0026rsquo;h} b_{h\u0026rsquo;}^{t-1}, \\]\nwhere \\(b_h^t = \\theta_h(a_h^t)\\) and we assume the bias term is concatenated with the weights.\nWeights in the hidden layer are crucial for RNNs to adapt to contextual features based on their occurrence relative to time. For example, a character-based language model based on a traditioinal network would produce similar output for consecutive letters that are the same. In an RNN, the hidden weights would produce a different output for each consecutive character even if it were the same.\nThe hidden layer outputs are used in both the subsequent computations through time as well as the output node for each instance \\(t\\). The inputs to the output node are computed from the hidden node at the same time as the output to the hidden activation:\n\\[ a_k^t = \\sum_{h=1}^H w_{hk}b_h^t. \\]\nBackwards Pass The gradients of a recurrent network are computed using backpropagation, similar to neural networks. Since the forward pass is over \\(t\\) time step, the backward pass must consider them as well. This variant of backpropagation for recurrent models is calling backpropagation through time (BPTT).\nLike a feed forward network, the output is dependent on the activation of the hidden layer. For a recurrent model, its dependence is through the output of the hidden layer as well as the pass to the next hidden time step.\n\\[ \\frac{\\partial \\mathcal{L}}{\\partial a_h^t} = \\frac{\\partial \\mathcal{L}}{\\partial b_h^t} \\frac{\\partial b_h^t}{\\partial a_h^t} \\]\n\\[ \\frac{\\partial \\mathcal{L}}{\\partial b_h^t} = \\sum_{k=1}^K \\frac{\\partial \\mathcal{L}}{\\partial a_k^t} \\frac{\\partial a_k^t}{\\partial b_h^t} + \\sum_{h\u0026rsquo;=1}^H \\frac{\\partial \\mathcal{L}}{\\partial a_{h\u0026rsquo;}^{t+1}} \\frac{\\partial a_{h\u0026rsquo;}^{t+1}}{\\partial a_{h}^t} \\]\nThe derivatives with respect to the weights are given as\n\\[ \\frac{\\partial \\mathcal{L}}{\\partial w_{ij}} = \\sum_{t=1}^T \\frac{\\partial \\mathcal{L}}{\\partial a_j^t} \\frac{\\partial a_j^t}{\\partial w_{ij}}. \\]\nBidirectional Recurrent Neural Networks Standard RNNs work for many problems with sequential input. Training such a model would consider the full input through time \\(T\\), but inference may only be able to consider the data up to time \\(t \u0026lt; T\\). There are sequential tasks which could leverage from both past and future context, such as language translation. For this case, BRNNs were proposed \u0026lt;\u0026amp;schusterBidirectionalRecurrentNeural1997\u0026gt;.\nFigure 2: Diagram of BRNN from Graves et al. References The Unreasonable Effectiveness of RNNs by Andrej Karpathy \u0026lt;\u0026amp;gravesSupervisedSequenceLabelling2012\u0026gt; Understanding LSTM Networks by Christopher Colah ","date":1649566800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649566800,"objectID":"7e13e6664e4126b5516049955a7d3ccb","permalink":"https://ajdillhoff.github.io/notes/recurrent_neural_networks/","publishdate":"2022-04-10T00:00:00-05:00","relpermalink":"/notes/recurrent_neural_networks/","section":"notes","summary":"Table of Contents Introduction Definition Bidirectional Recurrent Neural Networks References Introduction Neural networks are an effective tool for regression and classification tasks, but they do not consider the dependencies of information over time. Many tasks have implicit information that is dependent on input that may have already been processed or may not be seen until the future.\nRecurrent Neural Networks (RNN) consider the historical context of time-series data. Bi-directional Recurrent Neural Networks (BRNN) consider both historical and future context.","tags":["deep learning"],"title":"Recurrent Neural Networks","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Resources Introduction Gradient Descent and its Variants Adaptive Learning Rate Methods Parameter Initialization Resources Resources https://ruder.io/optimizing-gradient-descent/ https://www.deeplearningbook.org/contents/optimization.html Introduction empirical risk minimization - minimizing over an empirical distribution. Differs from risk minimization which is minimizing over the true distribution. We typically do not know the true distribution.\nComplex models are able to memorize the dataset.\nIn many applications for training, what we want to optimize is different from what we actually optimize since we need to have useful derivatives for gradient descent. For example, the 0-1 loss\n\\begin{equation*} L(i, j) = \\begin{cases} 0 \\qquad i = j \\\\ 1 \\qquad i \\ne j \\end{cases} \\qquad i,j \\in M \\end{equation*}\nis what we would really want to minimize for classification tasks. In practice we use something like Cross Entropy Loss. As pointed out by (Goodfellow et al.), there are sometimes advantages with using surrogate loss functions. A 0-1 loss may eventually fit the training set with 100% accuracy. At this point, no further optimization could take place as the error would be 0. With losses like negative log likelihood, optimization could continue which may result in increasing the margin between classes.\nLarger batch sizes provide a more accurate estimate of the gradient.\nRandomly selecting samples is crucial for learning. Datasets may be arranged in such a strong bias is present. Shuffling once isn\u0026rsquo;t enough because the data is biased after the first iteration. We could only get around this if we had the true distribution to generate new samples.\nIf our training set is extremely large, we may converge to a solution without ever having gone through all samples. Typically, models are able to train on multiple passes of the dataset to increase their generalization error. Each subsequent pass may increase the bias, but not enough to decrease generalization performance.\nThe gradient norm can be monitored while training to see if the issue is local minima or any other critical point \u0026lt;\u0026amp;zhaoPenalizingGradientNorm\u0026gt;. If the parameters were to get stuck at a critical point, the gradient norm should shrink over time.\nFigure 1: The gradient norm decreases as it settles into some minima (Zhao et al.). Gradient Descent and its Variants Original gradient descent update:\n\\[ \\theta = \\theta - \\eta \\nabla_{\\theta}J(\\theta) \\]\nHaving a constant value for \\(\\eta\\) means that the network will usually be unable to converge to a local minimum. As the parameters reach a minimum, the constant learning update means that it will jump around the true minimum point. This is usually remedied in part by setting up a decreasing learning rate schedule. This necessarily requires more manual guess work as to what the best annealing schedule would be.\nMomentum When the loss surface is more steep in one dimension than others, SGD will move back and forth in the directions of greatest descent while only slowly moving in the direction with a smaller decline. The figure below gives an example.\nFigure 2: SGD moves slower towards covergence for non-uniform surfaces. If the gradient had some momentum which built up over time, it would take fewer iterations to converge.\nFigure 3: SGD with momentum converges in fewer iterations. In practice, this can be implemented by adding some fraction of the previous update to the current step:\n\\begin{align*} \\mathbf{g}_t \u0026amp;= \\alpha \\mathbf{g}_{t-1} + \\eta \\nabla_{\\theta}J(\\theta)\\\\ \\theta \u0026amp;= \\theta - \\mathbf{g}_t \\end{align*}\nNesterov Momentum If we allow the momentum to keep increasing, the steps become greater and greater. This could lead to the parameters \u0026ldquo;rolling\u0026rdquo; out of the minimum up a steep incline. If our algorithm knew that it was coming up to an incline, it would be smarter to slow down. This is essentially what Nesterov momentum does.\nFigure 4: Nesterov momentum computes the gradient after applying momentum. \\begin{align*} \\mathbf{g}_t \u0026amp;= \\alpha \\mathbf{g}_{t-1} + \\eta \\nabla_{\\theta}J(\\theta - \\alpha \\mathbf{g}_{t-1})\\\\ \\theta \u0026amp;= \\theta - \\mathbf{g}_t \\end{align*}\nAdaptive Learning Rate Methods The rate at which a model converges to some solution is dependent on many factors. One that we can control is the learning rate. If the learning rate is too large, the model may never converge because it jumps too far in each iteration. If the learning rate is too small, it may take much longer to converge to any solution.\nIt would be ideal if the optimization algorithm could adapt its learning rate to local changes in the loss landscape. In that way, the algorithm would be less dependent on the initial learning rate.\nAdagrad Adagrad adapts the learning rate to the parameters following the idea that parameters associated with salient features should be updated less frequently \u0026lt;\u0026amp;duchiAdaptiveSubgradientMethods2011\u0026gt;. If they occur often, updating them with a larger step would result in a solution that is more dependent on them at the expense of other features.\nAdagrad uses a different learning rate for every parameter:\n\\[ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}}g_t \\]\nHere, \\(g_t = \\frac{\\partial J(\\theta)}{\\partial \\theta}\\). This provides a partial derivative for every parameter \\(\\theta_i\\). A history of gradient changes are accumulated in a matrix \\(G_t \\in \\mathbb{R}^{d \\times d}\\) which is a diagonal matrix containing the sum of squares of the gradients with respect to each \\(\\theta_i\\) up to the current step.\nIn effect, the parameters with larger partial derivatives have a sharper decrease in learning rate. The downside to this method is that, as squared gradients are accumulated in \\(G_t\\), the sum increases causing the learning rate to eventually be too small to learn.\nRMSProp To remedy the long term issues of Adagrad, Geoffrey Hinton proposed RMSProp. There was no formal publication for this. It was discussed and taught in Coursera course on Neural Networks. Instead of accumulating gradients, RMSProp uses an exponentially weighted moving average:\n\\[ \\mathbf{s}_t = \\rho \\mathbf{s} + (1 - \\rho)\\mathbf{g}_{t-1} \\odot \\mathbf{g}_t \\]\nA new parameter \\(\\rho\\) controls how much of the historical gradient is used. The update is\n\\[ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\mathbf{s}_t + \\epsilon}}\\mathbf{g}_t. \\]\nHinton proposed that \\(\\rho=0.9\\) in the original lectures.\nAdam One of the most popular gradient descent variants in used today is Adam \u0026lt;\u0026amp;kingmaAdamMethodStochastic2017\u0026gt;. Short for Adaptive Moment Estimation, Adam adapts the learning rate to each parameter. Similar to RMSProp, it stores an exponentially moving average of past squared gradients. Adam additionally stores first-order moments of the gradients.\nAfter calculating the gradients \\(g_t\\) at time \\(t\\) the first and second moment estimates are updated as\n\\begin{align*} m_t \u0026amp;= \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t\\\\ v_t \u0026amp;= \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2 \\end{align*}\nThe estimates \\(m_t\\) and \\(v_t\\) are initialized to zero leading to updated estimates that are biased to zero. The authors counteract this by computing bias-corrected estimates:\n\\begin{align*} \\hat{m}_t \u0026amp;= \\frac{m_t}{1 - \\beta_1^t}\\\\ \\hat{v}_t \u0026amp;= \\frac{v_t}{1 - \\beta_2^t} \\end{align*}\nThe final update rule step is\n\\[ \\theta_t = \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}} + \\epsilon}. \\]\nThere are several other varients. A good overview of these can be found on Sebastian Ruder\u0026rsquo;s blog. The figures below provide some visual intuition of the behavior of common gradient descent variants. These visualizations were provided by Alec Radford.\nFigure 5: Behavior of algorithms at a saddle point (Credit: Alec Radford). Figure 6: Behavior of each algorithm on a loss surface (Credit: Alec Radford). Additional visualizations can be found here.\nParameter Initialization Due to the complexity of their loss landscapes, the choice of initialization can have a significant impact on the solution. This affects how quickly the model converges. Although recent work aims to smooth loss surfaces so that models are easier to train, deep learning models can be tricky to reproduce.\nThere is not much known about what makes the most optimal initialization strategy, but one property is that of weight symmetry. If all weights are initialized to the same value, their update will also be uniform. If two nodes are connected to the same input, there update will be uniform as well. Understanding this, a reasonable initialization strategy would be to ensure that the weights to not permit any symmetry in nodes connected to the same input.\nSmall weights during initialization may lead to vanishing gradients. Large weights may lead to exploding gradients as successive multiplications are applied. The parameter values should be large enough to propagate information effectively through the network.\nNormalized Initialization (Xavier) Normalized initialization chooses an initial scale of the weights of a fully connected layer based on the number input and output nodes:\n\\[ W_{i,j} \\sim U\\Bigg(-\\sqrt{\\frac{6}{m + n}}, \\sqrt{\\frac{6}{m+n}}\\Bigg), \\]\nwhere \\(m\\) and \\(n\\) are the number of input and output nodes, respectively. This initialization was empirically validated by \u0026lt;\u0026amp;glorotUnderstandingDifficultyTraining\u0026gt; with the goal that all layers have the same activation variance and back-propagated gradient variance.\nHe Initialization Xavier initialization is based on successive matrix multiplications without any non-linearities. Any deep learning model will surely break this assumption. He et al. derive another initialization strategy while considering rectified linear units (ReLU) and parametric rectified linear units (PReLU) \u0026lt;\u0026amp;heDelvingDeepRectifiers2015\u0026gt;.\nResources https://spell.ml/blog/lr-schedulers-and-adaptive-optimizers-YHmwMhAAACYADm6F ","date":1649307600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649307600,"objectID":"4b310c5d80a400fae4538445a9f954e7","permalink":"https://ajdillhoff.github.io/notes/optimization_for_deep_learning/","publishdate":"2022-04-07T00:00:00-05:00","relpermalink":"/notes/optimization_for_deep_learning/","section":"notes","summary":"Table of Contents Resources Introduction Gradient Descent and its Variants Adaptive Learning Rate Methods Parameter Initialization Resources Resources https://ruder.io/optimizing-gradient-descent/ https://www.deeplearningbook.org/contents/optimization.html Introduction empirical risk minimization - minimizing over an empirical distribution. Differs from risk minimization which is minimizing over the true distribution. We typically do not know the true distribution.\nComplex models are able to memorize the dataset.\nIn many applications for training, what we want to optimize is different from what we actually optimize since we need to have useful derivatives for gradient descent.","tags":["deep learning"],"title":"Optimization for Deep Learning","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Convolution Operator Properties of Convolutions Parameter Sharing Pooling Backwards Pass Example Neural Networks for Image Classification Useful Resources Key Concepts\nInvariance and Equivariance Definition Padding, Stride, Kernel size, dilation Purpose of multiple feature maps Receptive fields and hierarchies of features Downsampling, Upsampling, Examples in research Introduction Dense neural networks made up of linear layers and a chosen activation function are not practical for image data. Consider an image of size \\(224\\times224\\times3\\). The first layer of a dense network would require a \\(150,528\\times n\\) parameter matrix, where \\(n\\) is the number of nodes in the first layer. It is common to build dense networks where the first layer has more nodes than input features. In this case, we would need a minimum of \\(150,528^2\\) parameters in the first layer. Even if we chose something much smaller like \\(n=1024\\), this would require \\(154,140,672\\) parameters for just the first layer. This is clearly impractical.\nAside from requiring a large number of parameters, we might ask whether it is beneficial to feed raw pixel values into a dense network. The network itself would be learning pixel-wise features with no regard to their spatial relationship. This makes our network\u0026rsquo;s job much more difficult because the spatial arrangement of features tells us so much about what we see. In practice, this means that the network would have to learn the same features at every location in the image. We would instead prefer this network to learn features that are invariant to translation. That is, the network should learn features that are the same regardless of where they appear in the image.\nInvariance to translation is very convenient and can save our network a lot of work in learning the same feature at every point in the input. It is also desirable that our network is invariant to other transformations such as rotation, scaling, skewing, and warping. Formally, a function \\(f(\\mathbf{x})\\) of an image \\(\\mathbf{x}\\) is invariant to a transformation \\(t(\\mathbf{x})\\) if\n\\[ f(t(\\mathbf{x})) = f(\\mathbf{x}). \\]\nAside from invariance, some models should be equivariant to certain transformations. That is, the output of the model should change in the same way as the input. Image segmentation models should be equivariant to translation. If we were to shift an image by a few pixels, the output segmentation mask should also shift by the same amount. Convolutional neural networks are equivariant to translation.\nConvolution Operator A convolution is a function that takes two functions as input and produces a third function as output. The first function is the input and the second function is the kernel. The output is called the feature map. The kernel is also sometimes called the filter.\n\\[ (f * g)(t) = \\int f(t-a)g(a)da \\]\nWe can view them more concretely by considering the functions to be vectors. For example, let the function \\(f\\) be an input vector \\(x\\) and \\(w\\) be a kernel representing a filter. The convolution operator is then\n\\[ (x * w)(t) = \\int x(t-a)w(a)da. \\]\nThe result the feature map representing the response of the kernel at each location in the input.\nIn the case of discrete values, the operator is written as\n\\[ (x * w)(t) = \\sum_{a}x(t-a)w(a). \\]\nIn machine learning, the kernel \\(w\\) is usually represented by some set of parameters that is optimized.\nCNNs for images use a 2D convolution defined as\n\\[ (I * K)(i, j) = \\sum_m \\sum_n I(i-m, j-n)K(m, n). \\]\nIn this formulation, the kernel is effectively flipped across the vertical and horizontal axis.\nFigure 1: 2D Convolution (Image Credit: Song Ho Ahn (linked above)). In practice, most deep learning APIs implement cross-correlation. Whether the function is implemented as true convolution makes no difference when it comes to optimizing a deep model since filter weights that are produced with cross-correlation would be produced, albeit flipped, with convolution.\n\\[ (K * I)(i, j) = \\sum_m \\sum_n I(i+m, j+n)K(m, n). \\]\nProperties of Convolutions Convolutional networks are commonly built on full or valid convolutions. Other variants have also been explored. Here, we will briefly discuss the different properties of this operator. A more detailed treatment can be found in (Dumoulin and Visin 2018).\nPadding By definition, a convolution of an input with a filter of size \\(n\\times n\\) will produce an output of size \\((m-n+1)\\times(m-n+1)\\), where \\(m\\) is the size of the input. This means that the output will be smaller than the input. This is often referred to as a valid convolution. The figure below shows a convolution between a \\(3\\times3\\) kernel and a \\(5\\times5\\) input.\nFigure 2: A valid convolution (Dumoulin and Visin 2018). The output of this convolution is a \\(3\\times3\\) feature map. This is a problem if we want to build a deep network. Each convolution will reduce the size of the input. If we were to stack multiple convolutional layers, the output would eventually be too small to be useful. If we want our output to be same size as the input, we can add padding to the original input image before convolving it. This is often known as a full convolution. An example is shown below.\nFigure 3: A full convolution (Dumoulin and Visin 2018). Stride So far, we have only looked at convolutions which step by 1 unit as they shift over the image. We can control the size of this step, or stride, to produce different outcomes. Picking a non-unit stride has a number of effects on the features that are learned in a convolutional neural network.\nDimensionality reduction: Skipping over pixels reduces the size of the output feature map. This provides another way of downsampling the input. Less computation: Fewer computations are required to produce the output feature map. Increased field of view: A larger stride increases the field of view of the kernel, leading to larger receptive fields in deeper layers. Given an input of size \\(m\\times m\\) and a kernel of size \\(n\\times n\\), the output size of a convolution with stride \\(s\\) is given by\n\\[ \\left\\lfloor\\frac{m-n}{s}\\right\\rfloor + 1. \\]\nThe figure below shows a convolution with stride 2 on a \\(5\\times5\\) input.\nFigure 4: A convolution with stride 2 (Dumoulin and Visin 2018). Kernel Size The size of the kernel has a large impact on the features that are learned. A larger kernel will have a larger receptive field. This means that the kernel will be able to capture more information about the input. However, this comes at the cost of increased computation. Common kernel sizes in most CNNs are \\(3\\times3\\), \\(5\\times5\\), and \\(7\\times7\\). It is also convenient to pick an odd kernel size so that the kernel has a center pixel.\nDilation Around 2015, a research trend for CNNs was to find a way to increase the receptive field without adding more parameters. The result is a dilated convolution. The output of a dilated convolution is computed by skipping over pixels in the input. The figure below shows a \\(3\\times3\\) kernel with a dilation of 2.\nFigure 5: A dilated convolution (Dumoulin and Visin 2018). The output size is computed as\n\\[ \\left\\lfloor\\frac{m + 2p - n - (n-1)(d-1)}{s}\\right\\rfloor + 1, \\]\nwhere \\(p\\) is the amount of padding and \\(d\\) is the dilation factor.\nParameter Sharing In a densely connected layer, each input has a corresponding weight attached to it. For example, we ran a few introductory experiments on the CIFAR10 dataset using a deep, densely connected network. To reduce the amount of parameters in the first layer, we converted each image to grayscale. The input also had to be vectorized in order to be processed. With a processed image of size \\(32 \\times 32\\), this resulted in a \\(1024\\) dimensional vector for each input. Our first layer had \\(512\\) nodes resulting in a parameter matrix of size \\(1024 \\times 512\\).\nConvolution layers have shared parameters, meaning the same parameters are used for each region on the input. A single channel 2D filter of size \\(n \\times n\\) only requires \\(n \\times n\\) parameters. Each kernel is applied to every location in the original input using the same parameters.\nKernels are equivariant to translation because of their shared parameters. That is, as the input changes, the output will change in the same way. Formally, two functions \\(f\\) and \\(g\\) are equivarient if\n\\[ f(g(x)) = g(f(x)). \\]\nIn the context of image features, a kernel applied across an image will produce strong responses in regions that exhibit the same local features. For example, a kernel that detects horizontal lines will produce strong responses across all parts of the image that show a large contrast between vertical pixels.\nPooling When a convolution is applied to some input image, the resulting output feature map represents the responses of the kernel applied to each location in the image. If this original image were to be shifted by a few pixels, the reponses would also be shifted. In order to increase the robustness of a model to small perturbations such as translation, a pooling layer was historically employed after each non-linear activation following a convolutional layer.\nThey effectively provide a summary statistic of a local region by selecting the average or maximum responses in a small window. This provides translation invariance since the maximum response will be the same for a region even if it is translated by a small amount. It also acts as a quick way to downsample the image, leading to fewer parameters in the model.\nModern works do not employ pooling operations as often. For example (He et al. 2016) perform dimensionality reduction with \\(1 \\times 1\\) convolutions. (Springenberg et al. 2015) argue that fully convolutional networks can achieve the same performance without max pooling.\n\u0026ldquo;The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster.\u0026rdquo; - Geoffrey Hinton\nBackwards Pass The parameters of a convolutional layer are updated via backpropagation like any other layer with trainable parameters. Given a kernel \\(w\\), it is necessary to compute \\(\\frac{\\partial \\mathcal{L}}{\\partial w_{m\u0026rsquo;,n\u0026rsquo;}}\\), where \\(w_{m\u0026rsquo;, n\u0026rsquo;}\\) is the \\((m\u0026rsquo;, n\u0026rsquo;)th\\) entry of the kernel. This entry affects all entries in the feature map, so \\(\\frac{\\partial \\mathcal{L}}{\\partial w_{m\u0026rsquo;,n\u0026rsquo;}}\\) will sum over all such entries.\nTo show the gradient calculation, we will assume a convolutional layer with zero padding and unit stride with a square \\(2 \\times 2\\) kernel applied to a square \\(3 \\times 3\\) input. The output map is then \\((3 - 2 + 1) \\times (3 - 2 + 1) = 2 \\times 2\\).\n\\[ \\frac{\\partial \\mathcal{L}}{\\partial w_{m\u0026rsquo;,n\u0026rsquo;}} = \\sum_{i=0}^2 \\sum_{j=0}^2 \\frac{\\partial \\mathcal{L}}{\\partial x_{i, j}} \\frac{\\partial x_{i,j}}{\\partial w_{m\u0026rsquo;, n\u0026rsquo;}} \\]\nIf \\(\\mathbf{z}^{(l-1)}\\) is the output from the previous layer, then\n\\begin{align*} \\frac{\\partial x_{i, j}}{\\partial w_{m\u0026rsquo;, n\u0026rsquo;}} \u0026amp;= \\frac{\\partial}{\\partial w_{m\u0026rsquo;, n\u0026rsquo;}} \\sum_{m} \\sum_{n} w_{m, n} z_{i+m, j+n}^{(l-1)} + b\\\\ \u0026amp;= \\frac{\\partial}{\\partial w_{m\u0026rsquo;, n\u0026rsquo;}} w_{m\u0026rsquo;, n\u0026rsquo;}z_{i+m\u0026rsquo;, j+n\u0026rsquo;}^{(l-1)}\\\\ \u0026amp;= z_{i+m\u0026rsquo;, j+n\u0026rsquo;}^{(l-1)} \\end{align*}\nThen \\(\\frac{\\partial \\mathcal{L}}{\\partial w_{m\u0026rsquo;,n\u0026rsquo;}}\\) becomes\n\\begin{align*} \\frac{\\partial \\mathcal{L}}{\\partial w_{m\u0026rsquo;,n\u0026rsquo;}} \u0026amp;= \\sum_{i=0}^2 \\sum_{j=0}^2 \\frac{\\partial \\mathcal{L}}{\\partial x_{i, j}} z_{i+m\u0026rsquo;, j+n\u0026rsquo;}^{(l-1)}\\\\ \u0026amp;= \\frac{\\partial \\mathcal{L}}{\\partial x_{i, j}} * z_{m\u0026rsquo;, n\u0026rsquo;}^{(l-1)}. \\end{align*}\n\\(\\frac{\\partial \\mathcal{L}}{\\partial x_{i, j}}\\) represent the gradients with respect to the feature maps. To match the flipped kernel used in the forward pass, they are flipped in an opposite manner.\nExample Let\u0026rsquo;s train and evaluate a convolutional neural network on the OG network: LeNet5 (LeCun et al. 1989).\nNeural Networks for Image Classification ILSVRC The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is the most popular image classification and object detection challenge starting in 2010. It now exists as the ILSVRC 2012-2017 challenge on Kaggle.\nAlexNet https://code.google.com/archive/p/cuda-convnet/\nThe network that arguably popuarlized deep learning by achieving a 37.5% top-1 and 17% top-5 error rate on the ILSVRC-2010 test set. This model performed significantly better than leading competitors (Krizhevsky, Sutskever, and Hinton 2017).\nFigure 6: ILSVRC-2010 results reported by Krizhevsky et al. This performance was based on many different insights and techniques including ReLU activations and dropout. The authors stated in their original publication that the large capacity of the model is necessary to fully describe the diversity of objects in ImageNet.\nArchitecture Figure 7: AlexNet architecture (from Krizhevsky et al.) AlexNet is made up of 5 convolutional layers followed by 3 fully-connected layers. The outputs of the last layer are used as input to the softmax function.\nEach layer uses the ReLU activation function.\n\\[ f(x) = \\max(0, x) \\]\nThe justification for switching to ReLU as opposed to sigmoid or tanh is the faster training times. Experiments on smaller CNNs show that networks with ReLU reach 25% training error on CIFAR-10 six times faster than those with tanh activations.\nFigure 8: Training loss over time using ReLU (solid) versus tanh (dotted) (from Krizhevsky et al.) Another benefit of ReLU activations is that they are less reliant on input normalization. In a saturating activation function like tanh, large absolute values in the inputs will be clamped to either -1 or 1. ReLU is unbounded above 0. Networks can still train as long as some input is positive. Local response normalization (LRN) is used after the first and second convolutional layers.\nThe motivation behind LRN is taken from lateral inhibition in neurobiology. An overly excited neuron (one with a high response) can subdue or dampen the responses from local neighbors. If all responses in a local region are uniformly large, which can happend since ReLU is unbounded, it will dampen them all.\nIn practice, they showed that applying LRNs to their model reduced the top-1 and top-5 error rates by 1.4% and 1.2%, respectively.\nRegularization The entire network has 60 million parameters. Even with so many parameters and training on a dataset with over 8 million images, their model overfits the training data quickly without the aid of regularization. They employ both image translations and horizontal reflections.\nThe use of translations is where the popular \\(224 \\times 224\\) training size originated. The original size of the images in the dataset is \\(256 \\times 256\\). To work with random translations without worrying about padding, they crop the final output to \\(224 \\times 224\\). The final output of the network extracts 5 \\(224 \\times 224\\) patches from the test input and averages the network prediction made on each patch.\nAdditionally, they alter the RGB intensities so that the network is less reliant on specific intensities and illumination for each object. The intuition is that the identity of an object is invariant to lighting conditions.\nAs a last form of regularization, they employ dropout in the first two fully-connected layers.\nTraining They trained their model on a training set of 1.2 million images using two NVIDIA GTX 580 3GB GPUs. They had to write their own optimized CUDA code for this since deep learning frameworks such as Tensorflow and PyTorch did not exist yet. The training took ~6 days to pass 90 epochs.\nVGG Published in 2015, (Simonyan and Zisserman 2015) explore how depth plays a role in convolutional neural networks. They systematically increase the depth of the network while keep other hyperparameters fixed. The filter sizes are also kept at \\(3 \\times 3\\).\nSimilar to (Krizhevsky, Sutskever, and Hinton 2017), they use ReLU activations and in only one of their models to they employ Local Response Normalization. They found that adding LRN to their model did not increase performance. Instead, it only increased computation time and memory consumption. Their models are summarized in the table below.\nFigure 9: Model configurations used (Simonyan and Zisserman). The number of parameters for each network is 133 million, 133 million, 134 million, 138 million, and 144 million starting from A to E.\nGoogLeNet Figure 10: The network-in-network architecture pairs perfectly with the Inception meme. Proposed a 22-layer network architecture that has \\(12 \\times\\) fewer parameters than (Krizhevsky, Sutskever, and Hinton 2017).\nThe authors were already thinking about applications in mobile computing, where hardware limitations would require smaller networks that still perform well.\n\u0026ldquo;In this paper, we will focus on an efficient deep neural network architecture for computer vision, codenamed Inception, which derives its name from the Network in network paper by Lin et al. in conjunction with the famous “we need to go deeper” internet meme.\u0026rdquo; - Szegedy et al.\nIt was apparent at the time that building larger networks would generally lead to better performance. Adding more parameters leads to easier overfitting. Bigger networks also mean more computation. If the goal is to adapt high quality networks into mobile computing, solutions would have to include more sophistication than simply adding more components.\nHebbian Learning A linear increase in filters leads to a quadratic increase in computation. If most filter parameters end up being close to 0, then this increase in model capacity is wasted. One solution is to include sparsity in the network instead of having dense connections. Szegedy et al. were motivated by the work of Arora et al., which they summarized as follows.\n\u0026ldquo;Their main result states that if the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs.\u0026rdquo; - Szegedy et al.\nThis result relates with Hebbian theory on synaptic plasticity which is summarized as \u0026ldquo;neurons that fire together, wire together.\u0026rdquo;\nFrom Theory to Architecture Motivated by sparse connections, the architecture is designed to approximate sparsity given current dense components like convolutional layers.\nFigure 11: Naive version of the Inception module (Szegedy et al.) The Inception module design as seen above is motivated as follows. In layers closer to the raw input, filters would be grouped into local regions. In this case, a \\(1 \\times 1\\) convolution would summarize these groups.\nFor clusters that are spread out, a larger filter would be needed to cover the larger regions. This motivates the use of \\(3 \\times 3\\) and \\(5 \\times 5\\) filters.\nThe choice to include a max pooling function in each module is based on previous successes of using max pooling.\nFigure 12: Description of layers from Szegedy et al. Vanishing Gradients Creating a deeper network means that training is more susceptible to the vanishing gradient problem. They noted that shallower networks that perform well on image classification would surely provide strong disciminative features. They leverage this idea by computing 2 additional intermediate outputs: one in the middle of the network and an additional output 3 layers beyond that one. This permits the gradients to be strengthened by intermediate losses when combined with the original gradients.\nFigure 13: GoogLeNet model (Szegedy et al.) Results GoogLeNet took 1st place in the 2014 ILSVRC with a 6.67% top-5 error rate.\nResNet By 2016, it was clear that deeper models could build a richer hierarchy of features leading to better performance on a wide range of computer vision tasks. However, with deeper networks comes the vanishing gradient problem. Training them remained difficult for a time, but initialization and other normalization techniques found ways to resolve this issue.\nWith deeper networks, a new problem appeared. Adding more layers generally results in higher accuracy. At a certain point, adding additional layers leads to a decrease in accuracy. Many experiments ruled out the possibility of overfitting by observing that the training error was increasing as well.\nFigure 14: Result of experiments showing that decreased accuracy was not a result of overfitting. Identity Mappings Consider a shallow network with some measure performance on a task. If we were to add additional layers to make this network deeper, but those layers were simply identity mappings, then we should expect an error no greater than the original shallow network. However, current solvers are unable to find such a solution in a reasonable amount of time on an equally deep network optimized from a random initialization.\nResidual Functions The main idea of this paper is to attempt to learn a residual function \\(\\mathcal{F}(\\mathbf{x}) := \\mathcal{H}(\\mathbf{x}) - \\mathbf{x}\\) of the desired mapping \\(\\mathcal{H}(\\mathbf{x})\\) rather than attempting to learn the mapping directly. The desired mapping then given by \\(\\mathcal{H}(\\mathbf{x}) = \\mathcal{F}(\\mathbf{x}) + \\mathbf{x}\\). If it were optimal to learn an identity mapping, the idea is that it would be simpler to learn by moving towards a 0 residual.\nFigure 15: Residual unit (He et al.) The function can be implemented into neural networks by using skip connections, as seen in the figure above. Adding these identity mappings does not require any additional parameters, as the input is simply passed to the end of the stack.\nArchitecture Complexity They compare a 34-layer plain network based on the VGG-19 architecture with a 34-layer residual network. They note that VGG-19 has more filters and higher complexity than their residual network. Specifically, VGG-19 requires 19.6 billion FLOPs compared to only 3.6 billion for their 34-layer residual network.\nFigure 16: Comparison of architectures and their complexity (He et al.) Results They evaluate how well the residual networks generalize when adding more layers. As mentioned in the introduction, typical models would see an increase in training error as the number of layers were increased.\nFigure 17: Training comparisons between plain and residual networks (He et al.) Their ensemble of models achieved 3.57% top-5 error on the ImageNet test set, achieving 1st place in the ILSVRC 2015 classification challenge. It additionally was adapted to other challenges and won first place on ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in both the ILSVRC and COCO 2015 competitions.\nUseful Resources https://www.songho.ca/dsp/convolution/convolution.html#convolution_2d https://github.com/vdumoulin/conv_arithmetic https://cs231n.github.io/convolutional-networks/ https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/ https://grzegorzgwardys.wordpress.com/2016/04/22/8/ References Dumoulin, Vincent, and Francesco Visin. 2018. “A Guide to Convolution Arithmetic for Deep Learning.” Arxiv:1603.07285 [Cs, Stat], January. http://arxiv.org/abs/1603.07285. He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “Deep Residual Learning for Image Recognition.” In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–78. Las Vegas, NV, USA: IEEE. https://doi.org/10.1109/CVPR.2016.90. Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. “ImageNet Classification with Deep Convolutional Neural Networks.” Communications of the Acm 60 (6): 84–90. https://doi.org/10.1145/3065386. LeCun, Yann, Bernhard Boser, John Denker, Donnie Henderson, R. Howard, Wayne Hubbard, and Lawrence Jackel. 1989. “Handwritten Digit Recognition with a Back-Propagation Network.” In Advances in Neural Information Processing Systems. Vol. 2. Morgan-Kaufmann. https://papers.nips.cc/paper/1989/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html. Simonyan, Karen, and Andrew Zisserman. 2015. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” Arxiv:1409.1556 [Cs], April. http://arxiv.org/abs/1409.1556. Springenberg, Jost Tobias, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. 2015. “Striving for Simplicity: The All Convolutional Net.” Arxiv:1412.6806 [Cs], April. http://arxiv.org/abs/1412.6806. ","date":1648875600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648875600,"objectID":"9924d8865e727ea2a7a88cf96aae9bff","permalink":"https://ajdillhoff.github.io/notes/convolutional_neural_networks/","publishdate":"2022-04-02T00:00:00-05:00","relpermalink":"/notes/convolutional_neural_networks/","section":"notes","summary":"Table of Contents Introduction Convolution Operator Properties of Convolutions Parameter Sharing Pooling Backwards Pass Example Neural Networks for Image Classification Useful Resources Key Concepts\nInvariance and Equivariance Definition Padding, Stride, Kernel size, dilation Purpose of multiple feature maps Receptive fields and hierarchies of features Downsampling, Upsampling, Examples in research Introduction Dense neural networks made up of linear layers and a chosen activation function are not practical for image data. Consider an image of size \\(224\\times224\\times3\\).","tags":["deep learning","computer vision"],"title":"Convolutional Neural Networks","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction What makes a model deep? Deep Networks Deep vs. Shallow Networks High Dimensional Structured Data Activation Functions Loss Functions A Typical Training Pipeline Introduction Deep learning is a term that you\u0026rsquo;ve probably heard of a million times by now in different contexts. It is an umbrella term that encompasses techniques for computer vision, bioinformatics, natural language processing, and much more. It almost always involves a neural network of some kind that was trained on a large corpus of data.\nThe existence of the word \u0026ldquo;deep\u0026rdquo; implies a contrast to \u0026ldquo;shallow\u0026rdquo; learning. Some definition define a deep network as an artificial neural network with more than 1 layer. Another definition is that a deep model will include a hierarchy of features that are learned from the data. These features are learned as part of the optimization process as opposed to being manually engineered as is required in other machine learning techniques.\nIf you are not yet familiar with neural networks, follow the link to learn about their basics as they are the foundation of deep learning systems.\nWe will cover how to implement an array of deep learning models for different tasks. Different layers and activation functions will be explored as well as the effect of regularization. There will also be a focus on best practices for organizing a machine learning project.\nWhat makes a model deep? We begin by comparing shallow networks with deep networks. What defines a deep network? Is it as simple as crossing a threshold into \\(n\\) layers? As evidenced by (Zeiler and Fergus 2013) deeper networks allow for a more robust hierarchy of image features.\nThere is work by (Montúfar et al. 2014) which suggests that shallow networks require an exponential amount of nodes as compared to deeper networks. Additionally, there are many individual results which suggest that deeper networks provide better task generalization.\nAs we will later see when studying Convolutional Neural Networks, the optimization of such deep networks produces features that maximize the performance of a task. That is, the network is not only optimizing the overall performance of task, but it produces features from the data that may be useful in other contexts. This is particularly useful for transfer learning, where large pre-trained models can be used as starting points for novel tasks. The benefit being that a complete retraining of the model is not necessary.\nDeep Networks Like neural networks, deep networks are defined by the number of layers, nodes per layer, activation functions, and loss functions. We now review the forward and backward pass, providing more insight into the structure and usage of deep networks along the way.\nConsider a deep network with \\(L\\) layers. Layer \\(l\\) has \\(n_{l-1}\\) input connections and \\(n_l\\) output nodes and activation function \\(g^{(l)}\\). The final output is evaluated with some ground truth using a loss function \\(\\mathcal{L}\\).\nForward Pass \\begin{align*} \\mathbf{a}^{(l)} \u0026amp;= W^{(l)}\\mathbf{z}^{(l-1)} + \\mathbf{b}^{(l)}\\\\ \\mathbf{z}^{(l)} \u0026amp;= g^{(l)}(\\mathbf{a}^{(l)})\\\\ \\end{align*}\nThis is repeated from the input to the last layer. For the first layer \\(l=1\\), the input \\(\\mathbf{z}^{(0)} = \\mathbf{x}\\). In practice, the output \\(\\mathbf{a}^{(l)}\\) is cached since it is required for the backward pass. This prevents the values from needing to be computed twice.\nIt is also worth it to study the sizes of the matrices while performing a forward pass. For a layer \\(l\\), \\(W^{(l)} \\in \\mathbb{R}^{n_l \\times n_{l-1}}\\) and the input \\(\\mathbf{z}^{(l-1)} \\in \\mathbb{R}^{n_{l-1} \\times 1}\\). When training, it is common to perform batch gradient descent with batches of input of size \\(B\\). Then, \\(\\mathbf{z}^{(l-1)} \\in \\mathbb{R}^{n_{l-1}\\times B}\\) and \\(\\mathbf{a}^{(l)}, \\mathbf{b}^{(l)} \\in \\mathbb{R}^{n_l \\times B}\\).\nBackward Pass During the backward pass, the gradient is propagated from the last layer to the first. Each layer that contains trainable parameters must also compute the gradient of the network output with respect to the weights and biases. This can be done in a modular way, as shown next.\nConsider the last layer. The gradients with respect to the weights and biases are\n\\begin{align*} \\frac{d\\mathcal{L}}{dW^{(L)}} \u0026amp;= \\frac{d\\mathcal{L}}{d\\mathbf{z}^{(L)}} \\frac{d\\mathbf{z}^{(L)}}{d\\mathbf{a}^{(L)}} \\frac{d\\mathbf{a}^{(L)}}{dW^{(L)}}\\\\ \\frac{d\\mathcal{L}}{d\\mathbf{b}^{(L)}} \u0026amp;= \\frac{d\\mathcal{L}}{d\\mathbf{z}^{(L)}} \\frac{d\\mathbf{z}^{(L)}}{d\\mathbf{a}^{(L)}} \\frac{d\\mathbf{a}^{(L)}}{d\\mathbf{b}^{(L)}}. \\end{align*}\nTo see how the gradient continues to be propagated backward, compute the same thing for layer \\(L-1\\)\n\\begin{align*} \\frac{d\\mathcal{L}}{dW^{(L-1)}} \u0026amp;= \\frac{d\\mathcal{L}}{d\\mathbf{z}^{(L)}} \\frac{d\\mathbf{z}^{(L)}}{d\\mathbf{a}^{(L)}} \\frac{d\\mathbf{a}^{(L)}}{d\\mathbf{z}^{(L-1)}} \\frac{d\\mathbf{z}^{(L-1)}}{d\\mathbf{a}^{(L-1)}} \\frac{d\\mathbf{a}^{(L-1)}}{dW^{(L-1)}}\\\\ \\frac{d\\mathcal{L}}{d\\mathbf{b}^{(L-1)}} \u0026amp;= \\frac{d\\mathcal{L}}{d\\mathbf{z}^{(L)}} \\frac{d\\mathbf{z}^{(L)}}{d\\mathbf{a}^{(L)}} \\frac{d\\mathbf{a}^{(L)}}{d\\mathbf{z}^{(L-1)}} \\frac{d\\mathbf{z}^{(L-1)}}{d\\mathbf{a}^{(L-1)}} \\frac{d\\mathbf{a}^{(L-1)}}{d\\mathbf{b}^{(L-1)}}. \\end{align*}\nAs seen above, to continue propagating the gradient backward, each layer \\(l\\) must also compute\n\\[ \\frac{d\\mathbf{a}^{(l)}}{d\\mathbf{z}^{(l-1)}}. \\]\nTo summarize, every layer with trainable parameters will compute\n\\begin{align*} \\frac{d\\mathcal{L}}{dW^{(l)}} = \\frac{d\\mathbf{a}^{(l+1)}}{d\\mathbf{z}^{(l)}} \\frac{d\\mathbf{z}^{(l)}}{d\\mathbf{a}^{(l)}} \\frac{d\\mathbf{a}^{(l)}}{dW^{(l)}}\\\\ \\frac{d\\mathcal{L}}{d\\mathbf{b}^{(l)}} = \\frac{d\\mathbf{a}^{(l+1)}}{d\\mathbf{z}^{(l)}} \\frac{d\\mathbf{z}^{(l)}}{d\\mathbf{a}^{(l)}} \\frac{d\\mathbf{a}^{(l)}}{d\\mathbf{b}^{(l)}}. \\end{align*}\nThe term \\(\\frac{d\\mathbf{a}^{(l+1)}}{d\\mathbf{z}^{(l)}}\\) is the gradient that is propagated from layer \\(l+1\\).\nDeep vs. Shallow Networks As mentioned above, a shallow network can approximate any continuous function to arbitrary precision. If a deep network can represent the composition of two shallow networks, then it can also approximate any continuous function to arbitrary precision. Then why are deep networks better than shallow networks when both can approximate any function? There are a few compelling reasons as to why, starting with the capacity of the network and the number of linear regions it can represent per parameter.\nAs discussed in Understanding Deep Learning (Prince 2023), a shallow network with 1 input, 1 output, and \\(D \u0026gt; 2\\) hidden units can create up to \\(D + 1\\) linear regions using \\(3D+1\\) parameters. The \\(3D + 1\\) comes from the fact that the hidden layer requires \\(D\\) parameters for the weights with an extra \\(D\\) parameters for the bias terms. To convert from the hidden layer to the output layer, there are \\(D\\) parameters for the weights and 1 parameter for the bias term. The figure below shows the maximum number of linear regions as a function of the number of parameters for networks that map one input to one output.\nFigure 1: Maximum number of linear regions as a function of the number of parameters for networks that map one input to one output (Prince 2023). High Dimensional Structured Data For high dimensional structured data, such as images, deep networks are able to learn a hierarchy of features that are useful for the task at hand while requiring a significantly smaller number of parameters than a shallow network. Consider a \\(100\\times100\\) image used as input to a shallow network with 1 hidden layer. This would require \\(10,001\\) parameters to represent the weights and biases. If we instead use a deep network with with convolutional layers, we can use significantly fewer parameters. We will see this more closely when we study Convolutional Neural Networks.\nActivation Functions Sigmoid Function\n\\[ \\sigma(\\mathbf{x}) = \\frac{1}{1 + \\exp(-\\mathbf{x})} \\]\nDerivative\n\\[ \\sigma(\\mathbf{x})(1 - \\sigma(\\mathbf{x})) \\]\nFigure 2: Sigmoid non-linearity (Wikipedia) Loss Functions Loss functions are used to evaluate the performance of a model. In the context of gradient descent, their gradient with respect to the model parameters is used to update the parameters. Loss functions can be constructed using maximum likelihood estimation over a probability distribution or by using a distance metric between the model output and the ground truth. The table below from (Prince 2023) shows some common loss functions and their use cases.\nData Type Domain Distribution Use Univariate, continuous, unbounded \\(\\mathbb{R}\\) univariate normal Regression Univariate, discrete, binary \\(\\{0, 1\\}\\) Bernoulli Binary Classification Univariate, discrete, bounded \\(\\{0, 1\\}^K\\) Multinoulli Multiclass Classification A Typical Training Pipeline When training and evaluating models, especially on benchmark datasets, it is important to properly test their generalization performance. This test is crucial when comparing the efficacy of your ideas versus baseline evaluations or competing methods.\nTo ensure that your model is evaluated in a fair way, it is common to set aside a set of test data that is only used during the final comparison. This data is typically annotated so that some metric can be used.\nIt is true that the training data drives the parameter tuning during optimization. This is most commonly done with gradient descent. However, we will also change the hyperparamers such as learning rate, batch size, and data augmentation. In this case, we want to evaluate the relative performance of each change.\nIf we use the test set to do this, then we are necessarily using the test set for training. Our biases and intuitions about the model\u0026rsquo;s performance would be implicitly influenced by that set. To track our relative changes without using the test set, we can take a portion of the original training set and label it as our validation set.\nThe split between training, validation, and test data is relatively small. Most modern datasets are large, with millions of samples. Consider ImageNet, an image classification dataset with over 14 million samples. Taking 10,000 samples to serve as a validation set is only \\(~.07\\%\\) of the dataset.\nMost modern machine learning frameworks have an easy way to split the dataset. We can do this in PyTorch using torch.utils.data.random_split.\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size]) References Montúfar, Guido, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. 2014. “On the Number of Linear Regions of Deep Neural Networks.” Arxiv:1402.1869 [Cs, Stat], June. http://arxiv.org/abs/1402.1869. Prince, Simon J.D. 2023. Understanding Deep Learning. MIT Press. https://udlbook.github.io/udlbook/. Zeiler, Matthew D., and Rob Fergus. 2013. “Visualizing and Understanding Convolutional Networks.” Arxiv:1311.2901 [Cs], November. http://arxiv.org/abs/1311.2901. ","date":1648530000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697932800,"objectID":"46433fcb0e55a5db6defcac8271e45b5","permalink":"https://ajdillhoff.github.io/notes/deep_learning/","publishdate":"2022-03-29T00:00:00-05:00","relpermalink":"/notes/deep_learning/","section":"notes","summary":"Table of Contents Introduction What makes a model deep? Deep Networks Deep vs. Shallow Networks High Dimensional Structured Data Activation Functions Loss Functions A Typical Training Pipeline Introduction Deep learning is a term that you\u0026rsquo;ve probably heard of a million times by now in different contexts. It is an umbrella term that encompasses techniques for computer vision, bioinformatics, natural language processing, and much more. It almost always involves a neural network of some kind that was trained on a large corpus of data.","tags":["deep learning"],"title":"Deep Learning","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents TODO Introduction AdaBoost TODO Gradient Boosting Introduction Combining predictions from multiple sources is usually preferred to a single source. For example, a medical diagnosis would carry much more weight if it was the result of a consensus of several experts. This idea of prediction by consensus is a powerful way to improve classification and regression models. In fact, good performance of a committee of models can be achieved even if each individual model is conceptually very simple.\nBoosting is one such way of building a committee of models for classification or regression and is popularly implemented by an algorithm called AdaBoost.\nAdaBoost Given a dataset \\(\\{\\mathbf{x}_i\\}\\) and target variables \\(\\{\\mathbf{y}_i\\}\\), AdaBoost first initializes a set of weights corresponding to each data sample as \\(w_i = \\frac{1}{N}\\). At each step of the algorithm, a simple classifier, called a weak learner is fit to the data. The weights for each sample are adjusted based on the individual classifier\u0026rsquo;s performance. If the sample was misclassified, the relative weight for that sample is increased. After all classifiers have been fit, they are combined to form an ensemble model.\nThe Algorithm Initialize data weights \\({w_i}\\) as \\(w_i^{(1)} = \\frac{1}{n}\\) for \\(i = 1, \\dots, n\\).\nFit each weak learner \\(j\\) to the training data by minimizing the misclassification cost:\n\\[ \\sum_{i=1}^n w_i^{(j)} \\mathbb{1}(f_j(\\mathbf{x}_i) \\neq \\mathbf{y}_i) \\]\nCompute a weighted error rate\n\\[ \\epsilon_j = \\frac{\\sum_{i=1}^n w_i^{(j)} \\mathbb{1}(f_j(\\mathbf{x}_i) \\neq \\mathbf{y}_i)}{\\sum_{i=1}^n w_i^{(j)}} \\]\nUse the weighted error rate to compute a weight for each classifier such that misclassified samples are given higher weight:\n\\[ \\alpha_j = \\ln \\bigg\\{\\frac{1 - \\epsilon_j}{\\epsilon_j}\\bigg\\}. \\]\nUpdate the data weights for the next model in the sequence:\n\\[ w_i^{j+1} = w_i^{j} \\exp\\{\\alpha_j \\mathbb{1}(f_j(\\mathbf{x}_i \\neq \\mathbf{y}_i)\\}. \\]\nOnce all weak learners are trained, the final model predictions are given by\n\\[ Y_M(\\mathbf{x}) = \\text{sign} \\Bigg(\\sum_{j=1}^M \\alpha_j f_j(\\mathbf{x})\\Bigg). \\]\nWeak Learners The weak learners can be any classification or regression model. However, they are typically chosen to be very simple to account for training time. For example, a complex deep learning model would be a poor choice for a weak learner.\nOne example of a weak learner is a simple linear model like a Perceptron or decision stump. A standard implementation of AdaBoost uses a decision tree with depth 1, as observed in sklearn\u0026rsquo;s implementation.\nExample Let\u0026rsquo;s put this together and walk through the first few steps of training an AdaBoost model using a decision stump as the weak learner. We will use a very simple dataset to keep the values easy to compute by hand.\nInitial Data\nx1 x2 y weight 1 5 0 0.2 2 6 1 0.2 3 7 0 0.2 4 8 1 0.2 5 9 1 0.2 Weak Learner 1\nThe first learner is trained on the initial data and picks \\(x_1 = 2.5\\) as the split threshold. Input where \\(x_1 \\leq 2.5\\) is assigned to class 0 and all other samples are assigned class 1. The data with this learner\u0026rsquo;s predictions are shown below.\nx1 x2 y weight prediction 1 5 0 0.2 0 2 6 1 0.2 0 3 7 0 0.2 1 4 8 1 0.2 1 5 9 1 0.2 1 Error and weight\nThe error is simple enough to compute as all samples are currently weighted equally. Since two of the samples were misclassified, the error is the sum of their weights.\nTotal error\n\\(e_1 = 0.2 + 0.2 = 0.4\\).\nThe weight of the classifier can then be computed.\nClassifier weight\n\\(\\alpha_1 = \\frac{1}{2} \\ln \\big(\\frac{1 - e_1}{e_1}\\big) = 0.2027\\).\nThe weights of our data can now be updated using this value of \\(\\alpha_1\\). The weight of each example is updated by multiplying each correctly classifed sample by \\(\\exp\\{-\\alpha_1\\}\\) and each incorrectly classified sample by \\(\\exp\\{\\alpha\\}\\):\n\\[ w_i^{j+1} = w_i^{j} \\exp\\{\\alpha_j \\mathbb{1}(f_j(\\mathbf{x}_i \\neq \\mathbf{y}_i)\\}. \\]\nNOTE: You will notice that the equation above is different from the actual update rule that was applied to the weights in this example. In the original publication (TODO: reference Fruend), the weights are renormalized at the end of the loop. In this example, the normalization is combined with the update. In either case, the updated weights are shown below.\nx1 x2 y weight 1 5 0 0.167 2 6 1 0.250 3 7 0 0.250 4 8 1 0.167 5 9 1 0.167 Weak Learner 2\nThe algorithm now moves to the next weak learner, which classifies the data given a threshold of \\(x_1 = 3.5\\). Its predictions are shown below.\nx1 x2 y weight prediction 1 5 0 0.167 0 2 6 1 0.250 0 3 7 0 0.250 0 4 8 1 0.167 1 5 9 1 0.167 1 Only a single sample is misclassified, and the error is computed as before.\nTotal error\n\\(e_2 = 0.250\\)\nClassifier weight\n\\(\\alpha_2 = \\frac{1}{2} \\ln \\big(\\frac{1 - e_2}{e_2}\\big) = 0.5493\\)\nThe weights are updated for each sample, yielding the following data:\nx1 x2 y weight 1 5 0 0.111 2 6 1 0.500 3 7 0 0.167 4 8 1 0.111 5 9 1 0.111 The second sample has been misclassified twice at this point, leading to a relatively high weight. This will hopefully be addressed by the third learner.\nWeak Learner 3\nThe final weak learner splits the data on \\(x_2 = 6.5\\), yielding the following output for each sample.\nx1 x2 y weight prediction 1 5 0 0.111 0 2 6 1 0.500 0 3 7 0 0.167 1 4 8 1 0.111 1 5 9 1 0.111 1 Unfortunately, sample 2 is too tricky for any of our weak learners. The total error is shown below. Since this is a binary classification problem, the error suggests that our weak learner performs worse than random guessing.\nTotal error\n\\(e_3 = 0.667\\)\nClassifier weight\n\\(\\alpha_3 = \\frac{1}{2} \\ln \\big(\\frac{1 - e_3}{e_3}\\big) = -0.3473\\)\nThe negative value of the classifier weight suggests that its predictions will be reversed when evaluated. The updated weights of each data sample are given below.\nx1 x2 y weight 1 5 0 0.167 2 6 1 0.375 3 7 0 0.125 4 8 1 0.167 5 9 1 0.167 Final Classifier\nThe final classifier is a weighted vote of the three weak learners, with the weights being the classifier weights we calculated (0.2027, 0.5493, and -0.3473). The negative weight means that the third learner\u0026rsquo;s predictions are reversed.\n","date":1648011600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648011600,"objectID":"a74fb661122bb4b9319a36a6668cd31f","permalink":"https://ajdillhoff.github.io/notes/boosting/","publishdate":"2022-03-23T00:00:00-05:00","relpermalink":"/notes/boosting/","section":"notes","summary":"Table of Contents TODO Introduction AdaBoost TODO Gradient Boosting Introduction Combining predictions from multiple sources is usually preferred to a single source. For example, a medical diagnosis would carry much more weight if it was the result of a consensus of several experts. This idea of prediction by consensus is a powerful way to improve classification and regression models. In fact, good performance of a committee of models can be achieved even if each individual model is conceptually very simple.","tags":["machine learning"],"title":"Boosting","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Resources Introduction Example: Iris Dataset Growing a Tree Examining the Iris Classification Tree Pruning a Tree The Algorithm Resources https://www.kaggle.com/dmilla/introduction-to-decision-trees-titanic-dataset Introduction A decision tree, or Classification and Regression Trees (CART), is a model that recursively partitions the input space based on a collection of features. The partitions are split based on very simple binary choices. If yes, branch to the left; if no, branch to the right.\nFigure 1: Regression tree (left) and its piecewise constant surface (right) (Source: Machine Learning: A Probabilistic Perspective by Kevin P. Murphy). To compute the response, we represent each individual decision as a function \\(\\phi\\) and sum the responses:\n\\[ f(\\mathbf{x}) = \\mathbb{E}[y | \\mathbf{x}] = \\sum_{m=1}^M w_m \\mathbb{1} (\\mathbf{x} \\in R_m) = \\sum_{m=1}^M w_m \\phi(\\mathbf{x};\\mathbf{v}_m), \\]\nwhere \\(R_m\\) is the \\(m^{\\text{th}}\\) region, \\(w_m\\) is the mean response, and \\(\\mathbf{v}_m\\) is the choice of variable to split on along with its threshold value.\nExample: Iris Dataset To see this on real data, consider the Iris flower dataset. For example, we will look at a decision tree model that classifies each flower into either setosa, versicolor, or virginica.\nFigure 2: Our initial Iris classifier. We are given data about a new iris and want to classify it using this tree. Our sample has a sepal length of 6.1cm, a sepal width of 2.8cm, a petal length of 4.7cm, and a petal width of 1.2cm. The first decision considers the petal width. Our sample has a width of 1.2, so it continues down the right branch.\nThe second decision consideres petal width again. Since our sample does not have a width greater than 1.75, we continue down the left branch.\nAt this point, the model was optimized to now consider the petal length. Our length comes in at 4.7, just shy of going down the right path.\nWe now arrive at the last decision. Since our petal length is not greater than 1.65, the model classifies this sample as versicolor.\nGrowing a Tree To grow a tree, a decision needs to be made as to whether or not the current set of data can be split based on some feature. As such, there should be a reliable way of determining if a feature provided a good split. This is evaluated using a cost function and selecting the feature and value corresponding to the minimum cost:\n\\[ (j^*, t^*) = \\text{arg} \\min_{j\\in\\{1, \\dots, D\\}} \\min_{t \\in \\mathcal{T}_j} \\text{cost}(\\{\\mathbf{x}_i, y_i : x_{ij} \\leq t\\}) + \\text{cost}(\\{\\mathbf{x}_i, y_i : x_{ij} \u0026gt; t\\}). \\]\nIn words, this function finds a value \\(t\\) such that groups the data with the lowest cost. For a regression task, the cost function is typically defined as\n\\[ \\text{cost}(\\mathcal{D}) = \\sum_{i \\in \\mathcal{D}}(y_i - \\bar{y})^2, \\]\nwhere\n\\[ \\bar{y} = \\frac{1}{|\\mathcal{D}|}\\sum_{i \\in \\mathcal{D}} y_i. \\]\nSplits that result in clusters with high variance may still see a higher cost, even though they are the minimum.\nAs their alternative name implies, decision trees can also be used for classification. The splits are still based on features and threshold values at each branch. When a split is considered, a class-conditional probability is estimated for that data. Given data satisfying \\(X_j \u0026lt; t\\), the class-conditional probability is\n\\[ \\hat{\\pi}_c = \\frac{1}{|\\mathcal{D}|}\\sum_{i \\in \\mathcal{D}} \\mathbb{1}(y_i = c). \\]\nThe common error functions used for classification are misclassification rate, entropy, and Gini index. Misclassification rate is computed by summing the number of misclassifications:\n\\[ \\frac{1}{|\\mathcal{D}|} \\sum_{i \\in \\mathcal{D}} \\mathbb{1}(y_i \\neq \\hat{y}) = 1 - \\hat{\\pi}_{\\hat{y}}. \\]\nEntropy is computed as\n\\[ \\mathbb{H}(\\mathbb{\\hat{\\pi}}) = -\\sum_{c=1}^C \\hat{\\pi}_c \\log \\hat{\\pi}_c. \\]\nGini index computes the expected error rate.\n\\[ G = \\sum_{c=1}^C \\hat{\\pi}_c (1 - \\hat{\\pi}_c) = 1 - \\sum_{c=1}^C \\hat{\\pi}_c^2 \\]\nFigure 3: Impurity measured for binary classification (Source: Machine Learning: A Probabilistic Perspective by Kevin P. Murphy) Like entropy, it promotes an equal number of observations across all classes in a node. For small values of \\(\\hat{\\pi}\\), the error is smaller than that of entropy. If the dataset is imbalanced, entropy is typically favored as it penalizes imbalanced datasets more than Gini will. Both will favor splits that result in one node being pure.\nStopping Growth If left unchecked, the algorithm to grow a tree will continue until the data can no longer be split. In the trivial case, this will be when every data point represents a leaf node. In order to prevent overfitting, there are several criteria that are considered.\nDoes the split reduce the cost enough? It may be ideal to only split the data if the cost is reduced by some acceptable value. The reduction can be computed by\n\\[ \\Delta = \\text{cost}(\\mathcal{D}) - \\bigg(\\frac{|\\mathcal{D}_L|}{|\\mathcal{D}|}\\text{cost}(\\mathcal{D}_L) + \\frac{|\\mathcal{D}_R|}{|\\mathcal{D}|} \\text{cost}(\\mathcal{D}_R)\\bigg). \\]\nHas the tree reached some maximum depth? The depth of a tree is set as a hyperparameter. Later, when we look at an example, we will use cross validation to select the best depth parameter for our model.\nIs the distribution of the split pure? If either of the splits is fully made up of data with the same label, there is no need to split it any further.\nIs the split too small? A split that is too small may lead to overfitting.\nExamining the Iris Classification Tree How exactly does the earlier example model make its decision at each node? The full tree is shown below.\nFigure 4: A detailed view of our Iris classifier. The first split is visually very simple to intuit. Using petal width can perfectly split all setosa samples into a single leaf node.\nPruning a Tree Depending on the data, stopping growth based on measuring the relative decrease in error may not result in a model that performs well. Image a dataset that requires multiple features to provide a sufficient classification. If only one of the features is considered in isolation, it may provide no decrease in error. A practical example of this is the XOR problem. Splitting on \\(x_1\\) or \\(x_2\\) in isolation does not provide any indication about the true output. It is only when \\(x_1 \\neq x_2\\) does the output equal to 1.\nTo rectify this, a tree can be grown until it is completely full before pruning the branches that result in the smallest increase in error.\nThe Algorithm The general algorithm is shown in MATLAB below.\n% node = fitTree(node, D, depth) % Recursive function to learn a decision tree % Returns the index of the current node. % % node - The node index in obj.Nodes. % D - Indices to the current data. % depth - Current depth of the tree. function node = fitTree(obj, node, D, depth) % Determine best split for the data and return the split [j, t, dSplit, classDist] = obj.split(D, obj.Nodes(node).features); obj.Nodes(node).prediction = classDist; disp(classDist); % Use heuristic to determine if node is worth splitting if obj.splitNode(depth, classDist) == true % set the node test, the function that determines the branch obj.Nodes(node .test = {j, t}; newFeatures = obj.Nodes(node).features(obj.Nodes(node).features ~= j); % set the child nodes to the left and right splits obj.Nodes(node).children = zeros(size(dSplit, 1), 1); numNewNodes = size(dSplit, 1); for i = 1 : numNewNodes obj.Nodes(end + 1) = struct(\u0026#39;prediction\u0026#39;, 0, \u0026#39;test\u0026#39;, 0, ... \u0026#39;features\u0026#39;, newFeatures, \u0026#39;children\u0026#39;, 0, \u0026#39;parent\u0026#39;, node); obj.Nodes(node).children(i) = obj.fitTree(length(obj.Nodes), dSplit{i}, depth + 1); end end end ","date":1647579600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647579600,"objectID":"cb086cf2a1081a79001139440f6b6605","permalink":"https://ajdillhoff.github.io/notes/decision_trees/","publishdate":"2022-03-18T00:00:00-05:00","relpermalink":"/notes/decision_trees/","section":"notes","summary":"Table of Contents Resources Introduction Example: Iris Dataset Growing a Tree Examining the Iris Classification Tree Pruning a Tree The Algorithm Resources https://www.kaggle.com/dmilla/introduction-to-decision-trees-titanic-dataset Introduction A decision tree, or Classification and Regression Trees (CART), is a model that recursively partitions the input space based on a collection of features. The partitions are split based on very simple binary choices. If yes, branch to the left; if no, branch to the right.","tags":["machine learning"],"title":"Decision Trees","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction The Markov Assumption Definition Evaluation The Viterbi Algorithm Estimating Parameters Expectation Maximization Introduction This article is essentially a grok of a tutorial on HMMs by (RABINER 1989). It will be useful for the reader to reference the original paper.\nUp to this point, we have only explored \u0026ldquo;atomic\u0026rdquo; data points. That is, all of the information about a particular sample is encapsulated into one vector. Sequential data is easily represented by graphical models. This article introduces Hidden Markov Models, a powerful probabilistic graphical model used in many applications from gesture recognition to natural language processing.\nThere are many tasks for which we do not know the underlying process. However, we can observe samples that are produced from such processes. Music, gesture recognition, speech, text, etc. All of these have some underlying process which forms their outputs together into a hopefully coherent sequence. If we wish to make predictions about future samples given these sequences, we will need to make some guess about the underlying processes defining their output.\nThe Markov Assumption Markov models make a convenient assumption about sequential data. That is, all relevant information required for predicting future samples is captured in the current time step \\(t\\). Given a joint distribution over an input of \\(T\\) frames, \\(p(\\mathbf{x}_{1:T})\\), the Markov assumption allows us to represent it as\n\\[ p(\\mathbf{x}_{1:T}) = p(\\mathbf{x}_1)\\prod_{t=2}^T p(\\mathbf{x}_t|\\mathbf{x}_{t-1}) \\]\nDefinition A more complicated case is when we are attempting to model some unknown process that is responsible for the observations. In this case, an ordinary Markov chain is not sufficient. A hidden Markov model (HMM) is defined by a set \\(z_t \\in \\{1, \\dots, K\\}\\) of discrete hidden states and an observation model \\(p(\\mathbf{x}_i|z_t)\\). The joint probability distribution of this model is given by\n\\[ p(\\mathbf{z}, \\mathbf{x}) = p(\\mathbf{z})p(\\mathbf{x}|\\mathbf{z}) = \\Big(p(z_1)\\prod_{t=2}^Tp(z_t|z_{t-1})\\Big)\\Big(\\prod_{t=1}^Tp(\\mathbf{x}_t|z_t)\\Big). \\]\nFigure 1: The observations y are generated by the latent states x. Source: Wikipedia Although the states themselves are discrete, the observations may be continuous: \\(p(\\mathbf{x}|z_t, \\mathbf{\\theta})\\). If they are discrete, they can be modeled by an observation matrix \\(B\\). Continuous observations are typically modeled using a conditional Gaussian:\n\\[ p(\\mathbf{x}_t|z_t=k, \\theta) = \\mathcal{N}(\\mathbf{x}_t|\\mathbf{\\mu}_k,\\mathbf{\\Sigma}_k). \\]\nFollowing Rabiner, an HMM can be characterized by\nThe number of states in the model \\(N\\). The number of distinct observation symbols per state \\(M\\). The state probability distribution \\(A = \\{a_{ij}\\}\\), \\(a_{ij} = p(z_t=j | z_{t-1} = i)\\). The observation symbol probability distribution \\(B = \\{b_j(k)\\} = p(\\mathbf{x}_t = k|z_t = j)\\). An initial state distribution \\(\\mathbf{\\pi}_i = p(z_t = i)\\). Figure 2: HMM with observation probabilities and state transition probabilities. Source: Wikipedia The observation probability distribution is commonly modeled as a Gaussian, Mixture of Gaussians, or Multinomial distribution. Thus, the parameter estimates for those distributions follow the likelihood estimates for each respective distribution.\nIn his famous tutorial on HMMs, Rabiner addressed the three fundamental problems of HMMs:\nGiven an observation sequence and model parameters, how do we compute the probability of the observation sequence given the parameters (likelihood)? Given an observation sequence and model parameters, how do we choose a state sequence which is optimal (decoding)? How do we adjust the model parameters (learning)? HMMs are able to solve several different inference problems.\nFiltering computes \\(p(z_t | \\mathbf{x}_{1:t})\\). That is, we are computing this probability as new samples come in up to time \\(t\\). Smoothing is accomplished when we have all the data in the sequence. This is expressed as \\(p(z_t|\\mathbf{x}_{1:T})\\). Fixed lag smoothing allows for a trade off between accuracy and delay. It is useful in cases where we might not have the full sequence, but we wish to compute \\(p(z_{t-l}|\\mathbf{x}_{1:t})\\) for some \\(l \u0026gt; 0\\). Predictions are represented as \\(p(z_{t+h}|\\mathbf{x}_{1:t})\\), where \\(h \u0026gt; 0\\). MAP estimation yields the most probably state sequence \\(\\text{arg}\\max_{\\mathbf{z}_{1:T}}p(\\mathbf{z}_{1:T}|\\mathbf{x}_{1:T})\\). We can sample the posterior \\(p(\\mathbf{z}_{1:T}|\\mathbf{x}_{1:T})\\). We can also compute \\(p(\\mathbf{x}_{1:T})\\) by summing up over all hidden paths. This is useful for classification tasks. Evaluation We start by solving the first problem posited by (RABINER 1989).\nGiven an observation sequence and model parameters, how do we compute the probability of the observation sequence given the parameters? That is, given some model parameters \\(\\lambda = (A, B, \\pi)\\), compute \\(p(z_t|\\mathbf{x}_{1:t})\\).\nForwards Pass The forwards algorithm solves two problems of interest. First, we want to know how well our current parameters explain the observation sequence. That is, \\(p(\\mathbf{x}_{1:T}|\\lambda)\\).\nSecond, we want to compute \\(p(z_t | \\mathbf{x}_{1:t})\\). To compute these in an efficient way, a recursive strategy is adopted. Let the forward variable \\(\\alpha_t(i)\\) be defined as\n\\[ \\alpha_t(i) = p(\\mathbf{x}_{1:t}, z_t = i | \\lambda). \\]\nThe forwards algorithm is defined as 3 steps.\nInitialization:\n\\[ \\alpha_1(i) = \\pi_i b_i(\\mathbf{x}_1),\\quad 1 \\leq i \\leq N. \\]\nRecursion:\n\\[ \\alpha_{t+1}(j) = \\Big(\\sum_{i=1}^N \\alpha_t(i)a_{ij}\\Big)b_j(\\mathbf{x}_{t+1}),\\quad 1 \\leq t \\leq T - 1,\\quad 1 \\leq j \\leq N \\]\nTermination:\n\\[ p(\\mathbf{x}_{1:T}) = \\sum_{i=1}^N \\alpha_T(i). \\]\nThe recursive step is visualized as a lattice structure as seen below.\nFigure 3: From Rabiner 1989. With this step, we have a solution for the first problem. We can now calculate more efficiently the probability of our observations given the current model parameters. This along with the following backwards pass will be essential for updating our model parameters.\nThe forwards algorithm is also used to solve the filtering problem. To see how, consider \\(p(z_t | \\mathbf{x}_{1:t-1})\\) right before time \\(t\\).\n\\begin{equation*} p(z_t=j|\\mathbf{x}_{1:t-1}) = \\sum_i p(z_t=j|z_{t-1}=i)p(z_{t-1}=i|\\mathbf{x}_{1:t-1}) \\end{equation*}\nWhen we update for time \\(t\\), we have that\n\\begin{align*} p(z_t=j|\\mathbf{x}_{1:t}) \u0026amp;= p(z_t=j|\\mathbf{x}_t, \\mathbf{x}_{1:t})\\\\ \u0026amp;=\\frac{p(\\mathbf{x}_t|z_t=j, \\mathbf{x}_{1:t-1})p(z_t=j|\\mathbf{x}_{1:t-1})}{p(\\mathbf{x}_t|\\mathbf{x}_{t-1})} \\end{align*}\nHowever, \\(\\mathbf{x}_{1:t-1}\\) is conditionally independent given \\(z_t\\), so it becomes\n\\begin{equation*} p(z_t=j|\\mathbf{x}_{1:t})=\\frac{p(\\mathbf{x}_t|z_t=j)p(z_t=j|\\mathbf{x}_{1:t-1})}{p(\\mathbf{x}_t|\\mathbf{x}_{t-1})}. \\end{equation*}\nWriting out \\(p(z_t=j|\\mathbf{x}_{1:t-1})\\) fully yields\n\\begin{equation*} p(z_t=j|\\mathbf{x}_{1:t}) \\propto p(\\mathbf{x}_t|z_t=j)\\sum_i p(z_t=j|z_{t-1}=i)p(z_{t-1}=i|\\mathbf{x}_{1:t-1}). \\end{equation*}\nThis is the recursion step from above!\nThis can also be represented in terms of the \\(\\alpha\\) variables from above. To compute \\(p(z_t=i|\\mathbf{x}_{1:t})\\), we can use the definition of a conditional probability distribution:\n\\begin{align*} p(z_t=i|\\mathbf{x}_{1:t}) \u0026amp;= \\frac{p(z_t=i, \\mathbf{x}_{1:t})}{p(\\mathbf{x}_{1:t})}\\\\ \u0026amp;= \\frac{\\alpha_t(i)}{\\sum_{j=1}^N \\alpha_t(j)} \\end{align*}\nCompared to the complexity of the explicit representation, the forwards pass needs only \\(N^2T\\) calculations. As pointed out in (RABINER 1989), with 5 hidden states and an observation sequence of length 100, the forwards pass only needs around 3000 computations. A direct calculation would require \\(10^{72}\\).\nBackwards Pass When updating the parameters of our model, we will need to consider the entire observation sequence. The forward pass did not require the entire sequence. Instead, we can compute the probability of the observation up to some time \\(t\\). The backwards pass begins by defining the variable\n\\[ \\beta_t(i) = p(\\mathbf{x}_{t+1:T} | z_t = i). \\]\nWe can utilize a recursive process similar to the forwards algorithm with the following steps:\nInitialization:\n\\[ \\beta_T(i) = 1,\\quad 1 \\leq i \\leq N \\]\nRecursion:\n\\[ \\beta_t(i) = \\sum_{j=1}^N a_{ij}b_j(\\mathbf{x}_{t+1})\\beta_{t+1}(j),\\quad t = T-1,\\dots,1,\\quad 1 \\leq i \\leq N. \\]\nTermination:\n\\[ p(\\mathbf{x}_{1:T}) = \\sum_{j=1}^N \\pi_j b_j(x_1) \\beta_1(j) \\]\nThe complexity of the backwards algorithm is similar to that of the forwards: \\(N^2T\\).\nWith both the forward and backwards passes defined, we can compute the smoothing problem:\n\\[ p(z_t=i|\\mathbf{x}_{1:T}) = \\frac{\\alpha_t(i)\\beta_t(i)}{\\sum_{j=1}^N \\alpha_t(j)\\beta_t(j)} \\]\nThe Viterbi Algorithm With problem 1 out of the way, we turn our attention to problem 2.\nGiven an observation sequence and model parameters, how do we choose a state sequence which is optimal? \\[ \\mathbf{z}^* = \\text{arg}\\max_{\\mathbf{z}_{1:T}}p(\\mathbf{z}_{1:T}|\\mathbf{x}_{1:T}) \\]\nWith respect to the lattice diagram, this is equivalent to computing the shortest path. This is accomplished via the Viterbi algorithm, sometimes referred to as the max-sum algorithm. As with the forwards-backwards algorithm, the Viterbi algorithm takes on a recursive approach. It starts by defining an intermediate variable\n\\[ \\gamma_t(i) = p(z_t=i|\\mathbf{x}_{1:T}). \\]\nUsing the variables defined in the forwards-backwards algorithm, this can be expressed as\n\\[ \\gamma_t(i) = \\frac{\\alpha_t(i) \\beta_t(i)}{\\sum_{i=1}^N \\alpha_t(i) \\beta_t(i)}. \\]\nThis \\(\\gamma_t(i)\\), we can compute the most likely state at time \\(t\\):\n\\[ z_t^* = \\text{arg}\\max_{1\\leq i \\leq N} \\gamma_t(i), \\quad 1 \\leq t \\leq T. \\]\nOne problem with this approach alone is that the most likely state at a particular time \\(t\\) may not lead us to the most probable sequence of states. As stated above, we need to maximize \\(p(\\mathbf{z}_{1:T}|\\mathbf{x}_{1:T})\\). In order to tackle this efficiently, Viterbi employs a dynamic programming approach.\nInitialization\nStart with the best initial state out of all states given the observation at \\(t=1\\). Additionally, we want to record the index of each state through time so that the best path can be retraced.\n\\begin{align*} \\delta_1(i) \u0026amp;= \\pi_i b_i(\\mathbf{x}_1),\\quad 1 \\leq i \\leq N\\\\ \\psi_1(i) \u0026amp;= 0 \\end{align*}\nRecursion:\nThe quantity \\(\\delta_t(i)\\) represents the joint probability of state sequences and observations up to time \\(t\\) ending with state \\(z_t=i\\). Thus, the recursive step is to maximize the probability of the intermediate output for \\(t-1\\):\n\\[ \\delta_t(j) = \\max_{1 \\leq i \\leq N} (\\delta_{t-1}(i) a_{ij})b_j(\\mathbf{x}_t), \\quad 2 \\leq t \\leq T,\\quad 1 \\leq j \\leq N. \\]\nThe corresponding index for this step is recorded in the path matrix:\n\\[ \\psi_t(j) = \\text{arg}\\max_{1 \\leq i \\leq N} \\delta_{t-1}(i)a_{ij},\\quad 2 \\leq t \\leq T,\\quad 1 \\leq j \\leq N. \\]\nTermination\nThe last step of the Viterbi algorithm completes the calcuation of the joint probability of state sequences and observations.\n\\[ p^* = \\max_{1 \\leq i \\leq N} \\delta_T(i) \\]\n\\[ \\mathbf{z}_T^* = \\text{arg}\\max_{1 \\leq i \\leq N} \\delta_T(i) \\]\nPath Backtrace\nWith the state sequence matrix recorded along the way, we can retrace it to get the most probable sequence:\n\\[ z_t^* = \\psi_{t+1}(z_{t+1}^*),\\quad t = T-1, \\cdots, 1. \\]\nEstimating Parameters If the hidden states were fully observable, then updating our model parameters would be as straightforward as computing the maximum likelihood estimates for the model parameters \\(\\lambda = (A, B, \\pi)\\). For \\(A\\) and \\(\\pi\\), we first tally up the following counts:\n\\[ \\hat{a}_{ij} = \\frac{N_{ij}}{\\sum_j N_{ij}}, \\]\nthe number of times we expect to transition from \\(i\\) to \\(j\\) divided by the number of times we transition from \\(i\\) to any other state.\nFor \\(\\pi\\), we have\n\\[ \\hat{\\pi_i} = \\frac{N_i}{\\sum_i N_i}, \\]\nThe number of times we expect to start in state \\(i\\) divided by the number of times we start in any other state.\nEstimating the parameters for \\(B\\) depends on which distribution we are using for our observation probabilities. For a multinomial distribution, we would compute the number of times we are in state \\(j\\) and observe a symbol \\(k\\) divided by the number of times we are in state \\(j\\):\n\\[ \\hat{B}_{jk} = \\frac{N_{jk}}{N_k}, \\]\nwhere\n\\[ N_{jk} = \\sum_{i=1}^N \\sum_{t=1}^T \\mathbb{1} (z_{i, t}=j, x_{i, t}=k). \\]\nIf the observation probability follows a Gaussian distribution, the MLEs for \\(\\mu\\) and \\(\\mathbf{\\Sigma}\\) are\n\\[ \\hat{\\mathbf{\\mu}}_k = \\frac{\\bar{\\mathbf{x}}_k}{N_k},\\quad \\hat{\\mathbf{\\Sigma}}_k = \\frac{(\\bar{\\mathbf{x}}\\bar{\\mathbf{x}})_k^T - N_k \\hat{\\mathbf{\\mu}}_k\\hat{\\mathbf{\\mu}}_k^T}{N_k}, \\]\nwhere\n\\[ \\bar{\\mathbf{x}}_k = \\sum_{i=1}^N \\sum_{t=1}^T \\mathbb{1}(z_{i, t}=k)\\mathbf{x}_{i, t} \\]\nand\n\\[ (\\bar{\\mathbf{x}}\\bar{\\mathbf{x}})_k^T) = \\sum_{i=1}^N \\sum_{t=1}^T \\mathbb{1} (z_{i, t}=k)\\mathbf{x}_{i,k}\\mathbf{x}_{i,k}^T. \\]\nExpectation Maximization Of course, HMMs have hidden states which are not fully observable. Thus, we need to come up with another strategy for updating our parameters based on the observable data. The intuition behind this approach is as follows. We first start out by using our current parameters to estimate the missing data, making it complete. Initially, we may randomize our estimates if we have no good heuristic or guess as to what they should be.\nWith the completed data, we can update our current parameters. In other words, the expected values of the sufficient statistics can be derived now that the data has been filled in. A new set of parameters is found such that it maximizes the likelihood function with respect to the estimated data.\nE Step Following (RABINER 1989), we start with the joint probability of being in state \\(i\\) at time \\(t\\) and state \\(j\\) at time \\(t+1\\):\n\\[ \\xi_t(i, j) = p(z_t = i, z_{t+1} = j|\\mathbf{x}_{1:T}). \\]\nThis can be computed using the forwards-backwards algorithm:\n\\[ \\xi_t(i, j) = \\frac{\\alpha_t(i)a_{ij}b_j(\\mathbf{x}_{t+1})\\beta_{t+1}(j)}{\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_t(i)a_{ij}b_j(\\mathbf{x}_{t+1})\\beta_{t+1}(j)}. \\]\nThis can be related back to \\(\\gamma_t(i)\\) by summing over over \\(j\\):\n\\[ \\gamma_t(i) = \\sum_{j=1}^N \\xi_t(i, j). \\]\nHere, \\(\\gamma_t(i)\\) is the expected number of times we transition from \\(z = i\\). Summing over all \\(t\\) yields the expected transitions from \\(z_i\\) over all time steps:\n\\[ \\sum_{t=1}^{T-1} \\gamma_t(i). \\]\nSince \\(\\xi_t(i, j)\\) is the expected transition from \\(i\\) to \\(j\\) at time \\(t\\), we can compute the total number of transitions from \\(i\\) to \\(j\\) via\n\\[ \\sum_{t=1}^{T-1} \\xi_t(i, j). \\]\nM Step The previous E Step computed the expected values given the current parameter estimates. Now that the data is complete, we can update our parameter estimates. Starting with the transition probabilities, we must add the expected number of transitions from \\(i\\) to \\(j\\) and divide by the expected number of times we transition from \\(i\\). Using the parameters from the E Step, this can be written\n\\[ \\hat{a}_{ij} = \\frac{\\sum_{t=1}^{T-1}\\xi_t(i, j)}{\\sum_{t=1}^{T-1}\\gamma_t(i)}. \\]\nThe initial state probability at \\(t=1\\) is the number of times we expect to be in state \\(z=i\\) at \\(t=1\\):\n\\[ \\gamma_1(i). \\]\nFinally, the observation probability parameters are updated by considering the number of times we are in state \\(z=j\\) and observing \\(x=k\\) divided by the number of times we are in state \\(z=j\\). Note that this is for a multinomial probabiliy distribution:\n\\[ \\hat{b}_j(k) = \\frac{\\sum_{t=1, x_t = k}^T \\gamma_t(j)}{\\sum_{t=1}^T \\gamma_t(j)}. \\]\nThese formulas are derived from maximizing Baum\u0026rsquo;s auxiliary function\n\\[ Q(\\lambda, \\hat{\\lambda}) = \\sum_{Q} p(\\mathbf{z}|\\mathbf{x}, \\lambda) \\log p(\\mathbf{x}, \\mathbf{z}|\\hat{\\lambda}) \\]\nover \\(\\hat{\\lambda}\\). It has further been shown that maximizing this function leads to increased likelihood:\n\\[ \\max_{\\hat{\\lambda}} Q(\\lambda, \\hat{\\lambda}) \\implies p(\\mathbf{x}|\\hat{\\lambda}) \\geq p(\\mathbf{x}|\\lambda). \\]\nIf we have a Gaussian observation model, the values for \\(\\hat{b}_j(k)\\) are computed to accommodate the parameters of the distribution. These parameter estimates assume a Gaussian mixture model. Starting with \\(\\hat{\\mu}_{jk}\\), it can be estimated by dividing the expected value of observations belonging to Gaussian density \\(k\\) by the expected number of times we are in state \\(j\\) using the \\(k^{\\text{th}}\\) mixture component:\n\\[ \\hat{\\mathbf{\\mu}}_{jk} = \\frac{\\sum_{t=1}^T \\gamma_t(j, k)\\mathbf{x}_t}{\\sum_{t=1}^T \\gamma_t(j, k)}. \\]\nHere, \\(\\gamma_t(j, k)\\) is the probability of being in state \\(j\\) at time \\(t\\) with the \\(k^{\\text{th}}\\) mixture component accounting for \\(\\mathbf{x}_t\\):\n\\[ \\gamma_t(j, k) = \\frac{\\alpha_t(j)\\beta_t(j)}{\\sum_{j=1}^N \\alpha_t(j) \\beta_t(j)} \\frac{c_{jk}\\mathcal{N}(\\mathbf{x}_t, \\mu_{jk}, \\mathbf{\\Sigma}_{jk})}{\\sum_{m=1}^M c_{jm}\\mathcal{N}(\\mathbf{x}_t, \\mu_{jm}, \\mathbf{\\Sigma}_{jm})}. \\]\nThis method is proven to improve the parameters.\nEach iteration is guaranteed to improve the log-likelihood function. The process is guaranteed to converge. The convergence point is a fixed point of the likelihood function. These guarantees are similar to gradient ascent.\nReferences RABINER, LAWRENCE R. 1989. “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.” Proceedings of the Ieee 77 (2): 30. ","date":1645509600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689379200,"objectID":"24afc41ee651bea868216ff2c5e1f084","permalink":"https://ajdillhoff.github.io/notes/hidden_markov_models/","publishdate":"2022-02-22T00:00:00-06:00","relpermalink":"/notes/hidden_markov_models/","section":"notes","summary":"Table of Contents Introduction The Markov Assumption Definition Evaluation The Viterbi Algorithm Estimating Parameters Expectation Maximization Introduction This article is essentially a grok of a tutorial on HMMs by (RABINER 1989). It will be useful for the reader to reference the original paper.\nUp to this point, we have only explored \u0026ldquo;atomic\u0026rdquo; data points. That is, all of the information about a particular sample is encapsulated into one vector. Sequential data is easily represented by graphical models.","tags":["machine learning","graphical models"],"title":"Hidden Markov Models","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":"Introduction Let\u0026rsquo;s take a simple constrained problem (from Nocedal and Wright).\n\\begin{align*} \\min \\quad \u0026amp; x_1 + x_2\\\\ \\textrm{s.t.} \\quad \u0026amp; x_1^2 + x_2^2 - 2 = 0 \\end{align*}\nThe set of possible solutions to this problem lie on the boundary of the circle defined by the constraint:\nFigure 1: Source: Nocedal and Wright If we let \\(g(\\mathbf{x}) = x_1^2 + x_2^2 - 2\\), then the gradient vector is \\((2x_1, 2x_2)\\)\nOur original function \\(f(\\mathbf{x}) = x_1 + x_2\\) has a gradient vector of \\((1, 1)\\).\nThe figure above visualizes these vectors at different points on the constraint boundary.\nNotice that the optimal solution \\(\\mathbf{x}^* = (-1, -1)\\) is at a point where \\(\\nabla g(\\mathbf{x}^*)\\) is parallel to \\(\\nabla f(\\mathbf{x}^*)\\). However, the gradients of the vectors are not equal. So there must be some scalar \\(\\lambda\\) such that \\(\\nabla f(\\mathbf{x}^*) = \\lambda \\nabla g(\\mathbf{x}^*)\\).\nThis scalar \\(\\lambda\\) is called a Lagrangian multiplier. We use this and introduce the Lagrangian function:\n\\begin{equation*} \\mathcal{L}(\\mathbf{x}, \\lambda) = f(\\mathbf{x}) - \\lambda g(\\mathbf{x}) \\end{equation*}\nThis yields a form for which we can analytically calculate the stationary points. That is,\n\\begin{equation*} \\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}^*, \\lambda^*) = 0. \\end{equation*}\nLagrangian Duality In general, the primal optimization problem is formulated as\n\\begin{align*} \\min_{w} \\quad \u0026amp; f(w)\\\\ \\textrm{s.t.} \\quad \u0026amp; g_i(w) \\leq 0, \\quad i = 1, \\dots, k\\\\ \u0026amp; h_i(w) = 0, \\quad i = 1, \\dots, l. \\end{align*}\nThe Lagrangian function is then\n\\[ L(w, \\alpha, \\beta) = f(w) + \\sum_{i=1}^k\\alpha_i g_i(w) + \\sum_{i=1}^l \\beta_i h_i(w). \\]\nAdditional Resources https://cs229.stanford.edu/notes2021fall/cs229-notes3.pdf ","date":1644040800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644040800,"objectID":"07ec05c44fcee043ca37fbb243255816","permalink":"https://ajdillhoff.github.io/notes/lagrangian_multipliers/","publishdate":"2022-02-05T00:00:00-06:00","relpermalink":"/notes/lagrangian_multipliers/","section":"notes","summary":"Introduction Let\u0026rsquo;s take a simple constrained problem (from Nocedal and Wright).\n\\begin{align*} \\min \\quad \u0026amp; x_1 + x_2\\\\ \\textrm{s.t.} \\quad \u0026amp; x_1^2 + x_2^2 - 2 = 0 \\end{align*}\nThe set of possible solutions to this problem lie on the boundary of the circle defined by the constraint:\nFigure 1: Source: Nocedal and Wright If we let \\(g(\\mathbf{x}) = x_1^2 + x_2^2 - 2\\), then the gradient vector is \\((2x_1, 2x_2)\\)","tags":["optimization"],"title":"Lagrangian Multipliers","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Dual Representation Relating Back to the Original Formulation Types of Kernels Constructing Kernels Introduction Notebook link: https://github.com/ajdillhoff/CSE6363/blob/main/svm/kernels.ipynb\nParametric models use training data to estimate a set of parameters that can then be used to perform inference on new data. An alternative approach uses nonparametric methods, meaning the function is estimated directly from the data instead of optimizing a set of parameters.\nOne possible downside to such an approach is that it becomes less efficient as the amount of training data increases. Additionally, the transformation into a feature space such that the data becomes linearly separable may be intractable. Consider sequential data such as text or audio. If each sample has a variable number of features, how do we account for this using standard linear models with a fixed number of parameters?\nThe situations described above can be overcome through the use of the kernel trick. We will see that, by computing a measure of similarity between samples in the feature space, we do not need to directly transform each individual sample to that space.\nA kernel function is defined as\n\\[ k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = \\phi(\\mathbf{x})^T \\phi(\\mathbf{x}\u0026rsquo;), \\]\nwhere \\(\\phi\\) is some function which transforms the input to a feature space.\nMethods that require part or all of the training data to make prediction will benefit from using kernel representations, especially when using high dimensional data. Instead of transforming the data into a high dimensional space which may be computationally intractable, a measure of similarity via the inner product is used. The inner product is not the projection into some space. Instead, it represents the outcome of that projection.\nIf the input vector takes on the form of scalar products, it can be represented as a kernel function.\nDual Representation The key to taking advantage of the kernel trick relies on reformulating our linear model into a dual representation. In this form, we will establish a dependence on the kernel function.\nThe following derivation of the dual representation for linear regression follows (Bishop). Consider the least squares loss with \\(L2\\) regularization, as we discussed with Linear Regression.\n\\[ J(\\mathbf{w}) = \\frac{1}{2}\\sum_{i=1}^n(\\mathbf{w}^T\\phi(\\mathbf{x}_i) - y_i)^2 + \\frac{\\lambda}{2} \\mathbf{w}^T \\mathbf{w} \\]\nHere, \\(\\phi\\) is a basis function that transforms the input. This could also be a simple identity function in which \\(\\phi(\\mathbf{x}) = \\mathbf{x}\\). To solve for \\(\\mathbf{w}\\), we take the gradient of \\(J(\\mathbf{w})\\) with respect to \\(\\mathbf{w}\\) and set it to 0.\n\\begin{align*} \\nabla_{\\mathbf{w}}J(\\mathbf{w}) \u0026amp;= \\sum_{i=1}^n(\\mathbf{w}^T\\phi(\\mathbf{x}_i) - y_i)\\phi(\\mathbf{x}_i) + \\lambda \\mathbf{w}\\\\ \\implies \\mathbf{w} \u0026amp;= -\\frac{1}{\\lambda}\\sum_{i=1}^n(\\mathbf{w}^T\\phi(\\mathbf{x}_i) - y_i)\\phi(\\mathbf{x}_i) \\end{align*}\nWe can formulate this as a matrix-vector product by letting\n\\begin{equation*} \\mathbf{\\Phi} = \\begin{bmatrix} \\phi(\\mathbf{x}_1)^T\\\\ \\vdots \\\\ \\phi(\\mathbf{x}_n)^T\\\\ \\end{bmatrix} \\text{ and } a_{i} = -\\frac{1}{\\lambda}(\\mathbf{w}^T\\phi(\\mathbf{x}_i) - y_i). \\end{equation*}\nThen, \\(\\mathbf{w} = \\mathbf{\\Phi}^T\\mathbf{a}\\), where \\(\\mathbf{a} = [a_1, \\dots, a_n]^T\\).\nThe dual representation is derived by reformulating \\(J(\\mathbf{w})\\) in terms of \\(\\mathbf{a}\\).\n\\begin{equation*} J(\\mathbf{a}) = \\frac{1}{2}\\mathbf{a}^T\\mathbf{\\Phi}\\mathbf{\\Phi}^T\\mathbf{\\Phi}\\mathbf{\\Phi}^T\\mathbf{a} - \\mathbf{a}^T\\mathbf{\\Phi}\\mathbf{\\Phi}^T\\mathbf{y} + \\frac{1}{2}\\mathbf{y}^T\\mathbf{y} + \\frac{\\lambda}{2} \\mathbf{a}^T\\mathbf{\\Phi}\\mathbf{\\Phi}^T\\mathbf{a}, \\end{equation*}\nwhere \\(\\mathbf{y} = [y_1, \\dots, y_n]\\).\nLooking at the products \\(\\mathbf{\\Phi}\\mathbf{\\Phi}^T\\), we see that these relate to our original kernel form: \\(\\phi(\\mathbf{x}_i)^T\\phi(\\mathbf{x}_j)\\). This product defines a Gram matrix \\(\\mathbf{K} = \\mathbf{\\Phi}\\mathbf{\\Phi}^T\\) whose elements are \\(k(\\mathbf{x}_i, \\mathbf{x}_j)\\). Thus, we can rewrite \\(J(\\mathbf{a})\\) as\n\\begin{equation*} J(\\mathbf{a}) = \\frac{1}{2}\\mathbf{a}^T\\mathbf{K}\\mathbf{K}\\mathbf{a} - \\mathbf{a}^T\\mathbf{K}\\mathbf{y} + \\frac{1}{2}\\mathbf{y}^T\\mathbf{y} + \\frac{\\lambda}{2}\\mathbf{a}^T\\mathbf{K}\\mathbf{a}. \\end{equation*}\nSolving for \\(\\mathbf{a}\\) can be done by computing the gradient of \\(J(\\mathbf{a})\\) with respect to \\(\\mathbf{a}\\) and setting the result to 0.\n\\begin{align*} \\nabla_\\mathbf{a}J(\\mathbf{a}) = \\mathbf{K}\\mathbf{K}\\mathbf{a} - \\mathbf{K}\\mathbf{y} + \\lambda \\mathbf{K}\\mathbf{a} \u0026amp;= 0\\\\ \\mathbf{K}\\mathbf{a} + \\lambda I\\mathbf{a} - \\mathbf{y} \u0026amp;= 0\\\\ (\\mathbf{K} + \\lambda I)\\mathbf{a} \u0026amp;= \\mathbf{y}\\\\ \\mathbf{a} \u0026amp;= (\\mathbf{K} + \\lambda I)^{-1} \\mathbf{y}. \\end{align*}\nWith \\(\\mathbf{a}\\) solved, we can complete the dual representation of our original linear regression model. Recall that\n\\begin{equation*} h(\\mathbf{x}; \\mathbf{w}) = \\mathbf{w}^T\\phi(\\mathbf{x}). \\end{equation*}\nIf we substitute \\(\\mathbf{w} = \\mathbf{\\Phi}^T\\mathbf{a}\\), we get\n\\begin{align*} f(\\mathbf{x};\\mathbf{a}) \u0026amp;= \\mathbf{a}^T\\mathbf{\\Phi}\\phi(\\mathbf{x})\\\\ \u0026amp;= \\Big[(\\mathbf{K} + \\lambda I)^{-1}\\mathbf{y})\\Big]^T\\mathbf{\\Phi}\\phi(\\mathbf{x}). \\end{align*}\nAgain, the kernel form is apparent in the product \\(\\mathbf{\\Phi}\\phi(\\mathbf{x})\\). If we let \\(k_i(\\mathbf{x}) = k(\\mathbf{x}_i,\\mathbf{x})\\) and\n\\begin{equation*} \\mathbf{k}(\\mathbf{x}) = \\begin{bmatrix} k_1(\\mathbf{x})\\\\ \\vdots \\\\ k_n(\\mathbf{x}) \\end{bmatrix}, \\end{equation*}\nwe can write the dual representation of our linear regression model as\n\\begin{equation*} f(\\mathbf{x}) = \\mathbf{k}(\\mathbf{x})^T(\\mathbf{K} + \\lambda \\mathbf{I})^{-1}\\mathbf{y}. \\end{equation*}\nRelating Back to the Original Formulation In this dual formulation, the solution for \\(\\mathbf{a}\\) can be expressed as a linear combination of elements \\(\\phi(\\mathbf{x})\\). From above, we see that\n\\[ a_i = -\\frac{1}{\\lambda}\\big(\\mathbf{w}^T\\phi(\\mathbf{x}_i) - y_i\\big). \\]\nExpanding this into individual coefficients yields\n\\begin{align*} a_i \u0026amp;= -\\frac{1}{\\lambda}\\big(w_1\\phi_1(\\mathbf{x}_i) + \\cdots + w_m \\phi_m(\\mathbf{x}_i) - y_i\\big)\\\\ \u0026amp;= -\\frac{w_1}{\\lambda}\\phi_1(\\mathbf{x}_i) - \\cdots - \\frac{w_m}{\\lambda} \\phi_m(\\mathbf{x}_i) + \\frac{y_i}{\\lambda}. \\end{align*}\nWe are close, but we still need to do something about the term \\(\\frac{y_i}{\\lambda}\\). For this, we can multiply both sides of our equation by a convenient 1. That is, we multiply by\n\\[ \\frac{\\phi_1(\\mathbf{x}_i) + \\cdots + \\phi_m(\\mathbf{x}_i)}{\\phi_1(\\mathbf{x}_i) + \\cdots + \\phi_m(\\mathbf{x}_i)}. \\]\nBy doing this and grouping the \\(\\phi_j\\) terms, we get\n\\begin{align*} \u0026amp;\\Big(\\frac{y_i}{\\lambda}\\cdot \\frac{1}{\\phi_1(\\mathbf{x}_i) + \\cdots + \\phi_m(\\mathbf{x}_i)} - \\frac{w_1}{\\lambda}\\Big)\\phi_1(\\mathbf{x}_i) + \\cdots\\\\ \u0026amp;+ \\Big(\\frac{y_i}{\\lambda}\\cdot \\frac{1}{\\phi_1(\\mathbf{x}_i) + \\cdots + \\phi_m(\\mathbf{x}_i)} - \\frac{w_m}{\\lambda}\\Big)\\phi_m(\\mathbf{x}_i). \\end{align*}\nWe can simplify this by introducing a term\n\\[ c_i = \\frac{y_i}{\\lambda}\\cdot \\frac{1}{\\phi_1(\\mathbf{x}_i) + \\cdots + \\phi_m(\\mathbf{x}_i)}. \\]\nThen the solution can be rewritten as\n\\[ \\Big(c_i - \\frac{w_1}{\\lambda}\\Big)\\phi_1(\\mathbf{x}_i) + \\cdots + \\Big(c_i - \\frac{w_m}{\\lambda}\\Big)\\phi_m(\\mathbf{x}_i). \\]\nWith this, we can step backwards using intermediate results in the previous section to get back to the original formulation of our linear regression model.\nTypes of Kernels There are several types of kernels that can be used to transform the input data depending on the problem. The simplest kernel is the identity kernel:\n\\[ k(\\mathbf{x}, \\mathbf{x\u0026rsquo;}) = \\mathbf{x}^T \\mathbf{x\u0026rsquo;}. \\]\nPolynomial Kernel A polynomial kernel is defined as\n\\[ k(\\mathbf{x}, \\mathbf{x\u0026rsquo;}) = (\\mathbf{x}^T\\mathbf{x\u0026rsquo;}+c)^d. \\]\nThis is a common choice for solving problems akin to polynomial regression. We can use this kernel to present a visual explanation of kernel functions. Consider the following dataset.\nFigure 1: Binary classification dataset that is not linearly separable. It is easy enough to see that this dataset could not be separated using a hyperplane in 2D. We could separate the two using some nonlinear decision boundary like a circle. If we could transform this into 3D space, we could come up with some features such that it is linearly separable in 3D. For example, let \\(\\phi(\\mathbf{x}) = (x_1^2, x_2^2, \\sqrt{2}x_1x_2)\\).\nTransforming all points and visualizing yields the figure below.\nFigure 2: Binary classification dataset transformed into a 3D feature space. From this perspective, we can clearly see that the data is linearly separable. The question remains: if we only have the original 2D features, how do we compare points in this 3D features space without explicitly transforming each point? The kernel function corresponding to the feature transform above is\n\\begin{align*} k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) \u0026amp;= (\\mathbf{x}^T\\mathbf{x}\u0026rsquo;)^2\\\\ \u0026amp;= (x_1x\u0026rsquo;_1 + x_2x\u0026rsquo;_2)^2\\\\ \u0026amp;= 2x_1x\u0026rsquo;_1x_2x\u0026rsquo;_2 + (x_1x\u0026rsquo;_1)^2 + (x_2x\u0026rsquo;_2)^2\\\\ \u0026amp;= \\phi(\\mathbf{x})^T \\phi(\\mathbf{x}\u0026rsquo;) \\end{align*}\nwhere\n\\[ \\phi(\\mathbf{x}) = \\begin{bmatrix} \\sqrt{2}x_1x_2\\\\ x_1^2\\\\ x_2^2 \\end{bmatrix}. \\]\nRadial Basis Function Kernel This kernel follows a Gaussian term and is commonly used with SVMs. It is defined as\n\\[ k(\\mathbf{x}, \\mathbf{x\u0026rsquo;}) = \\exp\\Big(-\\frac{\\|\\mathbf{x}-\\mathbf{x\u0026rsquo;}\\|^2}{2\\sigma^2}\\Big). \\]\nCosine Similarity Consider the problem of comparing text sequences for a document classification task. One approach is to compare the number of occurrences of each word. The idea is that documents that are similar will have a similar number of words that occur.\n\\[ k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = \\frac{\\mathbf{x}^T \\mathbf{x}\u0026rsquo;}{\\|\\mathbf{x}\\|_2 \\|\\mathbf{x}\u0026rsquo;\\|_2} \\]\nDocuments that are orthogonal, in the sense that the resulting cosine similarity is 0, are dissimilar. The similarity increases as the score approaches 1. There are several issues with this approach which are addressed by using the term frequence-inverse document frequency (TF-IDF) score.\nConstructing Kernels A valid kernel function must satisfy the following conditions:\nSymmetry: \\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = k(\\mathbf{x}\u0026rsquo;, \\mathbf{x})\\) Positive semi-definite: \\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) \\geq 0\\) If the feature space can be represented as a dot product, then it will satisfy the first condition by definition. The second condition can be shown by constructing a Gram matrix \\(\\mathbf{K}\\) and showing that it is positive semi-definite. A matrix \\(\\mathbf{K}\\) is positive semi-definite if and only if \\(\\mathbf{v}^T\\mathbf{K}\\mathbf{v} \\geq 0\\) for all \\(\\mathbf{v} \\in \\mathbb{R}^n\\).\nDirect Construction of a Kernel In this approach, we define a feature space \\(\\phi(\\mathbf{x})\\) and then compute the kernel function as\n\\[ k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = \\phi(\\mathbf{x})^T \\phi(\\mathbf{x}\u0026rsquo;). \\]\nThis is the approach used in the example from above. In that example, we used the kernel function \\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = (\\mathbf{x}^T\\mathbf{x}\u0026rsquo;)^2\\). For our 2D input, the feature space is \\(\\phi(\\mathbf{x}) = (x_1^2, x_2^2, \\sqrt{2}x_1x_2)\\). It is easy to see that the kernel function is the dot product of the feature space, and its kernel matrix is positive semi-definite.\nConstruction from other valid kernels As a more convenient approach, it is possible to construct complex kernels from known kernels. Given valid kernels \\(k_1(\\mathbf{x}, \\mathbf{x}\u0026rsquo;)\\) and \\(k_2(\\mathbf{x}, \\mathbf{x}\u0026rsquo;)\\), we can construct a new kernel \\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;)\\) using the following operations:\n\\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = ck_1(\\mathbf{x}, \\mathbf{x}\u0026rsquo;)\\) for \\(c \u0026gt; 0\\) \\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = f(\\mathbf{x})k_1(\\mathbf{x}, \\mathbf{x}\u0026rsquo;)f(\\mathbf{x}\u0026rsquo;)\\) for \\(f(\\mathbf{x})\\) \\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = k_1(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) + k_2(\\mathbf{x}, \\mathbf{x}\u0026rsquo;)\\) \\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = k_1(\\mathbf{x}, \\mathbf{x}\u0026rsquo;)k_2(\\mathbf{x}, \\mathbf{x}\u0026rsquo;)\\) \\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = \\exp(k_1(\\mathbf{x}, \\mathbf{x}\u0026rsquo;))\\) \\(k(\\mathbf{x}, \\mathbf{x}\u0026rsquo;) = \\tanh(k_1(\\mathbf{x}, \\mathbf{x}\u0026rsquo;))\\) ","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694044800,"objectID":"83e1acfd29cf0148a76ae1f5d8926675","permalink":"https://ajdillhoff.github.io/notes/kernels/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/kernels/","section":"notes","summary":"Table of Contents Introduction Dual Representation Relating Back to the Original Formulation Types of Kernels Constructing Kernels Introduction Notebook link: https://github.com/ajdillhoff/CSE6363/blob/main/svm/kernels.ipynb\nParametric models use training data to estimate a set of parameters that can then be used to perform inference on new data. An alternative approach uses nonparametric methods, meaning the function is estimated directly from the data instead of optimizing a set of parameters.\nOne possible downside to such an approach is that it becomes less efficient as the amount of training data increases.","tags":["machine learning"],"title":"Kernels","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Gaussian Class Conditional Densities Maximum Likelihood Estimation Quadratic Descriminant Analysis Example Introduction This section covers classification from a probabilistic perspective. The discriminative approach involves a parameterized function which assigns each input vector \\(\\mathbf{x}\\) to a specific class. We will see that modeling the conditional probability distribution \\(p(C_k|\\mathbf{x})\\) grants us additional benefits while still fulfilling our original classification task.\nLet\u0026rsquo;s begin with a 2 class problem. To classify this with a generative model, we use the class-conditional densities \\(p(\\mathbf{x}|C_i)\\) and class priors \\(p(C_i)\\). The posterior probability for \\(C_1\\) can be written in the form of a sigmoid function:\n\\begin{align*} p(C_1|\\mathbf{x}) \u0026amp;= \\frac{p(\\mathbf{x}|C_1)p(C_1)}{p(\\mathbf{x}|C_1)p(C_1) + p(\\mathbf{x}|C_2)p(C_2)} \\end{align*}\nThen multiply the numerator and denominator by\n\\begin{equation*} \\frac{(p(\\mathbf{x}|C_1))^{-1}}{(p(\\mathbf{x}|C_1))^{-1}}, \\end{equation*}\nwhich yields\n\\begin{equation*} \\frac{1}{1 + \\frac{p(\\mathbf{x}|C_2)p(C_2)}{p(\\mathbf{x}|C_1)p(C_1)}}. \\end{equation*}\nNoting that \\(a = \\exp(\\ln(a))\\), we can rewrite further\n\\begin{equation*} \\frac{1}{1 + \\exp(-a)}, \\end{equation*}\nwhere \\(a = \\ln \\frac{p(\\mathbf{x}|C_1)p(C_1)}{p(\\mathbf{x}|C_2)p(C_2)}\\).\nWriting this distribution in the form of the sigmoid function is convenient as it is a natural choice for many other classification models. It also has a very simple derivative which is convenient for models optimized using gradient descent.\nGiven certain choices for the class conditional densities, the posterior probabilty distribution will be a linear function of the input features:\n\\begin{equation*} \\ln p(C_k|\\mathbf{x};\\theta) = \\mathbf{w}^T \\mathbf{x} + c, \\end{equation*}\nwhere \\(\\mathbf{w}\\) is a parameter vector based on the parameters of the chosen probability distribution, and \\(c\\) is a constant term that is not dependent on the parameters. As we will see, the resulting model will take an equivalent form to the discriminative approach.\nGaussian Class Conditional Densities Let\u0026rsquo;s assume that our class conditional densities \\(p(\\mathbf{x}|C_k)\\) are Gaussian. We will additionally assume that the covariance matrices between classes are shared. This will result in linear decision boundaries. Since the conditional densities are chosen to be Gaussian, the posterior is given by\n\\begin{equation*} p(C_k|\\mathbf{x};\\theta) \\propto \\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_c,\\Sigma), \\end{equation*}\nwhere \\(\\pi_k\\) is the prior probability of class \\(k\\). We choose to ignore the normalizing constant since it is not dependent on the class.\nThe class conditional density function for class \\(k\\) is given by\n\\begin{equation*} p(\\mathbf{x}|C_k;\\theta) = \\frac{1}{2\\pi^{D/2}}\\frac{1}{|\\Sigma|^{1/2}}\\exp\\Big(-\\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu}_k)^T \\Sigma^{-1} (\\mathbf{x} - \\mathbf{\\mu}_k)\\Big). \\end{equation*}\nNow that we have a concrete function to work with, let\u0026rsquo;s go back to the simple case of two classes and define \\(a = \\ln \\frac{p(\\mathbf{x}|C_1)p(C_1)}{p(\\mathbf{x}|C_2)p(C_2)}\\). First, we rewrite \\(a\\):\n\\begin{equation*} a = \\ln p(\\mathbf{x}|C_1) - \\ln p(\\mathbf{x}|C_2) + \\ln \\frac{p(C_1)}{p(C_2)}. \\end{equation*}\nThe log of the class conditional density for a Gaussian is\n\\begin{equation*} \\ln p(\\mathbf{x}|C_k;\\mathbf{\\mu}_k,\\Sigma) = -\\frac{D}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\Sigma|-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu}_k)^T \\Sigma^{-1} (\\mathbf{x}-\\mathbf{\\mu}_k). \\end{equation*}\nTo simplify the above result, we will group the terms that are not dependent on the class parameters since they are consant:\n\\begin{equation*} \\ln p(\\mathbf{x}|C_k;\\mathbf{\\mu}_k,\\Sigma) = -\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu}_k)^T \\Sigma^{-1} (\\mathbf{x}-\\mathbf{\\mu}_k) + c. \\end{equation*}\nObserving that this quantity takes on a quadratic form, we can rewrite the above as\n\\begin{equation*} \\ln p(\\mathbf{x}|C_k;\\mathbf{\\mu}_k,\\Sigma) = -\\frac{1}{2}\\mathbf{\\mu}_k\\Sigma^{-1}\\mathbf{\\mu}_k + \\mathbf{x}^T \\Sigma^{-1} \\mathbf{\\mu}_k -\\frac{1}{2}\\mathbf{x}^T \\Sigma^{-1}\\mathbf{x} + c. \\end{equation*}\nUsing this, we complete the definition of \\(a\\):\n\\begin{align*} a \u0026amp;= \\ln p(\\mathbf{x}|C_1) - \\ln p(\\mathbf{x}|C_2) + \\ln \\frac{p(C_1)}{p(C_2)}\\\\ \u0026amp;= -\\frac{1}{2}\\mathbf{\\mu}_1\\Sigma^{-1}\\mathbf{\\mu}_1 + \\mathbf{x}^T \\Sigma^{-1} \\mathbf{\\mu}_1 + \\frac{1}{2}\\mathbf{\\mu}_2\\Sigma^{-1}\\mathbf{\\mu}_2 - \\mathbf{x}^T \\Sigma^{-1} \\mathbf{\\mu}_2 + \\ln \\frac{p(C_1)}{p(C_2)}\\\\ \u0026amp;= \\mathbf{x}^T(\\Sigma^{-1}(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)) - \\frac{1}{2}\\mathbf{\\mu}_1\\Sigma^{-1}\\mathbf{\\mu}_1 + \\frac{1}{2}\\mathbf{\\mu}_2\\Sigma^{-1}\\mathbf{\\mu}_2 + \\ln \\frac{p(C_1)}{p(C_2)}\\\\ \u0026amp;= (\\Sigma^{-1}(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2))^T \\mathbf{x} - \\frac{1}{2}\\mathbf{\\mu}_1\\Sigma^{-1}\\mathbf{\\mu}_1 + \\frac{1}{2}\\mathbf{\\mu}_2\\Sigma^{-1}\\mathbf{\\mu}_2 + \\ln \\frac{p(C_1)}{p(C_2)}. \\end{align*}\nFinally, we define\n\\begin{equation*} \\mathbf{w} = \\Sigma^{-1}(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2) \\end{equation*}\nand\n\\begin{equation*} w_0 = - \\frac{1}{2}\\mathbf{\\mu}_1\\Sigma^{-1}\\mathbf{\\mu}_1 - \\frac{1}{2}\\mathbf{\\mu}_2\\Sigma^{-1}\\mathbf{\\mu}_2 + \\ln \\frac{p(C_1)}{p(C_2)}. \\end{equation*}\nThus, our posterior takes on the form\n\\begin{equation*} p(C_1|\\mathbf{x};\\theta) = \\sigma(\\mathbf{w}^T \\mathbf{x} + w_0). \\end{equation*}\nMultiple Classes What if we have more than 2 classes? Recall that a generative classifier is modeled as\n\\[ p(C_k|\\mathbf{x};\\mathbf{\\theta}) = \\frac{p(C_k|\\mathbf{\\theta})p(\\mathbf{x}|C_k, \\mathbf{\\theta})}{\\sum_{k\u0026rsquo;}p(C_{k\u0026rsquo;}|\\mathbf{\\theta})p(\\mathbf{x}|C_{k\u0026rsquo;}, \\mathbf{\\theta})}. \\]\nAs stated above, \\(\\mathbf{\\pi}_k = p(C_k|\\mathbf{\\theta})\\) and \\(p(\\mathbf{x}|C_k,\\mathbf{\\theta}) = \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_c,\\Sigma)\\).\nFor LDA, the covariance matrices are shared across all classes. This permits a simplification of the class posterior distribution \\(p(C_k|\\mathbf{x};\\mathbf{\\theta})\\):\n\\begin{align*} p(C_k|\\mathbf{x};\\mathbf{\\theta}) \u0026amp;\\propto \\mathbf{\\pi}_k \\exp\\big(\\mathbf{\\mu}_k^T \\mathbf{\\Sigma}^{-1}\\mathbf{x} - \\frac{1}{2}\\mathbf{x}^T\\mathbf{\\Sigma}^{-1}\\mathbf{x} - \\frac{1}{2}\\mathbf{\\mu}_k\\mathbf{\\Sigma}^{-1}\\mathbf{\\mu}_k\\big)\\\\ \u0026amp;= \\exp\\big(\\mathbf{\\mu}_k^T \\mathbf{\\Sigma}^{-1}\\mathbf{x} - \\frac{1}{2}\\mathbf{\\mu}_k\\mathbf{\\Sigma}^{-1}\\mathbf{\\mu}_k + \\log \\mathbf{\\pi}_k \\big) \\exp\\big(- \\frac{1}{2}\\mathbf{x}^T\\mathbf{\\Sigma}^{-1}\\mathbf{x}\\big). \\end{align*}\nThe term \\(\\exp\\big(- \\frac{1}{2}\\mathbf{x}^T\\mathbf{\\Sigma}^{-1}\\mathbf{x}\\big)\\) is placed aside since it is not dependent on the class \\(k\\). When divided by the sum per the definition of \\(p(C_k|\\mathbf{x};\\mathbf{\\theta})\\), it will equal to 1.\nUnder this formulation, we let\n\\begin{align*} \\mathbf{w}_k \u0026amp;= \\mathbf{\\Sigma}^{-1}\\mathbf{\\mu}_k\\\\ \\mathbf{b}_k \u0026amp;= -\\frac{1}{2}\\mathbf{\\mu}_k^T \\mathbf{\\Sigma}^{-1}\\mathbf{\\mu}_k + \\log \\mathbf{\\pi}_k. \\end{align*}\nThis lets us express \\(p(C_k|\\mathbf{x};\\mathbf{\\theta})\\) as the softmax function:\n\\(p(C_k|\\mathbf{x};\\mathbf{\\theta}) = \\frac{\\exp(\\mathbf{w}_k^T\\mathbf{x}+\\mathbf{b}_k)}{\\sum_{k\u0026rsquo;}\\exp(\\mathbf{w}_{k\u0026rsquo;}^T\\mathbf{x}+\\mathbf{b}_{k\u0026rsquo;})}\\).\nMaximum Likelihood Estimation Given our formulation in the previous section, we can estimate the parameters of the model via maximum likelihood estimation. Assuming \\(K\\) classes with Gaussian class conditional densities, the likelihood function is\n\\begin{equation*} p(\\mathbf{X}|\\mathbf{\\theta}) = \\prod_{i=1}^n \\mathcal{M}(y_i|\\mathbf{\\pi})\\prod_{k=1}^K \\mathcal{N}(\\mathbf{x}_i|\\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k)^{\\mathbb{1}(y_i=k)}. \\end{equation*}\nTaking the log of this function yields\n\\begin{equation*} \\ln p(\\mathbf{X}|\\mathbf{\\theta}) = \\Big[\\sum_{i=1}^n \\sum_{k=1}^K \\mathbb{1}(y_i=k)\\ln \\pi_k\\Big] + \\sum_{k=1}^K\\Big[\\sum_{i:y_i=c} \\ln \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k)\\Big]. \\end{equation*}\nGiven that this is a sum of two different components, we can optimize the multinomial parameter \\(\\mathbf{\\pi}\\) and the class Gaussian parameters \\((\\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k)\\) separately.\nClass Prior For multinomial distributions, the class prior parameter estimation \\(\\hat{\\pi}_k\\) is easily calculated by counting the number of samples belonging to class \\(k\\) and dividing it by the total number of samples.\n\\[ \\hat{\\pi}_k = \\frac{n_k}{n} \\]\nClass Gaussians The Gaussian parameters can be calculated as discussed during the probability review. The parameter estimates are\n\\begin{align*} \\hat{\\mathbf{u}}_k \u0026amp;= \\frac{1}{n_k}\\sum_{i:y_i=k}\\mathbf{x}_i\\\\ \\hat{\\Sigma}_k \u0026amp;= \\frac{1}{n_k}\\sum_{i:y_i=k}(\\mathbf{x}_i - \\hat{\\mathbf{\\mu}}_k)(\\mathbf{x}_i - \\hat{\\mathbf{\\mu}}_k)^T \\end{align*}\nThe Decision Boundary The decision boundary between two classes can be visualized at the point when \\(p(C_k|\\mathbf{x};\\theta) = 0.5\\).\nQuadratic Descriminant Analysis Linear Discriminant Analysis is a special case of Quadratic Discriminant Analysis (QDA) where the covariance matrices are shared across all classes. Assuming each class conditional density is Gaussian, the posterior probability is given by\n\\begin{equation*} p(C_k|\\mathbf{x};\\theta) \\propto \\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k). \\end{equation*}\nTaking the log of this function yields\n\\begin{equation*} \\ln p(C_k|\\mathbf{x};\\theta) = \\ln \\pi_k - \\frac{1}{2}\\ln |\\Sigma_k| - \\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu}_k)^T \\Sigma_k^{-1}(\\mathbf{x} - \\mathbf{\\mu}_k) + c. \\end{equation*}\nWith LDA, the term \\(\\frac{1}{2}\\ln |\\Sigma_k|\\) is constant across all classes, so we treat it as another constant. Since QDA considers a different covariance matrix for each class, we must keep this term in the equation.\nQuadratic Decision Boundary In the more general case of QDA, the decision boundary is quadratic, leading to a quadratic discriminant function. As shown above, the posterior probability function for LDA is linear in \\(\\mathbf{x}\\), which leads to a linear discriminant function.\nExample See here for an example using scikit-learn.\n","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693440000,"objectID":"7b3b17d07ded3a7fc40ad7031aca0872","permalink":"https://ajdillhoff.github.io/notes/linear_discriminant_analysis/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/linear_discriminant_analysis/","section":"notes","summary":"Table of Contents Introduction Gaussian Class Conditional Densities Maximum Likelihood Estimation Quadratic Descriminant Analysis Example Introduction This section covers classification from a probabilistic perspective. The discriminative approach involves a parameterized function which assigns each input vector \\(\\mathbf{x}\\) to a specific class. We will see that modeling the conditional probability distribution \\(p(C_k|\\mathbf{x})\\) grants us additional benefits while still fulfilling our original classification task.\nLet\u0026rsquo;s begin with a 2 class problem. To classify this with a generative model, we use the class-conditional densities \\(p(\\mathbf{x}|C_i)\\) and class priors \\(p(C_i)\\).","tags":null,"title":"Linear Discriminant Analysis","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Picking a Model Binary Classification Multiple Classes Introduction With Linear Regression we were able to fit a model to our data in order to make inferences on unseen data points. In the examples, both the input features and observation were continuous. With logistic regression, we will use similar models to classify the data points based on their input features. We start out with the simplest approach: we assume that the data is linearly separable and can be assigned one of \\(K\\) discrete classes.\nIn the binary case, the target variable will takes on either a 0 or 1. For \\(K \u0026gt; 2\\), we will use a \\(K\\) dimensional vector that has a 1 corresponding to the class encoding for that input and a 0 for all other positions. For example, if our possible target classes were \\(\\{\\text{car, truck, person}\\}\\), then a target vector for \\(\\text{person}\\) would be \\(\\mathbf{y} = [0, 0, 1]^T\\).\nThis article will stick to a discriminative approach to logistic regression. That is, we define a discriminant function which assigns each data input \\(\\mathbf{x}\\) to a class. For a probabilistic perspective, see Linear Discriminant Analysis.\nPicking a Model We will again start with a linear model \\(y = f(\\mathbf{x}; \\mathbf{w})\\). Unlike the model used with Linear Regression, ours will need to predict a discrete class label. The logistic model is often approached by introducing the odds of an event occurring:\n\\[ \\frac{p}{1-p}, \\]\nwhere \\(p\\) is the probability of the event happening. As \\(p\\) increases, the odds of it happening increase exponentially.\nOur input \\(p\\) represents the probability in range \\((0, 1)\\) which we want to map to the real number space. To approximate this, we apply the natural logarithm to the odds.\nThe logistic model assumes a linear relationship between the linear model \\(\\mathbf{w}^T\\mathbf{x}\\) and the logit function\n\\[ \\text{logit}(p) = \\ln \\frac{p}{1-p}. \\]\nThis function maps a value in range \\((0, 1)\\) to the space of real numbers. Under this assumption, we can write\n\\[ \\text{logit}(p) = \\mathbf{w}^T\\mathbf{x}. \\]\nThis assumption is reasonable because we ultimately want to predict the probability that an event occurs. The output should then be in the range of \\((0, 1)\\). If the logit function produces output in the range of real numbers, as does our linear model \\(\\mathbf{w}^T\\mathbf{x}\\), then we ultimately want a function that maps from the range of real numbers to to \\((0, 1)\\).\nWe can achieve this using the inverse of the logit function, the logistic sigmoid function. It is defined as\n\\begin{equation*} \\sigma(z) = \\frac{1}{1 + \\exp(-z)}, \\end{equation*}\nwhere \\(z = \\mathbf{w}^T\\mathbf{x}\\).\nThe reason for this choice becomes more clear when plotting the function, as seen below.\nFigure 1: The logistic sigmoid function. Source: Wikipedia The inputs on the \\(x\\) axis are clamped to values between 0 and 1. It is also called a squashing function because of this property. This form is also convenient and arises naturally in many probabilistic settings. With this nonlinear activation function, the form of our model becomes\n\\begin{equation*} f(\\mathbf{x};\\mathbf{w}) = h(\\mathbf{w}^T\\mathbf{x}), \\end{equation*}\nwhere \\(h\\) is our choice of activation function.\nThe logistic sigmoid function also has a convenient derivative, which is useful when solving for the model parameters via gradient descent.\n\\[ \\frac{d}{dx} = \\sigma(x)(1 - \\sigma(x)) \\]\nBinary Classification Consider a simple dataset with 2 features per data sample. Our goal is to classify the data as being one of two possible classes. For now, we\u0026rsquo;ll drop the activation function so that our model represents a line that separates both groups of data.\nFigure 2: Two groups of data that are very clearly linearly separable. In the binary case, we are approximating \\(p(C_1|\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x})\\). Then \\(p(C_2|\\mathbf{x}) = 1 - p(C_1| \\mathbf{x})\\).\nThe parameter vector \\(\\mathbf{w}\\) is orthogonal to the decision boundary that separates the two classes. The model output is such that \\(f(\\mathbf{x};\\mathbf{w}) = 0\\) when \\(\\mathbf{x}\\) lies on the decision boundary. If \\(f(\\mathbf{x};\\mathbf{w}) \\geq 0\\) then \\(\\mathbf{x}\\) is assigned to class 1. It is assigned to class 2 otherwise. Since we originally stated that the model should predict either a 0 or 1, we can use the model result as input to the Heaviside step function.\nFitting the Model via Maximum Likelihood Let \\(y_i \\in \\{0, 1\\}\\) be the target for binary classification and \\(\\hat{y}_i \\in (0, 1)\\) be the output of a logistic regression model. The likelihood function is\n\\[ p(\\mathbf{y}|\\mathbf{w}) = \\prod_{i=1}^n \\hat{y}_i^{y_i}(1 - \\hat{y}_i)^{1 - y_i}. \\]\nLet\u0026rsquo;s briefly take a look at \\(\\hat{y}_i^{y_i}(1 - \\hat{y}_i)^{1 - y_i}\\) to understand the output when the model correctly predicts the \\(i^{\\text{th}}\\) sample or not. Since the output is restricted within the range \\((0, 1)\\), the model will never produce 0 or 1.\nIf the target \\(y_i = 0\\), then we can evaluate the subexpression \\(1 - \\hat{y}_i\\). In this case, the likelihood increases as \\(\\hat{y}_i\\) decreases.\nIf the target \\(y_i = 1\\), then we evaluate the subexpression \\(\\hat{y}_i\\).\nWhen fitting this model, we want to define an error measure based on the above function. This is done by taking the negative logarithm of \\(p(\\mathbf{y}|\\mathbf{w})\\).\n\\[ E(\\mathbf{w}) = -\\ln p(\\mathbf{y}|\\mathbf{w}) = -\\sum_{i=1}^n y_i \\ln \\hat{y}_i + (1 - y_i) \\ln (1 - \\hat{y}_i) \\]\nThis function is commonly referred to as the cross-entropy function.\nIf we use this as an objective function for gradient descent with the understanding that \\(\\hat{y}_i = \\sigma(\\mathbf{w}^T \\mathbf{x})\\), then the gradient of the error function is\n\\[ \\nabla E(\\mathbf{w}) = \\sum_{i=1}^n (\\hat{y}_i - y_i)\\mathbf{x}_i. \\]\nThis results in a similar update rule as linear regression, even though the problem itself is different.\nMeasuring Classifier Performance How do we determine how well our model is performing?\nWe will use L1 loss because it works well with discrete outputs. L1 loss is defined as\n\\begin{equation*} L_1 = \\sum_{i}|\\hat{y}_i - y_i|, \\end{equation*}\nwhere \\(\\hat{y}_i\\) is the ground truth corresponding to \\(\\mathbf{x}_i\\) and \\(y_i\\) is the output of our model. We can further normalize this loss to bound it between 0 and 1. Either way, a loss of 0 will indicate 100% classification accuracy.\nMultiple Classes In multiclass logistic regression, we are dealing with target values that can take on one of \\(k\\) values \\(y \\in \\{1, 2, \\dots, k\\}\\). If our goal is to model the distribution over \\(K\\) classes, a multinomial distribution is the obvious choice. Let \\(p(y|\\mathbf{x};\\theta)\\) be a distribution over \\(K\\) numbers \\(w_1, \\dots, w_K\\) that sum to 1. Our parameterized model cannot be represented exactly by a multinomial distribution, so we will derive it so that it satisfies the same constraints.\nWe can start by introducing \\(K\\) parameter vectors \\(\\mathbf{w}_1, \\dots, \\mathbf{w}_K \\in \\mathbb{R}^{d}\\), where \\(d\\) is the number of input features. Then each vector \\(\\mathbf{w}_k^T \\mathbf{x}\\) represents \\(p(C_k | \\mathbf{x};\\mathbf{w}_k)\\). We need to squash each \\(\\mathbf{w}_k^T \\mathbf{x}\\) so that the output sums to 1.\nThis is accomplished via the softmax function:\n\\[ p(C_k|\\mathbf{x}) = \\frac{\\exp(\\mathbf{w}_k^T \\mathbf{x})}{\\sum_{j} \\exp(\\mathbf{w}_j^T \\mathbf{x})}. \\]\nMaximum Likelihood The target vector for each sample is \\(\\mathbf{y}_i \\in \\mathbb{R}^{k}\\). Likewise, the output vector \\(\\hat{\\mathbf{y}}_i\\) also has \\(k\\) elements.\nThe maximum likelihood function for the multiclass setting is given by\n\\[ p(\\mathbf{Y}|\\mathbf{W}) = \\prod_{i=1}^n \\prod_{k=1}^K p(C_k|\\mathbf{x}_i)^{y_{ik}} = \\prod_{i=1}^n \\prod_{k=1}^K \\hat{y}_{ik}^{y_{ik}}. \\]\n\\(\\mathbf{Y} \\in \\mathbb{R}^{n \\times K}\\) is a matrix of all target vectors in the data set. As with the binary case, we can take the negative logarithm of this function to produce an error function.\n\\[ E(\\mathbf{W}) = -\\ln p(\\mathbf{Y}|\\mathbf{W}) = -\\sum_{i=1}^n \\sum_{k=1}^K y_{ik} \\ln \\hat{y}_{ik} \\]\nThis is the cross-entropy function for multiclass classification.\nThe gradient of this function is given as\n\\[ \\nabla_{\\mathbf{w}_j}E(\\mathbf{W}) = \\sum_{i=1}^n (\\hat{y}_{ij} - y_{ij}) \\mathbf{x}_i. \\]\nPart of your first assignment will be to work through the derivation of this function. It is standard practice at this point, but it is highly valuable to understand how the result was produced.\n","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642831200,"objectID":"91f03ca8739733727b9c6b475ec22353","permalink":"https://ajdillhoff.github.io/notes/logistic_regression/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/logistic_regression/","section":"notes","summary":"Table of Contents Introduction Picking a Model Binary Classification Multiple Classes Introduction With Linear Regression we were able to fit a model to our data in order to make inferences on unseen data points. In the examples, both the input features and observation were continuous. With logistic regression, we will use similar models to classify the data points based on their input features. We start out with the simplest approach: we assume that the data is linearly separable and can be assigned one of \\(K\\) discrete classes.","tags":null,"title":"Logistic Regression","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Definition Maximum Likelihood Estimation Making a Decision Relation to Multinomial Logistic Regression MNIST Example Gaussian Formulation Introduction To motivate naive Bayes classifiers, let\u0026rsquo;s look at slightly more complex data. The MNIST dataset was one of the standard benchmarks for computer vision classification algorithms for a long time. It remains useful for educational purposes. The dataset consists of 60,000 training images and 10,000 testing images of size \\(28 \\times 28\\). These images depict handwritten digits. For the purposes of this section, we will work with binary version of the images. This implies that each data sample has 784 binary features.\nWe will use the naive Bayes classifier to make an image classification model which predicts the class of digit given a new image. Each image will be represented by a vector \\(\\mathbf{x} \\in \\mathbb{R}^{784}\\). Modeling \\(p(\\mathbf{x}|C_k)\\) with a multinomial distribution would require \\(10^{784} - 1\\) parameters since there are 10 classes and 784 features.\nFigure 1: Samples of the MNIST training dataset. With the naive assumption that the features are independent conditioned on the class, the number model parameters becomes \\(10 \\times 784\\).\nDefinition A naive Bayes classifier makes the assumption that the features of the data are independent. That is, \\[ p(\\mathbf{x}|C_k, \\mathbf{\\theta}) = \\prod_{d=1}^D p(x_i|C_k, \\theta_{dk}), \\] where \\(\\mathbf{\\theta}_{dk}\\) are the parameters for the class conditional density for class \\(k\\) and feature \\(d\\). Using the MNIST dataset, \\(\\mathbf{\\theta}_{dk} \\in \\mathbb{R}^{784}\\). The posterior distribution is then\n\\begin{equation*} p(C_k|\\mathbf{x},\\mathbf{\\theta}) = \\frac{p(C_k|\\mathbf{\\pi})\\prod_{i=1}^Dp(x_i|C_k, \\mathbf{\\theta}_{dk})}{\\sum_{k\u0026rsquo;}p(C_{k\u0026rsquo;}|\\mathbf{\\pi})\\prod_{i=1}^Dp(x_i|C_{k\u0026rsquo;},\\mathbf{\\theta}_{dk\u0026rsquo;})}. \\end{equation*}\nIf we convert the input images to binary, the class conditional density \\(p(\\mathbf{x}|C_k, \\mathbf{\\theta})\\) takes on the Bernoulli pdf. That is,\n\\begin{equation*} p(\\mathbf{x}|C_k, \\mathbf{\\theta}) = \\prod_{i=1}^D\\text{Ber}(x_i|\\mathbf{\\theta}_{dk}). \\end{equation*}\nThe parameter \\(\\theta_{dk}\\) is the probability that the feature \\(x_i=1\\) given class \\(C_k\\).\nMaximum Likelihood Estimation Fitting a naive Bayes classifier is relatively simple using MLE. The likelihood is given by\n\\begin{equation*} p(\\mathbf{X}, \\mathbf{y}|\\mathbf{\\theta}) = \\prod_{n=1}^N \\mathcal{M}(y_n|\\mathbf{\\pi})\\prod_{d=1}^D\\prod_{k=1}^{K}p(x_{nd}|\\mathbf{\\theta}_{dk})^{\\mathbb{1}(y_n=k)}. \\end{equation*}\nTo derive the estimators, we first take the log of the likelihood:\n\\begin{equation*} \\ln p(\\mathbf{X}, \\mathbf{y}|\\mathbf{\\theta}) = \\Bigg[\\sum_{n=1}^N\\sum_{k=1}^K \\mathbb{1}(y_n = k)\\ln \\pi_k\\Bigg] + \\sum_{k=1}^K\\sum_{d=1}^D\\Bigg[\\sum_{n:y_n=k}\\ln p(x_{nd}|\\theta_{dk})\\Bigg]. \\end{equation*}\nThus, we have a term for the the multinomial and terms for the class-feature parameters. As with previous models that use a multinomial form, the parameter estimate for the first term is computed as\n\\begin{equation*} \\hat{\\pi}_k = \\frac{N_k}{N}. \\end{equation*}\nThe features used in our data are binary, so the parameter estimate for each \\(\\hat{\\theta}_{dk}\\) follows the Bernoulli distribution:\n\\begin{equation*} \\hat{\\theta}_{dk} = \\frac{N_{dk}}{N_{k}}. \\end{equation*}\nThat is, the number of times that feature \\(d\\) is in an example of class \\(k\\) divided by the total number of samples for class \\(k\\).\nMaking a Decision Given parameters \\(\\mathbf{\\theta}\\), how can we classify a given data sample?\n\\begin{equation*} \\text{arg}\\max_{k}p(y=k)\\prod_{i}p(x_i|y=k) \\end{equation*}\nRelation to Multinomial Logistic Regression Consider some data with discrete features having one of \\(K\\) states, then \\(x_{dk} = \\mathbb{1}(x_d=k)\\). The class conditional density, in this case, follows a multinomial distribution:\n\\[ p(y=c|\\mathbf{x}, \\mathbf{\\theta}) = \\prod_{d=1}^D \\prod_{k=1}^K \\theta_{dck}^{x_{dk}}. \\]\nWe can see a connection between naive Bayes and logistic regression when we evaluate the posterior over classes:\n\\begin{align*} p(y=c|\\mathbf{x}, \\mathbf{\\theta}) \u0026amp;= \\frac{p(y)p(\\mathbf{x}|y, \\mathbf{\\theta})}{p(\\mathbf{x})}\\\\ \u0026amp;= \\frac{\\pi_c \\prod_{d} \\prod_{k} \\theta_{dck}^{x_{dk}}}{\\sum_{c\u0026rsquo;}\\pi_{c\u0026rsquo;}\\prod_{d}\\prod_{k}\\theta_{dc\u0026rsquo;k}^{x_{dk}}} \\\\ \u0026amp;= \\frac{\\exp[\\log \\pi_c + \\sum_d \\sum_k x_{dk}\\log \\theta_{dck}]}{\\sum_{c\u0026rsquo;} \\exp[\\log \\pi_{c\u0026rsquo;} + \\sum_d \\sum_k x_{dk} \\log \\theta_{dc\u0026rsquo;k}]}. \\end{align*}\nThis has the same form as the softmax function:\n\\[ p(y=c|\\mathbf{x}, \\mathbf{\\theta}) = \\frac{e^{\\beta^{T}_c \\mathbf{x} + \\gamma_c}}{\\sum_{c\u0026rsquo;=1}^C e^{\\beta^{T}_{c\u0026rsquo;}\\mathbf{x} + \\gamma_{c\u0026rsquo;}}} \\]\nMNIST Example With the model definition and parameter estimates defined, we can fit and evaluate the model. Using scikit-learn, we fit a Bernoulli naive Bayes classifier on the MNIST training set: Naive Bayes.\nGaussian Formulation If our features are continuous, we would model them with univariate Gaussians.\n","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642831200,"objectID":"f86c9f09aae89628ddd3d235958176db","permalink":"https://ajdillhoff.github.io/notes/naive_bayes/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/naive_bayes/","section":"notes","summary":"Table of Contents Introduction Definition Maximum Likelihood Estimation Making a Decision Relation to Multinomial Logistic Regression MNIST Example Gaussian Formulation Introduction To motivate naive Bayes classifiers, let\u0026rsquo;s look at slightly more complex data. The MNIST dataset was one of the standard benchmarks for computer vision classification algorithms for a long time. It remains useful for educational purposes. The dataset consists of 60,000 training images and 10,000 testing images of size \\(28 \\times 28\\).","tags":["machine learning"],"title":"Naive Bayes","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Resources Introduction Definition Forward Pass Activation Functions Multi-Class Classification Backpropagation Non-Convex Optimization Resources https://playground.tensorflow.org/ Introduction Previously, we studied the Perceptron and saw that while it made for a simple linear classifier, it is severely limited to problems that are already linearly separable. This limitation was resolved by introduding a hidden layer with multiple perceptron units, aptly named Multi-Layer Perceptrons.\nIn this series, we will explore the more general method of neural networks. We will see that even a network of only two layers can approximate any continuous functional mapping to arbitrary accuracy. Through a discussion about network architectures, activation functions, and backpropagation, we will understand and use neural networks to resolve a large number of both classification and regression tasks.\nDefinition We will take an abstract view of neural networks in which any formulation of a neural network defines a nonlinear mapping from an input space to some output space. This implies that our choice of activation function must be nonlinear. The function we create will be parameterized by some weight matrix \\(W\\). Thus, any neural network can be simply formulated as\n\\[ f(\\mathbf{x};W). \\]\nFigure 1: General neural network diagram. A neural network is in part defined by its layers, the number of nodes in each layer, the choice of activation function, and the choice of loss function.\nEach layer has a number of weights equal to the number of input nodes times the number of output nodes. This is commonly represented as a weight matrix \\(W\\).\nThe network produces output through the forward pass and computes the gradients with respect to that output in the backwards pass.\nForward Pass Computing the output is done in what is called the forward pass.\nOur neural network function takes in an input \\(\\mathbf{x} \\in \\mathbb{R}^D\\), where \\(D\\) is the number of features in our input space. Each output node \\(a_j\\) in a hidden layer \\(h_l\\) has a corresponding weight vector \\(\\mathbf{w}_j^{(l)}\\). The intermediate output of a hidden layer \\(h_l\\) is a linear combination of the weights and the input followed by some nonlinear function. Node \\(a_j\\) of a hidden layer is computed as\n\\[ a_j = \\sum_{i=1}^d w_{ji}^{(l)} x_{i} + w_{j0}^{(l)}. \\]\nAs with Linear Regression, we will prepend a constant 1 to our input so that the computation is simply\n\\[ a_{j} = \\sum_{i=0}^d w_{ji}^{(i)} x_i = \\mathbf{w}_j^T \\mathbf{x}. \\]\nThe final output of the hidden layer is \\(a_j\\) transformed by a nonlinear function \\(g\\) such that\n\\[ z_j = g(a_j). \\]\nWe can combine all weight vectors for each hidden layer node into a weight matrix \\(W \\in \\mathbb{R}^{n \\times d}\\), where \\(n\\) is the number of nodes in the layer and \\(d\\) is the number of input features such that\n\\begin{equation*} W = \\begin{bmatrix} \\mathbf{w}_1^T\\\\ \\vdots\\\\ \\mathbf{w}_n^T\\\\ \\end{bmatrix}. \\end{equation*}\nThen the output of the hidden layer can be computed as\n\\[ \\mathbf{a} = W\\mathbf{x}. \\]\nIf you instead wanted to separate the bias term, this would be\n\\[ \\mathbf{a} = W\\mathbf{x} + \\mathbf{b}. \\]\nUsing the notation to specify the individual layer, we can write the output of a full network. Let \\(W^{(l)} \\in \\mathbb{R}^{n_{l} \\times n_{l-1}}\\) be the weights for layer \\(l\\) which have \\(n_{l-1}\\) input connections and \\(n_{l}\\) output nodes. The activation function for layer \\(l\\) is given by \\(g^{(l)}\\).\nThe complete forward pass of the network is computed by repeating the following step for all layers:\n\\[ \\mathbf{z}^{(l)} = g^{(l)}(\\mathbf{a}^{(l-1)}), \\]\nwhere\n\\[ \\mathbf{a}^{(l-1)} = W^{(l-1)}\\mathbf{z}^{(l-1)} + \\mathbf{b}^{(l-1)}. \\]\nOnce all layers have been computed, then the output of the last layer, \\(\\hat{\\mathbf{y}}^{(L)}\\) is used as the final output of the model. For training, this is compared with some ground truth label \\(\\mathbf{y}\\) using a loss function \\(\\mathcal{L}\\):\n\\[ \\mathcal{L}(\\hat{\\mathbf{y}}, \\mathbf{y}). \\]\nXOR Example Consider the XOR problem. A single Perceptron was unable to solve that problem. However, adding a hidden layer and forming a multi-layer perceptron network allowed for a more complex decision boundary. Consider the network below and produce the output given all combinations of binary input: \\(\\{(0, 0), (0, 1), (1, 0), (1, 1)\\}\\).\nFigure 2: A network with 1 hidden layer that computes XOR. Source: https://athitsos.utasites.cloud/courses/cse4309_fall2021/lectures/09a_neural_networks.pdf Activation Functions Sigmoid Function \\[ g(x) = \\frac{1}{1 + e^{-x}} \\]\nThe logistic sigmoid function serves two purposes. First, it allows the output of the neuron to be interpreted as a posterior probability. Note that this is not actually a probability. Second, it is a continuous function for which the derivative can be computed:\n\\[ g\u0026rsquo;(x) = g(x)(1 - g(x)). \\]\nHyperbolic Tangent Function \\[ \\tanh x = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \\]\nThe hyperbolic tangent function maps input to a range of \\((-1, 1)\\).\nThe derivative is calculated as\n\\[ \\frac{d}{dx} \\tanh x = 1 - \\tanh^2 x. \\]\nFigure 3: Hyperbolic Tangent Function. Source: Wolfram Key Terms\nbias activation function Neurons fire after input reaches some threshold. Differential activation functions necessary for backpropagation. Multi-class learning How long to train? Weight decay How many layers versus how many nodes per layer? Training Data split (train/test/val) Multi-Class Classification Consider an output layer of a network with \\(k\\) nodes. Each of these nodes represents a decision node for a one-versus-all classifier. For a classification task, we have to think about whether or not the sum of squares loss function works.\nAs far as activation functions go, the logistic sigmoid function is a good way to produce some interpretation of probability. If we treat every output node as its own one versus all classifier, then a logistic sigmoid at the end of each one would indicate the \u0026ldquo;probability\u0026rdquo; that node \\(k\\) assigns class \\(k\\).\nHow do we formulate this in a neural network?\nThe number of nodes in the output layer will be \\(K\\), the number of classes. Since the output of each node produces a value in range \\((0, 1)\\), we want to construct a target value that works with this. Instead of assigning an integer to each class label (e.g. 1 for class 2, 2 for class 3, etc.), we will encode the target label as a \\(K\\) dimensional vector. For example, if our class label is for the class 1, then the corresponding target vector will be\n\\begin{equation*} \\mathbf{t} = \\begin{bmatrix} 1\\\\ 0\\\\ \\vdots\\\\ 0 \\end{bmatrix}. \\end{equation*}\nSince the output of our final layer is also a \\(K\\) dimensional vector, we can compare the two using some loss function.\nBackpropagation Given a series of linear layers with nonlinear activation functions, how can we update the weights across the entire network?\nThe short answer is through the chain rule of differentiation. Let\u0026rsquo;s explore this through an example.\nAfter constructing some series of hidden layers with an arbitrary number of nodes, we will pick an error function that provides a metric of how our network performs on a given regression or classification task. This loss is given by \\(\\mathcal{L}\\).\nNeural networks are traditionally trained using gradient descent. The goal is to optimize the weights such that they result in the lowest loss, or error. This is also why our choice of loss function is important.\n\\[ \\mathbf{W}^* = \\text{argmin}\\frac{1}{n}\\sum_{i=1}^n \\mathcal{L}(f(\\mathbf{x}^{(i)}; \\mathbf{W}), \\mathbf{y}^{(i)}) \\]\nWe first compute the gradients of the network with respect to the weights and biases. Then, we use those gradients to update our previous values for the weights and biases.\nA Simple Example We will first look at computing these gradients on a smaller network for binary classification with 1 hidden layer and 1 output layer. The loss function is defined using the binary cross-entropy function:\n\\[ \\mathcal{L}(\\hat{\\mathbf{y}}, \\mathbf{y}) = -\\mathbf{y}\\log \\hat{\\mathbf{y}} - (1 - \\mathbf{y}) \\log (1 - \\hat{\\mathbf{y}}) \\]\nThe network\u0026rsquo;s output is computed in sequence following\n\\begin{align*} \\mathbf{a}^{(1)} \u0026amp;= W^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)}\\\\ \\mathbf{z}^{(1)} \u0026amp;= g^{(1)}(\\mathbf{a}^{(1)})\\\\ \\mathbf{a}^{(2)} \u0026amp;= W^{(2)}\\mathbf{z}^{(1)} + \\mathbf{b}^{(2)}\\\\ \\mathbf{z}^{(2)} \u0026amp;= g^{(2)}(\\mathbf{a}^{(2)})\\\\ \\end{align*}\nThe goal is to compute the gradients for all weights and biases:\n\\[ \\frac{d\\mathcal{L}}{dW^{(1)}},\\quad \\frac{d\\mathcal{L}}{d\\mathbf{b}^{(1)}},\\quad \\frac{d\\mathcal{L}}{dW^{(2)}},\\quad \\frac{d\\mathcal{L}}{d\\mathbf{b}^{(2)}}. \\]\nStarting with the weights of the output layer:\n\\[ \\frac{d\\mathcal{L}}{dW^{(2)}} = \\frac{d\\mathcal{L}}{d\\mathbf{z}^{(2)}} \\frac{d\\mathbf{z}^{(2)}}{d\\mathbf{a}^{(2)}} \\frac{d\\mathbf{a}^{(2)}}{dW^{(2)}}. \\]\nThe first step is to compute the partial gradient of the loss function with respect to its input \\(\\hat{\\mathbf{y}} = \\mathbf{z}^{(2)}\\):\n\\[ \\frac{d\\mathcal{L}}{d\\mathbf{z}^{(2)}} = \\frac{\\mathbf{z}^{(2)} - \\mathbf{y}}{\\mathbf{z}^{(2)}(1 - \\mathbf{z}^{(2)})}. \\]\nNext, compute the gradient of the last layer\u0026rsquo;s activation function with respect to its input \\(\\mathbf{a}^{(2)}\\):\n\\[ \\frac{d\\mathbf{z}^{(2)}}{d\\mathbf{a}^{(2)}} = \\mathbf{z}^{(2)}(1 - \\mathbf{z}^{(2)}). \\]\nFinally, we compute \\(\\frac{d\\mathbf{a}^{(2)}}{dW^{(2)}}\\): \\[ \\frac{d\\mathbf{a}^{(2)}}{dW^{(2)}} = \\mathbf{z}^{(1)}. \\]\nPutting all of this together yields\n\\begin{align*} \\frac{d\\mathcal{L}}{dW^{(2)}} \u0026amp;= \\frac{\\mathbf{z}^{(2)} - \\mathbf{y}}{\\mathbf{z}^{(2)}(1 - \\mathbf{z}^{(2)})} * \\mathbf{z}^{(2)}(1 - \\mathbf{z}^{(2)}) * \\mathbf{z}^{(1)}\\\\ \u0026amp;= \\mathbf{z}^{(1)} (\\mathbf{z}^{(2)} - \\mathbf{y}). \\end{align*}\nNon-Convex Optimization Optimizing networks with non-linearities produces a non-convex landscape. Depending on our choice of optimization algorithm and initial starting point, the algorithm will most likely get \u0026ldquo;stuck\u0026rdquo; in some local minimum. Consider the figure below produced by (Li et al. 2017).\nFigure 4: Loss surface of ResNet-56 (Li et al.) References Li, Hao, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2017. “Visualizing the Loss Landscape of Neural Nets,” 11. ","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642831200,"objectID":"16c8d84377f70f04087cbe417e84cd5f","permalink":"https://ajdillhoff.github.io/notes/neural_networks/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/neural_networks/","section":"notes","summary":"Table of Contents Resources Introduction Definition Forward Pass Activation Functions Multi-Class Classification Backpropagation Non-Convex Optimization Resources https://playground.tensorflow.org/ Introduction Previously, we studied the Perceptron and saw that while it made for a simple linear classifier, it is severely limited to problems that are already linearly separable. This limitation was resolved by introduding a hidden layer with multiple perceptron units, aptly named Multi-Layer Perceptrons.\nIn this series, we will explore the more general method of neural networks.","tags":["machine learning"],"title":"Neural Networks","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction The Perceptron Learning Algorithm Limitations of Single-Layer Perceptrons Introduction A popular example of a Logistic Regression model is the perceptron. Proposed by Frank Rosenblatt in 1962, the perceptron is defined as a generalized linear model:\n\\begin{equation*} f(\\mathbf{w}^T\\mathbf{\\phi}(\\mathbf{x})), \\end{equation*}\nwhere \\(\\phi\\) is a basis function and \\(f\\) is a stepwise function with the form\n\\begin{equation*} f(a) = \\begin{cases} 1, a \\geq 0\\\\ -1, a \u0026lt; 0 \\end{cases} \\end{equation*}\nTo match this, the targets will take on a value of either 1 or -1.\nThe Perceptron Learning Algorithm Based on the stepwise function, the parameters \\(\\mathbf{w}\\) should lead to outputs above 0 for one class and outputs below 0 for the other. There is 0 error with a correct classification.\nThe original formulation does not work well with gradient based optimization methods due to the fact that the derivative of the stepwise function is 0 almost everyone. To get around this, the perceptron criterion is used:\n\\begin{equation*} E(\\mathbf{w}) = -\\sum_i \\mathbf{w}^T\\phi(\\mathbf{x}_i)\\hat{y}_i, \\end{equation*}\nwhere \\(\\hat{y}_i\\) is the target class (either 1 or -1).\nAn incorrect classification will minimize \\(\\mathbf{w}^T\\phi_i y_i\\). We can consider this loss only for misclassified patterns.\nUpdate Steps\nFor each input, evaluate \\(f(\\mathbf{w}^T\\phi(\\mathbf{x}_i))\\). For incorrect classifications Add \\(\\phi(\\mathbf{x}_i)\\) to \\(\\mathbf{w}\\) estimate for class 1 Subtract \\(\\phi(\\mathbf{x}_i)\\) from \\(\\mathbf{w}\\) for class 2. Does not necessarily get better each step, but guaranteed to converge.\nLimitations of Single-Layer Perceptrons Single layer perceptrons are limited to solving linearly separable patterns. As we have seen with a few datasets now, expecting our data to be linearly separable is wishful thinking. Minsky and Papert exposed this limitation in their book Perceptrons: an introduction to computational geometry.\nConsider the example XOR problem. It is a binary classification problem consisting of 4 data points. It is not linearly separable as seen in the figure below.\nFigure 1: XOR cannot be solved with a linear classifier. This is the result of using only a single Perceptron. What if we added another perceptron? A single perceptron computes \\(\\mathbf{w}^T + b\\). It is important to transform the first perceptron\u0026rsquo;s output using a non-linear activation function, otherwise the output would be similar to that of a logistic regression model. The updated \u0026ldquo;network\u0026rdquo; is shown below.\nFigure 2: A 2 layer perceptron for which each layer has a single node. The result is the same! The original input in 2D is transformed to a single dimensional output. This is then used as input to the second perceptron. The result is a linear decision boundary followed by another linear decision boundary. What if we used 2 perceptrons in the first layer? The idea is that using two linear decision boundaries in a single space would allow our model to create a more complex boundary. The updated network is shown below.\nFigure 3: A 2 layer perceptron for which the first layer has 2 nodes. This effectively solves the XOR problem! Since each node computes a linear combination of the input, we can visualize two decision boundaries with respect to the input space.\nFigure 4: Visualization of input space. Similarly, we can visualize how the data points are transformed by visualizing the space of the output layer.\nFigure 5: Output space ","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642831200,"objectID":"05741d6538d42b28c94ed3a24f23ee36","permalink":"https://ajdillhoff.github.io/notes/perceptron/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/perceptron/","section":"notes","summary":"Table of Contents Introduction The Perceptron Learning Algorithm Limitations of Single-Layer Perceptrons Introduction A popular example of a Logistic Regression model is the perceptron. Proposed by Frank Rosenblatt in 1962, the perceptron is defined as a generalized linear model:\n\\begin{equation*} f(\\mathbf{w}^T\\mathbf{\\phi}(\\mathbf{x})), \\end{equation*}\nwhere \\(\\phi\\) is a basis function and \\(f\\) is a stepwise function with the form\n\\begin{equation*} f(a) = \\begin{cases} 1, a \\geq 0\\\\ -1, a \u0026lt; 0 \\end{cases} \\end{equation*}","tags":["machine learning"],"title":"Perceptron","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Summary Maximum Variance Formulation Motivating Example Noise and Redundancy Covariance Matrix Summary If we have some measurements of data, but do not know the underlying dynamics, PCA can resolve this by producing a change of basis such that the dynamics are reflected upon the eigenvectors.\nMaximum Variance Formulation Although there are several derivations of PCA. I really like the approach of projecting the data onto a lower dimensional space in order to maximize the variance of the projected data.\nLet \\(\\mathbf{X}\\) be a dataset of \\(N\\) samples, each with \\(D\\) features. The goal of PCA is to project this data onto an $M$-dimensional space such that \\(M \u0026lt; D\\).\nRemember that the goal here is to maximize the variance of the projected data.\nHow do we project the data? Let\u0026rsquo;s say that we want to go from $D$-dimensional space to $M$-dimensional space where \\(M = 1\\). Let the vector \\(\\mathbf{u}\\) define this 1D space. If \\(\\mathbf{u}\\) is a unit vector, then the scalar projection of a data point \\(\\mathbf{x}\\) onto \\(\\mathbf{u}\\) is simply \\(\\mathbf{u} \\cdot \\mathbf{x}\\).\nSince we are maximizing variance, we need to subtract the mean sample from our data\n\\begin{equation*} \\mathbf{\\bar{x}} = \\frac{1}{N}\\sum_{n=1}^{N}\\mathbf{x}_n \\end{equation*}\nThen, the mean of the projected data is \\(\\mathbf{u} \\cdot \\mathbf{\\bar{x}}\\).\nWith the mean of the projected data, we can calculate the variance:\n\\begin{equation*} \\frac{1}{N}\\sum_{n=1}^{N}\\{\\mathbf{u}^T\\mathbf{x}_n - \\mathbf{u}^T\\mathbf{\\bar{x}}\\}^2 = \\mathbf{u}^T\\mathbf{S}\\mathbf{u} \\end{equation*}\nwhere\n\\begin{equation*} \\mathbf{S} = \\frac{1}{N}\\sum_{n=1}^{N}(\\mathbf{x}_n - \\mathbf{\\bar{x}})(\\mathbf{x}_n - \\mathbf{\\bar{x}})^T \\end{equation*}\nThus, if we are maximizing the variance of the projected data, then we are maximizing \\(\\mathbf{u}^T\\mathbf{S}\\mathbf{u}\\)!\nSo this is an optimization problem, but there is one minor issue to deal with: if \\(\\mathbf{u}\\) is not constrained, then we scale it to infinity while maximizing the function.\nBefore, we stated that \\(\\mathbf{u}\\) is a unit vector. Thus, the constraint is that \\(\\mathbf{u} \\cdot \\mathbf{u} = 1\\).\nAfter reviewing Lagrangian multipliers\u0026hellip;\nTo enforce this constraint, we can use a lagrangian multiplier:\n\\begin{equation*} \\mathcal{L}(\\mathbf{u}, \\lambda) = \\mathbf{u}^T\\mathbf{S}\\mathbf{u} + \\lambda(1 - \\mathbf{u}^T\\mathbf{u}). \\end{equation*}\nLet\u0026rsquo;s see what happens when we compute the stationary points (critical points) of the given Lagrangian function.\n\\begin{equation*} \\nabla_{\\mathbf{u}}\\mathcal{L}(\\mathbf{u}, \\lambda) = \\mathbf{S}\\mathbf{u} - \\lambda \\mathbf{u} = 0 \\end{equation*}\nThis implies that\n\\begin{equation*} \\mathbf{S}\\mathbf{u} = \\lambda \\mathbf{u} \\end{equation*}\nThat particular equation means that \\(\\mathbf{u}\\) is an eigenvector of \\(\\mathbf{S}\\) with \\(\\lambda\\) being the corresponding eigenvalue. Since \\(\\mathbf{u}\\) is a unit vector, we can conveniently left-multiply both sides of that equation by \\(\\mathbf{u}^T\\), resulting in:\n\\begin{equation*} \\mathbf{u}^T\\mathbf{S}\\mathbf{u} = \\lambda \\end{equation*}\nWhat does this mean?\nThat means that the variance is maximized when \\(\\mathbf{u}\\) is the eigenvector corresponding to the largest eigenvalue \\(\\lambda\\).\nWe can repeat this process to find the direction (eigenvector) corresponding to the second largest variance by considering eigenvectors that are orthogonal to the first one. This is where an orthonormal eigenbasis comes in handy.\nMotivating Example Consider a frictionless, massless spring that produces dynamics in a single direction.\nFigure 1: Toy model of spring with ball observed from 3 perspectives. We can clearly understand that the spring will only move in a single direction. That movement reflects the underlying dynamics of this data. To understand how PCA can be useful in this situation, let\u0026rsquo;s pretend that we do not know the underlying dynamics. Instead, we observe that the data seems to go back and forth along a single axis. We observe the data over time from 3 different perspectives given by the cameras in the above figure.\nFrom the perspective of the observer, we are recording some observations in an effort to understand which dimensions are the most salient at representing the underlying mechanics. From the above figure, we know that the most important dimension in this system is that of the labeled x-axis.\nHow would we figure this out if we did not already know that?\nEach camera has its own coordinate system (basis). If each camera gives us a 2D location of the ball relative to that camera\u0026rsquo;s basis, then each sample in time gives us a 6D vector of locations.\nEquivalently, every time sample is a vector that lies in an $m$-dimensional vector space spanned by an orthonormal basis.\nIs it possible to find another basis that best expresses the data?\nMathemtically, is there some matrix \\(P\\) that changes our original data \\(X\\) into a new representation \\(Y\\)?\n\\(PX = Y\\)\nNoise and Redundancy When observing real data, we will have to account for noisy measurements. Noise can come from a wide variety of sources. Being able to reduce it or filter it out is vital to understanding the underlying system.\nNoise is an arbitrary measurement and means nothing without some measurement of a signal. Thus, we typically measure the amount of noise in our system using a Signal-to-Noise Ratio (SNR). This assumes we have some idea of what our signal is. This is usually given based on the nature of whatever problem we are investigating. In the toy example, we know that the spring largely moves in a single dimension. That is the signal we expect to observe.\nFor arguments sake, imagine we that the recordings over time from a single camera plot the following data:\nFrom our advantageous position of knowing the true nature of the problem, we understand there really should be no noise. However, let\u0026rsquo;s say that our camera has some noise in interpreting the precise location of the ball at any given time. In this case, our SNR is quite high, which is good! Ideally, it would be a straight line.\nThere is a second factor to consider: the fact that we are taking measurements from multiple sensors means that there may be some redundancy among the data collected from them. If we were to discover features that have high redundancy, we could be confident in concluding that they are highly correlated.\nCovariance Matrix Let \\(X\\) be a an \\(m \\times n\\) matrix of \\(n\\) observations with \\(m\\) features per observation.\nWe can produce a covariance matrix of the features via \\(S_{X} = \\frac{1}{n-1}XX^{T}\\).\nThis gives us a measurement of the correlations between all pairs of measurements.\nIf we want to reduce redunancy between separate measurements (those in the off-diagonal of the matrix), we would then want to diagonalize this matrix. In terms of the equation \\(PX=Y\\), this has the effective of finding a new covariance matrix \\(S_{Y}\\) that is diagonal. This means that each value in the off-diagonal of \\(S_{Y}\\) is 0.\nPCA has a convenient assumption: the change of basis matrix \\(P\\) is orthonormal. Why is this convenient? PCA can then select the normalized direction in the feature space for which the variance in the data is maximized. This is called the first principal component. Because we assume \\(P\\) is orthonormal, the subsequent principal components must be orthogonal to the previously discovered components.\nOne more thing If \\(P\\) is not orthonormal, then we can simply scale our eigenvectors to maximize variance.\n","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642831200,"objectID":"cddf33f5490eea81f22d826d26808a07","permalink":"https://ajdillhoff.github.io/notes/principal_component_analysis/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/principal_component_analysis/","section":"notes","summary":"Table of Contents Summary Maximum Variance Formulation Motivating Example Noise and Redundancy Covariance Matrix Summary If we have some measurements of data, but do not know the underlying dynamics, PCA can resolve this by producing a change of basis such that the dynamics are reflected upon the eigenvectors.\nMaximum Variance Formulation Although there are several derivations of PCA. I really like the approach of projecting the data onto a lower dimensional space in order to maximize the variance of the projected data.","tags":["dimensionality reduction","machine learning"],"title":"Principal Component Analysis","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction A Simple Example Probability Distributions Conditional Probability Rules of Probability Random Variables Continuous Variables Moments of a Distribution Introduction Probability theory provides a consistent framework for the quantification and manipulation of uncertainty. It allows us to make the best decisions given the limited information we may have. Many tasks, models, and evaluation metrics that we will explore in this course are either based on, or are inspired by, probability theory.\nA Simple Example Scenario: There are two cookie jars, a blue one for cookies with oatmeal raisin cookies and a red one for chocolate chip cookies. The jar with oatmeal raisin cookies has 8 cookies in it. The chocolate chip jar has 10 cookies. Some monster took 2 of the chocolate chip cookies and placed them in the oatmeal raisin jar and placed 1 of the oatmeal raisin cookies in the chocolate chip jar. Thus, the oatmeal raisin jar has 2 chocolate chip and 7 oatmeal raisin. The chocolate chip jar has 8 chocolate chip and 1 oatmeal raisin.\nLet\u0026rsquo;s say that we pick the chocolate chip jar 80% of the time and the oatmeal raisin jar 20% of the time. For a given jar, the cookies inside are all equally likely to be picked. We can assign this probability to random variables:\n\\(J\\) - The type of jar, either blue \\(b\\) or red \\(r\\). \\(C\\) - The type of cookie, either oatmeal \\(o\\) or chocolate chip \\(c\\). We can define the probability of picking a particular jar:\n\\(p(J=b) = 0.2\\) \\(p(J = r) = 0.8\\) Notice that their sum is 1.0. These probabilities can be estimated empirically given an observer recording the events. We may also define the probabilities of picking a particular type of cookie.\n\\(p(C = o)\\) \\(p(C = c)\\) For each jar, the probabilities of picking the cookies must sum to 1. The tables below show the individual probabilities of picking each type of cookie from each jar. Since we can observe the actual quantities, we can define the probabilities empirically.\nChocolate Chip Oatmeal Raisin Blue Jar 2 / 9 = 0.222 7 / 9 = 0.778 Chocolate Chip Oatmeal Raisin Red Jar 8 / 9 = 0.889 1 / 9 = 0.111 Given these quantities, we can ask slightly more complicated questions such as \u0026ldquo;what is the probability that I will select the red jar AND take a chocolate chip cookie?\u0026rdquo; This is expressed as a joint probability distribution, written as \\(p(J = r, C = c)\\). It is defined based on two events:\nthe prior probability of picking the red jar, the conditional probability of picking a chocolate chip cookie conditioned on the event that the red jar was picked. \\begin{equation*} p(J=r, C=c) = p(C=c | J=r) p(J = r) \\end{equation*}\nThis is also referred to as the product rule.\nWe already know \\(p(J=r) = 0.8\\). From the table above, we can see that \\(p(C=c|J=r) = 0.889\\). Thus, \\(p(C=c,J=r) = 0.8 * 0.889 = 0.711\\).\nIf we knew nothing about the contents of the jar or the prior probabilities of selecting a jar, we could measure the joint probability empirically. This would simply be the number of times we select the red jar AND a chocolate chip cookie divided by total number of trials. For best results, perform an infinite number of trials.\nIf instead we wanted to measure the conditional probability \\(p(C=c|J=r)\\), we would simply take the number of times a chocolate chip cookie is taken from the red jar and divide by the total number of times the red jar was selected.\nWe can construct a joint probability table given the joint probabilities of all the events listed.\nChocolate Chip Oatmeal Raisin Red Jar 0.711 0.089 Blue Jar 0.044 0.156 If you summed each row and further took the sum of the sum of rows, you would get 1. Likewise, the sum of the sum of columns would equal 1.\nSumming the columns for each row yields the prior probability of selecting each type of jar. Similarly, summing the rows for each column gives the prior probability of selecting that type of cookie. This is referred to as the marginal probability or sum rule, which is computed by summing out the other variables in the joint distribution. For example,\n\\begin{equation*} p(x_i) = \\sum_j p(x_i, y_j) \\end{equation*}\nEmpirically, this is computed as the number of times event \\(x_i\\) occurs out of ALL trials.\nAlthough the joint probabilities \\(p(X, Y)\\) and \\(p(Y, X)\\) would be written slightly differently, they are equal. With this in mind, we can set them equal to each other to derive Bayes\u0026rsquo; rule:\n\\begin{align*} p(X, Y) \u0026amp;= p(Y, X)\\\\ p(X|Y)p(Y) \u0026amp;= p(Y|X)p(X)\\\\ p(X|Y) \u0026amp;= \\frac{p(Y|X)p(X)}{p(Y)} \\end{align*}\nIn this context, \\(p(X|Y)\\) is referred to as the posterior probability of event \\(X\\) conditioned on the fact that we know event \\(Y\\) has occurred. On the right, \\(p(X)\\) is the prior probability of event \\(X\\) in the absence of any additional evidence.\nTwo variables are independent, then\n\\[ p(X, Y) = p(X)p(Y) \\]\nIf two variables are conditionally independent given a third event, then\n\\[ p(X, Y|Z) = P(X|Z)P(Y|Z) \\]\nProbability Distributions Events come from a space of possible outcomes.\n\\begin{equation*} \\Omega = {1, 2, 3, 4, 5, 6} \\end{equation*}\nA measureable event is one for which we can assign a probability.\nAn event space must satisfy the following:\nIt contains the empty event \\(\\emptyset\\) and trivial event \\(\\Omega\\) It is closed under union It is closed under complementation: if \\(\\alpha \\in S\\), so is \\(\\Omega - \\alpha\\) Statement 2 implies difference and intersection A probability distribution \\(P\\) over \\((\\Omega, S)\\) maps events \\(S\\) to real values and satisfies:\n\\(P(\\alpha) \\geq 0\\) for all \\(\\alpha \\in S\\) \\(P(\\Omega) = 1\\) If \\(\\alpha,\\beta \\in S\\) and \\(\\alpha \\cap \\beta = \\emptyset\\), then \\(P(\\alpha \\cup \\beta) = P(\\alpha)+P(\\beta)\\) \\(P(\\emptyset) = 0\\) \\(P(\\alpha \\cup \\beta) = P(\\alpha) + P(\\beta) - P(\\alpha \\cap \\beta)\\) Conditional Probability Defined as\n\\begin{equation*} P(\\beta | \\alpha) = \\frac{P(\\alpha \\cap \\beta)}{P(\\alpha)} \\end{equation*}\nThe more that \\(\\alpha\\) and \\(\\beta\\) relate, the higher the probability.\nThe Chain Rule of Probability \\begin{equation*} P(\\alpha \\cap \\beta) = P(\\alpha) P(\\beta | \\alpha) \\end{equation*}\nGenerally\u0026hellip;\n\\begin{equation*} P(\\alpha_1 \\cap \\dotsb \\cap \\alpha_k) = P(\\alpha_1)P(\\alpha_2 | \\alpha_1) \\dotsm P(\\alpha_k | \\alpha_1 \\cap \\dotsb \\cap \\alpha_{k-1}) \\end{equation*}\nBayes\u0026rsquo; Rule \\begin{equation*} P(\\alpha | \\beta) = \\frac{P(\\beta | \\alpha)P(\\alpha)}{P(\\beta)} \\end{equation*}\nComputes the inverse conditional probability.\nA general conditional version of Baye\u0026rsquo;s rule:\n\\begin{equation*} P(\\alpha | \\beta \\cap \\gamma) = \\frac{P(\\beta | \\alpha \\cap \\gamma)P(\\alpha | \\gamma)}{P(\\beta | \\gamma)} \\end{equation*}\nExample: TB Tests A common example for introduction Bayes\u0026rsquo; rule is that of the test that gives 95% accuracy. The naive assumption here is that if you receive a positive result with no prior information, then there is a 95% chance you have the infection. This is wrong because that value is conditioned on already being infected.\nRules of Probability Sum Rule: \\(p(X) = \\sum_{Y}p(X, Y)\\)\nProduct Rule: \\(p(X, Y) = p(Y|X)p(X)\\)\nRandom Variables Allows for compact notation when talking about an event. It can also be represented as a function:\n\\begin{equation*} f_{\\text{Grade}} \\end{equation*}\nmaps each person in \\(\\Omega\\) to a grade value.\nRandom variables are commonly either categorical or real numbers.\nThe multinoulli distribution is one over \\(k \u0026gt; 2\\) categorical random variables. If \\(k = 2\\), the distribution is called the Bernoulli or binomial distribution.\nThe marginal distribution is one over a single random variable \\(X\\).\nA joint distribution is one over a set of random variables.\nThe marginal can be computed from a joint distribution.\n\\begin{equation*} P(x) = \\sum_{y}P(x, y) \\end{equation*}\nContinuous Variables The introductory example looked at events that take on discrete values. That is, we either selected a cookie or did not. Most of the problems we will deal with in this course involve continuous values. In this case, we are concerned with intervals that the values may take on. If we consider a small differential of our random variable \\(x\\) as \\(\\delta x\\), we can compute the probability density \\(p(x)\\).\nFigure 1: PDF (p(x)) and CDF (P(x)). Source: Bishop With this differential \\(\\delta x\\), we can compute the probability that \\(x\\) lies on some interval \\((a, b)\\):\n\\begin{equation*} p(a \\leq x \\leq b) = \\int_{a}^{b} p(x) dx \\end{equation*}\nAs with discrete probability distributions, the probability density must sum to 1 and cannot take a negative value. That is\n\\begin{align*} p(x) \u0026amp;\\geq 0\\\\ \\int_{-\\infty}^{\\infty}p(x)dx \u0026amp;= 1 \\end{align*}\nIn the plot above, \\(p(x)\\) is the probability density function (pdf) and \\(P(x)\\) is the cumulative distribution function (cdf). It is possible for a pdf to have a value greater than 1, as long as integrals over any interval are less than or equal to 1.\nThe cumulative distribution function \\(P(x)\\) is the probability that \\(x\\) lies in the interval \\((-\\infty, z)\\), given by\n\\begin{equation*} P(z) = \\int_{\\infty}^{z} p(x)dx. \\end{equation*}\nNote that the derivative of the cdf is equal to the pdf.\nThe product rule for continuous probability distributions takes on the same form as that of discrete distributions. The sum rule is written in terms of integration:\n\\begin{equation*} p(x) = \\int p(x, y)dy. \\end{equation*}\nMoments of a Distribution A moment of a function describes a quantitative measurement related to its graph. With respect to probability densities, the $k$th moment of \\(p(x)\\) is defined as \\(\\mathbb{E}[x^k]\\). The first moment is the mean of the distribution, the second moment is the variance, and the third moment is the skewness.\nThree extremely important statistics for any probability distribution are the average, variance, and covariance.\nExpectation The average of a function \\(f(x)\\) under a probability distribution \\(p(x)\\) is referred to as the expectation of \\(f(x)\\), written as \\(\\mathbb{E}[f]\\). The expectation for discrete and continuous distributions are\n\\begin{align*} \\mathbb{E}[f] \u0026amp;= \\sum_x p(x)f(x) \\text{ and}\\\\ \\mathbb{E}[f] \u0026amp;= \\int p(x)f(x)dx, \\end{align*}\nrespectively.\nFigure 2: Expectation of rolling a d6 over ~1800 trials converges to 3.5. Source: Seeing Theory The mean value for a discrete and continuous probability distribution is define as\n\\begin{align*} \\mathbb{E}[f] \u0026amp;= \\sum_x p(x)x \\text{ and}\\\\ \\mathbb{E}[f] \u0026amp;= \\int_{-\\infty}^{\\infty} p(x)xdx, \\end{align*}\nrespectively.\nEmpirically, we can approximate this quantity given \\(N\\) samples as\n\\begin{equation*} \\mathbb{E}[f] \\approx \\frac{1}{N}\\sum_{i=1}^{N}f(x_i). \\end{equation*}\nVariance The variance of a function \\(f(x)\\) under a probability distribution \\(p(x)\\) measures how much variability is in \\(f(x)\\) around the expected value \\(\\mathbb{E}[f(x)]\\) and is defined by\n\\begin{align*} \\text{var}[f] \u0026amp;= \\mathbb{E}[(f(x) - \\mathbb{E}[f(x)])^2]\\\\ \u0026amp;= \\mathbb{E}[f(x)^2] - \\mathbb{E}[f(x)]^2. \\end{align*}\nFigure 3: Variance of drawing cars with values 1-10 100 trials converges to 8.79. True variance is 8.25. Source: Seeing Theory Covariance The covariance of two random variables \\(x\\) and \\(y\\) provides a measure of dependence between the two variables. This implies that the covariance between two independent variables is 0.\n\\begin{align*} \\text{cov}[\\mathbf{x},\\mathbf{y}] \u0026amp;= \\mathbf{E}_{\\mathbf{x},\\mathbf{y}}[\\{\\mathbf{x} - \\mathbb{E}[\\mathbf{x}]\\}\\{\\mathbf{y}^T - \\mathbb{E}[\\mathbf{y}^T]\\}]\\\\ \u0026amp;= \\mathbb{E}_{\\mathbf{x},\\mathbf{y}}[\\mathbf{x}\\mathbf{y}^T] - \\mathbb{E}[\\mathbf{x}]\\mathbb{E}[\\mathbf{y}^T]. \\end{align*}\nFigure 4: Plot of 2D data with negative covariance. Source: Wikipedia Figure 5: Plot of 2D data with approximately 0 covariance. Source: Wikipedia Figure 6: Plot of data with positive covariance. Source: Wikipedia Correlation The correlation between two random variables \\(x\\) and \\(y\\) relates to their covariance, but it is normalized to lie between -1 and 1.\n\\begin{equation*} \\text{corr}[\\mathbf{x},\\mathbf{y}] = \\frac{\\text{cov}[\\mathbf{x},\\mathbf{y}]}{\\sqrt{\\text{var}[\\mathbf{x}]\\text{var}[\\mathbf{y}]}} \\end{equation*}\nThe correlation between two variables will equal 1 if there is a linear relationship between them. We can then view the correlation as providing a measurement of linearity.\nFigure 7: Sets of points with their correlation coefficients. Source: Wikipedia Limitations of Moments Summary statistics can be useful but do not tell the whole story of your data. When possible, it is always better to visualize the data. An example of this is the Anscombosaurus, derived from the Anscombe\u0026rsquo;s quartet. The quartet consists of four datasets that have nearly identical summary statistics but are visually distinct. A modern version, called the Datasaurus Dozen, consists of 12 datasets that have the same summary statistics but are visually distinct.\nFigure 8: Datasaurus Dozen (source: Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing) ","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693267200,"objectID":"03c02884d03e4e29be21f5ab9a6ade33","permalink":"https://ajdillhoff.github.io/notes/probability_theory/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/probability_theory/","section":"notes","summary":"Table of Contents Introduction A Simple Example Probability Distributions Conditional Probability Rules of Probability Random Variables Continuous Variables Moments of a Distribution Introduction Probability theory provides a consistent framework for the quantification and manipulation of uncertainty. It allows us to make the best decisions given the limited information we may have. Many tasks, models, and evaluation metrics that we will explore in this course are either based on, or are inspired by, probability theory.","tags":null,"title":"Probability Theory","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Overfitting Penalizing Weights Dataset Augmentation Early Stopping Dropout Introduction Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error. - Goodfellow et al.\nRegularization comes in many forms. Some techniques may add an additional penalty to the loss function. Others, such as data augmentation, add artificial variation to the data. In all cases, regularization aims to improve the generalization performance by preventing the model from overfitting.\nOverfitting What happens when the complexity of our chosen model fits the data too well? Take a look at the following plot of data. The red curve is the true underlying function that generated the data. The blue line represents a polynomial of degree 9 fit via linear regression. It is first necessary to understand what is happening.\nFigure 1: A polynomial of degree 11 (blue) fit to data generated following the red line. The model with more parameters is able to fit some the noisy data slightly better. Does this necessarily mean it will perform better on new samples? No, it will usually perform worse. This is referred to as overfitting. Overfitting can be identified as the model trains. When the testing loss continues to decrease while the validation loss increases, the model is probably overfitting. It is also evident from looking at the weights.\nIdentifying the Cause The goal of training is to modify the weights such that they minimize the loss function. Models with more parameters have the capacity to fit more of their training data. Given the presence of noise, this is not a good thing. A very low loss on the training set may not translate to good performance on the validation set.\nLooking at weights of the trained model is a good way of detecting overfitting. From the model above, the mean of the absolute value of the weights is \\(11.1\\). Left unchecked, the weights will take on whatever values necessary to meet the objective function.\nPenalizing Weights The most common form of regularization is to penalize the weights from taking on a high value. That is, we define a penalty term \\(E(\\mathbf{w})\\) that is added to the loss. The higher the weight values, the higher the total loss. Thus, optimization will also include minimizing the absolute values of the weights. A simple choice for \\(E(\\mathbf{w})\\), especially in the context of least squares, is \\(L2\\) regularzation:\n\\[ E(\\mathbf{w}) = \\frac{\\lambda}{2}||\\mathbf{w}||^2 = \\frac{\\lambda}{2}\\mathbf{w}^T \\mathbf{w}. \\]\nAdded to the sum-of-squares error for least squares, the final loss becomes\n\\[ J(\\mathbf{w}) = \\frac{1}{2}\\sum_{i=1}^n(h(\\mathbf{x}_i;\\mathbf{w}) - \\mathbf{y}_i)^2 + \\frac{\\lambda}{2} \\mathbf{w}^T \\mathbf{w}. \\]\nThis choice of regularization also has the benefit of being in a form that can be minimized in closed form via the normal equations. Taking the gradient of \\(J(\\mathbf{w})\\) above with respect to 0 and solving for \\(\\mathbf{w}\\) yields\n\\[ \\mathbf{w} = (\\lambda I + \\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}, \\]\nwhere \\(\\lambda\\) is a regularization hyperparameter.\nApplying this regularization term to the model above with \\(\\lambda=1\\) yields the model shown below.\nFigure 2: Least squares model fit with (L2) regularization ((lambda = 1)). Inspecting the weights as before, we can see that the mean of the absolute values of \\(\\mathbf{w}\\) is \\(0.0938\\).\nEvaluating on the Testing Data To see which model generalizes better, we set aside some samples from the original dataset to use as testing.\nWith regularization, the model error on the test set is \\(1.8\\). Without regularization, the model error on the test set is \\(2.2\\).\nDataset Augmentation The same data augmentation techniques should be applied on both methods being compared. Getting a better result on a benchmark because of data augmentation does not mean the method was better suited for the task. By controlling these factors, a fair comparison can be made.\nThere are many forms of augmentation available for image tasks in particular. Rotating, translating, and scaling images are the most common. Additionally applying random crops can further augment the dataset.\nThe original dataset may only include samples of a class that have similar lighting. Color jitter is an effective way of including a broader range of hue or brightness and usually leads to a model that is robust to such changes.ZZ\nIt is important to make sure that the crops still contain enough information to properly classify it. Common forms of data augmentation are available through APIs like torchvision.\nEarly Stopping If the validation loss begins to increase while the training loss continues to decrease, this is a clear indication that the model is beginning to overfit the training data. Stopping the model in this case is the best way to prevent this. Frameworks like PyTorch Lightning include features to checkpoing the models based on best validation loss and stop the model whenever the validation loss begins to diverge.\nDropout Dropout is a regularization method introduced by \u0026lt;\u0026amp;srivastavaDropoutSimpleWay2014\u0026gt; which is motivated by ensemble methods. Ensembles of models are regularized by the fact that many different models are trained on random permutations of the dataset with varying parameters and initializations. Using an ensemble of networks is a powerful way of increasing generalization performance. However, it requires much more compute due to the fact that several models must be trained.\nTraining a single network with dropout approximates training several models in an ensemble. It works by randomly removing a node from the network during a forward/backward pass. The node is not truly removed. Instead, its output during the forward and backward passes is ignored via a binary mask.\nWhen training a network with dropout, it will generally take longer for the model to converge to a solution. Intuitively, this is because a different subnetwork is being used for each pass.\n","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642831200,"objectID":"4be961e39b7e422bb57adc26e82744ea","permalink":"https://ajdillhoff.github.io/notes/regularization/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/regularization/","section":"notes","summary":"Table of Contents Introduction Overfitting Penalizing Weights Dataset Augmentation Early Stopping Dropout Introduction Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error. - Goodfellow et al.\nRegularization comes in many forms. Some techniques may add an additional penalty to the loss function. Others, such as data augmentation, add artificial variation to the data. In all cases, regularization aims to improve the generalization performance by preventing the model from overfitting.","tags":["machine learning"],"title":"Regularization","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Maximum Margin Classifier Formulation Overlapping Class Distributions Multiclass SVM Additional Resources Introduction Support Vector Machines are a class of supervised learning methods primarily used for classification. Although they can be formulated for regression and outlier detection as well. Instead of optimizing a set of parameters which compress or summarize the training set, they use a small subset of the training data to compute the decision function.\nThey rely on the data being linearly separable, so feature transformations are critical for problems in which the original representation of the data is not linearly separable.\nMaximum Margin Classifier Let\u0026rsquo;s start with a simple classification model as we studied with Logistic Regression. That is, we have\n\\[ f(\\mathbf{x}) = \\mathbf{w}^T\\phi(\\mathbf{x}), \\]\nwhere \\(\\phi(\\mathbf{x})\\) is a function which transforms our original input into some new feature space. The transformed input is assumed to be linearly separable so that a decision boundary can be computed. In the original logistic regression problem, a decision boundary was found through optimization. For linearly separable data, there are an infinite number of decision boundaries that satisfy the problem.\nWhat about the quality of the decision boundary?\nIs one decision boundary better than the other?\nTODO: Add a few plots comparing decision boundaries\nFormulation Given a training set \\(\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\}\\) with labels \\(\\{y_1, \\dots, y_n\\}\\), where \\(y_i \\in \\{-1, 1\\}\\), we construct a linear model which classifies an input sample depending on the sign of the output.\nOur decision rule for classification, given some input \\(\\mathbf{x}\\), is\n\\begin{equation*} f(\\mathbf{x}) = \\begin{cases} 1\\text{ if }\\mathbf{w}^T\\mathbf{x} + b \\geq 0\\\\ -1\\text{ if }\\mathbf{w}^T\\mathbf{x} + b \u0026lt; 0 \\end{cases} \\end{equation*}\nHow large should the margin be?\nIn the original formulation of Logistic Regression, we saw that the parameter vector \\(\\mathbf{w}\\) described the normal to the decision boundary. The distance between a given point \\(\\mathbf{x}\\) and the decision boundary is given by\n\\[ \\frac{y_if(\\mathbf{x})}{||\\mathbf{w}||}. \\]\nWe can frame this as an optimization problem: come up with a value for \\(\\mathbf{w}\\) that maximizes the margin.\n\\[ \\text{arg max}_{\\mathbf{w}, b} \\frac{1}{\\|\\mathbf{w}\\|}\\min_{i} y_i (\\mathbf{w}^T\\phi(\\mathbf{x}_i) + b) \\]\nWe can arbitrarily scale the parameters, so we add an additional constraint that any point that lies on the boundary of the margin satisfies\n\\[ y_i(\\mathbf{w}^T\\mathbf{x} + b) = 1. \\]\nUnder this constraint, we have that all samples satisfy\n\\[ y_i(\\mathbf{w}^T\\mathbf{x} + b) \\geq 1. \\]\nThat is, all positive samples with target \\(1\\) will produce at least a \\(1\\), yielding a value greater than or equal to 1. All negative samples with target \\(-1\\) will produce at most a \\(-1\\), yielding a value greater than or equal to 1.\nAnother way of writing this is\n\\begin{equation*} f(\\mathbf{x}) = \\begin{cases} 1\\text{ if }\\mathbf{w}^T\\mathbf{x}_{+} + b \\geq 1\\\\ -1\\text{ if }\\mathbf{w}^T\\mathbf{x}_{-} + b \\leq -1, \\end{cases} \\end{equation*}\nwhere \\(\\mathbf{x}_+\\) is a positive sample and \\(\\mathbf{x}_-\\) is a negative sample. The decision rule can then be written as\n\\[ y_i(\\mathbf{w}^T\\mathbf{x} + b) - 1 \\geq 0. \\]\nThis implies that the only samples that would yield an output of 0 are those that lie directly on the margins of the decision boundary.\nGiven this constraint of \\(y_i(\\mathbf{w}^T\\mathbf{x} + b) - 1 = 0\\), we can derive our optimization objective.\nThe margin can be computed via the training data. To do this, consider two data points which lie on their respective boundaries, one positive and one negative, and compute the distance between them: \\(\\mathbf{x}_+ - \\mathbf{x}_-\\). This distance with respect to our decision boundary, defined by \\(\\mathbf{w}\\), is given by\n\\[ (\\mathbf{x}_+ - \\mathbf{x}_-) \\cdot \\frac{\\mathbf{w}}{||\\mathbf{w}||}. \\]\nFor clarity, we can rewrite this as\n\\begin{equation*} \\frac{1}{||\\mathbf{w}||}(\\mathbf{x}_{+} \\cdot \\mathbf{w} - \\mathbf{x}_{-} \\cdot \\mathbf{w}). \\end{equation*}\nIf we substitute the sample values into the equality constraint above, we can simplify this form. For the positive sample, we have \\(\\mathbf{w}^T\\mathbf{x} = 1 - b\\). For the negative sample, we get \\(\\mathbf{w}^T\\mathbf{x} = -1 - b\\). The equation above then becomes\n\\begin{equation*} \\frac{1}{||\\mathbf{w}||}(1 - b - (-1 - b)) = \\frac{2}{||\\mathbf{w}||}. \\end{equation*}\nThus, our objective is to maximize \\(\\frac{2}{||\\mathbf{w}||}\\) which is equivalent to minimizing \\(\\frac{1}{2}||\\mathbf{w}||^2\\) subject to the constraints \\(y_i(\\mathbf{w}^T\\mathbf{x}+b)\\geq 1\\). This is a constrainted optimization problem. As discussed previously, we can simplify such problems by introducing Lagrangian Multipliers. Doing this produces the dual representation of our optimization objection:\n\\begin{equation*} L = \\frac{1}{2}||\\mathbf{w}||^2 - \\sum_{i=1}^n \\alpha_i \\big(y_i(\\mathbf{w}^T\\mathbf{x}_i + b) - 1\\big). \\end{equation*}\nTo solve for \\(\\mathbf{w}\\) we compute \\(\\frac{\\partial}{\\partial \\mathbf{w}}L\\).\n\\begin{equation*} \\frac{\\partial}{\\partial \\mathbf{w}}L = \\mathbf{w} - \\sum_{i=1}^n \\alpha_i y_i \\mathbf{x}_i. \\end{equation*}\nSetting this to 0 yields\n\\begin{equation*} \\mathbf{w} = \\sum_{i=1}^n \\alpha_i y_i \\mathbf{x}_i. \\end{equation*}\nDoing the same for the other parameter \\(b\\) yields\n\\[ 0 = \\sum_{i=1}^n \\alpha_i y_i. \\]\nWe can now simplify our objective function by substituting these results into it:\n\\begin{align*} L \u0026amp;= \\frac{1}{2}\\Big(\\sum_{i=1}^n \\alpha_i y_i \\mathbf{x}_i\\Big)^2 - \\sum_{i=1}^n \\alpha_i\\Big(y_i\\big((\\sum_{i=1}^n\\alpha_i y_i \\mathbf{x}_i)^T\\mathbf{x}_i + b \\big) - 1 \\Big)\\\\ \u0026amp;= \\frac{1}{2}\\Big(\\sum_{i=1}^n \\alpha_i y_i \\mathbf{x}_i\\Big)^2 - \\Big(\\sum_{i=1}^n \\alpha_i y_i \\mathbf{x}_i \\Big)^2 - \\sum_{i=1}^n \\alpha_i y_i b + \\sum_{i=1}^n \\alpha_i\\\\ \u0026amp;= -\\frac{1}{2} \\Big(\\sum_{i=1}^n \\alpha_i y_i \\mathbf{x}_i \\Big)^2 + \\sum_{i=1}^n \\alpha_i\\\\ \u0026amp;= \\sum_{i=1}^n \\alpha_i - \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i \\cdot \\mathbf{x}_j \\end{align*}\nThus, the objective is dependent on the inner product of samples \\(\\mathbf{x}_i\\) and \\(\\mathbf{x}_j\\). If these were representations in some complex feature space, our problem would remain computationally inefficient. However, we can take advantage of Kernels for this.\nNote that, in most cases, \\(\\alpha_i\\) will be 0 since we only consider support vectors. That is, the points that lie on the margins of the decision boundary.\nOverlapping Class Distributions The above formulation is fine and works with datasets that have no overlap in feature space. That is, they are completely linearly separable. However, it is not always the case that they will be.\nTo account for misclassifications while still maximizing a the margin between datasets, we introduce a penalty value for points that are misclassified. As long as there aren\u0026rsquo;t too many misclassifications, this penalty will stay relatively low while still allowing us to come up with an optimal solution.\nThis penalty comes in the form of a slack variable \\(\\xi_i \\geq 0\\) for each sample that is \\(0\\) for points that are on or inside the correct margin and \\(\\xi_i = |y_i - f(\\mathbf{x})|\\) for others. If the point is misclassified, its slack variable will be \\(\\xi_i \u0026gt; 1\\).\nMulticlass SVM Similar to our simple Logistic Regression method, SVMs are binary classifiers by default. We can take a similar approach to extending them to multiple classes, but there are downsides to each approach.\nThe \u0026ldquo;one-vs-all\u0026rdquo; approach entails building \\(|K|\\) classifiers and choose the classifier which predicts the input with the greatest margin.\nThe \u0026ldquo;one-vs-one\u0026rdquo; approach involves building \\(|K|\\cdot\\frac{|K| - 1}{2}\\) classifiers. In this case, training each classifer will be more tractable since the amount of data required for each one is less. For example, you would have a model for class 1 vs 2, class 1 vs 3, \u0026hellip;, class 1 vs \\(K\\). Then repeat for class 2: 2 vs 3, 2 vs 4, \u0026hellip;, 2 vs \\(|K|\\), and so on.\nA third approach is to construct several models using a feature vector dependent on both the data and class label. When given a new input, the model computes\n\\[ y = \\text{arg}\\max_{y\u0026rsquo;}\\mathbf{w}^T\\phi(\\mathbf{x},y\u0026rsquo;). \\]\nThe margin for this classifier is the distance between the correct class and the closest data point of any other class.\nAdditional Resources https://web.mit.edu/6.034/wwwbob/svm.pdf https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf ","date":1642831200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642831200,"objectID":"0072e15bf08a567be6de4867b7b022a6","permalink":"https://ajdillhoff.github.io/notes/support_vector_machine/","publishdate":"2022-01-22T00:00:00-06:00","relpermalink":"/notes/support_vector_machine/","section":"notes","summary":"Table of Contents Introduction Maximum Margin Classifier Formulation Overlapping Class Distributions Multiclass SVM Additional Resources Introduction Support Vector Machines are a class of supervised learning methods primarily used for classification. Although they can be formulated for regression and outlier detection as well. Instead of optimizing a set of parameters which compress or summarize the training set, they use a small subset of the training data to compute the decision function.","tags":["machine learning"],"title":"Support Vector Machine","type":"notes"},{"authors":["Alex Dillhoff"],"categories":null,"content":" Table of Contents Introduction Probabilistic Interpretation Solving with Normal Equations Another Approach to Normal Equations Fitting Polynomials Linear Basis Functions Introduction Given a dataset of observations \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\), where \\(n\\) is the number of samples and \\(d\\) represents the number of features per sample, and corresponding target values \\(\\mathbf{Y} \\in \\mathbb{R}^n\\), create a simple prediction model which predicts the target value \\(\\mathbf{y}\\) given a new observation \\(\\mathbf{x}\\). The classic example in this case is a linear model, a function that is a linear combination of the input features and some weights \\(\\mathbf{w}\\).\nFigure 1: Plot of univariate data where the (x) values are features and (y) are observations. The generated data is plotted above along with the underlying true function that was used to generate it. If we already know what the true function is, our job is done. Suppose that we only have the data points (in blue). How do we go about modelling it? It is reasonable to first visualize the data and observe that it does follow a linear pattern. Thus, a linear model would be a decent model to choose.\nIf the data followed a curve, we may decide to fit a polynomial. We will look at an example of that later on. For now, let\u0026rsquo;s formalize all of the information that we have.\n\\((\\mathbf{x}, \\mathbf{y})\\) - Data points from the original dataset. Generally, \\(\\mathbf{x}\\) is a vector of features and \\(\\mathbf{y}\\) is the target vector. In our simple dataset above, these are both scalar values. \\(\\mathbf{w} = (w_0, w_1)\\) - Our model parameters. Comparing to the equation \\(y = mx + b\\), \\(w_0\\) is our bias term and \\(w_1\\) is our slope parameter. Making Predictions Given \\(\\mathbf{w}\\), we can make a prediction for a new data sample \\(\\mathbf{x} = x_1\\).\n\\[ h(\\mathbf{x}; \\mathbf{w}) = w_0 + w_1 x_1 \\]\nNote that the bias term is always added to the result. We can simplify this into a more general form by appending a constant 1 (s.t. \\(x_0 = 1\\)) to each of our samples such that \\(\\mathbf{x} = (1, x_1, \u0026hellip;, x_d)\\). Then, the general linear model becomes\n\\[ h(\\mathbf{x}; \\mathbf{w}) = \\sum_{i=0}^{d} w_i x_i = \\mathbf{w}^T \\mathbf{x}. \\]\nIf our data happened to have more than 1 feature, it would be easy enough to model it appropriately using this notation.\nDetermining Fitness If we really wanted to, we could fit our model by plotting it and manually adjusting the weights until our model looked acceptable by some qualitative standard. Fortunately we won\u0026rsquo;t be doing that. Instead, we will use a quantitative measurement that provides a metric of how well our current parameters fit the data.\nFor this, we use a cost function or loss function. The most common one to use for this type of model is the least-squares function:\n\\[ J(\\mathbf{w}) = \\frac{1}{2}\\sum_{i=1}^{n}(h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i})^2. \\]\nStochastic Gradient Descent Depending on the random initialization of parameters, our error varies greatly. We can observe that no matter what the chose parameters are, there is no possible way we can achieve an error of 0. The best we can do is minimize this error:\n\\[ \\min_{\\mathbf{w}} J(\\mathbf{w}). \\]\nFor this, we rely on stochastic gradient descent. The basic idea is as follows:\nBegin with an initial guess for \\(\\mathbf{w}\\). Compare the prediction for sample \\(\\mathbf{x}^{(i)}\\) with its target \\(\\mathbf{y}^{(i)}\\). Update \\(\\mathbf{w}\\) based on the comparison in part 2. Repeat steps 2 and 3 on the dataset until the loss has converged. Steps 1, 3, and 4 are easy enough. What about step 2? How can we possibly know how to modify \\(\\mathbf{w}\\) such that \\(J(\\mathbf{w})\\) will decrease? By computing the gradient \\(\\frac{d}{d\\mathbf{w}}J(\\mathbf{w})\\)! How will we know when we have arrived at a minima? When \\(\\nabla J(\\mathbf{w}) = 0\\).\n\\begin{align*} \\frac{d}{d\\mathbf{w}}J(\\mathbf{w}) \u0026amp;= \\frac{d}{d\\mathbf{w}}\\frac{1}{2}(h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i})^2\\\\ \u0026amp;= 2 \\cdot \\frac{1}{2}(h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i}) \\cdot \\frac{d}{d\\mathbf{w}} (h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i})\\\\ \u0026amp;= (h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i}) \\cdot \\frac{d}{d\\mathbf{w}} (\\mathbf{w}^T \\mathbf{x}_{i} - \\mathbf{y}_{i})\\\\ \u0026amp;= (h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i}) \\mathbf{x}_{i} \\end{align*}\nThe gradient represents the direction of greatest change for a function evaluated With this gradient, we can use an update rule to modify the previous parameter vector \\(\\mathbf{w}\\):\n\\[ \\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\alpha \\sum_{i=1}^{n} (h(\\mathbf{x}_{i};\\mathbf{w}_{t}) - \\mathbf{y}_{i}) \\mathbf{x}_{i}. \\]\nHere, \\(\\alpha\\) is an update hyperparameter that allows us to control how big or small of a step our weights can take with each update. In general, a smaller value will be more likely to get stuck in local minima. However, too large of a value may never converge to any minima.\nAnother convenience of this approach is that it is possible to update the weights based on a single sample, batch of samples, or the entire dataset. This sequential process makes optimization using very large dataset feasible.\nProbabilistic Interpretation \u0026ldquo;Probability theory is nothing but common sense reduced to calculation.\u0026rdquo; - Pierre-Simon Laplace\nRecall Bayes\u0026rsquo; theorem:\n\\[ p(\\mathbf{w}|\\mathbf{X}) = \\frac{p(\\mathbf{X}|\\mathbf{w})p(\\mathbf{w})}{p(\\mathbf{X})}. \\]\nThat is, the posterior probability of the weights conditioned on the observered data \\(\\mathbf{X}\\) is equal to the likelihood of the observed data given the times the prior distribution. This base notation doesn\u0026rsquo;t line up well with our problem. For our problem, we have observations \\(\\mathbf{Y}\\) which are dependent on the input features \\(\\mathbf{X}\\):\n\\[ p(\\mathbf{w}|\\mathbf{X}, \\mathbf{Y}) = \\frac{p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}) p(\\mathbf{w}|\\mathbf{X})}{p(\\mathbf{Y}|\\mathbf{X})}, \\]\nwhere \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) and \\(\\mathbf{Y} \\in \\mathbb{R}^n\\).\nThe choice of least squares also has statistical motivations. As discussed previously, we are making a reasonable assumption that there is some relationship between the features of the data and the observed output. This is typically modeled assume\n\\[ \\hat{\\mathbf{Y}} = f(\\mathbf{X}) + \\epsilon. \\]\nHere, \\(\\epsilon\\) is a random error term that is independent of \\(\\mathbf{X}\\) and has 0 mean. This term represents any random noise that occurs either naturally or from sampling. It also includes any effects that are not properly captured by \\(f\\). Rearranging the terms of this equation to solve for \\(\\epsilon\\) allows us to define the discrepencies in the model:\n\\[ \\mathbf{\\epsilon}_i = h(\\mathbf{x}_{i}; \\mathbf{w}) - \\mathbf{y}_{i}. \\]\nIf we assume that these discrepancies are independent and identically distributed with variance \\(\\sigma^2\\) and Gaussian PDF \\(f\\), the likelihood of observations \\(\\mathbf{y}^{(i)}\\) given parameters \\(\\mathbf{w}\\) is\n\\[ p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}, \\sigma) = \\prod_{i=1}^{n} f(\\epsilon_i; \\sigma), \\]\nwhere\n\\[ f(\\epsilon_i; \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\Big(-\\frac{\\epsilon^2}{2\\sigma^2}\\Big). \\]\nThis new parameter changes our original distribution function to\n\\[ p(\\mathbf{w}|\\mathbf{X}, \\mathbf{Y}, \\sigma) = \\frac{p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}, \\sigma) p(\\mathbf{w}|\\mathbf{X}, \\sigma)}{p(\\mathbf{Y}|\\mathbf{X}, \\sigma)}. \\]\nTwo things to note before moving on. First, the prior \\(p(\\mathbf{Y}|\\mathbf{X}, \\sigma)\\) is a normalizing constant to ensure that the posterior is a valid probability distribution. Second, if we assume that all value for \\(\\mathbf{w}\\) are equally likely, then \\(p(\\mathbf{w}|\\mathbf{x}, \\sigma)\\) also becomes constant. This is a convenient assumption which implies that maximizing the posterior is equivalent to maximizing the likelihood function.\nWith that out of the way, we can focus solely on the likelihood function. Expanding out the gaussian PDF \\(f\\) yields\n\\[ p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}, \\sigma) = -\\frac{n}{\\sqrt{2\\pi\\sigma^2}}\\exp\\Big(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i})^2\\Big). \\]\nWe can see that maximizing \\(p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}, \\sigma)\\) is the same as minimizing the sum of squares. In practice, we use the negative log of the likelihood function since the negative logarithm is monotonically decreasing.\nSolving with Normal Equations You may have studied the normal equations when you took Linear Algebra. The normal equations are motivated by finding approximate solutions to \\(A\\mathbf{x} = \\mathbf{b}\\). Most of the earlier part of linear algebra courses focus on finding exact solutions by solving systems of equations using Gaussian elimination (row reduction). Approximate solutions can be found by projecting the observed data points \\(\\mathbf{b}\\) onto the column space of \\(A\\) and solving \\(A \\mathbf{x} = \\hat{\\mathbf{b}}\\), where \\(\\hat{\\mathbf{b}} = \\text{proj}_{\\text{Col} A}\\mathbf{b}\\). Then, \\(\\mathbf{b} - \\hat{\\mathbf{b}}\\) represents a vector orthogonal to \\(\\text{Col}A\\).\nSince each column vector of \\(A\\) is orthogonal to \\(\\mathbf{b} - \\hat{\\mathbf{b}}\\), the dot product between them should be 0. Rewriting this, we get\n\\begin{aligned} A^T(\\mathbf{b} - A\\mathbf{x}) \u0026amp;= \\mathbf{0}\\\\ A^T \\mathbf{b} - A^T A \\mathbf{x} \u0026amp;= \\mathbf{0}. \\end{aligned}\nThis means that each least-squares solution of \\(A\\mathbf{x} = \\mathbf{b}\\) satisfies\n\\[ A^T A \\mathbf{x} = A^T \\mathbf{b}. \\]\nExample Let\u0026rsquo;s take our univariate problem of \\((\\mathbf{x}, \\mathbf{y})\\) pairs. To use the normal equations to solve the least squares problem, we first change the notation just a bit as not confuse our data points and our parameters:\n\\[ \\mathbf{X}^T \\mathbf{X} \\beta = \\mathbf{X}^T \\mathbf{y} \\]\nCreate the design matrix \\(\\mathbf{X}\\) where each row represents the the \\(\\mathbf{x}\\) values. Recall that even though we only have 1 feature for \\(\\mathbf{x}\\), we append the bias constant as \\(x_0 = 1\\) to account for the bias parameter. \\(\\mathbf{X}\\) is then\n\\begin{equation*} \\mathbf{X} = \\begin{bmatrix} x_0^{(0)} \u0026amp; x_1^{(0)}\\\\ x_0^{(1)} \u0026amp; x_1^{(1)}\\\\ \\vdots \u0026amp; \\vdots \\\\ x_0^{(n)} \u0026amp; x_1^{(n)} \\end{bmatrix}. \\end{equation*}\nThe parameter vector is\n\\begin{equation*} \\beta = \\begin{bmatrix} \\beta_0\\\\ \\beta_1 \\end{bmatrix}. \\end{equation*}\nThe observed values are packed into \\(\\mathbf{y}\\). We can then solve for \\(\\beta\\) using any standard solver:\n\\[ \\beta = (\\mathbf{X}^T \\mathbf{X})^{-1}X^T \\mathbf{y}. \\]\nRank-Deficient matrices In the event that the matrix \\(\\mathbf{X}^T \\mathbf{X}\\) is singular, then its inverse cannot be computed. This implies that one or more of the features is a linear combination of the others.\nThis can be detected by checking the rank of \\(\\mathbf{X}^T \\mathbf{X}\\) before attempting to compute the inverse. You can also determine which features are redundant via Gaussian elimination. The columns in the reduced matrix that do not have a pivot entry are redundant.\nAnother Approach to Normal Equations We can arrive at the normal equations by starting at the probabilistic perspective. Recall the likelihood function\n\\[ p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}, \\sigma) = -\\frac{n}{\\sqrt{2\\pi\\sigma^2}}\\exp\\Big(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i})^2\\Big). \\]\nTaking the natural log of this function yields\n\\[ \\ln p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}, \\sigma) = - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(h(\\mathbf{x}_{i}; \\mathbf{w}) - \\mathbf{y}_{i})^2 - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{n}{2}\\ln(2\\pi). \\]\nAs mentioned before, maximizing the likelihood function is equivalent to minimizing the sum-of-squares function. Thus, we must find the critical point of the likelihood function by computing the gradient (w.r.t. \\(\\mathbf{w}\\)) and solving for 0:\n\\begin{align*} \\nabla \\ln p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}, \\sigma) \u0026amp;= \\sum_{i=1}^{n}(\\mathbf{w}^T\\mathbf{x}_{i} - \\mathbf{y}_{i})\\mathbf{x}_{i}^{T}\\\\ \u0026amp;= \\mathbf{w}^T \\sum_{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^T - \\sum_{i=1}^{n}\\mathbf{y}_{i}\\mathbf{x}_{i}^{T}\\\\ \\end{align*}\nNoting that \\(\\sum_{i=1}^{n}\\mathbf{x}_i \\mathbf{x}_i^T\\) is simply matrix multiplication, we can use\n\\begin{equation*} \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1^T\\\\ \\vdots\\\\ \\mathbf{x}_n^T\\\\ \\end{bmatrix}. \\end{equation*}\nThen, \\(\\sum_{i=1}^{n}\\mathbf{x}_i \\mathbf{x}_i^T = \\mathbf{X}^T \\mathbf{X}\\), \\(\\sum_{i=1}^{n}\\mathbf{y}_i \\mathbf{x}_i^T = \\mathbf{Y}^T \\mathbf{X}\\), and\n\\[ \\nabla \\ln p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}, \\sigma) = \\mathbf{w}^T \\mathbf{X}^T \\mathbf{X} - \\mathbf{Y}^T \\mathbf{X}. \\]\nSince we are finding the maximum likelihood, we set \\(\\nabla \\ln p(\\mathbf{Y}|\\mathbf{X}, \\mathbf{w}, \\sigma) = 0\\) and solve for \\(\\mathbf{w}\\):\n\\[ \\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{Y}. \\]\nThus, we arrive again at the normal equations and can solve this using a linear solver.\nFitting Polynomials Not every dataset can be modeled using a simple line. Data can be exponential or logarithmic in nature. We may also look to use splines to model more complex data.\nFigure 2: Data generated from a nonlinear function with added noise. The dataset above was generated from the function as seen in red. Using a simple linear model (blue) does not fit the data well. For cases such as this, we can fit a polynomial to the data by changing our input data.\nThe simple dataset above has 100 paired samples \\((x_i, y_i)\\). There is only a single feature \\(x_i\\) for each sample. It is trivial to determine that the shape of the data follows a cubic function. One solution would be to raise each input to the power of 3. This results in the function (blue) below.\nFigure 3: Solution from raising each input to the power of 3. To fit this data, we need to add more features to our input. Along with the original \\(x_i\\) features, we will also add \\(x_i^2\\) and \\(x_i^3\\). Our data is then 3 dimensional. The figure below shows the least squares fit using the modified data (blue).\nFigure 4: Least squares fit using a polynomial model (blue). A demo of this can be found here.\nLinear Basis Functions Linear models are linear in their inputs. This formulation is simple, producing models with limited representation. Linear models can be extended as a linear combination of fixed nonlinear functions of the original features. In the previous section, was saw that they could easily be extended to fit polynomial functions.\nWe now consider creating a model that transforms the original input using one or more nonlinear functions. This type of model is called a linear basis function model.\n\\[ h(\\mathbf{x};\\mathbf{w}) = \\sum_{j=1}^{m} w_j\\phi_j(\\mathbf{x}) \\]\nCommon basis functions are the sigmoid, Gaussian, or exponential function. If we choose the \\(\\sin\\) function as a basis function, we can more closely fit our dataset using the least squares approach.\nFigure 5: A linear basis function model using the sin function as the choice of basis. ","date":1641967200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641967200,"objectID":"267d7f34a9ee73d8fc77237de6f23b2a","permalink":"https://ajdillhoff.github.io/notes/linear_regression/","publishdate":"2022-01-12T00:00:00-06:00","relpermalink":"/notes/linear_regression/","section":"notes","summary":"Table of Contents Introduction Probabilistic Interpretation Solving with Normal Equations Another Approach to Normal Equations Fitting Polynomials Linear Basis Functions Introduction Given a dataset of observations \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\), where \\(n\\) is the number of samples and \\(d\\) represents the number of features per sample, and corresponding target values \\(\\mathbf{Y} \\in \\mathbb{R}^n\\), create a simple prediction model which predicts the target value \\(\\mathbf{y}\\) given a new observation \\(\\mathbf{x}\\).","tags":null,"title":"Linear Regression","type":"notes"},{"authors":[],"categories":[],"content":"Welcome to Slides academia\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne **Two** Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://ajdillhoff.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using academia's Slides feature.","tags":[],"title":"Slides","type":"slides"}]