<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computer Vision on Alex Dillhoff</title>
    <link>https://ajdillhoff.github.io/sections/computer-vision/</link>
    <description>Recent content in Computer Vision on Alex Dillhoff</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Wed, 19 Feb 2025 00:00:00 -0500</lastBuildDate>
    <atom:link href="https://ajdillhoff.github.io/sections/computer-vision/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Patch Extraction</title>
      <link>https://ajdillhoff.github.io/notes/patch_extraction/</link>
      <pubDate>Thu, 06 Jun 2024 17:57:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/notes/patch_extraction/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#native-patch-extraction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Native Patch Extraction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#changing-perspective&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Changing Perspective&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#the-mechanics-of-as-strided&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;The Mechanics of &lt;code&gt;as_strided&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#what-about-rgb&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;What about RGB?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;p&gt;This post is a recreation of &lt;a href=&#34;https://x.com/MishaLaskin/status/1478500251376009220&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;Misha Laskin&amp;rsquo;s Twitter post&lt;/a&gt; about patch extraction in &lt;code&gt;numpy&lt;/code&gt;. I wanted to provide a version of it that can be accessed without requiring a Twitter account.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Transformers for Computer Vision</title>
      <link>https://ajdillhoff.github.io/notes/transfomers_for_computer_vision/</link>
      <pubDate>Tue, 18 Apr 2023 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/notes/transfomers_for_computer_vision/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#vision-transformer--vit&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Vision Transformer (ViT) (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Dosovitskiy et al. 2021&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#swin-transformer&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Swin Transformer (&lt;a href=&#34;#citeproc_bib_item_4&#34;&gt;Liu et al. 2021&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;vision-transformer--vit&#34;&gt;Vision Transformer (ViT) (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Dosovitskiy et al. 2021&lt;/a&gt;)&lt;/h2&gt;&#xA;&lt;p&gt;The original Vision Transformer (ViT) was published by Google Brain with a simple objective: apply the Transformer architecture to images, adding as few modifications necessary. When trained on ImageNet, as was standard practice, the performance of ViT does not match models like ResNet. However, scaling up to hundreds of millions results in a better performing model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Object Detection</title>
      <link>https://ajdillhoff.github.io/notes/object_detection/</link>
      <pubDate>Mon, 18 Apr 2022 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/notes/object_detection/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#papers&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Papers&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#evaluating-object-detection-methods&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Evaluating Object Detection Methods&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#datasets&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Datasets&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#an-incomplete-history-of-deep-learning-based-object-detection&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;An Incomplete History of Deep-Learning-based Object Detection&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;papers&#34;&gt;Papers&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://awesomeopensource.com/projects/object-detection&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;https://awesomeopensource.com/projects/object-detection&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;evaluating-object-detection-methods&#34;&gt;Evaluating Object Detection Methods&lt;/h2&gt;&#xA;&lt;p&gt;Object detection algorithms are evaluated using the mean of Average Precision (mAP) across all classes in the dataset.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Optical Flow</title>
      <link>https://ajdillhoff.github.io/notes/optical_flow/</link>
      <pubDate>Sun, 06 Mar 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/optical_flow/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#motion-features&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Motion Features&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#computing-optical-flow&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Computing Optical Flow&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#assumptions-of-small-motion&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Assumptions of Small Motion&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#applications&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Applications&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Optical flow refers to the apparent motion in a 2D image. Optical flow methods estimate a &lt;strong&gt;motion field&lt;/strong&gt;, which refers to the true motion of objects in 3D. If a fixed camera records a video of someone walking from the left side of the screen to the right, a difference of two consecutive frames reveals much about the apparent motion.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Segmentation via Clustering</title>
      <link>https://ajdillhoff.github.io/notes/segmentation_via_clustering/</link>
      <pubDate>Thu, 24 Feb 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/segmentation_via_clustering/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#agglomerative-clustering&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Agglomerative Clustering&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#k-means-clustering&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;K-Means Clustering&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#simple-linear-iterative-clustering--slic&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Simple Linear Iterative Clustering (SLIC)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#superpixels-in-recent-work&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Superpixels in Recent Work&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;The goal of segmentation is fairly broad: group visual elements together.&#xA;For any given task, the question is &lt;em&gt;how are elements grouped?&lt;/em&gt;&#xA;At the smallest level of an image, pixels can be grouped by color, intensity, or spatial proximity.&#xA;Without a model of higher level objects, the pixel-based approach will break down at a large enough scale.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Active Contours</title>
      <link>https://ajdillhoff.github.io/notes/active_contours/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/active_contours/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#resources&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Resources&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#parametric-representation&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Parametric Representation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#motivation-of-the-fundamental-snake-equation&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Motivation of the Fundamental Snake Equation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#external-force&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;External Force&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#energy-minimization&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Energy Minimization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#iterative-solution&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Iterative Solution&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#applications&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Applications&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;http://www.cs.ait.ac.th/~mdailey/cvreadings/Kass-Snakes.pdf&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;http://www.cs.ait.ac.th/~mdailey/cvreadings/Kass-Snakes.pdf&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Image Segmentation</title>
      <link>https://ajdillhoff.github.io/notes/image_segmentation/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/image_segmentation/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#resources&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Resources&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#gestalt-theory&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Gestalt Theory&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#grouping&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Grouping&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#segmentation-methods&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Segmentation Methods&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/&lt;/a&gt; (Berkeley Segmentation Database)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.15203v2&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;https://arxiv.org/abs/2105.15203v2&lt;/a&gt; (SegFormer)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1703.06870&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;https://arxiv.org/abs/1703.06870&lt;/a&gt; (Mask R-CNN)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/sithu31296/semantic-segmentation&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;https://github.com/sithu31296/semantic-segmentation&lt;/a&gt; (Collection of SOTA models)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Feature extraction methods such as &lt;a href=&#34;https://ajdillhoff.github.io/notes/scale_invariant_feature_transforms/&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;SIFT&lt;/a&gt; provide us with many distinct, low-level features that are useful for providing local descriptions images. We now &amp;ldquo;zoom out&amp;rdquo; and take a slightly higher level look at the next stage of image summarization.&#xA;Our goal here is to take these low-level features and group, or fit, them together such that they represent a higher level feature. For example, from small patches representing color changes or edges, we may wish to build higher-level feature representing an eye, mouth, and nose.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hough Transform</title>
      <link>https://ajdillhoff.github.io/notes/hough_transform/</link>
      <pubDate>Thu, 17 Feb 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/hough_transform/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#algorithm&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Algorithm&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#rectangle-detection-based-on-a-windowed-hough-transform&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Rectangle Detection based on a Windowed Hough Transform&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Fitting a model to a set of data by consensus, as in &lt;a href=&#34;https://ajdillhoff.github.io/notes/random_sample_consensus/&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;RANdom SAmple Consensus&lt;/a&gt;, produces a parameter estimate that is robust to outliers. A similar technique for detecting shapes in images is the &lt;strong&gt;Hough Transform&lt;/strong&gt;.&#xA;Originally it was designed for detecting simple lines, but it can be extended to detect &lt;a href=&#34;https://en.wikipedia.org/wiki/Generalised_Hough_transform&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;arbitrary shapes&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Edge Detection</title>
      <link>https://ajdillhoff.github.io/notes/edge_detection/</link>
      <pubDate>Tue, 01 Feb 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/edge_detection/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#computing-gradient-norms&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Computing Gradient Norms&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#nonmaxima-suppression&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Nonmaxima Suppression&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#thresholding&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Thresholding&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#connectivity-analysis&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Connectivity Analysis&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;outer-figure&#34;&gt;&#xA;&lt;figure class=&#34;custom-figure&#34;&gt;&#xA;&#xA;  &lt;a data-fancybox=&#34;&#34; href=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_22-45-59_screenshot.png&#34; data-caption=&#34;Figure 1: Vertical derivative filter (left) and horizontal derivative filter (right).&#34; class=&#34;glightbox&#34;&gt;&#xA;&#xA;&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_22-45-59_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Vertical derivative filter (left) and horizontal derivative filter (right).&#34; &gt;&#xA;&lt;/a&gt;&#xA;&#xA;&#xA;&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;&#xA;  &#xA;  &lt;p&gt;&#xA;    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Vertical derivative filter (left) and horizontal derivative filter (right).&#xA;    &#xA;    &#xA;    &#xA;  &lt;/p&gt;</description>
    </item>
    <item>
      <title>Sampling and Aliasing</title>
      <link>https://ajdillhoff.github.io/notes/sampling/</link>
      <pubDate>Sun, 30 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/sampling/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#resizing&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Resizing&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#sampling&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Sampling&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;resizing&#34;&gt;Resizing&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Aliasing arises through resampling an image&lt;/li&gt;&#xA;&lt;li&gt;How to resize - algorithm&lt;/li&gt;&#xA;&lt;li&gt;How to resolve aliasing&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Resizing an image, whether increase or decreasing the size, is a common image operation. In Linear Algebra, &lt;strong&gt;scaling&lt;/strong&gt; is one of the transformations usually discussed, along with rotation and skew. Scaling is performed by creating a transformation matrix&lt;/p&gt;</description>
    </item>
    <item>
      <title>Color</title>
      <link>https://ajdillhoff.github.io/notes/color/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/color/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#agenda&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Agenda&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#light-and-color&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Light and Color&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#the-human-eye&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;The Human Eye&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#color-matching&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Color Matching&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#color-physics&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Color Physics&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#color-spaces&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Color Spaces&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#hsv-color-space&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;HSV Color Space&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;agenda&#34;&gt;Agenda&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What is color?&lt;/li&gt;&#xA;&lt;li&gt;How do we process color?&lt;/li&gt;&#xA;&lt;li&gt;How is color modeled?&lt;/li&gt;&#xA;&lt;li&gt;What information does color contain?&lt;/li&gt;&#xA;&lt;li&gt;What can we infer from color?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;light-and-color&#34;&gt;Light and Color&lt;/h2&gt;&#xA;&lt;p&gt;Light is electromagnetic radiation. It is generally described as waves in an electromagnetic field, but it also considered as photons. Electromagnetic radiation is typically measured based on its wavelength and intensity. Intensity refers to the amount of power that is carried by the photons.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Histogram of Oriented Gradients</title>
      <link>https://ajdillhoff.github.io/notes/histogram_of_oriented_gradients/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/histogram_of_oriented_gradients/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#orientation-histograms&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Orientation Histograms&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#histogram-of-oriented-gradients&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Histogram of Oriented Gradients&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Key Questions&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Image Features</title>
      <link>https://ajdillhoff.github.io/notes/image_features/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/image_features/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#detecting-corners&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Detecting Corners&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#describing-image-patches&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Describing Image Patches&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#scale-invariance&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Scale Invariance&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Why do we care about image features? One of the main goals of computer vision is understanding of some environment through visual perception. In order to summarize a visual object, we need some description of it.&#xA;These descriptions can come in many forms, so we need to articulate some goals as to what we are ultimately looking for when describing an image.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Filters</title>
      <link>https://ajdillhoff.github.io/notes/linear_filters/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/linear_filters/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#smoothing&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Smoothing&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#convolution&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Convolution&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#gaussian-filters&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Gaussian Filters&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#image-derivatives&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Image Derivatives&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;p&gt;Liner filters are a fundamental concept in computer vision. They are used to process images in a variety of ways, such as smoothing, sharpening, and edge detection. They are also used in convolutional neural networks to extract features from images. An essential building block of the computer vision pipeline, understanding linear filters is crucial for anyone working in the field.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Scale Invariant Feature Transforms</title>
      <link>https://ajdillhoff.github.io/notes/scale_invariant_feature_transforms/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/scale_invariant_feature_transforms/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#difference-of-gaussians&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Difference of Gaussians&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#keypoint-localization&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Keypoint Localization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#orientation-assignment&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Orientation Assignment&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#descriptor-formation&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Descriptor Formation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
