<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GPU Programming on Alex Dillhoff</title>
    <link>https://ajdillhoff.github.io/sections/gpu-programming/</link>
    <description>Recent content in GPU Programming on Alex Dillhoff</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Tue, 25 Feb 2025 00:00:00 -0500</lastBuildDate>
    <atom:link href="https://ajdillhoff.github.io/sections/gpu-programming/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GPU Pattern: Convolution</title>
      <link>https://ajdillhoff.github.io/notes/gpu_pattern_convolution/</link>
      <pubDate>Mon, 15 Jan 2024 21:35:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/gpu_pattern_convolution/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#convolution&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Convolution&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#properties-of-convolutions&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Properties of Convolutions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#implementing-a-convolution-kernel&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Implementing a Convolution Kernel&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#constant-memory-and-caching&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Constant Memory and Caching&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#tiled-convolutions&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Tiled Convolutions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#caching-the-halo-cells&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Caching the Halo Cells&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;p&gt;This pattern involves tiling and input data staging.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Profiling CUDA Applications</title>
      <link>https://ajdillhoff.github.io/notes/profiling_cuda_applications/</link>
      <pubDate>Mon, 15 Jan 2024 14:48:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/profiling_cuda_applications/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#overview-of-nsight&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Overview of Nsight&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#getting-started-with-nsight&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Getting Started with Nsight&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#case-study-matrix-multiplication&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Case Study: Matrix Multiplication&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#tips-and-best-practices&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Tips and Best Practices&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#ocl-notes&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;OCL Notes&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;overview-of-nsight&#34;&gt;Overview of Nsight&lt;/h2&gt;&#xA;&lt;p&gt;NVIDIA NSight Compute is a profiling tool for CUDA kernels. It features an expert system that can help you identify performance bottlenecks in your code. It is essential for methodically optimizing your code. These notes will cover the basics of using Nsight Compute to profile your CUDA applications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CUDA Memory Architecture</title>
      <link>https://ajdillhoff.github.io/notes/cuda_memory_architecture/</link>
      <pubDate>Thu, 11 Jan 2024 15:07:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/cuda_memory_architecture/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#memory-access&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Memory Access&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#memory-types&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Memory Types&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#tiling&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Tiling&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#example-tiled-matrix-multiplication&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Example: Tiled Matrix Multiplication&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#boundary-checking&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Boundary Checking&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#memory-use-and-occupancy&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Memory Use and Occupancy&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#dynamically-changing-the-block-size&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Dynamically Changing the Block Size&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#the-takeaway&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;The Takeaway&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;So far, the kernels we have used assume everything is on global memory. Even though there are thousands of cores that can effectively hide the latency of transferring data to and from global memory, we will see this delay will become a bottleneck in many applications. These notes explore the different types of memory available on the GPU and how to use them effectively.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CUDA Architecture</title>
      <link>https://ajdillhoff.github.io/notes/cuda_architecture/</link>
      <pubDate>Mon, 08 Jan 2024 20:49:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/cuda_architecture/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#architecture&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Architecture&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#block-scheduling&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Block Scheduling&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#synchronization&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Synchronization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#warps&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Warps&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#control-divergence&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Control Divergence&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#warp-scheduling&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Warp Scheduling&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#resource-partitioning&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Resource Partitioning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#dynamic-launch-configurations&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Dynamic Launch Configurations&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#the-takeaway&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;The Takeaway&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;&#xA;&lt;p&gt;A GPU consists of chip that is composed of several &lt;strong&gt;streaming multiprocessors&lt;/strong&gt; (SMs). Each SM has a number of cores that execute instructions in parallel. The H100, seen below, has 144 SMs (you can actually count them by eye). Each SM has 128 FP32 cores for a total of 18,432 cores. Historically, CUDA has used DDR memory, but newer architectures use high-bandwidth memory (HBM). This is closely integrated with the GPU for faster data transfer.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multidimensional Grids and Data</title>
      <link>https://ajdillhoff.github.io/notes/multidimensional_grids_and_data/</link>
      <pubDate>Fri, 05 Jan 2024 11:56:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/multidimensional_grids_and_data/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#summary&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Summary&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#multidimensional-grid-organization&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Multidimensional Grid Organization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#example-color-to-grayscale&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Example: Color to Grayscale&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#no-longer-embarrassing-overlapping-data&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;No longer embarrassing: overlapping data&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#matrix-multiplication&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Matrix Multiplication&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#what-s-next&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;What&amp;rsquo;s Next?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;&#xA;&lt;p&gt;The CUDA Programming model allows us to organize our data in a multidimensional grid. The purpose of this is primarily for our own convenience, but it also allows us to take advantage of the GPU&amp;rsquo;s memory hierarchy. In Lab 0, we only required a single dimension for our grid as well as each block since the input was a vector. When performing computations on multidimensional data like matrices, we can match the dimensions of our launch configuration to the dimensions of our data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Heterogeneous Data Parallel Computing</title>
      <link>https://ajdillhoff.github.io/notes/heterogeneous_data_parallel_computing/</link>
      <pubDate>Sat, 30 Dec 2023 14:41:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/heterogeneous_data_parallel_computing/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#summary&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Summary&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#cuda-c-programs&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;CUDA C Programs&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#example-vector-addition&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Example: Vector Addition&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#error-checking&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Error Checking&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&#xA;&#xA;&#xA;&#xA;  &#xA;  &#xA;&#xA;  &#xA;  &#xA;&#xA;&#xA;&#xA;&#xA;&#xA;  &#xA;&#xA;&#xA;&lt;div class=&#34;notice info&#34;&gt;&#xA;  &lt;div class=&#34;notice-head&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; fill=&#34;none&#34; viewBox=&#34;0 0 24 24&#34; stroke-width=&#34;1.5&#34; stroke=&#34;currentColor&#34; width=&#34;22&#34; height=&#34;22&#34;&gt;&#xA;        &lt;path stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; d=&#34;m11.25 11.25.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0 9 9 0 0 1 18 0Zm-9-3.75h.008v.008H12V8.25Z&#34; /&gt;&#xA;      &lt;/svg&gt;&lt;p&gt;Terms &amp;amp; Concepts&lt;/p&gt;</description>
    </item>
    <item>
      <title>Introduction to GPGPU Programming</title>
      <link>https://ajdillhoff.github.io/notes/introduction_to_gpgpu_programming/</link>
      <pubDate>Wed, 20 Dec 2023 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/introduction_to_gpgpu_programming/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#structure-of-the-course&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Structure of the Course&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#heterogeneous-parallel-computing&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Heterogeneous Parallel Computing&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#measuring-speedup&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Measuring Speedup&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#gpu-programming-history&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;GPU Programming History&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#applications&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Applications&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#what-to-expect-from-this-course&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;What to expect from this course&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;structure-of-the-course&#34;&gt;Structure of the Course&lt;/h2&gt;&#xA;&lt;p&gt;The primary of this goal is of course to learn how to program GPUs. A key skill that will be developed is the ability to think in parallel. We will start with simple problems that are &lt;em&gt;embarrassingly parallel&lt;/em&gt; and then move on to more complex problems that require synchronization. One of the biggest challenges will be in converting processes that are simple to reason about in serial to parallel processes.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
