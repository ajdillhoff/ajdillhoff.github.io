<!DOCTYPE html>
<html lang="en-us">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academia 4.3.1">
  <meta name="theme-name" content="academia-hugo"/>

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Alex Dillhoff">

  
  
  
    
  
  <meta name="description" content="How do LLMs read and process the high dimensional landscape of text efficiently? Presented as a workshop at UTA&#39;s Datathon on April 13, 2024.">

  
  <link rel="alternate" hreflang="en-us" href="http://localhost:1313/articles/the-language-of-llms/">

  


  

  
  
  
  <meta name="theme-color" content="#fc6f5c">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Open+Sans|Roboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academia.min.b246554d075350d61b44c126dfbcbe05.css">

  
    
    
    
    
      
    
    
    
    <link rel="stylesheet" href="/css/academia.a75a9b8a9a725a2157c0c5b929a3d18b.css">
  

  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="http://localhost:1313/articles/the-language-of-llms/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Alex Dillhoff">
  <meta property="og:url" content="http://localhost:1313/articles/the-language-of-llms/">
  <meta property="og:title" content="The Language of LLMs | Alex Dillhoff">
  <meta property="og:description" content="How do LLMs read and process the high dimensional landscape of text efficiently? Presented as a workshop at UTA&#39;s Datathon on April 13, 2024."><meta property="og:image" content="http://localhost:1313/img/icon-192.png">
  <meta property="twitter:image" content="http://localhost:1313/img/icon-192.png"><meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2024-04-11T00:00:00-05:00">
  
  <meta property="article:modified_time" content="2024-04-11T00:00:00-05:00">
  

  


  





  <title>The Language of LLMs | Alex Dillhoff</title>

  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>



</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Alex Dillhoff</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation"><span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">
      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/notes/"><span>Brain Dump</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

      

        

        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>
    </div>
  </div>
</nav>


  <article class="article py-5" itemscope itemtype="http://schema.org/Article">

  












    

    
    
    
    <div class="article-container py-3">
      <h1 itemprop="name">The Language of LLMs</h1>

      

      
      



<meta content="2024-04-11 00:00:00 -0500 CDT" itemprop="datePublished">
<meta content="2024-04-11 00:00:00 -0500 CDT" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/alex-dillhoff/">Alex Dillhoff</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>Apr 11, 2024</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    6 min read
  </span>
  

  
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=http://localhost:1313/articles/the-language-of-llms/&amp;text=The%20Language%20of%20LLMs" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=http://localhost:1313/articles/the-language-of-llms/&amp;t=The%20Language%20of%20LLMs" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=The%20Language%20of%20LLMs&amp;body=http://localhost:1313/articles/the-language-of-llms/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=http://localhost:1313/articles/the-language-of-llms/&amp;title=The%20Language%20of%20LLMs" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=The%20Language%20of%20LLMs%20http://localhost:1313/articles/the-language-of-llms/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=http://localhost:1313/articles/the-language-of-llms/&amp;title=The%20Language%20of%20LLMs" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

      














      
      
    </div>
  </div>
</div>

  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      <p>The accompanying Colab notebook is <a href="https://colab.research.google.com/drive/1Ch5wCSkYxXU6AAntrWH731Qk4-oE2c1P?usp=sharing" target="_blank" rel="noopener noreferrer">available here.</a></p>
<p>Large Language Models like ChatGPT have rapidly become ubiquitous tools that enhance productivity, creativity, and even decision-making processes across various domains. Their ability to generate human-like text, comprehend complex instructions, and provide informative responses has captivated the imagination of users worldwide. This paragraph was generated by an LLM (and edited by me).</p>
<p>This workshop is for those that are curious as to how these models <strong>interpret</strong> the input. By the end of this hour, you will hopefully be able to answer the following questions, among others:</p>
<ul>
<li>How do Large Language Models <strong>read</strong> and process text?</li>
<li>Why are LLMs good at complex tasks, but seem to perform poorly on seemingly simple tasks like spelling or arithmetic?</li>
<li>How does an LLM understand what it is processing?</li>
</ul>
<h2 id="agenda">Agenda</h2>
<ol>
<li>Tokenization</li>
<li>Unicode byte encodings</li>
<li>Byte Pair Encoding (BPE)</li>
<li>Embeddings</li>
</ol>
<h2 id="tokenization">Tokenization</h2>
<p>Tokenization is the process of transforming a sequence of characters into a sequence of tokens. A token is a unit of text that we treat as a single entity. For example, in English, a token could be a word, a sentence, or a paragraph. In programming languages, a token could be a variable name, a keyword, or a string.</p>
<p>Before we get started, let&rsquo;s check out a <a href="https://tiktokenizer.vercel.app" target="_blank" rel="noopener noreferrer">live demonstration of tokenization.</a></p>
<p>Consider the input prompt below. <em>It isn&rsquo;t likely that you would have mixed emoji and code in a single text file, but it serves as a good example for tokenization.</em></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>Why does my code 💥 with a segmentation fault?
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>int main() {
</span></span><span style="display:flex;"><span>    int *arr = NULL;
</span></span><span style="display:flex;"><span>    scanf(&#34;%d&#34;, arr);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    return 0;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p><strong>At the most basic level, how is this text represented in a computer?</strong></p>
<p>These characters are represented as encodings such as ASCII or Unicode. For the purposes of the rest of this article, we will assume the input is represented using Unicode.</p>
<h3 id="unicode-byte-encodings">Unicode Byte Encodings</h3>
<p>If we were to print out the unicode values of the prompt above, we would get the following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>[10, 87, 104, 121, 32, 100, 111, 101, 115, 32, 109, 121, 32, 99, 111, 100, 101, 32, 128165, ...]
</span></span></code></pre></div><p>Most of the values displayed in the previous cell are the same for ASCII. The emoji value has a very large number and can easily be spotted in the list.</p>
<p><strong><strong>Is that it? Is this how the input is fed into the model?</strong></strong></p>
<p>This encoding is done at the character-level. What other types of encodings are there?</p>
<ul>
<li>Character encoding</li>
<li>Word encoding</li>
<li>Sub-word encoding</li>
</ul>
<p><strong><strong>What is the difference between them? Why would we pick one over another?</strong></strong></p>
<h3 id="character-encoding">Character Encoding</h3>
<p>Character encoding converts each character into a unique integer. This is by far the simplest form of tokenization and has the benefit of a compact vocabulary. However, it is not able to effectively compress any common subsequences in the input. This leads to much larger sequences and longer training times.</p>
<p>The biggest downside to this approach is that the individual characters are not very informative on a semantic level. For example, the word &ldquo;cat&rdquo; would be represented as three separate tokens, &lsquo;c&rsquo;, &lsquo;a&rsquo;, and &rsquo;t&rsquo;. If someone were to present you a single letter without context, you probably would likely not be able to understand the point of the message.</p>
<h3 id="word-encoding">Word Encoding</h3>
<p>Word encoding is a step up from character encoding. This encoding directly captures the semantic meaning of words and is a fine choice for text classification and sentiment analysis. The sequences formed are much shorter since every word can be converted into a unique token.</p>
<p>The vocabulary size is very large since individual tokens cannot be broken down or recombined in new contexts. It also struggles with out-of-vocabulary words, since there are no base tokens to build upon.</p>
<h3 id="sub-word-encoding">Sub-word Encoding</h3>
<p>Sub-word encoding is a compromise between character and word encoding. It is able to capture the semantic meaning of words and can be broken down into smaller tokens. This allows for the model to generalize better to unseen words and phrases. Most large language models use sub-word encoding.</p>
<h2 id="byte-pair-encoding--bpe">Byte Pair Encoding (BPE)</h2>
<p><a href="https://en.wikipedia.org/wiki/Byte_pair_encoding" target="_blank" rel="noopener noreferrer">Byte Pair Encoding</a> is a sub-word encoding technique that was originally designed for data compression. It is a simple algorithm that iteratively merges the most frequent pair of bytes in a sequence. This process is repeated until a predefined vocabulary size is reached.</p>
<p>The algorithm is as follows:</p>
<ol>
<li>Initialize the vocabulary with all the characters in the input.</li>
<li>Count the frequency of all pairs of characters in the vocabulary.</li>
<li>Merge the most frequent pair of characters.</li>
<li>Update the vocabulary with the merged pair.</li>
<li>Repeat steps 2-4 until the vocabulary size reaches a predefined limit.</li>
</ol>
<h2 id="embeddings">Embeddings</h2>
<p>A word embedding is a learned representation of text in which semantically similar words are mapped to nearby points in the embedding space. Since they are represented as vectors, all vector operations can be applied to them. This allows for the model to learn relationships between words and phrases, quantify their similarities and differences, and encode higher-level context information.</p>
<p>Embeddings can be learned independently or jointly with the model. For example, the Word2Vec model learns embeddings using an unsupervised approach. It predicts the context of a word given its surrounding words. The embeddings are then used as input to a downstream task (<a href="#citeproc_bib_item_3">Mikolov et al. 2013</a>).</p>
<p>LLMs typically train embeddings jointly with the model. This allows them to learn embeddings for sentences, paragraphs, or even whole documents.</p>
<h3 id="creating-an-embedding-layer">Creating an embedding layer</h3>
<p>We can use libraries such as PyTorch to create a learnable embedding layer. The code below creates an embedding layer that converts each individual token into a `1024` dimensional embedded layer.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>token_embedding <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(vocab_size, <span style="color:#ae81ff">1024</span>)
</span></span><span style="display:flex;"><span>prompt_embedded <span style="color:#f92672">=</span> token_embedding(torch<span style="color:#f92672">.</span>LongTensor(encode(prompt)))
</span></span><span style="display:flex;"><span>print(prompt_embedded<span style="color:#f92672">.</span>shape)
</span></span></code></pre></div><h3 id="training-the-embeddings">Training the embeddings</h3>
<p>Our corpus of a single C file is far too small to learn anything meaningful. Learning an embedding space requires a lot data and compute power. We can instead look at pre-trained embeddings. Huggingface has a large collection of pre-trained models that can be used for a variety of tasks. The accompanying notebook uses embeddings from <code>SentenceTransformer</code> to demonstrate how embeddings can be used in practice.</p>
<h2 id="sentence-embeddings">Sentence Embeddings</h2>
<p>To demonstrate the power of embeddings, we will close out the workshop by reviewing sentence embeddings. BERT (<a href="#citeproc_bib_item_1">Devlin et al. 2019</a>) and RoBERTa (<a href="#citeproc_bib_item_2">Liu et al. 2019</a>) are LLMs that perform tasks such as semantic textual similarity. They both require that whole sentences be input, resulting in a very expensive computation.</p>
<p>Sentence-BERT proposed an architecture that would embed these into meaningful embeddings that could be easily compared with vector operations (<a href="#citeproc_bib_item_4">Reimers and Gurevych 2019</a>).</p>
<p>In the cells below, we will use Huggingface (huggingface.co) to download and use a pre-trained sentence transformer. This particular one was trained on <strong><strong>1,170,060,424</strong></strong> sentence pairs.</p>
<h2 id="references">References</h2>
<style>.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}</style><div class="csl-bib-body">
  <div class="csl-entry"><a id="citeproc_bib_item_1"></a>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv. <a href="https://doi.org/10.48550/arXiv.1810.04805">https://doi.org/10.48550/arXiv.1810.04805</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_2"></a>Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” arXiv. <a href="https://doi.org/10.48550/arXiv.1907.11692">https://doi.org/10.48550/arXiv.1907.11692</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_3"></a>Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” arXiv. <a href="http://arxiv.org/abs/1301.3781">http://arxiv.org/abs/1301.3781</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_4"></a>Reimers, Nils, and Iryna Gurevych. 2019. “Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks.” arXiv. <a href="https://doi.org/10.48550/arXiv.1908.10084">https://doi.org/10.48550/arXiv.1908.10084</a>.</div>
</div>

    </div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/article/">article</a>
  
  <a class="badge badge-light" href="/tags/machine-learning/">machine learning</a>
  
  <a class="badge badge-light" href="/tags/llms/">llms</a>
  
</div>



    
      








  
  
    
  
  





  
  
  
    
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="http://localhost:1313/">Alex Dillhoff</a></h5>
      <h6 class="card-subtitle">Senior Lecturer</h6>
      <p class="card-text" itemprop="description">&quot;If we understood the world, we would realize that there is a logic of harmony underlying its manifold apparent dissonances.&quot; - Jean Sibelius</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="/#contact" >
              <i class="fas fa-envelope"></i>
            </a>
          </li>
        
          
          
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://scholar.google.co.uk/citations?user=UlLhCtkAAAAJ" target="_blank" rel="noopener">
              <i class="ai ai-google-scholar"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/ajdillhoff" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/notes/bag_of_visual_words/">Bag of Visual Words</a></li>
          
          <li><a href="/notes/pretraining_large_language_models/">Pretraining Large Language Models</a></li>
          
          <li><a href="/notes/gradient_boosting/">Gradient Boosting</a></li>
          
          <li><a href="/articles/intro_to_hmms/">An Introduction to Hidden Markov Models for Gesture Recognition</a></li>
          
          <li><a href="/notes/bias_and_variance/">Bias and Variance</a></li>
          
        </ul>
      </div>
      
    

    

    

  </div>
</article>

      

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
      

      
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    

    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academia.min.6c2ba2801d406881b3c2277043cedd76.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  <div class="container">
    <div class="row align-items-center">
      <div class="col-md-6 mb-4 mb-md-0">
        
        <p class="powered-by">
          <a href="/privacy/">Privacy Policy</a>
        </p>
        
        <p class="mb-0">
          Copyright © 2025 &middot; 
          Powered by
          <a href="https://gethugothemes.com" target="_blank" rel="noopener">Gethugothemes</a>
        </p>
      </div>
      <div class="col-md-6">
        <ul class="list-inline network-icon text-right mb-0">
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="list-inline-item">
            <a href="https://github.com/ajdillhoff" target="_blank" rel="noopener" title="My GitHub"><i class="fab fa-github" aria-hidden="true"></i></a>
          </li>
          
          
          
          
          
          
          
          
          
          
          <li class="list-inline-item">
            <a href="https://github.com/ajdillhoff" target="_blank" rel="noopener" title="My GitHub"><i class="ai ai-google-scholar" aria-hidden="true"></i></a>
          </li>
          
        </ul>
      </div>
    </div>
  </div>
</footer>
  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
