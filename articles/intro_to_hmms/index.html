<!DOCTYPE html>
<html lang="en-us">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academia 4.3.1">
  <meta name="theme-name" content="academia-hugo"/>

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Alex Dillhoff">

  
  
  
    
  
  <meta name="description" content="Hidden Markov Models provide a way of modeling the dynamics of sequential information. They have been used for speech recognition, part-of-speech tagging, machine translation, handwriting recognition, and, as we will see in this article, gesture recognition.">

  
  <link rel="alternate" hreflang="en-us" href="http://localhost:1313/articles/intro_to_hmms/">

  


  

  
  
  
  <meta name="theme-color" content="#fc6f5c">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Open+Sans|Roboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academia.min.b246554d075350d61b44c126dfbcbe05.css">

  
    
    
    
    
      
    
    
    
    <link rel="stylesheet" href="/css/academia.a75a9b8a9a725a2157c0c5b929a3d18b.css">
  

  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="http://localhost:1313/articles/intro_to_hmms/">

  
  
  
  
    
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Alex Dillhoff">
  <meta property="og:url" content="http://localhost:1313/articles/intro_to_hmms/">
  <meta property="og:title" content="An Introduction to Hidden Markov Models for Gesture Recognition | Alex Dillhoff">
  <meta property="og:description" content="Hidden Markov Models provide a way of modeling the dynamics of sequential information. They have been used for speech recognition, part-of-speech tagging, machine translation, handwriting recognition, and, as we will see in this article, gesture recognition."><meta property="og:image" content="http://localhost:1313/articles/intro_to_hmms/featured.png">
  <meta property="twitter:image" content="http://localhost:1313/articles/intro_to_hmms/featured.png"><meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2023-07-15T00:00:00-05:00">
  
  <meta property="article:modified_time" content="2023-07-15T00:00:00-05:00">
  

  


  





  <title>An Introduction to Hidden Markov Models for Gesture Recognition | Alex Dillhoff</title>

  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>



</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Alex Dillhoff</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation"><span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">
      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/notes/"><span>Brain Dump</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

      

        

        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>
    </div>
  </div>
</nav>


  <article class="article py-5" itemscope itemtype="http://schema.org/Article">

  













<div class="container split-header">
  <div class="row justify-content-center">
    <div class="col-lg-8">
        <img class="img-fluid w-100" src="/articles/intro_to_hmms/featured_hu9ea2252f821ae10782c26cd947831a28_980451_680x500_fill_q90_lanczos_smart1_3.png" itemprop="image" alt="">
        
    </div>
    <div class="col-lg-8">
      <h1 itemprop="name">An Introduction to Hidden Markov Models for Gesture Recognition</h1>

      

      



<meta content="2023-07-15 00:00:00 -0500 CDT" itemprop="datePublished">
<meta content="2023-07-15 00:00:00 -0500 CDT" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/alex-dillhoff/">Alex Dillhoff</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>Jul 15, 2023</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    11 min read
  </span>
  

  
  
  
  
  

  
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=http://localhost:1313/articles/intro_to_hmms/&amp;text=An%20Introduction%20to%20Hidden%20Markov%20Models%20for%20Gesture%20Recognition" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=http://localhost:1313/articles/intro_to_hmms/&amp;t=An%20Introduction%20to%20Hidden%20Markov%20Models%20for%20Gesture%20Recognition" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=An%20Introduction%20to%20Hidden%20Markov%20Models%20for%20Gesture%20Recognition&amp;body=http://localhost:1313/articles/intro_to_hmms/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=http://localhost:1313/articles/intro_to_hmms/&amp;title=An%20Introduction%20to%20Hidden%20Markov%20Models%20for%20Gesture%20Recognition" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=An%20Introduction%20to%20Hidden%20Markov%20Models%20for%20Gesture%20Recognition%20http://localhost:1313/articles/intro_to_hmms/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=http://localhost:1313/articles/intro_to_hmms/&amp;title=An%20Introduction%20to%20Hidden%20Markov%20Models%20for%20Gesture%20Recognition" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

      














    </div>
    
    </div>
  </div>
</div>

  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      <h2 id="introduction">Introduction</h2>
<p>Hidden Markov Models provide a way of modeling the dynamics of sequential information. They have been used for speech recognition, part-of-speech tagging, machine translation, handwriting recognition, and, as we will see in this article, gesture recognition.</p>
<p>Consider a somewhat practical use-case: you are going to throw a party with a meticulously curated playlist. You would rather not let anyone have the remote as it might get lost, and letting anyone interrupt the playlist with their own selections may derail the entire event. However, you still want to give your guests the ability to control the volume and skip back and forth between tracks in the playlist. We will also assume that guests will use change tracks and control the volume responsibly.</p>
<p>The solution to this problem is to implement a gesture recognition system to identify simple hand motions. In this case, we only have to model 4 separate gestures: VolumeUp, VolumeDown, PrevTrack, NextTrack. Since the motions are temporal in nature, we can model each gesture using Hidden Markov Models. First, we need to cover a bit of background on what a Hidden Markov Model actually is.</p>
<h2 id="background">Background</h2>
<ul>
<li>First, introduce Markov Chains</li>
<li>Then the Markov assumption</li>
</ul>
<p>At the core of our problem, we want to model a distribution over a sequence of states. Consider a sequence of only 3 states \(p(x_1, x_2, x_3)\). The full computation of this can be done using the chain rule of probability:</p>
<p>\[
p(x_1, x_2, x_3) = p(x_1) p(x_2 | x_1) p(x_3 | x_1, x_2).
\]</p>
<p>If the random variables of our problem are not conditionally independent, the complexity of calculating this is exponential in the number of random variables.</p>
<p>The <strong>Markov</strong> in Hidden Markov Models addresses this complexity. The <strong>Markov Assumption</strong> states that the probability of an event at time \(t\) is conditioned <em>only</em> on the previously observed event: \(p(x_t | x_{t-1})\). This is compactly represented with a graphical model, as seen in figure <strong>TODO</strong>.</p>
<p><strong>TODO: Figure of basic Markov Chain</strong></p>
<p>The <strong>hidden</strong> qualifier comes from the fact that the data we wish to model was generated from some underlying process that is not directly observable. A classic example for HMMs uses the weather. Imagine you had a log which had the number of water bottles a person had drank per day over the entire year. To make the problem slightly more difficult, the log entries were not associated with a date. It is reasonable to say that the amount of water a person drinks is influenced by how hot or cold it is on a particular day. So, the <strong>hidden state</strong> in this case is the weather: hot or cold. We can model this with an HMM by establishing that the amount of water (<strong>observed state</strong>) is conditioned on the weather (<strong>hidden state</strong>). Figure <strong>TODO</strong> shows this HMM graphically.</p>






<figure>

<img src="/ox-hugo/2023-07-20_19-12-54_screenshot.png" alt="&lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;An HMM with 4 states and 2 observation symbols (y_1) or (y_2)." >



<figcaption data-pre="Figure " data-post=":" >
  
  <p>
    <span class="figure-number">Figure 1: </span>An HMM with 4 states and 2 observation symbols (y_1) or (y_2).
    
    
    
  </p> 
</figcaption>

</figure>

<p>Formally, a Hidden Markov Model is defined by</p>
<ul>
<li>The number of hidden states \(N\).</li>
<li>A transition probability matrix \(A \in \mathbb{R}^{N \times N}\), where \(a_{ij} = p(z_t = j | z_{t-1} = i)\).</li>
<li>An observation symbol probability distribution \(B = \{b_j(k)\} = p(\mathbf{x}_t = k | z_t = j)\).</li>
<li>An initial state distribution \(\pi_i = p(z_t = i)\).</li>
</ul>
<p>The trainable parameters of our model are \(\lambda = (A, B, \pi)\).</p>
<h2 id="functions-of-an-hmm">Functions of an HMM</h2>
<p>Given the basic definition of what an HMM is, how can we train the parameters defined in \(\lambda\). If we somehow already knew the parameters, how can we extract useful information from the model? Depending on our task, we can use HMMs to answer many important questions:</p>
<ul>
<li><strong>Filtering</strong> computes \(p(z_t | \mathbf{x}_{1:t})\). That is, we are computing this probability as new samples come in up to time \(t\).</li>
<li><strong>Smoothing</strong> is accomplished when we have all the data in the sequence.
This is expressed as \(p(z_t|\mathbf{x}_{1:T})\).</li>
<li><strong>Fixed lag smoothing</strong> allows for a trade off between accuracy and delay. It is useful in cases where we might not have the full sequence, but we wish to compute \(p(z_{t-l}|\mathbf{x}_{1:t})\) for some \(l &gt; 0\).</li>
<li><strong>Predictions</strong> are represented as \(p(z_{t+h}|\mathbf{x}_{1:t})\), where \(h &gt; 0\).</li>
<li><strong>MAP estimation</strong> yields the most probably state sequence \(\text{arg}\max_{\mathbf{z}_{1:T}}p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).</li>
<li>We can sample the <strong>posterior</strong> \(p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).</li>
<li>We can also compute \(p(\mathbf{x}_{1:T})\) by summing up over all hidden paths. This is useful for classification tasks.</li>
</ul>
<p>Of course not all of these functions make sense for every possible task, more on that later. This article is not meant to be an exhaustive resource for all HMM functions; we will only look at the tasks necessary to train and use HMMs for isolated gesture recognition <strong>TODO: offer additional reading suggestions</strong>.</p>
<h2 id="data-processing">Data Processing</h2>
<p>As far as the efficacy of our model goes, how we process the data is the most important. Our system will start with a camera that records our guests performing one of the four simple motions. For simplicity, let&rsquo;s pretend that the camera has an onboard chip that detects the 2D centroids of the left hand for each frame. That helps a lot, but there is still the problem of isolating a group of frames based on when the user wanted to start and finish the command. Assuming we have a solution for both of these problems, we still need to take into account that users will gesture at different speeds. Since all of these problems are challenging in their own right, we will assume the computer vision fairy has taken care of this for us.</p>
<p>Each gesture in our dataset consists of 30 \((x, y)\) locations of the center of the left hand with respect to image coordinates. Even with this simplified data, we have another problem: different users may gesture from different locations. The hand locations for one user performing the <code>VolumeUp</code> gesture may be vastly different from another. This isn&rsquo;t too bad to deal with. We could normalize or training data by subtracting the location of the hand in the first frame from the gesture. That way every input would start at \((0, 0)\). We can simplify this even further by using <strong>relative motion states</strong>.</p>
<h3 id="relative-motion-states">Relative Motion States</h3>
<p>Relative motion states discretize our data, thus simplifying the input space. The idea is quite simple: if the hand moved to the right relative to the previous frame, we assign \(x = 1\) for that frame. If it moved to the left, assign \(x = -1\). If it didn&rsquo;t move at all, or did not move a significant amount, assign \(x = 0\). We apply similar rules for the \(y\) locations as well. The <strong>TODO: figure</strong> below shows the relative motion grid.</p>
<p>Besides greatly simplifying our input space, meaning we can use a simple categorical distribution to model these observations, we no longer have to worry about the discrepency between where each user performed the gesture.</p>
<h2 id="modeling-a-gesture">Modeling a Gesture</h2>
<p>Our system will consist of 4 HMM models to model the dynamics of each gesture. To determine which gesture was performed, we will given our input sequence to each one and have it compute \(p(\mathbf{x}_{1:T}; \lambda_i)\), the probability of the observation given the parameters of model \(i\). Whichever model gives the high probability wins.</p>
<p><strong>TODO</strong></p>
<ol>
<li>Describe EM at a high level, show the breakdown of probabilities that need to be known</li>
<li>Go into forward-backwards</li>
<li>Go back to EM and plug them in</li>
</ol>
<h3 id="training-expectation-maximization">Training: Expectation-Maximization</h3>
<p>If we cannot observe the hidden states directly, how are we supposed to update the model parameters \(\lambda = (A, B, \pi)\)? We may not have all of the information, but we do have <em>some</em> information. We can use that to fill in the missing values with what we would expect them to be given what we already know. Then, we can update our parameters using those expected values. This is accomplished through a two-stage algorithm called <strong>Expectation-Maximization</strong>. Those familiar with k-Nearest Neighbors should already be familiar with this process.</p>
<h4 id="updating-with-perfect-information">Updating with Perfect Information</h4>
<p>It is useful to know how we would update our parameters assuming we had perfect information. If the hidden states were fully observable, then updating our model parameters would be as straightforward as computing the maximum likelihood estimates.
For \(A\) and \(\pi\), we first tally up the following counts:</p>
<p>\[
\hat{a}_{ij} = \frac{N_{ij}}{\sum_j N_{ij}},
\]</p>
<p>the number of times we expect to transition from \(i\) to \(j\) divided by the number of times we transition from \(i\) to any other state. Put simply, this computes the expected transitions from \(i\) to \(j\) normalized by all the times we expect to start in state \(i\).</p>
<p>For \(\pi\), we have</p>
<p>\[
\hat{\pi_i} = \frac{N_i}{\sum_i N_i},
\]</p>
<p>the number of times we expect to start in state \(i\) divided by the number of times we start in any other state.</p>
<p>Estimating the parameters for \(B\) depends on which distribution we are using for our observation probabilities.
For a multinomial distribution, we would compute the number of times we are in state \(j\) and observe a symbol \(k\) divided by the number of times we are in state \(j\):</p>
<p>\[
\hat{b}_{jk} = \frac{N_{jk}}{N_k},
\]</p>
<p>where</p>
<p>\[
N_{jk} = \sum_{i=1}^N \sum_{t=1}^T \mathbb{1} (z_{i, t}=j, x_{i, t}=k).
\]</p>
<p>It is also common to model our emission probability using a Normal distribution. We can even use a parameterized model like a neural network. <strong>TODO: provide links to examples of these</strong></p>
<h4 id="updating-with-missing-information">Updating with Missing Information</h4>
<p>Now to the real problem: fill in our missing information using our observable data and the current parameter estimates. There are two important statistics that we need to compute, called the <strong>sufficient statistics</strong>.</p>
<ol>
<li>The expected number of transitions from \(i\) to \(j\).</li>
<li>The expected number of times we are transitioning from \(i\) to any other state.</li>
</ol>
<p>Both of these can be computed starting with the same probability <em>conditioned</em> on our observable data:</p>
<p>\[
p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T}).
\]</p>
<h3 id="forwards-backwards-algorithm">Forwards-Backwards Algorithm</h3>
<p>Computing joint distribution can be very computationally expensive. Fortunately for us, the Markov assumption along with operations on graphs open the door to a dynamic programming approach named the Forward-Backward algorithm.</p>
<p>The Forwards-Backwards Algorithm, also known as the <a href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm" target="_blank" rel="noopener noreferrer">Baum-Welch algorithm</a>, provides an effective solution to computing the joint described above. In fact, there are many useful distributions that can be computed with this algorithm such as the <strong>filtering</strong> and <strong>smoothing</strong> tasks.</p>
<h4 id="forward-probability">Forward Probability</h4>
<p>The forward probability, often denoted as \(\alpha\), represents the probability of ending up at a particular hidden state \(i\) at time \(t\) having seen the observations up to that time:</p>
<p>\[
\alpha_t(i) = p(z_t = i, \mathbf{x}_{1:t} | \lambda).
\]</p>
<p>This value is computed recursively starting from \(t=1\) and going forwards to \(t=T\).</p>
<p><strong>Initialization</strong></p>
<p>For \(t=1\), we calculate:</p>
<p>\[
\alpha_1(i) = \pi_i b_i(x_1),\quad 1 \leq i \leq N,
\]</p>
<p>where \(\pi_i\) is the initial probability of state \(i\) and \(b_i(x_1)\) is the emission probability of the first observation \(x_1\) given that we are in state \(i\).</p>
<p><strong>Recursion</strong></p>
<p>After that, we calculate the remaining \(\alpha_t(i)\) as follows:</p>
<p>\[
\alpha_{t+1}(j) = b_j(x_{t+1}) \sum_{i=1}^{N} \alpha_{t}(i)a_{ij},
\]</p>
<p>where \(N\) is the number of hidden states, and \(a_{ij}\) is the transition probability from state \(i\) to state \(j\).</p>
<h4 id="backward-probability">Backward Probability</h4>
<p>The backward probability, denoted as \(\beta\), gives the probability of observing the remaining observations from time \(t+1\) to \(T\) given that we are in state \(i\) at time \(t\):</p>
<p>\[
\beta_t(i) = p(\mathbf{x}_{t+1:T} | z_t = i, \lambda).
\]</p>
<p>Again, this is calculated recursively but this time starting from \(t=T\) and going backwards to \(t=1\).</p>
<p><strong>Initialization</strong></p>
<p>For \(t=T\), we initialize:</p>
<p>\[
\beta_T(i) = 1, \forall i.
\]</p>
<p><strong>Recursion</strong></p>
<p>Then we calculate the remaining \(\beta_t(i)\) as:</p>
<p>\[
\beta_{t}(i) = \sum_{j=1}^{N} a_{ij}b_j(x_{t+1})\beta_{t+1}(j).
\]</p>
<h4 id="calculating-the-sufficient-statistics">Calculating the Sufficient Statistics</h4>
<p>With these two sets of probabilities, we can calculate the two required sufficient statistics as follows:</p>
<ol>
<li>The expected number of transitions from \(i\) to \(j\):</li>
</ol>
<p>\[
\frac{\sum_{t=1}^{T-1} \alpha_t(i) a_{ij} b_j(x_{t+1}) \beta_{t+1}(j)}{P(X|\lambda)}
\]</p>
<ol>
<li>The expected number of times we are transitioning from \(i\) to any other state:</li>
</ol>
<p>\[
\frac{\sum_{t=1}^{T-1} \alpha_t(i) \beta_t(i)}{P(X|\lambda)}
\]</p>
<p>Where \(P(X|\lambda)\) is the total probability of the observations, calculated as:</p>
<p>\[
P(X|\lambda) = \sum_{i=1}^{N} \alpha_T(i)
\]</p>
<h4 id="how-does-this-give-us-p--z-t-i-z-t-plus-1-j-mathbf-x-1-t">How does this give us \(p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T})\)?</h4>
<p>To understand how the variables of the Forwards-Backwards algorithm relate to the original probabilities, we can express the term \(p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T})\) in terms of the original probability distributions in the HMM:</p>
<ul>
<li>\(\pi_i\) - the probability of starting in state \(i\),</li>
<li>\(a_{ij}\) - the probability of transitioning from state \(i\) to state \(j\),</li>
<li>\(b_j(x_t)\) - the probability that state \(j\) will emit observation \(x_t\).</li>
</ul>
<p>The joint probability \(p(z_t = i, z_{t+1} = j, \mathbf{x}_{1:T})\) would represent the probability of being in state \(i\) at time \(t\), moving to state \(j\) at time \(t+1\), and observing the sequence of emissions \(\mathbf{x}_{1:T}\). This can be factored as follows due to the Markov property:</p>
<p>\[
p(z_t = i, z_{t+1} = j, \mathbf{x}_{1:T}) = p(\mathbf{x}_{1:t}, z_t = i)p(z_{t+1} = j| z_t = i)p(\mathbf{x}_{t+1:T} | z_{t+1} = j, \mathbf{x}_{1:t}).
\]</p>
<p>Using our definitions of \(\alpha\) and \(\beta\), we can rewrite this in terms of our HMM quantities:</p>
<p>\[
p(z_t = i, z_{t+1} = j, \mathbf{x}_{1:T}) = \alpha_t(i)a_{ij}b_j(x_{t+1})\beta_{t+1}(j).
\]</p>
<p>Here, \(\alpha_t(i)\) represents \(p(\mathbf{x}_{1:t}, z_t = i)\), the joint probability of the observations until time \(t\) and being in state \(i\) at time \(t\), and \(\beta_{t+1}(j)\) represents \(p(\mathbf{x}_{t+1:T} | z_{t+1} = j)\), the probability of the observations from time \(t+1\) to \(T\) given we&rsquo;re in state \(j\) at time \(t+1\).</p>
<p>Then, to obtain \(p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T})\), we divide by \(p(\mathbf{x}_{1:T})\) to normalize the probabilities, which is the sum over all states of \(\alpha_T(i)\), or equivalently, the sum over all states of \(\beta_1(i)\pi_i b_i(x_1)\).</p>
<p>This gives us:</p>
<p>\[
p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T}) = \frac{\alpha_t(i)a_{ij}b_j(x_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^{N}\alpha_T(i)}.
\]</p>
<p>This is the same expression as before, but broken down in terms of the original HMM quantities and the forward and backward variables. This can also be explained through graph properties and operations. See <a href="https://cedar.buffalo.edu/~srihari/CSE574/Chap13/13.2.2-ForwardBackward.pdf" target="_blank" rel="noopener noreferrer">Sargur Srihari&rsquo;s excellent lecture slides</a> for more details.</p>
<h2 id="implementation-in-python">Implementation in Python</h2>
<h2 id="conclusion">Conclusion</h2>

    </div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/machine-learning/">machine learning</a>
  
</div>



    
      








  
  
    
  
  





  
  
  
    
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="http://localhost:1313/">Alex Dillhoff</a></h5>
      <h6 class="card-subtitle">Senior Lecturer</h6>
      <p class="card-text" itemprop="description">&quot;If we understood the world, we would realize that there is a logic of harmony underlying its manifold apparent dissonances.&quot; - Jean Sibelius</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="/#contact" >
              <i class="fas fa-envelope"></i>
            </a>
          </li>
        
          
          
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://scholar.google.co.uk/citations?user=UlLhCtkAAAAJ" target="_blank" rel="noopener">
              <i class="ai ai-google-scholar"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/ajdillhoff" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/notes/bias_and_variance/">Bias and Variance</a></li>
          
          <li><a href="/notes/sequential_minimal_optimization/">Sequential Minimal Optimization</a></li>
          
          <li><a href="/notes/boosting/">Boosting</a></li>
          
          <li><a href="/notes/decision_trees/">Decision Trees</a></li>
          
          <li><a href="/notes/camera_models/">Camera Models</a></li>
          
        </ul>
      </div>
      
    

    

    

  </div>
</article>

      

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
      

      
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    
    <script id="dsq-count-scr" src="//themefisher-template.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    

    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academia.min.6c2ba2801d406881b3c2277043cedd76.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  <div class="container">
    <div class="row align-items-center">
      <div class="col-md-6 mb-4 mb-md-0">
        
        <p class="powered-by">
          <a href="/privacy/">Privacy Policy</a>
        </p>
        
        <p class="mb-0">
          Copyright © 2024 &middot; 
          Powered by
          <a href="https://gethugothemes.com" target="_blank" rel="noopener">Gethugothemes</a>
        </p>
      </div>
      <div class="col-md-6">
        <ul class="list-inline network-icon text-right mb-0">
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="list-inline-item">
            <a href="https://github.com/ajdillhoff" target="_blank" rel="noopener" title="My GitHub"><i class="fab fa-github" aria-hidden="true"></i></a>
          </li>
          
          
          
          
          
          
          
          
          
          
          <li class="list-inline-item">
            <a href="https://github.com/ajdillhoff" target="_blank" rel="noopener" title="My GitHub"><i class="ai ai-google-scholar" aria-hidden="true"></i></a>
          </li>
          
        </ul>
      </div>
    </div>
  </div>
</footer>
  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
