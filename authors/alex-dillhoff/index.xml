<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alex Dillhoff</title>
    <link>https://ajdillhoff.github.io/authors/alex-dillhoff/</link>
    <description>Recent content on Alex Dillhoff</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; {year}</copyright>
    <lastBuildDate>Sat, 15 Jul 2023 00:00:00 -0500</lastBuildDate>
    
	    <atom:link href="https://ajdillhoff.github.io/authors/alex-dillhoff/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>An Introduction to Hidden Markov Models for Gesture Recognition</title>
      <link>https://ajdillhoff.github.io/articles/intro_to_hmms/</link>
      <pubDate>Sat, 15 Jul 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/articles/intro_to_hmms/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Hidden Markov Models provide a way of modeling the dynamics of sequential information. They have been used for speech recognition, part-of-speech tagging, machine translation, handwriting recognition, and, as we will see in this article, gesture recognition.&lt;/p&gt;
&lt;p&gt;Consider a somewhat practical use-case: you are going to throw a party with a meticulously curated playlist. You would rather not let anyone have the remote as it might get lost, and letting anyone interrupt the playlist with their own selections may derail the entire event. However, you still want to give your guests the ability to control the volume and skip back and forth between tracks in the playlist. We will also assume that guests will use change tracks and control the volume responsibly.&lt;/p&gt;
&lt;p&gt;The solution to this problem is to implement a gesture recognition system to identify simple hand motions. In this case, we only have to model 4 separate gestures: VolumeUp, VolumeDown, PrevTrack, NextTrack. Since the motions are temporal in nature, we can model each gesture using Hidden Markov Models. First, we need to cover a bit of background on what a Hidden Markov Model actually is.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;First, introduce Markov Chains&lt;/li&gt;
&lt;li&gt;Then the Markov assumption&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the core of our problem, we want to model a distribution over a sequence of states. Consider a sequence of only 3 states \(p(x_1, x_2, x_3)\). The full computation of this can be done using the chain rule of probability:&lt;/p&gt;
&lt;p&gt;\[
p(x_1, x_2, x_3) = p(x_1) p(x_2 | x_1) p(x_3 | x_1, x_2).
\]&lt;/p&gt;
&lt;p&gt;If the random variables of our problem are not conditionally independent, the complexity of calculating this is exponential in the number of random variables.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Markov&lt;/strong&gt; in Hidden Markov Models addresses this complexity. The &lt;strong&gt;Markov Assumption&lt;/strong&gt; states that the probability of an event at time \(t\) is conditioned &lt;em&gt;only&lt;/em&gt; on the previously observed event: \(p(x_t | x_{t-1})\). This is compactly represented with a graphical model, as seen in figure &lt;strong&gt;TODO&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TODO: Figure of basic Markov Chain&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;hidden&lt;/strong&gt; qualifier comes from the fact that the data we wish to model was generated from some underlying process that is not directly observable. A classic example for HMMs uses the weather. Imagine you had a log which had the number of water bottles a person had drank per day over the entire year. To make the problem slightly more difficult, the log entries were not associated with a date. It is reasonable to say that the amount of water a person drinks is influenced by how hot or cold it is on a particular day. So, the &lt;strong&gt;hidden state&lt;/strong&gt; in this case is the weather: hot or cold. We can model this with an HMM by establishing that the amount of water (&lt;strong&gt;observed state&lt;/strong&gt;) is conditioned on the weather (&lt;strong&gt;hidden state&lt;/strong&gt;). Figure &lt;strong&gt;TODO&lt;/strong&gt; shows this HMM graphically.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TODO: Figure of HMM&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Formally, a Hidden Markov Model is defined by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The number of hidden states \(N\).&lt;/li&gt;
&lt;li&gt;A transition probability matrix \(A \in \mathbb{R}^{N \times N}\), where \(a_{ij} = p(z_t = j | z_{t-1} = i)\).&lt;/li&gt;
&lt;li&gt;An observation symbol probability distribution \(B = \{b_j(k)\} = p(\mathbf{x}_t = k | z_t = j)\).&lt;/li&gt;
&lt;li&gt;An initial state distribution \(\pi_i = p(z_t = i)\).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The trainable parameters of our model are \(\lambda = (A, B, \pi)\).&lt;/p&gt;
&lt;h2 id=&#34;functions-of-an-hmm&#34;&gt;Functions of an HMM&lt;/h2&gt;
&lt;p&gt;Given the basic definition of what an HMM is, how can we train the parameters defined in \(\lambda\). If we somehow already knew the parameters, how can we extract useful information from the model? Depending on our task, we can use HMMs to answer many important questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Filtering&lt;/strong&gt; computes \(p(z_t | \mathbf{x}_{1:t})\). That is, we are computing this probability as new samples come in up to time \(t\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smoothing&lt;/strong&gt; is accomplished when we have all the data in the sequence.
This is expressed as \(p(z_t|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fixed lag smoothing&lt;/strong&gt; allows for a trade off between accuracy and delay. It is useful in cases where we might not have the full sequence, but we wish to compute \(p(z_{t-l}|\mathbf{x}_{1:t})\) for some \(l &amp;gt; 0\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Predictions&lt;/strong&gt; are represented as \(p(z_{t+h}|\mathbf{x}_{1:t})\), where \(h &amp;gt; 0\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MAP estimation&lt;/strong&gt; yields the most probably state sequence \(\text{arg}\max_{\mathbf{z}_{1:T}}p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;We can sample the &lt;strong&gt;posterior&lt;/strong&gt; \(p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;We can also compute \(p(\mathbf{x}_{1:T})\) by summing up over all hidden paths. This is useful for classification tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course not all of these functions make sense for every possible task, more on that later. This article is not meant to be an exhaustive resource for all HMM functions; we will only look at the tasks necessary to train and use HMMs for isolated gesture recognition &lt;strong&gt;TODO: offer additional reading suggestions&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;data-processing&#34;&gt;Data Processing&lt;/h2&gt;
&lt;p&gt;As far as the efficacy of our model goes, how we process the data is the most important. Our system will start with a camera that records our guests performing one of the four simple motions. For simplicity, let&amp;rsquo;s pretend that the camera has an onboard chip that detects the 2D centroids of the left hand for each frame. That helps a lot, but there is still the problem of isolating a group of frames based on when the user wanted to start and finish the command. Assuming we have a solution for both of these problems, we still need to take into account that users will gesture at different speeds. Since all of these problems are challenging in their own right, we will assume the computer vision fairy has taken care of this for us.&lt;/p&gt;
&lt;p&gt;Each gesture in our dataset consists of 30 \((x, y)\) locations of the center of the left hand with respect to image coordinates. Even with this simplified data, we have another problem: different users may gesture from different locations. The hand locations for one user performing the &lt;code&gt;VolumeUp&lt;/code&gt; gesture may be vastly different from another. This isn&amp;rsquo;t too bad to deal with. We could normalize or training data by subtracting the location of the hand in the first frame from the gesture. That way every input would start at \((0, 0)\). We can simplify this even further by using &lt;strong&gt;relative motion states&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;relative-motion-states&#34;&gt;Relative Motion States&lt;/h3&gt;
&lt;p&gt;Relative motion states discretize our data, thus simplifying the input space. The idea is quite simple: if the hand moved to the right relative to the previous frame, we assign \(x = 1\) for that frame. If it moved to the left, assign \(x = -1\). If it didn&amp;rsquo;t move at all, or did not move a significant amount, assign \(x = 0\). We apply similar rules for the \(y\) locations as well. The &lt;strong&gt;TODO: figure&lt;/strong&gt; below shows the relative motion grid.&lt;/p&gt;
&lt;p&gt;Besides greatly simplifying our input space, meaning we can use a simple categorical distribution to model these observations, we no longer have to worry about the discrepency between where each user performed the gesture.&lt;/p&gt;
&lt;h2 id=&#34;modeling-a-gesture&#34;&gt;Modeling a Gesture&lt;/h2&gt;
&lt;p&gt;Our system will consist of 4 HMM models to model the dynamics of each gesture. To determine which gesture was performed, we will given our input sequence to each one and have it compute \(p(\mathbf{x}_{1:T}; \lambda_i)\), the probability of the observation given the parameters of model \(i\). Whichever model gives the high probability wins.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TODO&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Describe EM at a high level, show the breakdown of probabilities that need to be known&lt;/li&gt;
&lt;li&gt;Go into forward-backwards&lt;/li&gt;
&lt;li&gt;Go back to EM and plug them in&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;training-expectation-maximization&#34;&gt;Training: Expectation-Maximization&lt;/h3&gt;
&lt;p&gt;If we cannot observe the hidden states directly, how are we supposed to update the model parameters \(\lambda = (A, B, \pi)\)? We may not have all of the information, but we do have &lt;em&gt;some&lt;/em&gt; information. We can use that to fill in the missing values with what we would expect them to be given what we already know. Then, we can update our parameters using those expected values. This is accomplished through a two-stage algorithm called &lt;strong&gt;Expectation-Maximization&lt;/strong&gt;. Those familiar with k-Nearest Neighbors should already be familiar with this process.&lt;/p&gt;
&lt;h4 id=&#34;updating-with-perfect-information&#34;&gt;Updating with Perfect Information&lt;/h4&gt;
&lt;p&gt;It is useful to know how we would update our parameters assuming we had perfect information. If the hidden states were fully observable, then updating our model parameters would be as straightforward as computing the maximum likelihood estimates.
For \(A\) and \(\pi\), we first tally up the following counts:&lt;/p&gt;
&lt;p&gt;\[
\hat{a}_{ij} = \frac{N_{ij}}{\sum_j N_{ij}},
\]&lt;/p&gt;
&lt;p&gt;the number of times we expect to transition from \(i\) to \(j\) divided by the number of times we transition from \(i\) to any other state. Put simply, this computes the expected transitions from \(i\) to \(j\) normalized by all the times we expect to start in state \(i\).&lt;/p&gt;
&lt;p&gt;For \(\pi\), we have&lt;/p&gt;
&lt;p&gt;\[
\hat{\pi_i} = \frac{N_i}{\sum_i N_i},
\]&lt;/p&gt;
&lt;p&gt;the number of times we expect to start in state \(i\) divided by the number of times we start in any other state.&lt;/p&gt;
&lt;p&gt;Estimating the parameters for \(B\) depends on which distribution we are using for our observation probabilities.
For a multinomial distribution, we would compute the number of times we are in state \(j\) and observe a symbol \(k\) divided by the number of times we are in state \(j\):&lt;/p&gt;
&lt;p&gt;\[
\hat{b}_{jk} = \frac{N_{jk}}{N_k},
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
N_{jk} = \sum_{i=1}^N \sum_{t=1}^T \mathbb{1} (z_{i, t}=j, x_{i, t}=k).
\]&lt;/p&gt;
&lt;p&gt;It is also common to model our emission probability using a Normal distribution. We can even use a parameterized model like a neural network. &lt;strong&gt;TODO: provide links to examples of these&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;updating-with-missing-information&#34;&gt;Updating with Missing Information&lt;/h4&gt;
&lt;p&gt;Now to the real problem: fill in our missing information using our observable data and the current parameter estimates. There are two important statistics that we need to compute, called the &lt;strong&gt;sufficient statistics&lt;/strong&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The expected number of transitions from \(i\) to \(j\).&lt;/li&gt;
&lt;li&gt;The expected number of times we are transitioning from \(i\) to any other state.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both of these can be computed starting with the same probability &lt;em&gt;conditioned&lt;/em&gt; on our observable data:&lt;/p&gt;
&lt;p&gt;\[
p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T}).
\]&lt;/p&gt;
&lt;h3 id=&#34;forwards-backwards-algorithm&#34;&gt;Forwards-Backwards Algorithm&lt;/h3&gt;
&lt;h2 id=&#34;implementation-in-python&#34;&gt;Implementation in Python&lt;/h2&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Boosting</title>
      <link>https://ajdillhoff.github.io/notes/boosting/</link>
      <pubDate>Wed, 23 Mar 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/boosting/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#todo&#34;&gt;TODO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adaboost&#34;&gt;AdaBoost&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h2 id=&#34;todo&#34;&gt;TODO&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Gradient Boosting&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Combining predictions from multiple sources is usually preferred to a single source.
For example, a medical diagnosis would carry much more weight if it was the result of a consensus of several experts.
This idea of prediction by consensus is a powerful way to improve classification and regression models.
In fact, good performance of a committee of models can be achieved even if each individual model is conceptually very simple.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Boosting&lt;/strong&gt; is one such way of building a committee of models for classification or regression and is popularly implemented by an algorithm called &lt;strong&gt;AdaBoost&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;adaboost&#34;&gt;AdaBoost&lt;/h2&gt;
&lt;p&gt;Given a dataset \(\{\mathbf{x}_i\}\) and target variables \(\{\mathbf{y}_i\}\), AdaBoost first initializes a set of weights corresponding to each data sample as \(w_i = \frac{1}{N}\).
At each step of the algorithm, a simple classifier, called a &lt;strong&gt;weak learner&lt;/strong&gt; is fit to the data.
The weights for each sample are adjusted based on the individual classifier&amp;rsquo;s performance.
If the sample was misclassified, the relative weight for that sample is increased.
After all classifiers have been fit, they are combined to form an ensemble model.&lt;/p&gt;
&lt;h3 id=&#34;the-algorithm&#34;&gt;The Algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Initialize data weights \({w_i}\) as \(w_i^{(1)} = \frac{1}{n}\) for \(i = 1, \dots, n\).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fit each weak learner \(j\) to the training data by minimizing the misclassification cost:&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^n w_i^{(j)} \mathbb{1}(f_j(\mathbf{x}_i) \neq \mathbf{y}_i)
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute a weighted error rate&lt;/p&gt;
&lt;p&gt;\[
\epsilon_j = \frac{\sum_{i=1}^n w_i^{(j)} \mathbb{1}(f_j(\mathbf{x}_i) \neq \mathbf{y}_i)}{\sum_{i=1}^n w_i^{(j)}}
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the weighted error rate to compute a weight for each classifier such that misclassified samples are given higher weight:&lt;/p&gt;
&lt;p&gt;\[
\alpha_j = \ln \bigg\{\frac{1 - \epsilon_j}{\epsilon_j}\bigg\}.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update the data weights for the next model in the sequence:&lt;/p&gt;
&lt;p&gt;\[
w_i^{j+1} = w_i^{j} \exp\{\alpha_j \mathbb{1}(f_j(\mathbf{x}_i \neq \mathbf{y}_i)\}.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once all weak learners are trained, the final model predictions are given by&lt;/p&gt;
&lt;p&gt;\[
Y_M(\mathbf{x}) = \text{sign} \Bigg(\sum_{j=1}^M \alpha_j f_j(\mathbf{x})\Bigg).
\]&lt;/p&gt;
&lt;h3 id=&#34;weak-learners&#34;&gt;Weak Learners&lt;/h3&gt;
&lt;p&gt;The weak learners can be any classification or regression model.
However, they are typically chosen to be very simple to account for training time.
For example, a complex deep learning model would be a poor choice for a weak learner.&lt;/p&gt;
&lt;p&gt;One example of a weak learner is a simple linear model like a &lt;a href=&#34;https://ajdillhoff.github.io/notes/perceptron/&#34;&gt;Perceptron&lt;/a&gt; or decision stump.
A standard implementation of AdaBoost uses a decision tree with depth 1, as observed in &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html?highlight=boost#sklearn.ensemble.AdaBoostClassifier&#34;&gt;sklearn&amp;rsquo;s implementation.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s put this together and walk through the first few steps of training an AdaBoost model using a decision stump as the weak learner. We will use a very simple dataset to keep the values easy to compute by hand.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Initial Data&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Weak Learner 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first learner is trained on the initial data and picks \(x_1 = 2.5\) as the split threshold.
Input where \(x_1 \leq 2.5\) is assigned to class 0 and all other samples are assigned class 1.
The data with this learner&amp;rsquo;s predictions are shown below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;th&gt;prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Error and weight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The error is simple enough to compute as all samples are currently weighted equally. Since two of the samples were misclassified, the error is the sum of their weights.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Total error&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(e_1 = 0.2 + 0.2 = 0.4\).&lt;/p&gt;
&lt;p&gt;The weight of the classifier can then be computed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classifier weight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(\alpha_1 = \frac{1}{2} \ln \big(\frac{1 - e_1}{e_1}\big) = 0.2027\).&lt;/p&gt;
&lt;p&gt;The weights of our data can now be updated using this value of \(\alpha_1\).
The weight of each example is updated by multiplying each correctly classifed sample by \(\exp\{-\alpha_1\}\) and each incorrectly classified sample by \(\exp\{\alpha\}\):&lt;/p&gt;
&lt;p&gt;\[
w_i^{j+1} = w_i^{j} \exp\{\alpha_j \mathbb{1}(f_j(\mathbf{x}_i \neq \mathbf{y}_i)\}.
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; You will notice that the equation above is different from the actual update rule that was applied to the weights in this example. In the original publication &lt;strong&gt;(TODO: reference Fruend)&lt;/strong&gt;, the weights are renormalized at the end of the loop. In this example, the normalization is combined with the update. In either case, the updated weights are shown below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Weak Learner 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The algorithm now moves to the next weak learner, which classifies the data given a threshold of \(x_1 = 3.5\). Its predictions are shown below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;th&gt;prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.250&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.250&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Only a single sample is misclassified, and the error is computed as before.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Total error&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(e_2 = 0.250\)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classifier weight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(\alpha_2 = \frac{1}{2} \ln \big(\frac{1 - e_2}{e_2}\big) = 0.5493\)&lt;/p&gt;
&lt;p&gt;The weights are updated for each sample, yielding the following data:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.500&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The second sample has been misclassified twice at this point, leading to a relatively high weight. This will hopefully be addressed by the third learner.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Weak Learner 3&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The final weak learner splits the data on \(x_2 = 6.5\), yielding the following output for each sample.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;th&gt;prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.500&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Unfortunately, sample 2 is too tricky for any of our weak learners. The total error is shown below. Since this is a binary classification problem, the error suggests that our weak learner performs worse than random guessing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Total error&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(e_3 = 0.667\)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classifier weight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(\alpha_3 = \frac{1}{2} \ln \big(\frac{1 - e_3}{e_3}\big) = -0.3473\)&lt;/p&gt;
&lt;p&gt;The negative value of the classifier weight suggests that its predictions will be reversed when evaluated. The updated weights of each data sample are given below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.375&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.125&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Final Classifier&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The final classifier is a weighted vote of the three weak learners, with the weights being the classifier weights we calculated (0.2027, 0.5493, and -0.3473). The negative weight means that the third learner&amp;rsquo;s predictions are reversed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hidden Markov Models</title>
      <link>https://ajdillhoff.github.io/notes/hidden_markov_models/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/hidden_markov_models/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-markov-assumption&#34;&gt;The Markov Assumption&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-viterbi-algorithm&#34;&gt;The Viterbi Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#estimating-parameters&#34;&gt;Estimating Parameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#expectation-maximization&#34;&gt;Expectation Maximization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This article is essentially a grok of a tutorial on HMMs by (&lt;!-- raw HTML omitted --&gt;RABINER 1989&lt;!-- raw HTML omitted --&gt;). It will be useful for the reader to reference the &lt;a href=&#34;https://courses.physics.illinois.edu/ece417/fa2017/rabiner89.pdf&#34;&gt;original paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Up to this point, we have only explored &amp;ldquo;atomic&amp;rdquo; data points.
That is, all of the information about a particular sample is encapsulated into one vector.
Sequential data is easily represented by graphical models.
This article introduces Hidden Markov Models, a powerful probabilistic graphical model used in many applications from gesture recognition to natural language processing.&lt;/p&gt;
&lt;p&gt;There are many tasks for which we do not know the underlying process.
However, we can observe samples that are produced from such processes.
Music, gesture recognition, speech, text, etc.
All of these have some underlying process which forms their outputs together into a hopefully coherent sequence.
If we wish to make predictions about future samples given these sequences, we will need to make some guess
about the underlying processes defining their output.&lt;/p&gt;
&lt;h2 id=&#34;the-markov-assumption&#34;&gt;The Markov Assumption&lt;/h2&gt;
&lt;p&gt;Markov models make a convenient assumption about sequential data.
That is, all relevant information required for predicting future samples is captured in the current time step \(t\).
Given a joint distribution over an input of \(T\) frames, \(p(\mathbf{x}_{1:T})\), the Markov assumption allows us to represent it as&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{x}_{1:T}) = p(\mathbf{x}_1)\prod_{t=2}^T p(\mathbf{x}_t|\mathbf{x}_{t-1})
\]&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;A more complicated case is when we are attempting to model some unknown process that is responsible for the observations.
In this case, an ordinary Markov chain is not sufficient.
A &lt;strong&gt;hidden Markov model (HMM)&lt;/strong&gt; is defined by a set \(z_t \in \{1, \dots, K\}\) of discrete hidden states and an &lt;strong&gt;observation&lt;/strong&gt; model \(p(\mathbf{x}_i|z_t)\).
The joint probability distribution of this model is given by&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{z}, \mathbf{x}) = p(\mathbf{z})p(\mathbf{x}|\mathbf{z}) = \Big(p(z_1)\prod_{t=2}^Tp(z_t|z_{t-1})\Big)\Big(\prod_{t=1}^Tp(\mathbf{x}_t|z_t)\Big).
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-24_20-41-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;The observations y are generated by the latent states x. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;!-- raw HTML omitted --&gt;Figure 1: &lt;!-- raw HTML omitted --&gt;The observations y are generated by the latent states x. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Although the states themselves are discrete, the observations may be continuous: \(p(\mathbf{x}|z_t, \mathbf{\theta})\).
If they are discrete, they can be modeled by an observation matrix \(B\).
Continuous observations are typically modeled using a conditional Gaussian:&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{x}_t|z_t=k, \theta) = \mathcal{N}(\mathbf{x}_t|\mathbf{\mu}_k,\mathbf{\Sigma}_k).
\]&lt;/p&gt;
&lt;p&gt;Following &lt;a href=&#34;https://courses.physics.illinois.edu/ece417/fa2017/rabiner89.pdf&#34;&gt;Rabiner&lt;/a&gt;, an HMM can be characterized by&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The number of states in the model \(N\).&lt;/li&gt;
&lt;li&gt;The number of distinct observation symbols per state \(M\).&lt;/li&gt;
&lt;li&gt;The state probability distribution \(A = \{a_{ij}\}\), \(a_{ij} = p(z_t=j | z_{t-1} = i)\).&lt;/li&gt;
&lt;li&gt;The observation symbol probability distribution \(B = \{b_j(k)\} = p(\mathbf{x}_t = k|z_t = j)\).&lt;/li&gt;
&lt;li&gt;An initial state distribution \(\mathbf{\pi}_i = p(z_t = i)\).&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-24_20-42-34_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;HMM with observation probabilities and state transition probabilities. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;!-- raw HTML omitted --&gt;Figure 2: &lt;!-- raw HTML omitted --&gt;HMM with observation probabilities and state transition probabilities. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The observation probability distribution is commonly modeled as a Gaussian, Mixture of Gaussians, or Multinomial distribution. Thus, the parameter estimates for those distributions follow the likelihood estimates for each respective distribution.&lt;/p&gt;
&lt;p&gt;In his famous tutorial on HMMs, Rabiner addressed the three fundamental problems of HMMs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Given an observation sequence and model parameters, how do we compute the probability of the observation sequence given the parameters (likelihood)?&lt;/li&gt;
&lt;li&gt;Given an observation sequence and model parameters, how do we choose a state sequence which is optimal (decoding)?&lt;/li&gt;
&lt;li&gt;How do we adjust the model parameters (learning)?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;HMMs are able to solve several different inference problems.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Filtering&lt;/strong&gt; computes \(p(z_t | \mathbf{x}_{1:t})\). That is, we are computing this probability as new samples come in up to time \(t\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smoothing&lt;/strong&gt; is accomplished when we have all the data in the sequence.
This is expressed as \(p(z_t|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fixed lag smoothing&lt;/strong&gt; allows for a trade off between accuracy and delay. It is useful in cases where we might not have the full sequence, but we wish to compute \(p(z_{t-l}|\mathbf{x}_{1:t})\) for some \(l &amp;gt; 0\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Predictions&lt;/strong&gt; are represented as \(p(z_{t+h}|\mathbf{x}_{1:t})\), where \(h &amp;gt; 0\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MAP estimation&lt;/strong&gt; yields the most probably state sequence \(\text{arg}\max_{\mathbf{z}_{1:T}}p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;We can sample the &lt;strong&gt;posterior&lt;/strong&gt; \(p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;We can also compute \(p(\mathbf{x}_{1:T})\) by summing up over all hidden paths. This is useful for classification tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;We start by solving the first problem posited by (&lt;!-- raw HTML omitted --&gt;RABINER 1989&lt;!-- raw HTML omitted --&gt;).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Given an observation sequence and model parameters, how do we compute the probability of the observation sequence given the parameters?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That is, given some model parameters \(\lambda = (A, B, \pi)\), compute \(p(z_t|\mathbf{x}_{1:t})\).&lt;/p&gt;
&lt;h3 id=&#34;forwards-pass&#34;&gt;Forwards Pass&lt;/h3&gt;
&lt;p&gt;The forwards algorithm solves two problems of interest.
First, we want to know how well our current parameters explain the observation sequence.
That is, \(p(\mathbf{x}_{1:T}|\lambda)\).&lt;/p&gt;
&lt;p&gt;Second, we want to compute \(p(z_t | \mathbf{x}_{1:t})\).
To compute these in an efficient way, a recursive strategy is adopted.
Let the forward variable \(\alpha_t(i)\) be defined as&lt;/p&gt;
&lt;p&gt;\[
\alpha_t(i) = p(\mathbf{x}_{1:t}, z_t = i | \lambda).
\]&lt;/p&gt;
&lt;p&gt;The forwards algorithm is defined as 3 steps.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Initialization:&lt;/p&gt;
&lt;p&gt;\[
\alpha_1(i) = \pi_i b_i(\mathbf{x}_1),\quad 1 \leq i \leq N.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recursion:&lt;/p&gt;
&lt;p&gt;\[
\alpha_{t+1}(j) = \Big(\sum_{i=1}^N \alpha_t(i)a_{ij}\Big)b_j(\mathbf{x}_{t+1}),\quad 1 \leq t \leq T - 1,\quad 1 \leq j \leq N
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Termination:&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{x}_{1:T})  = \sum_{i=1}^N \alpha_T(i).
\]&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The recursive step is visualized as a lattice structure &lt;a href=&#34;#figure--lattice&#34;&gt;as seen below.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-24_20-12-13_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;From Rabiner 1989.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;!-- raw HTML omitted --&gt;Figure 3: &lt;!-- raw HTML omitted --&gt;From Rabiner 1989.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;With this step, we have a solution for the first problem.
We can now calculate more efficiently the probability of our observations given the current model parameters.
This along with the following backwards pass will be essential for updating our model parameters.&lt;/p&gt;
&lt;p&gt;The forwards algorithm is also used to solve the &lt;strong&gt;filtering&lt;/strong&gt; problem.
To see how, consider \(p(z_t | \mathbf{x}_{1:t-1})\) right before time \(t\).&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(z_t=j|\mathbf{x}_{1:t-1}) = \sum_i p(z_t=j|z_{t-1}=i)p(z_{t-1}=i|\mathbf{x}_{1:t-1})
\end{equation*}&lt;/p&gt;
&lt;p&gt;When we update for time \(t\), we have that&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(z_t=j|\mathbf{x}_{1:t}) &amp;amp;= p(z_t=j|\mathbf{x}_t, \mathbf{x}_{1:t})\\
&amp;amp;=\frac{p(\mathbf{x}_t|z_t=j, \mathbf{x}_{1:t-1})p(z_t=j|\mathbf{x}_{1:t-1})}{p(\mathbf{x}_t|\mathbf{x}_{t-1})}
\end{align*}&lt;/p&gt;
&lt;p&gt;However, \(\mathbf{x}_{1:t-1}\) is conditionally independent given \(z_t\), so it becomes&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(z_t=j|\mathbf{x}_{1:t})=\frac{p(\mathbf{x}_t|z_t=j)p(z_t=j|\mathbf{x}_{1:t-1})}{p(\mathbf{x}_t|\mathbf{x}_{t-1})}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Writing out \(p(z_t=j|\mathbf{x}_{1:t-1})\) fully yields&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(z_t=j|\mathbf{x}_{1:t}) \propto p(\mathbf{x}_t|z_t=j)\sum_i p(z_t=j|z_{t-1}=i)p(z_{t-1}=i|\mathbf{x}_{1:t-1}).
\end{equation*}&lt;/p&gt;
&lt;p&gt;This is the recursion step from above!&lt;/p&gt;
&lt;p&gt;This can also be represented in terms of the \(\alpha\) variables from above. To compute \(p(z_t=i|\mathbf{x}_{1:t})\), we can use the definition of a conditional probability distribution:&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(z_t=i|\mathbf{x}_{1:t}) &amp;amp;= \frac{p(z_t=i, \mathbf{x}_{1:t})}{p(\mathbf{x}_{1:t})}\\
&amp;amp;= \frac{\alpha_t(i)}{\sum_{j=1}^N \alpha_t(j)}
\end{align*}&lt;/p&gt;
&lt;p&gt;Compared to the complexity of the explicit representation, the forwards pass needs only \(N^2T\) calculations.
As pointed out in (&lt;!-- raw HTML omitted --&gt;RABINER 1989&lt;!-- raw HTML omitted --&gt;), with 5 hidden states and an observation sequence of length 100, the forwards pass only needs around 3000 computations.
A direct calculation would require \(10^{72}\).&lt;/p&gt;
&lt;h3 id=&#34;backwards-pass&#34;&gt;Backwards Pass&lt;/h3&gt;
&lt;p&gt;When updating the parameters of our model, we will need to consider the entire observation sequence.
The forward pass did not require the entire sequence.
Instead, we can compute the probability of the observation up to some time \(t\).
The backwards pass begins by defining the variable&lt;/p&gt;
&lt;p&gt;\[
\beta_t(i) = p(\mathbf{x}_{t+1:T} | z_t = i).
\]&lt;/p&gt;
&lt;p&gt;We can utilize a recursive process similar to the forwards algorithm with the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Initialization:&lt;/p&gt;
&lt;p&gt;\[
\beta_T(i) = 1,\quad 1 \leq i \leq N
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recursion:&lt;/p&gt;
&lt;p&gt;\[
\beta_t(i) = \sum_{j=1}^N a_{ij}b_j(\mathbf{x}_{t+1})\beta_{t+1}(j),\quad t = T-1,\dots,1,\quad 1 \leq i \leq N.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Termination:&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{x}_{1:T}) = \sum_{j=1}^N \pi_j b_j(x_1) \beta_1(j)
\]&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The complexity of the backwards algorithm is similar to that of the forwards: \(N^2T\).&lt;/p&gt;
&lt;p&gt;With both the forward and backwards passes defined, we can compute the &lt;strong&gt;smoothing&lt;/strong&gt; problem:&lt;/p&gt;
&lt;p&gt;\[
p(z_t=i|\mathbf{x}_{1:T}) = \frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N \alpha_t(j)\beta_t(j)}
\]&lt;/p&gt;
&lt;h2 id=&#34;the-viterbi-algorithm&#34;&gt;The Viterbi Algorithm&lt;/h2&gt;
&lt;p&gt;With problem 1 out of the way, we turn our attention to problem 2.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Given an observation sequence and model parameters, how do we choose a state sequence which is optimal?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
\mathbf{z}^* = \text{arg}\max_{\mathbf{z}_{1:T}}p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})
\]&lt;/p&gt;
&lt;p&gt;With respect to the &lt;a href=&#34;#figure--lattice&#34;&gt;lattice diagram&lt;/a&gt;, this is equivalent to computing the shortest path.
This is accomplished via the &lt;strong&gt;Viterbi&lt;/strong&gt; algorithm, sometimes referred to as the max-sum algorithm.
As with the forwards-backwards algorithm, the Viterbi algorithm takes on a recursive approach.
It starts by defining an intermediate variable&lt;/p&gt;
&lt;p&gt;\[
\gamma_t(i) = p(z_t=i|\mathbf{x}_{1:T}).
\]&lt;/p&gt;
&lt;p&gt;Using the variables defined in the forwards-backwards algorithm, this can be expressed as&lt;/p&gt;
&lt;p&gt;\[
\gamma_t(i) = \frac{\alpha_t(i) \beta_t(i)}{\sum_{i=1}^N \alpha_t(i) \beta_t(i)}.
\]&lt;/p&gt;
&lt;p&gt;This \(\gamma_t(i)\), we can compute the most likely state at time \(t\):&lt;/p&gt;
&lt;p&gt;\[
z_t^* = \text{arg}\max_{1\leq i \leq N} \gamma_t(i), \quad 1 \leq t \leq T.
\]&lt;/p&gt;
&lt;p&gt;One problem with this approach alone is that the most likely state at a particular time \(t\) may not lead us to the most probable sequence of states.
As stated above, we need to maximize \(p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).
In order to tackle this efficiently, Viterbi employs a dynamic programming approach.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Initialization&lt;/p&gt;
&lt;p&gt;Start with the best initial state out of all states given the observation at \(t=1\).
Additionally, we want to record the index of each state through time so that the best path can be retraced.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\delta_1(i) &amp;amp;= \pi_i b_i(\mathbf{x}_1),\quad 1 \leq i \leq N\\
\psi_1(i) &amp;amp;= 0
\end{align*}&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recursion:&lt;/p&gt;
&lt;p&gt;The quantity \(\delta_t(i)\) represents the joint probability of state sequences and observations up to time \(t\) ending with state \(z_t=i\).
Thus, the recursive step is to maximize the probability of the intermediate output for \(t-1\):&lt;/p&gt;
&lt;p&gt;\[
\delta_t(j) = \max_{1 \leq i \leq N} (\delta_{t-1}(i) a_{ij})b_j(\mathbf{x}_t), \quad 2 \leq t \leq T,\quad 1 \leq j \leq N.
\]&lt;/p&gt;
&lt;p&gt;The corresponding index for this step is recorded in the path matrix:&lt;/p&gt;
&lt;p&gt;\[
\psi_t(j) = \text{arg}\max_{1 \leq i \leq N} \delta_{t-1}(i)a_{ij},\quad 2 \leq t \leq T,\quad 1 \leq j \leq N.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Termination&lt;/p&gt;
&lt;p&gt;The last step of the Viterbi algorithm completes the calcuation of the joint probability of state sequences and observations.&lt;/p&gt;
&lt;p&gt;\[
p^* = \max_{1 \leq i \leq N} \delta_T(i)
\]&lt;/p&gt;
&lt;p&gt;\[
\mathbf{z}_T^* = \text{arg}\max_{1 \leq i \leq N} \delta_T(i)
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Path Backtrace&lt;/p&gt;
&lt;p&gt;With the state sequence matrix recorded along the way, we can retrace it to get the most probable sequence:&lt;/p&gt;
&lt;p&gt;\[
z_t^* = \psi_{t+1}(z_{t+1}^*),\quad t = T-1, \cdots, 1.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;estimating-parameters&#34;&gt;Estimating Parameters&lt;/h2&gt;
&lt;p&gt;If the hidden states were fully observable, then updating our model parameters would be as straightforward as computing the maximum likelihood estimates for the model parameters \(\lambda = (A, B, \pi)\).
For \(A\) and \(\pi\), we first tally up the following counts:&lt;/p&gt;
&lt;p&gt;\[
\hat{a}_{ij} = \frac{N_{ij}}{\sum_j N_{ij}},
\]&lt;/p&gt;
&lt;p&gt;the number of times we expect to transition from \(i\) to \(j\) divided by the number of times we transition from \(i\) to any other state.&lt;/p&gt;
&lt;p&gt;For \(\pi\), we have&lt;/p&gt;
&lt;p&gt;\[
\hat{\pi_i} = \frac{N_i}{\sum_i N_i},
\]&lt;/p&gt;
&lt;p&gt;The number of times we expect to start in state \(i\) divided by the number of times we start in any other state.&lt;/p&gt;
&lt;p&gt;Estimating the parameters for \(B\) depends on which distribution we are using for our observation probabilities.
For a multinomial distribution, we would compute the number of times we are in state \(j\) and observe a symbol \(k\) divided by the number of times we are in state \(j\):&lt;/p&gt;
&lt;p&gt;\[
\hat{B}_{jk} = \frac{N_{jk}}{N_k},
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
N_{jk} = \sum_{i=1}^N \sum_{t=1}^T \mathbb{1} (z_{i, t}=j, x_{i, t}=k).
\]&lt;/p&gt;
&lt;p&gt;If the observation probability follows a Gaussian distribution, the MLEs for \(\mu\) and \(\mathbf{\Sigma}\) are&lt;/p&gt;
&lt;p&gt;\[
\hat{\mathbf{\mu}}_k = \frac{\bar{\mathbf{x}}_k}{N_k},\quad \hat{\mathbf{\Sigma}}_k = \frac{(\bar{\mathbf{x}}\bar{\mathbf{x}})_k^T - N_k \hat{\mathbf{\mu}}_k\hat{\mathbf{\mu}}_k^T}{N_k},
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\bar{\mathbf{x}}_k = \sum_{i=1}^N \sum_{t=1}^T \mathbb{1}(z_{i, t}=k)\mathbf{x}_{i, t}
\]&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;\[
(\bar{\mathbf{x}}\bar{\mathbf{x}})_k^T) = \sum_{i=1}^N \sum_{t=1}^T \mathbb{1} (z_{i, t}=k)\mathbf{x}_{i,k}\mathbf{x}_{i,k}^T.
\]&lt;/p&gt;
&lt;h2 id=&#34;expectation-maximization&#34;&gt;Expectation Maximization&lt;/h2&gt;
&lt;p&gt;Of course, HMMs have hidden states which are not fully observable.
Thus, we need to come up with another strategy for updating our parameters based on the observable data.
The intuition behind this approach is as follows.
We first start out by using our current parameters to estimate the missing data, making it complete.
Initially, we may randomize our estimates if we have no good heuristic or guess as to what they should be.&lt;/p&gt;
&lt;p&gt;With the completed data, we can update our current parameters.
In other words, the expected values of the sufficient statistics can be derived now that the data has been filled in.
A new set of parameters is found such that it maximizes the likelihood function with respect to the estimated data.&lt;/p&gt;
&lt;h3 id=&#34;e-step&#34;&gt;E Step&lt;/h3&gt;
&lt;p&gt;Following (&lt;!-- raw HTML omitted --&gt;RABINER 1989&lt;!-- raw HTML omitted --&gt;), we start with the joint probability of being in state \(i\) at time \(t\) and state \(j\) at time \(t+1\):&lt;/p&gt;
&lt;p&gt;\[
\xi_t(i, j) = p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T}).
\]&lt;/p&gt;
&lt;p&gt;This can be computed using the forwards-backwards algorithm:&lt;/p&gt;
&lt;p&gt;\[
\xi_t(i, j) = \frac{\alpha_t(i)a_{ij}b_j(\mathbf{x}_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^N \sum_{j=1}^N \alpha_t(i)a_{ij}b_j(\mathbf{x}_{t+1})\beta_{t+1}(j)}.
\]&lt;/p&gt;
&lt;p&gt;This can be related back to \(\gamma_t(i)\) by summing over over \(j\):&lt;/p&gt;
&lt;p&gt;\[
\gamma_t(i) = \sum_{j=1}^N \xi_t(i, j).
\]&lt;/p&gt;
&lt;p&gt;Here, \(\gamma_t(i)\) is the expected number of times we transition from \(z = i\).
Summing over all \(t\) yields the expected transitions from \(z_i\) over all time steps:&lt;/p&gt;
&lt;p&gt;\[
\sum_{t=1}^{T-1} \gamma_t(i).
\]&lt;/p&gt;
&lt;p&gt;Since \(\xi_t(i, j)\) is the expected transition from \(i\) to \(j\) at time \(t\), we can compute the total number of transitions from \(i\) to \(j\) via&lt;/p&gt;
&lt;p&gt;\[
\sum_{t=1}^{T-1} \xi_t(i, j).
\]&lt;/p&gt;
&lt;h3 id=&#34;m-step&#34;&gt;M Step&lt;/h3&gt;
&lt;p&gt;The previous &lt;strong&gt;E Step&lt;/strong&gt; computed the expected values given the current parameter estimates.
Now that the data is complete, we can update our parameter estimates.
Starting with the transition probabilities, we must add the expected number of transitions from \(i\) to \(j\) and divide by the expected number of times we transition from \(i\).
Using the parameters from the E Step, this can be written&lt;/p&gt;
&lt;p&gt;\[
\hat{a}_{ij} = \frac{\sum_{t=1}^{T-1}\xi_t(i, j)}{\sum_{t=1}^{T-1}\gamma_t(i)}.
\]&lt;/p&gt;
&lt;p&gt;The initial state probability at \(t=1\) is the number of times we expect to be in state \(z=i\) at \(t=1\):&lt;/p&gt;
&lt;p&gt;\[
\gamma_1(i).
\]&lt;/p&gt;
&lt;p&gt;Finally, the observation probability parameters are updated by considering the number of times we are in state \(z=j\) and observing \(x=k\) divided by the number of times we are in state \(z=j\). Note that this is for a multinomial probabiliy distribution:&lt;/p&gt;
&lt;p&gt;\[
\hat{b}_j(k) = \frac{\sum_{t=1, x_t = k}^T \gamma_t(j)}{\sum_{t=1}^T \gamma_t(j)}.
\]&lt;/p&gt;
&lt;p&gt;These formulas are derived from maximizing Baum&amp;rsquo;s auxiliary function&lt;/p&gt;
&lt;p&gt;\[
Q(\lambda, \hat{\lambda}) = \sum_{Q} p(\mathbf{z}|\mathbf{x}, \lambda) \log p(\mathbf{x}, \mathbf{z}|\hat{\lambda})
\]&lt;/p&gt;
&lt;p&gt;over \(\hat{\lambda}\). It has further been shown that maximizing this function leads to increased likelihood:&lt;/p&gt;
&lt;p&gt;\[
\max_{\hat{\lambda}} Q(\lambda, \hat{\lambda}) \implies p(\mathbf{x}|\hat{\lambda}) \geq p(\mathbf{x}|\lambda).
\]&lt;/p&gt;
&lt;p&gt;If we have a Gaussian observation model, the values for \(\hat{b}_j(k)\) are computed to accommodate the parameters of the distribution.
These parameter estimates assume a Gaussian mixture model.
Starting with \(\hat{\mu}_{jk}\), it can be estimated by dividing the expected value of observations belonging to Gaussian density \(k\) by the expected number of times we are in state \(j\) using the \(k^{\text{th}}\) mixture component:&lt;/p&gt;
&lt;p&gt;\[
\hat{\mathbf{\mu}}_{jk} = \frac{\sum_{t=1}^T \gamma_t(j, k)\mathbf{x}_t}{\sum_{t=1}^T \gamma_t(j, k)}.
\]&lt;/p&gt;
&lt;p&gt;Here, \(\gamma_t(j, k)\) is the probability of being in state \(j\) at time \(t\) with the \(k^{\text{th}}\) mixture component accounting for \(\mathbf{x}_t\):&lt;/p&gt;
&lt;p&gt;\[
\gamma_t(j, k) = \frac{\alpha_t(j)\beta_t(j)}{\sum_{j=1}^N \alpha_t(j) \beta_t(j)} \frac{c_{jk}\mathcal{N}(\mathbf{x}_t, \mu_{jk}, \mathbf{\Sigma}_{jk})}{\sum_{m=1}^M c_{jm}\mathcal{N}(\mathbf{x}_t, \mu_{jm}, \mathbf{\Sigma}_{jm})}.
\]&lt;/p&gt;
&lt;p&gt;This method is proven to improve the parameters.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Each iteration is guaranteed to improve the log-likelihood function.&lt;/li&gt;
&lt;li&gt;The process is guaranteed to converge.&lt;/li&gt;
&lt;li&gt;The convergence point is a fixed point of the likelihood function.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These guarantees are similar to gradient ascent.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
  </channel>
</rss>
