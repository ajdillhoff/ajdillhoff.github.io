<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alex Dillhoff</title>
    <link>https://ajdillhoff.github.io/authors/alex-dillhoff/</link>
    <description>Recent content on Alex Dillhoff</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; {year}</copyright>
    <lastBuildDate>Thu, 25 Apr 2024 11:03:00 -0500</lastBuildDate>
    
	    <atom:link href="https://ajdillhoff.github.io/authors/alex-dillhoff/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>NP-Completeness</title>
      <link>https://ajdillhoff.github.io/notes/np_completeness/</link>
      <pubDate>Thu, 25 Apr 2024 11:03:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/np_completeness/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#formal-languages&#34;&gt;Formal Languages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reductions&#34;&gt;Reductions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#clique-problem&#34;&gt;Clique Problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vertex-cover-problem&#34;&gt;Vertex Cover Problem&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Most of the algorithms discussed in a typical algorithms course run in polynomial time. This focus is reasonable since algorithms that run worse than polynomial time have little practical use. To simplify this notion: a problem for which a polynomial-time algorithm exists is &amp;ldquo;easy&amp;rdquo; and a problem for which no polynomial-time algorithm exists is &amp;ldquo;hard&amp;rdquo;. Knowing how to determine whether a problem is easy or hard is extremely useful. If one can identify a hard problem, then an approximate solution may be the best that can be achieved.&lt;/p&gt;
&lt;p&gt;One of the most fundamental problems in computer science is the classification of problems into these two categories. These notes provide an introduction to this classification.&lt;/p&gt;
&lt;h3 id=&#34;p-np-and-np-complete&#34;&gt;P, NP, and NP-Complete&lt;/h3&gt;
&lt;p&gt;There are three classes of algorithms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Polynomial-time&lt;/li&gt;
&lt;li&gt;NP (nondeterministic polynomial time)&lt;/li&gt;
&lt;li&gt;NP-complete&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Problems in P are those solvable in polynomial time. This means &lt;em&gt;any&lt;/em&gt; constant \(k\) such that the running time is \(O(n^k)\).&lt;/p&gt;
&lt;p&gt;The class NP is a superset of P. These a problems that can be &lt;strong&gt;verified&lt;/strong&gt; in polynomial time. This means that if someone gives you a solution to the problem, you can verify that it is correct in polynomial time. This is different from solving the problem in polynomial time. Problems in NP can be solved in &lt;strong&gt;nondeterministic&lt;/strong&gt; polynomial time. However, such a model of computation does not exist in the real world.&lt;/p&gt;
&lt;p&gt;NP-Complete problems are problems in NP that are as &lt;em&gt;hard&lt;/em&gt; as any other problem in NP. This means that if you can solve an NP-Complete problem in polynomial time, you can solve any problem in NP in polynomial time. This is why NP-Complete problems are so important.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TODO: Describe \(P \neq NP\)&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;verifying-a-solution&#34;&gt;Verifying a Solution&lt;/h3&gt;
&lt;p&gt;As long as we can come up with a verification algorithm for a problem in polynomial time, we can say that the problem is in \(NP\). This is true even if we later find a polynomial-time algorithm for the problem.&lt;/p&gt;
&lt;h3 id=&#34;proving-that-a-problem-is-np-complete&#34;&gt;Proving that a problem is NP-Complete&lt;/h3&gt;
&lt;p&gt;Proving that a problem belongs to either NP or NPC is difficult the first time you do it. Luckily, now that problems have been proven to be NP-Complete, you can use these problems to prove that other problems are NP-Complete. First, let&amp;rsquo;s introduce one more class: NP-Hard. Informally, a problem \(X\) is NP-Hard if it is at least as hard as any problem in NP. If we can reduce every problem \(Y \in NP\) to \(X\) in polynomial time, then \(X\) is NP-Hard. If \(X\) is also in NP, then \(X\) is NP-Complete.&lt;/p&gt;
&lt;h3 id=&#34;optimization-versus-decision-problems&#34;&gt;Optimization versus decision problems&lt;/h3&gt;
&lt;p&gt;Many problems are framed as optimization problems. Given some criteria, the goal is to find the best solution according to that criteria. For the shortest path problem, the algorithm finds a path between two vertices in the fewest number of edges. One can intuit that this is a slightly harder problem than that of determining if a path exists using only \(k\) edges. This latter problem is a decision problem.&lt;/p&gt;
&lt;p&gt;The reason this is worth talking about is that decision problems are often easier to come up with than optimization problems. If one can provide that a decision problem is hard, then its optimization problem is also hard.&lt;/p&gt;
&lt;h3 id=&#34;reducing-one-problem-to-another&#34;&gt;Reducing one problem to another&lt;/h3&gt;
&lt;p&gt;A common strategy for relating two problems is to reduce one to the other. For example, if problem \(B\) runs in polynomial time, and we can reduce problem \(A\) to problem \(B\) in polynomial time, then problem \(A\) is also in P. This is because we can solve \(A\) by reducing it to \(B\) and then solving \(B\) in polynomial time.&lt;/p&gt;
&lt;h2 id=&#34;formal-languages&#34;&gt;Formal Languages&lt;/h2&gt;
&lt;h2 id=&#34;reductions&#34;&gt;Reductions&lt;/h2&gt;
&lt;p&gt;The main idea behind reductions is to first show that a problem is NP-Complete. This first proof was done by Cook in 1971. With a problem proven to be NP-Complete, one can then show that other problems are NP-Complete by reducing them to the first problem. This method is far simpler than the original proof and provides a convenient process for proving that a problem is NP-Complete. The process is based on the following lemma.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 34.8&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If \(L\) is a language such that \(L&amp;rsquo; \leq_p L\) for some \(L&amp;rsquo; \in \text{NPC}\), then \(L\) is NP-hard. If, in addition, we have \(L \in \text{NP}\), then \(L\) is NP-Complete.&lt;/p&gt;
&lt;h3 id=&#34;circuit-satisfiability&#34;&gt;Circuit Satisfiability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What is this problem?&lt;/li&gt;
&lt;li&gt;Why is it important?&lt;/li&gt;
&lt;li&gt;How is it related to NP-Completeness?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given a boolean combinatorial circuit, is it satisfiable. That is, is there an assignment of values to the inputs that makes the output true?&lt;/p&gt;
&lt;p&gt;This problem is in the class NP. To prove this, we only need to show that given an assignment of values to the inputs, we can verify that the output is true in polynomial time. For a given circuit, we can verify that the output is true by following the circuit from the inputs to the outputs.&lt;/p&gt;
&lt;p&gt;Proving that it is NP-Complete is much more difficult, so a brief overview is provided here. Let \(A\) be any problem in \(NP\). Since any problem in \(NP\) has a polynomial-time verification algorithm, we can construct a boolean circuit that simulates this algorithm. This circuit will have a single output that is true if and only if the input is a valid solution to the problem. This circuit is satisfiable if and only if the input is a valid solution to the problem. Therefore, the circuit satisfiability problem is NP-Complete.&lt;/p&gt;
&lt;h4 id=&#34;example-exercise-34-dot-3-1&#34;&gt;Example: Exercise 34.3-1&lt;/h4&gt;
&lt;p&gt;Verify that the given circuit is unsatisfiable.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-27_14-31-53_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Figure 34.8 from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Figure 34.8 from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;For reference, here are definitions for each of the gates listed in the figure.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-27_14-35-18_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Definitions for the gates in the circuit from Figure 34.7 (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Definitions for the gates in the circuit from Figure 34.7 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;How can we prove this circuit is unsatisfiable?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The easiest way to do this is to code it up and brute force it.&lt;/p&gt;
&lt;h3 id=&#34;formula-satisfiability&#34;&gt;Formula Satisfiability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Satisfiability (SAT) is NP-Hard.&lt;/li&gt;
&lt;li&gt;Can show that CIRCUIT-SAT \(\leq_p\) SAT.&lt;/li&gt;
&lt;li&gt;Then show that SAT \(\leq_p\) 3SAT.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An instance of the &lt;strong&gt;formula satisfiability (SAT)&lt;/strong&gt; problem is a boolean formula \(\phi\) with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(n\) variables \(x_1, x_2, \ldots, x_n\)&lt;/li&gt;
&lt;li&gt;\(m\) clauses \(C_1, C_2, \ldots, C_m\)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, the formula&lt;/p&gt;
&lt;p&gt;\[
\phi = (x_1 \lor x_2) \land (\lnot x_1 \lor x_3) \land (x_2 \lor x_3)
\]&lt;/p&gt;
&lt;p&gt;has the satisfying assignment \(x_1 = 1, x_2 = 0, x_3 = 1\).&lt;/p&gt;
&lt;h4 id=&#34;sat-belongs-to-np&#34;&gt;SAT belongs to NP&lt;/h4&gt;
&lt;p&gt;Showing that SAT is in NP is straightforward. Given a boolean formula \(\phi\) and an assignment of values to the variables, one can verify that the formula is satisfied in polynomial time. This is enough to show that SAT is in NP.&lt;/p&gt;
&lt;h4 id=&#34;circuit-sat-leq-p-sat&#34;&gt;CIRCUIT-SAT \(\leq_p\) SAT&lt;/h4&gt;
&lt;p&gt;If we can reduce an instance of CIRCUIT-SAT, which is known to be NP-Complete, to SAT, then SAT is also NP-Complete. This proof is by contradiction: assume that SAT is not NP-Complete. Then, CIRCUIT-SAT is not NP-Complete. But we know that CIRCUIT-SAT is NP-Complete, so this is a contradiction.&lt;/p&gt;
&lt;p&gt;The reduction starts by introducing a variable for each wire and a clause for each gate, as seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-27_15-34-18_screenshot.png&#34; &gt;


&lt;/figure&gt;

&lt;p&gt;The reduction algorithm produces a formula for each gate in terms of an &amp;ldquo;if and only if&amp;rdquo; statement.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\phi = x_{10} &amp;amp;\land (x_4 \leftrightarrow \lnot x_3)\\
&amp;amp;\land (x_5 \leftrightarrow (x_1 \lor x_2))\\
&amp;amp;\land (x_6 \leftrightarrow \lnot x_4)\\
&amp;amp;\land (x_7 \leftrightarrow (x_1 \land x_2 \land x_4))\\
&amp;amp;\land (x_8 \leftrightarrow (x_5 \lor x_6))\\
&amp;amp;\land (x_9 \leftrightarrow (x_6 \lor x_7))\\
&amp;amp;\land (x_{10} \leftrightarrow (x_6 \land x_8 \land x_9))\\
\end{align*}&lt;/p&gt;
&lt;p&gt;A simpler explanation for this reduction is that a circuit can be represented as a boolean formula. This formula can be solved by the SAT algorithm.&lt;/p&gt;
&lt;h3 id=&#34;3sat&#34;&gt;3SAT&lt;/h3&gt;
&lt;p&gt;The 3SAT problem is a special case of SAT where each clause has exactly three literals. This problem is also NP-Complete. Many problems can be reduced to 3SAT, which is why it is so important.&lt;/p&gt;
&lt;h4 id=&#34;definition&#34;&gt;Definition&lt;/h4&gt;
&lt;p&gt;An instance of 3SAT is a boolean formula \(\phi\) with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(n\) &lt;strong&gt;literals&lt;/strong&gt; \(x_1, x_2, \ldots, x_n\)&lt;/li&gt;
&lt;li&gt;\(m\) &lt;strong&gt;clauses&lt;/strong&gt; \(C_1, C_2, \ldots, C_m\)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each clause has exactly three literals and is in &lt;strong&gt;conjunctive normal form&lt;/strong&gt; (CNF), which means it is expressed as an AND of clauses. For example, the formula&lt;/p&gt;
&lt;p&gt;\[
\phi = (x_1 \lor x_2 \lor x_3) \land (\lnot x_1 \lor x_2 \lor x_3) \land (x_1 \lor \lnot x_2 \lor x_3)
\]&lt;/p&gt;
&lt;p&gt;is a 3SAT formula.&lt;/p&gt;
&lt;h4 id=&#34;3sat-is-np-complete&#34;&gt;3SAT is NP-Complete&lt;/h4&gt;
&lt;p&gt;The 3SAT problem is NP-Complete. This can be shown by reducing SAT to 3SAT. A thorough proof is provided in the textbook (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;clique-problem&#34;&gt;Clique Problem&lt;/h2&gt;
&lt;p&gt;A clique is a &lt;strong&gt;complete subgraph&lt;/strong&gt; of an undirected graph \(G\). That is, a clique is a set of vertices such that every pair of vertices is connected by an edge. The &lt;strong&gt;clique problem&lt;/strong&gt; is to find the largest clique in a graph.&lt;/p&gt;
&lt;p&gt;\[
\text{CLIQUE} = \{ \langle G, k \rangle \mid G \text{ has a clique of size } k \}
\]&lt;/p&gt;
&lt;h3 id=&#34;clique-is-in-np&#34;&gt;Clique is in NP&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s say you have access to a clique of size \(k\). You can verify that this is a clique in polynomial time by checking that every pair of vertices is connected by an edge. That is, for each pair \(u, v \in V&amp;rsquo;\), the edge \((u, v)\) is in \(E\), where \(V&amp;rsquo;\) is the set of vertices in the clique.&lt;/p&gt;
&lt;p&gt;Thus, we have a polynomial-time verification algorithm for the clique problem, so it is in NP.&lt;/p&gt;
&lt;h3 id=&#34;clique-is-np-complete&#34;&gt;Clique is NP-Complete&lt;/h3&gt;
&lt;p&gt;Knowing that 3SAT is NP-Complete, we can reduce 3SAT to the clique problem. The reduction may not be intuitive as a boolean formula seems to have no relation to a graph.&lt;/p&gt;
&lt;p&gt;Let \(\phi = C_1 \land C_2 \land \ldots \land C_m\) be a 3SAT formula with \(n\) variables. We construct a graph \(G\) with \(3n\) vertices. Each \(C_r\) has three literals, \(l_1^r, l_2^r, l_3^r\).&lt;/p&gt;
&lt;p&gt;To construct the graph, we create a triplet of vertices \(v_1^r, v_2^r, v_3^r\) for each clause \(C_r\) such that there is no edge connecting any two vertices in the same triplet. There is an edge \((v_i^r, v_j^s) \in E\) if&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;\(v_i^r\) and \(v_j^s\) are in different triplets&lt;/li&gt;
&lt;li&gt;\(l_i^r\) and \(l_j^s\) are not negations of each other&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One such formula that we can convert is&lt;/p&gt;
&lt;p&gt;\[
\phi = (x_1 \lor \lnot x_2 \lor \lnot x_3) \land (\lnot x_1 \lor x_2 \lor x_3) \land (x_1 \lor x_2 \lor x_3).
\]&lt;/p&gt;
&lt;p&gt;The resulting graph is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-05-01_09-44-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Graph constructed from the 3SAT formula (phi) (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Graph constructed from the 3SAT formula (phi) (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;If a satisfying assignment exists for \(\phi\), then each \(C_r\) has at least one literal that is true. Consider the corresponding vertices in the graph for a satisfying assignment. Since there is at least one true literal in each clause, there is at least one edge between the corresponding vertices. Thus, a reduction from 3SAT to the clique problem is possible.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How does this show that Clique is NP-Complete?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is true that this example shows a very specialized graph. However, this is enough to show that the problem is NPC. If there were a polynomial time solution for Clique on a general graph \(G\), then surely it would work for a specialized graph as well.&lt;/p&gt;
&lt;p&gt;If it could solve this one, then the corresponding 3SAT formula would be solvable as well. This is a contradiction, so the Clique problem is NP-Complete.&lt;/p&gt;
&lt;h2 id=&#34;vertex-cover-problem&#34;&gt;Vertex Cover Problem&lt;/h2&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-05-01_09-49-06_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Vertex cover examples (Wikipedia).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Vertex cover examples (Wikipedia).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The &lt;strong&gt;vertex cover problem&lt;/strong&gt; is to find the smallest set of vertices such that every edge in the graph is incident to at least one vertex in the set. More formally, a vertex cover of a graph \(G\) is a set \(V&amp;rsquo; \subseteq V\) such that for every edge \((u, v) \in E\), either \(u \in V&amp;rsquo;\) or \(v \in V&amp;rsquo;\).&lt;/p&gt;
&lt;h3 id=&#34;vertex-cover-is-in-np&#34;&gt;Vertex Cover is in NP&lt;/h3&gt;
&lt;p&gt;Given a set of vertices \(V&amp;rsquo;\), one can verify that it is a vertex cover in polynomial time by checking that every edge is incident to at least one vertex in the set. This is a polynomial-time verification algorithm, so the vertex cover problem is in NP.&lt;/p&gt;
&lt;h3 id=&#34;vertex-cover-is-np-complete&#34;&gt;Vertex Cover is NP-Complete&lt;/h3&gt;
&lt;p&gt;We can show that the vertex cover problem is NP-Complete by reducing it to an instance of the clique problem. For this, we need to introduce the definition of a graph complement. Given a graph \(G = (V, E)\), the &lt;strong&gt;complement&lt;/strong&gt; of \(G\) is the graph \(\overline{G} = (V, E&amp;rsquo;)\) where \(E&amp;rsquo; = \{ (u, v) \mid u, v \in V \text{ and } (u, v) \notin E \}\). Basically, \(\overline{G}\) has all the edges that \(G\) does not have.&lt;/p&gt;
&lt;p&gt;Let \(G\) contain a clique \(V&amp;rsquo; \subseteq V\), where \(|V&amp;rsquo;| = k\). Then \(V - V&amp;rsquo;\) is a vertex cover of \(\overline{G}\). If \((u, v) \in \overline{E}\), but is not in \(E\), then at least one of \(u\) or \(v\) is not in \(V&amp;rsquo;\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-05-01_09-54-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Graph (G) and its complement (overline{G}) (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Graph (G) and its complement (overline{G}) (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the example above, any edge in \(\overline{G}\) has at least one vertex that is not in \(G\). On the same edge, at least one is in \(V - V&amp;rsquo;\), implying that \((u, v)\) is covered by \(V - V&amp;rsquo;\). Thus, a reduction from the clique problem to the vertex cover problem is possible.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>All-Pairs Shortest Paths</title>
      <link>https://ajdillhoff.github.io/notes/all_pairs_shortest_paths/</link>
      <pubDate>Sat, 20 Apr 2024 11:32:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/all_pairs_shortest_paths/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#problem-representation&#34;&gt;Problem Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-naive-solution&#34;&gt;A Naive Solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-floyd-warshall-algorithm&#34;&gt;The Floyd-Warshall Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;TychoLink is a telecommunications company looking to optimize its network for the fastest and most efficient data transfer possible. The network consists of multiple routers, each connected by various types of links that differ in latency and bandwidth. The company wants to ensure that data packets can travel from any router to any other router in the network using the path that offers the best balance between low latency and high bandwidth. There are three objectives in total:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Determine the all-pairs shortest paths across the network, taking into account both latency and bandwidth.&lt;/li&gt;
&lt;li&gt;Minimize the overall latency for data packet transmission across the network.&lt;/li&gt;
&lt;li&gt;Maximize the effective bandwidth along the chosen paths to ensure high data transfer rates.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Given the solutions discussed in &lt;a href=&#34;https://ajdillhoff.github.io/notes/single_source_shortest_paths/&#34;&gt;Single-Source Shortest Paths&lt;/a&gt;, we can simply run the Bellman-Ford algorithm for each router in the network to find the shortest paths to all other routers. This results in a time complexity of \(O(V^2 E)\). If the network is dense, then the number of edges \(E = \Theta(V^2)\), which results in a time complexity of \(O(V^4)\).&lt;/p&gt;
&lt;p&gt;These notes discuss another solution, the Floyd-Warshall algorithm, which can find the shortest paths between all pairs of routers in the network in \(O(V^3)\) time. The algorithm is particularly useful when the network is dense, as it is more efficient than running the Bellman-Ford algorithm for each router.&lt;/p&gt;
&lt;h2 id=&#34;problem-representation&#34;&gt;Problem Representation&lt;/h2&gt;
&lt;p&gt;Given a network \(G = (V, E)\) and a set of weights \(W = (w_{ij})\) for each edge \((i, j) \in E\), the goal is to find the shortest path between all pairs of vertices in \(V\). The graph and weights will be represented as an adjacency matrix with entries \(w_{ij}\) for each edge \((i, j) \in E\).&lt;/p&gt;
&lt;p&gt;\[
w_{ij} = \begin{cases}
0 &amp;amp; \text{if } i = j \\
\text{weight of edge } (i, j) &amp;amp; \text{if } i \neq j, (i, j) \in E \\
\infty &amp;amp; \text{if } i \neq j, (i, j) \notin E.
\end{cases}
\]&lt;/p&gt;
&lt;p&gt;The \((i, j)\) entry of the output matrix is \(\delta(i, j)\), the shortest-path weight from \(i\) to \(j\).&lt;/p&gt;
&lt;h2 id=&#34;a-naive-solution&#34;&gt;A Naive Solution&lt;/h2&gt;
&lt;p&gt;To construct a dynamic programming solution, we need to establish that the problem has &lt;strong&gt;optimal substructure&lt;/strong&gt;. The shortest path structure was first discussed in &lt;a href=&#34;https://ajdillhoff.github.io/notes/single_source_shortest_paths/&#34;&gt;Single-Source Shortest Paths&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;recursive-solution&#34;&gt;Recursive Solution&lt;/h3&gt;
&lt;p&gt;Step 2 is to state the recursive solution. Let \(l_{ij}^{( r)}\) be the minimum weight of any path \(i \leadsto j\) that contains at most \(r\) edges. For \(r = 0\), the cost is either 0 if \(i = j\) or \(\infty\) otherwise.&lt;/p&gt;
&lt;p&gt;For \(r = 1\), try all possible predecessors \(k\) of \(j\).&lt;/p&gt;
&lt;p&gt;\begin{align*}
l_{ij}^{( r)} &amp;amp;= \min \Big\{l_{ij}^{(r-1)}, \min \{l_{ik}^{(r-1)} + w_{kj} : 1 \leq k \leq n\}\Big\}\\
&amp;amp;= \min \{l_{ik}^{(r-1)} + w_{kj} : 1 \leq k \leq n\}.
\end{align*}&lt;/p&gt;
&lt;p&gt;Either the solution comes from a path of length \(r-1\) or from a path of length \(r-1\) with an additional edge \((k, j)\). The shortest path weights \(\delta(i, j)\) contain at most \(n-1\) edges since the shortest path cannot contain a cycle. This implies that \(\delta(i, j) = l_{ij}^{(n-1)} = l_{ij}^{(n)} = l_{ij}^{(n+1)} = \ldots\)&lt;/p&gt;
&lt;h3 id=&#34;bottom-up-approach&#34;&gt;Bottom-Up Approach&lt;/h3&gt;
&lt;p&gt;Starting with a matrix \(W = (w_{ij})\), where \(w_{ij}\) are the edge weights, the following approach computes a series of matrices \(L^{(0)}, L^{(1)}, \ldots, L^{(n-1)}\), where \(L^{( r)} = (l_{ij}^{( r)})\). The final matrix \(L^{(n-1)}\) contains the shortest path weights.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;extend_shortest_paths&lt;/span&gt;(L, W):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(L)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    L_prime &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;inf&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                L_prime[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(L_prime[i][j], L[i][k] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; W[k][j])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; L_prime
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This function has the structure of matrix multiplication and has a running time of \(O(n^3)\). Since this must be repeated \(n\) times, the total running time is \(O(n^4)\).&lt;/p&gt;
&lt;p&gt;Since they are so similar, we can actually reframe this as matrix multiplication. This will lead to a more efficient algorithm. Start with the statement being computed inside the loop:&lt;/p&gt;
&lt;p&gt;\[
l_{ij}^{( r)} = \min \{l_{ij}^{( r)}, l_{ik}^{(r-1)} + w_{kj}\}.
\]&lt;/p&gt;
&lt;p&gt;If we swap \(\min\) with \(+\) and \(\cdot\) with \(\min\), we can rewrite this as a matrix multiplication:&lt;/p&gt;
&lt;p&gt;\[
l_{ij}^{( r)} = l_{ij}^{( r)} + l_{ik}^{(r-1)} \cdot w_{kj}.
\]&lt;/p&gt;
&lt;p&gt;For this to yield the correct result, we also need to swap the identity of \(\min\), which is \(\infty\), with the identity of \(+\), which is 0.&lt;/p&gt;
&lt;h3 id=&#34;faster-apsp&#34;&gt;Faster APSP&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;What&amp;rsquo;s the point?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The purpose of this reframing is to benefit from the associative property of matrix multiplication. As it turns out, our modified matrix multiplication using \(\min\) is also associative. This allows us to compute the shortest paths in \(O(n^3 \lg n)\) time. Consider that we only really care about \(L^{(n-1)}\). If we are not using negative weights, then computing anything above \(n-1\) will yield the same result.&lt;/p&gt;
&lt;p&gt;We can get to this result in fewer steps by using &lt;strong&gt;repeated squaring&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;\begin{align*}
L^{(1)} &amp;amp;= &amp;amp;W, &amp;amp;\quad\\
L^{(2)} &amp;amp;= &amp;amp;W^2 &amp;amp;= W \cdot W,\\
L^{(4)} &amp;amp;= &amp;amp;W^4 &amp;amp;= W^2 \cdot W^2,\\
&amp;amp;   &amp;amp;\vdots &amp;amp;\\
L^{(2^{\lceil \lg (n-1) \rceil})} &amp;amp;= &amp;amp;W^{2^{\lceil \lg (n-1) \rceil}} &amp;amp;= W^{2^{\lceil \lg (n-1) \rceil - 1}} \cdot W^{2^{\lceil \lg (n-1) \rceil - 1}}.
\end{align*}&lt;/p&gt;
&lt;p&gt;In total, we compute \(O(\lg n)\) matrices, each of which takes \(O(n^3)\) time to compute. The total running time is \(O(n^3 \lg n)\).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;faster_all_pairs_shortest_paths&lt;/span&gt;(W):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(W)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    L &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; W
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; m &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        L &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; extend_shortest_paths(L, L)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        m &lt;span style=&#34;color:#f92672&#34;&gt;*=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; L
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Since we know that \(L^{( r)} = L^{(n-1)}\) for all \(r \geq n-1\), we can stop the loop when \(m \geq n-1\).&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Exercise 23.1-1:&lt;/strong&gt; Run APSP on the following graph and show the resulting matrices at each step.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-23_17-02-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Figure 23.2 from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Figure 23.2 from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;the-floyd-warshall-algorithm&#34;&gt;The Floyd-Warshall Algorithm&lt;/h2&gt;
&lt;p&gt;The next solution to the APSP problem is the &lt;strong&gt;Floyd-Warshall algorithm&lt;/strong&gt;. This algorithm can handle negative weight edges, but it will fail to produce a result if a negative weight cycle exists. It is a dynamic programming approach that reconsiders the structure of a shortest path.&lt;/p&gt;
&lt;p&gt;Given a path \(p = \langle v_1, v_2, \dots, v_l\rangle\), an &lt;strong&gt;intermediate vertex&lt;/strong&gt; is an vertex of \(p\) other than \(v_1\) and \(v_l\). Then define \(d_{ij}^{(k)}\) as the weight of the shortest path from \(i\) to \(j\) that uses only the vertices \(\{1, 2, \dots, k\}\) as intermediate vertices. Note that the vertices are arbitrarily numbered from 1 to \(n\).&lt;/p&gt;
&lt;p&gt;Consider a shortest path \(i \leadsto j\) that uses intermediate vertices in the set \(\{1, 2, \dots, k\}\):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Drop \(k\)&lt;/strong&gt;: If \(k\) is not an intermediate vertex, then the path is a shortest path from \(i\) to \(j\) that uses only the vertices \(\{1, 2, \dots, k-1\}\) as intermediate vertices.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Split \(k\)&lt;/strong&gt;: If \(k\) is an intermediate vertex, split the path \(p\) into \(i \overset{p_1}{\leadsto} k \overset{p_2}{\leadsto} j\). In this case, \(p_1\) is a shortest path from \(i\) to \(k\) that uses only the vertices \(\{1, 2, \dots, k-1\}\) as intermediate vertices, and \(p_2\) is a shortest path from \(k\) to \(j\) that uses only the vertices \(\{1, 2, \dots, k-1\}\) as intermediate vertices.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Notice that, in either case, the set of intermediate vertices is reduced.&lt;/p&gt;
&lt;h3 id=&#34;recursive-solution&#34;&gt;Recursive Solution&lt;/h3&gt;
&lt;p&gt;Let \(d_{ij}^{(k)}\) be the weight of a shortest path from \(i\) to \(j\) that uses only the vertices \(\{1, 2, \dots, k\}\) as intermediate vertices. The base case is \(d_{ij}^{(0)} = w_{ij}\).&lt;/p&gt;
&lt;p&gt;\[
d_{ij}^{(k)} = \begin{cases}
w_{ij} &amp;amp; \text{if } k = 0,\\
\min \{d_{ij}^{(k-1)}, d_{ik}^{(k-1)} + d_{kj}^{(k-1)}\} &amp;amp; \text{if } k \geq 1.
\end{cases}
\]&lt;/p&gt;
&lt;p&gt;The goal is to compute \(D^{(n)} = (d_{ij}^{(n)})\), since all intermediate vertices belong to the set \(\{1, 2, \dots, n\}\).&lt;/p&gt;
&lt;h3 id=&#34;bottom-up-approach&#34;&gt;Bottom-Up Approach&lt;/h3&gt;
&lt;p&gt;With a recurrent solution in hand, a bottom-up approach can be used to compute the shortest path. The &lt;strong&gt;Floyd-Warshall&lt;/strong&gt; algorithm takes as input a weighted adjacency matrix \(W = (w_{ij})\) and returns a matrix \(D = (d_{ij})\).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;floyd_warshall&lt;/span&gt;(W):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(W)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    D &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; W
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        D_prime &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;inf&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                D_prime[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(D[i][j], D[i][k] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; D[k][j])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        D &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; D_prime
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; D
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This provides a running time of \(\Theta(n^3)\).&lt;/p&gt;
&lt;h3 id=&#34;constructing-the-shortest-paths&#34;&gt;Constructing the Shortest Paths&lt;/h3&gt;
&lt;p&gt;The Floyd-Warshall algorithm can be modified to construct the shortest paths themselves. This can be done by maintaining a matrix \(P\) that stores the predecessor of each vertex along the shortest path. Let \(P^{(k)} = (p_{ij}^{(k)})\) for \(k = 0, 1, \dots, n\) be the matrix of predecessors. Each entry \(p_{ij}^{(k)}\) is defined recursively. The base case is&lt;/p&gt;
&lt;p&gt;\[
p_{ij}^{(0)} = \begin{cases}
i &amp;amp; \text{if } i \neq j \text{ and } w_{ij} &amp;lt; \infty,\\
\text{NIL} &amp;amp; \text{otherwise}.
\end{cases}
\]&lt;/p&gt;
&lt;p&gt;The recursive case is&lt;/p&gt;
&lt;p&gt;\[
p_{ij}^{(k)} = \begin{cases}
p_{kj}^{(k-1)} &amp;amp; \text{if } d_{ij}^{(k-1)} &amp;gt; d_{ik}^{(k-1)} + d_{kj}^{(k-1)},\\
p_{ij}^{(k-1)} &amp;amp; \text{otherwise}.
\end{cases}
\]&lt;/p&gt;
&lt;p&gt;In words, the recursive case is split into two parts. If the shortest path from \(i\) to \(j\) has \(k\) as an intermediate vertex, then it is \(i \leadsto k \leadsto j\) where \(k \neq j\). In this case, choose \(j\)&amp;rsquo;s predecessor to be the predecessor of \(j\) on a shortest path from \(k\) to \(j\) with all intermediate vertices less than \(k\): \(p_{ij}^{(k)} = p_{kj}^{(k-1)}\).&lt;/p&gt;
&lt;p&gt;The second subcase is when \(k\) is not an intermediate vertex. Keep the same predecessor as the shortest path from \(i\) to \(j\) with all intermediate vertices less than \(k\): \(p_{ij}^{(k)} = p_{ij}^{(k-1)}\).&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Walk through the Floyd-Warshall algorithm on the graph from the previous example. A Python notebook of this example is available &lt;a href=&#34;https://github.com/ajdillhoff/python-examples/blob/main/data_structures/graphs/floyd_warshall.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;in the repository.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;transitive-closure-of-a-graph&#34;&gt;Transitive Closure of a Graph&lt;/h3&gt;
&lt;p&gt;The algorithms presented above successfully solve the all-pairs shortest paths problem. &lt;strong&gt;What if we simply wanted to determine if a path exists between for all pairs of vertices?&lt;/strong&gt; The answer to this question lies in the &lt;strong&gt;transitive closure&lt;/strong&gt; of a graph.&lt;/p&gt;
&lt;p&gt;Let \(G = (V, E)\) be a directed graph with a vertex set \(V = \{1, 2, \dots, n\}\). The &lt;strong&gt;transitive closure&lt;/strong&gt; of \(G\) is a graph \(G^* = (V, E^*)\) such that \((i, j) \in E^*\) if there is a path from \(i\) to \(j\) in \(G\).&lt;/p&gt;
&lt;p&gt;One solution to this problem is to assign a weight of 1 to each edge of \(E\) and run the Floyd-Warshall algorithm. If \(d_{ij} &amp;lt; n\), then there is a path from \(i\) to \(j\) in \(G\). If \(d_{ij} = \infty\), then there is no path from \(i\) to \(j\) in \(G\). First, substitute the \(\min\) and \(+\) operations with \(\lor\) (OR) and \(\land\) (AND), respectively. This will allow us to determine if a path exists between two vertices.&lt;/p&gt;
&lt;p&gt;Just like Floyd-Warshall, we will maintain a series of matrices \(T^{(0)}, T^{(1)}, \ldots, T^{(n-1)}\), where \(T^{( r)} = (t_{ij}^{( r)})\). The final matrix \(T^{(n)}\) contains the transitive closure of the graph. The values are defined as&lt;/p&gt;
&lt;p&gt;\[
t_{ij}^{( r)} = \begin{cases}
1 &amp;amp; \text{if } r = 0 \text{ and } (i, j) \in E,\\
1 &amp;amp; \text{if } r &amp;gt; 0 \text{ and } (t_{ij}^{(r-1)} = 1 \lor (t_{ik}^{(r-1)} \land t_{kj}^{(r-1)})),\\
0 &amp;amp; \text{otherwise}.
\end{cases}
\]&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;transitive_closure&lt;/span&gt;(W):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(W)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    T &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; W
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        T_prime &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                T_prime[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; (T[i][k] &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; T[k][j])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        T &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T_prime
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; T
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This algorithm has a running time of \(\Theta(n^3)\) while using simpler operations compared to the Floyd-Warshall algorithm.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic Parallelism</title>
      <link>https://ajdillhoff.github.io/notes/dynamic_parallelism/</link>
      <pubDate>Fri, 19 Apr 2024 16:52:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/dynamic_parallelism/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Dynamic Parallelism&lt;/strong&gt; is an extension to CUDA that enables kernels to directly call other kernels. Earlier versions of CUDA only allowed kernels to be launched from the host code. When we studied &lt;GPU Pattern: Parallel Scan&gt;, the segmented approach required multiple kernel calls.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using the cuDNN Library</title>
      <link>https://ajdillhoff.github.io/notes/using_the_cudnn_library/</link>
      <pubDate>Mon, 15 Apr 2024 20:14:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/using_the_cudnn_library/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-cudnn&#34;&gt;What is cuDNN?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#setting-up-cudnn&#34;&gt;Setting up cuDNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#handling-errors&#34;&gt;Handling Errors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#representing-data&#34;&gt;Representing Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dense-layers&#34;&gt;Dense Layers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#activation-functions&#34;&gt;Activation Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#loss-functions&#34;&gt;Loss Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#convolutions&#34;&gt;Convolutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pooling&#34;&gt;Pooling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;what-is-cudnn&#34;&gt;What is cuDNN?&lt;/h2&gt;
&lt;p&gt;NVIDIA cuDNN provides optimized implementations of core operations used in deep learning. It is designed to be integrated into higher-level machine learning frameworks, such as TensorFlow, PyTorch, and Caffe.&lt;/p&gt;
&lt;h2 id=&#34;setting-up-cudnn&#34;&gt;Setting up cuDNN&lt;/h2&gt;
&lt;p&gt;To use cuDNN in your applications, each program needs to establish a handle to the cuDNN library. This is done by creating a &lt;code&gt;cudnnHandle_t&lt;/code&gt; object and initializing it with &lt;code&gt;cudnnCreate&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;cudnn.h&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;() {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt; handle;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnCreate&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;handle);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Use the handle
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnDestroy&lt;/span&gt;(handle);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;handling-errors&#34;&gt;Handling Errors&lt;/h2&gt;
&lt;p&gt;cuDNN functions return a &lt;code&gt;cudnnStatus_t&lt;/code&gt; value, which indicates whether the function call was successful. As with previous CUDA code that we have reviewed, it is best to check the return value of each call. Not only does this help with debugging, but it also allows you to handle errors gracefully.&lt;/p&gt;
&lt;h2 id=&#34;representing-data&#34;&gt;Representing Data&lt;/h2&gt;
&lt;p&gt;All data in cuDNN is represented as a &lt;strong&gt;tensor&lt;/strong&gt;. A tensor is a multi-dimensional array of data. In cuDNN, tensors have the following parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;# of dimensions&lt;/li&gt;
&lt;li&gt;data type&lt;/li&gt;
&lt;li&gt;an array of integers indicating the size of each dimension&lt;/li&gt;
&lt;li&gt;an array of integers indicating the stride of each dimension&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are a few tensor descriptors for commonly used tensor types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3D Tensors (BMN): Batch, Height, Width&lt;/li&gt;
&lt;li&gt;4D Tensors (NCHW): Batch, Channel, Height, Width&lt;/li&gt;
&lt;li&gt;5D Tensors (NCDHW): Batch, Channel, Depth, Height, Width&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When creating a tensor to use with cuDNN operations, we need to create a &lt;strong&gt;tensor descriptor&lt;/strong&gt; as well as the data itself. The tensor descriptor is a struct that contains the parameters of the tensor.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// Create descriptor
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnDataType_t&lt;/span&gt; data_type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CUDNN_DATA_FLOAT;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorFormat_t&lt;/span&gt; format &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CUDNN_TENSOR_NCHW;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, h &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;, w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; tensor_desc;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnCreateTensorDescriptor&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;tensor_desc);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSetTensor4dDescriptor&lt;/span&gt;(tensor_desc, format, data_type, n, c, h, w);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The descriptor is then used to allocate memory for the tensor data.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;data;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudaMalloc&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;data, n &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; h &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;sizeof&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;retrieving-tensor-information&#34;&gt;Retrieving Tensor Information&lt;/h3&gt;
&lt;p&gt;To retrieve the properties of a tensor that already exists, we can use the &lt;code&gt;cudnnGetTensor4dDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnGetTensor4dDescriptor&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;  tensorDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnDataType_t&lt;/span&gt;         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dataType,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;n,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;c,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;nStride,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;cStride,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;hStride,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;wStride)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tensorDesc&lt;/code&gt;: the tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dataType&lt;/code&gt;: the data type of the tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n&lt;/code&gt;: the number of batches&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt;: the number of channels&lt;/li&gt;
&lt;li&gt;&lt;code&gt;h&lt;/code&gt;: the height of the tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;w&lt;/code&gt;: the width of the tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nStride&lt;/code&gt;: the stride of the batch dimension&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cStride&lt;/code&gt;: the stride of the channel dimension&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hStride&lt;/code&gt;: the stride of the height dimension&lt;/li&gt;
&lt;li&gt;&lt;code&gt;wStride&lt;/code&gt;: the stride of the width dimension&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;dense-layers&#34;&gt;Dense Layers&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;dense layer&lt;/strong&gt; refers to a fully connected layer in a neural network. Each neuron in the layer is connected to every neuron in the previous layer. The weights of the connections are stored in a matrix, and the biases are stored in a vector. Implementing the forward and backward pass of a dense layer involves matrix multiplication and addition for which cuBLAS has optimized routines.&lt;/p&gt;
&lt;h3 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h3&gt;
&lt;p&gt;The forward pass of a dense layer is computed as follows:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{a} = W \mathbf{x} + \mathbf{b}
\]&lt;/p&gt;
&lt;p&gt;where \(\mathbf{W}\) is the weight matrix, \(\mathbf{x}\) is the input tensor, \(\mathbf{b}\) is the bias vector, and \(\mathbf{a}\) is the output tensor.&lt;/p&gt;
&lt;p&gt;This can be implemented in CUDA with a matrix multiply followed by a vector addition. The first operation we will use is &lt;a href=&#34;https://docs.nvidia.com/cuda/cublas/index.html?highlight=cublasSgemm#cublas-t-gemm&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&lt;code&gt;cublasSgemm&lt;/code&gt;&lt;/a&gt;. The function declaration is&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cublasStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cublasSgemm&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;cublasHandle_t&lt;/span&gt; handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;cublasOperation_t&lt;/span&gt; transa, &lt;span style=&#34;color:#66d9ef&#34;&gt;cublasOperation_t&lt;/span&gt; transb,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; m, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; k,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;           &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;           &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;A, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; lda,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;           &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;B, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; ldb,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;           &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;           &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;C, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; ldc);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This computes&lt;/p&gt;
&lt;p&gt;\[
C = \alpha \text{op}(A) \text{op}(B) + \beta C.
\]&lt;/p&gt;
&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;handle&lt;/code&gt;: the cuBLAS handle&lt;/li&gt;
&lt;li&gt;&lt;code&gt;transa&lt;/code&gt;: the operation to perform on matrix A (transpose or not)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;transb&lt;/code&gt;: the operation to perform on matrix B (transpose or not)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;m&lt;/code&gt;: the number of rows in matrix A and C&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n&lt;/code&gt;: the number of columns in matrix B and C&lt;/li&gt;
&lt;li&gt;&lt;code&gt;k&lt;/code&gt;: the number of columns in matrix A and rows in matrix B&lt;/li&gt;
&lt;li&gt;&lt;code&gt;alpha&lt;/code&gt;: scalar multiplier for the product of A and B&lt;/li&gt;
&lt;li&gt;&lt;code&gt;A&lt;/code&gt;: matrix A&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lda&lt;/code&gt;: leading dimension of matrix A&lt;/li&gt;
&lt;li&gt;&lt;code&gt;B&lt;/code&gt;: matrix B&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ldb&lt;/code&gt;: leading dimension of matrix B&lt;/li&gt;
&lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: scalar multiplier for matrix C&lt;/li&gt;
&lt;li&gt;&lt;code&gt;C&lt;/code&gt;: matrix C&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ldc&lt;/code&gt;: leading dimension of matrix C&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This function is called twice in the forward pass of a dense layer: once for the matrix multiplication and once for the vector addition.&lt;/p&gt;
&lt;h3 id=&#34;backward-pass&#34;&gt;Backward Pass&lt;/h3&gt;
&lt;p&gt;The backward pass of a dense layer computes the gradients of the weights and biases with respect to the loss.&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathbf{a}}{\partial W} = \frac{\partial}{\partial W} (W \mathbf{x} + \mathbf{b}) = \mathbf{x}
\]&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathbf{a}}{\partial \mathbf{b}} = \frac{\partial}{\partial \mathbf{b}} (W \mathbf{x} + \mathbf{b}) = 1
\]&lt;/p&gt;
&lt;p&gt;Additionally, the layer should propagate the gradients of the loss with respect to the input tensor.&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathbf{a}}{\partial \mathbf{x}} = \frac{\partial}{\partial \mathbf{x}} (W \mathbf{x} + \mathbf{b}) = W
\]&lt;/p&gt;
&lt;p&gt;These gradients are only the local gradients of the layer. During backpropagation, the gradients are multiplied by the gradients propagated from the subsequent layer, as shown below:&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial \mathbf{a}} \frac{\partial \mathbf{a}}{\partial W}
\]&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial L}{\partial \mathbf{b}} = \frac{\partial L}{\partial \mathbf{a}} \frac{\partial \mathbf{a}}{\partial \mathbf{b}}
\]&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial L}{\partial \mathbf{x}} = \frac{\partial L}{\partial \mathbf{a}} \frac{\partial \mathbf{a}}{\partial \mathbf{x}}
\]&lt;/p&gt;
&lt;p&gt;The first two gradients are used to update the weights and biases of the current layer. The last gradient is propagated to the previous layer.&lt;/p&gt;
&lt;p&gt;These can be implemented in CUDA with matrix multiplication and vector addition, similar to the forward pass.&lt;/p&gt;
&lt;h2 id=&#34;activation-functions&#34;&gt;Activation Functions&lt;/h2&gt;
&lt;p&gt;cuDNN supports a variety of activation functions, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sigmoid&lt;/li&gt;
&lt;li&gt;Hyperbolic Tangent&lt;/li&gt;
&lt;li&gt;Rectified Linear Unit (ReLU)&lt;/li&gt;
&lt;li&gt;Clipped Rectified Linear Unit (CLReLU)&lt;/li&gt;
&lt;li&gt;Exponential Linear Unit (ELU)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To use an activation function, we need to create an activation descriptor and set the activation function type.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnActivationDescriptor_t&lt;/span&gt; activation_desc;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnCreateActivationDescriptor&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;activation_desc);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSetActivationDescriptor&lt;/span&gt;(activation_desc, CUDNN_ACTIVATION_RELU, CUDNN_NOT_PROPAGATE_NAN, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The third enum &lt;code&gt;CUDNN_NOT_PROPAGATE_NAN&lt;/code&gt; indicates that NaN values should not be propagated through the activation function. The last parameter is a coefficient value, which is used by clipped ReLU and ELU.&lt;/p&gt;
&lt;p&gt;We can also query the activation descriptor to extract the properties.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnGetActivationDescriptor&lt;/span&gt;(activation_desc, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;mode, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;reluNanOpt, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;coef);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h3&gt;
&lt;p&gt;To process the forward pass of an activation function, we use the &lt;code&gt;cudnnActivationForward&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnActivationForward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt; handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnActivationDescriptor_t&lt;/span&gt; activationDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha, &lt;span style=&#34;color:#75715e&#34;&gt;// scalar multiplier
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta, &lt;span style=&#34;color:#75715e&#34;&gt;// scalar modifier
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; zDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;z);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This computes the following operation:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{z} = \alpha \cdot g(\mathbf{x}) + \beta \cdot \mathbf{z}
\]&lt;/p&gt;
&lt;p&gt;where \(\mathbf{x}\) is the input tensor, \(\mathbf{z}\) is the output tensor, \(\alpha\) is a scalar multiplier, and \(\beta\) is a scalar modifier.&lt;/p&gt;
&lt;h3 id=&#34;backward-pass&#34;&gt;Backward Pass&lt;/h3&gt;
&lt;p&gt;Likewise, the backward pass is done with &lt;code&gt;cudnnActivationBackward&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnActivationBackward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt; handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnActivationDescriptor_t&lt;/span&gt; activationDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha, &lt;span style=&#34;color:#75715e&#34;&gt;// gradient modifier
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; zDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;z,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; dzDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dz,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; dxDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dx
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This computes the following operation:&lt;/p&gt;
&lt;p&gt;\[
d\mathbf{x} = \alpha \cdot \nabla_{\mathbf{x}} g(\mathbf{x}) d\mathbf{z} + \beta \cdot d\mathbf{x}
\]&lt;/p&gt;
&lt;p&gt;where \(d\mathbf{z}\) is the input tensor to the backward function. Under this same notation, \(\mathbf{z}\) was the output of the activation function. The input to the activation function \(d\mathbf{x}\) is the output tensor of the backward pass, since it is being propagated in the backwards direction.&lt;/p&gt;
&lt;h2 id=&#34;loss-functions&#34;&gt;Loss Functions&lt;/h2&gt;
&lt;p&gt;cuDNN also provides optimized implementations of loss functions such as cross-entropy. Since the related lab focuses on classification, we will limit our discussion to the cross-entropy loss combined with the softmax function.&lt;/p&gt;
&lt;h3 id=&#34;softmax&#34;&gt;Softmax&lt;/h3&gt;
&lt;p&gt;The softmax function is used to convert the output of a neural network into a probability distribution. It is defined as&lt;/p&gt;
&lt;p&gt;\[
\text{softmax}(\mathbf{x})_i = \frac{e^{x_i}}{\sum_j e^{x_j}}
\]&lt;/p&gt;
&lt;p&gt;where \(\mathbf{x}\) is the input tensor and \(i\) is the index of the output tensor.&lt;/p&gt;
&lt;h4 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h4&gt;
&lt;p&gt;Implementing the forward pass of the softmax function is straightforward. We use the &lt;code&gt;cudnnSoftmaxForward&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSoftmaxForward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                    handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnSoftmaxAlgorithm_t&lt;/span&gt;          algorithm,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnSoftmaxMode_t&lt;/span&gt;               mode,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    yDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;y)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Most of the parameters are similar to other cuDNN functions. The &lt;code&gt;algorithm&lt;/code&gt; parameter specifies the algorithm to use for the softmax function, and the &lt;code&gt;mode&lt;/code&gt; parameter specifies the mode of the softmax function.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;algorithm&lt;/code&gt;: &lt;code&gt;CUDNN_SOFTMAX_FAST&lt;/code&gt;, &lt;code&gt;CUDNN_SOFTMAX_ACCURATE&lt;/code&gt;, or &lt;code&gt;CUDNN_SOFTMAX_LOG&lt;/code&gt;. The most numerically stable is &lt;code&gt;CUDNN_SOFTMAX_ACCURATE&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mode&lt;/code&gt;: &lt;code&gt;CUDNN_SOFTMAX_MODE_INSTANCE&lt;/code&gt; or &lt;code&gt;CUDNN_SOFTMAX_MODE_CHANNEL&lt;/code&gt;. The former computes the softmax function for each instance in the batch, while the latter computes the softmax function for each channel in the tensor.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;backward-pass&#34;&gt;Backward Pass&lt;/h4&gt;
&lt;p&gt;The backward pass of the softmax function is implemented with &lt;code&gt;cudnnSoftmaxBackward&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSoftmaxBackward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                    handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnSoftmaxAlgorithm_t&lt;/span&gt;          algorithm,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnSoftmaxMode_t&lt;/span&gt;               mode,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    yDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;y,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    dyDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dy,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    dxDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dx)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that the softmax function is used in the forward pass of the loss function, so the gradients are propagated from the loss function to the softmax function. In practice, the two are combined into a much simpler gradient calculation. If the softmax function is followed by the cross-entropy loss, the gradients are computed as&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial L}{\partial \mathbf{x}} = \mathbf{x} - \mathbf{y}
\]&lt;/p&gt;
&lt;p&gt;where \(\mathbf{y}\) is the target tensor.&lt;/p&gt;
&lt;h2 id=&#34;convolutions&#34;&gt;Convolutions&lt;/h2&gt;
&lt;p&gt;For a background on convolutions, see &lt;a href=&#34;https://ajdillhoff.github.io/notes/convolutional_neural_networks/&#34;&gt;these notes&lt;/a&gt;. The notes in this article refer to the cuDNN implementation of convolutions.&lt;/p&gt;
&lt;p&gt;When using convolution operations in cuDNN, we need to create a convolution descriptor &lt;code&gt;cudnnConvolutionDescriptor_t&lt;/code&gt; as well as a filter descriptor &lt;code&gt;cudnnFilterDescriptor_t&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;creating-a-filter&#34;&gt;Creating a filter&lt;/h3&gt;
&lt;p&gt;To create a filter descriptor, we use the &lt;code&gt;cudnnCreateFilterDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnFilterDescriptor_t&lt;/span&gt; filter_desc;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnCreateFilterDescriptor&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;filter_desc);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We then set the filter descriptor with the &lt;code&gt;cudnnSetFilter4dDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSetFilter4dDescriptor&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnFilterDescriptor_t&lt;/span&gt;    filterDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnDataType_t&lt;/span&gt;            dataType,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorFormat_t&lt;/span&gt;        format,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                        k,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                        c,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                        h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                        w)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;filterDesc&lt;/code&gt;: the filter descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dataType&lt;/code&gt;: the data type of the filter&lt;/li&gt;
&lt;li&gt;&lt;code&gt;format&lt;/code&gt;: the format of the filter (NCHW or NHWC). Use &lt;code&gt;CUDNN_TENSOR_NCHW&lt;/code&gt; for most cases.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;k&lt;/code&gt;: the number of output feature maps&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt;: the number of input feature maps&lt;/li&gt;
&lt;li&gt;&lt;code&gt;h&lt;/code&gt;: the height of the filter&lt;/li&gt;
&lt;li&gt;&lt;code&gt;w&lt;/code&gt;: the width of the filter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also query the filter descriptor to extract the properties.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnDataType_t&lt;/span&gt; data_type;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorFormat_t&lt;/span&gt; format;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; k, c, h, w;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnGetFilter4dDescriptor&lt;/span&gt;(filter_desc, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;data_type, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;format, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;k, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;c, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;h, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;w);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;creating-a-convolution&#34;&gt;Creating a convolution&lt;/h3&gt;
&lt;p&gt;To create a convolution descriptor, we use the &lt;code&gt;cudnnCreateConvolutionDescriptor&lt;/code&gt; function. Once we are done with it, we should destroy it with &lt;code&gt;cudnnDestroyConvolutionDescriptor&lt;/code&gt;. Since our convolution is 2D, we use the &lt;code&gt;cudnnSetConvolution2dDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSetConvolution2dDescriptor&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionDescriptor_t&lt;/span&gt;    convDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                             pad_h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                             pad_w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                             u,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                             v,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                             dilation_h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                             dilation_w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionMode_t&lt;/span&gt;          mode,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnDataType_t&lt;/span&gt;                 computeType)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;convDesc&lt;/code&gt;: the convolution descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pad_h&lt;/code&gt;: the height padding&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pad_w&lt;/code&gt;: the width padding&lt;/li&gt;
&lt;li&gt;&lt;code&gt;u&lt;/code&gt;: the vertical stride&lt;/li&gt;
&lt;li&gt;&lt;code&gt;v&lt;/code&gt;: the horizontal stride&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dilation_h&lt;/code&gt;: the height dilation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dilation_w&lt;/code&gt;: the width dilation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mode&lt;/code&gt;: the convolution mode (&lt;code&gt;CUDNN_CONVOLUTION&lt;/code&gt; or &lt;code&gt;CUDNN_CROSS_CORRELATION&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;computeType&lt;/code&gt;: the data type used for the convolution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although the library supports both convolution and cross-correlation, the difference is only in the order of the operands. In practice, the two are equivalent. Most deep learning frameworks use cross-correlation.&lt;/p&gt;
&lt;p&gt;To query the convolution descriptor, we can use the &lt;code&gt;cudnnGetConvolution2dDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnGetConvolution2dDescriptor&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionDescriptor_t&lt;/span&gt;    convDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;pad_h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;pad_w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;u,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;v,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dilation_h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dilation_w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionMode_t&lt;/span&gt;         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;mode,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnDataType_t&lt;/span&gt;                &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;computeType)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If we know all the parameters of the convolution, we can use the &lt;code&gt;cudnnGetConvolution2dForwardOutputDim&lt;/code&gt; function to calculate the output dimensions.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnGetConvolution2dForwardOutputDim&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionDescriptor_t&lt;/span&gt;    convDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;         inputTensorDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnFilterDescriptor_t&lt;/span&gt;         filterDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                                 &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;n,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                                 &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;c,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                                 &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                                 &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;cuDNN&lt;/code&gt; supports several methods for performing a convolution operation. An evaluation of the available algorithms can be found &lt;a href=&#34;https://core.ac.uk/download/pdf/224976536.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here.&lt;/a&gt; The algorithms provide tradeoffs in terms of speed and memory usage. Diving into these details is beyond the scope of this article, but it is important to be aware of the options.&lt;/p&gt;
&lt;p&gt;The forward pass of a convolution is implemented with the &lt;code&gt;cudnnConvolutionForward&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnConvolutionForward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                       handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;       xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnFilterDescriptor_t&lt;/span&gt;       wDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionDescriptor_t&lt;/span&gt;  convDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionFwdAlgo_t&lt;/span&gt;           algo,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                               &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;workSpace,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;size_t&lt;/span&gt;                              workSpaceSizeInBytes,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;       yDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                               &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;y)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;handle&lt;/code&gt;: the cuDNN handle&lt;/li&gt;
&lt;li&gt;&lt;code&gt;alpha&lt;/code&gt;: scalar multiplier for the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;xDesc&lt;/code&gt;: the input tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt;: the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;wDesc&lt;/code&gt;: the filter descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;w&lt;/code&gt;: the filter tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;convDesc&lt;/code&gt;: the convolution descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;algo&lt;/code&gt;: the algorithm to use for the convolution&lt;/li&gt;
&lt;li&gt;&lt;code&gt;workSpace&lt;/code&gt;: the workspace for the convolution&lt;/li&gt;
&lt;li&gt;&lt;code&gt;workSpaceSizeInBytes&lt;/code&gt;: the size of the workspace&lt;/li&gt;
&lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: scalar modifier for the output tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;yDesc&lt;/code&gt;: the output tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: the output tensor&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;backward-pass&#34;&gt;Backward Pass&lt;/h3&gt;
&lt;p&gt;There are three different backward passes for a convolutional layer: one for the weights, one for the input tensor, and one for the bias.&lt;/p&gt;
&lt;h4 id=&#34;weights&#34;&gt;Weights&lt;/h4&gt;
&lt;p&gt;The backward pass for the weights is implemented with the &lt;code&gt;cudnnConvolutionBackwardFilter&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnConvolutionBackwardFilter&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                       handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;       xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;       dyDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dy,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionDescriptor_t&lt;/span&gt;  convDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionBwdFilterAlgo_t&lt;/span&gt;     algo,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                               &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;workSpace,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;size_t&lt;/span&gt;                              workSpaceSizeInBytes,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnFilterDescriptor_t&lt;/span&gt;       dwDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                               &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dw)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;A detailed description of the parameters can be found &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-cnn-library.html#cudnnconvolutionbackwardfilter&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;bias&#34;&gt;Bias&lt;/h4&gt;
&lt;p&gt;The backward pass for the bias is implemented with the &lt;code&gt;cudnnConvolutionBackwardBias&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnConvolutionBackwardBias&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                    handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    dyDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dy,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    dbDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;db)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;A detailed description of the parameters can be found &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-cnn-library.html#cudnnconvolutionbackwardbias&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;input&#34;&gt;Input&lt;/h4&gt;
&lt;p&gt;The backward pass for the input tensor is implemented with the &lt;code&gt;cudnnConvolutionBackwardData&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnConvolutionBackwardData&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                       handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnFilterDescriptor_t&lt;/span&gt;       wDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;       dyDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dy,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionDescriptor_t&lt;/span&gt;  convDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionBwdDataAlgo_t&lt;/span&gt;       algo,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                               &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;workSpace,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;size_t&lt;/span&gt;                              workSpaceSizeInBytes,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;       dxDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                               &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dx)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;A detailed description of the parameters can be found &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-cnn-library.html#cudnnconvolutionbackwarddata&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;pooling&#34;&gt;Pooling&lt;/h2&gt;
&lt;p&gt;Pooling is a common operation in convolutional neural networks. It reduces the spatial dimensions of the input tensor, which helps to reduce the number of parameters and computation in the network. Using pooling in cuDNN requires creating a descriptor. Make sure to destroy it when you&amp;rsquo;re done.&lt;/p&gt;
&lt;h3 id=&#34;creating-a-pooling-descriptor&#34;&gt;Creating a pooling descriptor&lt;/h3&gt;
&lt;p&gt;To create a pooling descriptor, we use the &lt;code&gt;cudnnCreatePoolingDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnPoolingDescriptor_t&lt;/span&gt; pooling_desc;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnCreatePoolingDescriptor&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;pooling_desc);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We then set the pooling descriptor with the &lt;code&gt;cudnnSetPooling2dDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSetPooling2dDescriptor&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnPoolingDescriptor_t&lt;/span&gt;    poolingDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnPoolingMode_t&lt;/span&gt;          mode,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnNanPropagation_t&lt;/span&gt;       maxpoolingNanOpt,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                         windowHeight,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                         windowWidth,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                         verticalPadding,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                         horizontalPadding,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                         verticalStride,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                         horizontalStride)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;poolingDesc&lt;/code&gt;: the pooling descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mode&lt;/code&gt;: the pooling mode (&lt;code&gt;CUDNN_POOLING_MAX&lt;/code&gt; or &lt;code&gt;CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;maxpoolingNanOpt&lt;/code&gt;: the NaN propagation option for max pooling&lt;/li&gt;
&lt;li&gt;&lt;code&gt;windowHeight&lt;/code&gt;: the height of the pooling window&lt;/li&gt;
&lt;li&gt;&lt;code&gt;windowWidth&lt;/code&gt;: the width of the pooling window&lt;/li&gt;
&lt;li&gt;&lt;code&gt;verticalPadding&lt;/code&gt;: the vertical padding&lt;/li&gt;
&lt;li&gt;&lt;code&gt;horizontalPadding&lt;/code&gt;: the horizontal padding&lt;/li&gt;
&lt;li&gt;&lt;code&gt;verticalStride&lt;/code&gt;: the vertical stride&lt;/li&gt;
&lt;li&gt;&lt;code&gt;horizontalStride&lt;/code&gt;: the horizontal stride&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also query the pooling descriptor to extract the properties.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnPoolingMode_t&lt;/span&gt; mode;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnNanPropagation_t&lt;/span&gt; nan_opt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; window_h, window_w, pad_h, pad_w, stride_h, stride_w;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnGetPooling2dDescriptor&lt;/span&gt;(pooling_desc, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;mode, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;nan_opt, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;window_h, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;window_w, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;pad_h, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;pad_w, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;stride_h, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;stride_w);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h3&gt;
&lt;p&gt;The forward pass of a pooling operation is implemented with the &lt;code&gt;cudnnPoolingForward&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnPoolingForward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                    handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnPoolingDescriptor_t&lt;/span&gt;   poolingDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    yDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;y)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;handle&lt;/code&gt;: the cuDNN handle&lt;/li&gt;
&lt;li&gt;&lt;code&gt;poolingDesc&lt;/code&gt;: the pooling descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;alpha&lt;/code&gt;: scalar multiplier for the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;xDesc&lt;/code&gt;: the input tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt;: the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: scalar modifier for the output tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;yDesc&lt;/code&gt;: the output tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: the output tensor&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;backward-pass&#34;&gt;Backward Pass&lt;/h3&gt;
&lt;p&gt;The backward pass of a pooling operation is implemented with the &lt;code&gt;cudnnPoolingBackward&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnPoolingBackward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                    handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnPoolingDescriptor_t&lt;/span&gt;   poolingDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    yDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;y,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    dyDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dy,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    dxDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dx)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;handle&lt;/code&gt;: the cuDNN handle&lt;/li&gt;
&lt;li&gt;&lt;code&gt;poolingDesc&lt;/code&gt;: the pooling descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;alpha&lt;/code&gt;: scalar multiplier for the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;yDesc&lt;/code&gt;: the output tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: the output tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dyDesc&lt;/code&gt;: the input tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dy&lt;/code&gt;: the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;xDesc&lt;/code&gt;: the input tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt;: the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: scalar modifier for the output tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dxDesc&lt;/code&gt;: the output tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dx&lt;/code&gt;: the output tensor&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Maximum Flow</title>
      <link>https://ajdillhoff.github.io/notes/maximum_flow/</link>
      <pubDate>Fri, 12 Apr 2024 18:51:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/maximum_flow/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#objective-questions&#34;&gt;Objective Questions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maximum-flow&#34;&gt;Maximum Flow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-polynomial-time-solution&#34;&gt;A polynomial time solution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;A flow network is a directed graph in which the edges begin at a node that produces the flow and the adjacent nodes are the ones that receive it. &lt;em&gt;Flow&lt;/em&gt; in this context could take on many meanings, such as the amount of water that can flow through a pipe, the amount of data that can be sent through a network, or the amount of traffic that can be sent through a road network. The goal of a flow network is to maximize the flow from the source to the sink.&lt;/p&gt;
&lt;p&gt;The problem may have intermediate constraints. For example, a network graph may have a node with limited bandwidth, so the flow through that node must be less than or equal to the bandwidth. These notes review the formal definition of the problem followed by a solution using the Ford-Fulkerson algorithm as well as one related to bipartite matching.&lt;/p&gt;
&lt;h2 id=&#34;objective-questions&#34;&gt;Objective Questions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; What is the &lt;strong&gt;maximum flow&lt;/strong&gt; problem?&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; How can we solve it?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;maximum-flow&#34;&gt;Maximum Flow&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;flow network&lt;/strong&gt; \(G = (V, E)\) is a directed graph in which each edge \((u, v) \in E\) has a nonnegative &lt;strong&gt;capacity&lt;/strong&gt; \(c(u, v) \geq 0\). The graph does not contain reverse edges between two vertices. If an edge does not exist in the set, then its capacity is 0. Each graph has a &lt;strong&gt;source&lt;/strong&gt; and a &lt;strong&gt;sink&lt;/strong&gt;, which will be the main edges of note when analyzing the graph. The goal is to maximize the flow going from the source to the sink. This implies that the source has no incoming edges.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-13_19-02-08_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;A flow network. Each edge depicts (f(u,v)/c(u,v)), the flow and capacity (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;A flow network. Each edge depicts (f(u,v)/c(u,v)), the flow and capacity (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;A &lt;strong&gt;flow&lt;/strong&gt; in a graph \(G\) is a function \(f : V \times V \rightarrow \mathbb{R}\) that satisfies two properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Capacity constraint:&lt;/strong&gt; For all \(u, v \in V\),
\[
0 \leq f(u,v) \leq c(u,v).
\]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flow conservation:&lt;/strong&gt; For all \(u \in V - \{s, t\}\),
\[
\sum_{v \in V} f(v, u) = \sum_{v \in V} f(u, v).
\]&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are usually many different possibly paths of flow in a flow network. The &lt;strong&gt;maximum flow problem&lt;/strong&gt; asks: what is the path that yields the maximum flow?&lt;/p&gt;
&lt;h3 id=&#34;antiparallel-edges&#34;&gt;Antiparallel Edges&lt;/h3&gt;
&lt;p&gt;The restriction that no two nodes may have more than one edge seems to be unrealistic. For example, modeling the flow of a network graph with this restriction means that network traffic can only move in one direction between two datacenters. If there were two edges between adjacent nodes \(v_1\) and \(v_2\) such that \((v_1, v_2) \in E\) and \((v_1, v_2) \in E\), we would call these edges &lt;strong&gt;antiparallel&lt;/strong&gt;. In such cases, the graph is modified with a new node \(v&amp;rsquo;\) such that \((v_1, v&amp;rsquo;) \in E\) and \((v&amp;rsquo;, v_2) \in E\). An example is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-13_19-23-43_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Addressing antiparallel edges in a flow network (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Addressing antiparallel edges in a flow network (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;multiple-sources-and-sinks&#34;&gt;Multiple Sources and Sinks&lt;/h3&gt;
&lt;p&gt;Another restriction that is unrealistic in many real-world scenarios is that maximum flow graphs can only have a single source and sink. It is easy to imagine a scenario where multiple sources and sinks within a network. Again, the graph can be modified to accommodate this scenario by defining a &lt;strong&gt;supersource&lt;/strong&gt; and &lt;strong&gt;supersink&lt;/strong&gt; whose outgoing and incoming flows are infinite.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-13_19-27-09_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Assing a supersource and supersink to a graph with multiple sources and sinks (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Assing a supersource and supersink to a graph with multiple sources and sinks (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;a-polynomial-time-solution&#34;&gt;A polynomial time solution&lt;/h2&gt;
&lt;p&gt;Ford-Fulkerson relies on three foundational concepts:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Residual networks&lt;/li&gt;
&lt;li&gt;Augmenting paths&lt;/li&gt;
&lt;li&gt;Cuts&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The solution presented by Ford and Fulkerson is not a single algorithm but rather a set of general instructions.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize \(f(u, v) = 0\) for all \(u,v \in V\), giving an initial flow of 0.&lt;/li&gt;
&lt;li&gt;Increase the flow by finding an &lt;strong&gt;augmenting path&lt;/strong&gt; in a &lt;strong&gt;residual network&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;The edges of the augmented path indicate where to increase the flow.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is repeated until the residual network has no more augmenting paths.&lt;/p&gt;
&lt;h3 id=&#34;residual-networks&#34;&gt;Residual Networks&lt;/h3&gt;
&lt;p&gt;Consider a pair of vertices \(u, v \in V\), the residual capacity \(c_f(u, v)\) is amount of additional flow that can be pushed from \(u\) to \(v\).&lt;/p&gt;
&lt;p&gt;\[
c_f(u, v)= \begin{cases}
c(u,v) - f(u, v)\quad \text{if } (u,v) \in E,\\
f(v, u)\quad \text{if } (v, u) \in E,\\
0\quad \text{otherwise (i.e., } (u, v), (v, u) \notin E).
\end{cases}
\]&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;residual network&lt;/strong&gt; is \(G_f = (V, E_f)\), where&lt;/p&gt;
&lt;p&gt;\[
E_f= \{(u, v) \in V \times V : c_f(u, v) &amp;gt; 0\}.
\]&lt;/p&gt;
&lt;p&gt;The edges of \(G_f\) represent those edges in \(G\) with the capacity to change the flow. There is also no requirement for all edges in \(G\) to be present in \(G_f\). As the algorithm works out the solution, we are only considered with edges that permit more flow.&lt;/p&gt;
&lt;p&gt;An edge \((u, v) \in E\) means that the reverse edge \((v, u) \notin E\). However, the residual network can have edges that are not in \(G\). These are used to represent paths in which flow is sent in the reverse direction. This can happen if reducing flow from one edge results in a net increase across some other.&lt;/p&gt;
&lt;p&gt;In \(G_f\), the reverse edges \((v, u)\) represent the flow on \((u, v) \in G\) that could be sent back.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-19_15-29-03_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;A flow network and its residual network (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;A flow network and its residual network (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;augmentation-function&#34;&gt;Augmentation Function&lt;/h4&gt;
&lt;p&gt;Given flows \(f\) in \(G\) and \(f&amp;rsquo;\) in \(G_f\), define the &lt;strong&gt;augmentation&lt;/strong&gt; of \(f\) by \(f&amp;rsquo;\), as a function \(V \times V \rightarrow \mathbb{R}\):&lt;/p&gt;
&lt;p&gt;\[
(f \uparrow f&amp;rsquo;)(u, v) = \begin{cases}
f(u, v) + f&amp;rsquo;(u, v) - f&amp;rsquo;(u, v)\quad \text{if } (u, v) \in E,\\
0 \quad \text{otherwise}
\end{cases}
\]&lt;/p&gt;
&lt;p&gt;for all \((u, v) \in V\).&lt;/p&gt;
&lt;p&gt;This augmentation function represents an increase of flow on \((u, v)\) by \(f&amp;rsquo;(u, v)\) with a decrease by \(f&amp;rsquo;(v, u)\) since pushing flow on the reverse edge in \(G_f\) represents a decrease in \(G\). This is known as &lt;strong&gt;cancellation&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given a flow network \(G\), a flow \(f\) in \(G\), and the residual network \(G_f\), let \(f&amp;rsquo;\) be a flow in \(G_f\). Then \((f \uparrow f&amp;rsquo;)\) is a flow in \(G\) with value \(|f \uparrow f&amp;rsquo;| = |f| + |f&amp;rsquo;|\).&lt;/p&gt;
&lt;p&gt;This lemma defines the idea of &lt;strong&gt;net&lt;/strong&gt; flow. If there are 10 units of flow in one direction and 4 in the other, the edge effectively has 6 units of flow.&lt;/p&gt;
&lt;h3 id=&#34;augmenting-paths&#34;&gt;Augmenting Paths&lt;/h3&gt;
&lt;p&gt;An &lt;strong&gt;augmenting path&lt;/strong&gt; is a simple path from the source to the sink in the residual network.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-28_13-46-13_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;An augmenting path in a flow network (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;An augmenting path in a flow network (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The purpose of an augmenting path is to increase the flow from the source to the sink. The flow is increased by the minimum capacity of the edges in the path.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 24.2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let \(G = (V, E)\) be a flow network, let \(f\) be a flow in \(G\), and let \(p\) be an augmenting path in \(G_f\). Define \(f_p : V \times V \rightarrow \mathbb{R}\) by&lt;/p&gt;
&lt;p&gt;\[
f_p(u, v) = \begin{cases}
c_f(p) &amp;amp; \text{if } (u, v) \text{ is in } p,\\
0 &amp;amp; \text{otherwise}.
\end{cases}
\]&lt;/p&gt;
&lt;p&gt;Then \(f_p\) is a flow in \(G_f\) with value \(|f_p| &amp;gt; 0\).&lt;/p&gt;
&lt;p&gt;The maximum amount that an augmenting path can be increased is the minimum capacity of the edges in the path. This is known as the &lt;strong&gt;residual capacity&lt;/strong&gt; of the path.&lt;/p&gt;
&lt;p&gt;\[
c_f(p) = \min \{c_f(u, v) : (u, v) \text{ is in } p\}.
\]&lt;/p&gt;
&lt;p&gt;Put simply, if there is a path in the residual network, the flow can be increased by the minimum capacity of the edges in the path. If there is no path, the flow is at its maximum.&lt;/p&gt;
&lt;h3 id=&#34;cuts&#34;&gt;Cuts&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;cut&lt;/strong&gt; \((S, T)\) of a flow network \(G = (V, E)\) is a partition of \(V\) into two sets \(S\) and \(T = V - S\) such that \(s \in S\) and \(t \in T\). The &lt;strong&gt;capacity&lt;/strong&gt; of the cut is the sum of the capacities of the edges from \(S\) to \(T\). Any cut is a valid cut as long as the source is in \(S\) and the sink is in \(T\).&lt;/p&gt;
&lt;p&gt;If \(f\) is a flow in \(G\) and \((S, T)\) is a cut of \(G\), then the &lt;strong&gt;net flow&lt;/strong&gt; across the cut is&lt;/p&gt;
&lt;p&gt;\[
f(S, T) = \sum_{u \in S} \sum_{v \in T} f(u, v) - \sum_{u \in S} \sum_{v \in T} f(v, u).
\]&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;capacity&lt;/strong&gt; of the cut is&lt;/p&gt;
&lt;p&gt;\[
c(S, T) = \sum_{u \in S} \sum_{v \in T} c(u, v).
\]&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;minimum cut&lt;/strong&gt; is a cut whose capacity is the smallest among all cuts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For any flow \(f\) and any cut \((S, T)\) of \(G\), we have that $|f| = f(S, T).$&lt;/p&gt;
&lt;p&gt;This lemma states that the flow across a cut is equal to the value of the flow.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\begin{align*}
f(S, T) &amp;amp;= f(S, V) - f(S, S)\\
&amp;amp;= f(S, V)\\
&amp;amp;= f(s, V) + f(S - s, V)\\
&amp;amp;= f(s, V) = |f|.
\end{align*}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key concept:&lt;/strong&gt; We can determine the flow by making cuts in the graph. The minimum cut leads to the maximum flow.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Corollary&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The value of any flow \(f\) in a flow network \(G\) is bounded from above by the capacity of any cut of \(G\).&lt;/p&gt;
&lt;h4 id=&#34;max-flow-min-cut-theorem&#34;&gt;Max-flow Min-cut Theorem&lt;/h4&gt;
&lt;p&gt;The following statements are logically equivalent.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The flow \(f\) is a maximum flow in \(G\).&lt;/li&gt;
&lt;li&gt;The residual network \(G_f\) contains no augmenting paths.&lt;/li&gt;
&lt;li&gt;The value of the flow \(f\) is equal to the capacity of the cut \((S, T)\) for some cut of \(G\).&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;ford-fulkerson-algorithm&#34;&gt;Ford-Fulkerson Algorithm&lt;/h3&gt;
&lt;p&gt;The Ford-Fulkerson algorithm is a general method for solving the maximum flow problem. The algorithm is not a single algorithm but rather a set of instructions that can be implemented in different ways. The algorithm is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ford_fulkerson&lt;/span&gt;(G, s, t):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {u: {v: &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G} &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; u &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Find an augmenting path&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bfs(G, s, t, f)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; path:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        cf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(G[u][v] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; f[u][v] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; u, v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; u, v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; path:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            f[u][v] &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; cf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            f[v][u] &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; cf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; f
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;example&#34;&gt;Example&lt;/h4&gt;
&lt;p&gt;Run the Ford-Fulkerson algorithm on the following graph.&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;The running time of Ford-Fulkerson hinges on how the augmenting path is found. If implemented with a breadth-first search, the algorithm runs in \(O(VE^2)\) time.&lt;/p&gt;
&lt;h3 id=&#34;edmonds-karp-algorithm&#34;&gt;Edmonds-Karp Algorithm&lt;/h3&gt;
&lt;p&gt;The Edmonds-Karp algorithm is a specific implementation of Ford-Fulkerson that uses breadth-first search to find the augmenting path. The algorithm presented above is actually the Edmonds-Karp algorithm.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Language of LLMs</title>
      <link>https://ajdillhoff.github.io/articles/the-language-of-llms/</link>
      <pubDate>Thu, 11 Apr 2024 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/articles/the-language-of-llms/</guid>
      <description>&lt;p&gt;The accompanying Colab notebook is &lt;a href=&#34;https://colab.research.google.com/drive/1Ch5wCSkYxXU6AAntrWH731Qk4-oE2c1P?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;available here.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Large Language Models like ChatGPT have rapidly become ubiquitous tools that enhance productivity, creativity, and even decision-making processes across various domains. Their ability to generate human-like text, comprehend complex instructions, and provide informative responses has captivated the imagination of users worldwide. This paragraph was generated by an LLM (and edited by me).&lt;/p&gt;
&lt;p&gt;This workshop is for those that are curious as to how these models &lt;strong&gt;interpret&lt;/strong&gt; the input. By the end of this hour, you will hopefully be able to answer the following questions, among others:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How do Large Language Models &lt;strong&gt;read&lt;/strong&gt; and process text?&lt;/li&gt;
&lt;li&gt;Why are LLMs good at complex tasks, but seem to perform poorly on seemingly simple tasks like spelling or arithmetic?&lt;/li&gt;
&lt;li&gt;How does an LLM understand what it is processing?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;agenda&#34;&gt;Agenda&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Tokenization&lt;/li&gt;
&lt;li&gt;Unicode byte encodings&lt;/li&gt;
&lt;li&gt;Byte Pair Encoding (BPE)&lt;/li&gt;
&lt;li&gt;Embeddings&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;tokenization&#34;&gt;Tokenization&lt;/h2&gt;
&lt;p&gt;Tokenization is the process of transforming a sequence of characters into a sequence of tokens. A token is a unit of text that we treat as a single entity. For example, in English, a token could be a word, a sentence, or a paragraph. In programming languages, a token could be a variable name, a keyword, or a string.&lt;/p&gt;
&lt;p&gt;Before we get started, let&amp;rsquo;s check out a &lt;a href=&#34;https://tiktokenizer.vercel.app&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;live demonstration of tokenization.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Consider the input prompt below. &lt;em&gt;It isn&amp;rsquo;t likely that you would have mixed emoji and code in a single text file, but it serves as a good example for tokenization.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Why does my code 💥 with a segmentation fault?
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;int main() {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    int *arr = NULL;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    scanf(&amp;#34;%d&amp;#34;, arr);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    return 0;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;At the most basic level, how is this text represented in a computer?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These characters are represented as encodings such as ASCII or Unicode. For the purposes of the rest of this article, we will assume the input is represented using Unicode.&lt;/p&gt;
&lt;h3 id=&#34;unicode-byte-encodings&#34;&gt;Unicode Byte Encodings&lt;/h3&gt;
&lt;p&gt;If we were to print out the unicode values of the prompt above, we would get the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[10, 87, 104, 121, 32, 100, 111, 101, 115, 32, 109, 121, 32, 99, 111, 100, 101, 32, 128165, ...]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Most of the values displayed in the previous cell are the same for ASCII. The emoji value has a very large number and can easily be spotted in the list.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Is that it? Is this how the input is fed into the model?&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This encoding is done at the character-level. What other types of encodings are there?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Character encoding&lt;/li&gt;
&lt;li&gt;Word encoding&lt;/li&gt;
&lt;li&gt;Sub-word encoding&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;What is the difference between them? Why would we pick one over another?&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;character-encoding&#34;&gt;Character Encoding&lt;/h3&gt;
&lt;p&gt;Character encoding converts each character into a unique integer. This is by far the simplest form of tokenization and has the benefit of a compact vocabulary. However, it is not able to effectively compress any common subsequences in the input. This leads to much larger sequences and longer training times.&lt;/p&gt;
&lt;p&gt;The biggest downside to this approach is that the individual characters are not very informative on a semantic level. For example, the word &amp;ldquo;cat&amp;rdquo; would be represented as three separate tokens, &amp;lsquo;c&amp;rsquo;, &amp;lsquo;a&amp;rsquo;, and &amp;rsquo;t&amp;rsquo;. If someone were to present you a single letter without context, you probably would likely not be able to understand the point of the message.&lt;/p&gt;
&lt;h3 id=&#34;word-encoding&#34;&gt;Word Encoding&lt;/h3&gt;
&lt;p&gt;Word encoding is a step up from character encoding. This encoding directly captures the semantic meaning of words and is a fine choice for text classification and sentiment analysis. The sequences formed are much shorter since every word can be converted into a unique token.&lt;/p&gt;
&lt;p&gt;The vocabulary size is very large since individual tokens cannot be broken down or recombined in new contexts. It also struggles with out-of-vocabulary words, since there are no base tokens to build upon.&lt;/p&gt;
&lt;h3 id=&#34;sub-word-encoding&#34;&gt;Sub-word Encoding&lt;/h3&gt;
&lt;p&gt;Sub-word encoding is a compromise between character and word encoding. It is able to capture the semantic meaning of words and can be broken down into smaller tokens. This allows for the model to generalize better to unseen words and phrases. Most large language models use sub-word encoding.&lt;/p&gt;
&lt;h2 id=&#34;byte-pair-encoding--bpe&#34;&gt;Byte Pair Encoding (BPE)&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Byte_pair_encoding&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Byte Pair Encoding&lt;/a&gt; is a sub-word encoding technique that was originally designed for data compression. It is a simple algorithm that iteratively merges the most frequent pair of bytes in a sequence. This process is repeated until a predefined vocabulary size is reached.&lt;/p&gt;
&lt;p&gt;The algorithm is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize the vocabulary with all the characters in the input.&lt;/li&gt;
&lt;li&gt;Count the frequency of all pairs of characters in the vocabulary.&lt;/li&gt;
&lt;li&gt;Merge the most frequent pair of characters.&lt;/li&gt;
&lt;li&gt;Update the vocabulary with the merged pair.&lt;/li&gt;
&lt;li&gt;Repeat steps 2-4 until the vocabulary size reaches a predefined limit.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;embeddings&#34;&gt;Embeddings&lt;/h2&gt;
&lt;p&gt;A word embedding is a learned representation of text in which semantically similar words are mapped to nearby points in the embedding space. Since they are represented as vectors, all vector operations can be applied to them. This allows for the model to learn relationships between words and phrases, quantify their similarities and differences, and encode higher-level context information.&lt;/p&gt;
&lt;p&gt;Embeddings can be learned independently or jointly with the model. For example, the Word2Vec model learns embeddings using an unsupervised approach. It predicts the context of a word given its surrounding words. The embeddings are then used as input to a downstream task (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Mikolov et al. 2013&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;LLMs typically train embeddings jointly with the model. This allows them to learn embeddings for sentences, paragraphs, or even whole documents.&lt;/p&gt;
&lt;h3 id=&#34;creating-an-embedding-layer&#34;&gt;Creating an embedding layer&lt;/h3&gt;
&lt;p&gt;We can use libraries such as PyTorch to create a learnable embedding layer. The code below creates an embedding layer that converts each individual token into a `1024` dimensional embedded layer.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch.nn &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; nn
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;token_embedding &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Embedding(vocab_size, &lt;span style=&#34;color:#ae81ff&#34;&gt;1024&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;prompt_embedded &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; token_embedding(torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;LongTensor(encode(prompt)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(prompt_embedded&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;training-the-embeddings&#34;&gt;Training the embeddings&lt;/h3&gt;
&lt;p&gt;Our corpus of a single C file is far too small to learn anything meaningful. Learning an embedding space requires a lot data and compute power. We can instead look at pre-trained embeddings. Huggingface has a large collection of pre-trained models that can be used for a variety of tasks. The accompanying notebook uses embeddings from &lt;code&gt;SentenceTransformer&lt;/code&gt; to demonstrate how embeddings can be used in practice.&lt;/p&gt;
&lt;h2 id=&#34;sentence-embeddings&#34;&gt;Sentence Embeddings&lt;/h2&gt;
&lt;p&gt;To demonstrate the power of embeddings, we will close out the workshop by reviewing sentence embeddings. BERT (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Devlin et al. 2019&lt;/a&gt;) and RoBERTa (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Liu et al. 2019&lt;/a&gt;) are LLMs that perform tasks such as semantic textual similarity. They both require that whole sentences be input, resulting in a very expensive computation.&lt;/p&gt;
&lt;p&gt;Sentence-BERT proposed an architecture that would embed these into meaningful embeddings that could be easily compared with vector operations (&lt;a href=&#34;#citeproc_bib_item_4&#34;&gt;Reimers and Gurevych 2019&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In the cells below, we will use Huggingface (huggingface.co) to download and use a pre-trained sentence transformer. This particular one was trained on &lt;strong&gt;&lt;strong&gt;1,170,060,424&lt;/strong&gt;&lt;/strong&gt; sentence pairs.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv. &lt;a href=&#34;https://doi.org/10.48550/arXiv.1810.04805&#34;&gt;https://doi.org/10.48550/arXiv.1810.04805&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” arXiv. &lt;a href=&#34;https://doi.org/10.48550/arXiv.1907.11692&#34;&gt;https://doi.org/10.48550/arXiv.1907.11692&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_3&#34;&gt;&lt;/a&gt;Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” arXiv. &lt;a href=&#34;http://arxiv.org/abs/1301.3781&#34;&gt;http://arxiv.org/abs/1301.3781&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_4&#34;&gt;&lt;/a&gt;Reimers, Nils, and Iryna Gurevych. 2019. “Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks.” arXiv. &lt;a href=&#34;https://doi.org/10.48550/arXiv.1908.10084&#34;&gt;https://doi.org/10.48550/arXiv.1908.10084&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Parallel Graph Traversal</title>
      <link>https://ajdillhoff.github.io/notes/parallel_graph_traversal/</link>
      <pubDate>Sat, 06 Apr 2024 15:26:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/parallel_graph_traversal/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parallelization-over-vertices&#34;&gt;Parallelization over vertices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parallelization-over-edges&#34;&gt;Parallelization over edges&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#improving-work-efficiency&#34;&gt;Improving work efficiency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#privatization-to-reduce-contention&#34;&gt;Privatization to reduce contention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#additional-optimizations&#34;&gt;Additional Optimizations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;For an introduction on basic graph theory and traversal algorithms, see &lt;a href=&#34;https://ajdillhoff.github.io/notes/introduction_to_graph_theory/&#34;&gt;these notes.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;parallelization-over-vertices&#34;&gt;Parallelization over vertices&lt;/h2&gt;
&lt;p&gt;When it comes to parallel algorithms, the first thing that may come to your mind is that they must operate on either the edges or the vertices. In either case, it doesn&amp;rsquo;t take much to imagine that such algorithms will require communication between threads due to dependencies in the graph or traversal algorithm.&lt;/p&gt;
&lt;p&gt;In a breadth-first search, all vertices in one level must be explored before moving into the next. The first approach we look at will require multiple calls to the kernel, one for each level.&lt;/p&gt;
&lt;h3 id=&#34;top-down-approach&#34;&gt;Top-down Approach&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void bfs_kernel(CSRGraph csrGraph, uint *level,
                           uint *newVertexVisited, uint *currLevel) {
    int vertex = blockIdx.x * blockDim.x + threadIdx.x;

    if (vertex &amp;lt; csrGraph.numVertices) {
        if (level[vertex] == currLevel - 1) {
            for (uint edge = csrGraph.srcPtrs[vertex]; edge &amp;lt; csrGraph.srcPtrs[vertex + 1]; edge++) {
                int neighbor = csrGraph.edges[edge];
                if (level[neighbor] == UINT_MAX) {  // Neighbor not visited
                    level[neighbor] = currLevel;
                    *newVertexVisited = 1;
                }
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The first kernel above labels the vertices that belong on the given level. For each vertex, it iterates over the outgoing edges. These can be accessed as the nonzero elements in the adjacency matrix for each row. The CSR format is ideal for this case. The figure below shows the result of the first kernel. Only 2 of the threads are active for the first level based on the input graph. This particular version of the algorithm is called the &lt;em&gt;push&lt;/em&gt; version.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-08_20-06-23_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Vertex-centric _push_ BFS traversal from level 1 to level 2 (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Vertex-centric &lt;em&gt;push&lt;/em&gt; BFS traversal from level 1 to level 2 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The boundary check ensures that the kernel only processes vertices that belong to the current level. Each thread has access to a global &lt;code&gt;newVertexVisited&lt;/code&gt; and will set this to 1 if it finds a new vertex to visit. This is used to determine if the traversal should continue to the next level.&lt;/p&gt;
&lt;h3 id=&#34;bottom-up-approach&#34;&gt;Bottom-up Approach&lt;/h3&gt;
&lt;p&gt;The second kernel is also a vertex-centric approach, except it considers incoming edges rather than outgoing ones. This is called the &lt;em&gt;pull&lt;/em&gt; version.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void bfs_kernel(CSCGraph cscGraph, uint *level,
                           uint *newVertexVisited, uint *currLevel) {
    int vertex = blockIdx.x * blockDim.x + threadIdx.x;

    if (vertex &amp;lt; cscGraph.numVertices) {
        if (level[vertex] == UINT_MAX) {
            for (uint edge = cscGraph.dstPtrs[vertex]; edge &amp;lt; cscGraph.dstPtrs[vertex + 1]; edge++) {
                int neighbor = cscGraph.edges[edge];
                if (level[neighbor] == currLevel - 1) {  // Neighbor visited
                    level[vertex] = currLevel;
                    *newVertexVisited = 1;
                    break;
                }
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note the use of the Compressed Sparse Column (CSC) format for the graph. The kernel requires that each thread be able to access the incoming edges, which would be determined by the nonzero elements of a give column of the adjacency matrix.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-08_20-35-48_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Vertex-centric _pull_ BFS traversal from level 1 to level 2 (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Vertex-centric &lt;em&gt;pull&lt;/em&gt; BFS traversal from level 1 to level 2 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;From each thread&amp;rsquo;s point of view, if there is an incoming edge at the previous level then it will be visited at the current level. In that case its job is done and it can break out of the loop. This approach is more efficient for graphs with a high average degree and variance.&lt;/p&gt;
&lt;p&gt;In early levels, the &lt;em&gt;push&lt;/em&gt; approach is more efficient because they have a relatively smaller number of vertices per level. As more vertices are visited, the &lt;em&gt;pull&lt;/em&gt; approach is more efficient because there is a higher chance of finding an incoming edge and exiting early. Since each level is a separate kernel call, both of these can be combined. An additional piece of overhead would be that one would require both CSR and CSC representations of the graph.&lt;/p&gt;
&lt;h2 id=&#34;parallelization-over-edges&#34;&gt;Parallelization over edges&lt;/h2&gt;
&lt;p&gt;As the name suggests, the edge-centric approach processes the edges in parallel. If the source vertex belongs to a previous level and the destination vertex is unvisited, then the destination vertex is labeled with the current level.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void bfs_kernel(COOGraph cooGraph, uint *level,
                           uint *newVertexVisited, uint *currLevel) {
    int edge = blockIdx.x * blockDim.x + threadIdx.x;

    if (edge &amp;lt; csrGraph.numEdges) {
        uint vertex = cooGraph.src[edge];
        if (level[vertex] == currLevel - 1) {
            uint neighbor = cooGraph.dst[edge];
            if (level[neighbor] == UINT_MAX) {
                level[neighbor] = currLevel;
                *newVertexVisited = 1;
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The edge-centric approach has more parallelism than the vertex-centric approaches. Graphs typically have more edges than vertices, so the largest benefit is on smaller graphs. Another advantage related to load imbalance. In the vertex-centric approach, imbalance comes from the fact that some vertices have more edges than others.&lt;/p&gt;
&lt;p&gt;The tradeoff of this that every edge is considered. There may be many edges that are not relevant for a particular level. The vertex-centric approach could skip these entirely. Since every edge needs to be indexed, the COO format is used. This requires more space than the CSR or CSC formats.&lt;/p&gt;
&lt;p&gt;Since these sparse representations are already used, we could perform these operations using sparse matrix multiplications. Libraries such as &lt;a href=&#34;https://github.com/rapidsai/cugraph/blob/branch-24.06/docs/cugraph/source/graph_support/algorithms.md&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;cugraph&lt;/a&gt; provide implementations of these algorithms.&lt;/p&gt;
&lt;h2 id=&#34;improving-work-efficiency&#34;&gt;Improving work efficiency&lt;/h2&gt;
&lt;p&gt;The previous two approaches have a common problem: it is likely that many threads will perform no useful work. Take the vertex-centric approach, for example. The threads that are launched only to find out that their vertex is not in the current level will not do anything. This is a waste of resources. Ideally, those threads would not be launched in the first place. A simple solution to this is to have each thread build a &lt;strong&gt;frontier&lt;/strong&gt; of vertices that they visit, so that only the vertices in the frontier are processed.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void bfs_kernel(CSRGraph csrGraph, uint *level,
                           uint *prevFrontier, uint *currFrontier,
                           uint numPrevFrontier, uint *numCurrFrontier,
                           uint currLevel) {
    uint i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &amp;lt; numPrevFrontier) {
        uint vertex = prevFrontier[i];
        for (uint edge = csrGraph.srcPtrs[vertex]; edge &amp;lt; csrGraph.srcPtrs[vertex + 1]; edge++) {
            uint neighbor = csrGraph.dst[edge];
            if (atomicCAS(&amp;amp;level[neighbor], UINT_MAX, currLevel) == UINT_MAX) {
                uint currFrontierIdx = atomicAdd(numCurrFrontier, 1);
                currFrontier[currFrontierIdx] = neighbor;
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When launched, only the threads corresponding to a frontier will be active. They start by loading the elements from the previous frontier. As before, they iterate over the outgoing edges. If the neighbor has not been visited, then it is labeled with the current level and added to the current frontier. The atomic operation ensures that the size of the frontier is updated correctly.&lt;/p&gt;
&lt;p&gt;The call to &lt;code&gt;atomicCAS&lt;/code&gt; prevents multiple threads from adding the same vertex to the frontier. It checks whether the current vertex is unvisited. Not every thread is going to visit the same neighbor, so the contention should be low for this call.&lt;/p&gt;
&lt;h2 id=&#34;privatization-to-reduce-contention&#34;&gt;Privatization to reduce contention&lt;/h2&gt;
&lt;p&gt;The use of atomic operations in the previous example introduces contention between threads. As we have previously studied, privatization can be applied in these cases to reduce that contention. In this case, each block will have its own private frontier. The contention is then reduced to atomic operations within a block. An added benefit is that the local frontier is in shared memory, resulting in lower latency atomic operations.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void bfs_kernel(CSRGraph csrGraph, uint *level,
                           uint *prevFrontier, uint *currFrontier,
                           uint numPrevFrontier, uint *numCurrFrontier,
                           uint currLevel) {

    // Initialize privatized frontier
    __shared__ uint currFrontier_s[LOCAL_FRONTIER_CAPACITY];
    __shared__ uint numCurrFrontier_s;

    if (threadIdx.x == 0) {
        numCurrFrontier_s = 0;
    }
    __syncthreads();

    // Perform BFS
    uint i = blockIdx.x * blockDim.x + threadIdx.x;

    if (i &amp;lt; numPrevFrontier) {
        uint vertex = prevFrontier[i];
        for (uint edge = csrGraph.srcPtrs[vertex]; edge &amp;lt; csrGraph.srcPtrs[vertex + 1]; edge++) {
            uint neighbor = csrGraph.dst[edge];
            if (atomicCAS(&amp;amp;level[neighbor], UINT_MAX, currLevel) == UINT_MAX) {
                uint currFrontierIdx_s = atomicAdd(&amp;amp;numCurrFrontier_s, 1);
                if (currFrontierIdx_s &amp;lt; LOCAL_FRONTIER_CAPACITY) {
                    currFrontier_s[currFrontierIdx_s] = neighbor;
                } else {
                    numCurrFrontier_s = LOCAL_FRONTIER_CAPACITY;
                    uint currFrontierIdx = atomicAdd(numCurrFrontier, 1);
                    currFrontier[currFrontierIdx] = neighbor;
                }
            }
        }
    }
    __syncthreads();

    // Allocate in global frontier
    __shared__ uint currFrontierStartIdx;
    if (threadIdx.x == 0) {
        currFrontierStartIdx = atomicAdd(numCurrFrontier, numCurrFrontier_s);
    }
    __syncthreads();

    // Commit to global frontier
    for (uint currFrontierIdx_s = threadIdx.x; currFrontierIdx_s &amp;lt; numCurrFrontier_s; currFrontierIdx_s += blockDim.x) {
        currFrontier[currFrontierStartIdx + currFrontierIdx_s] = currFrontier_s[currFrontierIdx_s];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The main BFS block of the kernel above will write to the local frontier as long as there is space. If the capacity is hit, all future writes will go to the global frontier.&lt;/p&gt;
&lt;p&gt;After BFS completes, a representative thread (index 0) from each block will allocate space in the global frontier, giving it a unique starting index. This allows each block to safely write to the global frontier without contention. The figure below shows the result of the privatized frontier.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-09_20-23-22_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Privatized frontier for BFS traversal (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Privatized frontier for BFS traversal (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;additional-optimizations&#34;&gt;Additional Optimizations&lt;/h2&gt;
&lt;h3 id=&#34;reducing-launch-overhead&#34;&gt;Reducing launch overhead&lt;/h3&gt;
&lt;p&gt;If the frontiers of a BFS are small, the overhead of launching a kernel for each level can be significant. In such cases, a kernel with a grid size of 1 can be launched to handle multiple levels. This block would synchronize after each level to ensure that all threads have completed the current level before moving on to the next.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-09_20-27-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Reducing launch overhead by handling multiple levels in a single kernel (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Reducing launch overhead by handling multiple levels in a single kernel (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;improving-load-balance&#34;&gt;Improving load balance&lt;/h3&gt;
&lt;p&gt;In the first vertex-centric approach we looked at, the threads were not evenly balanced due to the fact that some vertices have more edges than others. For graphs that have high variability in the number of edges per vertex, the frontier can be sorted and placed into multiple buckets. The buckets would be processed by a separate kernel.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Topological Sort</title>
      <link>https://ajdillhoff.github.io/notes/topological_sort/</link>
      <pubDate>Thu, 04 Apr 2024 10:21:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/topological_sort/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#topological-sort&#34;&gt;Topological Sort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#strongly-connected-components&#34;&gt;Strongly Connected Components&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#application-recommender-graphs&#34;&gt;Application: Recommender Graphs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;topological-sort&#34;&gt;Topological Sort&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;topological sort&lt;/strong&gt; of a directed acyclic graph \(G = (V, E)\) is a linear ordering of all its vertices such that for every directed edge \((u, v) \in E\), vertex \(u\) comes before vertex \(v\) in the ordering.&lt;/p&gt;
&lt;p&gt;The process itself can be described simply:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Call \(\text{DFS}(G)\) to compute the finishing times for each vertex \(v\).&lt;/li&gt;
&lt;li&gt;As each vertex is finished, insert it onto the front of a linked list.&lt;/li&gt;
&lt;li&gt;Return the linked list of vertices.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The entire call takes \(\Theta(V + E)\) since DFS\((G)\) takes \(\Theta(V + E)\) time. Inserting each vertex onto the front of the list can be done in constant time.&lt;/p&gt;
&lt;h4 id=&#34;lemma&#34;&gt;Lemma&lt;/h4&gt;
&lt;p&gt;A directed graph \(G\) is acyclic if and only if a DFS of \(G\) yields no &lt;em&gt;back edges&lt;/em&gt; &amp;ndash; an edge \((u, v)\) such that \(v\) is an ancestor of \(u\) in the DFS forest.&lt;/p&gt;
&lt;h4 id=&#34;proof&#34;&gt;Proof&lt;/h4&gt;
&lt;p&gt;The proof is by contradiction: if a back edge exists, then there is a cycle in the graph.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-04_10-59-53_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;A back edge between edges (u) and (v) (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;A back edge between edges (u) and (v) (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Suppose there is a back edge \((u, v)\) as shown in the figure above. In this case, \(v\) is an ancestor of \(u\) in the depth-first forest. There is a path \(v \leadsto u\), so \(v \leadsto u \rightarrow v\) is a cycle.&lt;/p&gt;
&lt;p&gt;In the other direction, suppose that \(G\) contains a cycle \(c\). Let \(v\) be the first vertex discovered in \(c\), and let \((u, v)\) be the preceding edge in \(c\). At time \(v.d\), vertices of \(c\) form a white path \(v \leadsto u\). Since \(u\) is a descendant of \(v\), \((u, v)\) is a back edge. \(\blacksquare\)&lt;/p&gt;
&lt;h4 id=&#34;theorem&#34;&gt;Theorem&lt;/h4&gt;
&lt;p&gt;The topological sort algorithm produces a topological sort of a directed acyclic graph.&lt;/p&gt;
&lt;h4 id=&#34;proof&#34;&gt;Proof&lt;/h4&gt;
&lt;p&gt;Run a DFS on the graph \(G\) to determine finish times for its vertices. For any pair of vertices \((u, v)\), if \(G\) contains an edge from \(u\) to \(v\), then \(v.f &amp;lt; u.f\). (Review DFS).&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-06_13-04-12_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;DAG for topological sorting. Figure 20.8 from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;DAG for topological sorting. Figure 20.8 from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;strongly-connected-components&#34;&gt;Strongly Connected Components&lt;/h2&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-06_14-40-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Strongly connected components of a directed graph (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Strongly connected components of a directed graph (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;A &lt;strong&gt;strongly connected component&lt;/strong&gt; of a directed graph \(G\) is a maximal set of vertices such that for every pair of vertices \(u\) and \(v\) in the set, there is a path from \(u\) to \(v\) and a path from \(v\) to \(u\). The algorithm goes as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Call \(\text{DFS}(G)\) to compute the finishing times for each vertex \(v\).&lt;/li&gt;
&lt;li&gt;Compute the transpose of \(G\).&lt;/li&gt;
&lt;li&gt;Call \(\text{DFS}(G^T)\), but in the main loop of DFS, consider the vertices in order of decreasing finishing times.&lt;/li&gt;
&lt;li&gt;Output the vertices of each tree in the depth-first forest as a separate strongly connected component.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The transpose of a graph \(G^T\) is the graph \(G\) with all edges reversed.&lt;/p&gt;
&lt;p&gt;\begin{align*}
G^T &amp;amp;= (V, E^T) \\
E^T &amp;amp;= \{(v, u) \mid (u, v) \in E\}
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;component-graphs&#34;&gt;Component Graphs&lt;/h3&gt;
&lt;p&gt;The resulting &lt;strong&gt;component graph&lt;/strong&gt; is a directed graph \(G_{SCC} = (V_{SCC}, E_{SCC})\) where each vertex represents a strongly connected component of the original graph \(G\). There is an edge \((C_i, C_j)\) in \(G_{SCC}\) if there is a vertex \(u \in C_i\) and a vertex \(v \in C_j\) such that \((u, v) \in E\). The component graph of the DAG from above is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-06_14-42-39_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Component graph of the DAG from above (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Component graph of the DAG from above (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;lemma&#34;&gt;Lemma&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;The component graph \(G^{SCC}\) is a directed acyclic graph.&lt;/strong&gt; Let \(C\) and \(C&amp;rsquo;\) be distinct strongly connected components in \(G\), where \(u, v \in C\) and \(u&amp;rsquo;, v&amp;rsquo; \in C&amp;rsquo;\), and suppose there is a path \(u \leadsto u&amp;rsquo;\) in \(G\). Then there cannot also be a path \(v&amp;rsquo; \leadsto v\) in \(G\).&lt;/p&gt;
&lt;h4 id=&#34;proof&#34;&gt;Proof&lt;/h4&gt;
&lt;p&gt;Suppose there is a path \(v&amp;rsquo; \leadsto v\) in \(G\). This implies there are paths \(u \leadsto u&amp;rsquo; \leadsto v&amp;rsquo;\) and \(v&amp;rsquo; \leadsto v \leadsto u\). If this were possible, then \(u\) and \(v&amp;rsquo;\) are reachable from each other, which contradicts the assumption that \(C\) and \(C&amp;rsquo;\) are distinct strongly connected components. \(\blacksquare\)&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;h3 id=&#34;finishing-times&#34;&gt;Finishing Times&lt;/h3&gt;
&lt;p&gt;The previous and following lemmas establish, given the algorithm presented above, the rules of the finishing times of strongly connected components. These are used to prove the correctness of the algorithm.&lt;/p&gt;
&lt;h4 id=&#34;lemma&#34;&gt;Lemma&lt;/h4&gt;
&lt;p&gt;Let \(C\) and \(C&amp;rsquo;\) be strongly connected components in a directed graph \(G\). If there is an edge \((u, v) \in E\) such that \(u \in C\) and \(v \in C&amp;rsquo;\), then \(f( C) &amp;gt; f(C&amp;rsquo;)\).&lt;/p&gt;
&lt;h4 id=&#34;proof&#34;&gt;Proof&lt;/h4&gt;
&lt;p&gt;There are two cases to consider depending on which strongly connected component had the first discovered vertex during the first DFS call.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 1&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If \(d( C) &amp;lt; d(C&amp;rsquo;)\), let \(x\) be the first vertex discovered in \(C\). At time \(x.d\), the time of discovery, all vertices in \(C\) and \(C&amp;rsquo;\) are white. Thus, there exists paths of white vertices from \(x\) to all vertices in \(C\) and \(C&amp;rsquo;\).&lt;/li&gt;
&lt;li&gt;By the &lt;a href=&#34;https://ajdillhoff.github.io/notes/introduction_to_graph_theory/&#34;&gt;white-path theorem&lt;/a&gt;, all vertices in \(C\) and \(C&amp;rsquo;\) are descendants of \(x\) in the depth-first tree.&lt;/li&gt;
&lt;li&gt;By the parenthesis theorem, \(x.f = f( C) &amp;gt; f(C&amp;rsquo;)\).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Case 2&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If \(d( C) &amp;gt; d(C&amp;rsquo;)\), let \(y\) be the first vertex discovered in \(C&amp;rsquo;\). At time \(y.d\), all vertices in \(C\) and \(C&amp;rsquo;\) are white. Thus, there exists paths of white vertices from \(y\) to all vertices in \(C&amp;rsquo;\). All vertices in \(C&amp;rsquo;\) become descendants of \(y\). Again, \(y.f = f(C&amp;rsquo;)\).&lt;/li&gt;
&lt;li&gt;At time \(y.d\), all vertices in \(C\) are also white.&lt;/li&gt;
&lt;li&gt;Since there is an edge \((u, v)\), where \(u \in C\) and \(u&amp;rsquo; \in C&amp;rsquo;\), we cannot have a path from \(C&amp;rsquo;\) to \(C\).&lt;/li&gt;
&lt;li&gt;No vertex in \(C\) is reachable from \(y\).&lt;/li&gt;
&lt;li&gt;Therefore, at time \(y.f\), all vertices in \(C\) are white.&lt;/li&gt;
&lt;li&gt;Therefore, for all \(w \in C, w.f &amp;gt; y.f\), which implies that \(f( C) &amp;gt; f(C&amp;rsquo;)\). \(\blacksquare\)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;corollary&#34;&gt;Corollary&lt;/h4&gt;
&lt;p&gt;Let \(C\) and \(C&amp;rsquo;\) be distinct strongly connected components in \(G\). Suppose there is an edge \((u, v) \in E^T\), where \(u \in C\) and \(v \in C&amp;rsquo;\). Then \(f( C) &amp;lt; f(C&amp;rsquo;)\).&lt;/p&gt;
&lt;h4 id=&#34;proof&#34;&gt;Proof&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;\((u, v) \in E^T \implies (v, u) \in E\)&lt;/li&gt;
&lt;li&gt;Since strongly connected components of \(G\) and \(G^T\) are the same, \(f(C&amp;rsquo;) &amp;gt; f( C)\).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;correctness&#34;&gt;Correctness&lt;/h3&gt;
&lt;p&gt;Now we can combine the previous results to prove the correctness of the algorithm.&lt;/p&gt;
&lt;h4 id=&#34;corollary&#34;&gt;Corollary&lt;/h4&gt;
&lt;p&gt;Let \(C\) and \(C&amp;rsquo;\) be distinct strongly connected components in \(G\), and suppose that \(f( C) &amp;gt; f(C&amp;rsquo;)\). Then there cannot be an edge from \(C\) to \(C&amp;rsquo;\) in \(G^T\).&lt;/p&gt;
&lt;h4 id=&#34;proof&#34;&gt;Proof&lt;/h4&gt;
&lt;p&gt;When we perform the second DFS call, on \(G^T\), it starts with the component \(C\) such that \(f( C)\) is the maximum. This call starts from some \(x \in C\) and explores all vertices in \(C\). The corollary says that since \(f( C) &amp;gt; f(C&amp;rsquo;)\), there cannot be an edge from \(C\) to \(C&amp;rsquo;\) in \(G^T\). Therefore, DFS will visit &lt;em&gt;only&lt;/em&gt; vertices in \(C\). This means that the depth-first tree rooted at \(x\) will contain only vertices in \(C\).&lt;/p&gt;
&lt;p&gt;The next root chosen is in \(C&amp;rsquo;\) such that \(f(C&amp;rsquo;)\) is maximum over all strongly connected components &lt;strong&gt;other than&lt;/strong&gt; \(C\). DFS visits all vertices in \(C&amp;rsquo;\), but the only edges out of \(C&amp;rsquo;\) go to \(C\), &lt;strong&gt;which have already been visited.&lt;/strong&gt; Therefore, the only tree edges will be to vertices in \(C&amp;rsquo;\).&lt;/p&gt;
&lt;p&gt;As this process continues, we can observe that each root chosen for the second DFS can reach only&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;vertices in its own strongly connected component, and&lt;/li&gt;
&lt;li&gt;vertices in strongly connected components &lt;em&gt;already visited&lt;/em&gt; in the second DFS.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;application-recommender-graphs&#34;&gt;Application: Recommender Graphs&lt;/h2&gt;
&lt;p&gt;A recommender graph is a directed graph where each vertex represents an item and each edge represents a transition between the items based on the context of the data. For example, in a movie recommender graph, each vertex represents a movie, and an edge from \(u\) to \(v\) indicates that users typically transitioned from watching movie \(u\) to watching movie \(v\). The weight of such an edge could be the number of users who made the transition or the average rating improvement when moving from \(u\) to \(v\) (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Lamprecht, Strohmaier, and Helic 2017&lt;/a&gt;).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-07_16-56-29_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;A recommender graph (&amp;lt;a href=&amp;#34;#citeproc_bib_item_2&amp;#34;&amp;gt;Lamprecht, Strohmaier, and Helic 2017&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;A recommender graph (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Lamprecht, Strohmaier, and Helic 2017&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In such a graph, strongly connected components can be used to identify groups of items that are closely related. Based on this information, a recommender system can suggest items that are similar to the ones a user has already interacted with. If the edges contained information such as improvement of ratings, the recommendation system could suggest items that are likely to be enjoyed by the user.&lt;/p&gt;
&lt;p&gt;Identifying such a strongly connected component can also provide insights into the structure of the data. Given the current recommender graph, it is possible that the strongly connected component related to a particular sub-genre of movies is small, leading to a cycle of recommendations within that sub-genre. This discovery would prompt the recommender system to suggest items from other genres to provide a more diverse set of recommendations.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Lamprecht, Daniel, Markus Strohmaier, and Denis Helic. 2017. “A Method for Evaluating Discoverability and Navigability of Recommendation Algorithms.” &lt;i&gt;Computational Social Networks&lt;/i&gt; 4 (1): 9. &lt;a href=&#34;https://doi.org/10.1186/s40649-017-0045-3&#34;&gt;https://doi.org/10.1186/s40649-017-0045-3&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Parallel Sorting Algorithms</title>
      <link>https://ajdillhoff.github.io/notes/parallel_sorting_algorithms/</link>
      <pubDate>Sun, 31 Mar 2024 10:50:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/parallel_sorting_algorithms/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#radix-sort&#34;&gt;Radix Sort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimizing-memory-access-efficiency&#34;&gt;Optimizing Memory Access Efficiency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#choosing-a-different-radix-value&#34;&gt;Choosing a different Radix value&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;radix-sort&#34;&gt;Radix Sort&lt;/h2&gt;
&lt;p&gt;For a background on Radix Sort, see these notes on &lt;a href=&#34;https://ajdillhoff.github.io/notes/sorting_in_linear_time/&#34;&gt;Sorting in Linear Time&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Radix sort relies on counting sort for each section, and each section must be processed before moving onto the next. The parallel solution will not attempt to address this sequential dependency. Instead, we will focus on the parallelization of the counting sort step.&lt;/p&gt;
&lt;p&gt;Each thread must determine where to place its input elements. For each bit, the thread will assign it to either a 0 or 1 bucket. Since all values will either be 0 or 1, the thread needs to compute the number of 0s and 1s that come before it in the current section. Radix sort is also a stable sort, so the order of elements with the same key must be preserved. Consider the following array separated into 4 threads of 4 elements each:&lt;/p&gt;
&lt;p&gt;\begin{array}{l|cccc|cccc|cccc|cccc}
Value &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0\\
Index &amp;amp; 0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4 &amp;amp; 5 &amp;amp; 6 &amp;amp; 7 &amp;amp; 8 &amp;amp; 9 &amp;amp; 10 &amp;amp; 11 &amp;amp; 12 &amp;amp; 13 &amp;amp; 14 &amp;amp; 15\\
\end{array}&lt;/p&gt;
&lt;p&gt;The least significant bits for each thread are \([1, 0, 1, 0]\). From thread 4&amp;rsquo;s perspective, there are 2 1s and a single 0 that come before it. Its key index is 3 (using 0-based indexing), so it only needs to compute the number of 1s that come before it and subtract that from its key index: \(3 - 2 = 1\). More generally, for a 0 bit:&lt;/p&gt;
&lt;p&gt;\[
\text{output index} = \text{key index} - \text{number of 0s that come before it}
\]&lt;/p&gt;
&lt;p&gt;The calculation for the 1 bit hinges on the fact that all keys mapping to 0 must come before it.&lt;/p&gt;
&lt;p&gt;\[
\text{output index} = \text{input size} - \text{number of ones total} + \text{number of 1s that come before it}
\]&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void radix_sort_iter(unsigned int *input, unsigned int *output,
                                unsigned int *bits, unsigned int N, unsigned int iter) {
    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int key, bit;

    if (idx &amp;lt; N) {
        key = input[i];
        bit = (key &amp;gt;&amp;gt; iter) &amp;amp; 1;
        bits[i] = bit;
    }

    exclusiveScan(bits, N);

    if (idx &amp;lt; N) {
        unsigned int numOnesBefore = bits[idx];
        unsigned int numOnesTotal = bits[N];
        unsigned int dst = (bit == 0) ? idx - numOnesBefore
                                      : N - numOnesTotal + numOnesBefore;
        output[dst] = key;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Consider the fourth thread with &lt;code&gt;idx = 3&lt;/code&gt; using the array from above. This thread index is certainly less than the array size, so the &lt;code&gt;key&lt;/code&gt; is read from &lt;code&gt;input&lt;/code&gt; before extracting the least significant bit. &lt;strong&gt;Note that there is a call to thread synchronization inside &lt;code&gt;exclusiveScan&lt;/code&gt;.&lt;/strong&gt; The result of &lt;code&gt;exclusiveScan&lt;/code&gt; is an array that indicates, for each index, the number of ones that came before it. For our array, this is:&lt;/p&gt;
&lt;p&gt;\[
[0, 1, 1, 2]
\]&lt;/p&gt;
&lt;p&gt;The destination can be computed for each thread. The result is shown in the table below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Thread&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Bit&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;#1s Before&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;#1s Total&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Output Index&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;optimizing-memory-access-efficiency&#34;&gt;Optimizing Memory Access Efficiency&lt;/h2&gt;
&lt;p&gt;Each thread write their keys to global memory in an uncoalesced manner. This can be optimized by having each block maintain local buckets in shared memory. The keys within each block will be coalesced when written to global memory.&lt;/p&gt;
&lt;p&gt;TODO: Show visualization similar to 13.5&lt;/p&gt;
&lt;p&gt;In order to make this work, each thread needs to calculate where in the output array the values from its bucket should be placed. For 0 bits, the block&amp;rsquo;s 0 bucket will come after the 0 buckets from all previous blocks. These positions can be computed by performing an exclusive scan on the block&amp;rsquo;s local bucket sizes.&lt;/p&gt;
&lt;p&gt;TODO: Show visualization similar to 13.6&lt;/p&gt;
&lt;h2 id=&#34;choosing-a-different-radix-value&#34;&gt;Choosing a different Radix value&lt;/h2&gt;
&lt;p&gt;Picking a larger radix value will reduce the number of iterations required to sort the array.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sparse Matrix Computation</title>
      <link>https://ajdillhoff.github.io/notes/sparse_matrix_computation/</link>
      <pubDate>Sat, 30 Mar 2024 10:36:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/sparse_matrix_computation/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coordinate-list-format--coo&#34;&gt;Coordinate List Format (COO)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compressed-sparse-row-format--csr&#34;&gt;Compressed Sparse Row Format (CSR)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ell-format&#34;&gt;ELL Format&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ell-coo-format&#34;&gt;ELL-COO Format&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#jagged-diagonal-storage-format--jds&#34;&gt;Jagged Diagonal Storage Format (JDS)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Sparse matrices are matrices with mostly zero elements. They are common in scientific computing, machine learning, and other fields. It is important to study them in the context of GPU computing because they can be very large and require a lot of memory. Effeciently representing and computing with sparse matrices provides a substantial benefit to many applications.&lt;/p&gt;
&lt;p&gt;The obvious benefits of sparsity is that we can typically represent the same matrix using a smaller memory footprint. Fewer elements means fewer wasted operations as well. The challenges of GPU implementations are that the memory access patterns are not always friendly to the GPU architecture. This is especially true for sparse matrices, where the memory access patterns are often irregular.&lt;/p&gt;
&lt;p&gt;These notes will review the sparse matrix formats as presented in (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;). Each will be evaluated using the following criteria:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Compaction&lt;/strong&gt;: How well does the format compact the data?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexibility&lt;/strong&gt;: Is the format easy to modify?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accessibility:&lt;/strong&gt; How easy is it to access the data?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory access efficiency:&lt;/strong&gt; Are the accesses coalesced?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Load balance:&lt;/strong&gt; Are the operations balanced across threads?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;coordinate-list-format--coo&#34;&gt;Coordinate List Format (COO)&lt;/h2&gt;
&lt;p&gt;This format stores non-zero elements in a 1D array of values. It also requires two 1D arrays to store the row and column indices, incurrent an overhead of 2N. The values in each array are contiguous, which is good for memory access.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-02_19-42-49_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;COO Format (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;COO Format (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;kernel-implementation&#34;&gt;Kernel Implementation&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void spmv_coo_kernel(COOMatrix cooMatrix, float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &amp;lt; cooMatrix.numNonzeros) {
        int row = cooMatrix.rowIdx[i];
        int col = cooMatrix.colIdx[i];
        float val = cooMatrix.values[i];
        atomicAdd(&amp;amp;y[row], val * x[col]);
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Compaction&lt;/strong&gt;: Compared to representing the matrices in dense format, the COO format is very compact. However, it is not as compact as some other sparse matrix formats. It requires an additional over head of 2N elements to store the row and column indices.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; Indices and values can be easily modified in this format. This is good for applications that require frequent modifications.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accessibility:&lt;/strong&gt; It is easy to access nonzero elements. It is &lt;strong&gt;not&lt;/strong&gt; easy to access the original 0s in each row.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory access efficiency:&lt;/strong&gt; The values in this format are contiguous, resulting in coalesced memory access.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Load balance:&lt;/strong&gt; The data is uniformly distributed across threads, resulting in good load balance.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One major drawback, as seen in the code above, is the use of atomic operations.&lt;/p&gt;
&lt;h2 id=&#34;compressed-sparse-row-format--csr&#34;&gt;Compressed Sparse Row Format (CSR)&lt;/h2&gt;
&lt;p&gt;The key idea of this format is that each thread is responsible for all nonzeros in a row.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-02_19-50-47_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;CSR Format (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;CSR Format (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;kernel-implementation&#34;&gt;Kernel Implementation&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void spmv_csr_kernel(CSRMatrix csrMatrix, float *x, float *y) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    if (row &amp;lt; csrMatrix.numRows) {
        float sum = 0.0f;
        for (int j = csrMatrix.rowPtr[row]; j &amp;lt; csrMatrix.rowPtr[row + 1]; j++) {
            sum += csrMatrix.values[j] * x[csrMatrix.colIdx[j]];
        }
        y[i] = sum;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The rows are mapped to a single pointer index, so it only needs \(m\) entries to store them. The columns are not required to be in order. If the columns &lt;em&gt;are&lt;/em&gt; in order, the data is represented in row-major order without the zero elements.&lt;/p&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Compaction&lt;/strong&gt;: The CSR format is more compact than the COO format since it only requires \(m\) entries to store the row pointers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; The CSR format is not as flexible as the COO format. It is not easy to modify the values or indices.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accessibility:&lt;/strong&gt; There is less parallelization than COO due to the row sizes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory access efficiency:&lt;/strong&gt; The memory access pattern is poor since the data is separated over columns.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Load balance:&lt;/strong&gt; The load is not balanced across threads. Some threads will have more work than others, leading to control divergence.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;ell-format&#34;&gt;ELL Format&lt;/h2&gt;
&lt;p&gt;ELL fixes the non-coalesced memory accesses of CSR via data padding and transposition. This is visualized below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Start with CSR format&lt;/li&gt;
&lt;li&gt;Pad rows to equal size&lt;/li&gt;
&lt;li&gt;Store in column-major order&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-02_19-53-05_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;ELL Format (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;ELL Format (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;kernel-implementation&#34;&gt;Kernel Implementation&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void spmv_ell_kernel(ELLMatrix ellMatrix, float *x, float *y) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    if (row &amp;lt; ellMatrix.numRows) {
        float sum = 0.0f;
        for (int j = 0; j &amp;lt; ellMatrix.nnzPerRow[row]; j++) {
            int col = ellMatrix.colIdx[j * ellMatrix.numRows + row];
            sum += ellMatrix.values[j * ellMatrix.numRows + row] * x[col];
        }
        y[row] = sum;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Compaction:&lt;/strong&gt; Padding the rows means this is less space efficient than CSR.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; More flexible than CSR; adding nonzeros in CSR requires a shift of values. This format can replaced a padded element if necessary.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accessibility:&lt;/strong&gt; ELL can return the row given the index of a nonzero element as well as the nonzero of a row given that index.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory access efficiency:&lt;/strong&gt; Consecutive threads access consecutive memory locations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Load balance:&lt;/strong&gt; Shares the same control divergence issues as CSR.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;ell-coo-format&#34;&gt;ELL-COO Format&lt;/h2&gt;
&lt;p&gt;ELL-COO combines the two formats to improve space efficiency and control divergence.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-02_20-01-43_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;ELL-COO Format (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;ELL-COO Format (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Compaction:&lt;/strong&gt; ELL-COO has the same compaction as ELL.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; ELL-COO is more flexible than ELL thanks to inclusion of the COO format.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accessibility:&lt;/strong&gt; It is not always possible to access all nonzeros given a row index.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory access efficiency:&lt;/strong&gt; The memory access pattern is coalesced.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Load balance:&lt;/strong&gt; COO reduces the control divergence seen in ELL alone.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;jagged-diagonal-storage-format--jds&#34;&gt;Jagged Diagonal Storage Format (JDS)&lt;/h2&gt;
&lt;p&gt;The last format we will consider is the Jagged Diagonal Storage format. This format reduces divergence and improves memory coalescing without padding. The main idea is to sort the rows by length from longest to shortest.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Group nonzeros by row&lt;/li&gt;
&lt;li&gt;Sort rows by length while preserving their original row indices&lt;/li&gt;
&lt;li&gt;Store in column-major order&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-02_20-07-30_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;JDS Format (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;JDS Format (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Compaction:&lt;/strong&gt; Avoid paddding, so it is more space efficient than ELL.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; Less flexible than ELL since it requires sorting when adding new elements.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accessibility:&lt;/strong&gt; Cannot access a row and column given the index of a nonzero element.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory access efficiency:&lt;/strong&gt; Without padding, the starting location of memory accesses in each iteration can vary.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Load balance:&lt;/strong&gt; Since the rows are sorted, threads of the same warp are likely to iterate over rows of similar length.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Recursion Tree Method</title>
      <link>https://ajdillhoff.github.io/notes/recursion_tree_method/</link>
      <pubDate>Mon, 18 Mar 2024 22:10:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/recursion_tree_method/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#example-4-dot-13-from-clrs&#34;&gt;Example 4.13 from CLRS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Visualizing the characteristics of an algorithm is a great way to build intuition about its runtime. Although it can be used to prove a recurrence, it is often a good jumping off point for the &lt;a href=&#34;https://ajdillhoff.github.io/notes/substitution_method/&#34;&gt;Substitution Method&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;example-4-dot-13-from-clrs&#34;&gt;Example 4.13 from CLRS&lt;/h2&gt;
&lt;p&gt;Consider the recurrence \(T(n) = 3T(n/4) + \Theta(n^2)\). We start by describing \(\Theta(n^2) = cn^2\), where the constant \(c &amp;gt; 0\) serves as an upper-bound constant. It reflects the amount of work done at each level of the recursion tree. The tree is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-19_09-14-00_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Example 4.13 from CLRS (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Example 4.13 from CLRS (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;As the tree expands out over a few levels, we can see a pattern in the cost at depth \(i\). Each level of increasing depth has 3 times as many nodes as the previous. With the exception of the leaves, the cost for each level is \((\frac{3}{16})^i cn^2\). The total cost of the leaves is based on the number of leaves, which is \(3^{\log_4 n}\) since each level has \(3^i\) nodes and the depth is \(\log_4 n\). Using the identity \(a^{\log_b c} = c^{\log_b a}\), we can simplify the leaves to \(n^{\log_4 3}\). The total cost of the leaves is \(\Theta(n^{\log_4 3})\).&lt;/p&gt;
&lt;p&gt;The last step is to add up the costs over all levels:&lt;/p&gt;
&lt;p&gt;\begin{align*}
T(n) &amp;amp;= \sum_{i=0}^{\log_4 n} \left( \frac{3}{16} \right)^i cn^2 + \Theta(n^{\log_4 3}) \\
&amp;amp;&amp;lt; \sum_{i=0}^{\infty} \left( \frac{3}{16} \right)^i cn^2 + \Theta(n^{\log_4 3}) \\
&amp;amp;= \frac{cn^2}{1 - \frac{3}{16}} + \Theta(n^{\log_4 3}) \\
&amp;amp;= \frac{16}{13}cn^2 + \Theta(n^{\log_4 3}) \\
&amp;amp;= \Theta(n^2).
\end{align*}&lt;/p&gt;
&lt;p&gt;The second line in the equation is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Geometric_series&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;geometric series.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;verifying-using-the-substitution-method&#34;&gt;Verifying using the Substitution Method&lt;/h3&gt;
&lt;p&gt;Even if we weren&amp;rsquo;t so particular with the maths, the recursion tree method is a great way to build intuition about the runtime. Let&amp;rsquo;s verify that this recurrence is bounded above by \(O(n^2)\) using the &lt;a href=&#34;https://ajdillhoff.github.io/notes/substitution_method/&#34;&gt;Substitution Method&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here we show that \(T(n) \leq dn^2\) for a constant \(d &amp;gt; 0\). The previous constant \(c &amp;gt; 0\) is reused to describe the cost at each level of the recursion tree.&lt;/p&gt;
&lt;p&gt;\begin{align*}
T(n) &amp;amp;\leq 3T(n/4) + cn^2 \\
&amp;amp;\leq 3(d(n/4)^2) + cn^2 \\
&amp;amp;= \frac{3}{16}dn^2 + cn^2 \\
&amp;amp;\leq dn^2 \text{ if } d \geq \frac{16}{13}c.
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;exercises&#34;&gt;Exercises&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Solve the recurrence \(T(n) = 2T(n/2) + cn\) using the recursion tree method.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Greedy Algorithms</title>
      <link>https://ajdillhoff.github.io/notes/greedy_algorithms/</link>
      <pubDate>Mon, 18 Mar 2024 14:45:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/greedy_algorithms/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#activity-selection&#34;&gt;Activity Selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#properties-of-greedy-solutions&#34;&gt;Properties of Greedy Solutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#huffman-codes&#34;&gt;Huffman Codes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Greedy algorithms are a class of algorithms that yield &lt;em&gt;locally&lt;/em&gt; optimal solutions. In cases where the local optimum is also the global optimum, greedy algorithms are ideal. Even in cases where the global solution is more elusive, a local solution may be sufficient.&lt;/p&gt;
&lt;h2 id=&#34;activity-selection&#34;&gt;Activity Selection&lt;/h2&gt;
&lt;p&gt;Given a set of activities that need to be scheduled using a common resource, the &lt;strong&gt;activity selection&lt;/strong&gt; problem is to find the maximum number of activities that can be scheduled without overlapping.&lt;/p&gt;
&lt;p&gt;Each activity has a start time \(s_i\) and finish time \(f_i\), where \(0 \leq s_i &amp;lt; f_i &amp;lt; \infty\). An activity \(a_i\) takes place over the interval \([s_i, f_i)\). Two activities \(a_i\) and \(a_j\) are mutually compatible if \(s_i \geq f_j\) or \(s_j \geq f_i\).&lt;/p&gt;
&lt;p&gt;Sort activities by their finish time. &lt;strong&gt;Objective:&lt;/strong&gt; Find the largest subset of mutually compatible activities.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;\(i\)&lt;/th&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;\(s_i\)&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;\(f_i\)&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-22_17-14-45_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Visualization of activities over time (Cormen et al. 2022).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Visualization of activities over time (Cormen et al. 2022).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;How many mutually compatible sets are there?&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;\(\{a_1, a_3, a_6, a_8\}\)&lt;/li&gt;
&lt;li&gt;\(\{a_1, a_3, a_6, a_9\}\)&lt;/li&gt;
&lt;li&gt;\(\{a_1, a_3, a_7, a_9\}\)&lt;/li&gt;
&lt;li&gt;\(\{a_1, a_5, a_7, a_8\}\)&lt;/li&gt;
&lt;li&gt;\(\{a_1, a_5, a_7, a_9\}\)&lt;/li&gt;
&lt;li&gt;\(\{a_2, a_5, a_7, a_8\}\)&lt;/li&gt;
&lt;li&gt;\(\{a_2, a_5, a_7, a_8\}\)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;optimal-substructure&#34;&gt;Optimal Substructure&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;How do we verify that this problem has optimal substructure?&lt;/strong&gt; First, it is important to formalize the problem based on the definition given previously. Define \(S_{ij}\) as the set of all activities that start after \(a_i\) finishes and finish before \(a_j\) starts.&lt;/p&gt;
&lt;p&gt;\[
S_{ij} = \{a_k \in S : f_i \leq s_k &amp;lt; f_k \leq s_j\}
\]&lt;/p&gt;
&lt;p&gt;This defines a clear subset of the original set of data. That is, we have defined a subproblem of the original problem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Which activities are those in \(S_{ij}\) compatible with?&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;any \(a_i\) that finish by \(f_i\)&lt;/li&gt;
&lt;li&gt;any \(a_i\) that start no earlier than \(s_j\).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Given this subset, our subproblem is that of finding a maximum set of mutually compatible activities in \(S_{ij}\), denoted \(A_{ij}\). If \(a_k \in A_{ij}\), we are left with two subproblems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Find mutually compatible activities in \(S_{ik}\) &amp;ndash; starts after \(a_i\) finishes and finish before \(a_k\) starts.&lt;/li&gt;
&lt;li&gt;Find mutually compatible activities in \(S_{kj}\) &amp;ndash; starts after \(a_k\) finishes and finish before \(a_j\) start.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The two subsets above are defined as \(A_{ik} = A_{ij} \cap S_{ik}\) and \(A_{kj} = A_{ij} \cap S_{kj}\), respectively. Then \(A_{ij} = A_{ik} \cup \{a_k\} \cup A_{kj}\). The size of the set is given by \(|A_{ij}| = |A_{ik}| + 1 + |A_{kj}|\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Claim:&lt;/strong&gt; If our problem has optimal substructure, then the optimal solution \(A_{ij}\) must include optimal solutions for \(S_{ik}\) and \(S_{kj}\).&lt;/p&gt;
&lt;p&gt;This claim can be proven using the &lt;strong&gt;cut-and-paste&lt;/strong&gt; method used in &lt;a href=&#34;https://ajdillhoff.github.io/notes/dynamic_programming/&#34;&gt;Dynamic Programming&lt;/a&gt;. This technique works by showing that if a solution to a problem is not optimal, then there exists a way to &lt;strong&gt;cut&lt;/strong&gt; the suboptimal portion and &lt;strong&gt;paste&lt;/strong&gt; an optimal one. This will lead to a contradiction because the original was assumed to be optimal.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Suppose that \(A_{kj}\) is not optimal, and we could find a set \(A&amp;rsquo;_{kj}\) that is larger. Then we could replace \(A_{kj}\) with \(A&amp;rsquo;_{kj}\) in \(A_{ij}\) to obtain a larger set. This contradicts the assumption that \(A_{ij}\) is optimal.&lt;/p&gt;
&lt;p&gt;Simply put, if the claim is that the given solution is optimal, and the solution is constructed from optimal solutions to subproblems, then there cannot exist any other solution that is better. Another way to look at this: if we construct optimal solutions to subproblems, then the solution to the original problem must be optimal.&lt;/p&gt;
&lt;h3 id=&#34;recursive-solution&#34;&gt;Recursive Solution&lt;/h3&gt;
&lt;p&gt;Let \(c[i, j]\) be the size of the optimal solution for \(S_{ij}\). Based on the above discussion, the size is computed as&lt;/p&gt;
&lt;p&gt;\[
c[i, j] = c[i, k] + c[k, j] + 1.
\]&lt;/p&gt;
&lt;p&gt;This dynamic programming solution assumes we know the optimal solution for all subproblems. To know this, we need to examine all possibilities which include \(a_k\) in the solution.&lt;/p&gt;
&lt;p&gt;\[
c[i, j] = \begin{cases}
0 &amp;amp; \text{if } S_{ij} = \emptyset, \\
\max \{c[i, k] + c[k, j] + 1 : a_k \in S_{ij}\} &amp;amp; \text{if } S_{ij} \neq \emptyset.
\end{cases}
\]&lt;/p&gt;
&lt;h3 id=&#34;greedy-solution&#34;&gt;Greedy Solution&lt;/h3&gt;
&lt;p&gt;The greedy solution is the naive one: select an activity that leaves the resource available for as many other activities as possible, which is the activity that finishes first. If multiple activities finish at the same time, select one arbitrarily.&lt;/p&gt;
&lt;p&gt;The subproblem is that of finding a maximum size set of mutually compatible activities that start after \(a_1\) finishes. More generally, the optimal solution consists of all \(a_i\) that start after \(a_k\) finishes, where \(a_k\) is the last activity to finish:&lt;/p&gt;
&lt;p&gt;\[
S_{k} = \{a_i \in S : s_i \geq f_k\}.
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is the greedy solution optimal?&lt;/strong&gt; Suppose that \(a_m \in S_k\) is the activity that finishes first. Then it must be included in the maximum size subset of mutually compatible activities \(A_k\). Suppose we are given \(A_k\) and we look at \(a_j \in A_k\), the activity that finishes first. If \(a_j = a_m\), then the greedy solution is optimal. If \(a_j \neq a_m\), then we can replace \(a_j\) with \(a_m\) since they are both compatible with all other activities in \(A_k\). Picking the activity that finishes first does not change the size of the optimal solution.&lt;/p&gt;
&lt;p&gt;The solution is top-down:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;pick the solution that finishes first,&lt;/li&gt;
&lt;li&gt;remove all activities that are incompatible with the chosen activity,&lt;/li&gt;
&lt;li&gt;repeat until no activities remain.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;recursive-greedy-algorithm&#34;&gt;Recursive Greedy Algorithm&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;recursive_activity_selector&lt;/span&gt;(s, f, k, n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; m &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; s[m] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; f[k]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        m &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; m &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; n:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; [m] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; recursive_activity_selector(s, f, m, n)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This algorithm assumes that &lt;code&gt;f&lt;/code&gt; is sorted in increasing order. The index &lt;code&gt;k&lt;/code&gt; represents the index of the current subproblem. The number of activities is given by &lt;code&gt;n&lt;/code&gt;. The &lt;code&gt;while&lt;/code&gt; loop increments &lt;code&gt;m&lt;/code&gt; until it finds an activity that starts after activity &lt;code&gt;k&lt;/code&gt; finishes. If such an activity exists, it is added to the solution set and the algorithm is called recursively with the new subproblem.&lt;/p&gt;
&lt;h4 id=&#34;example&#34;&gt;Example&lt;/h4&gt;
&lt;p&gt;A run of this algorithm is visualized below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-24_14-42-29_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Recursive activity selector example (Cormen et al. 2022).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Recursive activity selector example (Cormen et al. 2022).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;analysis&#34;&gt;Analysis&lt;/h4&gt;
&lt;p&gt;At first glance, the algorithm appears to be \(O(n^2)\) because of the &lt;code&gt;while&lt;/code&gt; loop coupled with the recursive call. Once an activity has been selected, the recursive call only considers the activities next \(n - k\) activities. The &lt;code&gt;while&lt;/code&gt; loop picks up where the previous call left off, so the total number of iterations is \(n\). The algorithm is \(O(n)\).&lt;/p&gt;
&lt;h3 id=&#34;iterative-algorithm&#34;&gt;Iterative Algorithm&lt;/h3&gt;
&lt;p&gt;The above solution can be adapted to an iterative one.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;greedy_activity_selector&lt;/span&gt;(s, f):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(s) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; m &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, n &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; s[m] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; f[k]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            A&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(m)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; A
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the &lt;code&gt;for&lt;/code&gt; loop, the first line essentially asks if \(a_m \in S_k\). If so, then add it to the solution set and update \(k\) to \(m\).&lt;/p&gt;
&lt;h4 id=&#34;analysis&#34;&gt;Analysis&lt;/h4&gt;
&lt;p&gt;The analysis of the iterative approach is much clearer. The loop goes over all activities once, so the algorithm is \(O(n)\).&lt;/p&gt;
&lt;h2 id=&#34;properties-of-greedy-solutions&#34;&gt;Properties of Greedy Solutions&lt;/h2&gt;
&lt;p&gt;You can probably imagine a problem for which a greedy solution would not provide the optimal solution. Path planning is one such problem. If we greedily chose the shortest path at each step, we may have missed a shorter path that is not the shortest at each step. The activity selection problem just so happens to be a perfect candidate for a greedy solution, &lt;strong&gt;but what makes it so?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s review the major steps that led us to the greedy solution for activity selection.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Determine the optimal substructure.&lt;/li&gt;
&lt;li&gt;Develop a recursive solution.&lt;/li&gt;
&lt;li&gt;Show that making the greedy choice leaves only a single subproblem.&lt;/li&gt;
&lt;li&gt;Prove that making the greedy choice leads to an optimal solution.&lt;/li&gt;
&lt;li&gt;Develop a recursive algorithm.&lt;/li&gt;
&lt;li&gt;Convert it to an iterative algorithm.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first couple of steps are common to dynamic programming problems. In this case, we could have jumped straight to the greedy approach. Filtering out these extra steps leaves us with:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Cast the optimization problem as one in which we make a choice and are left with a single subproblem.&lt;/li&gt;
&lt;li&gt;Prove that the greedy choice is optimal.&lt;/li&gt;
&lt;li&gt;Demonstrate optimal substructure: if you make a greedy choice, then you are left with a subproblem such that combining an optimal solution with the greedy choice made previously, you end up with an optimal solution to the original problem.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As we will see, we need two properties to prove that a greedy solution is optimal: the &lt;strong&gt;greedy choice property&lt;/strong&gt; and the &lt;strong&gt;optimal substructure property&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;greedy-choice-property&#34;&gt;Greedy Choice Property&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;greedy choice property&lt;/strong&gt; states that the optimal solution can be found by making locally greedy choices. This approach is opposite of dynamic programming, where the choices at each step are made from the knowledge of optimal solutions to subproblems. That is, dynamic programming is a &lt;strong&gt;bottom-up&lt;/strong&gt; approach.&lt;/p&gt;
&lt;p&gt;A greedy solution also makes a choice at each step, but it is only based on local information. This is a &lt;strong&gt;top-down&lt;/strong&gt; approach. They key of this property is to show that the greedy choice is optimal at each step. For the activity selection problem, the steps were&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Examine the optimal solution.&lt;/li&gt;
&lt;li&gt;If it has the greedy choice, then the greedy choice is optimal.&lt;/li&gt;
&lt;li&gt;If it does not have the greedy choice, then replace the suboptimal choice with the greedy choice.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;optimal-substructure-property&#34;&gt;Optimal Substructure Property&lt;/h3&gt;
&lt;p&gt;A problem has optimal substructure if the optimal solution contains optimal solutions to subproblems. We demonstrated this earlier for activity selection. We can start with the assumption that we arrived to a subproblem by making greedy choices. The next step is to show that the optimal solution to the subproblem combined with the greedy choice leads to an optimal solution to the original problem.&lt;/p&gt;
&lt;h3 id=&#34;greedy-vs-dot-dynamic-programming&#34;&gt;Greedy vs. Dynamic Programming&lt;/h3&gt;
&lt;p&gt;Since there is such overlap between greedy algorithms and dynamic programming in terms of their properties, it is important to understand the differences between the two. To illustrate these difference, we will look at two variations of the same problem.&lt;/p&gt;
&lt;h4 id=&#34;0-1-knapsack-problem&#34;&gt;\(0-1\) Knapsack Problem&lt;/h4&gt;
&lt;p&gt;Consider the &lt;strong&gt;\(0-1\) knapsack problem.&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You have \(n\) items.&lt;/li&gt;
&lt;li&gt;Item \(i\) is worth \(v_i\) and weighs \(w_i\).&lt;/li&gt;
&lt;li&gt;Find the most valuable subset of items with total weight less than or equal to \(W\).&lt;/li&gt;
&lt;li&gt;Items cannot be divided.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the most valuable subset of items weighing at most \(W\) includes item \(j\), then the remaining weight must be the most valuable subset of items weighing at most \(W - w_j\) taken from \(n-1\) original items excluding item \(j\).&lt;/p&gt;
&lt;h4 id=&#34;fractional-knapsack-problem&#34;&gt;Fractional Knapsack Problem&lt;/h4&gt;
&lt;p&gt;This is similar to the \(0-1\) knapsack problem, but items can be divided. The objective is to maximize the value of the items in the knapsack.&lt;/p&gt;
&lt;p&gt;The optimal substructure of this problem varies slightly: if the most valuable subset weighing at most \(W\) includes the weight \(w\) of item \(j\), then the remaining weight must be the most valuable subset weighing at most \(W- w\) that can be taken from the \(n-1\) original items plus \(w_j - w\) of item \(j\). There is some fraction of item \(j\) left after taking \(w\) of it.&lt;/p&gt;
&lt;h4 id=&#34;showing-the-greedy-property&#34;&gt;Showing the Greedy Property&lt;/h4&gt;
&lt;p&gt;It is established that both problems have optimal substructure. However, only the fractional knapsack problem has the greedy property. Examine the following code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;fractional_knapsack&lt;/span&gt;(v, w, W):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(v)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    load &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; load &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; W &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; n:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; w[i] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; W &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; load:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            load &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; w[i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            i &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            load &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; (W &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; load) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; v[i] &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; w[i]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If we are able to sort each item by its value-to-weight ratio, then the greedy choice is to take as much as possible from the most valuable item first, the second most valuable item next, and so on. This considers \(n\) items in the worst case, and the items need to be sorted by value-to-weight ratio. The algorithm is \(O(n \log n)\).&lt;/p&gt;
&lt;p&gt;This does not work for the \(0-1\) knapsack problem. Consider the problem visualized below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-24_17-05-36_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Greedy solution to the (0-1) knapsack problem (Cormen et al. 2022).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Greedy solution to the (0-1) knapsack problem (Cormen et al. 2022).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In this problem, we have a knapsack whose total capacity is \(W = 50\). A table of the weights, values, and value-to-weight ratios is given below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;\(i\)&lt;/th&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;\(v_i\)&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;120&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;\(w_i\)&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;\(v_i/w_i\)&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The fractional algorithm would selection the first item since it has the greatest value-to-weight ratio. The \(0-1\) knapsack problem, however, would select the second and third items to maximize the value of the items in the knapsack.&lt;/p&gt;
&lt;h2 id=&#34;huffman-codes&#34;&gt;Huffman Codes&lt;/h2&gt;
&lt;p&gt;Huffman coding is a lossless data compression algorithm that assigns variable-length codes to input characters, with lengths based on the frequencies of occurrence for those characters. Originally developed by David A. Huffman in 1952 during his Ph.D. at MIT, he published the algorithm under the title &amp;ldquo;&lt;a href=&#34;https://ieeexplore.ieee.org/document/4051119&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;A Method for the Construction of Minimum-Redundancy Codes&lt;/a&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Huffman coding involves:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Building a Huffman tree from the input characters and their frequencies.&lt;/li&gt;
&lt;li&gt;Traversing the tree to assign codes to each character.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Before getting into the specifics of the algorithm, let&amp;rsquo;s look at an example. Suppose we have a document consisting of 6 unique characters, each represented by a byte (8 bits). We could represent these 6 characters using 3 bits, since that is the minimum number of bits needed to represent 6 unique values. This is known as a &lt;strong&gt;fixed-length code&lt;/strong&gt;. If we instead assigned a &lt;strong&gt;variable-length code&lt;/strong&gt; to each character based on its frequency of occurrence, we would further reduce the footprint of the file size.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;A&lt;/th&gt;
&lt;th&gt;B&lt;/th&gt;
&lt;th&gt;C&lt;/th&gt;
&lt;th&gt;D&lt;/th&gt;
&lt;th&gt;E&lt;/th&gt;
&lt;th&gt;F&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Frequency (in thousands)&lt;/td&gt;
&lt;td&gt;45&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fixed-length code&lt;/td&gt;
&lt;td&gt;000&lt;/td&gt;
&lt;td&gt;001&lt;/td&gt;
&lt;td&gt;010&lt;/td&gt;
&lt;td&gt;011&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;101&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Variable-length code&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;101&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;111&lt;/td&gt;
&lt;td&gt;1101&lt;/td&gt;
&lt;td&gt;1100&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Does this seem like the best coding?&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Why isn&amp;rsquo;t a code of 1 used?&lt;/li&gt;
&lt;li&gt;What about 2 bit encoding?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The truth is that it depends on the document being compressed. The encoding is optimal consider the overall but length of the encoded file. Based on the above &lt;strong&gt;fixed-length code&lt;/strong&gt;, the file size is 300,000 bits. The &lt;strong&gt;variable-length code&lt;/strong&gt; reduces the file size to 224,000 bits.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How many bits are needed to encode \(n \geq 2\) characters?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\[
\lceil \lg n \rceil
\]&lt;/p&gt;
&lt;h3 id=&#34;prefix-free-codes&#34;&gt;Prefix-free Codes&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;prefix-free code&lt;/strong&gt; is a code in which no codeword is also a prefix of another codeword. This property simplifies decoding since the code can be read from left to right without ambiguity. The codeword &amp;ldquo;beef&amp;rdquo; has the encoding \(101110111011100 = 101 \cdot 1101 \cdot 1101 \cdot 1100\), where \(\cdot\) denotes concatenation.&lt;/p&gt;
&lt;p&gt;This definition may start to clarify why a code of 1 is not used. If a code of 1 were used, then the code would be a prefix of all other codes. This would make decoding ambiguous.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How are the codes decoded?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One solution to this is to keep a table of all codewords and their corresponding characters. A more compact solution is to use a binary tree. Starting with the first bit in the encoded message, traverse the tree until a leaf node is reached. The character at the leaf node is the decoded character. The tree is known as a &lt;strong&gt;Huffman tree&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;full binary tree&lt;/strong&gt;, where each nonleaf node has two subnodes, is optimal for decoding. If the tree has this property then an optimal prefix-free code has \(|C|\) leaves and exactly \(|C| - 1\) internal nodes. The tree for the variable-length code is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-26_09-07-14_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Huffman tree for the variable-length code (Cormen et al. 2022).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Huffman tree for the variable-length code (Cormen et al. 2022).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Given a character \(c\) with frequency \(c.freq\), let \(d_T( c)\) denote the depth of character \(c\) in tree \(T\). The cost of the code in bits is given by&lt;/p&gt;
&lt;p&gt;\[
B(T) = \sum_{c \in C} c.freq \cdot d_T( c).
\]&lt;/p&gt;
&lt;p&gt;The depth of the character \(d_T( c)\) is used since it also denotes the length of the codeword.&lt;/p&gt;
&lt;h3 id=&#34;constructing-a-huffman-code&#34;&gt;Constructing a Huffman Code&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Node&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, freq, char&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;freq &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; freq
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;char &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; char
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;huffman&lt;/span&gt;(C):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(C)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; build_min_heap(C)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        z &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Node(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; extract_min(Q)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; extract_min(Q)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;freq &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;freq &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;freq
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        insert(Q, z)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; extract_min(Q)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The function above builds a Huffman tree from a set of characters \(C\). At each iteration, the two nodes with the smallest frequencies are extracted from the queue \(Q\) and are used to create a new node \(z\). This node represents the sum of the frequencies of the two nodes. The node is then inserted back into the queue so that it can be used in future iterations. The result is a Huffman tree.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-01_20-48-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Huffman tree for the data in the table above (Cormen et al. 2022).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Huffman tree for the data in the table above (Cormen et al. 2022).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;analysis&#34;&gt;Analysis&lt;/h4&gt;
&lt;p&gt;If the priority queue is implemented as a binary min-heap, the call to &lt;code&gt;build_min_heap&lt;/code&gt; initializes the priority queue in \(O(n)\) time. The &lt;code&gt;for&lt;/code&gt; loop runs \(n-1\) times, calling &lt;code&gt;extract_min&lt;/code&gt; twice and &lt;code&gt;insert&lt;/code&gt; once. Each call to &lt;code&gt;extract_min&lt;/code&gt; takes \(O(\lg n)\) time yielding a total of \(O(n \lg n)\) time.&lt;/p&gt;
&lt;h3 id=&#34;correctness-of-huffman-codes&#34;&gt;Correctness of Huffman Codes&lt;/h3&gt;
&lt;p&gt;The correctness of an algorithm means that it produces the expected output based on the input and its properties. To show that the Huffman algorithm is correct, we can show that it exhibits the greedy choice and optimal substructure properties.&lt;/p&gt;
&lt;h4 id=&#34;lemma-15-dot-2-optimal-prefix-free-codes-have-the-greedy-choice-property&#34;&gt;Lemma 15.2: Optimal prefix-free codes have the greedy-choice property&lt;/h4&gt;
&lt;p&gt;For alphabet \(C\), let \(x\) and \(y\) be the two characters with the lowest frequencies. Then there exists an optimal prefix-free code for \(C\) where the codewords for \(x\) and \(y\) have the same length and differ only in the last bit.&lt;/p&gt;
&lt;p&gt;This establishes the greedy choice property because the algorithm selects the two characters with the lowest frequencies at each step.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the given optimal tree \(T\), leaves \(a\) and \(b\) are two siblings with maximum depth. It&amp;rsquo;s also given that \(x\) and \(y\) are the two characters with the lowest frequencies, but they appear in arbitrary positions.&lt;/p&gt;
&lt;p&gt;Assume that \(x \neq b\). Swapping \(a\) and \(x\) produces tree \(T&amp;rsquo;\) does not increase the cost. Swapping \(b\) and \(y\) produces tree \(T&amp;rsquo;&amp;rsquo;\) that also does not increase the cost. &lt;strong&gt;This is the key argument: if swapping the lowest frequency characters with the deepest characters does not increase the cost, then the greedy choice is optimal.&lt;/strong&gt;&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-02_10-40-38_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Creating (T&amp;#39;) and (T&amp;#39;&amp;#39;) from (T) (Cormen et al. 2022).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Creating (T&amp;rsquo;) and (T&amp;rsquo;&amp;rsquo;) from (T) (Cormen et al. 2022).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Next, we need to show that exchanging \(a\) and \(x\) does not increase the cost. The cost of the tree is given by&lt;/p&gt;
&lt;p&gt;\[
B(T) = \sum_{c \in C} c.freq \cdot d_T( c).
\]&lt;/p&gt;
&lt;p&gt;\begin{align*}
B(T) - B(T&amp;rsquo;) &amp;amp;= \sum_{c \in C} c.freq \cdot d_{T}( c) - \sum_{c \in C} c.freq \cdot d_{T&amp;rsquo;}( c) \\
&amp;amp;= x.freq \cdot d_T(x) + a.freq \cdot d_T(a) - x.freq \cdot d_{T&amp;rsquo;}(x) - a.freq \cdot d_{T&amp;rsquo;}(a) \\
&amp;amp;= x.freq \cdot d_T(x) + a.freq \cdot d_T(a) - x.freq \cdot d_{T}(a) - a.freq \cdot d_{T}(x) \\
&amp;amp;= (x.freq - a.freq)(d_T(x) - d_T(a)) \\
&amp;amp;\geq 0.
\end{align*}&lt;/p&gt;
&lt;p&gt;The last line is true because \(x.freq \leq a.freq\) and \(d_T(x) \geq d_T(a)\). A similar argument can be made for \(T&amp;rsquo;&amp;rsquo;\). \(B(T&amp;rsquo;&amp;rsquo;) \leq B(T&amp;rsquo;)\) since exchanging \(y\) and \(b\) does not increase the cost. This means that \(B(T&amp;rsquo;&amp;rsquo;) \leq B(T&amp;rsquo;) \leq B(T)\). \(T\) is an optimal tree, so \(B(T) \leq B(T&amp;rsquo;&amp;rsquo;) \implies B(T) = B(T&amp;rsquo;&amp;rsquo;) \implies T\) is optimal where \(x\) and \(y\) are siblings of maximum depth.&lt;/p&gt;
&lt;h4 id=&#34;lemma-15-dot-3-optimal-substructure-property&#34;&gt;Lemma 15.3: Optimal-substructure property&lt;/h4&gt;
&lt;p&gt;Let \(x\) and \(y\) be two characters with minimum frequency in alphabet \(C\) and let \(C&amp;rsquo; = (C - \{x, y\}) \cup z\) for a new character \(z\) with \(z.freq = x.freq + y.freq\). Additionally, let \(T&amp;rsquo;\) be a tree representing an optimal prefix-free code for \(C&amp;rsquo;\), and \(T\) be \(T&amp;rsquo;\) with the leaf for \(z\) replaced by an internal node with children \(x\) and \(y\). Then \(T\) represents an optimal prefix-free code for \(C\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Simplified:&lt;/strong&gt; If the algorithm computed the optimal solution to the simplified problem, where \(z\) replaced \(x\) and \(y\), it can extend this to an optimal solution to the original problem by putting \(x\) and \(y\) back.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first part of the proof establishes the costs of the relevant trees.&lt;/p&gt;
&lt;p&gt;\(c \in C - \{x, y\} \implies d_T( c) = d_{T&amp;rsquo;}( c) \implies c.freq \cdot d_T( c) = c.freq \cdot d_{T&amp;rsquo;}( c)\)&lt;/p&gt;
&lt;p&gt;The depth of \(x\) and \(y\) are equal to the depth of \(z\) in \(T&amp;rsquo;\) + 1:&lt;/p&gt;
&lt;p&gt;\begin{align*}
d_T(x) = d_T(y) = d_{T&amp;rsquo;}(z) + 1 &amp;amp;\implies x.freq \cdot d_T(x) + y.freq \cdot d_T(y)\\
&amp;amp;= (x.freq + y.freq)(d_{T&amp;rsquo;}(z) + 1)\\
&amp;amp;= z.freq \cdot d_{T&amp;rsquo;}(z) + (x.freq + y.freq).
\end{align*}&lt;/p&gt;
&lt;p&gt;This means that \(B(T) = B(T&amp;rsquo;) + x.freq + y.freq\), which is equivalent to \(B(T&amp;rsquo;) = B(T) - x.freq - y.freq\).&lt;/p&gt;
&lt;p&gt;The second part of this proof supposes that \(T\) is not an optimal prefix code for \(C\) and ends in a contradiction, thus proving the original lemma. If \(T\) is not optimal for \(C\), then \(B(T&amp;rsquo;&amp;rsquo;) &amp;lt; B(T)\) for some optimal tree \(T&amp;rsquo;&amp;rsquo;\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Here, \(T&amp;rsquo;&amp;rsquo;\) is introduced as the supposed optimal tree for \(C\) if it turns out that \(T\) is not.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If \(T&amp;rsquo;&amp;rsquo;\) is optimal, then lemma 15.2 (greedy property) from above implies that it has \(x\) and \(y\) as siblings. After all, \(\{x, y\} \in C\) and \(T&amp;rsquo;&amp;rsquo;\) is an optimal tree for \(C\). Create a tree \(T&amp;rsquo;&amp;rsquo;&amp;rsquo;\) by replacing the parent of \(x\) and \(y\) with a leaf (implying we remove \(x\) and \(y\)) \(z\) with \(z.freq = x.freq + y.freq\). Then,&lt;/p&gt;
&lt;p&gt;\begin{align*}
B(T&amp;rsquo;&amp;rsquo;&amp;rsquo;) &amp;amp;= B(T&amp;rsquo;&amp;rsquo;) - x.freq - y.freq\\
&amp;amp;&amp;lt; B(T) - x.freq - y.freq\\
&amp;amp;= B(T&amp;rsquo;).
\end{align*}&lt;/p&gt;
&lt;p&gt;\(B(T&amp;rsquo;&amp;rsquo;&amp;rsquo;) &amp;lt; B(T&amp;rsquo;)\) is a contradiction because it was previously established that \(T&amp;rsquo;\) is an optimal tree for \(C&amp;rsquo;\). Therefore a suboptimal \(T\) is impossible if \(T&amp;rsquo;\) is optimal.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hash Tables</title>
      <link>https://ajdillhoff.github.io/notes/hash_tables/</link>
      <pubDate>Thu, 14 Mar 2024 15:16:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/hash_tables/</guid>
      <description>&lt;p&gt;See &lt;a href=&#34;https://ajdillhoff.github.io/teaching/dasc5300/lectures/hash_maps.pdf&#34;&gt;these slides&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic Programming</title>
      <link>https://ajdillhoff.github.io/notes/dynamic_programming/</link>
      <pubDate>Thu, 14 Mar 2024 10:40:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/dynamic_programming/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#rod-cutting&#34;&gt;Rod Cutting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix-chain-multiplication&#34;&gt;Matrix-chain Multiplication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#applying-dynamic-programming&#34;&gt;Applying Dynamic Programming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#longest-common-subsequence&#34;&gt;Longest Common Subsequence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exercises&#34;&gt;Exercises&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Dynamic programming is a technique for solving problems by breaking them down into simpler subproblems, very much like divide and conquer algorithms. One primary difference is that the subproblems are designed in such a way that they do not need to be recomputed.&lt;/p&gt;
&lt;p&gt;Many common problems have efficience dynamic programming solutions, and we will investigate several of them in these notes. In general, a dynamic programming solution can be applied if the problem has the following features.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Optimal substructure:&lt;/strong&gt; An optimal solution can be constructed by optimal solutions to the subproblems.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overlapping subproblems:&lt;/strong&gt; The problem can be broken down into subproblems which can be reused.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, the Fibonacci sequence has &lt;strong&gt;optimal substructure&lt;/strong&gt; because the value of the sequence at any index is the sum of the values at the two previous indices. It also has &lt;strong&gt;overlapping subproblems&lt;/strong&gt; because the value of the sequence at any index is used in the calculation of the values at the two subsequent indices. A recursive solution to the Fibonacci sequence will have exponential time complexity, but a dynamic programming solution will have linear time complexity.&lt;/p&gt;
&lt;p&gt;The two main approaches to dynamic programming are top-down (memoization) and bottom-up (tabulation). &lt;strong&gt;Memoization&lt;/strong&gt; involves writing a recursive solution that stores each sub=solution in a table so that it can be reused. &lt;strong&gt;Tabulation&lt;/strong&gt; involves solving the problem by filling in a table of subproblems from the bottom up. In either case, a dynamic programming solution can be formulated with the following steps.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Identify subproblems&lt;/strong&gt; so that the problem can be broken down.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Solve the subproblems&lt;/strong&gt; following an optimal solution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Store the solutions&lt;/strong&gt; to avoid redundant computation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Combine solutions&lt;/strong&gt; from the subproblems to solve the original problem.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;rod-cutting&#34;&gt;Rod Cutting&lt;/h2&gt;
&lt;p&gt;Given a rod of length \(n\) and table of prices \(p_i\) for \(i = 1, 2, \ldots, n\), determine the maximum revenue \(r_n\) that can be obtained by cutting up the rod and selling the pieces.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Length&lt;/th&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;th&gt;10&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Price&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;24&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The table of prices is shown above. For a rod of length 4, there are 8 (\(2^{n-1}\), where \(n=4\)) different ways to cut the rod.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-14_11-47-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;8 different ways to cut a rod of length 4 (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;8 different ways to cut a rod of length 4 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The maximum revenue for a rod of length \(n\) can be determined by the following optimization problem:&lt;/p&gt;
&lt;p&gt;\[
r_n = \max(p_n, r_1 + r_{n-1}, r_2 + r_{n-2}, \ldots, r_{n-1} + r_1),
\]&lt;/p&gt;
&lt;p&gt;where \(r_i\) is the maximum revenue for a rod of length \(i\). The maximum revenue for a rod of length \(n\) can be determined by solving the subproblems for rods of length \(i\) for \(i = 1, 2, \ldots, n-1\). Each of the terms \(r_i\) in the equation above implies a recursive solution to the problem. You should be able to see that solving this recursively would lead to many redundant computations. For example, \(r_1\) is computed at least twice in the equation above.&lt;/p&gt;
&lt;p&gt;This recursion is more compactly written as&lt;/p&gt;
&lt;p&gt;\[
r_n = \max_{1 \leq i \leq n}(p_i + r_{n-i}).
\]&lt;/p&gt;
&lt;p&gt;This problem has &lt;strong&gt;optimal substructure&lt;/strong&gt;. If we cut the rod into smaller subsections, we can recursively solve the subproblems and combine them. The recursive algorithm is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cut_rod&lt;/span&gt;(p, n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;inf&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(q, p[i] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; cut_rod(p, n&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;i))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; q
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If \(T(n)\) is a recurrence that represents the number of times &lt;code&gt;cur_rod&lt;/code&gt; is called recursively, then we can write the following recurrence relation:&lt;/p&gt;
&lt;p&gt;\[
T(n) = 1 + \sum_{j=0}^{n-1}T(j).
\]&lt;/p&gt;
&lt;p&gt;Using the substitution method, we can show that \(T(n) = 2^n\).&lt;/p&gt;
&lt;p&gt;\begin{align*}
T(n) &amp;amp;= 1 + \sum_{j=0}^{n-1}2^j \\
&amp;amp;= 1 + (2^n - 1) \\
&amp;amp;= 2^n.
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;memoization-solution&#34;&gt;Memoization Solution&lt;/h3&gt;
&lt;p&gt;To solve this with dynamic programming, the goal is to make sure that each subproblem is computed &lt;em&gt;only once&lt;/em&gt;. This is accomplished by saving the result of each subproblem in a table so that it can be reused. This does incur a space complexity of \(O(n)\), but it reduces the time complexity to \(O(n^2)\).&lt;/p&gt;
&lt;p&gt;The solution requires a small modification to the recursive algorithm. When the solution to a subproblem is required, the table is first checked for a stored solution. If the solution is not found, the subproblem is solved recursively and the solution is stored in the table. The code is given below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;memoized_cut_rod&lt;/span&gt;(p, n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    r &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;inf&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; memoized_cut_rod_aux(p, n, r)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;memoized_cut_rod_aux&lt;/span&gt;(p, n, r):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; r[n] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; r[n]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;inf&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(q, p[i] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; memoized_cut_rod_aux(p, n&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;i, r))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    r[n] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; q
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; q
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The algorithm starts off with a call to &lt;code&gt;memoized_cut_rod&lt;/code&gt; which initializes the table &lt;code&gt;r&lt;/code&gt; and then calls &lt;code&gt;memoized_cut_rod_aux&lt;/code&gt;. The table &lt;code&gt;r&lt;/code&gt; is initialized with \(-\infty\) so that we can check if a solution has been computed for a subproblem. Each subproblem is solved only once, leading to \(O(1)\) lookups after that. The time complexity of this solution is \(O(n^2)\).&lt;/p&gt;
&lt;h3 id=&#34;bottom-up-solution&#34;&gt;Bottom-Up Solution&lt;/h3&gt;
&lt;p&gt;The other dynamic programming solution is to first sort the subproblems by their size, solve the smaller ones first, and build up to the larger ones. This is called &lt;strong&gt;tabulation&lt;/strong&gt;. The time complexity of this solution is also \(O(n^2)\).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bottom_up_cut_rod&lt;/span&gt;(p, n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    r &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;inf&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, j&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(q, p[i] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; r[j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;i])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        r[j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; q
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; r[n]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first &lt;code&gt;for&lt;/code&gt; loop effectively sorts the problem by size. It starts with a cut of size 1 and builds up to a cut of size \(n\).&lt;/p&gt;
&lt;h3 id=&#34;subproblem-graphs&#34;&gt;Subproblem Graphs&lt;/h3&gt;
&lt;p&gt;Subproblem graphs offer a concise way to visualize the subproblems and their dependencies. The subproblem graph for the rod cutting problem with \(n=4\) is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-14_14-54-03_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Subproblem graph for the rod cutting problem with (n=4) (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Subproblem graph for the rod cutting problem with (n=4) (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Subproblem \(n=4\) is dependent on subproblems \(n=3\), \(n=2\), and \(n=1\). The bottom-up approach follows this dependency by ensuring that the subproblems are solved in the correct order.&lt;/p&gt;
&lt;p&gt;Besides serving as a helpful visualization, depicting the problem using a DAG can also help to identify the time complexity of the problem. This is the sum of the time needed to solve each subproblem. Each problem of size \(n\) requires \(n-1\) subproblems to be solved, and each subproblem of size \(n-1\) requires \(n-2\) subproblems to be solved. This leads to a time complexity of \(O(n^2)\).&lt;/p&gt;
&lt;h3 id=&#34;reconstructing-a-solution&#34;&gt;Reconstructing a Solution&lt;/h3&gt;
&lt;p&gt;The two dynamic programming solutions above return the maximum revenue that can be obtained by cutting up the rod. However, they do not return the actual cuts that should be made. This can be done by modifying the algorithms to store the cuts that are made. The code is given below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;extended_bottom_up_cut_rod&lt;/span&gt;(p, n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    r &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;inf&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, j&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; q &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; p[i] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; r[j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;i]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; p[i] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; r[j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                s[j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        r[j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; q
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; r, s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;print_cut_rod_solution&lt;/span&gt;(p, n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    r, s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; extended_bottom_up_cut_rod(p, n)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(s[n])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        n &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; s[n]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the bottom-up approach, the table &lt;code&gt;s&lt;/code&gt; is used to store the size of the first piece to cut off. The function &lt;code&gt;print_cut_rod_solution&lt;/code&gt; uses this table to print the cuts that should be made.&lt;/p&gt;
&lt;h2 id=&#34;matrix-chain-multiplication&#34;&gt;Matrix-chain Multiplication&lt;/h2&gt;
&lt;p&gt;The next problem covered by Cormen et al. is &lt;strong&gt;matrix-chain multiplication&lt;/strong&gt; (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;). Given a sequence of matrices \(A_1, A_2, \ldots, A_n\), where the dimensions of matrix \(A_i\) are \(p_{i-1} \times p_i\), determine the most efficient way to multiply the matrices. The problem is to determine the order in which the matrices should be multiplied so that the number of scalar multiplications is minimized.&lt;/p&gt;
&lt;p&gt;Understanding the solution to this problem requires understanding the problem itself. Depending on the order in which matrices are multiplied in a chain, the number of scalar multiplications can vary. Consider three matrices \(A \in \mathbb{R}^{10 \times 100}\), \(B \in \mathbb{R}^{100 \times 5}\), and \(C \in \mathbb{R}^{5 \times 50}\). The number of scalar multiplications required to compute \(A(BC)\) is \(10 \times 100 \times 5 + 10 \times 5 \times 50 = 7500\), while the number of scalar multiplications required to compute \((AB)C\) is \(10 \times 100 \times 50 + 100 \times 5 \times 50 = 75000\). The order in which the matrices are multiplied can have a significant impact on the number of scalar multiplications required.&lt;/p&gt;
&lt;p&gt;Matrix multiplication is associative, so the order in which the matrices are grouped does not matter. The key to solving this problem is to find the most efficient way to group the matrices. The first part of the solution is to determine the number of possible groupings, or parenthesizations, we can make.&lt;/p&gt;
&lt;h3 id=&#34;determining-parenthesizations&#34;&gt;Determining Parenthesizations&lt;/h3&gt;
&lt;p&gt;The number of possible parenthesizations of a chain of \(n\) matrices is given by \(P(n)\). When \(n \geq 2\), the number of possible parenthesizations is given by&lt;/p&gt;
&lt;p&gt;\[
P(n) = \sum_{k=1}^{n-1}P(k)P(n-k).
\]&lt;/p&gt;
&lt;p&gt;A brute force solution to this problem would require \(O(2^n)\) time (see Exercise 14.2-3 in (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;)).&lt;/p&gt;
&lt;h3 id=&#34;dynamic-programming-solution&#34;&gt;Dynamic Programming Solution&lt;/h3&gt;
&lt;p&gt;We now review the four step process to formulating a dynamic programming solution as put forth by Cormen et al. (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/p&gt;
&lt;h4 id=&#34;optimal-substructure&#34;&gt;Optimal Substructure&lt;/h4&gt;
&lt;p&gt;What is the optimal substructure of this problem? Consider matrix-chain sequence \(A_{i:j} = A_i A_{i+1} \cdots A_j\). If we split the sequence at \(k\), then the optimal solution to the problem is the optimal solution to the subproblems \(A_{i:k}\) and \(A_{k+1:j}\). This is because the number of scalar multiplications required to compute \(A_{i:j}\) is the sum of the number of scalar multiplications required to compute \(A_{i:k}\) and \(A_{k+1:j}\) plus the number of scalar multiplications required to compute the product of the two subproblems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How can we ensure that there is not a more optimal grouping of \(A_{h:l}\), where \(i \leq h &amp;lt; k\) and \(k &amp;lt; l \leq j\)?&lt;/strong&gt; The answer lies in evaluating &lt;strong&gt;all&lt;/strong&gt; possible splits.&lt;/p&gt;
&lt;h4 id=&#34;recursive-solution&#34;&gt;Recursive Solution&lt;/h4&gt;
&lt;p&gt;What is the cost of an optimal solution to the problem? We must first compute the minimum cost of parenthesizing \(A_{i:j}\) for \(1 \leq i \leq j \leq n\). Let \(m[i,j]\) be the minimum number of scalar multiplications needed to compute \(A_{i:j}\). Starting with the base case, \(m[i,i]\) is the cost to compute the multiplication of a single matrix, which is 0. Assuming optimal subproblems are chosen, \(m[i,j] = m[i,k] + m[k+1,j] + p_{i-1}p_kp_j\), where the last term is the cost of multiplying \(A_{i:k}A_{k+1:j}\).&lt;/p&gt;
&lt;p&gt;All possible splits must be evaluated. So, how many are there? Omitting the first and last matrices, there are \(j - i\) possible splits. We can now define the optimal solution in terms of the following recursion:&lt;/p&gt;
&lt;p&gt;\[
m[i,j] = \min \{m[i,k] + m[k+1,j] + p_{i-1}p_kp_j : i \leq k &amp;lt; j\}.
\]&lt;/p&gt;
&lt;h4 id=&#34;storing-the-solutions&#34;&gt;Storing the Solutions&lt;/h4&gt;
&lt;p&gt;The problem is taking shape and we have a recursive formula. However, this is no better than the brute-force method until we figure out how to select the optimal subproblems and store their solutions. That is, we need to optimally select \(k\). A bottom-up approach involves computing the cost of all possible combinations of the \(n\) matrices and building up from there. This requires \(O(n^2)\) memory to store both the costs \(m[i, j]\) as well as the value of \(k\) that splits them \(s[i, j]\).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;matrix_chain_order&lt;/span&gt;(p):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(p) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; l &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, n&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;l&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; l &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            m[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;inf&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(i, j):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; m[i][k] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; m[k&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][j] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; p[i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;p[k]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;p[j]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; q &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; m[i][j]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    m[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; q
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    s[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; k
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; m, s
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The function &lt;code&gt;matrix_chain_order&lt;/code&gt; computes the cost of all possible combinations of the \(n\) matrices and stores the value of \(k\) that splits them. The outer-most &lt;code&gt;for&lt;/code&gt; loop controls the length of the chain being evaluated. We start at 2 since the cost of a length 1 chain is 0. Intuition tells us that the triply-nested &lt;code&gt;for&lt;/code&gt; loop has a time complexity of \(O(n^3)\).&lt;/p&gt;
&lt;p&gt;This algorithm computes the cost in ascending order of chain length. When \(l=2\), the cost of all chains of length 2 is computed. When \(l=3\), the cost of all chains of length 3 is computed, and so on. The recursion in the inner-most nested loop will only ever access the entries in &lt;code&gt;m&lt;/code&gt; which have been previously computed.&lt;/p&gt;
&lt;h4 id=&#34;reconstructing-a-solution&#34;&gt;Reconstructing a Solution&lt;/h4&gt;
&lt;p&gt;We now have a solution which generates the optimal number of scalar multiplications needed for all possible combinations of the \(n\) matrices. However, we do not yet have a solution which tells us the order in which the matrices should be multiplied. This information is held in &lt;code&gt;s&lt;/code&gt;, which records the value of \(k\) that splits the chain. The function &lt;code&gt;print_optimal_parens&lt;/code&gt; is given below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;print_optimal_parens&lt;/span&gt;(s, i, j):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; j:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;A_&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;i&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;, end&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;(&amp;#34;&lt;/span&gt;, end&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print_optimal_parens(s, i, s[i][j])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print_optimal_parens(s, s[i][j]&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, j)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;)&amp;#34;&lt;/span&gt;, end&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Using figure 14.5 from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;) as a reference, we can test the function &lt;code&gt;print_optimal_parens&lt;/code&gt;. It is first called as &lt;code&gt;print_optimal_parens(s, 1, 6)&lt;/code&gt;. This recursively calls &lt;code&gt;print_optimal_parens(s, 1, 3)&lt;/code&gt; and &lt;code&gt;print_optimal_parens(s, 4, 6)&lt;/code&gt;. We will work from left to right, top to bottom and fill out the values.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Second call: &lt;code&gt;print_optimal_parens(s, 1, 3)&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This recursively calls &lt;code&gt;print_optimal_parens(s, 1, 1)&lt;/code&gt; and &lt;code&gt;print_optimal_parens(s, 2, 3)&lt;/code&gt;. We can see that this first call has \(i==j\), so it prints \(A_1\). The second call prints \((A_2A_3)\). This initial call already set up the first set of parenthesis, so the intermediate result is \(((A_1(A_2A_3))\cdots)\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Third call: &lt;code&gt;print_optimal_parens(s, 4, 6)&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This recursively calls &lt;code&gt;print_optimal_parens(s, 4, 5)&lt;/code&gt; and &lt;code&gt;print_optimal_parens(s, 6, 6)&lt;/code&gt;. This first call will recursively call &lt;code&gt;print_optimal_parens(s, 4, 4)&lt;/code&gt; and &lt;code&gt;print_optimal_parens(s, 5, 5)&lt;/code&gt;. This produces \((A_4A_5)\) from the first subcall and \(A_6\) from the second subcall. The intermediate result is now \((\cdots((A_4A_5)A_6))\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Putting it all together&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Combining these results yields \(((A_1(A_2A_3))((A_4A_5)A_6))\). This is the optimal parenthesization of the matrix chain \(A_1A_2A_3A_4A_5A_6\).&lt;/p&gt;
&lt;h2 id=&#34;applying-dynamic-programming&#34;&gt;Applying Dynamic Programming&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Section 14.3 of (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;) focuses on the two core components of a dynamic programming solution: &lt;strong&gt;optimal substructure&lt;/strong&gt; and &lt;strong&gt;overlapping subproblems&lt;/strong&gt;. By taking a closer look at how these two components were used in various dynamic programming solutions, you should have a greater understanding of how to apply dynamic programming to new problems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As shown in previous examples, determining the &lt;strong&gt;optimal substructure&lt;/strong&gt; is the first step in formulating a dynamic programming solution. In most cases, this comes from understanding the problem itself well. It is the result of a natural way of analysis and decomposition of the problem. When learning a new concerto, a musician must have a strong command of technique and fundamental musical concepts. Similarly, a strong understanding of the problem is required to determine the optimal substructure.&lt;/p&gt;
&lt;h3 id=&#34;determining-optimal-substructure&#34;&gt;Determining Optimal Substructure&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Show that a solution to a problem requires making a choice, like where to cut in the rod cutting problem.&lt;/li&gt;
&lt;li&gt;Assume that you are given an optimal choice.&lt;/li&gt;
&lt;li&gt;Identify the subproblems that result from this choice.&lt;/li&gt;
&lt;li&gt;Show that solutions to these subproblems are optimal.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the last step, we are typically looking for a contradiction. The assumption of step 2 means that if we end up finding a more optimal solution to a subproblem, then the original choice was not optimal. The result is that we have a better overall solution.&lt;/p&gt;
&lt;p&gt;The efficiency of a dynamic programming solution depends on the number of subproblems times the number of choices we have for each subproblem. When investigating solutions to a new problem, it is better to start with a simple case and expand outward as necessary. Using a subproblem graph is a great way to visualize the subproblems and their dependencies.&lt;/p&gt;
&lt;h3 id=&#34;counter-example-the-longest-simple-path&#34;&gt;Counter-Example: The Longest Simple Path&lt;/h3&gt;
&lt;p&gt;Consider the following problems which first appear to have optimal substructure.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Shortest path&lt;/strong&gt;: find a path \(u \leadsto v\) with the fewest edges without cycles.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Longest simple path&lt;/strong&gt;: find a path \(u \leadsto v\) with the most edges without cycles.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first problem has optimal substructure. Suppose that the shortest path \(u \leadsto v\) is given by \(p\). Given some intermediate vertex \(w\), the optimal path from \(u \leadsto w\) is given by \(p_1\) and the optimal path from \(w \leadsto v\) is given by \(p_2\). If there were a shorter path \(p&amp;rsquo;_1\) from \(u \leadsto w\) then we could replace \(p_1\) with it and get a total path with fewer edges.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Check your understanding&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Why does that argument reinforce the idea of optimal substructure? By showing that the optimal solution to a subproblem is the optimal solution to the original problem. This argument becomes clearer as we consider the longest simple path problem.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-15_13-43-21_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Subproblem graph for the longest simple path problem (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Subproblem graph for the longest simple path problem (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Consider the directed graph above. The path \(q \rightarrow r \rightarrow t\) is the longest simple path from \(q\) to \(t\). &lt;strong&gt;Keep in mind that the problem is to find a simple path with the most edges. If the substructure is optimal, then the subpaths must also exhibit maximal edges.&lt;/strong&gt; The subpath \(q \leadsto r\) in this case is simply \(q \rightarrow r\), but the longest simple path from \(q\) to \(r\) is \(q \rightarrow s \rightarrow t \rightarrow r\). Therefore, the subpath \(q \leadsto r\) is not optimal. This is a counter-example to the idea that the longest simple path problem has optimal substructure.&lt;/p&gt;
&lt;p&gt;The longest simple path problem does not have &lt;strong&gt;independent&lt;/strong&gt; subproblems. Consider a path from \(q\) to \(t\). This could be broken down into subproblem \(q \leadsto r\) and \(r \leadsto t\). For \(q \leadsto r\), we have \(q \rightarrow s \rightarrow t \rightarrow r\). This subproblem is dependent on \(s\) and \(t\), so we cannot use them in the second subproblem \(r \leadsto t\) without forming a path that is not simple. Specifically, the first subproblem includes \(t\), so the second subproblem cannot include \(t\). &lt;strong&gt;However, the second subproblem MUST include \(t\).&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;questions&#34;&gt;Questions&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;What are the independent subproblems in the rod cutting and matrix-chain multiplication problems?&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;using-overlapping-subproblems&#34;&gt;Using Overlapping Subproblems&lt;/h3&gt;
&lt;p&gt;First, do not confuse the idea of &lt;strong&gt;overlapping subproblems&lt;/strong&gt; with the need for the subproblems to be &lt;strong&gt;independent&lt;/strong&gt;. Subproblems are independent if they do not share resources, which the longest simple path problem does not have. Overlapping subproblems means that a subproblem may require the result of another independent subproblem. This is the case in the rod cutting problem, where the value of a subproblem is used in the calculation of the value of the next subproblem.&lt;/p&gt;
&lt;p&gt;A desirable trait of any recursive problem is that it have a small number of unique subproblems. The running time of such a solution is dependent on the number of subproblems, so having more of them will naturally lead to a less efficient solution. Section 14.3 reviews the bottom-up solution to matrix-chain multiplication, specifically focusing on the number of times the solution of each subproblem is required. It is recommended to review this section for further understanding.&lt;/p&gt;
&lt;h4 id=&#34;questions&#34;&gt;Questions&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;How many subproblem solutions are reused in the rod cutting problem of \(n=4\)?&lt;/li&gt;
&lt;li&gt;How many subproblem solutions are reused when computing the Fibonacci sequence of \(n\)?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;longest-common-subsequence&#34;&gt;Longest Common Subsequence&lt;/h2&gt;
&lt;p&gt;A longest common subsequence (LCS) of two input sequences \(X = \langle x_1, x_2, \ldots, x_m \rangle\) and \(Y = \langle y_1, y_2, \ldots, y_n \rangle\) is a sequence \(Z = \langle z_1, z_2, \ldots, z_k \rangle\) such that \(Z\) is a subsequence of both \(X\) and \(Y\) and \(k\) is as large as possible. For example, given \(X = \langle A, B, C, B, D, A, B \rangle\) and \(Y = \langle B, D, C, A, B, A \rangle\), the LCS is \(\langle B, C, A, B \rangle\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The subsequence is not necessarily consecutive!&lt;/strong&gt; A subsequence \(Z\) is common to a sequence \(X\) if it corresponds to a strictly increasing sequence of indices such that \(x_{i_j} = z_j\).&lt;/p&gt;
&lt;h3 id=&#34;naive-solution&#34;&gt;Naive Solution&lt;/h3&gt;
&lt;p&gt;First, how would we solve this problem using a brute-force method? We could generate all possible subsequences of \(X\) and \(Y\) and then compare them. This would require \(O(n2^m)\) time.&lt;/p&gt;
&lt;h3 id=&#34;dynamic-programming-solution&#34;&gt;Dynamic Programming Solution&lt;/h3&gt;
&lt;p&gt;Following the four step process, we can formulate a dynamic programming solution to the LCS problem. Step 1 is to determine the optimal substructure of the problem.&lt;/p&gt;
&lt;h4 id=&#34;optimal-substructure&#34;&gt;Optimal Substructure&lt;/h4&gt;
&lt;p&gt;Let \(X = \langle x_1, x_2, \ldots, x_m \rangle\) and \(Y = \langle y_1, y_2, \ldots, y_n \rangle\). Let \(Z = \langle z_1, z_2, \ldots, z_k \rangle\) be an LCS of \(X\) and \(Y\).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If \(x_m = y_n\), then \(z_k = x_m = y_n\) and \(Z_{k-1}\) is an LCS of \(X_{m-1}\) and \(Y_{n-1}\).&lt;/li&gt;
&lt;li&gt;If \(x_m \neq y_n\) and \(z_k \neq x_m\), then \(Z\) is an LCS of \(X_{m-1}\) and \(Y\).&lt;/li&gt;
&lt;li&gt;If \(x_m \neq y_n\) and \(z_k \neq y_n\), then \(Z\) is an LCS of \(X\) and \(Y_{n-1}\).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The theorem above shows that the LCS problem has optimal substructure. Let&amp;rsquo;s break this down a bit. Consider two sequences (words): rocinante and canterbury. The longest common subsequence is &amp;ldquo;cante&amp;rdquo;. Since the last characters of the two original words do not match, we can remove the last character from either word and find the LCS of the two remaining words. This implies that we could have found the LCS of the two original words by finding the LCS of a smaller subproblem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if the two words had the same last character?&lt;/strong&gt; The LCS of the shorter strings is the same as the LCS of the original strings with the last character removed.&lt;/p&gt;
&lt;h4 id=&#34;recursive-solution&#34;&gt;Recursive Solution&lt;/h4&gt;
&lt;p&gt;The next step is to write a recursive solution to the problem. Given the substrucure just presented, a bottom-up approach seems intuitive. Starting with indices \(i=0\) and \(j=0\) which indicate the length of the current strings \(X_i\) and \(Y_j\), increase the length and compute the LCS as we go.&lt;/p&gt;
&lt;p&gt;Define \(c[i, j]\) as the LCS length of \(X_i\) and \(Y_j\). The goal is to compute \(c[m,n]\), where \(m\) and \(n\) are the lengths of \(X\) and \(Y\), respectively. The recursive formula is given by&lt;/p&gt;
&lt;p&gt;\[
c[i, j] = \begin{cases}
0 &amp;amp; \text{if } i = 0 \text{ or } j = 0, \\
c[i-1, j-1] + 1 &amp;amp; \text{if } i, j &amp;gt; 0 \text{ and } x_i = y_j, \\
\max(c[i-1, j], c[i, j-1]) &amp;amp; \text{if } i, j &amp;gt; 0 \text{ and } x_i \neq y_j.
\end{cases}
\]&lt;/p&gt;
&lt;!--list-separator--&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Example: &amp;ldquo;atom&amp;rdquo; and &amp;ldquo;ant&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The LCS of &amp;ldquo;atom&amp;rdquo; and &amp;ldquo;ant&amp;rdquo; is &amp;ldquo;at&amp;rdquo;. The tree below shows the recursive calls to each subproblem. A dashed line indicates that the subproblem has already been solved.&lt;/p&gt;

    
    
    
    
    
    &lt;figure&gt;
    
    &lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-16_10-33-21_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Recursion tree for the LCS problem (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;
    
    
    
    &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
      
      &lt;p&gt;
        &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Recursion tree for the LCS problem (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
        
        
        
      &lt;/p&gt; 
    &lt;/figcaption&gt;
    
    &lt;/figure&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;storing-the-solutions&#34;&gt;Storing the Solutions&lt;/h4&gt;
&lt;p&gt;The LCS problem has \(\Theta(mn)\) distinct subproblems, so storing the solutions to these subproblems will allow us to avoid redundant computation. A dynamic programming solution goes as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Store the lengths of the LCS of the prefixes of \(X\) and \(Y\) in a table \(c\).&lt;/li&gt;
&lt;li&gt;Additionally store the solution to the subproblems in a table \(b\) so that we can reconstruct the LCS.&lt;/li&gt;
&lt;li&gt;The entries are filled in a row-major order.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The code is given below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;lcs_length&lt;/span&gt;(X, Y):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(X)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(Y)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(m&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(m&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, m&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; X[i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; Y[j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                c[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; c[i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                b[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;↖&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; c[i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][j] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; c[i][j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                c[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; c[i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][j]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                b[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;↑&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                c[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; c[i][j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                b[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;←&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; c, b
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;print_lcs&lt;/span&gt;(b, X, i, j):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; b[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;↖&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print_lcs(b, X, i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(X[i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], end&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; b[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;↑&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print_lcs(b, X, i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, j)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print_lcs(b, X, i, j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;reconstructing-a-solution&#34;&gt;Reconstructing a Solution&lt;/h4&gt;
&lt;p&gt;Printing the solution starts with the last entry in the table \(b\). If the entry is &amp;ldquo;↖&amp;rdquo;, then the last characters of \(X\) and \(Y\) are the same and we print the character. If the entry is &amp;ldquo;↑&amp;rdquo;, then we move up in the table. If the entry is &amp;ldquo;←&amp;rdquo;, then we move left in the table.&lt;/p&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Exercise 14.1-5&lt;/strong&gt; from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exercise 14.2-1&lt;/strong&gt; from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Write a recursive function to compute the Fibonacci sequence. What is the time complexity of this function? What is the time complexity of the dynamic programming solution?&lt;/li&gt;
&lt;li&gt;Write a function that prints a table similar to Figure 14.8 from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;) for the LCS problem.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Medians and Order Statistics</title>
      <link>https://ajdillhoff.github.io/notes/medians_and_order_statistics/</link>
      <pubDate>Tue, 12 Mar 2024 13:17:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/medians_and_order_statistics/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#order-statistics&#34;&gt;Order Statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#minimum-and-maximum&#34;&gt;Minimum and Maximum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#selection-in-expected-linear-time&#34;&gt;Selection in expected linear time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#problems-and-exercises&#34;&gt;Problems and Exercises&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;We briefly touched on a median finding algorithm when discussing &lt;a href=&#34;https://ajdillhoff.github.io/notes/divide_and_conquer_algorithms/&#34;&gt;Divide and Conquer Algorithms&lt;/a&gt;. This section will be a bit of a review, but the point is to touch on the topic of order statistics more generally.&lt;/p&gt;
&lt;h2 id=&#34;order-statistics&#34;&gt;Order Statistics&lt;/h2&gt;
&lt;p&gt;The \(i^{\text{th}}\) order statistic is the \(i^{\text{th}}\) smallest element in a set of \(n\) elements. The median is the \(\frac{n}{2}^{\text{th}}\) order statistic. The minimum and maximum are the \(1^{\text{st}}\) and \(n^{\text{th}}\) order statistics, respectively. When \(n\) is even, there are two medians:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the lower median \(\frac{n}{2}^{\text{th}}\) and&lt;/li&gt;
&lt;li&gt;the upper median \(\frac{n}{2} + 1^{\text{th}}\).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The goal of the algorithm of focus in these notes is to determine how select order statistic \(i\) in a set of \(n\) elements. As we saw previously, and will review in these notes, we can use a divide and conquer approach to solve this problem. Further, we will study a linear approach to this problem under the assumption that the elements are distinct.&lt;/p&gt;
&lt;h2 id=&#34;minimum-and-maximum&#34;&gt;Minimum and Maximum&lt;/h2&gt;
&lt;p&gt;One could reason very simply that the lower bound on the number of comparisons needed to find either the minimum or maximum of a set is \(n-1\). One such argument could be that if we left even 1 comparison out of the \(n-1\) comparisons, we could not guarantee that we had found the minimum or maximum. When implementing an algorithm, we would say that an optimal implementation would require \(n-1\) comparisons.&lt;/p&gt;
&lt;p&gt;As a quick aside, there are plenty of algorithms that we implement which are not optimal in terms of their theoretical lower bound. Consider a naive matrix multiplication algorithm. There are many redundant reads from memory in this algorithm. For example, if we compute \(C = AB\), we need to calculate the output values \(C_{1, 1}\) and \(C_{1, 2}\), among others. Both of these outputs require reading from the first row of \(A\).&lt;/p&gt;
&lt;p&gt;We could find both the minimum and maximum of a set in \(2n - 2\) operations by passing over the set twice. This is theoretically optimal since each pass is performing the optimal \(n-1\) comparisons. If we first compared a pair of elements with each other before comparing them to the minimum and maximum, respectively, we could find both the minimum and maximum in \(3\left\lfloor\frac{n}{2}\right\rfloor\) comparisons.&lt;/p&gt;
&lt;h2 id=&#34;selection-in-expected-linear-time&#34;&gt;Selection in expected linear time&lt;/h2&gt;
&lt;p&gt;We now turn to the problem of &lt;strong&gt;selection&lt;/strong&gt;. Given a set of \(n\) elements and an integer \(i\), we want to find the \(i^{\text{th}}\) order statistic. We will assume that all elements are distinct. We will also assume that \(i\) is between 1 and \(n\).&lt;/p&gt;
&lt;p&gt;The randomized select algorithm returns the \(i^{\text{th}}\) smallest element of an array bounded between indices \(p\) and \(r\). It relies on &lt;code&gt;randomized_partition&lt;/code&gt;, just like &lt;a href=&#34;https://ajdillhoff.github.io/notes/quicksort/&#34;&gt;Quicksort&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;randomized_select&lt;/span&gt;(A, p, r, i):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; r:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; A[p]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; randomized_partition(A, p, r)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; q &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; k:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; A[q]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; k:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; randomized_select(A, p, q&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, i)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; randomized_select(A, q&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, r, i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;k)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first conditional checks if the array had only a single element, in which case it must be the value we are looking for. If not, the array is partitioned so that each element in \(A[p:q-1]\) is less than or equal to \(A[q]\) which is less than or equal to the elements in \(A[q+1:r]\). The line \(k = q - p + 1\) calculates the number of elements less than or equal to the pivot. If the index we are looking for is equal to this number, then we have found it and can return the value immediately.&lt;/p&gt;
&lt;p&gt;If the value was not yet found and \(i &amp;lt; k\), then \(i\) must be in the subarray \(A[p:q-1]\). Therefore, the function is recursively called on that subarray. Otherwise, the subarray \(A[q+1:r]\) is checked. An example from Cormen et al. is shown below (Cormen et al. 2022).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-12_18-05-54_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Randomized select from (Cormen et al. 2022).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Randomized select from (Cormen et al. 2022).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Explanation of figure&lt;/strong&gt; \(A^{(0)}\) shows that \(A[5] = 14\) was chosen as the pivot. The next row, \(A^{(1)}\), depicts the completed partitioning. Cormen et al. note that this is &lt;em&gt;not&lt;/em&gt; a helpful partitioning since less than \(\frac{1}{4}\) of the elements are ignored. A helpful partition is one that leaves at most \(\frac{3}{4}\) of the elements after the partitioning.&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;The worst-case running time of &lt;code&gt;randomized_select&lt;/code&gt; is \(O(n^2)\) since we are partitioning \(n\) elements at \(\Theta(n)\) each. Since the pivot of &lt;code&gt;randomized_partition&lt;/code&gt; is selected at random, we can expect a &lt;em&gt;good&lt;/em&gt; split at least every 2 times it is called. The proof for this is similar to the one made when analyzing &lt;a href=&#34;https://ajdillhoff.github.io/notes/quicksort/&#34;&gt;Quicksort&lt;/a&gt;. Briefly, the expected number of times we must partition before we get a helpful split is 2, which only doubles the running time. The recurrence is still \(T(n) = T(3n/4) + \Theta(n) = \Theta(n)\).&lt;/p&gt;
&lt;p&gt;The first step to showing that the expected runtime of &lt;code&gt;randomized_select&lt;/code&gt; is \(\Theta(n)\) is to show that a partitioning is helpful with probability at least \(\frac{1}{2}\) (Cormen et al. 2022). The rest of the proof requires further examination and comprehension.&lt;/p&gt;
&lt;p&gt;The proof presented by Cormen et al. begins with the following terms:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;\(h_i\) is the event that the \(i^{\text{th}}\) partitioning is helpful.&lt;/li&gt;
&lt;li&gt;\(\{h_0, h_1, \dots, h_m\}\) is the sequence of helpful partitionings.&lt;/li&gt;
&lt;li&gt;\(n_k = |A^{(h_k)}|\) is the number of elements in the subarray \(A^{(h_k)}\) at the \(k^{\text{th}}\) partitioning.&lt;/li&gt;
&lt;li&gt;\(n_k \leq (3/4)n_{k-1}\) for \(k \geq 1\), or \(n_k \leq (3/4)^kn_0\).&lt;/li&gt;
&lt;li&gt;\(X_k = h_{k+1} - h_k\) is the number of unhelpful partitionings between the \(k^{\text{th}}\) and \((k+1)^{\text{th}}\) helpful partitionings.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are certainly partitionings that are not helpful. These are depicted as subarrays within each generation of helpful partitionings. The figure below exemplifies this.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-13_10-12-23_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;The sets within each generation of helpful partitionings are not helpful. From (Cormen et al. 2022).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;The sets within each generation of helpful partitionings are not helpful. From (Cormen et al. 2022).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Given that the probability that a partitioning is helpful is at least \(\frac{1}{2}\), we know that \(E[X_k] \leq 2\). With this, &lt;strong&gt;an upper bound on the number of comparisons of partitioning is derived.&lt;/strong&gt; The total number of comparisons made when partitioning is less than&lt;/p&gt;
&lt;p&gt;\begin{align*}
\sum_{k=0}^{m-1} \sum_{j=h_k}^{h_k + X_k - 1} |A^{(j)}| &amp;amp;\leq \sum_{k=0}^{m-1} \sum_{j=h_k}^{h_k + X_k - 1} |A^{(h_k)}| \\
&amp;amp;= \sum_{k=0}^{m-1} X_k|A^{(h_k)}| \\
&amp;amp;\leq \sum_{k=0}^{m-1} \left(\frac{3}{4}\right)^k n_0. \\
\end{align*}&lt;/p&gt;
&lt;p&gt;The first term on the first line represents the total number of comparisons across all sets. The first sum loops through the \(m\) helpful partitionings, and the inner loop sums the number of comparisons made for each unhelpful partitioning. It is bounded by the term on the right. &lt;strong&gt;This is because \(|A^{(j)}| \leq |A^{(h_k)}|\) if \(A^{(j)}\) is in the \(k^{\text{th}}\) generation of helpful partitionings (see term 4 above).&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using term 5 from above, the second line is derived. The third line leverages term 4 again. The sum is a geometric series, and the total number of comparisons is less than&lt;/p&gt;
&lt;p&gt;\begin{align*}
\text{E} \left[\sum_{k=0}^{m-1} X_k \left(\frac{3}{4}\right)^k n_0\right] &amp;amp;= n_0 \sum_{k=0}^{m-1} \left(\frac{3}{4}\right)^k \text{E}[X_k]\\
&amp;amp;\leq 2n_0 \sum_{k=0}^{m-1} \left(\frac{3}{4}\right)^k \\
&amp;amp;&amp;lt; 2n_0 \sum_{k=0}^{\infty} \left(\frac{3}{4}\right)^k \\
&amp;amp;= 8n_0.
\end{align*}&lt;/p&gt;
&lt;p&gt;The last line is the result of a geometric series. This concludes the proof that &lt;code&gt;randomized_partition&lt;/code&gt; runs in expected linear time.&lt;/p&gt;
&lt;h2 id=&#34;problems-and-exercises&#34;&gt;Problems and Exercises&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Show that the second largest of \(n\) elements can be found with \(n + \lceil\log_2 n\rceil - 2\) comparisons in the worst case.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Sorting in Linear Time</title>
      <link>https://ajdillhoff.github.io/notes/sorting_in_linear_time/</link>
      <pubDate>Mon, 11 Mar 2024 17:10:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/sorting_in_linear_time/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#establishing-a-lower-bound-on-comparison-sorts&#34;&gt;Establishing a Lower Bound on Comparison Sorts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#counting-sort&#34;&gt;Counting Sort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#radix-sort&#34;&gt;Radix Sort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bucket-sort&#34;&gt;Bucket Sort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#questions-and-exercises&#34;&gt;Questions and Exercises&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;These are my personal notes for Chapter 8 of &lt;em&gt;Introduction to Algorithms&lt;/em&gt; (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;). Readers should reference the book for more details when necessary.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;All sorting algorithms discussed up to this point are &lt;strong&gt;comparison based&lt;/strong&gt;. You may have thought, as I did, that sorting cannot be done without a comparison. If you have no way to evaluate the relative ordering of two different objects, how can you possibly arrange them in any order?&lt;/p&gt;
&lt;p&gt;The answer will become clear shortly and is investigated through &lt;strong&gt;counting sort&lt;/strong&gt;, &lt;strong&gt;radix sort&lt;/strong&gt;, and &lt;strong&gt;bucket sort&lt;/strong&gt;. First, Cormen et al. make it clear that sorting algorithms cannot reach linear time. As we will see, any comparison sort &lt;em&gt;must&lt;/em&gt; make \(\Omega (n \lg n)\) comparisons in the worst case to sort \(n\) elements. This bound is motivation enough to explore a different class of sorting algorithms.&lt;/p&gt;
&lt;h2 id=&#34;establishing-a-lower-bound-on-comparison-sorts&#34;&gt;Establishing a Lower Bound on Comparison Sorts&lt;/h2&gt;
&lt;p&gt;The basis of the proof presented in Chapter 8 it to consider that all comparison sorts can be viewed as a decision tree, where each leaf represents a unique permutation of the inputs. If there are \(n\) distinct elements in the original array, then there must be \(n!\) leaves in the decision tree. Consider the figure from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;) below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-11_17-45-30_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Decision tree for comparison sort on three elements.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Decision tree for comparison sort on three elements.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the figure above, each node compares two values as \(a:b\). If \(a \leq b\), the left path is taken. The worst case of a comparison sort can be determined by the height of the tree. A proof on the lower bound goes as follows.&lt;/p&gt;
&lt;p&gt;Consider a binary tree of height \(h\) with \(l\) reachable leaves. Each of the \(n!\) permutations occurs as one of the leaves, so \(n! \leq l\) since there may be duplicate permutations in the leaves. A binary tree with height \(h\) has no more than \(2^h\) leaves, so \(n! \leq l \leq 2^h\). Taking the logarithm of this inequality implies that \(h \geq \lg n!\). Since \(\lg n! = \Theta(n \lg n)\), and is a lower bound on the height of the tree, then any comparison sort must make \(\Omega (n \lg n)\) comparisons.&lt;/p&gt;
&lt;h2 id=&#34;counting-sort&#34;&gt;Counting Sort&lt;/h2&gt;
&lt;p&gt;Counting sort can sort an array of integers in \(O(n+k)\) time, where \(k \geq 0\) is the largest integer in the set. It works by counting the number of elements less than or equal to each element \(x\).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;counting_sort&lt;/span&gt;(A, k):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    B &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    C &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(k&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        C[A[i]] &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# C[i] contains the number of elements equal to i&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, k):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        C[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; C[i] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; C[i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# C[i] contains the number of elements less than or equal to i&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        B[C[A[i]]&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        C[A[i]] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; C[A[i]] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; B
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first two loops establish the number of elements less than or equal to \(i\) for each element \(i\). The main sticking point in understanding this algorithm is the last loop. It starts at the very end of loop, placing the last element from \(A\) into the output array \(B\) in its correct position as determined by \(C\).&lt;/p&gt;
&lt;p&gt;Consider a simple example with \(\{2, 5, 5, 3, 4\}\). After the second loop, \(C = \{0, 0, 1, 2, 3, 5\}\). On the first iteration of the last loop, \(A[4] = 4\) is used as the index into \(C\), which yields \(3\) since the value \(4\) is greater than or equal to \(3\) elements in the original array. It is then placed in the correct spot \(B[3-1] = 4\).&lt;/p&gt;
&lt;p&gt;Another example is shown in figure 8.2 from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;radix-sort&#34;&gt;Radix Sort&lt;/h2&gt;
&lt;p&gt;Dating back to 1887 by Herman Hollerith&amp;rsquo;s work on &lt;a href=&#34;https://en.wikipedia.org/wiki/Tabulating_machine&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;tabulating machines&lt;/a&gt;, this algorithm places numbers in one of \(k\) bins based on their &lt;strong&gt;radix&lt;/strong&gt;, or the number of unique digits. It was used for sorting punch cards via multi-column sorting. It works by iteratively sorting a series of inputs based on a column starting with the least-significant digit. An example is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-12_10-34-07_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Radix sort in action (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Radix sort in action (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the figure above, the items are first sorted based on the least significant digits. By the end of the process, the data is numerically sorted in ascending order. The algorithm can be written simply:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;radix_sort&lt;/span&gt;(A, d):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(d):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; counting_sort(A, len(A), &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; A
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Counting sort is the typical sorting algorithm that is used to sort the digits in each column. In fact, any &lt;em&gt;stable&lt;/em&gt; sorting algorithm can be used in place of counting sort.&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;We know that counting sort is \(\Theta(n + k)\), and that radix sort calls it \(d\) times. Therefore, the time complexity of radix sort is \(\Theta(d(n + k))\). If \(k = O(n)\), then the time complexity is \(\Theta(dn)\).&lt;/p&gt;
&lt;h3 id=&#34;complex-keys&#34;&gt;Complex Keys&lt;/h3&gt;
&lt;p&gt;What if the data is not just a single integer, but a complex key or series of keys? The keys themselves can be broken up into digits. Consider a 32-bit word. If we want to sort \(n\) of these words, and we have \(b = 32\) bits per word, we can break the words into \(r=8\) bit digits. This yields \(d = \lceil b / r \rceil = 4\) digits. The largest value for each digit is then \(k = 2^r - 1 = 255\). Plugging these values into the analysis from above yields $Θ((b/r)(n + 2^r)).$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What is the best choice of \(r\)?&lt;/strong&gt; Consider what happens for different values of \(r\). As \(r\) increases, \(2^r\) increases. As it decreases, \(\frac{b}{r}\) increases. The best choice depends on whether \(b &amp;lt; \lfloor \lg n \rfloor\). If \(b &amp;lt; \lfloor \lg n \rfloor\), then \(r \leq b\) implies \((n + 2^r) = \Theta(n)\) since \(2^{\lg n} = n\). If \(b \geq \lfloor \lg n \rfloor\), then we should choose \(r \approx \lg n\). This would yield \(\Theta((b / \lg n)(n + n)) = \Theta(bn / \lg n)\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You should spend some time to think about the choices for \(r\).&lt;/strong&gt; Specifically, what will happen to the time complexity as \(r\) increases above \(\lg n\)? What about as \(r\) decreases below \(\lg n\)?&lt;/p&gt;
&lt;h3 id=&#34;example-sorting-words&#34;&gt;Example: Sorting words&lt;/h3&gt;
&lt;p&gt;Use radix sort to sort the following list of names: &amp;ldquo;Beethoven&amp;rdquo;, &amp;ldquo;Bach&amp;rdquo;, &amp;ldquo;Mozart&amp;rdquo;, &amp;ldquo;Chopin&amp;rdquo;, &amp;ldquo;Liszt&amp;rdquo;, &amp;ldquo;Schubert&amp;rdquo;, &amp;ldquo;Haydn&amp;rdquo;, &amp;ldquo;Brahms&amp;rdquo;, &amp;ldquo;Wagner&amp;rdquo;, &amp;ldquo;Tchaikovsky&amp;rdquo;. First, we need to figure out how to encode the names as integers. If we convert the input to lowercase, we only have to deal with \(k=26\) unique characters. This only requires 5 bits. Since each name has varying length, we can use a sentinel value of 0 to pad the shorter names. That is, 0 represents a padding character and the alphabet starts at 1. The names are then encoded as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Original Name&lt;/th&gt;
&lt;th&gt;Encoded Name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Beethoven&lt;/td&gt;
&lt;td&gt;[2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bach&lt;/td&gt;
&lt;td&gt;[2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mozart&lt;/td&gt;
&lt;td&gt;[13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chopin&lt;/td&gt;
&lt;td&gt;[3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Liszt&lt;/td&gt;
&lt;td&gt;[12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Schubert&lt;/td&gt;
&lt;td&gt;[19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Haydn&lt;/td&gt;
&lt;td&gt;[8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Brahms&lt;/td&gt;
&lt;td&gt;[2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Wagner&lt;/td&gt;
&lt;td&gt;[23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tchaikovsky&lt;/td&gt;
&lt;td&gt;[20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;No changes will be made for the first 2 iterations of the sort. The third iteration will yield the following:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Original Name&lt;/th&gt;
&lt;th&gt;Encoded Name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Bach&lt;/td&gt;
&lt;td&gt;[2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mozart&lt;/td&gt;
&lt;td&gt;[13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chopin&lt;/td&gt;
&lt;td&gt;[3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Liszt&lt;/td&gt;
&lt;td&gt;[12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Schubert&lt;/td&gt;
&lt;td&gt;[19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Haydn&lt;/td&gt;
&lt;td&gt;[8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Brahms&lt;/td&gt;
&lt;td&gt;[2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Wagner&lt;/td&gt;
&lt;td&gt;[23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Beethoven&lt;/td&gt;
&lt;td&gt;[2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tchaikovsky&lt;/td&gt;
&lt;td&gt;[20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Iteration 3&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Original Name&lt;/th&gt;
&lt;th&gt;Encoded Name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Bach&lt;/td&gt;
&lt;td&gt;[2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mozart&lt;/td&gt;
&lt;td&gt;[13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chopin&lt;/td&gt;
&lt;td&gt;[3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Liszt&lt;/td&gt;
&lt;td&gt;[12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Haydn&lt;/td&gt;
&lt;td&gt;[8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Brahms&lt;/td&gt;
&lt;td&gt;[2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Wagner&lt;/td&gt;
&lt;td&gt;[23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Schubert&lt;/td&gt;
&lt;td&gt;[19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Beethoven&lt;/td&gt;
&lt;td&gt;[2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tchaikovsky&lt;/td&gt;
&lt;td&gt;[20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Iteration 4&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Original Name&lt;/th&gt;
&lt;th&gt;Encoded Name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Bach&lt;/td&gt;
&lt;td&gt;[2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Liszt&lt;/td&gt;
&lt;td&gt;[12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Haydn&lt;/td&gt;
&lt;td&gt;[8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Schubert&lt;/td&gt;
&lt;td&gt;[19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tchaikovsky&lt;/td&gt;
&lt;td&gt;[20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chopin&lt;/td&gt;
&lt;td&gt;[3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Beethoven&lt;/td&gt;
&lt;td&gt;[2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Wagner&lt;/td&gt;
&lt;td&gt;[23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Brahms&lt;/td&gt;
&lt;td&gt;[2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mozart&lt;/td&gt;
&lt;td&gt;[13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Iteration 5&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Original Name&lt;/th&gt;
&lt;th&gt;Encoded Name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Bach&lt;/td&gt;
&lt;td&gt;[2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Schubert&lt;/td&gt;
&lt;td&gt;[19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Wagner&lt;/td&gt;
&lt;td&gt;[23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Beethoven&lt;/td&gt;
&lt;td&gt;[2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tchaikovsky&lt;/td&gt;
&lt;td&gt;[20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chopin&lt;/td&gt;
&lt;td&gt;[3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Brahms&lt;/td&gt;
&lt;td&gt;[2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Haydn&lt;/td&gt;
&lt;td&gt;[8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mozart&lt;/td&gt;
&lt;td&gt;[13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Liszt&lt;/td&gt;
&lt;td&gt;[12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Iteration 6&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Original Name&lt;/th&gt;
&lt;th&gt;Encoded Name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Tchaikovsky&lt;/td&gt;
&lt;td&gt;[20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mozart&lt;/td&gt;
&lt;td&gt;[13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Haydn&lt;/td&gt;
&lt;td&gt;[8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bach&lt;/td&gt;
&lt;td&gt;[2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Brahms&lt;/td&gt;
&lt;td&gt;[2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Wagner&lt;/td&gt;
&lt;td&gt;[23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chopin&lt;/td&gt;
&lt;td&gt;[3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Beethoven&lt;/td&gt;
&lt;td&gt;[2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Schubert&lt;/td&gt;
&lt;td&gt;[19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Liszt&lt;/td&gt;
&lt;td&gt;[12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Iteration 7&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Original Name&lt;/th&gt;
&lt;th&gt;Encoded Name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Brahms&lt;/td&gt;
&lt;td&gt;[2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bach&lt;/td&gt;
&lt;td&gt;[2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Beethoven&lt;/td&gt;
&lt;td&gt;[2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Wagner&lt;/td&gt;
&lt;td&gt;[23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tchaikovsky&lt;/td&gt;
&lt;td&gt;[20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Schubert&lt;/td&gt;
&lt;td&gt;[19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chopin&lt;/td&gt;
&lt;td&gt;[3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Liszt&lt;/td&gt;
&lt;td&gt;[12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Haydn&lt;/td&gt;
&lt;td&gt;[8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mozart&lt;/td&gt;
&lt;td&gt;[13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Iteration 8&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Original Name&lt;/th&gt;
&lt;th&gt;Encoded Name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Bach&lt;/td&gt;
&lt;td&gt;[2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Haydn&lt;/td&gt;
&lt;td&gt;[8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Wagner&lt;/td&gt;
&lt;td&gt;[23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tchaikovsky&lt;/td&gt;
&lt;td&gt;[20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Schubert&lt;/td&gt;
&lt;td&gt;[19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Beethoven&lt;/td&gt;
&lt;td&gt;[2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chopin&lt;/td&gt;
&lt;td&gt;[3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Liszt&lt;/td&gt;
&lt;td&gt;[12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mozart&lt;/td&gt;
&lt;td&gt;[13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Brahms&lt;/td&gt;
&lt;td&gt;[2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Iteration 9&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Original Name&lt;/th&gt;
&lt;th&gt;Encoded Name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Bach&lt;/td&gt;
&lt;td&gt;[2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Beethoven&lt;/td&gt;
&lt;td&gt;[2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Brahms&lt;/td&gt;
&lt;td&gt;[2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chopin&lt;/td&gt;
&lt;td&gt;[3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Haydn&lt;/td&gt;
&lt;td&gt;[8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Liszt&lt;/td&gt;
&lt;td&gt;[12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mozart&lt;/td&gt;
&lt;td&gt;[13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Schubert&lt;/td&gt;
&lt;td&gt;[19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tchaikovsky&lt;/td&gt;
&lt;td&gt;[20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Wagner&lt;/td&gt;
&lt;td&gt;[23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;bucket-sort&#34;&gt;Bucket Sort&lt;/h2&gt;
&lt;p&gt;The final sorting algorithm in Chapter 8 is bucket sort (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;). As the name suggests, bucket sort distributes the input into a number of distinct buckets based on the input value. The key here is the assumption that the data is uniformly distributed. If the data were not uniformly distributed, then more elements would be concentrated. The uniformity ensures that a relatively equal number of data points are placed in each bucket. This is also a convenient assumption to have for a parallelized implementation.&lt;/p&gt;
&lt;p&gt;The algorithm works by placing values into a bucket based on their most significant digits. Once the values are assigned, then a simple sort, like insertion sort, is used to sort the values within each bucket. Once sorted, the buckets are concatenated together to produce the final output. Under the assumption of uniformity, each bucket will contain no more than \(1/n\) of the total elements. This implies that each call to &lt;code&gt;insertion_sort&lt;/code&gt; will take \(O(1)\) time.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-12_11-54-07_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Bucket sort in action (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Bucket sort in action (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bucket_sort&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    B &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        B[int(n &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; A[i])]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(A[i])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        insertion_sort(B[i])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; B
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;Initializing the array and placing each item into a bucket takes \(\Theta(n)\) time. The call to each insertion sort is \(O(n^2)\). Therefore, the recurrence is given as&lt;/p&gt;
&lt;p&gt;\[
T(n) = \Theta(n) + \sum_{i=0}^{n-1} O(n_i^2).
\]&lt;/p&gt;
&lt;p&gt;The key is to determine the expected value \(E[n_i^2]\). We will frame the problem as a binomial distribution, where a success occurs when an element goes into bucket \(i\).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(p\) is the probability of success: \(p = \frac{1}{n}\).&lt;/li&gt;
&lt;li&gt;\(q\) is the probability of failure: \(q = 1 - \frac{1}{n}\).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Under a binomial distribution, we have that \(E[n_i] = np = n(1/n) = 1\) and \(\text{Var}[n_i] = npq = 1 - 1/n\), where \(p = 1/n\) and \(q = 1 - 1/n\). The expected value is then&lt;/p&gt;
&lt;p&gt;\[
E[n_i^2] = \text{Var}[n_i] + E[n_i]^2 = 1 - 1/n + 1 = 2 - 1/n.
\]&lt;/p&gt;
&lt;p&gt;This gives way to the fact that \(E[T(n)] = \Theta(n) + \sum_{i=0}^{n-1} O(2 - 1/n) = \Theta(n) + O(n) = \Theta(n)\).&lt;/p&gt;
&lt;h2 id=&#34;questions-and-exercises&#34;&gt;Questions and Exercises&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Come up with applications of counting sort when \(k = O(n)\).&lt;/li&gt;
&lt;li&gt;Paul wants to use radix sort to sort \(2^{16}\) 32-bit numbers. What is the best value of \(r\) to use? How many calls to a stable sort will be made?&lt;/li&gt;
&lt;li&gt;In what ways does radix sort differ from quicksort? When is one better than the other?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GPU Pattern: Merge</title>
      <link>https://ajdillhoff.github.io/notes/gpu_pattern_merge/</link>
      <pubDate>Wed, 28 Feb 2024 19:19:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/gpu_pattern_merge/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#key-concepts-and-challenges&#34;&gt;Key Concepts and Challenges&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-merge-operation&#34;&gt;The Merge Operation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tiled-merge&#34;&gt;Tiled Merge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#circular-buffers&#34;&gt;Circular Buffers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#thread-coarsening&#34;&gt;Thread Coarsening&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;key-concepts-and-challenges&#34;&gt;Key Concepts and Challenges&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Dynamic input data identification&lt;/li&gt;
&lt;li&gt;Data locality&lt;/li&gt;
&lt;li&gt;Buffer management schemes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;merge&lt;/strong&gt; operation takes two sorted subarrays and combines them into a single sorted array. You may be familiar with this approach from studying &lt;a href=&#34;https://ajdillhoff.github.io/notes/divide_and_conquer_algorithms/&#34;&gt;Divide and Conquer Algorithms&lt;/a&gt;. Parallelizing the merge operation is a non-trivial task and will require the use of a few new techniques.&lt;/p&gt;
&lt;h2 id=&#34;the-merge-operation&#34;&gt;The Merge Operation&lt;/h2&gt;
&lt;p&gt;The specific parallel merge operation studied in these notes is from &amp;ldquo;Perfectly load-balanced, optimal, stable, parallel merge&amp;rdquo; (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Siebert and Träff 2013&lt;/a&gt;). Their approach works by first computing which values are needed in each merge step, and then using a parallel kernel to compute the merge. These steps can be computed by each thread independently.&lt;/p&gt;
&lt;h3 id=&#34;co-rank-function&#34;&gt;Co-rank Function&lt;/h3&gt;
&lt;p&gt;The key to the parallel merge algorithm reviewed in these notes is the &lt;strong&gt;co-ranking function&lt;/strong&gt; (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Siebert and Träff 2013&lt;/a&gt;). This function computes the range of indices needed from the two input values to produce a given value in the output array, without actually needing to merge the two input arrays.&lt;/p&gt;
&lt;p&gt;When merging two sorted arrays, we can observe that the output index \(0 \leq k &amp;lt; m + n\) comes from either \(0 \leq i &amp;lt; m\) from input \(A\) or \(0 \leq j &amp;lt; n\) from input \(B\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-11_14-22-24_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Merging (A) and (B) to produce (C).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Merging (A) and (B) to produce (C).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the figure above, the element at \(k = 3\) comes from \(A[2]\), so \(i = 2\). It must be that \(k=3\) is the result of merging the first \(i=2\) elements of \(A\) with the first \(j=k - i\) elements of \(B\). This works both ways: for \(k=6\), the value is taken from \(B[3]\), so \(j=3\), and the result is the merge of the first \(i=k-j\) elements of \(A\) with the first \(j=3\) elements of \(B\).&lt;/p&gt;
&lt;p&gt;An efficient method for computing the co-rank function follows the first lemma put forth in (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Siebert and Träff 2013&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Lemma 1&lt;/strong&gt;&lt;/strong&gt;. For any \(k, 0 \leq k &amp;lt; m + n\), there exists a unique \(i, 0 \leq i \leq m\), and a unique \(j, 0 \leq j \leq n\), with \(i + j = k\) such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;\(i = 0\) or \(A[i-1] \leq B[j]\) and&lt;/li&gt;
&lt;li&gt;\(j = 0\) or \(B[j-1] &amp;lt; A[i]\).&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-11_13-58-23_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Co-rank function visualization (&amp;lt;a href=&amp;#34;#citeproc_bib_item_2&amp;#34;&amp;gt;Siebert and Träff 2013&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Co-rank function visualization (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Siebert and Träff 2013&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;implementation&#34;&gt;Implementation&lt;/h4&gt;
&lt;p&gt;Given the rank \(k\) of an element in an output array \(C\) and two input arrays \(A\) and \(B\), the co-rank function \(f\) returns the co-rank value for the corresponding element in \(A\) and \(B\).&lt;/p&gt;
&lt;p&gt;How would the co-rank function be used in the example above? Given two threads, let thread 1 compute the co-rank for \(k=4\). This would return \(i=3\) and \(j=1\). We quickly verify that this passes the first lemma stated above.&lt;/p&gt;
&lt;p&gt;\[
A[2] = 5 \leq B[1] = 5$ and $B[0] = 3 &amp;lt; A[3] = 7.
\]&lt;/p&gt;
&lt;p&gt;Code for the co-rank function is given below. Since the input arrays are already sorted, we can use a binary search to find the co-rank values.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;co_rank&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; k, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;A, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; m, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;B, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(k, m);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; i;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i_low &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, k&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;n);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; j_low &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, k&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;m);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; delta;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;bool&lt;/span&gt; active &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; true;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; (active) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; A[i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; B[j]) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            delta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (i &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; i_low &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            j_low &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; j;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            i &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; delta;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            j &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; delta;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        } &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (j &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; m &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; B[j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; A[i]) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            delta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (j &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; j_low &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            i_low &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            j &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; delta;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            i &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; delta;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        } &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            active &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; false;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; i;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Consider running a merge kernel across 3 threads where each thread takes 3 sequential output values. Use the co-rank function to compute the co-rank values for \(k=3\) and \(k=6\), simulating the tasks for the second and third threads. The values for \(k=3\) should be \(i=2\) and \(j=1\), for reference. All values below these indices would be used by the first thread.&lt;/p&gt;
&lt;h3 id=&#34;parallel-kernel&#34;&gt;Parallel Kernel&lt;/h3&gt;
&lt;p&gt;We can now implement a basic parallel merge kernel. Each thread is responsible for determining how many elements it will be responsible for merging. The range of input values is determined via two calls to &lt;code&gt;co_rank&lt;/code&gt;, one for the starting and ending point.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;__global__ &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;merge_basic_kernel&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;A, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; m, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;B, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;C) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; tid &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; blockIdx.x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; blockDim.x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; threadIdx.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; elementsPerThread &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ceil((m &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; n) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (blockDim.x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; gridDim.x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; k_curr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tid &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; elementsPerThread; &lt;span style=&#34;color:#75715e&#34;&gt;// start output index
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; k_next &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min((tid &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; elementsPerThread, m &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; n); &lt;span style=&#34;color:#75715e&#34;&gt;// end output index
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i_curr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; co_rank(k_curr, A, m, B, n);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i_next &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; co_rank(k_next, A, m, B, n);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; j_curr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; k_curr &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; i_curr;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; j_next &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; k_next &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; i_next;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    merge_sequential(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;A[i_curr], i_next &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; i_curr, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;B[j_curr], j_next &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; j_curr, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;C[k_curr]);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Two major issues should be clear from the code above. First, the memory being accesses is not coalesced. The binary search in &lt;code&gt;co_rank&lt;/code&gt; also means that the memory access pattern is less than ideal. Since the main issue in both cases relates to memory efficiency, we should look at tools that address memory access patterns.&lt;/p&gt;
&lt;h2 id=&#34;tiled-merge&#34;&gt;Tiled Merge&lt;/h2&gt;
&lt;p&gt;The memory access pattern is sparse and thus does not take advantage of coalescing. We can improve upon this by having the threads transfer data from global memory to shared memory in a coalesced manner. That way the higher latency operation will be coalesced. The data in shared memory may be accessed out of order, but the latency is much lower.&lt;/p&gt;
&lt;p&gt;The subarrays from \(A\) and \(B\) that are used by adjacent threads are also adjacent in memory. By considering block-level subarrays, we can ensure that the data is coalesced. This is the idea behind the tiled merge algorithm. The figure below visualizes this concept.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-16_13-22-25_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Design of a tiled merge kernel (recreated from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;)).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Design of a tiled merge kernel (recreated from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;)).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The shared memory blocks \(A_s\) and \(B_s\) obviously cannot store the entire range of data needed. In each iteration, the threads in a block will load a new set of data from global memory to shared memory. The light gray section of the block from \(A\) and \(B\) are loaded into \(A_s\) and \(B_s\), respectively. If they collectively load \(2n\) elements, only \(n\) elements will be used in the merge operation. This is because in the worst case, all elements going to the output array will come from one of the two input arrays. See the exercise at the end of this section for a more detailed explanation.&lt;/p&gt;
&lt;p&gt;Each block will use a portion of both \(A_s\) and \(B_s\) to compute the merge. This is shown with dotted lines going from the shared memory to the output array.&lt;/p&gt;
&lt;h3 id=&#34;part-1&#34;&gt;Part 1&lt;/h3&gt;
&lt;p&gt;The code below shows the first part of the tiled merge kernel.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void merge_tiled_kernel(int *A, int m, int n, int *C, int tile_size) {
    extern __shared__ int shareAB[];
    int *A_s = &amp;amp;shareAB[0];
    int *B_s = &amp;amp;shareAB[tile_size];
    int C_curr = blockIdx.x * ceil((m+n)/gridDim.x);
    int C_next = min((blockIdx.x+1) * ceil((m+n)/gridDim.x), m+n);

    if (threadIdx.x == 0) {
        // Block-level co-rank values will be available to all threads in the block
        A_s[0] = co_rank(C_curr, A, m, B, n);
        A_s[1] = co_rank(C_next, A, m, B, n);
    }
    __syncthreads();

    int A_curr = A_s[0];
    int A_next = A_s[1];
    int B_curr = C_curr - A_curr;
    int B_next = C_next - A_next;
    __syncthreads();
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The first part establishes the shared memory and the co-rank values for the &lt;strong&gt;block&lt;/strong&gt;. Each thread will have access to the start and end values for input matrices \(A\) and \(B\) as well. If the kernel is just getting started, we would have that &lt;code&gt;A_curr = 0&lt;/code&gt;, &lt;code&gt;B_curr = 0&lt;/code&gt;, and &lt;code&gt;C_curr = 0&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;part-2&#34;&gt;Part 2&lt;/h3&gt;
&lt;p&gt;The second part of the kernel is responsible for loading the input data into shared memory. This is done in a coalesced manner, as the threads in a block will load a contiguous section of the input arrays.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;int counter = 0;
int C_length = C_next - C_curr;
int A_length = A_next - A_curr;
int B_length = B_next - B_curr;
int total_iteration = ceil(C_length / tile_size);
int C_completed = 0;
int A_consumed = 0;
int B_consumed = 0;
while (counter &amp;lt; total_iteration) {
    for (int i = 0; i &amp;lt; tile_size; i += blockDim.x) {
        if (i + threadIdx.x &amp;lt; A_length - A_consumed) {
            A_s[i + threadIdx.x] = A[A_curr + A_consumed + i + threadIdx.x];
        }
        if (i + threadIdx.x &amp;lt; B_length - B_consumed) {
            B_s[i + threadIdx.x] = B[B_curr + B_consumed + i + threadIdx.x];
        }
    }
    __syncthreads();
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;part-3&#34;&gt;Part 3&lt;/h3&gt;
&lt;p&gt;With the input in shared memory, each thread will divide up this input and merge their respective sections in parallel. This is done by calculating the &lt;code&gt;c_curr&lt;/code&gt; and &lt;code&gt;c_next&lt;/code&gt; first, which is the output section of the thread. Using those boundaries, two calls to &lt;code&gt;co_rank&lt;/code&gt; will determine the input sections the thread.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;        int c_curr = threadIdx.x * (tile_size / blockDim.x);
        int c_next = (threadIdx.x + 1) * (tile_size / blockDim.x);
        c_curr = (c_curr &amp;lt;= C_length - C_completed) ? c_curr : C_length - C_completed;
        c_next = (c_next &amp;lt;= C_length - C_completed) ? c_next : C_length - C_completed;

        int a_curr = co_rank(c_curr,
                             A_s, min(tile_size, A_length - A_consumed),
                             B_s, min(tile_size, B_length - B_consumed));
        int b_curr = c_curr - a_curr;
        int a_next = co_rank(c_next,
                             A_s, min(tile_size, A_length - A_consumed),
                             B_s, min(tile_size, B_length - B_consumed));
        int b_next = c_next - a_next;

        merge_sequential(&amp;amp;A_s[a_curr], a_next - a_curr, &amp;amp;B_s[b_curr], b_next - b_curr, &amp;amp;C[C_urr + C_completed + c_curr]);
        counter++;
        C_completed += tile_size;
        A_consumed += co_rank(tile_size A_s, tile_size, B_s, tile_size);
        B_consumed = C_completed - A_consumed;
        __syncthreads();
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;example-walkthrough-of-kernel&#34;&gt;Example: Walkthrough of Kernel&lt;/h3&gt;
&lt;p&gt;Consider the following example. We have two input arrays \(A = [1, 3, 5, 7, 9]\) and \(B = [2, 4, 6, 8, 10]\). The output array \(C\) will have 10 elements. We will use 2 blocks and 4 threads per block. The tile size is 4. With 10 elements and 2 blocks, each block is responsible for 5 elements.&lt;/p&gt;
&lt;p&gt;The main &lt;code&gt;while&lt;/code&gt; loop will need to iterate twice to cover the entire output array. The first iteration will load the first 4 elements of \(A\) and \(B\) into shared memory. Once the data is in memory, each thread divides the input tiles by running the co-rank function on the data that is in shared memory. The computed indices are the boundaries between each thread.&lt;/p&gt;
&lt;p&gt;In each iteration, a block is responsible for 4 elements. Given that we have 4 threads per block, each thread will be responsible for 1 output element per iteration. For thread 0 we have that &lt;code&gt;c_curr = 0&lt;/code&gt; and &lt;code&gt;c_next = 2&lt;/code&gt;. This results in &lt;code&gt;a_curr = 0&lt;/code&gt;, &lt;code&gt;b_curr = 0&lt;/code&gt;, &lt;code&gt;a_next = 1&lt;/code&gt;, and &lt;code&gt;b_next = 1&lt;/code&gt;. The merge operation will then be performed on the first element of \(A\) and \(B\).&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Coalesces global memory accesses&lt;/li&gt;
&lt;li&gt;Shared memory is used to reduce latency&lt;/li&gt;
&lt;li&gt;Only half the data loaded into shared memory is used; wasted memory bandwidth&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;exercise&#34;&gt;Exercise&lt;/h3&gt;
&lt;p&gt;Hwu et al. suggest that you can first call the co-rank function to get the current and next output sections. This would increase memory bandwidth at the cost of an additional binary search.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Where would this be done with respect to the tiled solution discussed in this section?&lt;/li&gt;
&lt;li&gt;How do these co-rank values differ from the ones used to calculate &lt;code&gt;C_curr&lt;/code&gt; and &lt;code&gt;C_next&lt;/code&gt;?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Hint:&lt;/strong&gt; If we knew the co-rank value for the start of the next section, we could ensure that only the data below that index is loaded into shared memory.&lt;/p&gt;
&lt;h2 id=&#34;circular-buffers&#34;&gt;Circular Buffers&lt;/h2&gt;
&lt;p&gt;The tiled merge algorithm is a significant improvement over the basic merge kernel. One glaring issue is that only half of the data loaded into shared memory is used, leading to a waste of memory bandwidth. The circular buffer merge algorithm addresses this issue by using a circular buffer to store the input data. Instead of writing over the shared memory values on each iteration, the data to be used in the next iteration stays in shared memory. A portion of new data is loaded into shared memory based on how much was used in the previous iteration.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-17_14-58-19_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Circular buffer scheme (recreated from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;)).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Circular buffer scheme (recreated from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;)).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The figure above outlines the main idea behind the circular buffer merge algorithm. Part A shows the initial data layout of global and shared memory. Only a portion of the data loaded into shared memory is used in the merge operation. This is shown in part B, where the blank portion of the shared memory depicts the data that was used. The light gray regions of shared memory are the left over data that can be used in the next iteration.&lt;/p&gt;
&lt;p&gt;The next block of data is loaded into shared memory. Since some of the required data already exists from the last iteration, a smaller portion needs to be loaded. This is shown in part C, where the new data (dark gray) is loaded into shared memory. The starting indices for both arrays was already set in the previous iteration. Consecutive values are simple to calculate using the mod operator.&lt;/p&gt;
&lt;p&gt;Part D shows the state of the arrays after the end of the second iteration. The blank areas in shared memory are the data that was used in the merge operation. For array &lt;code&gt;A_S&lt;/code&gt;, the index wrapped around to the beginning of the array. It is ready to be used in the next iteration.&lt;/p&gt;
&lt;h3 id=&#34;implementation&#34;&gt;Implementation&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;A_consumed&lt;/code&gt; can be used to keep track of how many new elements need to be read into shared memory.
The &lt;code&gt;co_rank&lt;/code&gt; and &lt;code&gt;merge_sequential&lt;/code&gt; functions need to be updated to work with circular buffers. It is easier to treat the shared memory as an extended array, that way we avoid situations where the &lt;code&gt;next&lt;/code&gt; index is less than the &lt;code&gt;current&lt;/code&gt; index.&lt;/p&gt;
&lt;h4 id=&#34;co-rank-function&#34;&gt;Co-Rank Function&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;int co_rank_circular(int k, int *A, int m, int *B, int n, int A_S_start, int B_S_start, int tile_length) {
    int i = min(k, m);
    int j = k - i;
    int i_low = max(0, k-n);
    int j_low = max(0, k-m);
    int delta;
    bool active = true;
    while (active) {
        int i_cir = (A_S_start + i) % tile_length;
        int j_cir = (B_S_start + j) % tile_length;
        int i_m_1_cir = (A_S_start + i - 1) % tile_length;
        int j_m_1_cir = (B_S_start + j - 1) % tile_length;
        if (i &amp;gt; 0 &amp;amp;&amp;amp; j &amp;lt; n &amp;amp;&amp;amp; A[i_m_1_cir] &amp;gt; B[j_cir]) {
            delta = ((i - i_low + 1) &amp;gt;&amp;gt; 1);
            j_low = j;
            i -= delta;
            j += delta;
        } else if (j &amp;gt; 0 &amp;amp;&amp;amp; i &amp;lt; m &amp;amp;&amp;amp; B[j_m_1_cir] &amp;gt;= A[i_cir]) {
            delta = ((j - j_low + 1) &amp;gt;&amp;gt; 1);
            i_low = i;
            j -= delta;
            i += delta;
        } else {
            active = false;
        }
    }
    return i;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In this updated version of the co-rank function, the user only needs to provide the start indices for the shared memory arrays along with the tile size.&lt;/p&gt;
&lt;h4 id=&#34;merge-sequential&#34;&gt;Merge Sequential&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;void merge_sequential_circular(int *A, int m, int *B, int n, int *C, int A_S_start, int B_S_start, int tile_size) {
    int i = 0;
    int j = 0;
    int k = 0;
    while (i &amp;lt; m &amp;amp;&amp;amp; j &amp;lt; n) {
        int i_cir = (A_S_start + i) % tile_size;
        int j_cir = (B_S_start + j) % tile_size;
        if (A[i_cir] &amp;lt;= B[j_cir]) {
            C[k] = A[i_cir];
            i++;
        } else {
            C[k] = B[j_cir];
            j++;
        }
        k++;
    }
    if (i == m) {
        while (j &amp;lt; n) {
            int j_cir = (B_S_start + j) % tile_size;
            C[k] = B[j_cir];
            j++;
            k++;
        }
    } else {
        while (i &amp;lt; m) {
            int i_cir = (A_S_start + i) % tile_size;
            C[k] = A[i_cir];
            i++;
            k++;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Again, this revision makes it easier on the user since they only need to provide the start indices for the shared memory arrays and the tile size. These are used to compute the indices for the circular buffer.&lt;/p&gt;
&lt;h4 id=&#34;circular-buffer-kernel&#34;&gt;Circular Buffer Kernel&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;        int c_curr = threadIdx.x * (tile_size / blockDim.x);
        int c_next = (threadIdx.x + 1) * (tile_size / blockDim.x);
        c_curr = (c_curr &amp;lt;= C_length - C_completed) ? c_curr : C_length - C_completed;
        c_next = (c_next &amp;lt;= C_length - C_completed) ? c_next : C_length - C_completed;

        int a_curr = co_rank_circular(c_curr,
                                      A_s, min(tile_size, A_length - A_consumed),
                                      B_s, min(tile_size, B_length - B_consumed),
                                      A_curr, B_curr, tile_size);
        int b_curr = c_curr - a_curr;
        int a_next = co_rank_circular(c_curr,
                                      A_s, min(tile_size, A_length - A_consumed),
                                      B_s, min(tile_size, B_length - B_consumed),
                                      A_curr, B_curr, tile_size);
        int b_next = c_next - a_next;

        merge_sequential_circular(A_s, a_next - a_curr, B_s, b_next - b_curr, &amp;amp;C[C_urr + C_completed + c_curr],
                                  A_S_start + A_curr, B_S_start + B_curr, tile_size);

        // Compute the indices that were used
        counter++;
        A_S_consumed = co_rank_circular(min(tile_size, C_length - C_completed),
                                        A_s, min(tile_size, A_length - A_consumed),
                                        B_s, min(tile_size, B_length - B_consumed),
                                        A_S_start, B_S_start, tile_size);
        B_S_consumed = min(tile_size, C_length - C_completed) - A_S_consumed;
        A_consumed += A_S_consumed;
        C_completed += min(tile_size, C_length - C_completed);
        B_consumed = C_completed - A_consumed;

        // Update the start indices for the next iteration
        A_S_start = (A_S_start + A_S_consumed) % tile_size;
        B_S_start = (B_S_start + B_S_consumed) % tile_size;
        __syncthreads();
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The first section of part 3 of the original kernel remains mostly unchanged, with the exceptions that the co-rank function and merge function are now called with the circular versions. The larger change is in the second half of the kernel. &lt;code&gt;A_S_consumed&lt;/code&gt; and &lt;code&gt;B_S_consumed&lt;/code&gt; are used to keep track of how much of the shared memory arrays were used. This is then used to offset the used indices from the original arrays. Finally, the start indices for the shared memory arrays are updated for the next iteration.&lt;/p&gt;
&lt;h2 id=&#34;thread-coarsening&#34;&gt;Thread Coarsening&lt;/h2&gt;
&lt;p&gt;The kernels presented in these notes already utilize thread coarsening. Each thread is responsible for a range of output values. The simple example presented earlier demonstrates what a non-coarse approach would look like. Each thread was only responsible for a single output value.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Siebert, Christian, and Jesper Larsson Träff. 2013. “Perfectly Load-Balanced, Optimal, Stable, Parallel Merge.” arXiv. &lt;a href=&#34;http://arxiv.org/abs/1303.4312&#34;&gt;http://arxiv.org/abs/1303.4312&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Substitution Method</title>
      <link>https://ajdillhoff.github.io/notes/substitution_method/</link>
      <pubDate>Tue, 27 Feb 2024 19:12:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/substitution_method/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#example-from-clrs&#34;&gt;Example from CLRS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#making-the-wrong-guess&#34;&gt;Making the Wrong Guess&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;The &lt;strong&gt;substitution method&lt;/strong&gt; is a technique for solving recurrences. It works in two steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Guess the solution&lt;/li&gt;
&lt;li&gt;Use mathematical induction to verify the guess&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This can work very well, especially if we already have some intuition about the problem. Let&amp;rsquo;s start with a simple example: The Tower of Hanoi. In this classic puzzle, we have three pegs and a number of disks of different sizes which can slide onto any peg. The puzzle starts with the disks in a neat stack in ascending order of size on one peg, with the smallest disk on top. The objective is to move the entire stack to another peg, obeying the following rules:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Only one disk can be moved at a time&lt;/li&gt;
&lt;li&gt;Each move consists of taking the top disk from one of the stacks and placing it on top of another stack&lt;/li&gt;
&lt;li&gt;No disk may be placed on top of a smaller disk&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;An algorithm to solve the puzzle goes like this:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Move \(n-1\) disks from peg 1 to peg 2 using peg 3 as a temporary holding area&lt;/li&gt;
&lt;li&gt;Move the $n$th disk from peg 1 to peg 3&lt;/li&gt;
&lt;li&gt;Move the \(n-1\) disks from peg 2 to peg 3 using peg 1 as a temporary holding area&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The number of moves required to solve the Tower of Hanoi puzzle is given by the recurrence relation \(T(n) = 2T(n-1) + 1\) with the initial condition \(T(1) = 1\). We can solve this recurrence using the substitution method.&lt;/p&gt;
&lt;p&gt;Our hypothesis might be that \(T(n) \leq c2^n - 1\) for all \(n \geq n_0\), where \(c &amp;gt; 0\) and \(n_0 &amp;gt; 0\). For the base case, we have \(T(1) = c * 2^1 - 1 = c\), so \(c = 1\). Now we need to show that \(T(n) \leq c2^n - 1\) for all \(n \geq n_0\). Then we have:&lt;/p&gt;
&lt;p&gt;\begin{align*}
T(n) &amp;amp;\leq 2T(n-1) + 1 \\
&amp;amp;\leq 2\left(2^{n-1} - 1\right) + 1 \\
&amp;amp;= 2^n - 2 + 1 \\
&amp;amp;= 2^n - 1 \\
\end{align*}&lt;/p&gt;
&lt;p&gt;What if we made a bad guess? Let&amp;rsquo;s try \(T(n) \leq cn\) for all \(n \geq n_0\). We have \(T(1) = c = 1\), so \(c = 1\). Now we need to show that \(T(n) \leq cn\) for all \(n \geq n_0\). We assume that \(T(k) \leq ck\) for all \(k &amp;lt; n\). Then we have:&lt;/p&gt;
&lt;p&gt;\begin{align*}
T(n) &amp;amp;\leq 2T(n-1) + 1 \\
&amp;amp;\leq 2c(n-1) + 1 \\
&amp;amp;= 2cn - 2c + 1 \\
\end{align*}&lt;/p&gt;
&lt;p&gt;This does not work because \(2cn - 2c + 1 &amp;gt; cn\) for all \(c &amp;gt; 1\). Therefore, our guess was wrong.&lt;/p&gt;
&lt;h2 id=&#34;example-from-clrs&#34;&gt;Example from CLRS&lt;/h2&gt;
&lt;p&gt;Determine an asymptotic upper bound for&lt;/p&gt;
&lt;p&gt;\[
T(n) = 2T(\lfloor n/2 \rfloor) + \Theta(n).
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Guess:&lt;/strong&gt; \(T(n) = O(n \lg n)\)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inductive hypothesis:&lt;/strong&gt; \(T(n) \leq cn \lg n\) for all \(n \geq n_0\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inductive step:&lt;/strong&gt; Assume \(T(n) \leq cn \lg n\) for all \(n_0 \leq k &amp;lt; n\). For \(T(\lfloor n/2 \rfloor) \leq c\lfloor n/2 \rfloor \lg \lfloor n/2 \rfloor\), it holds when \(n \geq 2\).&lt;/p&gt;
&lt;p&gt;\begin{align*}
T(n) &amp;amp;\leq 2T(\lfloor n/2 \rfloor) + \Theta(n) \\
&amp;amp;\leq 2c\lfloor n/2 \rfloor \lg \lfloor n/2 \rfloor + \Theta(n) \\
&amp;amp;= cn \lg (n / 2) + \Theta(n) \\
&amp;amp;= cn \lg n - cn\lg 2 + \Theta(n) \\
&amp;amp;= cn \lg n - cn + \Theta(n) \\
&amp;amp;\leq cn \lg n
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;making-the-wrong-guess&#34;&gt;Making the Wrong Guess&lt;/h2&gt;
&lt;p&gt;What if we took the same recurrence and guessed that \(T(n) = O(n)\)?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Guess:&lt;/strong&gt; \(T(n) = O(n)\)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inductive hypothesis:&lt;/strong&gt; \(T(n) \leq cn\) for all \(n \geq n_0\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inductive step:&lt;/strong&gt; Assume \(T(n) \leq cn\) for all \(n \geq n_0\).&lt;/p&gt;
&lt;p&gt;\begin{align*}
T(n) &amp;amp;\leq 2c\lfloor n/2 \rfloor + \Theta(n) \\
&amp;amp;\leq cn + \Theta(n) \\
\end{align*}&lt;/p&gt;
&lt;p&gt;This does not work because \(cn + \Theta(n) &amp;gt; cn\). Therefore, our guess was wrong.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quicksort</title>
      <link>https://ajdillhoff.github.io/notes/quicksort/</link>
      <pubDate>Sun, 25 Feb 2024 17:24:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/quicksort/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#basic-quicksort&#34;&gt;Basic Quicksort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#performance&#34;&gt;Performance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#randomized-quicksort&#34;&gt;Randomized Quicksort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#paranoid-quicksort&#34;&gt;Paranoid Quicksort&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Quicksort is a popular sorting algorithm implemented in many language libraries that has a worst-case running time of \(\Theta(n^2)\). &lt;strong&gt;Why would anyone choose this as the default sorting algorithm if one like mergesort has better worst-case performance?&lt;/strong&gt; As you will see, the devil is in the details. Quicksort is often faster in practice. It also has a small memory footprint and is easy to implement.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TODO: Discuss distinct values and the impact on quicksort&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;basic-quicksort&#34;&gt;Basic Quicksort&lt;/h2&gt;
&lt;p&gt;Quicksort follows a divide-and-conquer approach to sorting. Given an input array of \(n\) elements, it selects a pivot element and partitions the array into two sub-arrays: one with elements less than the pivot and one with elements greater than the pivot. It then recursively sorts the sub-arrays. Since the subarrays are recursively sorted, there is no need for a merge step as in mergesort.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;quicksort&lt;/span&gt;(arr, p, r):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; partition(arr, p, r)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    quicksort(arr, p, q &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    quicksort(arr, q &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, r)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This algorithm looks deceptively simple. The complexity is hidden in the partitioning step. This procedure will rearrange the elements in the array such that the pivot element is in its final position and all elements less than the pivot are to the left of it and all elements greater than the pivot are to the right of it.&lt;/p&gt;
&lt;h3 id=&#34;partitioning&#34;&gt;Partitioning&lt;/h3&gt;
&lt;p&gt;For basic quicksort, the first or last element is chosen as the pivot. Picking it this way yields a fairly obvious recurrence of \(T(n) = T(n-1) + O(n)\), which is \(\Theta(n^2)\). As mentioned before, this algorithm is executed in-place, meaning there is no need for additional memory to store the sub-arrays. It is done through a clever use of indices.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;partition&lt;/span&gt;(arr, p, r):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; arr[r]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(p, r):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; arr[j] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; x:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            i &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            arr[i], arr[j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; arr[j], arr[i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    arr[i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], arr[r] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; arr[r], arr[i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The indices are used to define the following loop invariant.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Left:&lt;/strong&gt; if \(p \leq k \leq i\), then \(A[k] \leq x\)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Middle:&lt;/strong&gt; if \(i + 1 \leq k \leq j - 1\), then \(A[k] &amp;gt; x\)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Right:&lt;/strong&gt; if \(k = r\), then \(A[k] = x\)&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-25_19-18-19_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Operation of partition (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Operation of partition (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The figure above is from CRLS which shows an example run of the partitioning algorithm. It starts with \(i, p, j\) on the left and \(r\) on the right. Since the value at \(j\) is less than the pivot, \(i\) is incremented and the values at \(i\) and \(j\) are swapped. In the next iteration, the value at \(j\) is greater than the pivot, so nothing is done. This continues until \(j\) reaches \(r\). At this point, the pivot is swapped with the value at \(i + 1\).&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-26_16-34-22_Quicksort%20Example.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Example of Quicksort.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Example of Quicksort.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The example above starts with an unsorted array. The second row shows the array after the first call to &lt;code&gt;partition&lt;/code&gt;. The left and right subarrays are called recursively. The first subarray in row 3 has a pivot 3 and is already partitioned. The right subarray follows the same convenience.&lt;/p&gt;
&lt;p&gt;On row 4, the left subarray is modified by swapping the pivot of 2 with 1. The right subarray first swaps 5 and 7 before swapping 6 and 7. The final array is sorted as shown on the last row.&lt;/p&gt;
&lt;h2 id=&#34;performance&#34;&gt;Performance&lt;/h2&gt;
&lt;p&gt;The performance of quicksort is dependent on the pivot. If the subarrays on either side of the pivot are balanced, the running time is asymptotically similar to mergesort. In the worst case, it will run in quadratic time.&lt;/p&gt;
&lt;p&gt;The worst partitioning occurs when the pivot is the smallest or largest element in the array. This creates a subarray of size \(n - 1\) and another of size 0. The partitioning itself takes \(\Theta(n)\) time, yielding a recurrence of \(T(n) = T(n - 1) + \Theta(n)\). We can use the substitution method to solve this recurrence and find that the worst-case running time is \(\Theta(n^2)\). This can happen even if the input is already sorted before hand.&lt;/p&gt;
&lt;p&gt;Cormen et al. present a recursive analysis of the running time where, at each level, the partition produces a 9-to-1 split. This is visualized in the sketch below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-26_18-20-51_Quicksort%20Recursion%2001%20Artboard%201%20%282%29.jpg&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Recursion tree for quicksort.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Recursion tree for quicksort.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The subtree for the \(\frac{1}{10}\) split eventually bottoms out after being called \(\log_{10} n\) times. Until this happens, the cost of each level of the tree is \(n\). After the left-tree bottoms out, the right tree continues with an upper bound of \(\leq n\). The right tree completes after \(\log_{10/9} n = \Theta(\lg n)\) levels. Since each level of the tree cost no more than \(n\), the total cost is \(\Theta(n \lg n)\).&lt;/p&gt;
&lt;h3 id=&#34;best-case&#34;&gt;Best Case&lt;/h3&gt;
&lt;p&gt;In the best-case, the pivot is the median of the array and two balanced subarrays are created: one of size \(n/2\) and another of size \(\lfloor (n-1)/2 \rfloor\). The recurrence is \(T(n) = 2T(n/2) + \Theta(n)\), which is \(\Theta(n \log n)\).&lt;/p&gt;
&lt;p&gt;Using the substitution method, we can show the best-case running time. We start with the fact that the partitioning produces two subproblems with a total size of \(n-1\). This gives the following recurrence:&lt;/p&gt;
&lt;p&gt;\[
T(n) = \min_{0 \leq q \leq n-1} \{T(q) + T(n - q - 1)\} + \Theta(n).
\]&lt;/p&gt;
&lt;p&gt;The minimum function accounts for the minimum time taken to sort any partition of the array, where \(q\) represents the pivot element&amp;rsquo;s position.&lt;/p&gt;
&lt;p&gt;Our &lt;strong&gt;hypothesis&lt;/strong&gt; will be that&lt;/p&gt;
&lt;p&gt;\[
T(n) \geq cn \lg n = \Omega(n \lg n).
\]&lt;/p&gt;
&lt;p&gt;Plugging in our hypothesis, we get&lt;/p&gt;
&lt;p&gt;\begin{align*}
T(n) &amp;amp;\geq \min_{0 \leq q \leq n-1} \{cq \lg q + c(n - q - 1) \lg (n - q - 1)\} + \Theta(n) \\
&amp;amp; c \min_{0 \leq q \leq n-1} \{q \lg q + (n - q - 1) \lg (n - q - 1)\} + \Theta(n).
\end{align*}&lt;/p&gt;
&lt;p&gt;If we take the derivative of the function inside the minimum with respect to \(q\), we get&lt;/p&gt;
&lt;p&gt;\[
\frac{d}{dq} \{q \lg q + (n - q - 1) \lg (n - q - 1)\} = c\{\frac{q}{q} + \lg q - \lg (n - q - 1) - \frac{(n - q - 1)}{(n - q - 1)}\}.
\]&lt;/p&gt;
&lt;p&gt;Setting this equal to zero and solving for \(q\) yields&lt;/p&gt;
&lt;p&gt;\[
q = \frac{n - 1}{2}.
\]&lt;/p&gt;
&lt;p&gt;We can then plug this value of \(q\) into the original function to get&lt;/p&gt;
&lt;p&gt;\begin{align*}
T(n) &amp;amp;\geq c \frac{n - 1}{2} \lg \frac{n - 1}{2} + c \frac{n - 1}{2} \lg \frac{n - 1}{2} + \Theta(n) \\
&amp;amp;= cn \lg (n - 1) + c (n - 1) + \Theta(n) \\
&amp;amp;= cn \lg (n - 1) + \Theta(n) \\
&amp;amp;\geq cn \lg n \\
&amp;amp;= \Omega(n \lg n).
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;average-case&#34;&gt;Average Case&lt;/h3&gt;
&lt;p&gt;As it turns out, the average-case running time is \(\Theta(n \log n)\) (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;). Quicksort is highly dependent on the relative ordering of the input. Consider the case of a randomly ordered array. The cost of partitioning the original input is \(O(n)\). Let&amp;rsquo;s say that the pivot was the last element, yielding a split of 0 and \(n - 1\). Now, let&amp;rsquo;s say we get lucky on the next iteration and get a balanced split. Even if the rest of the algorithm splits between the median and the last element, the upper bound on the running time is \(\Theta(n \log n)\). It is highly unlikely that the split will be unbalanced on every iteration given a random initial ordering.&lt;/p&gt;
&lt;p&gt;We can make this a tad more formal by defining a lucky \(L(n) = 2U(n/2) + \Theta(n)\) and an unlucky split \(U(n) = L(n-1) + \Theta(n)\). We can solve for \(L(n)\) by plugging in the definition of \(U(n)\).&lt;/p&gt;
&lt;p&gt;\begin{align*}
L(n) &amp;amp;= 2U(n/2) + \Theta(n) \\
&amp;amp;= 2(L(n/2 - 1) + \Theta(n/2)) + \Theta(n) \\
&amp;amp;= 2L(n/2 - 1) + \Theta(n) \\
&amp;amp;= \Theta(n \log n)
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;randomized-quicksort&#34;&gt;Randomized Quicksort&lt;/h2&gt;
&lt;p&gt;The intuition of the crude analysis above is that we would have to be extremely unlucky to get a quadratic running time if the input is randomly ordered. Randomized quicksort builds on this intuition by selection a random pivot on each iteration. This is done by swapping the pivot with a random element before partitioning.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; random
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;randomized_partition&lt;/span&gt;(arr, p, r):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(p, r)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    arr[i], arr[r] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; arr[r], arr[i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; partition(arr, p, r)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;randomized_quicksort&lt;/span&gt;(arr, p, r):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; r:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; randomized_partition(arr, p, r)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        randomized_quicksort(arr, p, q &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        randomized_quicksort(arr, q &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, r)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;The lightweight analysis above reasoned that, as long as each split puts a constant amount of elements to one side of the split, then the running time is \(\Theta(n \log n)\).&lt;/p&gt;
&lt;p&gt;We can understand this analysis simply by asking the right questions. First, our primary question: &lt;strong&gt;What is the running time of Quicksort dependent on?&lt;/strong&gt; The biggest bottleneck is the partitioning function. At most, we get really unlucky and the first pivot is picked every time. This means it is called \(n\) times yielding \(O(n)\). The variable part of this is figuring out how many element comparisons are made. The running time is then \(O(n + X)\).&lt;/p&gt;
&lt;h4 id=&#34;the-expected-value-of-x&#34;&gt;The Expected Value of \(X\)&lt;/h4&gt;
&lt;p&gt;The number of comparisons can be expressed as&lt;/p&gt;
&lt;p&gt;\[
X = \sum_{i=1}^{n-1} \sum_{j=i+1}^{n} X_{ij},
\]&lt;/p&gt;
&lt;p&gt;where \(X_{ij}\) is the indicator random variable that is 1 if \(A[i]\) and \(A[j]\) are compared and 0 otherwise. This works with our worst case analysis. If we always get a split of 0 and \(n - 1\), then the indicator random variable is 1 for every comparison, yielding \(O(n^2)\). Taking the expectation of both sides:&lt;/p&gt;
&lt;p&gt;\begin{align*}
E[X] &amp;amp;= E\left[\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} X_{ij}\right] \\
&amp;amp;= \sum_{i=1}^{n-1} \sum_{j=i+1}^{n} E[X_{ij}] \\
&amp;amp;= \sum_{i=1}^{n-1} \sum_{j=i+1}^{n} P(X_{ij} = 1).
\end{align*}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What is P(X&lt;sub&gt;ij&lt;/sub&gt; = 1)?&lt;/strong&gt; Let \(z_i, \dots, z_j\) be the indices of elements in a sorted version of the array. Under this assumption, \(z_i\) is compared to \(z_j\) only if \(z_i\) or \(z_j\) is the first pivot chosen from the subarray \(A[i \dots j]\). In a set of distinct elements, the probability of picking any pivot from the array from \(i\) to \(j\) is \(\frac{1}{j - i + 1}\). This means that the probability of comparing \(z_i\) and \(z_j\) is \(\frac{2}{j - i + 1}\). We can now finish the calculation.&lt;/p&gt;
&lt;p&gt;\begin{align*}
E[X] &amp;amp;= \sum_{i=1}^{n-1} \sum_{j=i+1}^{n} \frac{2}{j - i + 1} \\
&amp;amp;= \sum_{i=1}^{n-1} \sum_{k=1}^{n-i} \frac{2}{k + 1} \\
&amp;amp;&amp;lt; \sum_{i=1}^{n-1} \sum_{k=1}^{n-i} \frac{2}{k} \\
&amp;amp;= \sum_{i=1}^{n-1} O(\log n) \\
&amp;amp;= O(n \log n).
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;paranoid-quicksort&#34;&gt;Paranoid Quicksort&lt;/h2&gt;
&lt;p&gt;Repeat the following until the partitioning until the left or right subarray is less than or equal to \(\frac{3}{4}\) of the original array.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Choose a random pivot.&lt;/li&gt;
&lt;li&gt;Partition the array.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Priority Queues</title>
      <link>https://ajdillhoff.github.io/notes/priority_queues/</link>
      <pubDate>Sat, 24 Feb 2024 14:10:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/priority_queues/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#quick-facts&#34;&gt;Quick Facts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementation&#34;&gt;Implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exercises&#34;&gt;Exercises&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;quick-facts&#34;&gt;Quick Facts&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Time Complexity&lt;/strong&gt;&lt;/strong&gt;: \(O(\lg n)\)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Space Complexity&lt;/strong&gt;&lt;/strong&gt;: \(O(n)\)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Besides being the primary data structure for &lt;a href=&#34;https://ajdillhoff.github.io/notes/heapsort/&#34;&gt;Heapsort&lt;/a&gt;, a heap is also used to implement a priority queue. A priority queue is a key-value data structure in which the keys are used to determine the priority of each element in the queue. There are two variants, maximum and minimum, and they support the following operations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Insert: Add a new element to the queue.&lt;/li&gt;
&lt;li&gt;Extract: Remove the element with the maximum/minimum key.&lt;/li&gt;
&lt;li&gt;Maximum/Minimum: Return the element with the maximum/minimum key without removing it.&lt;/li&gt;
&lt;li&gt;Increase/Decrease Key: Increase or decrease the key of a given element.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You could probably imagine a few use cases for such a queue. For example, a priority queue could be used to schedule tasks in a multitasking operating system. The tasks with the highest priority would be executed first. Another example would be a network router that uses a priority queue to schedule packets for transmission. The packets with the highest priority would be transmitted first. In high performance computing, a priority queue could be used to schedule jobs on a supercomputer. The jobs with the highest priority would be executed first. SLURM is an example of a job scheduler that uses a priority queue.&lt;/p&gt;
&lt;p&gt;For simple applications, you could reference your application object directly inside the heap. If the objects themselves are too complex, it is optimal to simply set the &lt;em&gt;value&lt;/em&gt; of the heap as a reference to the object. A &lt;strong&gt;handle&lt;/strong&gt; is a reference that is added to both the heap and the object; it requires little overhead. This requires that your priority queue update both its own index as well as the object&amp;rsquo;s index as changes are made.&lt;/p&gt;
&lt;p&gt;An alternative approach is to establish the map using a hash table. In this case, the priority queue is the only data structure that needs to be updated.&lt;/p&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;Let us now consider the implementation and analysis of the require operations for a priority queue.&lt;/p&gt;
&lt;h3 id=&#34;extract&#34;&gt;Extract&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;keys&lt;/em&gt; in a priority queue represent the priority. The &lt;em&gt;values&lt;/em&gt; will need to be moved around with them as the priority queue is constructed. Getting the maximum or minimum value is a constant time operation and is executed by returning the first element in the array. To extract the item with the highest priority, the first element is removed and the heap is then heapified.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;max_heap_maximum&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; len(A) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ValueError&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Heap underflow&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; A[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;max_heap_extract_max&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    max_val &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max_heap_maximum(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pop()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    max_heapify(A, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; max_val
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As we saw with &lt;a href=&#34;https://ajdillhoff.github.io/notes/heapsort/&#34;&gt;Heapsort&lt;/a&gt;, &lt;code&gt;max_heapify&lt;/code&gt; runs in \(O(\lg n)\) time. The call to &lt;code&gt;max_heap_extract_max&lt;/code&gt; only adds a few constant operations on top of that, so it runs in \(O(\lg n)\) time as well.&lt;/p&gt;
&lt;h3 id=&#34;increase&#34;&gt;Increase&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;max_heap_increase_key&lt;/code&gt; function is used to increase the key of a given element. The function first checks if the new key is less than the current key. If it is, an error is raised. The function then updates the key and then traverses up the heap to ensure that the heap property is maintained.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;max_heap_increase_key&lt;/span&gt;(A, obj, key):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; key &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; obj&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ValueError&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;New key is smaller than current key&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    obj&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;index(obj) &lt;span style=&#34;color:#75715e&#34;&gt;# gets the index of the object&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; A[parent(i)]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; A[i]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A[i], A[parent(i)] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[parent(i)], A[i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; parent(i)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Moving through the height of the tree is done in \(O(\lg n)\) time. Depending on the how the index of the object is found, the complexity could be higher. In most cases, the index is found in constant time.&lt;/p&gt;
&lt;h3 id=&#34;insert&#34;&gt;Insert&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;max_heap_insert&lt;/code&gt; function is used to insert a new element into the heap. The function first appends the new element to the end of the array. It then sets the key of the new element to a very small value and then calls &lt;code&gt;max_heap_increase_key&lt;/code&gt; to update the key to the correct value.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;max_heap_insert&lt;/span&gt;(A, obj, n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; len(A) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; n:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ValueError&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Heap overflow&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-inf&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    obj&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(obj)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# map obj to the last index -- dependent on the implementation&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    max_heap_increase_key(A, obj, key)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The call to &lt;code&gt;max_heap_increase_key&lt;/code&gt; runs in \(O(\lg n)\) time, so the &lt;code&gt;max_heap_insert&lt;/code&gt; function also runs in \(O(\lg n)\) time in addition to the time it takes to map the object to its index.&lt;/p&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Implement a minimum priority queue.&lt;/li&gt;
&lt;li&gt;Implement a decrease key function for a maximum priority queue.&lt;/li&gt;
&lt;li&gt;Simulate a job scheduler using a priority queue that considers the priority of the job and the time it was submitted.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Heapsort</title>
      <link>https://ajdillhoff.github.io/notes/heapsort/</link>
      <pubDate>Wed, 21 Feb 2024 14:58:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/heapsort/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#maintaining-the-heap-property&#34;&gt;Maintaining the Heap Property&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#building-the-heap&#34;&gt;Building the Heap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#heapsort&#34;&gt;Heapsort&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;ul&gt;
&lt;li&gt;Running time is \(O(n \lg n)\).&lt;/li&gt;
&lt;li&gt;Sorts in place, only a constant number of elements needed in addition to the input.&lt;/li&gt;
&lt;li&gt;Manages data with a &lt;strong&gt;heap&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A &lt;strong&gt;binary heap&lt;/strong&gt; can be represented as a binary tree, but is stored as an array. The root is the first element of the array. The left subnode for the element at index \(i\) is located at \(2i\) and the right subnode is located at \(2i + 1\). &lt;strong&gt;This assumes a 1-based indexing&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Using 0-based indexing, we can use \(2i + 1\) for the left and \(2i + 2\) for the right. The parent could be accessed via \(\lfloor \frac{i-1}{2} \rfloor\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-21_15-22-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;A binary tree as a heap with its array representation (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;A binary tree as a heap with its array representation (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Heaps come in two flavors: &lt;strong&gt;max-heaps&lt;/strong&gt; and &lt;strong&gt;min-heaps&lt;/strong&gt;. They can be identified by satisfying a &lt;strong&gt;heap property&lt;/strong&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;max-heap property&lt;/strong&gt;: \(A[parent(i)] \geq A[i]\)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;min-heap property&lt;/strong&gt;: \(A[parent(i)] \leq A[i]\)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These properties imply that the root is the largest element in a max-heap and the smallest element in a min-heap.&lt;/p&gt;
&lt;p&gt;When it comes to heapsort, a max-heap is used. Min-heaps are used in priority queues. These notes will cover both.&lt;/p&gt;
&lt;h2 id=&#34;maintaining-the-heap-property&#34;&gt;Maintaining the Heap Property&lt;/h2&gt;
&lt;p&gt;When using heapsort, the heap should always satisfy the &lt;strong&gt;max-heap&lt;/strong&gt; property. This relies on a procedure called &lt;code&gt;max_heapify&lt;/code&gt;. This function assumes that the root element may violate the max-heap property, but the subtrees rooted by its subnodes are valid max-heaps. The function then swaps nodes down the tree until the misplaced element is in the correct position.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;max_heapify&lt;/span&gt;(A, i, heap_size):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    l &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; left(i)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    r &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; right(i)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    largest &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; l &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; heap_size &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; A[l] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; A[i]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        largest &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; l
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; r &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; heap_size &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; A[r] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; A[largest]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        largest &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; r
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; largest &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; i:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A[i], A[largest] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[largest], A[i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        max_heapify(A, largest, heap_size)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;analysis-of-max-heapify&#34;&gt;Analysis of Max Heapify&lt;/h3&gt;
&lt;p&gt;Given that &lt;code&gt;max_heapify&lt;/code&gt; is a recursive function, we can analyze it with a recurrence. The driving function in this case would be the fix up that happens between the current node and its two subnodes, which is a constant time operation. The recurrence is based on how many elements are in the subheap rooted at the current node.&lt;/p&gt;
&lt;p&gt;In the worst case of a binary tree, the last level of the tree is half full. That means that the left subtree has height \(h + 1\) compared to the right subtree&amp;rsquo;s height of \(h\). For a tree of size \(n\), the left subtree has \(2^{h+2}-1\) nodes and the right subtree has \(2^{h+1}-1\) nodes. This is based on a geometric series.&lt;/p&gt;
&lt;p&gt;We now have that the number of nodes in the tree is equal to \(1 + (2^{h+2}-1) + (2^{h+1}-1)\).&lt;/p&gt;
&lt;p&gt;\begin{align*}
n &amp;amp;= 1 + 2^{h+2} - 1 + 2^{h+1} - 1 \\
n &amp;amp;= 2^{h+2} + 2^{h+1} - 1 \\
n &amp;amp;= 2^{h+1}(2 + 1) - 1 \\
n &amp;amp;= 3 \cdot 2^{h+1} - 1
\end{align*}&lt;/p&gt;
&lt;p&gt;This implies that \(2^{h+1} = \frac{n+1}{3}\). That means that, in the worst case, the left subtree would have \(2^{h+2} - 1 = \frac{2(n+1)}{3} - 1\) nodes which is bounded by \(\frac{2n}{3}\). Thus, the recurrence for the worst case of &lt;code&gt;max_heapify&lt;/code&gt; is \(T(n) = T(\frac{2n}{3}) + O(1)\).&lt;/p&gt;
&lt;h2 id=&#34;building-the-heap&#34;&gt;Building the Heap&lt;/h2&gt;
&lt;p&gt;Given an array of elements, &lt;strong&gt;how do we build the heap in the first place?&lt;/strong&gt; The solution is to build it using a bottom-up approach from the leaves. The elements from \(\lfloor \frac{n}{2} \rfloor + 1\) to \(n\) are all leaves. This means that they are all 1-element heaps. We can then run &lt;code&gt;max_heapify&lt;/code&gt; on the remaining elements to build the heap.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;build_max_heap&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    heap_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(len(A) &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        max_heapify(A, i, heap_size)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;why-does-this-work&#34;&gt;Why does this work?&lt;/h3&gt;
&lt;p&gt;Each node starting at \(\lfloor \frac{n}{2} \rfloor + 1\) is the root of a 1-element heap. The subnodes, which are to the right of node \(\lfloor \frac{n}{2} \rfloor\), are roots of their own max-heaps. The procedure loops down to the first node until all sub-heaps have been max-heapified.&lt;/p&gt;
&lt;p&gt;The figure below is from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;) and shows the process of building a max-heap from an array.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-23_16-37-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Building a max-heap from an array (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Building a max-heap from an array (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;analysis-of-build-max-heap&#34;&gt;Analysis of Build Max Heap&lt;/h3&gt;
&lt;p&gt;A general analysis is fairly straightforward considering that the call to &lt;code&gt;max_heapify&lt;/code&gt; is \(O(\lg n)\). The loop in &lt;code&gt;build_max_heap&lt;/code&gt; runs \(O(n)\) times. This means that the overall running time is \(O(n \lg n)\). A more careful analysis can be done by considering the height of the tree and the number of nodes at each level.&lt;/p&gt;
&lt;p&gt;A heap of \(n\) elements has height \(\lfloor \lg n \rfloor\) Each call to &lt;code&gt;max_heapify&lt;/code&gt; can also be viewed in terms of the height of the tree \(h\), so the upper bound is \(O(h)\). This bounds &lt;code&gt;build_max_heap&lt;/code&gt; at \(\sum_{h=0}^{\lfloor \lg n \rfloor} \lceil \frac{n}{2^{h+1}} \rceil ch\). When \(h = 0\), the first term \(\lceil \frac{n}{2^{h+1}} \rceil = \lceil \frac{n}{2} \rceil\). When \(h = \lfloor \lg n \rfloor\), \(\lceil \frac{n}{2^{h+1}} \rceil = 1\). Thus, \(\lceil \frac{n}{2^{h+1}} \rceil \geq \frac{1}{2}\) for \(0 \leq h \leq \lfloor \lg n \rfloor\).&lt;/p&gt;
&lt;p&gt;Let \(x = \frac{n}{2^{h+1}}\). Since \(x \geq \frac{1}{2}\), we have that \(\lceil x \rceil \leq 2x\). This means that \(\lceil \frac{n}{2^{h+1}} \rceil \leq \frac{2n}{2^{h+1}} = \frac{n}{2^h}\). An upper bound can now be derived.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\sum_{h=0}^{\lfloor \lg n \rfloor} \lceil \frac{n}{2^{h+1}} \rceil ch &amp;amp;\leq \sum_{h=0}^{\lfloor \lg n \rfloor} \frac{n}{2^h} ch \\
&amp;amp;= cn \sum_{h=0}^{\lfloor \lg n \rfloor} \frac{h}{2^h} \\
&amp;amp;\leq cn \sum_{h=0}^{\infty} \frac{h}{2^h} \\
&amp;amp;\leq cn \cdot \frac{1 / 2}{(1 - 1/2)^2}\quad \text{(See CRLS for details)} \\
&amp;amp;= O(n)
\end{align*}&lt;/p&gt;
&lt;p&gt;Thus, a heap can be constructed in linear time. This is independent on whether the original data is already sorted.&lt;/p&gt;
&lt;h2 id=&#34;heapsort&#34;&gt;Heapsort&lt;/h2&gt;
&lt;p&gt;We now have all of the components necessary to implement heapsort. The algorithm is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;heapsort&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    build_max_heap(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    heap_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(len(A) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], A[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[i], A[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        heap_size &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        max_heapify(A, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, heap_size)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It starts by building a max-heap on the input array. As seen in the previous section, this is done in linear time. From there, it&amp;rsquo;s a matter of taking the root element out of the heap and then running &lt;code&gt;max_heapify&lt;/code&gt; to maintain the max-heap property. This is done \(n-1\) times, so the overall running time is \(O(n \lg n)\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-24_13-48-17_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Heapsort in action (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Heapsort in action (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Heapsort is visualized in the figure above, starting with a constructed max-heap in (a) (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;questions&#34;&gt;Questions&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;What is the running time of heapsort given an array that is already sorted in ascending order?&lt;/li&gt;
&lt;li&gt;What is the running time of heapsort given an array that is already sorted in descending order?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GPU Pattern: Parallel Scan</title>
      <link>https://ajdillhoff.github.io/notes/gpu_pattern_parallel_scan/</link>
      <pubDate>Wed, 14 Feb 2024 20:09:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/gpu_pattern_parallel_scan/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-it&#34;&gt;What is it?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#naive-parallel-reduction&#34;&gt;Naive Parallel Reduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kogge-stone-algorithm&#34;&gt;Kogge-Stone Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#brent-kung-algorithm&#34;&gt;Brent-Kung Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-coarsening&#34;&gt;Adding Coarsening&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#segmented-parallel-scan&#34;&gt;Segmented Parallel Scan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimizing-memory-efficiency&#34;&gt;Optimizing Memory Efficiency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-takeaway&#34;&gt;The Takeaway&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;what-is-it&#34;&gt;What is it?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Parallelizes sequential problems.&lt;/li&gt;
&lt;li&gt;Works with computations that can be described in terms of a recursion.&lt;/li&gt;
&lt;li&gt;Used as a primitive operation for sorting, tree operations, and recurrences.&lt;/li&gt;
&lt;li&gt;Studying this will also reveal how parallelization can increase the complexity beyond that of a traditional sequential approach.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example-inclusive-scan&#34;&gt;Example: Inclusive Scan&lt;/h3&gt;
&lt;p&gt;Given an array of numbers, the inclusive scan computes the sum of all elements up to a given index. For example, given the array [1, 2, 3, 4, 5], the inclusive scan would produce [1, 3, 6, 10, 15]. You could solve this recursively, but it would be horribly inefficient. A sequential solution is achievable with dynamic programming. However, a parallel solution is much more efficient.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sequential_scan&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x, &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;y, uint N) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (uint i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; N; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y[i &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; x[i];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;naive-parallel-reduction&#34;&gt;Naive Parallel Reduction&lt;/h2&gt;
&lt;p&gt;If we have \(n\) elements, we could have \(n\) threads each compute the sum of a single element. How many operations would that take? The first thread computes the sum of 1 element, or 0 operations. The second thread computes the sum of 2 elements, 1 operation, and so on. This can be described as a sum of the first \(n\) natural numbers, which is \(n(n + 1)/2\). This parallel solution is worse than the sequential solution, coming in at \(O(n^2)\).&lt;/p&gt;
&lt;h2 id=&#34;kogge-stone-algorithm&#34;&gt;Kogge-Stone Algorithm&lt;/h2&gt;
&lt;p&gt;The first solution to this problem relies on &lt;a href=&#34;https://ajdillhoff.github.io/notes/gpu_pattern_reduction/&#34;&gt;GPU Pattern: Reduction&lt;/a&gt; and is called the Kogge-Stone algorithm. The algorithm was published in 1973 by Peter M. Kogge and Harold S. Stone during their time at Stanford University.&lt;/p&gt;
&lt;h3 id=&#34;adapting-the-reduction-tree&#34;&gt;Adapting the Reduction Tree&lt;/h3&gt;
&lt;p&gt;Design reduction tree so that each thread has access to relevant inputs. The input matrix is modified to that input \(A_i\) contains the sum of up to \(2^k\) elements after \(k\) iterations. For example, after iteration 2, \(A_3\) contains the sum \(A_0 + A_1 + A_2 + A_3\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-19_18-08-45_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Visualization of parallel inclusive scan based on the Kogge-Stone algorithm (Source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Visualization of parallel inclusive scan based on the Kogge-Stone algorithm (Source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This is implemented in the following code:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void Kogge_Stone_scan_kernel(float *X, float *Y, unsigned int N) {
        __shared__ float A[SECTION_SIZE];
        unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;

        if (i &amp;lt; N) {
            A[threadIdx.x] = X[i];
        } else {
            A[threadIdx.x] = 0;
        }

        for (unsigned int stride = 1; stride &amp;lt; blockDim.x; stride *= 2) {
            __syncthreads();
            float temp;
            if (threadIdx.x &amp;gt;= stride) {
                temp = A[threadIdx.x] + A[threadIdx.x - stride];
            }
            __syncthreads();
            if (threadIdx.x &amp;gt;= stride) {
                A[threadIdx.x] = temp;
            }
        }

        if (i &amp;lt; N) {
            Y[i] = A[threadIdx.x];
        }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It is not possible to guarantee that a block can cover the entire input, so &lt;code&gt;SECTION_SIZE&lt;/code&gt; is used to ensure that the input is covered. This should be the same as the block size. Each thread starts off by loading its initial input into shared memory. Starting with thread 2, each thread computes a sum of the value assigned to its thread as well as the one before it by a factor of &lt;code&gt;stride&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The loop itself is moving &lt;em&gt;down&lt;/em&gt; the reduction tree, which is bounded logarithmically. The local variable &lt;code&gt;temp&lt;/code&gt; is used to store the intermediate result before barrier synchronization takes place. Otherwise there would be a possibility of a &lt;em&gt;write-after-read&lt;/em&gt; race condition.&lt;/p&gt;
&lt;h3 id=&#34;double-buffering&#34;&gt;Double Buffering&lt;/h3&gt;
&lt;p&gt;The temporary variable and second call to &lt;code&gt;__syncthreads()&lt;/code&gt; are necessary since a thread may read from a location that another thread is writing to. If the input and output arrays were represented by two different areas of shared memory, this call could be removed. This approach is called &lt;strong&gt;double-buffering&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;It works as follows: the input array is read from global memory into shared memory. At each iteration the data read from the first array is used to write new values to the second array. Since the values used in that iteration are only read from the first array, the second array can be used as the input array for the next iteration. This cycle continues back and forth until the final result is written to the output array.&lt;/p&gt;
&lt;h3 id=&#34;efficiency-analysis&#34;&gt;Efficiency Analysis&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Work efficiency&lt;/strong&gt; is a measure of how efficient a particular algorithm is compared to the minimum number of operations required. For inclusive scan, the minimum number of add operations required is \(n - 1\), yielding an efficiency of \(O(n)\). During each iteration of the Kogge-Stone algorithm, each thread iterates over a loop that is logarithmic in size. That is, it starts with a stride of 2, then 4, 8, \(\dots \frac{n}{2}\). This yields a complexity of \(O(n \log_2 n)\).&lt;/p&gt;
&lt;p&gt;Due to the parallel nature of the algorithm, it still requires fewer steps than the sequential algorithm. The details muddy the water a bit as threads that stop execution early still expend resources from the device. In general, we can say that the parallel algorithm takes \(\frac{1}{m} n \log_2 n\) steps, where \(m\) is the number of execution units. If we have as many execution units as we have elements, then we only need \(\log_2 n\) steps.&lt;/p&gt;
&lt;h2 id=&#34;brent-kung-algorithm&#34;&gt;Brent-Kung Algorithm&lt;/h2&gt;
&lt;p&gt;Sharing intermediate results
Distribute to different threads
Reduction tree
Sub-sums used to calculate some of the scan output values
Brent-Kung follows the same idea as Kogge-Stone, but with better work efficiency.&lt;/p&gt;
&lt;p&gt;Use reduction tree on the first \(n/2\) elements, then use the results to reverse the tree.&lt;/p&gt;
&lt;p&gt;At the start of the reverse direction, the elements with index \(2^i - 1\), for \(i = 1, 2, \dots, \log_2 n\), already have the correct value.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-18_18-31-36_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Visualization of parallel inclusive scan based on the Brent-Kung algorithm (Source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Visualization of parallel inclusive scan based on the Brent-Kung algorithm (Source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The figure above shows the state of the algorithm before the reverse direction begins. The second half is better seen as a table of values. The row labeled &lt;code&gt;Initial&lt;/code&gt; contains the state of the array after the first half of the algorithm is completed. For the values that already have their correct value, no update is needed. The two rows following &lt;code&gt;Initial&lt;/code&gt; show the state of the array after the first and second iterations of the reverse direction.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Initial&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&amp;hellip;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0&amp;hellip;3&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;4&amp;hellip;5&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;0&amp;hellip;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Iteration 1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;0&amp;hellip;5&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Iteration 2&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;0&amp;hellip;2&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;0&amp;hellip;4&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;0&amp;hellip;6&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;implementing-the-forward-half&#34;&gt;Implementing the Forward Half&lt;/h3&gt;
&lt;p&gt;The relevant reduction tree phase of Brent-Kung is implemented in CUDA C++ below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;for (uint stride = 1; stride &amp;lt;= blockDim.x; stride *= 2) {
    __syncthreads();
    if ((threadIdx.x + 1) % (stride * 2) == 0) {
        A[threadIdx.x] += A[threadIdx.x - stride];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;From the perspective of &lt;code&gt;threadIdx.x = 7&lt;/code&gt;, the first iteration would add the value from &lt;code&gt;threadIdx.x = 6&lt;/code&gt; to its own value. On the next iteration, the stride offset would add &lt;code&gt;threadIdx.x = 5&lt;/code&gt;. Thread 5 already has the sum from 4 and 5 before being added to 7, so 7 now has the sums from indices 4 through 7. On its last iteration, the value for stride is now 4, and 7 adds the value from 3 to its own value. This is the final result for the first half of the algorithm.&lt;/p&gt;
&lt;p&gt;There is a lot of control divergence present in this code. Since fewer threads stay active as the loop goes on, it is better to organize the threads such that they are contiguous. We can do that with slightly more complicated indexing, so that contiguous threads use data from the active portions of the array.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;for (uint stride = 1; stride &amp;lt;= blockDim.x; stride *= 2) {
    __syncthreads();
    int index = (threadIdx.x + 1) * 2 * stride - 1;
    if (index &amp;lt; blockDim.x) {
        A[index] += A[index - stride];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Thread 0 maps to index, thread 1 maps to index 3, thread 2 to index 5, and so on. Only when the number of active threads drops below the warp size does control divergence become a problem.&lt;/p&gt;
&lt;h3 id=&#34;implementing-the-reverse-half&#34;&gt;Implementing the Reverse Half&lt;/h3&gt;
&lt;p&gt;The reverse half of the algorithm is implemented in CUDA C++ below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;for (uint stride = blockDim.x / 4; stride &amp;gt; 0; stride /= 2) {
    __syncthreads();
    int index = (threadIdx.x + 1) * 2 * stride - 1;
    if (index + stride &amp;lt; blockDim.x) {
        A[index + stride] += A[index];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Continuing the example from above, the first thread &lt;code&gt;threadIdx.x = 0&lt;/code&gt; maps to index 3. This will load the value from index 3 and add it to the value at index 5. At that point, the value at index 5 will be the sum of the values from indices 0 through 5.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-19_22-19-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Full Brent-Kung visualization (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Full Brent-Kung visualization (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;efficiency-analysis&#34;&gt;Efficiency Analysis&lt;/h3&gt;
&lt;p&gt;The reduction tree of the first half of the algorithm require \(n - 1\) operations. For \(n\) elements, the reverse half requires \((2 - 1) + (4 - 1) + \dots + (n/2 - 1)\) operations for a total of \(N - 1 - \log_2 n\). This yields a work efficiency of \(O(n)\). Even though the theoretical work efficiency is better than Kogge-Stone, doesn&amp;rsquo;t mean its performance will always be better in practice. The drop off in active threads for Brent-Kung is much more severe than Kogge-Stone. It also requires additional steps to perform the reverse half. In general, Kogge-Stone is a better choice when we have more execution units, owing to its better parallelism.&lt;/p&gt;
&lt;p&gt;The full code is given below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void Brent_Kung_scan_kernel(float *X, float *Y, uint N) {
    __shared__ float A[SECTION_SIZE];
    unsigned int i = 2 * blockIdx.x * blockDim.x + threadIdx.x;

    if (i &amp;lt; N) {
        A[threadIdx.x] = X[i];
    }
    if (i + blockDim.x &amp;lt; N) {
        A[threadIdx.x + blockDim.x] = X[i + blockDim.x];
    }

    for (uint stride = 1; stride &amp;lt;= blockDim.x; stride *= 2) {
        __syncthreads();
        int index = (threadIdx.x + 1) * 2 * stride - 1;
        if (index &amp;lt; blockDim.x) {
            A[index] += A[index - stride];
        }
    }

    for (uint stride = blockDim.x / 4; stride &amp;gt; 0; stride /= 2) {
        __syncthreads();
        int index = (threadIdx.x + 1) * 2 * stride - 1;
        if (index + stride &amp;lt; blockDim.x) {
            A[index + stride] += A[index];
        }
    }
    __syncthreads();

    if (i &amp;lt; N) {
        Y[i] = A[threadIdx.x];
    }
    if (i + blockDim.x &amp;lt; N) {
        Y[i + blockDim.x] = A[threadIdx.x + blockDim.x];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;adding-coarsening&#34;&gt;Adding Coarsening&lt;/h2&gt;
&lt;p&gt;Similar to other problems such as tiled matrix multiplication, if the hardware does not meet the capacity ti parallelize the entire problem, the price for parallelization is wasted. In such cases, we can coarsen the problem so that the available resources are fully utilized. Each thread will execute a &lt;em&gt;phase&lt;/em&gt; of sequential scan, which is more work efficiency than other of the solutions presented above.&lt;/p&gt;
&lt;h3 id=&#34;phase-1&#34;&gt;Phase 1&lt;/h3&gt;
&lt;p&gt;Such a solution starts off by performing a sequential scan. The threads can also collaborate in the beginning to load data into shared memory.&lt;/p&gt;
&lt;h3 id=&#34;phase-2&#34;&gt;Phase 2&lt;/h3&gt;
&lt;p&gt;In the next phase, the threads execute a parallel scan via Kogge-Stone or Brent-Kung. Since each thread has already performed a sequential scan. This phase starts off with the last element assigned to each thread.&lt;/p&gt;
&lt;h3 id=&#34;phase-3&#34;&gt;Phase 3&lt;/h3&gt;
&lt;p&gt;In the last phase, each thread adds its last value to the first \(n-1\) elements of the next section, where \(n\) is the number of elements assigned to each thread.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-19_22-25-15_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Three-phase parallel scan (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Three-phase parallel scan (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;segmented-parallel-scan&#34;&gt;Segmented Parallel Scan&lt;/h2&gt;
&lt;p&gt;Input to scan operations may be too large to fit in memory.
Hierarchical scans can be used to solve this problem.
Partition input so that each section fits into shared memory for an SM.
Another kernel launches after this is done to consolidate the results by adding the sum of the preceding scan blocks to each element of a scan block.&lt;/p&gt;
&lt;p&gt;Each scan block is treated as an individual application of one of the previous kernels. Each successive scan block initially does not contain the sums of the preceding blocks. Those will be added in a separate kernel.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-19_22-31-17_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Segmented scan (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Segmented scan (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;After each scan block has computed the scan for its partition, the last elements are then processed in the next step. However, these elements are all from different blocks, which means we must write them to a global space so they can all be accessed. Completing this step yields an array that has the final values of the scan corresponding to the indices from the original scan blocks (see the figure above). These values can then be used to update the preceding elements in each scan block to complete the scan.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kernel 1: Section Scan&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first kernel essentially implements one of the previously discussed parallel scan kernels. The only difference is that the accumulation array is passed so that the blocks can write their output element values.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__syncthreads();
if (threadIdx.x == blockDim.x - 1) {
    accumulation[blockIdx.x] = A[threadIdx.x];
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Kernel 2: Update Scan&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For step 2, we need to run a parallel scan kernel like Kogge-Stone or Brent-Kung on the accumulation array.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kernel 3: Update Elements&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The final kernel takes the accumulated values and updates the original array so that all the elements have their correct scan value.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;uint i = blockIdx.x * blockDim.x + threadIdx.x;
output[i] += accumulation[blockIdx.x - 1];
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;optimizing-memory-efficiency&#34;&gt;Optimizing Memory Efficiency&lt;/h2&gt;
&lt;p&gt;The previous solution provides a way of working with large inputs, but it is not memory efficient because of the need to store the accumulation array. There have been several attempts at optimizing this flow of information in the form of a &lt;em&gt;stream-based&lt;/em&gt; scan. To understand the general strategy behind this approach, consider the segmented scan from the previous section.&lt;/p&gt;
&lt;p&gt;The process starts by executing all the scan blocks in parallel. The first scan block has all the information it needs to compute the full scan for its partition. However, the second block needs the final output value from the first before it can update its own values. It will have to wait until block 0 has finished and written its value to global memory. It can then add that value to its own elements before sending its final value to the next block. This process continues until all the blocks have been processed.&lt;/p&gt;
&lt;p&gt;To be clear, the first phase runs completely in parallel, and the second phase is sequential. If the final values are passed quickly enough between each block, the overall scan will still be efficient. Once each block has that passed value, it can continue its work in parallel since it is no longer dependent on the previous block.&lt;/p&gt;
&lt;p&gt;For this to work, there needs to be a form of synchronization between blocks. The CUDA API does not provide grid-wide synchronization, so &lt;strong&gt;how can we accomplish this&lt;/strong&gt;? One solution is to use a lock to effectively halt a thread until the value is ready to be read (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Yan, Long, and Zhang 2013&lt;/a&gt;). The code below shows how this can be implemented.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__shared__ float previous_sum;
if (threadIdx.x == 0) {
    // Wait for previous flag
    while(atomicAdd(&amp;amp;flags[bid], 0) == 0);
    // Read previous partial sum
    previous_sum = scan_value[bid];
    // Propagate partial sum
    scan_value[bid + 1] = previous_sum + local_sum;
    // Memory fence
    __threadfence();
    // Update flag
    atomicAdd(&amp;amp;flags[bid + 1], 1);
}
__syncthreads();
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The section ensures that only the first element of each block will perform this update. The &lt;code&gt;flags&lt;/code&gt; array is a global array that is used to store the lock values. Once the flag is set to 1, the value from the global array &lt;code&gt;scan_value&lt;/code&gt; is read and used to update the local sum. We haven&amp;rsquo;t used &lt;code&gt;__threadfence()&lt;/code&gt; before, but it is used to ensure that the update to &lt;code&gt;scan_value[bid + 1]&lt;/code&gt; is written before the call to &lt;code&gt;atomicAdd&lt;/code&gt; is made.&lt;/p&gt;
&lt;p&gt;Your first thought might be that there are too many global memory accesses. Shouldn&amp;rsquo;t this incur a large access penalty? Remember that many of these are overlapping values that were accessed by previous blocks, so they are most likely in the cache. Of course, if there is ever any doubt on the performance of a kernel, we can always profile it to verify our assumptions.&lt;/p&gt;
&lt;p&gt;As opposed to the previous solution, this one only requires a single kernel. In the three kernel approach, there is no overlap between the values since each kernel is executed sequentially.&lt;/p&gt;
&lt;h3 id=&#34;preventing-deadlocks&#34;&gt;Preventing Deadlocks&lt;/h3&gt;
&lt;p&gt;This issue isn&amp;rsquo;t production ready just yet. Depending on how the blocks are scheduled, it is possible that a deadlock could occur. For example, the second block could be scheduled before the first block. If there aren&amp;rsquo;t enough SMs on the device then the first block will never be able to write its value to the global array. One solution to this is &lt;strong&gt;dynamic block index assignment&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In this approach, the block index assignment is not dependent on &lt;code&gt;blockIdx.x&lt;/code&gt;. Instead it is assigned dynamically as blocks are processed.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__shared__ uint bid_s;
if (threadIdx.x == 0) {
    bid_s = atomicAdd(&amp;amp;block_index, 1);
}
__syncthreads();
uint bid = bid_s;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This ensures that each block gets a unique index and that the blocks are processed in the order they are ready.&lt;/p&gt;
&lt;h2 id=&#34;the-takeaway&#34;&gt;The Takeaway&lt;/h2&gt;
&lt;p&gt;Parallel scan is a powerful tool for solving problems that can be described in terms of a recursion. The Kogge-Stone and Brent-Kung algorithms are two ways of parallelizing the scan operation. This problem presents a unique look at how tradeoffs must be made when parallelizing a problem. At the end of the day, we must work within the constraints of the hardware and framework made available to us.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Yan, Shengen, Guoping Long, and Yunquan Zhang. 2013. “StreamScan: Fast Scan Algorithms for GPUs without Global Barrier Synchronization.” In &lt;i&gt;Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming&lt;/i&gt;, 229–38. PPoPP ’13. New York, NY, USA: Association for Computing Machinery. &lt;a href=&#34;https://doi.org/10.1145/2442516.2442539&#34;&gt;https://doi.org/10.1145/2442516.2442539&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GPU Pattern: Reduction</title>
      <link>https://ajdillhoff.github.io/notes/gpu_pattern_reduction/</link>
      <pubDate>Mon, 05 Feb 2024 15:47:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/gpu_pattern_reduction/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reduction-trees&#34;&gt;Reduction Trees&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-simple-kernel&#34;&gt;A Simple Kernel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#minimizing-control-divergence&#34;&gt;Minimizing Control Divergence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#memory-divergence-of-reduction&#34;&gt;Memory Divergence of Reduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reducing-the-number-of-global-memory-requests&#34;&gt;Reducing the number of global memory requests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hierarchical-reduction&#34;&gt;Hierarchical Reduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#thread-coarsening-back-again&#34;&gt;Thread Coarsening - Back Again&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;The following notes follow Chapter 10 of &lt;em&gt;Programming Massively Parallel Processors&lt;/em&gt; (Hwu, Kirk, and El Hajj 2022).&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Given a set of values, a &lt;strong&gt;reduction&lt;/strong&gt; produces a single output. It is an important part of many parallel algorithms including &lt;a href=&#34;https://ajdillhoff.github.io/notes/mapreduce/&#34;&gt;MapReduce&lt;/a&gt;. Other patterns that we have studied can also be viewed as reductions, such as &lt;a href=&#34;https://ajdillhoff.github.io/notes/gpu_pattern_parallel_histogram/&#34;&gt;GPU Pattern: Parallel Histogram&lt;/a&gt;. Implementing this parallel pattern requires careful consideration of thread communication, and will be the focus of these notes.&lt;/p&gt;
&lt;p&gt;Many of the operations you rely on are examples of reductions. For example, the `sum` function is a reduction, as is the `max` function. A reduction can be viewed as a linear combination of the input values, or transformed values, and is often used to compute a summary statistic. If \(\phi(\cdot)\) is a binary operator, then a reduction computes the following:&lt;/p&gt;
&lt;p&gt;\[
v = \phi(v, x_i)\ \text{for}\ i = 1, 2, \ldots, n,
\]&lt;/p&gt;
&lt;p&gt;where \(v\) is the accumulated value and \(x_i\) are the input values. The operator \(\phi(\cdot)\) can be any associative and commutative operation, such as addition or multiplication. Each operator has a corresponding identity element, such as 0 for addition or 1 for multiplication. The identity element is used to initialize the reduction and can be represented as \(v = v_0\) in the equation above.&lt;/p&gt;
&lt;h2 id=&#34;reduction-trees&#34;&gt;Reduction Trees&lt;/h2&gt;
&lt;p&gt;Reductions of any kind are well represented using trees. The first level of reduction maximizes the amount of parallelism. As the input is gradually reduced, fewer threads are needed.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-13_18-11-10_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Sum reduce as a reduction tree.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Sum reduce as a reduction tree.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In order to implement a parallel reduction, the chosen operator must be associative. For example, \(a + (b + c) = (a + b) + c\). The operator must also be commutative, such that \(a + b = b + a\).&lt;/p&gt;
&lt;p&gt;Reduction trees reveal the logarithmic nature of parallel reductions. Just like divide and conquer algorithms, the number of threads is halved at each level of the tree. The number of levels in the tree is \(\log_2(n)\), where \(n\) is the number of input values. Given an input size of \(n = 1024\), the number of threads required is \(\log_2(1024) = 10\). This is a significant reduction from the original input size. The sequential version of this reduction would require 1023 operations.&lt;/p&gt;
&lt;h2 id=&#34;a-simple-kernel&#34;&gt;A Simple Kernel&lt;/h2&gt;
&lt;p&gt;As mentioned above, reduction requires communication between threads. Since only the threads within a single block can communicate, we will focus on a block-level reduction. For now, each block can work with a total of 2048 input values based on the limitation of 1024 threads per block.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void sumReduceKernel(float *input, float *output) {
    unsigned int i = 2 * threadIdx.x;

        for (unsigned int stride = 1; stride &amp;lt;= blockDim.x; stride *= 2) {
            // Only threads in even positions participate
            if (threadIdx.x % stride == 0) {
                input[i] += input[i + stride];
            }
            __syncthreads();
        }

        if (threadIdx.x == 0) {
            *output = input[0];
        }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Each thread is assigned to a single write location &lt;code&gt;2 * threadIdx.x&lt;/code&gt;. The stride is doubled after each iteration of the loop, effectively halving the number of active threads. The stride also determines the second value that is added to the first. By the last iteration, only one thread is active to perform that last reduction.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-03_19-24-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Execution of kernel reduction (Source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Execution of kernel reduction (Source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;You can see that the kernel is simple, but it is also inefficient. There is a great deal of control divergence that will be addressed in the next section.&lt;/p&gt;
&lt;h2 id=&#34;minimizing-control-divergence&#34;&gt;Minimizing Control Divergence&lt;/h2&gt;
&lt;p&gt;As we just saw, the key to optimizing a reduction kernel is to minimize control divergence and make sure as many threads stay active as possible. A warp of 32 threads would consume the execution resources even if half of them are inactive. As each stage of the reduction tree is completed, the amount of wasted resources increases. Depending on the input size, entire warps could be launched and then immediately become inactive.&lt;/p&gt;
&lt;p&gt;The number of execution resources consumes is proportional to the number of active warps across all iterations. We can compute the number of resources consumed as follows:&lt;/p&gt;
&lt;p&gt;\[
(\frac{5N}{64} + \frac{N}{128} + \frac{N}{256} + \cdots + 1) * 32
\]&lt;/p&gt;
&lt;p&gt;where \(N\) is the number of input values. Each thread operates on 2 values, so \(\frac{N}{2}\) are launched in total. Since every warp has 32 threads, a total of \(\frac{N}{64}\) warps are launched. For the first 5 iterations, all warps will be active. The 5th iteration only has 1 active thread in each warp. On the 6th iteration, the number of active warps is halved, and so on.&lt;/p&gt;
&lt;p&gt;For an input of size \(N = 1024\), the number of resources consumed is \((80 + 8 + 4 + 2 + 1) * 32 = 3040\). The total number of results committed by the active threads is equal to the number of operations performed, which is \(N - 1 = 1023\). The efficiency of the kernel is then \(\frac{1023}{3040} = 0.34\). Only around 34% of the resources are used to perform the reduction.&lt;/p&gt;
&lt;h3 id=&#34;rearranging-the-threads&#34;&gt;Rearranging the Threads&lt;/h3&gt;
&lt;p&gt;A simple rearrangement of where the active results are stored can improve the efficiency of the kernel by reducing control divergence. The idea is to keep the threads that own the results of the reduction close together. Instead of increasing the stride, it should be decreased. The figure below shows the rearrangement of the threads.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-03_21-28-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Optimized reduction kernel execution (Source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Optimized reduction kernel execution (Source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void sumReduceKernel(float *input, float *output) {
    unsigned int i = threadIdx.x;

    for (unsigned int stride = blockDim.x; stride &amp;gt;= 1; stride /= 2) {
        if (i &amp;lt; stride) {
            input[i] += input[i + stride];
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        *output = input[0];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The kernel itself is effectively the same, but the rearrangement of the threads ensures that each warp has less control divergence. Additionally, warps that drop off after each iteration are no longer consuming execution resources. For an input of 256, the first 4 warps are fully utilized (barring the last thread of the last warp). After the first iteration, the number of active warps is halved. Warps 3 and 4 are now fully inactive, leaving warps 1 and 2 to perform the reduction operation on all threads. We can compute the number of resources consumed under this new arrangement as follows:&lt;/p&gt;
&lt;p&gt;\[
(\frac{N}{64} + \frac{N}{128} + \frac{N}{256} + \cdots + 1 + 5) * 32
\]&lt;/p&gt;
&lt;p&gt;At each iteration, half of warps become inactive and no longer consume resources. The last warp will consume execution resources for all 32 threads, even though the number of active threads is less than 32. For our input of size \(N = 1024\), the number of resources consumed is \((16 + 8 + 4 + 2 + 1 + 5) * 32 = 1152\), resulting in an efficiency of \(\frac{1023}{1152} = 0.89\). This is a significant improvement over the original kernel. This will increase based on the input size.&lt;/p&gt;
&lt;h2 id=&#34;memory-divergence-of-reduction&#34;&gt;Memory Divergence of Reduction&lt;/h2&gt;
&lt;p&gt;Does this kernel take advantage of memory coalescing? Each thread reads and writes from and to its &lt;em&gt;assigned&lt;/em&gt; location. It also makes a read from a location that is a stride away. These locations are certainly not adjacent and will not be coalesced.&lt;/p&gt;
&lt;p&gt;Adjacent threads do not access adjacent locations. The warp itself is unable to coalesce the thread requests into a single global memory request. Each data element is 4 bytes. Since each of the 32 threads in a warp are accessing their assigned locations with a separation of &lt;code&gt;stride&lt;/code&gt;, the &lt;code&gt;64 * 4&lt;/code&gt; bytes will require two 128 byte memory requests to access the data. With each iteration, the assigned locations will always be separated such that two 128 byte memory requests will need to be made. Only on the last iteration, where only a single thread accesses a single assigned location, will a single memory request be made.&lt;/p&gt;
&lt;p&gt;The convergent kernel from the last section takes advantage of memory coalescing, leading to fewer memory requests.&lt;/p&gt;
&lt;h2 id=&#34;reducing-the-number-of-global-memory-requests&#34;&gt;Reducing the number of global memory requests&lt;/h2&gt;
&lt;p&gt;As we saw with tiling in &lt;a href=&#34;https://ajdillhoff.github.io/notes/gpu_performance_basics/&#34;&gt;GPU Performance Basics&lt;/a&gt;, we can reduce the number of global memory requests by using shared memory. Threads write their results to global memory, which is read again in the next iteration. By keeping the intermediate results in shared memory, we can reduce the number of global memory requests. If implemented correctly, only the original input values will need to be read from global memory.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void sumReduceSharedKernel(float *input, float *output) {
    __shared__ float input_s[BLOCK_DIM];
    unsigned int i = threadIdx.x;
    input_s[i] = input[i] + input[i + BLOCK_DIM];

    for (unsigned int stride = blockDim.x / 2; stride &amp;gt;= 1; stride /= 2) {
        __syncthreads();
        if (i &amp;lt; stride) {
            input_s[i] += input_s[i + stride];
        }
    }

    if (i == 0) {
        *output = input_s[0];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;At the very top of this kernel, the necessary input is loaded from global memory, added, and written to shared memory. This is the only time global memory is accessed, with the exception of the final write to the output. The call to &lt;code&gt;syncthreads()&lt;/code&gt; moves to the top so that the shared memory is guaranteed before the next update.&lt;/p&gt;
&lt;p&gt;This approach not only requires fewer global memory requests, but the original input is left unmodified.&lt;/p&gt;
&lt;h2 id=&#34;hierarchical-reduction&#34;&gt;Hierarchical Reduction&lt;/h2&gt;
&lt;p&gt;One major assumption that has been made in each of these kernels is that they are running on a single block. Thread synchronization is critical to the success of the reduction. If we want to reduce a larger number of input across multiple blocks, the kernel should allow for independent execution. This is achieved by segmenting the input and performing a reduction on each segment. The final reduction is then performed on the results of the segment reductions.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void sumReduceHierarchicalKernel(float *input, float *output) {
    __shared__ float input_s[BLOCK_DIM];
    unsigned int segment = 2 * blockDim.x * blockIdx.x;
    unsigned int i = segment + threadIdx.x;
    unsigned int t = threadIdx.x;
    input_s[t] = input[i] + input[i + BLOCK_DIM];

    for (unsigned int stride = blockDim.x / 2; stride &amp;gt;= 1; stride /= 2) {
        __syncthreads();
        if (t &amp;lt; stride) {
            input_s[t] += input_s[t + stride];
        }
    }

    if (t == 0) {
        atomicAdd(output, input_s[0]);
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Each block has its own shared memory and can independently perform the reduction. Depending on the completion order, an atomic operation to add the local result is necessary.&lt;/p&gt;
&lt;h2 id=&#34;thread-coarsening-back-again&#34;&gt;Thread Coarsening - Back Again&lt;/h2&gt;
&lt;p&gt;Thread coarsening was first analyzed in the context of matrix multiplication in &lt;a href=&#34;https://ajdillhoff.github.io/notes/gpu_performance_basics/&#34;&gt;GPU Performance Basics&lt;/a&gt;. Whenever the device does not have enough resources to execute the number of threads requested, it is forced to serialize the execution. In this case, we can serialize the work done by each thread so that no extra overhead is incurred. Another benefit to thread coarsening is improved data locality.&lt;/p&gt;
&lt;p&gt;Successive iterations increase the amount of inactive warps. For reduction, thread coarsening can be applied by increasing the number of elements that each one processes. If the time to perform the arithmetic is much faster than the time to load the data, then thread coarsening can be beneficial. We could further analyze our program to determine the optimal coarsening factor.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ coarsenedSumReductionKernel(float *input, float *output) {
    __shared__ float input_s[BLOCK_DIM];
    uint segment = COARSE_FACTOR * 2 * blockDim.x * blockIdx.x;
    uint i = segment + threadIdx.x;
    uint t = threadIdx.x;

    float sum = input[i];
    for (uint tile = 1; tile &amp;lt; COARSE_FACTOR * 2; tile++) {
        sum += input[i + tile * BLOCK_DIM];
    }

    input_s[t] = sum;

    for (uint stride = blockDim.x / 2; stride &amp;gt;= 1; stride /= 2) {
        __syncthreads();
        if (t &amp;lt; stride) {
            input_s[t] += input_s[t + stride];
        }
    }
    if (t == 0) {
        atomicAdd(output, input_s[0]);
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In the coarsened version, less thread communication is required since the first several steps are computed in a single thread.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bag of Visual Words</title>
      <link>https://ajdillhoff.github.io/notes/bag_of_visual_words/</link>
      <pubDate>Sun, 04 Feb 2024 18:54:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/bag_of_visual_words/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#bag-of-visual-words&#34;&gt;Bag of Visual Words&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;&lt;strong&gt;Bag of Words&lt;/strong&gt; is a technique used in Natural Language Processing for document classification. It is a collection of word counts. To create a Bag of Words for a document, it necessary to create a dictionary first. Choosing the a dictionary is based on many factors including computational limitations. Next, the documents in a dataset are tokenized into words. The word counts are collected as part of a histogram and used as a feature vector for a machine learning model.&lt;/p&gt;
&lt;p&gt;The dictionary is the same for all documents in the original dataset. Ideally, the Bag of Word vectors for each document in the same class will be similar. This technique works well for problems in natural language processing, where each input document will have a varying number of words. By using a Bag of Words, the input data is transformed into a fixed length feature vector.&lt;/p&gt;
&lt;h2 id=&#34;bag-of-visual-words&#34;&gt;Bag of Visual Words&lt;/h2&gt;
&lt;p&gt;The Bag of Visual Words model adapts this technique to computer vision. Instead of words, distinct visual features are extracted from each image. Some images may have more features than others, similar to how some documents will have different word counts. The dictionary is created by clustering the visual features into a finite number of groups, determined as a hyperparameter. The visual features for each image are then counted and used as a feature vector for a machine learning model.&lt;/p&gt;
&lt;h3 id=&#34;extract-visual-features&#34;&gt;Extract Visual Features&lt;/h3&gt;
&lt;p&gt;The first step in creating a Bag of Visual Words is to extract visual features from each image. The visual features are typically extracted using a technique like SIFT, SURF, or ORB. These techniques are designed to extract features that are invariant to scaling, rotation, and translation. The visual features are then stored in a list for each image.&lt;/p&gt;
&lt;h3 id=&#34;create-visual-words&#34;&gt;Create Visual Words&lt;/h3&gt;
&lt;p&gt;Creating the dictionary requires clustering the features into a finite number of groups. The number of groups will vary depending on the complexity of the data. For a given dataset, this can be determined empirically. The most common clustering algorithm for this is K-Means, in which \(k\) different clusters are created and updated iteratively. The visual features are then assigned to the nearest cluster, and the cluster centers are updated. This process is repeated until the cluster centers converge.&lt;/p&gt;
&lt;h3 id=&#34;build-sparse-frequency-vectors&#34;&gt;Build Sparse Frequency Vectors&lt;/h3&gt;
&lt;p&gt;The next step is to create a histogram of the visual features for each image. The histogram is a sparse vector, where each element represents the count of a visual feature in the image. The histogram is then normalized to create a feature vector. Given an input image, the feature vector is extracted and assigned a label based on the cluster model. That label is one of the \(n\) chosen words in the vocabulary, which is incremented in the histogram.&lt;/p&gt;
&lt;h3 id=&#34;adjust-frequency-vectors&#34;&gt;Adjust Frequency Vectors&lt;/h3&gt;
&lt;p&gt;The feature vectors are then adjusted to account for the frequency of the visual features. This is done by applying a weighting scheme to them. The most common weighting scheme is called Term Frequency-Inverse Document Frequency (TF-IDF). TF-IDF scheme adjusts the frequency of a word in a document based on the frequency in the entire dataset. It is calculated as follows:&lt;/p&gt;
&lt;p&gt;\[
\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t),
\]&lt;/p&gt;
&lt;p&gt;where \(\text{TF}(t, d)\) is the term frequency of term \(t\) in document \(d\) and \(\text{IDF}(t)\) is the inverse document frequency of term \(t\) in the entire dataset.&lt;/p&gt;
&lt;p&gt;\(\text{TF}(t, d)\) is simply the number of times that visual feature \(t\) appears in the image \(d\). \(\text{IDF}(t)\) is calculated as follows:&lt;/p&gt;
&lt;p&gt;\[
\text{IDF}(t) = \log\left(\frac{N}{n_t}\right),
\]&lt;/p&gt;
&lt;p&gt;where \(N\) is the total number of images in the dataset and \(n_t\) is the number of images that contain the visual feature \(t\).&lt;/p&gt;
&lt;h3 id=&#34;compare-vectors&#34;&gt;Compare Vectors&lt;/h3&gt;
&lt;p&gt;The last step is to compare the feature vectors in service of some downstream task like classification. Since every feature vector is a fixed length, they can be used as input to a machine learning model.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Master Theorem</title>
      <link>https://ajdillhoff.github.io/notes/master_theorem/</link>
      <pubDate>Sun, 04 Feb 2024 17:49:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/master_theorem/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#example-merge-sort&#34;&gt;Example: Merge Sort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-matrix-multiplication&#34;&gt;Example: Matrix Multiplication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-median-finding&#34;&gt;Example: Median Finding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-cormen-et-al-dot-exercise-4-dot-5-2&#34;&gt;Example: Cormen et al. Exercise 4.5-2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;In the study of &lt;a href=&#34;https://ajdillhoff.github.io/notes/divide_and_conquer_algorithms/&#34;&gt;Divide and Conquer Algorithms&lt;/a&gt;, a recurrence tree can be used to determine the runtime complexity. These notes focus on the &lt;strong&gt;master theorem&lt;/strong&gt;, a blueprint for solving any recurrence of the form&lt;/p&gt;
&lt;p&gt;\[
T(n) = aT(n/b) + f(n).
\]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(n\) is the size of the problem,&lt;/li&gt;
&lt;li&gt;\(a \geq 1\) is the number of subproblems,&lt;/li&gt;
&lt;li&gt;\(b &amp;gt; 1\) is the factor by which the problem size is reduced, and&lt;/li&gt;
&lt;li&gt;\(f(n)\) is the cost of the work done outside of the recursive calls.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each recurrence is solved in \(T(n/b)\) time, and \(f(n)\) would include the cost of dividing and recombining the problem. The full theorem as described in &lt;em&gt;Introduction to Algorithms&lt;/em&gt; is restated below (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Master Theorem&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let \(a &amp;gt; 0\) and \(b &amp;gt; 1\) be constants, and let \(f(n)\) be a driving function that is defined and nonnegative on all sufficiently large reals. Define the recurrence \(T(n)\) on \(n \in \mathbb{N}\) by&lt;/p&gt;
&lt;p&gt;\[
T(n) = aT(n/b) + f(n),
\]&lt;/p&gt;
&lt;p&gt;where \(aT(n/b)\) actually means \(a&amp;rsquo;T(\lfloor n/b \rfloor) + a{&amp;rsquo;&amp;rsquo;}T(\lceil n / b \rceil)\) for some constants \(a&amp;rsquo; \geq 0\) and \(a&amp;rsquo;&amp;rsquo; \geq 0\) such that \(a = a&amp;rsquo; + a&amp;rsquo;&amp;rsquo;\). Then \(T(n)\) has the following asymptotic bounds:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If \(f(n) = O(n^{\log_b a - \epsilon})\) for some constant \(\epsilon &amp;gt; 0\), then \(T(n) = \Theta(n^{\log_b a})\).&lt;/li&gt;
&lt;li&gt;If \(f(n) = \Theta(n^{\log_b a} \log^k n)\) for some constant \(k \geq 0\), then \(T(n) = \Theta(n^{\log_b a} \log^{k+1} n)\).&lt;/li&gt;
&lt;li&gt;If \(f(n) = \Omega(n^{\log_b a + \epsilon})\) for some constant \(\epsilon &amp;gt; 0\), and if \(a f(n/b) \leq k f(n)\) for some constant \(k &amp;lt; 1\) and all sufficiently large \(n\), then \(T(n) = \Theta(f(n))\).&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;theorem-breakdown&#34;&gt;Theorem Breakdown&lt;/h3&gt;
&lt;p&gt;The common function \(n^{\log_b a}\) is called the &lt;strong&gt;watershed function&lt;/strong&gt;. The driving function \(f(n)\) is compared to it to determine which case applies. If the watershed function grows at a faster rate than \(f(n)\), case 1 applies. If they grow at the same rate, case 2 applies. If \(f(n)\) grows at a faster rate, case 3 applies.&lt;/p&gt;
&lt;p&gt;In case 1, the watershed function should grow faster than \(f(n)\) by a factor of \(n^\epsilon\) for some \(\epsilon &amp;gt; 0\). In case 2, technically the watershed function should grow at least the same rate as \(f(n)\), if not faster. That is, it grows faster by a factor of \(\Theta(\log^k n)\), where \(k \geq 0\). You can think of the extra \(\log^k n\) as an augmentation to the watershed function to ensure that they grow at the same rate. In most cases, \(k = 0\) which results in \(T(n) = \Theta(n^{\log_b a} \log n)\).&lt;/p&gt;
&lt;p&gt;Since case 2 allows for the watershed function to grow faster than \(f(n)\), case 3 requires that it grow at least &lt;strong&gt;polynomially&lt;/strong&gt; faster. \(f(n)\) should grow faster by at least a factor of \(\Theta(n^\epsilon)\) for some \(\epsilon &amp;gt; 0\). Additionally, the driving function must satisfy the regularity condition \(a f(n/b) \leq k f(n)\) for some constant \(k &amp;lt; 1\) and all sufficiently large \(n\). This condition ensures that the cost of the work done outside of the recursive calls is not too large.&lt;/p&gt;
&lt;h3 id=&#34;application-of-the-master-method&#34;&gt;Application of the Master Method&lt;/h3&gt;
&lt;p&gt;In most cases, the master method can be applied by looking at the recurrence and applying the relevant case. If the driving and watershed functions are not immediately obvious, you can use a different method as discussed in &lt;a href=&#34;https://ajdillhoff.github.io/notes/divide_and_conquer_algorithms/&#34;&gt;Divide and Conquer Algorithms&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When this can be applied, it is much simpler than the other methods. Let&amp;rsquo;s revisit some of the main problems we&amp;rsquo;ve explored before discussing applications for which the master method could not be used.&lt;/p&gt;
&lt;h2 id=&#34;example-merge-sort&#34;&gt;Example: Merge Sort&lt;/h2&gt;
&lt;p&gt;Merge Sort has a recurrence of the form \(T(n) = 2T(n/2) + \Theta(n)\). The driving function is \(f(n) = \Theta(n)\). The constants \(a\) and \(b\) are both 2, so the watershed function is \(n^{\log^2 2}\), which is \(n\). Since \(f(n)\) grows at the same rate as the watershed function, case 2 applies. Therefore, \(T(n) = \Theta(n \log n)\).&lt;/p&gt;
&lt;h2 id=&#34;example-matrix-multiplication&#34;&gt;Example: Matrix Multiplication&lt;/h2&gt;
&lt;p&gt;The recurrence of the divide and conquer version of matrix multiplication for square matrices is \(T(n) = 8T(n/2) + \Theta(1)\). Given \(a = 8\) and \(b = 2\), we can see that the complexity is inherent in the recurrence, not the driving function. The watershed function is \(n^{\log_2 8}\), which is \(n^3\). This grows at a faster rate than \(\Theta(1)\), so case 1 applies. Therefore, \(T(n) = \Theta(n^3)\).&lt;/p&gt;
&lt;h2 id=&#34;example-median-finding&#34;&gt;Example: Median Finding&lt;/h2&gt;
&lt;p&gt;Median finding has a recurrence of the form \(T(n) = T(n/5) + T(7n/10) + \Theta(n)\). Given the two recurrence factors, how do we evaluate the driving function? The form itself does not fit the master theorem, so it cannot be applied in this case. We could use the substitution method, recurrence trees, or the Akra-Bazzi theorem to solve this one.&lt;/p&gt;
&lt;h2 id=&#34;example-cormen-et-al-dot-exercise-4-dot-5-2&#34;&gt;Example: Cormen et al. Exercise 4.5-2&lt;/h2&gt;
&lt;p&gt;In this exercise from &lt;em&gt;Introduction to Algorithms&lt;/em&gt;, we are asked to find the largest integer value \(a\) such that an algorithm with the recurrence \(T(n) = aT(n/4) + \Theta(n^2)\) is asymptotically faster than \(\Theta(n^{\log_2 7})\). Since \(b = 4\), the largest integer \(a\) will be the smallest integer such that \(\log_4 a &amp;lt; \log_2 7\). Solving for the inequality shows that \(a = 48\) is the largest such integer.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GPU Pattern: Parallel Histogram</title>
      <link>https://ajdillhoff.github.io/notes/gpu_pattern_parallel_histogram/</link>
      <pubDate>Mon, 29 Jan 2024 17:22:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/gpu_pattern_parallel_histogram/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#histograms&#34;&gt;Histograms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#latency-of-atomic-operations&#34;&gt;Latency of Atomic Operations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#privatization&#34;&gt;Privatization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coarsening&#34;&gt;Coarsening&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aggregation&#34;&gt;Aggregation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-takeaway&#34;&gt;The Takeaway&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;These notes follow the presentation of the parallel histogram pattern in the book &lt;strong&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/strong&gt; (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;histograms&#34;&gt;Histograms&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Examples of histograms include:&lt;/li&gt;
&lt;li&gt;Frequency of words in a document&lt;/li&gt;
&lt;li&gt;Distribution of pixel intensities in an image&lt;/li&gt;
&lt;li&gt;Distribution of particle energies in a physics simulation&lt;/li&gt;
&lt;li&gt;Distribution of thread block execution times in a GPU kernel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Consider the program below which computes a histogram of the letters in a string. The input is assumed to be lower case.
Since this is executed sequentially, there is no risk of multiple threads writing to the same memory location at the same time.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;histogram_sequential&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;char&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;data, &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; length, &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;hist) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; length; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        hist[data[i] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;a&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To parallelize, we could launch a kernel which has each thread work with on character from the input. This presents a major problem when updating the histogram, as multiple threads may try and increment the same location simultaneously. This is called &lt;em&gt;output interference&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;race-conditions&#34;&gt;Race Conditions&lt;/h3&gt;
&lt;p&gt;For example, thread 1 and thread 2 may have the letter &amp;lsquo;a&amp;rsquo; as their input. They will both issue a &lt;em&gt;read-modify-write&lt;/em&gt; procedure where the current value of the histogram is read, incremented, and then written back to memory. If thread 1 reads the value of the histogram before thread 2 writes to it, the value that thread 1 writes back will be incorrect. This is a classic example of a &lt;em&gt;race condition&lt;/em&gt;. Depending on the timing, one thread could have read the updated value from the other thread, or both threads could have read the same value and incremented it, resulting in a loss of data.&lt;/p&gt;
&lt;h3 id=&#34;atomic-operations&#34;&gt;Atomic Operations&lt;/h3&gt;
&lt;p&gt;One solution to this problem is to perform atomic operations. This is a special type of operation that locks a memory location while it is being updated. This prevents other threads from reading or writing to the same location until the operation is complete. Each thread attempting to access a memory location will be forced to wait until the lock is released.&lt;/p&gt;
&lt;p&gt;The CUDA API provides several atomic operations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;atomicAdd&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;atomicSub&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;atomicExch&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;atomicMin&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;atomicMax&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;atomicInc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;atomicDec&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;atomicCAS&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are all &lt;em&gt;intrinsic functions&lt;/em&gt;, meaning they are processed in a special way by the compiler. Instead of acting like a function call that comes with the typical overhead from the stack, these are implemented as inline machine instructions. The CUDA kernel below uses &lt;code&gt;atomicAdd&lt;/code&gt; to increment the histogram.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void histogram_atomic(char *data, unsigned int length, unsigned int *hist) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &amp;lt; length) {
        atomicAdd(&amp;amp;hist[data[i] - &amp;#39;a&amp;#39;], 1);
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;latency-of-atomic-operations&#34;&gt;Latency of Atomic Operations&lt;/h2&gt;
&lt;p&gt;Atomic operations prevent the hardware from maximizing DRAM bursts since they serialize memory accesses. This can lead to a significant performance penalty. For each atomic operation, there are two delays: the delay from loading an element and the delay from storing the updated values.&lt;/p&gt;
&lt;p&gt;Not all threads will be loading and storing to the same memory location; this is dependent on the number of bins used in the histogram. If the number of bins is small, the performance penalty will be greater. This analysis is further complicated based on the distribution of the data.&lt;/p&gt;
&lt;p&gt;Atomic operations can be performed on the last level of cache. If the value is not in the cache, it will be brought into cache for future accesses. Although this does provide a performance benefit, it is not enough to offset the performance penalty of atomic operations.&lt;/p&gt;
&lt;h2 id=&#34;privatization&#34;&gt;Privatization&lt;/h2&gt;
&lt;p&gt;If much of the traffic is concentrated on a single area, the solution involves directing the traffic away in some manner. This is what &lt;strong&gt;privatization&lt;/strong&gt; does. Since the bottleneck is the data load and store, privatization gives each thread its own private store so that it can update without contention. Of course, each copy must be combined in some way at the end. The cost of merging these copies is much less than the cost of the atomic operations. In practice, privatization is done for groups of threads, not individual ones.&lt;/p&gt;
&lt;p&gt;The example above can be privatized by making a copy of the histogram for each thread block. The level of contention is much lower since only a single block will update their own private copy. All copies of the histogram are allocated as one monolithic array. Each individual block can use its local indices to offset the pointer. Private copies of values will likely still be cached in L2, so the cost of merging the copies is minimal.&lt;/p&gt;
&lt;p&gt;For example, if we have 256 threads per block and 26 bins, we can allocate a \(26 \times 256\) array of integers. Each thread block will have its own copy of the histogram. The kernel below demonstrates this.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;#define NUM_BINS 26
__global__ void histogram_privatized(char *data, unsigned int length, unsigned int *hist) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &amp;lt; length) {
        int pos = data[i] - &amp;#39;a&amp;#39;;
        atomicAdd(&amp;amp;hist[blockIdx.x * NUM_BINS + pos], 1);
    }
    __syncthreads();
    if (blockIdx.x &amp;gt; 0) {
        for (unsigned int bin = threadIdx.x; bin &amp;lt; NUM_BINS; bin += blockDim.x) {
            unsigned int binValue = hist[blockIdx.x * NUM_BINS + bin];
            if (binValue &amp;gt; 0) {
                atomicAdd(&amp;amp;hist[bin], binValue);
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Each block is free to use its section of the histogram without contention. After all blocks have finished, the histogram is merged by summing the values of each bin across all blocks. This is done by having each block add its values to the global histogram. The &lt;code&gt;__syncthreads&lt;/code&gt; function is used to ensure that all blocks have finished updating their private copies before the merge begins.&lt;/p&gt;
&lt;p&gt;Each block after index 0 will add its values to the global histogram, represented by the first block. Only a single thread per block will be accessing each bin, so the only contention is with other blocks. If the bins are small enough, shared memory can be used to store the private copies. Even though an atomic operation is still required, the latency for loading and storing is reduced by an order of magnitude. The shared kernel below demonstrates this.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;#define NUM_BINS 26
__global__ void histogram_privatized(char *data, unsigned int length, unsigned int *hist) {
    __shared__ unsigned int hist_s[NUM_BINS];
    for (unsigned int bin = threadIdx.x; bin &amp;lt; NUM_BINS; bin += blockDim.x) {
        hist_s[bin] = 0;
    }
    __syncthreads();

    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &amp;lt; length) {
        int pos = data[i] - &amp;#39;a&amp;#39;;
        atomicAdd(&amp;amp;hist_s[pos], 1);
    }
    __syncthreads();
    if (blockIdx.x &amp;gt; 0) {
        for (unsigned int bin = threadIdx.x; bin &amp;lt; NUM_BINS; bin += blockDim.x) {
            unsigned int binValue = hist_s[bin];
            if (binValue &amp;gt; 0) {
                atomicAdd(&amp;amp;hist[bin], binValue);
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;coarsening&#34;&gt;Coarsening&lt;/h2&gt;
&lt;p&gt;The bottleneck of using privatization moves from DRAM access to merging copies back to the &lt;em&gt;public&lt;/em&gt; copy of the data. This scales up based on the number of blocks used, since each thread in a block will be sharing a bin in the worst case. If the problem exceeds the capacity of the hardware, the scheduler will serialize the blocks. If they are serialized anyway, then the cost of privatization is not worth it.&lt;/p&gt;
&lt;p&gt;Coarsening will reduce the overhead of privatization by reducing the number of private copies that are committed to the public one. Each thread will process multiple elements.&lt;/p&gt;
&lt;h3 id=&#34;contiguous-partitioning&#34;&gt;Contiguous Partitioning&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-03_11-59-22_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Contiguous partitioning. Recreated from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Contiguous partitioning. Recreated from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Each thread is assigned a contiguous range of elements to process. The kernel is a straightforward extension of the privatized kernel. This approach works better on a CPU, where there are only a small number of threads. This is due to the caching behavior of the CPU. With so many threads on a GPU, it is less likely that the data will be in the cache since so many threads are competing.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;#define NUM_BINS 26
__global__ void histogram_privatized_cc(char *data, unsigned int length, unsigned int *hist) {
    __shared__ unsigned int hist_s[NUM_BINS];
    for (unsigned int bin = threadIdx.x; bin &amp;lt; NUM_BINS; bin += blockDim.x) {
        hist_s[bin] = 0;
    }
    __syncthreads();

    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &amp;lt; length) {
        int pos = data[i] - &amp;#39;a&amp;#39;;
        atomicAdd(&amp;amp;hist_s[pos], 1);
    }
    __syncthreads();
    if (blockIdx.x &amp;gt; 0) {
        for (unsigned int bin = threadIdx.x; bin &amp;lt; NUM_BINS; bin += blockDim.x) {
            unsigned int binValue = hist_s[bin];
            if (binValue &amp;gt; 0) {
                atomicAdd(&amp;amp;hist[bin], binValue);
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;interleaved-partitioning&#34;&gt;Interleaved Partitioning&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-03_12-00-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Interleaved partitioning. Recreated from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Interleaved partitioning. Recreated from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Contiguous partitioning allowed for contiguous access to values relative to each thread. However, the memory was not contiguous with respect to other threads. In terms of DRAM accesses, each individual read from memory was too far apart to take advantage of coalescing. With &lt;strong&gt;interleaved partitioning&lt;/strong&gt;, the memory can be accessed in a single DRAM access since the memory is coalesced.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;#define NUM_BINS 26
__global__ void histogram_privatized_ic(char *data, unsigned int length, unsigned int *hist) {
    __shared__ unsigned int hist_s[NUM_BINS];
    for (unsigned int bin = threadIdx.x; bin &amp;lt; NUM_BINS; bin += blockDim.x) {
        hist_s[bin] = 0;
    }
    __syncthreads();

    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    for (int i = tid; i &amp;lt; length; i += blockDim.x * gridDim.x) {
        int pos = data[i] - &amp;#39;a&amp;#39;;
        if (pos &amp;gt;= 0 &amp;amp;&amp;amp; pos &amp;lt; 26) {
            atomicAdd(&amp;amp;hist_s[pos], 1);
        }
    }
    __syncthreads();
    if (blockIdx.x &amp;gt; 0) {
        for (unsigned int bin = threadIdx.x; bin &amp;lt; NUM_BINS; bin += blockDim.x) {
            unsigned int binValue = hist_s[bin];
            if (binValue &amp;gt; 0) {
                atomicAdd(&amp;amp;hist[bin], binValue);
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In the code above, the main difference is the second &lt;code&gt;for&lt;/code&gt; loop. The index &lt;code&gt;i&lt;/code&gt; is incremented by &lt;code&gt;blockDim.x * gridDim.x&lt;/code&gt;. This ensures that the threads of each block access memory in a contiguous manner rather than each thread being contiguous. The differences are visualized in the figures.&lt;/p&gt;
&lt;h2 id=&#34;aggregation&#34;&gt;Aggregation&lt;/h2&gt;
&lt;p&gt;It is not uncommon that the input data will have a skewed distribution. There may be sections of the input that are locally dense. This will lead to a large number of atomic operations within a small area. To reduce the number of atomic operations, the input can be aggregated into a larger update before being committed to the global histogram. Consider the code below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void histogram_aggregate(char *data, unsigned int length, unsigned int *histo) {
    // Initialize shared memory
    __shared__ unsigned int hist_s[NUM_BINS];
    for (unsigned int bin = threadIdx.x; bin &amp;lt; NUM_BINS; bin += blockDim.x) {
        hist_s[bin] = 0;
    }
    __syncthreads();

    // Build histogram
    unsigned int accumulator = 0;
    int prevBinIdx = -1;
    unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;
    for (unsigned int i = tid; i a, length; i += blockDim.x * gridDim.x) {
        int binIdx = data[i] - &amp;#39;a&amp;#39;;
        if (binIdx == prevBinIdx) {
            accumulator++;
        } else {
            if (prevBinIdx &amp;gt;= 0) {
                atomicAdd(&amp;amp;hist_s[prevBinIdx], accumulator);
            }
            accumulator = 1;
            prevBinIdx = binIdx;
        }
    }
    if (accumulator &amp;gt; 0) {
        atomicAdd(&amp;amp;hist_s[prevBinIdx], accumulator);
    }
    __syncthreads();

    // Commit to global memory
    for (unsigned int bin = threadIdx.x; bin &amp;lt; NUM_BINS; bin += blockDim.x) {
        unsigned int binValue = hist_s[bin];
        if (binValue &amp;gt; 0) {
            atomicAdd(&amp;amp;histo[bin], binValue);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The difference in this kernel is the histogram loop in the middle. The previous bin index is tracked to determine if contiguous values would be aggregated. As long as the values are the same, the accumulator is increased. As soon as a new value is encountered, a batch update is performed. This reduces the number of atomic operations by a factor of the number of contiguous values.&lt;/p&gt;
&lt;p&gt;If the data is relatively uniform, the cost of aggregation exceeds the simple kernel. If you are working with images, spatially local data will usually be aggregated. This kernel would be beneficial in that case. Another downside to the aggregated kernel is that it requires more registers and has an increased chance for control divergence. As with all implementations, you should profile this against your use case.&lt;/p&gt;
&lt;h2 id=&#34;the-takeaway&#34;&gt;The Takeaway&lt;/h2&gt;
&lt;p&gt;Computing histograms is a common operation in fields such as image processing, natural language processing, and physics simulations. For example, a core preprocessing step for training a large language model is to compute the frequency of words in a corpus. This is a perfect example of a task that can be parallelized on a GPU.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Divide and Conquer Algorithms</title>
      <link>https://ajdillhoff.github.io/notes/divide_and_conquer_algorithms/</link>
      <pubDate>Tue, 23 Jan 2024 08:38:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/divide_and_conquer_algorithms/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solving-recurrences&#34;&gt;Solving Recurrences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-merge-sort&#34;&gt;Example: Merge Sort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-multiplying-square-matrices&#34;&gt;Example: Multiplying Square Matrices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-convex-hull&#34;&gt;Example: Convex Hull&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-median-search&#34;&gt;Example: Median Search&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;Divide and conquer algorithms are a class of algorithms that solve a problem by breaking it into smaller subproblems, solving the subproblems recursively, and then combining the solutions to the subproblems to form a solution to the original problem. Problems that can be solved in this manner are typically highly parallelizable. These notes investigate a few examples of classic divide and conquer algorithms and their analysis.&lt;/p&gt;
&lt;p&gt;A divide and conquer method is split into three steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Divide the problem into smaller subproblems.&lt;/li&gt;
&lt;li&gt;Conquer the subproblems by solving them recursively.&lt;/li&gt;
&lt;li&gt;Combine the solutions to the subproblems to form a solution to the original problem.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Their runtime can be characterized by the recurrence relation \(T(n)\). A recurrence \(T(n)\) is &lt;em&gt;algorithmic&lt;/em&gt; if, for every sufficiently large &lt;em&gt;threshold&lt;/em&gt; constant \(n_0 &amp;gt; 0\), the following two properties hold:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For all \(n \leq n_0\), the recurrence defines the running time of a constant-size input.&lt;/li&gt;
&lt;li&gt;For all \(n \geq n_0\), every path of recursion terminates in a defined base case within a finite number of recursive calls.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The algorithm must output a solution in finite time.
If the second property doesn&amp;rsquo;t hold, the algorithm is not correct &amp;ndash; it may end up in an infinite loop.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Whenever a recurrence is stated without an explicit base case, we assume that the recurrence is algorithmic.&amp;rdquo; (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This assumption means that the algorithm is correct and terminates in finite time, so there must be a base case. The base case is less important for analysis than the recursive case. For example, your base case might work with 100 elements, and that would still be \(\Theta(1)\) because it is a constant.&lt;/p&gt;
&lt;p&gt;It is common to break up each subproblem uniformly, but it is not always the best way to do it. For example, an application such as matrix multiplication is typically broken up uniformly since there is no spatial or temporal relationship to consider. Algorithms for image processing, on the other hand, may have input values that are locally correlated, so it may be better to break up the input in a way that preserves this correlation.&lt;/p&gt;
&lt;h2 id=&#34;solving-recurrences&#34;&gt;Solving Recurrences&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Substitution method&lt;/li&gt;
&lt;li&gt;Recursion-tree method&lt;/li&gt;
&lt;li&gt;Master method&lt;/li&gt;
&lt;li&gt;Akra-Bazzi method&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-merge-sort&#34;&gt;Example: Merge Sort&lt;/h2&gt;
&lt;p&gt;Merge sort is a classic example of a divide and conquer algorithm. It works by dividing the input array into two halves, sorting each half recursively, and then merging the two sorted halves.&lt;/p&gt;
&lt;h3 id=&#34;divide&#34;&gt;Divide&lt;/h3&gt;
&lt;p&gt;The divide step takes an input subarray \(A[p:r]\) and computes a midpoint \(q\) before partitioning it into two subarrays \(A[p:q]\) and \(A[q+1:r]\). These subarrays will be sorted recursively until the base case is reached.&lt;/p&gt;
&lt;h3 id=&#34;conquer&#34;&gt;Conquer&lt;/h3&gt;
&lt;p&gt;The conquer step recursively sorts the two subarrays \(A[p:q]\) and \(A[q+1:r]\). If the base case is such that the input array has only one element, the array is already sorted.&lt;/p&gt;
&lt;h3 id=&#34;combine&#34;&gt;Combine&lt;/h3&gt;
&lt;p&gt;The combine step merges the two sorted subarrays to produce the final sorted array.&lt;/p&gt;
&lt;h3 id=&#34;python-implementation&#34;&gt;Python Implementation&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;merge_sort&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; len(A) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Conquer -- base case&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; A
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Divide Step&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    mid &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(A) &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; merge_sort(A[:mid])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; merge_sort(A[mid:])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Combine Step&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; merge(left, right)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;merge&lt;/span&gt;(left, right):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    i, j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Merge the two subarrays&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; (i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; len(left)) &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; (j &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; len(right)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; left[i] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; right[j]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            result&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(left[i])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            i &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            result&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(right[j])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            j &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Add the remaining elements to the final array&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    result &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; left[i:]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    result &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; right[j:]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; result
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This function assumes that the subarrays &lt;code&gt;left&lt;/code&gt; and &lt;code&gt;right&lt;/code&gt; are already sorted. If the value in the left subarray is less than the value in the right subarray, the left value is added to the final array. Otherwise, the right value is added. As soon as one of the subarrays is exhausted, the remaining elements in the other subarray are added to the final array. This is done with slicing in Python.&lt;/p&gt;
&lt;p&gt;The divide step simply splits the data into &lt;code&gt;left&lt;/code&gt; and &lt;code&gt;right&lt;/code&gt; subarrays. The conquer step simplifies the sorting process by reducing it down to the base case &amp;ndash; a single element. Finally, the combine step merges or &lt;em&gt;folds&lt;/em&gt; the two sorted subarrays together.&lt;/p&gt;
&lt;p&gt;Example code can be found &lt;a href=&#34;https://github.com/ajdillhoff/python-examples/blob/main/sorting/merge_sort.py&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;When analyzing the running time of a divide and conquer algorithm, it is safe to assume that the base case runs in constant time. The focus of the analysis should be on the &lt;strong&gt;recurrence equation&lt;/strong&gt;. For merge sort, we originally have a problem of size \(n\). We then divide the problem into 2 subproblems of size \(n/2\). Therefore the recurrence is \(T(n) = 2T(n/2)\). This recurrence continues for as long as the base case is not reached.&lt;/p&gt;
&lt;p&gt;Of course we also have to factor in the time it takes for the divide and combine steps. These can be represented as \(D(n)\) and \(C(n)\), respectively. The total running time of the algorithm is then \(T(n) = 2T(n/2) + D(n) + C(n)\) when \(n &amp;gt;= n_0\), where \(n_0\) is the base case.&lt;/p&gt;
&lt;p&gt;For merge sort specifically, the base case is \(D(n) = \Theta(1)\) since all it does is compute the midpoint. As we saw above, the conquer step is the recurrence \(T(n) = 2T(n/2)\). The combine step is \(C(n) = \Theta(n)\) since it takes linear time to merge the two subarrays. Thus, the worst-case running time of merge sort is \(T(n) = 2T(n/2) + \Theta(n)\).&lt;/p&gt;
&lt;p&gt;Not every problem will have a recurrence of \(2T(n/2)\). We can generalize this to \(aT(n/b)\), where \(a\) is the number of subproblems and \(b\) is the size of the subproblems.&lt;/p&gt;
&lt;p&gt;We haven&amp;rsquo;t finished the analysis yet since it is still not clear what the asymptotic upper bound is. &lt;strong&gt;Recurrence trees&lt;/strong&gt; can be used to visualize the running time of a divide and conquer algorithms. After inspecting the result of the tree, we will be able to easily determine the complexity of merge sort in terms of big-O notation.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-04_15-09-47_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Expansion of recursion tree for merge sort (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Expansion of recursion tree for merge sort (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the figure above from &lt;em&gt;Introduction to Algorithms&lt;/em&gt;, the root of the tree represents the original problem of size \(n\) in (a). In (b), the divide step splits the problem into two problems of size \(n/2\). The cost of this step is indicated by \(c_2n\). Here, \(c_2\) represents the constant cost per element for dividing and combining. As mentioned above, the combine step is dependent on the size of the subproblems, so the cost is \(c_2n\). Subfigure (c) shows a third split, where each new subproblem has size \(n/4\). This would continue recursively until the base case is reached, as shown in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-04_15-14-25_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Full recursion tree for merge sort (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Full recursion tree for merge sort (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The upper bound for each level of the tree is \(c_2n\). The height of a binary tree is \(\log_b n\). The total cost of the tree is the sum of the costs at each level. In this case, the cost is \(c_2n \log n + c_1n\), where the last \(c1_n\) comes from the base case. The first term is the dominating factor in the running time, so the running time of merge sort is \(\Theta(n \log n)\).&lt;/p&gt;
&lt;h3 id=&#34;questions&#34;&gt;Questions&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Assume that the base case is \(n &amp;gt; 1\). What is the running time of the conquer step dependent on?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;example-multiplying-square-matrices&#34;&gt;Example: Multiplying Square Matrices&lt;/h2&gt;
&lt;p&gt;Matrix multiplication is defined as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;square_matrix_multiply&lt;/span&gt;(A, B):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    C &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                C[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; A[i][k] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; B[k][j]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; C
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Initializing values takes \(\Theta(n^2)\). The full process takes \(\Theta(n^3)\).&lt;/p&gt;
&lt;h3 id=&#34;divide-and-conquer&#34;&gt;Divide and Conquer&lt;/h3&gt;
&lt;p&gt;In this approach, the matrix will be split into block matrices of size \(n/2\). Each submatrix can be multiplied with the corresponding submatrix of the other matrix. The resulting submatrices can be added together to form the final matrix. This is permissible based on the definition of matrix multiplication.&lt;/p&gt;
&lt;p&gt;Base case is \(n=1\) where only a single addition and multiplication are performed. This is \(T(1) = \Theta(1)\). For \(n &amp;gt; 1\), the recursive algorithm starts by splitting into 8 subproblems of size \(n/2\). There are 8 subproblems because there are 4 submatrices in each matrix, and each submatrix is multiplied with the corresponding submatrix in the other matrix.&lt;/p&gt;
&lt;p&gt;Each recursive call contributes \(T(n/2)\) to the running time. There are 8 recursive calls, so the total running time is \(8T(n/2) + \Theta(n^2)\). There is no need to implement a combine step since the matrix is updated in place. The final running time is \(T(n) = 8T(n/2) + \Theta(1)\) &lt;strong&gt;for the recursive portion&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This method easily adapts to parallel processing. The size of each &lt;em&gt;tile&lt;/em&gt; can be adjusted to fit the number of processors available. The algorithm can be parallelized by assigning each processor to a subproblem.&lt;/p&gt;
&lt;p&gt;We now walk through an example on a \(4 \times 4\) matrix. Assume that each \(A_{ij}\) and \(B_{ij}\) is a \(2 \times 2\) matrix.&lt;/p&gt;
&lt;p&gt;\begin{bmatrix}
A_{11} &amp;amp; A_{12} \\
A_{21} &amp;amp; A_{22}
\end{bmatrix}&lt;/p&gt;
&lt;p&gt;\begin{bmatrix}
B_{11} &amp;amp; B_{12} \\
B_{21} &amp;amp; B_{22}
\end{bmatrix}&lt;/p&gt;
&lt;p&gt;These matrices are already partitioned. They currently don&amp;rsquo;t meet the base case, so 8 recursive calls are made which compute the following products:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;\(A_{11}B_{11}\)&lt;/li&gt;
&lt;li&gt;\(A_{12}B_{21}\)&lt;/li&gt;
&lt;li&gt;\(A_{11}B_{12}\)&lt;/li&gt;
&lt;li&gt;\(A_{12}B_{22}\)&lt;/li&gt;
&lt;li&gt;\(A_{21}B_{11}\)&lt;/li&gt;
&lt;li&gt;\(A_{22}B_{21}\)&lt;/li&gt;
&lt;li&gt;\(A_{21}B_{12}\)&lt;/li&gt;
&lt;li&gt;\(A_{22}B_{22}\)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Peeking into the first recursive call, the \(2 \times 2\) matrices are partitioned into 4 \(1 \times 1\) matrices, or scalars. The base case is reached, and the product is computed. The same process is repeated for the other 7 recursive calls. The final matrix is then formed by adding the products together.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;partition&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    mid &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A11 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [row[:mid] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; A[:mid]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A12 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [row[mid:] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; A[:mid]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A21 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [row[:mid] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; A[mid:]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A22 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [row[mid:] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; A[mid:]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; A11, A12, A21, A22
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;matrix_multiply_recursive&lt;/span&gt;(A, B, C, n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        C[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; A[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; B[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Partition the matrices&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A11, A12, A21, A22 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; partition(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        B11, B12, B21, B22 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; partition(B)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        C11, C12, C21, C22 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; partition(C)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Recursively compute the products&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        matrix_multiply_recursive(A11, B11, C11, n&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        matrix_multiply_recursive(A12, B21, C11, n&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        matrix_multiply_recursive(A11, B12, C11, n&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        matrix_multiply_recursive(A12, B22, C11, n&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        matrix_multiply_recursive(A21, B11, C11, n&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        matrix_multiply_recursive(A22, B21, C11, n&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        matrix_multiply_recursive(A21, B12, C11, n&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        matrix_multiply_recursive(A22, B22, C11, n&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;Each recursive call contributes \(T(n/2)\) to the running time. Unless the base case is reached, each call contributes 8 recursive calls to the recurrence, yielding a running time of \(T(n) = 8T(n/2) + \Theta(1)\).&lt;/p&gt;
&lt;h2 id=&#34;example-convex-hull&#34;&gt;Example: Convex Hull&lt;/h2&gt;
&lt;p&gt;Given \(n\) points in plane, the convex hull is the smallest convex polygon that contains all the points.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;No two points have the same \(x\) or \(y\) coordinate.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sequence of points on boundary in clockwise order as doubly linked list.&lt;/p&gt;

    
    
    
    
    
    &lt;figure&gt;
    
    &lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-19_11-46-30_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Convex Hull (source: Wikipedia)&#34; &gt;
    
    
    
    &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
      
      &lt;p&gt;
        &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Convex Hull (source: Wikipedia)
        
        
        
      &lt;/p&gt; 
    &lt;/figcaption&gt;
    
    &lt;/figure&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;naive-solution&#34;&gt;Naive Solution&lt;/h3&gt;
&lt;p&gt;Draw lines between each pair of points. If all other points are on the same side of the line, the line is part of the convex hull. This is \(\Theta(n^3)\).&lt;/p&gt;
&lt;h3 id=&#34;divide-and-conquer&#34;&gt;Divide and Conquer&lt;/h3&gt;
&lt;p&gt;Sort the points by \(x\) coordinate. Split into two halves by \(x\). Recursively find the convex hull of each half. Merge the two convex hulls.&lt;/p&gt;
&lt;h4 id=&#34;merging&#34;&gt;Merging&lt;/h4&gt;
&lt;p&gt;Find upper tangent and lower tangent.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why not just select the highest point from each half?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The highest point in each half may not be part of the convex hull. The question assumes that the two convex hulls are relatively close to each other.&lt;/p&gt;
&lt;h4 id=&#34;two-finger-algorithm&#34;&gt;Two Finger Algorithm&lt;/h4&gt;
&lt;p&gt;Start at the rightmost point of the left convex hull and the leftmost point of the right convex hull. Move the right finger clockwise and the left finger counterclockwise until the tangent is found. The pseudocode is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;merge_convex_hulls&lt;/span&gt;(left, right):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Find the rightmost point of the left convex hull&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    left_max &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(left, key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; p: p[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Find the leftmost point of the right convex hull&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    right_min &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(right, key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; p: p[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Find the upper tangent&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Move the right finger clockwise&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        right_max &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(right, key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; p: p[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; is_upper_tangent(left_max, right_max):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Move the left finger counterclockwise&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        left_max &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(left, key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; p: p[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Find the lower tangent&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Move the right finger clockwise&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        right_min &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(right, key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; p: p[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; is_lower_tangent(left_min, right_min):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Move the left finger counterclockwise&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        left_min &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(left, key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; p: p[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This runs in \(\Theta(n)\) time.&lt;/p&gt;
&lt;p&gt;Removing the lines that are not part of the convex hull require the cut and paste operations. Starting at the upper tangent, move clockwise along the right convex hull until you reach the point in the lower tangent of the right convex hull. Make the connection to the corresponding point on the left convex hull based on the lower tangent, then move clockwise until you reach the upper tangent of the left convex hull. This is \(\Theta(n)\).&lt;/p&gt;
&lt;h4 id=&#34;orientation-test&#34;&gt;Orientation Test&lt;/h4&gt;
&lt;p&gt;The orientation test is a technique from computational geometry which determines the orientation of three points. For our purposes, it will tell us if a third point lies below or above a given line segment. The orientation test is used to determine if a point is part of the convex hull.&lt;/p&gt;
&lt;p&gt;Given three points \(p\), \(q\), and \(r\), the orientation is determined by the sign of the cross product of the vectors \(\overrightarrow{pq}\) and \(\overrightarrow{pr}\). If the cross product is positive, the orientation is clockwise. If the cross product is negative, the orientation is counterclockwise. If the cross product is zero, the points are collinear.&lt;/p&gt;
&lt;p&gt;This is expressed by a simple formula:&lt;/p&gt;
&lt;p&gt;\[
\text{orientation}(p, q, r) = (q_y - p_y)(r_x - q_x) - (q_x - p_x)(r_y - q_y)
\]&lt;/p&gt;
&lt;p&gt;Consider the visualization below. Let \(p\) be a point in the left convex hull and \(q\) be a point in the right convex hull. The orientation test will tell us if \(r\) is above or below the line segment \(\overline{pq}\). If the test is negative, \(r\) is above the line segment and is part of the convex hull, and vice versa.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-19_18-53-57_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Visualization of the orientation test.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Visualization of the orientation test.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;When checking to see if a line is an upper tangent, consider the points \(p\), \(q\), and \(r\), where \(p\) is from the left convex hull and \(q\) is from the right convex hull. Let \(r\) be the point immediately after \(q\) in a clockwise direction.&lt;/p&gt;
&lt;h2 id=&#34;example-median-search&#34;&gt;Example: Median Search&lt;/h2&gt;
&lt;p&gt;Finding the median value of a set can be performed in linear time without fully sorting the data. The recurrence is based on discarding a constant fraction of the elements at each step.&lt;/p&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Divide&lt;/strong&gt;&lt;/strong&gt;: Partition the set into groups of 5 elements. Depending on the size of the set, there may be less than 5 elements in the last set.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Conquer&lt;/strong&gt;&lt;/strong&gt;: Sort each group and find the median of each group. Since the subsets are of constant size, this is done in constant time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Combine&lt;/strong&gt;&lt;/strong&gt;: Given the median of each group from step 2, find the median of medians. This value will be used as a pivot for the next step.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Partition&lt;/strong&gt;&lt;/strong&gt;: Use the pivot to separate values smaller and larger than the pivot.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Select&lt;/strong&gt;&lt;/strong&gt;: If the given pivot is the true median based on its position in the original set, select it. If not, recursively select the median from the appropriate partition.&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-31_18-33-21_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Visualization of median of medians (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Visualization of median of medians (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Given a set of \(n\) numbers, define \(rank(X)\) as the number in the set that are less than or equal to \(X\).&lt;/p&gt;
&lt;p&gt;\(Select(S, i)\)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pick \(x \in S\)&lt;/li&gt;
&lt;li&gt;Compute \(k = rank(x)\)&lt;/li&gt;
&lt;li&gt;B = {y in S | y &amp;lt; x}&lt;/li&gt;
&lt;li&gt;C = {y in S | y &amp;gt; x}&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If \(i = k\), return \(x\).
else if \(k &amp;gt; i\), return \(Select(B, i)\).
else return \(Select(C, i-k)\).&lt;/p&gt;
&lt;p&gt;How do we get balanced partitions?&lt;/p&gt;
&lt;p&gt;Arrange \(S\) into columns of size 5.
Sort each column descending (linear time).
Find &amp;ldquo;median of medians&amp;rdquo; as \(X\)&lt;/p&gt;
&lt;p&gt;If the columns are sorted, it is trivial to find the median of each column.&lt;/p&gt;
&lt;p&gt;Half of the groups contribute at least 3 elements greater than \(X\), except for the last group. We have one group that contains \(x\).&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;Given a set \(A = \{a_1, a_2, \ldots, a_n\}\), the median search algorithm returns the $k$th smallest element of \(A\). We now analyze the runtime of this algorithm.&lt;/p&gt;
&lt;h4 id=&#34;divide&#34;&gt;Divide&lt;/h4&gt;
&lt;p&gt;In the &lt;strong&gt;divide&lt;/strong&gt; step, the set is partitioned into \(\lceil n/5 \rceil\) groups of 5 elements each. This is done in linear time.&lt;/p&gt;
&lt;h4 id=&#34;conquer&#34;&gt;Conquer&lt;/h4&gt;
&lt;p&gt;Each group is sorted using insertion sort or some other algorithm. Even though the search itself may have a higher complexity, the sorting of the groups is \(\Theta(1)\) since the groups are of constant size. Collectively across all groups, the sorting is \(\Theta(n)\).&lt;/p&gt;
&lt;h4 id=&#34;combine&#34;&gt;Combine&lt;/h4&gt;
&lt;p&gt;The median of each group is found, introducing a recurrence of \(T(\lceil n/5 \rceil)\). Once each median is found, the median of medians is computed.&lt;/p&gt;
&lt;h4 id=&#34;partition&#34;&gt;Partition&lt;/h4&gt;
&lt;p&gt;As reasoned above, the median of medians is used as a pivot to partition the set into two groups. At least 30% of the elements are greater than the pivot. This leaves a search space of at most \(7n/10\). Partitioning is done in linear time.&lt;/p&gt;
&lt;h4 id=&#34;select&#34;&gt;Select&lt;/h4&gt;
&lt;p&gt;If the pivot is the $k$th smallest element, the algorithm terminates. Otherwise, the algorithm recursively searches the appropriate partition. Comparing the size of the partition to the size of the original set, the recurrence is \(T(7n/10)\).&lt;/p&gt;
&lt;p&gt;Pseudocode is available &lt;a href=&#34;https://en.wikipedia.org/wiki/Median_of_medians&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GPU Pattern: Stencils</title>
      <link>https://ajdillhoff.github.io/notes/gpu_pattern_stencils/</link>
      <pubDate>Mon, 22 Jan 2024 19:39:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/gpu_pattern_stencils/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#differential-equations&#34;&gt;Differential Equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#stencils&#34;&gt;Stencils&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-basic-stencil&#34;&gt;Example: Basic Stencil&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tiled-stencil&#34;&gt;Tiled Stencil&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#thread-coarsening&#34;&gt;Thread Coarsening&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#register-tiling&#34;&gt;Register Tiling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#questions&#34;&gt;Questions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;ul&gt;
&lt;li&gt;Used in differential equations&lt;/li&gt;
&lt;li&gt;Frequently use higher precision&lt;/li&gt;
&lt;li&gt;Some similarity to convolutions&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;differential-equations&#34;&gt;Differential Equations&lt;/h2&gt;
&lt;p&gt;Any computational problem requires discretization of data or equations so that they can be solved numerically. This is fundamental in numerical analysis, where differential equations need to be approximated.&lt;/p&gt;
&lt;p&gt;Structured grids are used in finite-difference methods for solving partial differential equations (PDEs). Approximate derivatives can be computed point-wise by considering the neighbors on the grid. As we saw earlier in this course, a grid representation is a natural way to think about data parallelism.&lt;/p&gt;
&lt;p&gt;Depending on the function and level of discretization, interpolation will be more or less accurate. Consider the logistic sigmoid function sampled at 4 points. In the middle, linear interpolation would work just fine. Near the &lt;em&gt;bends&lt;/em&gt; of the function, a linear approximation would introduce error. The closer the spacing, the more accurate a linear approximation becomes. The downside is that more memory is required to store the points.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-27_13-03-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Logistic sigmoid function (Wikipedia).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Logistic sigmoid function (Wikipedia).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The precision of the data type also plays an important role. Higher precision data types like &lt;code&gt;double&lt;/code&gt; require more bandwidth to transfer and will typically require more cycles when computing arithmetic operations.&lt;/p&gt;
&lt;h2 id=&#34;stencils&#34;&gt;Stencils&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;stencil&lt;/strong&gt; is a geometric pattern of weights applied at each point of a structured grid. The points on the grid will derive their values from neighboring points using some numerical approximation. For example, this is used to solve differential equations. Consider a 1D grid of discretized values from a function \(f(x)\). The finite difference approximation can be used to find \(f&amp;rsquo;(x)\):&lt;/p&gt;
&lt;p&gt;\[ f&amp;rsquo;(x) = \frac{f(x+h) - f(x-h)}{2h} + O(h^2) \]&lt;/p&gt;
&lt;p&gt;In code, this would look like:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void finite_difference(float *f, float *df, float h) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    df[i] = (f[i+1] - f[i-1]) / (2 * h);
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;A PDE of two variables can be solved using a 2D stencil. Likewise, a PDE of three variables can be solved using a 3D stencil. The figures below show examples of common 2D and 3D stencils. Note that they typically have an odd number of points so that there is a center point.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-27_15-23-57_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;1D stencils. Recreated from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;1D stencils. Recreated from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The 1D stencils shown above are used to approximate the first-order (A), second-order (B), and third-order (C) derivatives of a function \(f(x)\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-27_15-24-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;2D and 3D stencils. Recreated from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;2D and 3D stencils. Recreated from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The 2D stencils shown above are used to approximate first-order (A) and second-order (B) derivatives of a function \(f(x, )\). Likewise, the 3D stencils are used to approximate first-order (C) and second-order (D) derivatives of a function \(f(x, y, z)\).&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;stencil sweep&lt;/em&gt; is the process of applying the stencil to all points on the grid, similar to how a convolution is applied. There are many similarities between the two, but the subtle differences will require us to think differently about how to optimize them.&lt;/p&gt;
&lt;h2 id=&#34;example-basic-stencil&#34;&gt;Example: Basic Stencil&lt;/h2&gt;
&lt;p&gt;The code below presents a naive kernel for a stencil pattern using a 3D seven-point stencil.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void stencil_kernel(float *in, float *out, unsigned int N) {
    unsigned int i = blockIdx.z * blockDim.z + threadIdx.z;
    unsigned int j = blockIdx.y * blockDim.y + threadIdx.y;
    unsigned int k = blockIdx.x * blockDim.x + threadIdx.x;

    if (i &amp;gt;= 1 &amp;amp;&amp;amp; i &amp;lt; N - 1 &amp;amp;&amp;amp; j &amp;gt;= 1 &amp;amp;&amp;amp; j &amp;lt; N - 1 &amp;amp;&amp;amp; k &amp;gt;= 1 &amp;amp;&amp;amp; k &amp;lt; N - 1) {
        out[i*N*N + j*N + k] = c0 * in[i*N*N + j*N + k]
                             + c1 * in[i*N*N + j*N + (k - 1)]
                             + c2 * in[i*N*N + j*N + (k + 1)]
                             + c3 * in[i*N*N + (j - 1)*N + k]
                             + c4 * in[i*N*N + (j + 1)*N + k]
                             + c5 * in[(i - 1)*N*N + j*N + k]
                             + c6 * in[(i + 1)*N*N + j*N + k];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This example assumes the input and output are 3D grids. For this particular stencil, you should try to identify the number of memory accesses and operations performed. Can you already see some opportunities for optimization?&lt;/p&gt;
&lt;h2 id=&#34;tiled-stencil&#34;&gt;Tiled Stencil&lt;/h2&gt;
&lt;p&gt;Just like with convolution, it is possible to use shared memory to improve the performance of a stencil. The code below shows a tiled stencil kernel that uses shared memory.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void stencil_kernel(float *in, float *out, unsigned int N) {
    __shared__ float tile[IN_TILE_DIM][IN_TILE_DIM][IN_TILE_DIM];
    int i = blockIdx.z * OUT_TILE_DIM + threadIdx.z - 1;
    int j = blockIdx.y * OUT_TILE_DIM + threadIdx.y - 1;
    int k = blockIdx.x * OUT_TILE_DIM + threadIdx.x - 1;

    if (i &amp;gt;= 0 &amp;amp;&amp;amp; i &amp;lt; N &amp;amp;&amp;amp; j &amp;gt;= 0 &amp;amp;&amp;amp; j &amp;lt; N &amp;amp;&amp;amp; k &amp;gt;= 0 &amp;amp;&amp;amp; k &amp;lt; N) {
        tile[threadIdx.z][threadIdx.y][threadIdx.x] = in[i*N*N + j*N + k];
    }
    __syncthreads();
    if (i &amp;gt;=1 &amp;amp;&amp;amp; i &amp;lt; N-1 &amp;amp;&amp;amp; j &amp;gt;= 1 &amp;amp;&amp;amp; j &amp;lt; N-1 &amp;amp;&amp;amp; k &amp;gt;= 1 &amp;amp;&amp;amp; k &amp;lt; N-1) {
        if (threadIdx.z &amp;gt;= 1 &amp;amp;&amp;amp; threadIdx.z &amp;lt; IN_TILE_DIM-1 &amp;amp;&amp;amp;
            threadIdx.y &amp;gt;= 1 &amp;amp;&amp;amp; threadIdx.y &amp;lt; IN_TILE_DIM-1 &amp;amp;&amp;amp;
            threadIdx.x &amp;gt;= 1 &amp;amp;&amp;amp; threadIdx.x &amp;lt; IN_TILE_DIM-1) {
            out[i*N*N + j*N + k] = c0 * tile[threadIdx.z][threadIdx.y][threadIdx.x]
                                 + c1 * tile[threadIdx.z][threadIdx.y][threadIdx.x - 1]
                                 + c2 * tile[threadIdx.z][threadIdx.y][threadIdx.x + 1]
                                 + c3 * tile[threadIdx.z][threadIdx.y - 1][threadIdx.x]
                                 + c4 * tile[threadIdx.z][threadIdx.y + 1][threadIdx.x]
                                 + c5 * tile[threadIdx.z - 1][threadIdx.y][threadIdx.x]
                                 + c6 * tile[threadIdx.z + 1][threadIdx.y][threadIdx.x];
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Just like before, the threads of a block will first collaborate to load a tile with the relevant data. Stencil patterns are more sparse than convolutional filters and require less data to be loaded into shared memory. This will be the central detail in our analysis of stencil patterns.&lt;/p&gt;
&lt;h3 id=&#34;scaling-stencil-size-vs-dot-convolution-filter-size&#34;&gt;Scaling stencil size vs. Convolution filter size&lt;/h3&gt;
&lt;p&gt;Convolutions are more efficient as the size increases since more values are accessed in shared memory. For a \(3 \times 3\) convolution, the upper bound on compute to memory ratio is 4.5 OP/B. A 5 point 2D stencil has a ratio of 2.5 OP/B due to the sparsity of the pattern. The threads in the block would load the diagonal values from global memory, but each thread would only use the 5 points defined by the kernel.&lt;/p&gt;
&lt;h3 id=&#34;tiling-analysis&#34;&gt;Tiling Analysis&lt;/h3&gt;
&lt;p&gt;Let us consider the effectiveness of shared memory tiling where each thread performs 13 floating-point ops (7 multiplies and 6 adds) with each block using \((T - 2)^3\) threads. Each block also performs \(T^3\) loads of 4 bytes each. The compute to memory ratio can be express as:&lt;/p&gt;
&lt;p&gt;\[
\frac{13(T - 2)^3}{4T^3} = \frac{13}{4} \left(1 - \frac{2}{T}\right)^3
\]&lt;/p&gt;
&lt;p&gt;Due to the low limit on threads, the size of \(T\) is typically small. This means there is a smaller amount of reuse of data in shared memory. The ratio of floating-point ops to memory accesses will be low.&lt;/p&gt;
&lt;p&gt;Each warp loads values from 4 distant locations in global memory. This means that the memory accesses are not coalesced: the memory bandwidth is low. Consider an \(8 \times 8 \times 8\) block. A warp of 32 threads will load 4 rows of 8 values each. &lt;strong&gt;The values within each row contiguous, but the rows are not contiguous.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;thread-coarsening&#34;&gt;Thread Coarsening&lt;/h2&gt;
&lt;p&gt;Stencils do not benefit from shared memory as much as convolutions due to the sparsity of the sampled points. Most applications of the stencil patterns work with a 3D grid, resulting in relatively small tile sizes per block.&lt;/p&gt;
&lt;p&gt;A solution to this is to increase the amount of work each thread performs, AKA &lt;em&gt;thread coarsening&lt;/em&gt;. The price paid for parallelism in the stencil pattern is the low frequency of memory reuse.&lt;/p&gt;
&lt;p&gt;Each thread performs more work in the \(z\) direction for a 3D seven-point stencil. All threads collaborate to load in a \(z\) layer at time from \(z-1\) to \(z+1\). There are then 3 different shared memory tiles per block. After computing values in the current output tile, the shared memory is rearranged for the next layer. This means that there are more transfers between shared memory as opposed to global memory.&lt;/p&gt;
&lt;p&gt;The threads are launched to work with a 2D tile at a time, so the size of the block is now \(T^2\). This means we can use a larger value for \(T\). The compute to memory ratio is almost doubled under this scheme. Additionally, the amount of shared memory required is \(3T^2\) rather than \(T^3\).&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void stencil_kernel(float *in, float *out, unsigned int N) {
    int iStart = blockIdx.z * OUT_TILE_DIM;
    int j = blockIdx.y * OUT_TILE_DIM + threadIdx.y - 1;
    int k = blockIdx.x * OUT_TILE_DIM + threadIdx.x - 1;
    __shared__ float inPrev_s[IN_TILE_DIM][IN_TILE_DIM];
    __shared__ float inCurr_s[IN_TILE_DIM][IN_TILE_DIM];
    __shared__ float inNext_s[IN_TILE_DIM][IN_TILE_DIM];

    if (iStart - 1 &amp;gt;= 0 &amp;amp;&amp;amp; iStart - 1 &amp;lt; N &amp;amp;&amp;amp; j &amp;gt;= 0 &amp;amp;&amp;amp; j &amp;lt; N &amp;amp;&amp;amp; k &amp;gt;= 0 &amp;amp;&amp;amp; k &amp;lt; N) {
        inPrev_s[threadIdx.y][threadIdx.x] = in[(iStart - 1)*N*N + j*N + k];
    }
    if (iStart &amp;gt;= 0 &amp;amp;&amp;amp; iStart &amp;lt; N &amp;amp;&amp;amp; j &amp;gt;= 0 &amp;amp;&amp;amp; j &amp;lt; N &amp;amp;&amp;amp; k &amp;gt;= 0 &amp;amp;&amp;amp; k &amp;lt; N) {
        inCurr_s[threadIdx.y][threadIdx.x] = in[iStart*N*N + j*N + k];
    }
    for (int i = iStart; i &amp;lt; iStart + OUT_TILE_DIM; i++) {
        if (i + 1 &amp;gt;= 0 &amp;amp;&amp;amp; i + 1 &amp;lt; N &amp;amp;&amp;amp; j &amp;gt;= 0 &amp;amp;&amp;amp; j &amp;lt; N &amp;amp;&amp;amp; k &amp;gt;= 0 &amp;amp;&amp;amp; k &amp;lt; N) {
            inNext_s[threadIdx.y][threadIdx.x] = in[(i + 1)*N*N + j*N + k];
        }
        __syncthreads();
        if (i &amp;gt;= 1 &amp;amp;&amp;amp; i &amp;lt; N - 1 &amp;amp;&amp;amp; j &amp;gt;= 1 &amp;amp;&amp;amp; j &amp;lt; N - 1 &amp;amp;&amp;amp; k &amp;gt;= 1 &amp;amp;&amp;amp; k &amp;lt; N - 1) {
            if (threadIdx.y &amp;gt;= 1 &amp;amp;&amp;amp; threadIdx.y &amp;lt; IN_TILE_DIM - 1 &amp;amp;&amp;amp;
                threadIdx.x &amp;gt;= 1 &amp;amp;&amp;amp; threadIdx.x &amp;lt; IN_TILE_DIM - 1) {
                out[i*N*N + j*N + k] = c0 * inCurr_s[threadIdx.y][threadIdx.x]
                    + c1 * inCurr_s[threadIdx.y][threadIdx.x - 1]
                    + c2 * inCurr_s[threadIdx.y][threadIdx.x + 1]
                    + c3 * inCurr_s[threadIdx.y - 1][threadIdx.x]
                    + c4 * inCurr_s[threadIdx.y + 1][threadIdx.x]
                    + c5 * inPrev_s[threadIdx.y][threadIdx.x]
                    + c6 * inNext_s[threadIdx.y][threadIdx.x];
            }
        }
        __syncthreads();
        inPrev_s[threadIdx.y][threadIdx.x] = inCurr_s[threadIdx.y][threadIdx.x];
        inCurr_s[threadIdx.y][threadIdx.x] = inNext_s[threadIdx.y][threadIdx.x];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This kernel is visualized below with a block size of \(6 \times 6\). The left and top sides of the tile have been removed for clarity. All of the blue blocks are loaded from global memory. The dark blue blocks represent the &lt;em&gt;active&lt;/em&gt; plane that is used to compute the corresponding output values. After the current plane is completed, the block synchronizes before moving the current values to the previous plane and loads the next plane&amp;rsquo;s values into the current plane &lt;code&gt;inCurr_s&lt;/code&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-28_15-28-25_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Visualization of the thread coarsening tile.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Visualization of the thread coarsening tile.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;register-tiling&#34;&gt;Register Tiling&lt;/h2&gt;
&lt;p&gt;In the coarsening solution presented above, each thread works with a single element in the previous and next shared memory tiles. There are 4 elements in that example that really need to be loaded from shared memory. For the 3 elements that are only required by the current thread, they can be loaded into registers.&lt;/p&gt;
&lt;p&gt;Since only the values in the \(x-y\) direction are required for shared memory, the amount of memory used is reduced by \(\frac{1}{3}\).&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void stencil_kernel(float *in, float *out, unsigned int N) {
    int iStart = blockIdx.z * OUT_TILE_DIM;
    int j = blockIdx.y * OUT_TILE_DIM + threadIdx.y - 1;
    int k = blockIdx.x * OUT_TILE_DIM + threadIdx.x - 1;
    float inPrev;
    float inCurr;
    float inNext;

    __shared__ float inCurr_s[IN_TILE_DIM][IN_TILE_DIM];

    if (iStart - 1 &amp;gt;= 0 &amp;amp;&amp;amp; iStart - 1 &amp;lt; N &amp;amp;&amp;amp; j &amp;gt;= 0 &amp;amp;&amp;amp; j &amp;lt; N &amp;amp;&amp;amp; k &amp;gt;= 0 &amp;amp;&amp;amp; k &amp;lt; N) {
        inPrev = in[(iStart - 1)*N*N + j*N + k];
    }
    if (iStart &amp;gt;= 0 &amp;amp;&amp;amp; iStart &amp;lt; N &amp;amp;&amp;amp; j &amp;gt;= 0 &amp;amp;&amp;amp; j &amp;lt; N &amp;amp;&amp;amp; k &amp;gt;= 0 &amp;amp;&amp;amp; k &amp;lt; N) {
        inCurr = in[iStart*N*N + j*N + k];
        inCurr_s[threadIdx.y][threadIdx.x] = inCurr;
    }
    for (int i = iStart; i &amp;lt; iStart + OUT_TILE_DIM; i++) {
        if (i + 1 &amp;gt;= 0 &amp;amp;&amp;amp; i + 1 &amp;lt; N &amp;amp;&amp;amp; j &amp;gt;= 0 &amp;amp;&amp;amp; j &amp;lt; N &amp;amp;&amp;amp; k &amp;gt;= 0 &amp;amp;&amp;amp; k &amp;lt; N) {
            inNext = in[(i + 1)*N*N + j*N + k];
        }
        __syncthreads();
        if (i &amp;gt;= 1 &amp;amp;&amp;amp; i &amp;lt; N - 1 &amp;amp;&amp;amp; j &amp;gt;= 1 &amp;amp;&amp;amp; j &amp;lt; N - 1 &amp;amp;&amp;amp; k &amp;gt;= 1 &amp;amp;&amp;amp; k &amp;lt; N - 1) {
            if (threadIdx.y &amp;gt;= 1 &amp;amp;&amp;amp; threadIdx.y &amp;lt; IN_TILE_DIM - 1 &amp;amp;&amp;amp;
                threadIdx.x &amp;gt;= 1 &amp;amp;&amp;amp; threadIdx.x &amp;lt; IN_TILE_DIM - 1) {
                out[i*N*N + j*N + k] = c0 * inCurr
                    + c1 * inCurr_s[threadIdx.y][threadIdx.x - 1]
                    + c2 * inCurr_s[threadIdx.y][threadIdx.x + 1]
                    + c3 * inCurr_s[threadIdx.y - 1][threadIdx.x]
                    + c4 * inCurr_s[threadIdx.y + 1][threadIdx.x]
                    + c5 * inPrev
                    + c6 * inNext;
            }
        }
        __syncthreads();
        inPrev = inCurr;
        inCurr = inNext;
        inCurr_s[threadIdx.y][threadIdx.x] = inNext_s[threadIdx.y][threadIdx.x];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The kernel always has the active plane in shared memory. Every thread collectively store the previous and next planes in registers.&lt;/p&gt;
&lt;p&gt;The larger the stencil size, the more registers are required per thread. In this case, a tradeoff between shared memory space and register usage could be made. This will be explored in your lab.&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Stencils are a useful pattern for solving differential equations. They have some similarities to convolutions, but present unique challenges in terms of optimization. The sparsity of the pattern means that shared memory is not as effective as it is for convolutions. Thread coarsening and register tiling are two techniques that can be used to improve performance.&lt;/p&gt;
&lt;h2 id=&#34;questions&#34;&gt;Questions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;How many registers per thread are required for a 3D seven-point stencil, 3D nine-point stencil, and 3D 27-point stencil?&lt;/li&gt;
&lt;li&gt;How do convolutions relate to stencil patterns? Could you implement a stencil pattern using a convolution filter?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GPU Pattern: Convolution</title>
      <link>https://ajdillhoff.github.io/notes/gpu_pattern_convolution/</link>
      <pubDate>Mon, 15 Jan 2024 21:35:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/gpu_pattern_convolution/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#convolution&#34;&gt;Convolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#properties-of-convolutions&#34;&gt;Properties of Convolutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementing-a-convolution-kernel&#34;&gt;Implementing a Convolution Kernel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#constant-memory-and-caching&#34;&gt;Constant Memory and Caching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tiled-convolutions&#34;&gt;Tiled Convolutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#caching-the-halo-cells&#34;&gt;Caching the Halo Cells&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;This pattern involves tiling and input data staging.&lt;/p&gt;
&lt;p&gt;Recall from Lab 1 where we implementing a kernel to blur an image. This kernel worked on each individual output pixel by computing the weighted average of each pixel from the input image, centered on the output pixel location. When we implemented this, we set the weight of every pixel to 1. Whether you were aware of it or not, you implemented a convolution between an input image and weighted kernel.&lt;/p&gt;
&lt;p&gt;The convolution is an extremely important operator that works on time-varying signals of different dimensionality. In computer vision, they are commonly used to compute responses between a known pattern and an input image. The pixels that return a greater response as a result of convolution indicate a match between the pattern and the input image.&lt;/p&gt;
&lt;p&gt;This operator can and has been efficiently implemented in a GPU. Through studying this pattern, you will learn to utilize constant memory storage and shared memory storage to efficiently implement a convolution kernel. These techniques will be useful in later applications as well.&lt;/p&gt;
&lt;h2 id=&#34;convolution&#34;&gt;Convolution&lt;/h2&gt;
&lt;p&gt;A convolution is a function that takes two functions as input and produces a third function as output. The first function is the input and the second function is the kernel. The output is called the feature map. The kernel is also sometimes called the filter.&lt;/p&gt;
&lt;p&gt;\[
(f * g)(t) = \int f(t-a)g(a)da
\]&lt;/p&gt;
&lt;p&gt;We can view them more concretely by considering the functions to be vectors. For example, let the function \(f\) be an input vector \(x\) and \(w\) be a kernel representing a filter. The convolution operator is then&lt;/p&gt;
&lt;p&gt;\[
(x * w)(t) = \int x(t-a)w(a)da.
\]&lt;/p&gt;
&lt;p&gt;The result the &lt;strong&gt;feature map&lt;/strong&gt; representing the response of the kernel at each location in the input.&lt;/p&gt;
&lt;p&gt;In the case of discrete values, it is common to use an odd-sized kernel and center it on an input value. The kernel size is given by some radius \(r\). The convolution operator is then&lt;/p&gt;
&lt;p&gt;\[
(x * w)(t) = \sum_{-r}^r x(t-r)w( r).
\]&lt;/p&gt;
&lt;p&gt;The figure below shows an example of a 1D convolution of a vector if size 8 with a kernel of size 5, centered on the \(t = 2\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-17_18-00-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;1D Convolution between a vector of size 8 and a kernel of size 5.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;1D Convolution between a vector of size 8 and a kernel of size 5.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Convolution is defined in such a way that the kernel is traversed in an inverted manner. In the example above, \(y_2\) is computed by applying the kernel to \(\mathbf{x}\) centered on \(x_2\). The calculation in terms of the locations accesses is&lt;/p&gt;
&lt;p&gt;\[
y_2 = x_4 w_{-2} + x_3 w_{-1} + x_2 w_0 + x_1 w_1 + x_0 w_2.
\]&lt;/p&gt;
&lt;p&gt;This operation is very similar to the &lt;em&gt;correlation&lt;/em&gt; operator, which is defined as&lt;/p&gt;
&lt;p&gt;\[
(x \star w)(t) = \sum_{-r}^r x(t+r)w( r).
\]&lt;/p&gt;
&lt;p&gt;We can use the correlation operator to compute the convolution by flipping the kernel. In this case, the calculation can be represented using the dot product. We can also slightly adjust the indexing so that the first index is 0.&lt;/p&gt;
&lt;p&gt;\[
y_i = \sum_{k=0}^{2r} x_{i+k-r} w_{2r-k}.
\]&lt;/p&gt;
&lt;p&gt;Note that the convolution shown above would be undefined for \(i = 0\) and \(i = 1\) since the kernel would be accessing negative indices. Based on the definition, we would ignore these values. This is called a &lt;em&gt;valid&lt;/em&gt; convolution. The output size is then \(n - 2r\). There is also a &lt;em&gt;full&lt;/em&gt; convolution where the output size is \(n\). In this case, the kernel is padded with zeros so that it can be applied to all elements of the input.&lt;/p&gt;
&lt;h3 id=&#34;2d-convolution&#34;&gt;2D Convolution&lt;/h3&gt;
&lt;p&gt;Image convolutions use 2D filters applied to 2D images. For a filter with radius \(r\), size of the filter is \((2r + 1) \times (2r + 1)\). The convolution is then&lt;/p&gt;
&lt;p&gt;\[
(x * w)(i, j) = \sum_{-r}^r \sum_{-r}^r x(i-r, j-r)w(r, s).
\]&lt;/p&gt;
&lt;h2 id=&#34;properties-of-convolutions&#34;&gt;Properties of Convolutions&lt;/h2&gt;
&lt;p&gt;Convolutional networks are commonly built on &lt;em&gt;full&lt;/em&gt; or &lt;em&gt;valid&lt;/em&gt; convolutions. Other variants have also been explored. Here, we will briefly discuss the different properties of this operator. A more detailed treatment can be found in (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;padding&#34;&gt;Padding&lt;/h3&gt;
&lt;p&gt;By definition, a convolution of an input with a filter of size \(n\times n\) will produce an output of size \((m-n+1)\times(m-n+1)\), where \(m\) is the size of the input. This means that the output will be smaller than the input. This is often referred to as a &lt;strong&gt;valid&lt;/strong&gt; convolution. The figure below shows a convolution between a \(3\times3\) kernel and a \(5\times5\) input.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-26_16-31-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A valid convolution (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A valid convolution (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The output of this convolution is a \(3\times3\) feature map. This is a problem if we want to build a deep network. Each convolution will reduce the size of the input. If we were to stack multiple convolutional layers, the output would eventually be too small to be useful. If we want our output to be same size as the input, we can add padding to the original input image before convolving it. This is often known as a &lt;strong&gt;full&lt;/strong&gt; convolution. An example is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-26_16-34-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;A full convolution (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;A full convolution (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;stride&#34;&gt;Stride&lt;/h3&gt;
&lt;p&gt;So far, we have only looked at convolutions which step by 1 unit as they shift over the image. We can control the size of this step, or &lt;strong&gt;stride&lt;/strong&gt;, to produce different outcomes. Picking a non-unit stride has a number of effects on the features that are learned in a convolutional neural network.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dimensionality reduction&lt;/strong&gt;: Skipping over pixels reduces the size of the output feature map. This provides another way of downsampling the input.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Less computation&lt;/strong&gt;: Fewer computations are required to produce the output feature map.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Increased field of view&lt;/strong&gt;: A larger stride increases the field of view of the kernel, leading to larger receptive fields in deeper layers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given an input of size \(m\times m\) and a kernel of size \(n\times n\), the output size of a convolution with stride \(s\) is given by&lt;/p&gt;
&lt;p&gt;\[
\left\lfloor\frac{m-n}{s}\right\rfloor + 1.
\]&lt;/p&gt;
&lt;p&gt;The figure below shows a convolution with stride 2 on a \(5\times5\) input.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-26_16-45-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;A convolution with stride 2 (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;A convolution with stride 2 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;kernel-size&#34;&gt;Kernel Size&lt;/h3&gt;
&lt;p&gt;The size of the kernel has a large impact on the features that are learned. A larger kernel will have a larger receptive field. This means that the kernel will be able to capture more information about the input. However, this comes at the cost of increased computation. Common kernel sizes in most CNNs are \(3\times3\), \(5\times5\), and \(7\times7\). It is also convenient to pick an odd kernel size so that the kernel has a center pixel.&lt;/p&gt;
&lt;h3 id=&#34;dilation&#34;&gt;Dilation&lt;/h3&gt;
&lt;p&gt;Around 2015, a research trend for CNNs was to find a way to increase the receptive field without adding more parameters. The result is a &lt;strong&gt;dilated&lt;/strong&gt; convolution. The output of a dilated convolution is computed by skipping over pixels in the input. The figure below shows a \(3\times3\) kernel with a dilation of 2.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-27_08-19-10_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;A dilated convolution (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;A dilated convolution (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The output size is computed as&lt;/p&gt;
&lt;p&gt;\[
\left\lfloor\frac{m + 2p - n - (n-1)(d-1)}{s}\right\rfloor + 1,
\]&lt;/p&gt;
&lt;p&gt;where \(p\) is the amount of padding and \(d\) is the dilation factor.&lt;/p&gt;
&lt;h2 id=&#34;implementing-a-convolution-kernel&#34;&gt;Implementing a Convolution Kernel&lt;/h2&gt;
&lt;p&gt;It is straightforward to write the convolution operation in CUDA C++. Each thread will compute the value for a single output pixel using the filter. We already implemented something very similar with the blurring kernel. The kernel itself should accept the following arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The input image&lt;/li&gt;
&lt;li&gt;The output image&lt;/li&gt;
&lt;li&gt;The kernel&lt;/li&gt;
&lt;li&gt;The radius of the kernel&lt;/li&gt;
&lt;li&gt;The width of the output image&lt;/li&gt;
&lt;li&gt;The height of the output image&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A more robust implementation would consider things like padding, stride, dilation, and whether or not a valid or full convolution is desired. For now, we will focus on the simplest case: a valid convolution with a stride of 1 and no padding or dilation. First, let&amp;rsquo;s review the initial naive solution from &lt;em&gt;Programming Massively Parallel Processors&lt;/em&gt; (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void conv2D(float *input, float *filter, float *output,
                       int r, int width, int height) {
    int outCol = blockIdx.x * blockDim.x + threadIdx.x;
    int outRow = blockIdx.y * blockDim.y + threadIdx.y;

    float sum = 0.0f;
    for (int row = 0; row &amp;lt; 2*r+1; row++) {
        for (int col = 0; col &amp;lt; 2*r+1; col++) {
            int inRow = outRow + row - r;
            int inCol = outCol + col - r;
            if (inRow &amp;gt;= 0 &amp;amp;&amp;amp; inRow &amp;lt; height &amp;amp;&amp;amp; inCol &amp;gt;= 0 &amp;amp;&amp;amp; inCol &amp;lt; width) {
                sum += input[inRow * width + inCol] * filter[row * (2*r+1) + col];
            }
        }
    }
    output[outRow * width + outCol] = sum;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With this kernel, the input and output sizes are assumed to be the same. There is a boundary check in the inner-most loop to account for pixels for which a convolution cannot be computed. Based on this check, we can see that this kernel is performing a &lt;em&gt;valid&lt;/em&gt; convolution. The extra \(2r\) pixels on each side of the output are skipped. This presents a computational problem in the form of control divergence. Recall that all threads in a warp must execute the same instruction. If the boundary check fails for some threads, they will still execute the instructions in the loop, but will not contribute to the output. This is a waste of resources.&lt;/p&gt;
&lt;p&gt;It is also a waste of resources in terms of memory used for the output. If we already know that we want to perform a valid convolution, we can allocate the output image to be the appropriate size before calling it. A slightly modified version is shown below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void conv2D(float *input, float *filter, float *output,
                       int r, int width, int height) {
    int outCol = blockIdx.x * blockDim.x + threadIdx.x;
    int outRow = blockIdx.y * blockDim.y + threadIdx.y;

    float sum = 0.0f;
    for (int row = 0; row &amp;lt; 2*r+1; row++) {
        for (int col = 0; col &amp;lt; 2*r+1; col++) {
            int inRow = outRow + row;
            int inCol = outCol + col;
            sum += input[inRow * width + inCol] * filter[row * (2*r+1) + col];
        }
    }
    output[outRow * width + outCol] = sum;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;constant-memory-and-caching&#34;&gt;Constant Memory and Caching&lt;/h2&gt;
&lt;p&gt;There is a much larger issue present in both versions of this kernel in terms of memory bandwidth. Similar to the matrix multiplication kernel, this kernel can benefit from tiling. However, there is a new problem that arises specifically with convolution. The same filter is accessed by every single thread. This filter does not change for the entire duration of the kernel. This means that we are wasting memory bandwidth by having every thread access the same filter.&lt;/p&gt;
&lt;p&gt;Given its relatively small size, this kernel is a perfect candidate for constant memory. This is a special type of memory that is cached on the GPU. It is read-only and has a limited size, but it is much faster than global memory. We can write to the devices constant memory from the host code.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;#define FILTER_RADIUS 1
__constant__ float kFilter_d[2*FILTER_RADIUS+1][2*FILTER_RADIUS+1];
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This informs the compiler to allocate a 2D array of floats in constant memory. The size of the array is determined by the constant `FILTER_RADIUS`. We can then copy the filter to the device using the `cudaMemcpyToSymbol` function.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;cudaMemcpyToSymbol(kFilter_d, filter_h, (2*FILTER_RADIUS+1)*(2*FILTER_RADIUS+1)*sizeof(float));
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The line above assumes there is some data on the host in the array `filter_h`. This array is copied to the device. A small note on naming convention, &lt;a href=&#34;https://google.github.io/styleguide/cppguide.html#Constant_Names&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Google&amp;rsquo;s C++&lt;/a&gt; style guide recommends naming constant variables with a &lt;code&gt;k&lt;/code&gt; prefix. I have adopted this convention here.&lt;/p&gt;
&lt;p&gt;At this point, &lt;code&gt;kFilter_d&lt;/code&gt; is accessible from the kernel as a global variable. There is no need to pass it as an argument. The kernel can be modified to use this constant memory as follows.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void conv2D(float *input, float *output,
                       int r, int width, int height) {
    int outCol = blockIdx.x * blockDim.x + threadIdx.x;
    int outRow = blockIdx.y * blockDim.y + threadIdx.y;

    float sum = 0.0f;
    for (int row = 0; row &amp;lt; 2*r+1; row++) {
        for (int col = 0; col &amp;lt; 2*r+1; col++) {
            int inRow = outRow + row;
            int inCol = outCol + col;
            sum += input[inRow * width + inCol] * F_d[row][col];
        }
    }
    output[outRow * width + outCol] = sum;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you organize your files such that the kernel is in a separate file from the host code, you will need to declare the constant variable in the kernel file as well.&lt;/p&gt;
&lt;p&gt;Constant memory variables are stored in DRAM with global memory. The CUDA runtime will cache them since it knows they will not be modified. Processors use caches to reduce the latency of memory accesses by keeping frequently used data in a small, fast memory that is often located directly on the chip. This type of &lt;em&gt;constant cache&lt;/em&gt; is preferable to one that would support high-throughput writes in terms of chip design. It would require specialized hardware to support both which would increase the cost of the chip.&lt;/p&gt;
&lt;h2 id=&#34;tiled-convolutions&#34;&gt;Tiled Convolutions&lt;/h2&gt;
&lt;p&gt;Even with caching, the convolutional kernel still makes many accesses to DRAM. Similar to matrix multiplication, we can tile the input image to reduce the number of accesses. Similar to that example, we will use a \(4 \times 4\) tile size. If the input is a \(16 \times 16\) image and we apply a kernel with radius \(r=2\), the output image under a valid convolution will be \(12 \times 12\). This is visualized in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-19_16-35-39_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Left: The input image and its tiling. Middle: the filter. Right: The output image and its tiling.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Left: The input image and its tiling. Middle: the filter. Right: The output image and its tiling.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The parallel solution to this problem will follow the tiled approach used for matrix multiplication. One key difference in this case is that the input tile size will be larger than the output tile size. This size difference would further be complicated if we left the kernel size as a parameter.&lt;/p&gt;
&lt;p&gt;Following the design presented by (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;) in Chapter 7, there are two immediate approaches to this problem based on the tile size. The first is to choose a block size that matches the size of the input tiles. The benefit to this approach is that each thread can load a single input element into shared memory. The drawback is that some of the threads will be disabled when computing the output value since the output tile is smaller. This is a form of control divergence and will result in wasted resources.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;#define FILTER_RADIUS 1
#define IN_TILE_DIM 4
#define OUT_TILE_DIM ((IN_TILE_DIM) - 2*(FILTER_RADIUS))
__constant__ float kFilter_d[2*FILTER_RADIUS+1][2*FILTER_RADIUS+1];
__global__ void conv2DTiledConstKernel(float *input, float *output,
                                       int width, int height) {
    __shared__ float inputTile[IN_TILE_DIM][IN_TILE_DIM];
    // Input tile coordinates
    int col = blockIdx.x * OUT_TILE_DIM + threadIdx.x;
    int row = blockIdx.y * OUT_TILE_DIM + threadIdx.y;
    if (row &amp;lt; height &amp;amp;&amp;amp; col &amp;lt; width) {
        inputTile[threadIdx.y][threadIdx.x] = input[row * width + col];
    } else {
        inputTile[threadIdx.y][threadIdx.x] = 0.0f;
    }
    __syncthreads();

    // Output tile coordinates
    int tileCol = threadIdx.x - FILTER_RADIUS;
    int tileRow = threadIdx.y - FILTER_RADIUS;

    // In a valid convolution, the output is smaller than the input
    row -= FILTER_RADIUS;
    col -= FILTER_RADIUS;

    if (tileCol &amp;gt;= 0 &amp;amp;&amp;amp; tileCol &amp;lt; OUT_TILE_DIM &amp;amp;&amp;amp; tileRow &amp;gt;= 0 &amp;amp;&amp;amp; tileRow &amp;lt; OUT_TILE_DIM) {
        float sum = 0.0f;
        for (int fRow = 0; fRow &amp;lt; 2*FILTER_RADIUS+1; fRow++) {
            for (int fCol = 0; fCol &amp;lt; 2*FILTER_RADIUS+1; fCol++) {
                sum += inputTile[tileRow + fRow][tileCol + fCol] * kFilter_d[fRow][fCol];
            }
        }
        output[row * (width - 2 * FILTER_RADIUS) + col] = sum;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;There are a few things to consider here. The first phase of this kernel collaboratively loads data into a shared memory space, similar to what we have seen before. This kernel assumes a convenient indexing scheme where the row and column will always be &amp;gt;= 0. We could adopt a scheme that centers the convolution on the center point of the kernel by allowing for negative indices. In this case, it would be necessary to check if the row and column are less than 0. This implementation only needs to verify that the row and column are within the given image size.&lt;/p&gt;
&lt;p&gt;When it comes to computing the output, not every thread will contribute. This is depicted by the lightly shaded areas in the figure below. You should also note which threads are active for output computation per block. In this simple example, a \(3 \times 3\) filter is used. The input tile dimension is \(4 \times 4\) which means the output tile will be \(2 \times 2\). Only the threads corresponding to the darker blue on the left contribute to the output calculation. Since this one block computes 4 output values, the next block should start 2 units to the right of this one.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-21_16-08-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;The active threads for computing the output tile.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;The active threads for computing the output tile.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;performance-analysis&#34;&gt;Performance Analysis&lt;/h3&gt;
&lt;p&gt;The purpose of this approach was to increase the ratio of arithmetic operations to global memory accesses. For the threads that compute an output tile, there is one multiplication and one addition which yields \(\mathtt{OUT\_TILE\_DIM}^2*(2*\mathtt{FILTER\_RADIUS} + 1)^2*2\) operations total. Each thread in the input tile loads a single &lt;code&gt;float&lt;/code&gt; value for a total of \(\mathtt{IN\_TILE\_DIM}^2 * 4\) bytes. For our small example above, this gives&lt;/p&gt;
&lt;p&gt;\[
\frac{2^2 * 3^2 * 2}{4^2 * 4} = 1.125\ \text{Ops/byte}.
\]&lt;/p&gt;
&lt;p&gt;In a more realistic example, we would maximize our input tile size to take advantage of the available threads on the device. Currently, the maximum number of supported threads is 1024. This allows for an input tile size of \(32 \times 32\). The resulting operations per byte under this tile size is&lt;/p&gt;
&lt;p&gt;\[
\frac{30^2 * 3^2 * 2}{32^2 * 4} = 3.955\ \text{Ops/byte}.
\]&lt;/p&gt;
&lt;p&gt;This ratio increases with the size of the filter.&lt;/p&gt;
&lt;h2 id=&#34;caching-the-halo-cells&#34;&gt;Caching the Halo Cells&lt;/h2&gt;
&lt;p&gt;In the previous example, the size of the input tile compared to the output tile means that there were some threads that did not contribute to the output computation. These are the threads managing the lightly shaded cells in the figure above. We will refer to these as &lt;em&gt;halo cells&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This implementation is going to take advantage of the caching behavior in the chip itself. &lt;strong&gt;Values that have been recently used are more likely to already be in L2 cache.&lt;/strong&gt; This is a safe assumption since the neighboring blocks will have loaded these values into shared memory. This means that the input and output tile sizes can be the same; there is no need to waste any threads in the block. The full kernel is given below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void conv2DTiledCachedConstKernel(float *input, float *output,
                                             int width, int height) {
    __shared__ float inputTile[IN_TILE_DIM][IN_TILE_DIM];
    // Input tile coordinates
    int col = blockIdx.x * IN_TILE_DIM + threadIdx.x;
    int row = blockIdx.y * IN_TILE_DIM + threadIdx.y;
    if (row &amp;lt; height &amp;amp;&amp;amp; col &amp;lt; width) {
        inputTile[threadIdx.y][threadIdx.x] = input[row * width + col];
    } else {
        inputTile[threadIdx.y][threadIdx.x] = 0.0f;
    }
    __syncthreads();

    if (row &amp;lt; FILTER_RADIUS || col &amp;lt; FILTER_RADIUS || col &amp;gt;= (width - FILTER_RADIUS) || row &amp;gt;= (height - FILTER_RADIUS)) return;

    // Output tile coordinates
    row -= FILTER_RADIUS;
    col -= FILTER_RADIUS;
    int tileCol = threadIdx.x - FILTER_RADIUS;
    int tileRow = threadIdx.y - FILTER_RADIUS;

    float sum = 0.0f;
    for (int fRow = 0; fRow &amp;lt; 2 * FILTER_RADIUS + 1; fRow++) {
        for (int fCol = 0; fCol &amp;lt; 2 * FILTER_RADIUS + 1; fCol++) {
            // If this value is in shared memory, access it there
            if (tileCol + fCol &amp;gt;= 0 &amp;amp;&amp;amp;
                tileCol + fCol &amp;lt; IN_TILE_DIM &amp;amp;&amp;amp;
                tileRow + fRow &amp;gt;= 0 &amp;amp;&amp;amp;
                tileRow + fRow &amp;lt; IN_TILE_DIM) {
                sum += inputTile[tileRow + fRow][tileCol + fCol] * kFilter_d[fRow][fCol];
            } else {
                // Otherwise, access it from global memory
                sum += input[(row + fRow) * width + (col + fCol)] * kFilter_d[fRow][fCol];
            }
        }
    }

    output[row * (width - 2 * FILTER_RADIUS) + col] = sum;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Dumoulin, Vincent, and Francesco Visin. 2018. “A Guide to Convolution Arithmetic for Deep Learning.” &lt;i&gt;Arxiv:1603.07285 [Cs, Stat]&lt;/i&gt;, January. &lt;a href=&#34;http://arxiv.org/abs/1603.07285&#34;&gt;http://arxiv.org/abs/1603.07285&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Profiling CUDA Applications</title>
      <link>https://ajdillhoff.github.io/notes/profiling_cuda_applications/</link>
      <pubDate>Mon, 15 Jan 2024 14:48:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/profiling_cuda_applications/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#overview-of-nsight&#34;&gt;Overview of Nsight&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getting-started-with-nsight&#34;&gt;Getting Started with Nsight&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-matrix-multiplication&#34;&gt;Case Study: Matrix Multiplication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tips-and-best-practices&#34;&gt;Tips and Best Practices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ocl-notes&#34;&gt;OCL Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;overview-of-nsight&#34;&gt;Overview of Nsight&lt;/h2&gt;
&lt;p&gt;NVIDIA NSight Compute is a profiling tool for CUDA kernels. It features an expert system that can help you identify performance bottlenecks in your code. It is essential for methodically optimizing your code. These notes will cover the basics of using Nsight Compute to profile your CUDA applications.&lt;/p&gt;
&lt;h2 id=&#34;getting-started-with-nsight&#34;&gt;Getting Started with Nsight&lt;/h2&gt;
&lt;h3 id=&#34;profiling-our-first-program&#34;&gt;Profiling our first program&lt;/h3&gt;
&lt;p&gt;In Lab 0, you implemented a vector addition kernel that is &lt;em&gt;embarrassingly parallel&lt;/em&gt;. We will now use Nsight to profile its performance. Realistically, there is not much we can do to increase the performance of this kernel, but it will still help us understand the information that Nsight gives. To profile the application, simply launch &lt;code&gt;ncu&lt;/code&gt; with your application.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ncu ./build/main&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Depending on where you are running this program, it may be necessary to launch it with &lt;code&gt;sudo&lt;/code&gt;. If everything was successful, it will output something similar to the following:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nsight Output&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vec_add(float *, float *, float *, int), 2024-Jan-16 10:42:52, Context 1, Stream 7
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Section: GPU Speed Of Light Throughput
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  ---------------------------------------------------------------------- --------------- ------------------------------
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  DRAM Frequency                                                           cycle/nsecond                           5.71
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  SM Frequency                                                             cycle/nsecond                           1.15
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Elapsed Cycles                                                                   cycle                          3,279
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Memory [%]                                                                           %                           7.54
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  DRAM Throughput                                                                      %                           7.54
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Duration                                                                       usecond                           2.85
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  L1/TEX Cache Throughput                                                              %                           4.32
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  L2 Cache Throughput                                                                  %                           4.86
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  SM Active Cycles                                                                 cycle                         623.58
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Compute (SM) [%]                                                                     %                           0.82
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  ---------------------------------------------------------------------- --------------- ------------------------------
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        waves across all SMs. Look at Launch Statistics for more details.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Section: Launch Statistics
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  ---------------------------------------------------------------------- --------------- ------------------------------
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Block Size                                                                                                        256
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Function Cache Configuration                                                                  cudaFuncCachePreferNone
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Grid Size                                                                                                          16
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Registers Per Thread                                                   register/thread                             16
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Shared Memory Configuration Size                                                 Kbyte                           8.19
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Driver Shared Memory Per Block                                             Kbyte/block                           1.02
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Dynamic Shared Memory Per Block                                             byte/block                              0
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Static Shared Memory Per Block                                              byte/block                              0
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Threads                                                                         thread                          4,096
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Waves Per SM                                                                                                     0.07
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  ---------------------------------------------------------------------- --------------- ------------------------------
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  WRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU&amp;#39;s 38
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        concurrently with other workloads, consider reducing the block size to have at least one block per
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        description for more details on launch configurations.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Section: Occupancy
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  ---------------------------------------------------------------------- --------------- ------------------------------
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Block Limit SM                                                                   block                             16
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Block Limit Registers                                                            block                             16
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Block Limit Shared Mem                                                           block                            100
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Block Limit Warps                                                                block                              6
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Theoretical Active Warps per SM                                                   warp                             48
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Theoretical Occupancy                                                                %                            100
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Achieved Occupancy                                                                   %                          15.85
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Achieved Active Warps Per SM                                                      warp                           7.61
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  ---------------------------------------------------------------------- --------------- ------------------------------
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  WRN   This kernel&amp;#39;s theoretical occupancy is not impacted by any block limit. The difference between calculated
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        theoretical (100.0%) and measured achieved occupancy (15.9%) can be the result of warp scheduling overheads
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        as well as across blocks of the same kernel. See the CUDA Best Practices Guide
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        optimizing occupancy.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;viewing-results-in-the-gui&#34;&gt;Viewing Results in the GUI&lt;/h3&gt;
&lt;p&gt;Nsight comes with both CLI and GUI clients. It is recommended to parse the information from the GUI. The GUI can launch programs both locally and remotely. It can also display the result of a previous launch. To generate a profiling report for our vector addition kernel, run the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ncu -o vec_add_profile ./build/main&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The argument after &lt;code&gt;-o&lt;/code&gt; is the name of the output file. Open Nsight Compute and load the saved file. It should look something like this.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-16_11-59-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Nsight Compute GUI output&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Nsight Compute GUI output
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This basic report only includes three sections: GPU Speed of Light, Launch Statistics, and Occupancy Analysis. We will go over each of these sections in detail.&lt;/p&gt;
&lt;h3 id=&#34;gpu-speed-of-light&#34;&gt;GPU Speed of Light&lt;/h3&gt;
&lt;p&gt;This section displays high level aspects of your kernel. The main metrics report what your application is doing relative to peak performance. Sparing the full details of the documentation, the most important metrics are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Duration: The total time spent executing the kernel. This is the most important metric for performance.&lt;/li&gt;
&lt;li&gt;SM &lt;code&gt;[%]&lt;/code&gt;: The relative throughput of the SMs as compared to the theoretical maximum.&lt;/li&gt;
&lt;li&gt;Memory &lt;code&gt;[%]&lt;/code&gt;: The relative throughput of the memory as compared to the theoretical maximum.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Do not get lost in the numbers!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Remember that this tool is simply reporting facts about your kernel. Take care not to misinterpret the data. In the run from above, the kernel throughput is only 0.85%. There are a number of reasons as to why this number is so low.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Latency Issues: The kernel may have to wait for memory operations, resulting in a low throughput.&lt;/li&gt;
&lt;li&gt;Workload Characteristics: Your particular kernel may not need to do much work, resulting in a low throughput.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;launch-statistics&#34;&gt;Launch Statistics&lt;/h3&gt;
&lt;p&gt;This section shows us the launch configuration that was used for this kernel. In our earlier programs, we may set these manually for testing. Later on, we will want our programs to adapt to changing input sizes, so these statistics will becomes more useful.&lt;/p&gt;
&lt;p&gt;More importantly, this shows you the resource usage per block.&lt;/p&gt;
&lt;p&gt;If you are profiling an application for which you are not familiar with the code, it is convenient to know the grid and block sizes that were used when launching the kernel.&lt;/p&gt;
&lt;h3 id=&#34;occupancy-analysis&#34;&gt;Occupancy Analysis&lt;/h3&gt;
&lt;h3 id=&#34;memory-workload-analysis&#34;&gt;Memory Workload Analysis&lt;/h3&gt;
&lt;h2 id=&#34;case-study-matrix-multiplication&#34;&gt;Case Study: Matrix Multiplication&lt;/h2&gt;
&lt;h2 id=&#34;tips-and-best-practices&#34;&gt;Tips and Best Practices&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Do not confuse high throughput for high performance. Throughput is a measure of how much work is being done, not how fast it is being done.&lt;/li&gt;
&lt;li&gt;Using a larger grid size is not always better. More grids introduce more overhead and can lead to lower performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ocl-notes&#34;&gt;OCL Notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Analysis Driven Optimization&lt;/li&gt;
&lt;li&gt;Understanding Performance Limiters&lt;/li&gt;
&lt;li&gt;Metrics Review&lt;/li&gt;
&lt;li&gt;Memory Bound Analysis&lt;/li&gt;
&lt;li&gt;Compute Bound Analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Goals&lt;/strong&gt;
Make efficient use of memory subsystem
Expose enough parallelism to hide latency&lt;/p&gt;
&lt;h3 id=&#34;analysis-driven-optimization&#34;&gt;Analysis Driven Optimization&lt;/h3&gt;
&lt;p&gt;Cyclical process&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Profile&lt;/li&gt;
&lt;li&gt;Determine Limiter&lt;/li&gt;
&lt;li&gt;Inspect&lt;/li&gt;
&lt;li&gt;Optimize&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Determine if memory or compute bound. If neither, analyze where the latency is coming from.&lt;/p&gt;
&lt;h3 id=&#34;metrics&#34;&gt;Metrics&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Latency&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sm efficiency&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dram utilization&lt;/li&gt;
&lt;li&gt;L2 utilization&lt;/li&gt;
&lt;li&gt;shared utilization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Compute&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DP utilization&lt;/li&gt;
&lt;li&gt;SP utilization&lt;/li&gt;
&lt;li&gt;HP utilization&lt;/li&gt;
&lt;li&gt;TC utilization&lt;/li&gt;
&lt;li&gt;Integer&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>GPU Performance Basics</title>
      <link>https://ajdillhoff.github.io/notes/gpu_performance_basics/</link>
      <pubDate>Sun, 14 Jan 2024 13:31:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/gpu_performance_basics/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#memory-coalescing&#34;&gt;Memory Coalescing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hiding-memory-latency&#34;&gt;Hiding Memory Latency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#thread-coarsening&#34;&gt;Thread Coarsening&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimization-checklist&#34;&gt;Optimization Checklist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#identifying-bottlenecks&#34;&gt;Identifying Bottlenecks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-takeaway&#34;&gt;The Takeaway&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;These notes are on &amp;ldquo;Chapter 6: Performance Considerations&amp;rdquo; from the book &lt;em&gt;Programming Massively Parallel Processors&lt;/em&gt; (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;memory-coalescing&#34;&gt;Memory Coalescing&lt;/h2&gt;
&lt;p&gt;Global memory accesses are one of the largest bottlenecks in GPU applications.
DRAM has high latency based on its design. Each cell has a transistor and a capacitor. If the capacitor is charged, it represents a 1. The process to detect the charges in these cells is on the order of 10s of nanoseconds. DRAM can read consecutive groups of cells via &lt;em&gt;bursts&lt;/em&gt;. This means that if the data we wish to access is stored consecutively, it can be accessed within the same burst. Contrast that was random access, in which the DRAM will have to make multiple bursts to read the required data. &lt;strong&gt;Memory coalescing&lt;/strong&gt; refers to optimizing our global memory accesses to take advantage of DRAM bursts.&lt;/p&gt;
&lt;p&gt;Matrices are &lt;em&gt;naturally coalesced&lt;/em&gt;, so we have already been utilizing this performance pattern in previous examples.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-15_13-00-31_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Memory accesses for a matrix in row-major ordering (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Memory accesses for a matrix in row-major ordering (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Strategies to optimize code for memory coalescing are to rearrange the threads, the data, or transfer the data first to shared memory so that accesses are faster, referred to as &lt;strong&gt;corner turning&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;example-matrix-multiplication&#34;&gt;Example: Matrix Multiplication&lt;/h3&gt;
&lt;p&gt;Consider \(C = AB\), where \(A\) is in row-major order and \(B\) is in column-major order. The naive implementation of this algorithm will have poor memory coalescing. The figure below demonstrates the memory accesses for this scenario. The values required are not consecutive in memory, so the DRAM will have to make multiple bursts to read the data.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-14_20-47-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Memory accesses for a matrix in column-major ordering (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Memory accesses for a matrix in column-major ordering (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The accesses to the elements in \(B\) will be slower since the data is not coalesced. Accessing the elements &lt;em&gt;is&lt;/em&gt; efficient if we assign \(N\) consecutive threads to load \(N\) consecutive elements from the same column of \(B\). This works in conjunction with tiling. The original loads to shared memory pull from consecutive elements in \(B\) which allows the application to take advantage of DRAM bursts. Once the data is in shared memory, the rest of the algorithm can be performed with coalesced accesses. Shared memory uses SRAM instead of DRAM, so coalescing is not an issue.&lt;/p&gt;
&lt;h2 id=&#34;hiding-memory-latency&#34;&gt;Hiding Memory Latency&lt;/h2&gt;
&lt;p&gt;DRAMS have &lt;em&gt;banks&lt;/em&gt; and &lt;em&gt;channels&lt;/em&gt;. A controller has a bus that connects banks to the processor. When the DRAM accesses data, the decoder enables the cells so that they can share the information stored with the sensing amplifier. This presents a high latency relative to the time it takes to actually transfer the data. This is why there are multiple banks per channel. The controller can initiate accesses on other banks instead of sitting and waiting for a single bank to finish.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-14_17-19-55_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Single versus Multi-bank burst timings (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Single versus Multi-bank burst timings (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;It is possible that the controller will initiate a request to a bank that is already busy. This is called a &lt;em&gt;bank conflict&lt;/em&gt;. The controller will have to wait for the bank to finish its current request before it can service the new request. The more banks that are available, the less likely it is that a bank conflict will occur.&lt;/p&gt;
&lt;h3 id=&#34;example-matrix-multiplication&#34;&gt;Example: Matrix Multiplication&lt;/h3&gt;
&lt;p&gt;Consider DRAM with 4 channels and 2 banks per channel. The burst size of this DRAM is 8 bytes, or 2 elements. When data is written to DRAM in the first place, it is distributed in an interleaved fashion across the different channels and banks. The first figure below shows the input matrix \(M\) and output matrix \(P\). The second input matrix is omitted for brevity. The indices of \(M\) are linearized in row-major order to show how they are distributed across the DRAM banks. \(P\) is split into 4 blocks of size \(2 \times 2\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-15_13-55-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Matrix M with linearized indices and matrix P split into 4 blocks.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Matrix M with linearized indices and matrix P split into 4 blocks.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;\(M\) is loaded into DRAM in an interleaved fashion. The first 8 bytes are loaded into bank 0 of channel 0. The next 8 bytes go into bank 0 of channel 1, and so on. Each burst returns 8 bytes. While the first access is being performed on bank 0 channel 0, the controller can initiate a request to bank 0 channel 1. This is visualized in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-15_14-14-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;DRAM distribution for matrix M.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;DRAM distribution for matrix M.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Given the distribution visualized above, we can see that the data accesses for the blocks of \(P\) will be coalesced. Output tile 1 in matrix \(P\) requires $M_0, M_1, M_4,$ and \(M_5\). The first two are available in a single burst from channel 0 bank 0, and the second two are available in a single burst from channel 2 bank 0.&lt;/p&gt;
&lt;h2 id=&#34;thread-coarsening&#34;&gt;Thread Coarsening&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;price of parallelism&lt;/em&gt; may refer to the cost of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;launching threads&lt;/li&gt;
&lt;li&gt;synchronization&lt;/li&gt;
&lt;li&gt;redundant work&lt;/li&gt;
&lt;li&gt;redundant memory accesses&lt;/li&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It there are enough hardware resources available, parallelism at the finest level is ideal. If there are not enough resources, there is a price to pay for this parallelism. The hardware will need to serialize the work into blocks of threads that can be executed in parallel.&lt;/p&gt;
&lt;p&gt;If this is the case for a particular application, it may be beneficial to apply some form of &lt;strong&gt;thread coarsening&lt;/strong&gt;. If the hardware would serialize the work due to inefficient resources, the price of parallelism was paid for nothing. As the programmer, you have the ability to coarsen the threads to alleviate the price of parallelism.&lt;/p&gt;
&lt;h3 id=&#34;example-coarsening-tiled-matrix-multiplication&#34;&gt;Example: Coarsening Tiled Matrix Multiplication&lt;/h3&gt;
&lt;p&gt;In tiled matrix multiplication, it is possible that two separate blocks work with the same tile of data from an input matrix. We pay a price for this redundancy, but the benefit is that we can parallelize the work. If the hardware does not have sufficient resource, it will serialize these two blocks. This results in paying the price of data redundancy without the benefit of parallelism.&lt;/p&gt;
&lt;p&gt;Let \(A, B \in \mathbb{R}^{6 \times 6}\), then \(C = AB \in \mathbb{R}^{6 \times 6}\). If we use a \(2 \times 2\) tile size, then we have 9 blocks of work that can be executed concurrently. For argument&amp;rsquo;s sake, let&amp;rsquo;s say that the hardware can only execute 3 blocks of work at a time. We can use thread coarsening to reduce the number of blocks to 3. Each block will be responsible for a single row of the tiled output matrix. That is, if the output matrix is \(6 \times 6\), then each block will be responsible for a \(2 \times 6\) tile of the output matrix. This is visualized in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-14_21-42-55_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Thread coarsening for tiled matrix multiplication.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Thread coarsening for tiled matrix multiplication.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The block itself will perform a similar function as the implementation of tiled matrix multiplication we saw previously. We will need to modify the kernel so that it processes values to fill for 3 blocks of work, spanning each row. In the figure above, this is represented by the three gray, numbered blocks. Although each block uses a different column from matrix \(N\), they all use the same row from matrix \(M\). Our solution will take advantage of this reuse of data.&lt;/p&gt;
&lt;p&gt;Consider the thread that computes the value for the top left entry of block 1. This thread will compute the output value as normal before looping to compute the corresponding relative position in blocks 2 and 3. That is, if the first entry computed is \((0, 0)\) of block 1, then the next entry computed will be \((0, 0)\) of block 2, and so on. This is visualized by the three solid black cells in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-14_21-45-47_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;A single thread loops through three blocks as a result of thread coarsening.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;A single thread loops through three blocks as a result of thread coarsening.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The kernel code is given below. The additional loop controls the switch between the three consecutive tiles. The values from matrix &lt;code&gt;M&lt;/code&gt; are loaded inside the outer-most loop and are reused across the coarse tiles.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;#define TILE_WIDTH 2
#define COARSE_FACTOR 3
__global__ void matMulCoarse(float *M, float *N, float *P, int width) {
    __shared__ float Mds[TILE_WIDTH][TILE_WIDTH];
    __shared__ float Nds[TILE_WIDTH][TILE_WIDTH];

    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    // Identify the row and column of the element to work on
    int row = by * TILE_WIDTH + ty;
    int colStart = bx * TILE_WIDTH * COARSE_FACTOR;

    // Initialize Pvalue
    float Pvalue[COARSE_FACTOR];
    for (int i = 0; i &amp;lt; COARSE_FACTOR; i++) {
        Pvalue[i] = 0.0f;
    }

    // Loop over the tiles required to compute the current output value
    for (int ph = 0; ph &amp;lt; width / TILE_WIDTH; ph++) {
        Mds[ty][tx] = M[row * width + ph * TILE_WIDTH + tx];

        for (int c = 0; c &amp;lt; COARSE_FACTOR; c++) {
            int col = colStart + c * TILE_WIDTH;

            Nds[ty][tx] = N[(ph * TILE_WIDTH + ty) * width + col];
            __syncthreads();

            for (int k = 0; k &amp;lt; TILE_WIDTH; k++) {
                Pvalue[c] += Mds[ty][k] * Nds[k][tx];
            }
            __syncthreads();
        }
    }

    for (int c = 0; c &amp;lt; COARSE_FACTOR; c++) {
        int col = colStart + c * TILE_WIDTH;
        P[row * width + col] = Pvalue[c];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;how-to-use-coarsening-in-your-applications&#34;&gt;How to use coarsening in your applications&lt;/h3&gt;
&lt;p&gt;Thread coarsening is yet another technique that can be applied to optimize your parallel programs. The previous section demonstrated &lt;em&gt;how&lt;/em&gt; it can be applied, but you are probably wondering &lt;em&gt;when&lt;/em&gt; it should be applied. Deciding on whether to apply this technique is largely determined by careful analysis of your current application. This analysis should include benchmarking and profiling. There is work that provides an automatic solution (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Stawinoga and Field 2018&lt;/a&gt;), but we will rely on determining that for ourselves.&lt;/p&gt;
&lt;p&gt;For now, we can at least discuss when &lt;em&gt;not&lt;/em&gt; to apply coarsening. The most obvious instance is when coarsening is completely unnecessary. Consider the vector addition kernel. Each thread performs an independent computation that has no overlapping data with other thread. There is no need to apply coarsening in this case.&lt;/p&gt;
&lt;p&gt;Another bad case for implementation would be when the coarsening factor causes hardware to be underutilized. Parallelization in hardware is scalable. If we take away the opporunity for scale, there may be unused compute. This is typically something we can determine via benchmarking.&lt;/p&gt;
&lt;p&gt;In the coarsened version of matrix multiplication above, we had to create additional private variables to store the coarsened values. These use additional registers per thread. If our application required more than the 32 registers available on our H100, for example, this would have a direct effect on occupancy. Keep that in mind when developing your thread coarsened solution.&lt;/p&gt;
&lt;h2 id=&#34;optimization-checklist&#34;&gt;Optimization Checklist&lt;/h2&gt;
&lt;p&gt;Section 6.4 of &lt;em&gt;Programming Massively Parallel Processors&lt;/em&gt; (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;) provides a checklist of items to consider when optimizing your GPU applications. These are summarized below.&lt;/p&gt;
&lt;h3 id=&#34;maximizing-occupancy&#34;&gt;Maximizing Occupancy&lt;/h3&gt;
&lt;p&gt;Having more threads than physical cores available is beneficial, as it hides the latency required for other operations such as data fetching. Instead of waiting on some operation to return, the hardware can switch to another thread to perform work. This is implemented by adjusting the launch configurations or optimizing the number of registers used per thread, for example. We also discussed solutions for hiding memory-based latency.&lt;/p&gt;
&lt;h3 id=&#34;coalesced-global-memory-accesses&#34;&gt;Coalesced Global Memory Accesses&lt;/h3&gt;
&lt;p&gt;Random accesses to memory are less efficient than consecutive ones. This is a theme that is repeated through many themes of computer science, such as sorting. Understanding of how the underlying hardware works brought to light new ways to optimize our applications. We can rearrange our data to take advantage of DRAM bursts, or we can use shared memory to reduce the latency of memory accesses.&lt;/p&gt;
&lt;h3 id=&#34;minimizing-control-divergence&#34;&gt;Minimizing Control Divergence&lt;/h3&gt;
&lt;p&gt;Although we have not used any applications that exhibit control divergence, we have studied the concept. During SIMD execution, the hardware executes the same instructions on multiple data elements. If a thread or group of threads would diverge from the others, the hardware would have to make multiple passes to cover all of the possible paths.&lt;/p&gt;
&lt;h3 id=&#34;tiling&#34;&gt;Tiling&lt;/h3&gt;
&lt;p&gt;Global memory accesses exhibit higher latency due to the nature of DRAM. We can reduce the number of global memory accesses by using shared memory. This was exemplified in the tiled matrix multiplication examples, where there are many redundant data accesses. Moving these data to shared memory reduces the number of global memory accesses.&lt;/p&gt;
&lt;h3 id=&#34;thread-coarsening&#34;&gt;Thread Coarsening&lt;/h3&gt;
&lt;p&gt;In cases where the hardware would serialize execution of a kernel, thread coarsening can eliminate redundant work. In the tiled matrix multiplication example, we saw that the hardware would serialize execution of the kernel if there were not enough resources available. In this case, the same redundant loads to shared memory would be performed. To reduce this overhead, we coarsened the thread by having a single kernel perform the work of multiple blocks.&lt;/p&gt;
&lt;h2 id=&#34;identifying-bottlenecks&#34;&gt;Identifying Bottlenecks&lt;/h2&gt;
&lt;p&gt;Knowing when to apply each of these optimization techniques comes down to understanding your application. &lt;strong&gt;The single most important step in optimizing your application is to identify the bottleneck&lt;/strong&gt;. What resource is limiting the performance of your solution? Benchmarking and profiling are two techniques that can be used to identify these bottlenecks. We will begin learning these tools in the next lecture.&lt;/p&gt;
&lt;h2 id=&#34;the-takeaway&#34;&gt;The Takeaway&lt;/h2&gt;
&lt;p&gt;At this point, you have learned the basics of GPU programming with CUDA. You should be familiar with writing kernels, setting launch configurations, and compiling them. You should be familiar with a few optimization techniques that can be applied to your applications, but you are probably not confident in your ability to identify when they should be used.&lt;/p&gt;
&lt;p&gt;The next module of this course will focus on problems for which a straightforward solution is not obvious. These are problems that come from other domains of computer science, such as graph theory and linear algebra. We will learn how to apply the techniques we have learned to these problems, and we will learn new techniques that are specific to these problems. Even though the applications themselves may be specific, the techniques used to optimize them are not.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Stawinoga, Nicolai, and Tony Field. 2018. “Predictable Thread Coarsening.” &lt;i&gt;Acm Transactions on Architecture and Code Optimization&lt;/i&gt; 15 (2): 23:1–23:26. &lt;a href=&#34;https://doi.org/10.1145/3194242&#34;&gt;https://doi.org/10.1145/3194242&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>CUDA Memory Architecture</title>
      <link>https://ajdillhoff.github.io/notes/cuda_memory_architecture/</link>
      <pubDate>Thu, 11 Jan 2024 15:07:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/cuda_memory_architecture/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#memory-access&#34;&gt;Memory Access&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#memory-types&#34;&gt;Memory Types&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tiling&#34;&gt;Tiling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-tiled-matrix-multiplication&#34;&gt;Example: Tiled Matrix Multiplication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#boundary-checking&#34;&gt;Boundary Checking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#memory-use-and-occupancy&#34;&gt;Memory Use and Occupancy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dynamically-changing-the-block-size&#34;&gt;Dynamically Changing the Block Size&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-takeaway&#34;&gt;The Takeaway&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;So far, the kernels we have used assume everything is on global memory. Even though there are thousands of cores that can effectively hide the latency of transferring data to and from global memory, we will see this delay will become a bottleneck in many applications. These notes explore the different types of memory available on the GPU and how to use them effectively.&lt;/p&gt;
&lt;h2 id=&#34;memory-access&#34;&gt;Memory Access&lt;/h2&gt;
&lt;p&gt;Transferring memory is one of the biggest bottlenecks in GPU programming. Companies like NVIDIA devote a lot of resources to improving the bandwidth and latency of memory transfers. When training a deep learning model, the datasets used are far too large to fit on the GPU. This means that the data must be transferred to the GPU before the actual training code can execute on the device. Training large models can take days or weeks, so the time spent transferring data can be significant.&lt;/p&gt;
&lt;p&gt;The example provided in Chapter 5 of &amp;ldquo;Programming Massively Parallel Processors&amp;rdquo; is a great introduction to understanding memory access efficiency (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;). In matrix multiplication, the data accesses are limited to a single line of code in the inner-most loop. This means that the memory access pattern is very regular and predictable. The example code is shown below:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;for (int k = 0; i k &amp;lt; numCols; k++) {
    Cvalue += A[row * numCols + k] * B[k * numCols + col];
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This line consists of a floating-point multiplication, floating-point addition, and two memory accesses. Note that we are not storing the result yet, so there is no access to the C matrix. The operation effiency can be described in terms of floating-point operations per second (FLOP/s) and the accesses can be measured in the number of bytes transferred. In this case, we have 2 FLOPs and 8 bytes transferred. This means that the ratio of FLOPs to bytes transferred is 0.25 FLOP/B. This is described as &lt;em&gt;computational intensity&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;With this definition, we get a clearer picture on how to improve the performance of our code. If our kernel relies on too many memory accesses, then the computational intensity will be low. This means that the GPU will be spending more time waiting for data to be transferred than actually performing computations. The goal is to increase the computational intensity as much as possible.&lt;/p&gt;
&lt;p&gt;To put this in perspective, the H100 SXM5 has 3TB/s of memory bandwidth. This global memory bandwidth limits the kernel to 3000 * 0.25 = 750 GFLOP/s. The peak performance of the H100 is 66.9 TFLOPS. If the specialized Tensor cores are utilized, the peak performance is 494.7 TFLOPS. That means that are kernel is only using 0.15% of the peak performance of the GPU. This program is certainly &lt;strong&gt;memory bound&lt;/strong&gt;. Our theoretical limit to computational intensity is the peak performance of the GPU. Programs that achieve this peak are called &lt;strong&gt;compute bound&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Based on the tools we have discussed so far, it is not clear how we can optimize this kernel. The only way to improve the computational intensity is to reduce the number of memory accesses. Modern GPUs have more than just global memory. The next section will explore the different types of memory available on the GPU.&lt;/p&gt;
&lt;h2 id=&#34;memory-types&#34;&gt;Memory Types&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Global Memory&lt;/li&gt;
&lt;li&gt;Local Memory
Resides on global memory, but is not shared between threads. This includes local variables and function arguments.&lt;/li&gt;
&lt;li&gt;Shared Memory
Resides on the chip. Allocated to thread blocks. Shared between threads in the same block.&lt;/li&gt;
&lt;li&gt;Constant Memory&lt;/li&gt;
&lt;li&gt;Registers
Resides on the chip. Each thread has its own registers. Very fast memory.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Data in CPU registers are swapped depending on the context of the program. GPU registers are consistent even when other threads are launched to hide latency. This results in a larger register file on the GPU.&lt;/p&gt;
&lt;p&gt;Following the von Neumann architecture, memory that is closer to the chip is faster but more expensive. Data residing on registers is the most ideal for performance since the processor can work directly with the register values. This benefit comes in the form of energy consumption as well. Transferring data from global memory to the chip requires additional cycles resulting in more energy used.&lt;/p&gt;
&lt;p&gt;When a private variable is declared in a kernel, every single thread will have its own copy of that variable.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Variable declaration&lt;/th&gt;
&lt;th&gt;Memory&lt;/th&gt;
&lt;th&gt;Scope&lt;/th&gt;
&lt;th&gt;Lifetime&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Automatic variables (not arrays)&lt;/td&gt;
&lt;td&gt;Register&lt;/td&gt;
&lt;td&gt;Thread&lt;/td&gt;
&lt;td&gt;Grid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Automatic array variables&lt;/td&gt;
&lt;td&gt;Local&lt;/td&gt;
&lt;td&gt;Thread&lt;/td&gt;
&lt;td&gt;Grid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;__device__ __shared__ int SharedVar;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Shared&lt;/td&gt;
&lt;td&gt;Block&lt;/td&gt;
&lt;td&gt;Grid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;__device__ int GlobalVar;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Global&lt;/td&gt;
&lt;td&gt;Grid&lt;/td&gt;
&lt;td&gt;Application&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;__device__ __constant__ int ConstVar;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Constant&lt;/td&gt;
&lt;td&gt;Grid&lt;/td&gt;
&lt;td&gt;Application&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Automatic array variables should seldom be used. It may have seemed convenient to use a static array for computing channel-specific values in an image processing kernel, but it is more efficient to use three separate variables. Each variable will be allocated to a register resulting in faster access times.&lt;/p&gt;
&lt;p&gt;Global variables are more commonly used to pass information to another kernel that is being launched.&lt;/p&gt;
&lt;h2 id=&#34;tiling&#34;&gt;Tiling&lt;/h2&gt;
&lt;p&gt;These memory types serve as tools that we can use to increase efficiency. The first pattern discussed is &lt;strong&gt;tiling&lt;/strong&gt;. Throughout the rest of the course, we will add many more patterns to our repertoire. Tiling is a well-described technique that has a fitting analogy. If a wall needs to be tiled, it is more efficient to use many small tiles that are lighter and easier to handle. In GPU programming, the wall represents the entire global memory space. The individual tiles are local memory that is allocated to each thread block.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-13_10-13-54_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Global memory access pattern (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Global memory access pattern (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The kernels we have seen so far have used a &lt;em&gt;global memory access pattern&lt;/em&gt;. In this pattern, all threads have access to every data point from the input. Using a &lt;em&gt;tiling pattern&lt;/em&gt;, we can optimize memory accesses by moving shared resources to local memory that is faster to access.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-13_10-16-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Tiling pattern (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Tiling pattern (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The tool itself is quite simple in concept, but the challenge will be identifying when the tool can be properly applied. Consider matrix multiplication. The naive kernel we explored previously uses each thread to compute one value of the output matrix. This kernel uses a global memory access pattern, and we can identify that many of the computations require the same input. They key to introducing tiling for matrix multiplication will be identifying which data use reused.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-13_10-28-30_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Memory accesses for matrix multiplication (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Memory accesses for matrix multiplication (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the figure above, the block size is \(2 \times 2\). Each row of the block relies on the same input row from the matrix on the left. That is, \(P_{0,0}\) and \(P_{0,1}\) will use the same data from the first row of \(M\). In our original kernel, this requires 8 global memory accesses. If we placed this row in shared memory, each output thread could access the values much quicker. We can see a similar pattern for the column values in \(N\).&lt;/p&gt;
&lt;p&gt;Since we are using tiling with a block size of \(B\), we will consider working with \(2B\) values from the input at a time. If the number of values we need to compute an output entry exceeds \(2B\), then we can synchronize the threads before moving to the next section.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-13_10-25-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Tiled matrix multiplication overview (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Tiled matrix multiplication overview (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;Verify that the potential reduction in global memory traffic in matrix multiplication is proportional to the dimension of the blocks used.&lt;/li&gt;
&lt;li&gt;Verify that the reduction is by a factor of \(N\) if the tiles are \(N \times N\).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-tiled-matrix-multiplication&#34;&gt;Example: Tiled Matrix Multiplication&lt;/h2&gt;
&lt;p&gt;The concept of tiled matrix multiplication is this: load a subset of data from \(M\) and \(N\) into shared memory before using that data to perform the dot product. We have a few limitations to think about here. First, the amount of shared memory is much smaller than global memory; we cannot fit all the data at once. Second, the block size will limit how many elements can be loaded into shared memory at once. As suggested by tiling, we are only working with a small chunk at a time.&lt;/p&gt;
&lt;p&gt;Using a \(2 \times 2\) block gives us 4 threads to work with. Overlaying that block on the input only allows us to grab 2 values from the first 2 rows in \(M\) and 2 values from the first 2 columns in \(M\). For each tile, the subset of data will be loaded in followed by adding the dot product of the subset to the current value.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-13_11-14-57_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Loading the first tile (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Loading the first tile (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-13_11-15-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Computing the dot product of the first subset (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Computing the dot product of the first subset (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In this example, the block will move to the next subset of data to finish computing the first block of the output matrix. This process can be arbitrarily scaled up to support larger matrices without necessarily increasing the block size. Although, we would want to increase the block size to take advantage of the additional threads. The figure below shows a table of the computations required for each phase.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-13_11-18-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Tiled matrix multiplication computations (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Tiled matrix multiplication computations (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Check your understanding&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;By using tiling with a block size of \(B \times B\), what is the total reduction in global memory traffic?&lt;/p&gt;
&lt;h3 id=&#34;implementation-in-cuda&#34;&gt;Implementation in CUDA&lt;/h3&gt;
&lt;p&gt;Our implementation should follow these steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Establish shared memory for the input from matrix \(M\) and matrix \(N\).&lt;/li&gt;
&lt;li&gt;Load the first subset of data from \(M\) and \(N\) into shared memory (remember to synchronize threads).&lt;/li&gt;
&lt;li&gt;Compute the dot product of the subset (remember to synchronize threads).&lt;/li&gt;
&lt;li&gt;Repeat steps 2 and 3 until all subsets have been computed.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Step 1 is obvious. We need to establish the shared memory for this solution. Steps 2 and 3 are the same as described above, but we do need to remember to synchronize the threads. Without synchronization, the computation may continue before all the data is properly loaded. Step 4 implies that each thread will loop through the subsets until all values have been computed. The kernel is shown below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void MatMulKernel(float* M, float* N, float* P, int Width) {
    // Block index
    int bx = blockIdx.x;
    int by = blockIdx.y;

    // Thread index
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    __shared__ float Mds[TILE_WIDTH][TILE_WIDTH];
    __shared__ float Nds[TILE_WIDTH][TILE_WIDTH];

    // Identify the row and column of the P element to work on
    int Row = by * TILE_WIDTH + ty;
    int Col = bx * TILE_WIDTH + tx;

    float Pvalue = 0;
    for (int ph = 0; ph &amp;lt; Width / TILE_WIDTH; ++ph) {
        // Collaborative loading of M and N tiles into shared memory
        Mds[ty][tx] = M[Row * Width + ph * TILE_WIDTH + tx];
        Nds[ty][tx] = N[(ph * TILE_WIDTH + ty) * Width + Col];
        __syncthreads();

        for (int k = 0; k &amp;lt; TILE_WIDTH; ++k) {
            Pvalue += Mds[ty][k] * Nds[k][tx];
        }
        __syncthreads();
    }

    P[Row * Width + Col] = Pvalue;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Let&amp;rsquo;s break this down with a small example. Consider multiplying two \(4 \times 4\) matrices. We will use a block size of \(2 \times 2\), as seen in the figure below. Our block will compute the top left submatrix of the output, \(P_{0,0}\), \(P_{0,1}\), \(P_{1,0}\), and \(P_{1,1}\). We will view the computation from the perspective of the thread for \(P_{0,0}\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-13_14-15-32_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Setup of tiled matrix multiplication example.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Setup of tiled matrix multiplication example.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The row and column of the output computed by the current thread is calculated using the block and thread indices. Of course, this is simply \((0, 0)\) for the first thread. It gets slightly more complicated when computing the input subset in the loop. The input needs to be transferred to shared memory. The loop will skip over a tile at a time. At this point, we already know which row of \(M\) and column of \(N\) we need to access. We need to compute the column index for \(M\) and the row index for \(N\).&lt;/p&gt;
&lt;p&gt;For \(M\), we start with &lt;code&gt;Row * Width&lt;/code&gt;. This needs to be offset by the tile offset index &lt;code&gt;ph&lt;/code&gt; of the main loop, yielding &lt;code&gt;Row * Width + ph * TILE_WIDTH&lt;/code&gt;. Finally, we need to add the thread index &lt;code&gt;tx&lt;/code&gt; to get the final index &lt;code&gt;Row * Width + ph * TILE_WIDTH + tx&lt;/code&gt;. The same process is applied to \(N\). &lt;strong&gt;Note that this only transfers a single value from each matrix to shared memory, but our computation relies on 2 values from each matrix.&lt;/strong&gt; Each thread in the block is collaboratively loading the data into shared memory. This is why the call to &lt;code&gt;__syncthreads()&lt;/code&gt; is necessary.&lt;/p&gt;
&lt;p&gt;Specifically, the thread for \(P_{0, 0}\) copies \(M_{0, 0}\) and \(N_{0, 0}\) to shared memory. The thread for \(P_{0, 1}\) copies \(M_{0, 1}\) and \(N_{1, 0}\) to shared memory. The thread for \(P_{1, 0}\) copies \(M_{1, 0}\) and \(N_{0, 1}\) to shared memory. Finally, the thread for \(P_{1, 1}\) copies \(M_{1, 1}\) and \(N_{1, 1}\) to shared memory.&lt;/p&gt;
&lt;p&gt;The next step is to compute the dot product of the subset. Again, we see a call to &lt;code&gt;__syncthreads()&lt;/code&gt;. Without this synchronization, the loop may be allowed to continue and overwrite the data in shared memory before a thread has finished. Once the final value is computed, each thread can freely write it back to global memory. Since each thread is computing a different value, there is no need to synchronize the threads before writing to global memory.&lt;/p&gt;
&lt;p&gt;\begin{align*}
P_{0, 0} &amp;amp;+= 2 \times 2 + 1 \times 1 \\
P_{0, 1} &amp;amp;+= 2 \times 1 + 1 \times 0 \\
P_{1, 0} &amp;amp;+= 1 \times 2 + 0 \times 1 \\
P_{1, 1} &amp;amp;+= 1 \times 1 + 0 \times 0
\end{align*}&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-13_14-21-27_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Updated values for the first subset.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Updated values for the first subset.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The next iteration of the loop will grab the next subset of the data and repeat the process. The result after this step is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-13_14-26-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;Updated values for the second subset.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;Updated values for the second subset.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To summarize, &lt;code&gt;ph&lt;/code&gt; is the tile offset index, &lt;code&gt;Row&lt;/code&gt; and &lt;code&gt;Col&lt;/code&gt; are the row and column of the output computed by the current thread, and &lt;code&gt;tx&lt;/code&gt; and &lt;code&gt;ty&lt;/code&gt; will give the offset with respect to the current tile.&lt;/p&gt;
&lt;p&gt;The kernel above has an outer loop that calls another loop managed by thread synchronization, breaking the computation up into several distinct phases. This is called &lt;strong&gt;strip-mining&lt;/strong&gt; and is an important part of tiling. This existed even before GPUs were used (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;performance-analysis&#34;&gt;Performance Analysis&lt;/h3&gt;
&lt;p&gt;In the naive implementation, we had a computational intensity of 0.25 FLOP/B. With a \(16 \times 16\) tile, the number of global memory accesses is reduced by a factor of 16. This gives us a computational intensity of 4 FLOP/B. We previously stated that the H100 has a global memory bandwidth of 3TB/s. This means that the theoretical limit for the performance of this kernel is 3000 * 4 = 12000 GFLOP/s which is much better than the 750 GFLOP/s we had before.&lt;/p&gt;
&lt;p&gt;This is not the most optimal way to implement matrix multiplication, and you should always refer to the cuBLAS library for matrix operations. The purpose of this example is to demonstrate the use of tiling.&lt;/p&gt;
&lt;h2 id=&#34;boundary-checking&#34;&gt;Boundary Checking&lt;/h2&gt;
&lt;p&gt;The previous implementation assumed that the width of the matrices was a multiple of the tile width and that the input would always be square matrices. Consider changing our \(2 \times 2\) block to a \(3 \times 3\) block using the same input sizes.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-13_12-56-49_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 11: &amp;lt;/span&amp;gt;Using a 3x3 block (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;Using a 3x3 block (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Our implementation would follow the same process for the first subset of pattern. An issue arises when computing the second tile offset since the block exceeds the boundaries of our input and output. One solution would be to check the boundary condition on both the input, when transferring the data to shared memory, and the output, when reading the data from shared memory. This would require a conditional statement in the inner loop. This is not ideal since the conditional statement would be executed for every thread in the block.&lt;/p&gt;
&lt;p&gt;Another solution is to pad the input with zeros. If the index is outside our boundary, adding a 0 will not affect the result of the dot product. This allows for a simpler implementation while still being flexible enough to handle matrices of any size. The relevant portion of the kernel is shown below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;float Pvalue = 0;
for (int ph = 0; ph &amp;lt; ceil(Width/(float)TILE_WIDTH); ph++) {
    // Collaborative loading of M and N tiles into shared memory
    if (Row &amp;lt; Width &amp;amp;&amp;amp; ph * TILE_WIDTH + tx &amp;lt; Width) {
        Mds[ty][tx] = M[Row * Width + ph * TILE_WIDTH + tx];
    } else {
        Mds[ty][tx] = 0.0;
    }
    if (ph * TILE_WIDTH + ty &amp;lt; Width &amp;amp;&amp;amp; Col &amp;lt; Width) {
        Nds[ty][tx] = N[(ph * TILE_WIDTH + ty) * Width + Col];
    } else {
        Nds[ty][tx] = 0.0;
    }
    __syncthreads();
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The rest of the kernel remains the same. In Lab 2, you will implement this and adapt it to work with non square matrices as well.&lt;/p&gt;
&lt;h2 id=&#34;memory-use-and-occupancy&#34;&gt;Memory Use and Occupancy&lt;/h2&gt;
&lt;p&gt;Just like exceeding the number of registers per thread can negatively affect occupancy, so can over allocating shared memory. The H100 can have up to 228 KB per SM. If we are maximizing the 2048 threads available per SM, each block cannot exceed 228 KB / 2048 threads = 112 B/thread.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How much shared memory is used by each block?&lt;/strong&gt; Each block has 2 arrays of size \(TILE\_WIDTH \times TILE\_WIDTH\) of type &lt;code&gt;float&lt;/code&gt;. This gives us a total of \(2 \times TILE\_WIDTH \times TILE\_WIDTH \times 4 = 8(TILE\_WIDTH)^2\) B. Each block uses \(TILE\_WIDTH^2\) threads, resulting in 8 B/thread. This is well below the limit of 112 B/thread.&lt;/p&gt;
&lt;h2 id=&#34;dynamically-changing-the-block-size&#34;&gt;Dynamically Changing the Block Size&lt;/h2&gt;
&lt;p&gt;The solution presented above uses a constant to determine the tile size. What if this tile size was not optimal for a given hardware configuration? We would surely want to adjust this dynamically to maximize performance. In CUDA, we can support this by using the &lt;code&gt;extern&lt;/code&gt; keyword. First, we need to define our shared memory as one array: &lt;code&gt;extern __shared__ float Mds_Nds[];&lt;/code&gt;. This is a 1D array that represents the shared memory for both input matrices.&lt;/p&gt;
&lt;p&gt;When launching this kernel, we need some way to inform it of the tile size. First, we would query the device properties and determine the optimal tile size based on the hardware. This size can be used as a third launch configuration input, as shown below. Additionally, the size of the shared memory for each input matrix is provided as two additional arguments to the kernel.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;size_t size = compute_optimal_size(); // Determine optimal tile size
MatMulKernel&amp;lt;&amp;lt;&amp;lt;dimGrid, dimBlock, size&amp;gt;&amp;gt;&amp;gt;(M_d, N_d, P_d, Width, size/2, size/2);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The kernel will need to be modified to use the new shared memory array. The first step is to determine the offset for each matrix. This is done by multiplying the tile size by the thread index. The second step is to use the offset to access the correct value in the shared memory array. The kernel is shown below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void MatMulKernel(float* M, float* N, float* P, int Width, int Mds_offset, int Nds_offset) {
    extern __shared__ float Mds_Nds[];

    float *Mds = (float *)Mds_Nds;
    float *Nds = (float *)Mds_Nds + Mds_offset;

    // Rest of the kernel
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Completing this modification would require us to use linear indexing for &lt;code&gt;Mds&lt;/code&gt; and &lt;code&gt;Nds&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;the-takeaway&#34;&gt;The Takeaway&lt;/h2&gt;
&lt;p&gt;Tiling is a powerful tool that can be used to improve the performance of a kernel. It is important to understand the memory access pattern of your kernel and identify which data is reused. This will allow you to move that data to shared memory and reduce the number of global memory accesses. Tiling is the first of many &lt;em&gt;patterns&lt;/em&gt; that we will explore. Just like not every tool is useful for every job, not every pattern will be useful for each problem we face. Increasing the number of tools, or patterns, that we have available will allow us to solve a wider range of problems efficiently.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>CUDA Architecture</title>
      <link>https://ajdillhoff.github.io/notes/cuda_architecture/</link>
      <pubDate>Mon, 08 Jan 2024 20:49:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/cuda_architecture/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#architecture&#34;&gt;Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#block-scheduling&#34;&gt;Block Scheduling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#synchronization&#34;&gt;Synchronization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#warps&#34;&gt;Warps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#control-divergence&#34;&gt;Control Divergence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#warp-scheduling&#34;&gt;Warp Scheduling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#resource-partitioning&#34;&gt;Resource Partitioning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dynamic-launch-configurations&#34;&gt;Dynamic Launch Configurations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-takeaway&#34;&gt;The Takeaway&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;A GPU consists of chip that is composed of several &lt;strong&gt;streaming multiprocessors&lt;/strong&gt; (SMs). Each SM has a number of cores that execute instructions in parallel. The H100, seen below, has 144 SMs (you can actually count them by eye). Each SM has 128 FP32 cores for a total of 18,432 cores. Historically, CUDA has used DDR memory, but newer architectures use high-bandwidth memory (HBM). This is closely integrated with the GPU for faster data transfer.&lt;/p&gt;
&lt;p&gt;In the image below, you can see 6 HBM3 memory modules surrounding the GPU, 3 on either side of the die. HBM3 is capable of 3 TB/s of bandwidth. The platform shown only uses 5 of these modules. The full version will utilize all 6.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-11_14-29-08_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;NVIDIA H100 GPU with 144 SMs ([NVIDIA](https://resources.nvidia.com/en-us-tensor-core)).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;NVIDIA H100 GPU with 144 SMs (&lt;a href=&#34;https://resources.nvidia.com/en-us-tensor-core&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;NVIDIA&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;block-scheduling&#34;&gt;Block Scheduling&lt;/h2&gt;
&lt;p&gt;When a kernel is launched, the blocks that we configure in our code are assigned to SMs. All threads in each block will be assigned to each SM. Depending on the platform, the number of blocks that can be assigned to an SM will vary. This is discussed in more detail below. Since all threads in a block are on the same SM, they can share data and communicate with each other.&lt;/p&gt;
&lt;h2 id=&#34;synchronization&#34;&gt;Synchronization&lt;/h2&gt;
&lt;p&gt;Threads that run on the same block can be synchronized using &lt;code&gt;__syncthreads()&lt;/code&gt;. This is a pretty straightforward concept, but it is important to understand the caveats. When a kernel reaches this call, the execution of the threads will stop until all of them have reached that point. This construct is typically used when threads need to share data or are dependent on the results of other threads.&lt;/p&gt;
&lt;p&gt;Be careful on using this call. An example of incorrect usage is shown below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__
void kernel(int *a, int *b, int *c) {
    if (threadIdx.x % 2 == 0) {
        // Perform some work
        __syncthreads();
    else {
        // Perform some other work
        __syncthreads();
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Unlike a general-purpose processor, a GPU does not have control hardware for each individual core. This means that all threads must execute the same instructions using shared resources. In the example above, it is possible for some threads to branch off into a different part of the program. However, only one of the paths can be executed based on this limitation. This is called &lt;strong&gt;control divergence&lt;/strong&gt; and is discussed in more detail below.&lt;/p&gt;
&lt;p&gt;Even though the call looks the same, each &lt;code&gt;__syncthreads()&lt;/code&gt; is different. The first call will only synchronize the threads that executed the first path. The second call will only synchronize the threads that executed the second path. The result is either undefined output or a deadlock, in which the threads will never reach the second call.&lt;/p&gt;
&lt;p&gt;Since threads in separate blocks cannot be synchronized, the blocks can be executed in any arbitrary order. You might immediately ask yourself how a complex problem that requires synchronization between all parts of the data can get around this limitation. We will explore more complex patterns and their solutions in later sections.&lt;/p&gt;
&lt;h2 id=&#34;warps&#34;&gt;Warps&lt;/h2&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-28_21-03-23_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Warps across several blocks (credit: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Warps across several blocks (credit: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Streaming Multiprocessors in a CUDA chip execute threads in a group of 32 called &lt;strong&gt;warps&lt;/strong&gt;. Since Compute Capability 1.0, the warp size has not changed. When a block is assigned to an SM, it is divided into warps. Given this size, you can easily determine the number of warps assigned to an SM. For example, if you have a block of 256 threads, the SM has 256 / 32 = 8 warps. If the block size is not evenly divisible by the number of warps per SM, the last warp will be padded with inactive threads.&lt;/p&gt;
&lt;p&gt;When multi-dimensional thread blocks are assigned to an SM, the threads are linearly mapped in a &lt;strong&gt;row-major&lt;/strong&gt; order before being partitioned into warps. For example, a 2D block of \(16 \times 16\) threads will be mapped to a 1D array of 256 threads. The first 32 threads will be assigned to the first warp, the next 32 to the second warp, and so on.&lt;/p&gt;
&lt;p&gt;Warps are executed following the Single-Instruction, Multiple-Data (SIMD) model. There is a single program that runs the same instruction on all threads in the same order. If a thread would have executed a different path based on its input data, it would not be executed with the others. This is called &lt;strong&gt;control divergence&lt;/strong&gt; and is explained in the next section.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-28_21-08-27_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;SM layout (source: NVIDIA DLI)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;SM layout (source: NVIDIA DLI)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The advantage of this model is that more physical space can be dedicated to ALUs instead of control logic. In a traditional CPU, each processing core would have its own control logic. The tradeoff is that different cores can execute their own programs at varying points in time.&lt;/p&gt;
&lt;h2 id=&#34;control-divergence&#34;&gt;Control Divergence&lt;/h2&gt;
&lt;p&gt;Since a traditional CPU has separate control logic for each core, it can execute different programs at the same time. If the program has a conditional statement, it does not need to worry about synchronizing instructions with another core. This is not the case with a GPU. Since every thread in a warp executes the same instruction, only threads that would execute the same path can be processed at the same time. If a thread would execute a different path, it is not executed with the others. This is called &lt;strong&gt;control divergence&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;What exactly happens then if a warp has 32 threads of which only 16 would execute the same path? Simply, multiple passes are made until all possible paths of execution are considered based on the divergence of the threads. The SM would process the first 16 threads that all follow the same path before processing the second 16 threads.&lt;/p&gt;
&lt;p&gt;This also applies to other control flow statements such as loops. Consider a CUDA program that processes the elements of a vector. Depending on the loop and data used, the threads may execute a different number of iterations. As threads finished their iterations, they would be disabled while the remaining threads continue.&lt;/p&gt;
&lt;p&gt;There are some cases in which it is apparent that your program will exhibit control divergence. For example, if you have a conditional statement based on the thread index, you can be sure that the threads will execute different paths.&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Consider a \(200 \times 150\) image that is processed by a CUDA program. The kernel is launched with \(16 \times 16\) blocks which means there are \(200 / 16 = 13\) blocks in the x-direction and \(150 / 16 = 10\) blocks in the y-direction. The total number of blocks is \(13 \times 10 = 130\). Each block has 256 threads, or 8 warps. That means that the total number of warps is \(130 \times 8 = 1040\).&lt;/p&gt;
&lt;h2 id=&#34;warp-scheduling&#34;&gt;Warp Scheduling&lt;/h2&gt;
&lt;p&gt;An SM can only execute instructions for a small number of warps. The architecture allows for more warps than the SM can execute since warps will often be waiting for some result or data transfer. Warps are selected based on a priority mechanism. This is called &lt;strong&gt;latency tolerance&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Zero-overhead thread scheduling allows for selecting warps without any overhead. A CPU has more space on the chip for caching and branch prediction so that latency is as low as possible. GPUs have more floating point units and can switch between warps, effectively hiding latency.&lt;/p&gt;
&lt;p&gt;The execution states for all assigned warps are stored in the hardware registers, eliminating the need to save and restore registers when switching between warps.&lt;/p&gt;
&lt;p&gt;Under this model, it is ideal for an SM to be assigned more threads than it can execute at once. This increases the odds that the SM will have a warp ready to execute when another warp is waiting for data.&lt;/p&gt;
&lt;h2 id=&#34;resource-partitioning&#34;&gt;Resource Partitioning&lt;/h2&gt;
&lt;p&gt;There is a limit on the number of warps that an SM can support. In general, we want to maximize the throughput of an SM by assigning as many warps as possible. The ratio of warps assigned to the number of warps an SM supports is called &lt;strong&gt;occupancy&lt;/strong&gt;. If we understand how the architecture partitions the resources, we can optimize our programs for peak performance. Consider the NVIDIA GH100 GPU, pictured below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-11_11-44-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;GH100 Full GPU with 144 SMs ([NVIDIA](https://resources.nvidia.com/en-us-tensor-core)).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;GH100 Full GPU with 144 SMs (&lt;a href=&#34;https://resources.nvidia.com/en-us-tensor-core&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;NVIDIA&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The H100 architecture shares the same limitations in compute capability as the A100, so this example will follow the book closely (Hwu, Kirk, and El Hajj 2022). The H100 supports 32 threads per warp, 64 warps per SM, 32 blocks per SM, and 2048 threads per SM. Depending on the block size chosen, the number of blocks per SM will differ. For example, a block size of 256 threads means that there are 2048 / 256 = 8 blocks per SM. This block size would maximize occupancy since the architecture supports more than 8 blocks per SM. Also, the number of threads per block is less than the limit of 1024.&lt;/p&gt;
&lt;p&gt;What if we chose 32 threads per block? Then there would be 2048 / 32 = 64 blocks per SM. However, the device only supports 32 blocks per SM. With only 32 blocks allocated with 32 threads per block, a total of 1024 threads would be utilized. The occupancy in this case is 1024 / 2048 = 50%.&lt;/p&gt;
&lt;p&gt;Historically, NVIDIA provided an excel spreadsheet to compute occupancy. It has since been deprecated in favor of Nsight Compute, a tool that provides more information about the performance of your program. We will cover this tool in a later section.&lt;/p&gt;
&lt;h3 id=&#34;including-registers&#34;&gt;Including Registers&lt;/h3&gt;
&lt;p&gt;Another factor for occupancy is the number of registers used per thread. The H100 has 65,536 registers available for use. As long as your program does not use more than this, you can follow the simpler occupancy calculation from above. With 2048 threads, that leaves 65,536 / 2048 = 32 registers per thread. If we run a program with 256 threads/block, there would be 2048 / 256 = 8 blocks per SM. This means that there are 8 * 256 = 2048 threads per SM. With 31 registers per thread, the total number of registers used per SM is 2048 * 31 = 63,488. In this case we still maximize occupancy since 63,488 &amp;lt; 65,536.&lt;/p&gt;
&lt;p&gt;What if each thread required 33 registers? In that case, the total number of registers used per SM would be 2048 * 33 = 67,584. How would these resources be partitioned? Only 7 blocks could be assigned since 7 * 256 * 33 = 59,136 &amp;lt; 65,536. This means that only 7 * 256 = 1792 threads would be used, reducing the occupancy to 1792 / 2048 = 87.5%.&lt;/p&gt;
&lt;h2 id=&#34;dynamic-launch-configurations&#34;&gt;Dynamic Launch Configurations&lt;/h2&gt;
&lt;p&gt;Depending on our application requirements, we may need to support a range of devices across several compute capabalities. The CUDA API makes this simple by providing several different functions for querying device properties. These can be called from the host before configuring and launching a kernel. This is not an exhaustive list, but it covers the most important properties. When we first launch a program that utilizes CUDA, we will want to know how many devices are available. Later in this course, we will develop programs that utilize multiple GPUs, but we would also want our code to adapt dynamically to a single GPU.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; deviceCount;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cudaGetDeviceCount(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;deviceCount);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once the device count is known, the properties of each device can be acquired with &lt;code&gt;cudaGetDeviceProperties&lt;/code&gt;. This function takes a pointer to a &lt;code&gt;cudaDeviceProp&lt;/code&gt; struct. The struct contains several properties that can be used to configure the kernel launch. The most important properties are listed below. A full list can be found &lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;in the CUDA documentation.&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Property&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of the device&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;totalGlobalMem&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Total amount of global memory available on the device in bytes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;sharedMemPerBlock&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Shared memory available per block in bytes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;regsPerBlock&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;32-bit registers available per block&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;warpSize&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Warp size in threads&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;maxThreadsPerBlock&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Maximum number of threads per block&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;maxThreadsDim&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Maximum size of each dimension of a block&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;maxGridSize&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Maximum size of each dimension of a grid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;multiProcessorCount&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Number of SMs on the device&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;maxThreadsPerMultiProcessor&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Maximum number of threads per SM&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The following example iterates through all devices and queries their properties.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; deviceCount; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    cudaDeviceProp prop;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    cudaGetDeviceProperties(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;prop, i);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Use properties to configure kernel launch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;the-takeaway&#34;&gt;The Takeaway&lt;/h2&gt;
&lt;p&gt;The CUDA architecture is designed to maximize the number of threads that can be executed in parallel. This is achieved by partitioning the resources of the GPU into SMs. Each SM can execute a small number of warps at a time. The number of warps that can be assigned to an SM is called &lt;strong&gt;occupancy&lt;/strong&gt;. The occupancy is determined by the number of threads per block, the number of blocks per SM, and the number of registers used per thread. The CUDA API provides functions for querying device properties so that the kernel launch can be configured dynamically.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multidimensional Grids and Data</title>
      <link>https://ajdillhoff.github.io/notes/multidimensional_grids_and_data/</link>
      <pubDate>Fri, 05 Jan 2024 11:56:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/multidimensional_grids_and_data/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multidimensional-grid-organization&#34;&gt;Multidimensional Grid Organization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-color-to-grayscale&#34;&gt;Example: Color to Grayscale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#no-longer-embarrassing-overlapping-data&#34;&gt;No longer embarrassing: overlapping data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix-multiplication&#34;&gt;Matrix Multiplication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-s-next&#34;&gt;What&amp;rsquo;s Next?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;The CUDA Programming model allows us to organize our data in a multidimensional grid. The purpose of this is primarily for our own convenience, but it also allows us to take advantage of the GPU&amp;rsquo;s memory hierarchy. In Lab 0, we only required a single dimension for our grid as well as each block since the input was a vector. When performing computations on multidimensional data like matrices, we can match the dimensions of our launch configuration to the dimensions of our data.&lt;/p&gt;
&lt;h2 id=&#34;multidimensional-grid-organization&#34;&gt;Multidimensional Grid Organization&lt;/h2&gt;
&lt;p&gt;All threads share a block index, &lt;code&gt;blockIdx&lt;/code&gt;, and a thread index, &lt;code&gt;threadIdx&lt;/code&gt;. These indices are three-dimensional vectors of type &lt;code&gt;dim3&lt;/code&gt;. The &lt;code&gt;dim3&lt;/code&gt; type is defined as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dim3&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; x, y, z;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Each grid is a 3D array of blocks, and every block a 3D array of threads. Consider the kernel execution for &lt;code&gt;vecAdd&lt;/code&gt; from Lab 0:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dim3 &lt;span style=&#34;color:#a6e22e&#34;&gt;blocksPerGrid&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dim3 &lt;span style=&#34;color:#a6e22e&#34;&gt;threadsPerBlock&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vecAdd&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;blocksPerGrid, threadsPerBlock&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;(a_d, b_d, c_d, n);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will execute with \(32 \times 128 = 4096\) threads.&lt;/p&gt;
&lt;p&gt;If our input is a matrix, we should organize our launch dimensions to match its 2D structure. We seemingly have two options: either the grid size or the block size. Consider the figure below, there are 4 blocks in the grid, each with 16 threads organized as a \(4 \times 2 \times 2\) volume.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-05_14-20-10_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;A 2D grid of blocks, each with 16 threads arranged in a 3D configuration (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;A 2D grid of blocks, each with 16 threads arranged in a 3D configuration (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Under such a configuration, we would make use of &lt;code&gt;gridDim.x&lt;/code&gt;, &lt;code&gt;gridDim.y&lt;/code&gt;, and &lt;code&gt;gridDim.z&lt;/code&gt; to access the dimensions of the grid. The dimensions of the block would be accessed with &lt;code&gt;blockDim.x&lt;/code&gt;, &lt;code&gt;blockDim.y&lt;/code&gt;, and &lt;code&gt;blockDim.z&lt;/code&gt;. The thread indices would be accessed with &lt;code&gt;threadIdx.x&lt;/code&gt;, &lt;code&gt;threadIdx.y&lt;/code&gt;, and &lt;code&gt;threadIdx.z&lt;/code&gt;. Would this be the best way to organize our launch configuration? &lt;strong&gt;Not exactly.&lt;/strong&gt; We have no use for the 3D structure if we are only working with matrices.&lt;/p&gt;
&lt;p&gt;Consider an \(n \times m\) matrix. If the matrix is small enough, we could launch a single block with a 2D arrangement of threads to perform the necessary computation. For larger matrices, we would optimally split the work into multiple blocks. This would allow us to perform more work in parallel. Let \(n=62\) and \(m=76\). If we chose a \(16 \times 16\) block size, we would need \(4 \times 5 = 20\) blocks to cover the entire matrix, as shown in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-05_15-04-59_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A 2D grid of blocks, each with 16 threads arranged in 2D (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A 2D grid of blocks, each with 16 threads arranged in 2D (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;notes-on-compute-capability&#34;&gt;Notes on Compute Capability&lt;/h3&gt;
&lt;p&gt;It is more important to dynamically adjust the grid size so that your program can adapt to varying input sizes. As of CC 9.0, the maximum number of threads a block can have is 1024, this means that a \(32 \times 32\) block is the largest we can do for matrix data.&lt;/p&gt;
&lt;p&gt;If the input matrix is smaller than \(32 \times 32\), then only a single block is needed. The additional threads allocated to that block will be inactive for indices outside the range of our input.&lt;/p&gt;
&lt;p&gt;If the input matrix is larger than \(32 \times 32\), additional blocks should be added to the grid to accommodate the increased size. It is safe to keep the block size fixed, but the grid size &lt;strong&gt;must&lt;/strong&gt; be dynamic.&lt;/p&gt;
&lt;h3 id=&#34;optimal-launch-parameters&#34;&gt;Optimal Launch Parameters&lt;/h3&gt;
&lt;p&gt;Is it better to have fewer blocks that maximize the amount of threads per block? Or is it better to have more blocks with fewer threads per block? The current maximum number of threads per block is 1024. In practice, a maximum block dimension size of 128 or 256 is ideal. This has more to do with the specific problem and the amount of shared memory required. You will explore this question in Lab 1.&lt;/p&gt;
&lt;h2 id=&#34;example-color-to-grayscale&#34;&gt;Example: Color to Grayscale&lt;/h2&gt;
&lt;p&gt;Given the layout just described, we will write a kernel that converts a color image to grayscale. This is an &lt;em&gt;embarrassingly parallel&lt;/em&gt; problem since each pixel can be converted independently of the others. We will use the following formula to convert each pixel:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;gray &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.299f&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; red &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.587f&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; green &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.114f&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; blue
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;A CPU implementation would require a &lt;code&gt;for&lt;/code&gt; loop over the exact number of pixels. The CUDA kernel for this is straightforward since it only depends on the current pixel. The only real challenge is to compute the correct indices for each thread.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__
void colorToGrayscale(unsigned char *rgbImage,
                      unsigned char *grayImage,
                      int numRows, int numCols)
{
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;
    if (x &amp;gt;= numCols || y &amp;gt;= numRows) return;

    int index = y * numCols + x;
    int rgbOffset = index * 3;
    unsigned char r = rgbImage[rgbOffset];
    unsigned char g = rgbImage[rgbOffset + 1];
    unsigned char b = rgbImage[rgbOffset + 2];
    float channelSum = 0.299f * r + 0.587f * g + 0.114f * b;
    grayImage[index] = channelSum;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In this example, we assume an RGB image where each pixel is represented by three unsigned characters. It is standard convention in C to pass a pointer to the first element of the array. This implies that we cannot use the &lt;code&gt;[]&lt;/code&gt; operator to access the elements in a multidimensional way. Instead, we must compute the index ourselves. If you are not currently familiar with flat indexing, you certainly will be by the end of this course.&lt;/p&gt;
&lt;p&gt;In C, multi-dimensional arrays are stored in row-major order. To compute the index of row &lt;code&gt;j&lt;/code&gt; and column &lt;code&gt;i&lt;/code&gt; in a 2D array, we need to skip over &lt;code&gt;j&lt;/code&gt; rows and &lt;code&gt;i&lt;/code&gt; columns. The total number of columns is the width of the array. The total number of rows is the height of the array. The index is computed as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; width &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; i;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This is represented in the following figure.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-05_16-56-55_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;A 2D array stored in row-major order (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;A 2D array stored in row-major order (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Since the image is now represented as a flat 1D array, we can use the index computed above to access the correct pixel. The image is typically stored in the same row-major format, although this is not always the case. You should always check the documentation for the image format you are using.&lt;/p&gt;
&lt;h3 id=&#34;launch-configuration&#34;&gt;Launch Configuration&lt;/h3&gt;
&lt;p&gt;As stated above, we are going to launch 20 blocks in a \(4 \times 5\) grid. Each block will have 256 threads arranged in a \(16 \times 16\) 2D configuration. This totals to \(20 \times 256 = 5120\) threads. The example figure above shows this configuration overlaid on a \(76 \times 62\) image. That means we have 4712 pixels that need to be converted. The remaining 408 threads will be idle.&lt;/p&gt;
&lt;p&gt;You might be wondering if all 5120 threads launch at the same time. What if the number of pixels exceeded the number of threads available on the GPU? The short answer is that the GPU will launch as many threads as possible, but the long answer is slightly more complicated and will be discussed in a later lesson.&lt;/p&gt;
&lt;p&gt;In any case, our kernel can be launched using the following code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dim3 &lt;span style=&#34;color:#a6e22e&#34;&gt;blockSize&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dim3 &lt;span style=&#34;color:#a6e22e&#34;&gt;gridSize&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;colorToGrayscale&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;gridSize, blockSize&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;(rgbImage, grayImage, numRows, numCols);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;no-longer-embarrassing-overlapping-data&#34;&gt;No longer embarrassing: overlapping data&lt;/h2&gt;
&lt;p&gt;At this point, you should have a basic understanding of how to solve problems that are embarrassingly parallel. Now comes the next step in shaping your parallel thinking skills. What if the thread relies on multiple data points that may be used by other threads. This is further complicated with problems that require some computation to complete before a thread can begin its work. Let&amp;rsquo;s take a step into deeper waters by looking at image blurring. This is a common technique used in image processing to reduce noise and detail. The basic idea is to replace each pixel with a weighted average of its neighboring pixels. The size of the neighborhood is called the &lt;strong&gt;kernel size&lt;/strong&gt;. The kernel size is typically an odd number so that the pixel of interest is in the center of the neighborhood.&lt;/p&gt;
&lt;p&gt;The core operation behind blurring is called a &lt;strong&gt;convolution&lt;/strong&gt;. We will explore this operation in depth as it serves as a more advanced pattern for parallelism. For now, we will focus on the basic idea. Given a kernel size of \(5 \times 5\) centered on a pixel, we will compute the weighted average of the 25 pixels in the neighborhood. To keep it simple, the weights will be uniform.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-06_15-50-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;A blurring kernel (red) centered on a pixel (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;A blurring kernel (red) centered on a pixel (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Given a pixel location \((x, y)\), we can compute the index of the pixel in the neighborhood as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (y &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; ky) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; numCols &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; kx);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Where \(ky\) and \(kx\) are the row and column indices of the kernel. The kernel is centered on the pixel of interest, so \(ky\) and \(kx\) range from \(-2\) to \(2\). The total number of pixels in the neighborhood is \(5 \times 5 = 25\). The weighted average is computed as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; sum &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0f&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; numPixels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; ky &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;; ky &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;; ky&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; kx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;; kx &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;; kx&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; kx &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; kx &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; numCols) &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (y &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; ky &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; ky &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; numRows) &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (y &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; ky) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; numCols &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; kx);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        sum &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; image[index];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        numPixels&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;image[y &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; numRows &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; x] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sum &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; numPixels;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Some extra care will be needed to account for pixels outside the boundaries. There are several strategies to handle out-of-bounds pixels. The simplest is to ignore them. We will explore other strategies when discussing convolutions. In Lab 1, you will implement a blur kernel that can support a varying kernel size.&lt;/p&gt;
&lt;h2 id=&#34;matrix-multiplication&#34;&gt;Matrix Multiplication&lt;/h2&gt;
&lt;p&gt;Matrix multiplication is one of the most important operations in linear algebra. Many high performance computing applications rely on it. It is one of the most widely called operations in deep learning, for example. Parallelizing this and other linear algebra operations has resulted in an explosion of research and applications ranging from computer vision to computational fluid dynamics. Exploring the parallelism of matrix multiplication will give us a deeper understanding of the CUDA programming model. It will also serve as a jumping off point for more advanced topics like shared memory and convolutional neural networks.&lt;/p&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;Let \(A = \mathbb{R}^{m \times n}\) and \(B = \mathbb{R}^{n \times p}\) be two matrices. The product \(C = AB\) is defined as follows:&lt;/p&gt;
&lt;p&gt;\[
C_{ij} = \sum_{k=1}^n A_{ik} B_{kj}\quad \text{for } i = 1, \ldots, m \text{ and } j = 1, \ldots, p
\]&lt;/p&gt;
&lt;p&gt;This operation is only defined on compatible matrices. That is, the number of columns in \(A\) must equal the number of rows in \(B\). The resulting matrix \(C\) will have \(m\) rows and \(p\) columns.&lt;/p&gt;
&lt;h3 id=&#34;cpu-implementation&#34;&gt;CPU Implementation&lt;/h3&gt;
&lt;p&gt;The CPU implementation of matrix multiplication is straightforward. There is a double &lt;code&gt;for&lt;/code&gt; loop to iterate through each element in the &lt;em&gt;output&lt;/em&gt; matrix. The inner loop computes the dot product of the $i$th row of \(A\) and the $j$th column of \(B\). The dot product is computed by summing the element-wise product of the two vectors.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;matrixMultiplyCPU&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;A, &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;B, &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;C, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; m, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; p) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; m; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; j &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; p; j&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; sum &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0f&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; k &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; n; k&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                sum &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; A[i &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; k] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; B[k &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; j];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            C[i &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sum;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;gpu-implementation&#34;&gt;GPU Implementation&lt;/h3&gt;
&lt;p&gt;For a parallel implementation, we can reason that each thread should compute a single element of the output matrix. To compute element \(C_{ij}\), the thread needs access to row \(i\) from \(A\) and column \(j\) from \(B\). Each thread is simply computing the dot product between these two vectors. The figure below visualizes this process.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-08_12-56-34_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Matrix multiplication (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Matrix multiplication (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The output matrix is separated into blocks based on our block size. When writing the kernel, it is necessary to make sure that the index is not out of bounds.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__
void matrixMultiplyGPU(float *A, float *B, float *C, int m, int n, int p) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    if (row &amp;gt;= m || col &amp;gt;= p) return;

    float sum = 0.0f;
    for (int k = 0; k &amp;lt; n; k++) {
        sum += A[row * n + k] * B[k * p + col];
    }
    C[row * p + col] = sum;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;launch-configuration&#34;&gt;Launch Configuration&lt;/h3&gt;
&lt;p&gt;The launch configuration is similar to the previous examples. We will launch a 2D grid of blocks, each with a 2D arrangement of threads. The block size will be \(16 \times 16\) and the grid size will be \(m / 16 \times p / 16\). The kernel is launched as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dim3 &lt;span style=&#34;color:#a6e22e&#34;&gt;blockSize&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dim3 &lt;span style=&#34;color:#a6e22e&#34;&gt;gridSize&lt;/span&gt;((p &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; blockSize.x &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; blockSize.x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              (m &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; blockSize.y &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; blockSize.y, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;matrixMultiplyGPU&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;gridSize, blockSize&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;(A_d, B_d, C_d, m, n, p);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What happens when the output matrix size exceeds the number of blocks per grid and threads per block? Either multiple kernels will be launched, each working with a submatrix of the original input, or each thread will be responsible for multiple elements.&lt;/p&gt;
&lt;h2 id=&#34;what-s-next&#34;&gt;What&amp;rsquo;s Next?&lt;/h2&gt;
&lt;p&gt;The complexity was slightly increased by considering multidimensional data. Matrices are a prime example of this. The algorithms explored required us to consider multiple input values to compute a single output value. However, the computation did not rely on any thread synchronization, so the task was still simple enough.&lt;/p&gt;
&lt;p&gt;Before diving into more complex operations like thread synchronization, was need a better understanding of the GPU&amp;rsquo;s architecture and memory hierarchy. With this knowledge at our disposal, we can begin to optimize our kernels for maximum performance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Heterogeneous Data Parallel Computing</title>
      <link>https://ajdillhoff.github.io/notes/heterogeneous_data_parallel_computing/</link>
      <pubDate>Sat, 30 Dec 2023 14:41:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/heterogeneous_data_parallel_computing/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#key-concepts&#34;&gt;Key Concepts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cuda-c-programs&#34;&gt;CUDA C Programs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-vector-addition&#34;&gt;Example: Vector Addition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#error-checking&#34;&gt;Error Checking&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;key-concepts&#34;&gt;Key Concepts&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Task Parallelism vs. Data Parallelism&lt;/li&gt;
&lt;li&gt;kernels&lt;/li&gt;
&lt;li&gt;threads&lt;/li&gt;
&lt;li&gt;grids&lt;/li&gt;
&lt;li&gt;blocks&lt;/li&gt;
&lt;li&gt;global memory&lt;/li&gt;
&lt;li&gt;data transfer&lt;/li&gt;
&lt;li&gt;error checking&lt;/li&gt;
&lt;li&gt;compilation of CUDA programs&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;This topic introduces the basics of data parallelism and CUDA programming. The most important concept is that data parallelism is achieved through independent computations on each sample or groups of samples. The basic structure of a CUDA C program consists of writing a &lt;strong&gt;kernel&lt;/strong&gt; that is executed independently on many threads. Memory must be allocated on the GPU device before transferring the data from the host machine (CPU). Upon completion of the kernel, the results need to be transferred back to the &lt;strong&gt;host&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;cuda-c-programs&#34;&gt;CUDA C Programs&lt;/h2&gt;
&lt;p&gt;A basic CUDA program consists of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;kernel&lt;/strong&gt; function defining the work to be performed on each thread.&lt;/li&gt;
&lt;li&gt;Data that is accessible on the &lt;strong&gt;device&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Device memory allocation.&lt;/li&gt;
&lt;li&gt;Memory transfer from the &lt;strong&gt;host&lt;/strong&gt; to the &lt;strong&gt;device&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Execution of the &lt;strong&gt;kernel&lt;/strong&gt; from the &lt;strong&gt;host&lt;/strong&gt; machine.&lt;/li&gt;
&lt;li&gt;Data transfer from the &lt;strong&gt;device&lt;/strong&gt; back to the &lt;strong&gt;host&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Memory cleanup.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At first glance, the execution flow of a CUDA program appears sequential; you launch the threads on the GPU and wait for it to complete. A more realistic program would launch the threads and continue local execution, if necessary.&lt;/p&gt;
&lt;h2 id=&#34;example-vector-addition&#34;&gt;Example: Vector Addition&lt;/h2&gt;
&lt;p&gt;Hwu et al. refer to vector addition as the &amp;ldquo;Hello World&amp;rdquo; of GPU programming (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;). It is a simple problem that can be described as &lt;em&gt;embarrassingly parallel&lt;/em&gt;. Vector addition is a simple operation. Given two vectors of the same length, \(\mathbf{x}\) and \(\mathbf{y}\), the vector addition operation is defined as:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{z}_i = \mathbf{x}_i + \mathbf{y}_i \quad \forall i \in \{1, \ldots, n\}
\]&lt;/p&gt;
&lt;p&gt;The vector addition operation is commutative and associative. The operation can be performed in parallel on each element of the vectors. This can be implemented simply in C.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;vecAdd&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x_h, &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;y_h, &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;z_h, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; n; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        z_h[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x_h[i] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; y_h[i];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;One small note about the variable names: it is common to use the suffix `_h` to denote a variable that is allocated on the host (CPU) and `_d` to denote a variable that is allocated on the device (GPU). In this case, the vector addition operation is performed on the host machine.&lt;/p&gt;
&lt;p&gt;An equivalent implementation in CUDA C is shown below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__
void vecAdd(float *x_d, float *y_d, float *z_d, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &amp;lt; n) {
        z_d[i] = x_d[i] + y_d[i];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This kernel executes on a single thread. The thread index is computed using built-in variables `blockIdx.x`, `blockDim.x`, and `threadIdx.x`. The details of how these variables are defined are not important right now. The main point is that each kernel is executed on a single thread. For a GPU with thousands of individual threads, this kernel will be executed thousands of times in parallel.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;__global__&lt;/code&gt; keyword placed before the function definition indicates that the function can be called from both the host and the device, but it is only executed on the device. The table below shows the different keywords used to define functions in CUDA C.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Keyword&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;__global__&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Executed on the device, callable from the host and device&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;__device__&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Executed on the device, callable from the device only&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;__host__&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Executed on the host, callable from the host only&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Unless otherwise specified, functions that you define will be executed on the host. That is, it is not necessary to specify the &lt;code&gt;__host__&lt;/code&gt; keyword. If you want the compiler to generate both host and device code, you can use the &lt;code&gt;__host__ __device__&lt;/code&gt; keyword combination.&lt;/p&gt;
&lt;p&gt;The kernel is executed on the host machine using the following code.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;int main() {
    // Allocate memory on the host
    float *x_h, *y_h, *z_h;
    int n = 1024;

    x_h = malloc(n * sizeof(float));
    y_h = malloc(n * sizeof(float));
    z_h = malloc(n * sizeof(float));

    // Allocate memory on the device
    float *x_d, *y_d, *z_d;
    cudaMalloc(&amp;amp;x_d, n * sizeof(float));
    cudaMalloc(&amp;amp;y_d, n * sizeof(float));
    cudaMalloc(&amp;amp;z_d, n * sizeof(float));

    // Transfer data from host to device
    cudaMemcpy(x_d, x_h, n * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(y_d, y_h, n * sizeof(float), cudaMemcpyHostToDevice);

    // Execute kernel
    vecAdd&amp;lt;&amp;lt;&amp;lt;ceil(n / 256.0), 256&amp;gt;&amp;gt;&amp;gt;(x_d, y_d, z_d, n);

    // Transfer data from device to host
    cudaMemcpy(z_h, z_d, n * sizeof(float), cudaMemcpyDeviceToHost);

    // Free memory on host and device
    free(x_h);
    free(y_h);
    free(z_h);
    cudaFree(x_d);
    cudaFree(y_d);
    cudaFree(z_d);
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;There is a lot to unpack here, so we&amp;rsquo;ll start from the top.&lt;/p&gt;
&lt;h3 id=&#34;memory-allocation&#34;&gt;Memory Allocation&lt;/h3&gt;
&lt;p&gt;It doesn&amp;rsquo;t really matter where the host data comes from or how it is allocated, but the above example allocates memory using &lt;code&gt;malloc&lt;/code&gt; anyway. Before transferring data to the device, we must allocate memory on it. This is done via &lt;code&gt;cudaMalloc&lt;/code&gt;. The first argument is a pointer to address of the variable. Remember that taking the address of a pointer will result in a double pointer. This is necessary because the function will need to dereference the pointer to store the address to the allocated data. Once the memory is allocated on the device, it cannot be accessed from the host until it is transferred back.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-05_11-54-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Overview of memory layout (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Overview of memory layout (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The memory that is allocated on the device is called &lt;strong&gt;global memory&lt;/strong&gt;. It is accessible by all threads on the device. There is also a small amount of &lt;strong&gt;shared memory&lt;/strong&gt; that is accessible by threads within a single block along with a &lt;strong&gt;unified memory&lt;/strong&gt; model.&lt;/p&gt;
&lt;h3 id=&#34;memory-transfer&#34;&gt;Memory Transfer&lt;/h3&gt;
&lt;p&gt;Now that the memory has been allocated, the data can be safely transferred from the host to the device. This is accomplished using &lt;code&gt;cudaMemcpy&lt;/code&gt;. The arguments are the &lt;strong&gt;destination pointer&lt;/strong&gt;, &lt;strong&gt;source pointer&lt;/strong&gt;, &lt;strong&gt;size&lt;/strong&gt;, and &lt;strong&gt;direction&lt;/strong&gt;. The direction is an enumerated type that can be one of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cudaMemcpyHostToDevice&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cudaMemcpyDeviceToHost&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cudaMemcpyDeviceToDevice&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will only focus on the first two for now.&lt;/p&gt;
&lt;h3 id=&#34;grids-blocks-and-threads&#34;&gt;Grids, Blocks, and Threads&lt;/h3&gt;
&lt;p&gt;The CUDA programming model is based on a hierarchy of &lt;strong&gt;grids&lt;/strong&gt;, &lt;strong&gt;blocks&lt;/strong&gt;, and &lt;strong&gt;threads&lt;/strong&gt;. A &lt;strong&gt;grid&lt;/strong&gt; is a collection of &lt;strong&gt;blocks&lt;/strong&gt;. A &lt;strong&gt;block&lt;/strong&gt; is a collection of &lt;strong&gt;threads&lt;/strong&gt;. The number of &lt;strong&gt;blocks&lt;/strong&gt; and &lt;strong&gt;threads&lt;/strong&gt; is defined by the programmer. The number of &lt;strong&gt;blocks&lt;/strong&gt; and &lt;strong&gt;threads&lt;/strong&gt; that can be executed in parallel is limited by the hardware. The number of &lt;strong&gt;blocks&lt;/strong&gt; and &lt;strong&gt;threads&lt;/strong&gt; that can be executed in parallel is called the &lt;strong&gt;grid size&lt;/strong&gt; and &lt;strong&gt;block size&lt;/strong&gt;, respectively.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-05_11-22-39_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A single block of 256 threads (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A single block of 256 threads (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The figure above shows a single block of 256 threads. This could be one of many blocks in a grid. The threads within each block are executed in parallel and do not interact with threads in other blocks. For threads within a single block, there is a small amount of shared memory as well as other tools for communication. We will explore these in more depth as we dive into the details of the CUDA architecture.&lt;/p&gt;
&lt;h3 id=&#34;kernel-execution&#34;&gt;Kernel Execution&lt;/h3&gt;
&lt;p&gt;Calling the kernel function almost looks like any ordinary function call. The main difference is the inclusion of the &lt;code&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/code&gt; and &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; syntax. These are used to specify the size of the grid and blocks, respectively. In this example, we specified that each block has 256 threads. We can use that specification to dynamically determine the number of blocks based on the input size. The number of blocks is computed as the ceiling of the input size divided by the number of threads per block. This ensures that there are enough blocks to cover the entire input size.&lt;/p&gt;
&lt;p&gt;Returning to the kernel function, the thread index is computed using built-in variables &lt;code&gt;blockIdx.x&lt;/code&gt;, &lt;code&gt;blockDim.x&lt;/code&gt;, and &lt;code&gt;threadIdx.x&lt;/code&gt;. These are defined as &lt;code&gt;struct&lt;/code&gt; variables. Modern GPUs have a 3-dimensional grid, but we only need to worry about the first dimension for now. The thread index is computed as the product of the block index and the number of threads per block plus the thread index within the block. This is a common pattern for computing the thread index.&lt;/p&gt;
&lt;p&gt;You may have noticed that it is possible to have more threads than there are blocks. As much as possible, you should try and work with powers of 2. This will ensure that the hardware is used as efficiently as possible. You can always request more threads than there are data points and ignore the threads that are not needed. In this example, we check to see if the thread index is less than the input size. If it is, the vector addition operation is performed. Otherwise, the function exits.&lt;/p&gt;
&lt;p&gt;There are limits to the number of blocks and threads that can be executed in parallel. These limits are based on the compute capability of the device, referenced &lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;compiling&#34;&gt;Compiling&lt;/h3&gt;
&lt;p&gt;CUDA code is compiled using the NVCC compiler driver. It works by compiling host code using the host&amp;rsquo;s native C/C++ compiler and device code to PTX, the CUDA instruction set architecture. Each snippet of code is separated based on the CUDA keyword used to define it. For example, the &lt;code&gt;__global__&lt;/code&gt; keyword used to define the kernel function informs &lt;code&gt;nvcc&lt;/code&gt; that it should be compiled to a PTX file.&lt;/p&gt;
&lt;h2 id=&#34;error-checking&#34;&gt;Error Checking&lt;/h2&gt;
&lt;p&gt;The functions we use in the CUDA API return an error code. We can use this to create robust code that checks for errors and either corrects them or exits gracefully. The following example shows a simple way to check the result of `cudaMalloc`:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;cudaError_t err = cudaMalloc(&amp;amp;x_d, n * sizeof(float));
if (err != cudaSuccess) {
    fprintf(stderr, &amp;#34;Error: %s\n&amp;#34;, cudaGetErrorString(err));
    exit(EXIT_FAILURE);
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;A common pattern is to define a macro that checks the result of a CUDA function and exits if there is an error. This is shown below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true) {
   if (code != cudaSuccess) {
      fprintf(stderr,&amp;#34;GPUassert: %s %s %d\n&amp;#34;, cudaGetErrorString(code), file, line);
      if (abort) exit(code);
   }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;A small note on the above macro, it is technically C++ code. As of writing this, CUDA does not support all features of C++, but much of the code you will see is written as a mix of C and C++. CUDA was originally developed for C, but C++ features have slowly been introduced over time. If you view the &lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;official documentation&lt;/a&gt;, you can see that the link is defined as `cuda-c-programming-guide`, but the actual document has been renamed to `CUDA C++ Programming Guide`.&lt;/p&gt;
&lt;p&gt;Don&amp;rsquo;t overthink the C/C++ distinction. The main point is that you can use C++ features in your CUDA code, but you should be aware that not all features are supported.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to GPGPU Programming</title>
      <link>https://ajdillhoff.github.io/notes/introduction_to_gpgpu_programming/</link>
      <pubDate>Wed, 20 Dec 2023 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/introduction_to_gpgpu_programming/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#structure-of-the-course&#34;&gt;Structure of the Course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#heterogeneous-parallel-computing&#34;&gt;Heterogeneous Parallel Computing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#measuring-speedup&#34;&gt;Measuring Speedup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gpu-programming-history&#34;&gt;GPU Programming History&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#applications&#34;&gt;Applications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-to-expect-from-this-course&#34;&gt;What to expect from this course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;structure-of-the-course&#34;&gt;Structure of the Course&lt;/h2&gt;
&lt;p&gt;The primary of this goal is of course to learn how to program GPUs. A key skill that will be developed is the ability to think in parallel. We will start with simple problems that are &lt;em&gt;embarrassingly parallel&lt;/em&gt; and then move on to more complex problems that require synchronization. One of the biggest challenges will be in converting processes that are simple to reason about in serial to parallel processes.&lt;/p&gt;
&lt;p&gt;The course is divided into three parts. The first part will cover the fundamentals of heterogeneous parallel computing and the CUDA programming model. We will focus on problems that are mostly embarrassingly parallel, but will also step into more complicated problems.&lt;/p&gt;
&lt;p&gt;The second part will cover primitive parallel patterns. These are patterns from well-known algorithms that can be used to solve a wide variety of problems. Think of these as useful blueprints for solving problems in parallel. During the second part, we will also dive into more advanced usages of CUDA.&lt;/p&gt;
&lt;p&gt;Part three will cover advanced patterns from more specific applications, such as iterative MRI reconstruction. The course will conclude with expert practices.&lt;/p&gt;
&lt;p&gt;There will be regular assignments that focus on the concepts learned throughout the course. These will typically be accompanied by a series of questions to reinforce and verify that you are successful in each step. Quizzes will be given after each assignment to serve as a checkpoint.&lt;/p&gt;
&lt;h2 id=&#34;heterogeneous-parallel-computing&#34;&gt;Heterogeneous Parallel Computing&lt;/h2&gt;
&lt;p&gt;Heterogeneous computing refers to systems that use more than one kind of processor or core. One common theme in the course will be to focus on a perfect union between the CPU and GPU. Not every task can be fully parallelized. Many tasks are well suited for sequential processing and others are better suited for parallel processing. Parallelism can be further broken down into data parallelism and task parallelism. The majority of our time will be focused on data parallelism, but it is important to keep in mind that not everything fits into this category. Over time, you will develop a sense for what fits this paradigm and what does not.&lt;/p&gt;
&lt;p&gt;The idea of parallelism is certainly not new, but it has become ubiquitous in the computing space. Consider 30 years ago, when most consumer computers had a single core. The race between chip designers resulted in increasing single-core performance year after year in the form of increased clock speeds. This was a great way to increase performance, but it came at the cost of increased power consumption and heat. Scaling down transistors has also be a tried and true way of decreasing processor size and increasing performance. However, we are quickly reaching a physical limit on the size of a transistor.&lt;/p&gt;
&lt;p&gt;The solution to these problems is the same solution seen in scaling up large systems: horizontal scaling. The intuition is straightforward: many things can do the work faster than a single thing. For large-scale systems, the answer is distributed systems in which no single unit needs to be overly powerful or complicated. For consumer processors, this comes in the form of additional cores on a chip.&lt;/p&gt;
&lt;p&gt;In the context of CPUs, adding multiple cores means that we have a multi-core homogeneous system. These are general-purpose processors that can complete any computational task. The cores are identical and can be used interchangeably. The cores are also tightly coupled, meaning that they share memory and can communicate with each other. A similar statement can be made for GPUs. Let&amp;rsquo;s take a look at the differences between them.&lt;/p&gt;
&lt;h3 id=&#34;latency-vs-dot-throughput&#34;&gt;Latency vs. Throughput&lt;/h3&gt;
&lt;p&gt;CPUs follow a latency-first design. The space on the chip itself is not fully dedicated to the processing units. Instead, space is reserved for things like cache, branch prediction, and other features that reduce latency. All computational tasks can be completed on a CPU, but the throughput may be lower than a GPU.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-12-21_15-33-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;CPU Architecture from CUDA C&amp;#43;&amp;#43; Programming Guide.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;CPU Architecture from CUDA C++ Programming Guide.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;GPUs follow a throughput-first design. The space on the chip is dedicated to processing units such as ALUs. The cores themselves are not as sophisticated as those found on a CPU. Communication between cores takes more time and is more difficult, but having more of them means that the raw throughput of the chip is higher.&lt;/p&gt;
&lt;p&gt;The development of GPUs was driven by the gaming industry, specifically with rendering, where many vertices and pixels need to be processed in parallel. As we explore GPU solutions to different problems, we will see that data delivery is a key bottleneck. There are techniques available to get around this, which we will need to study closely.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-12-21_15-34-18_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;GPU Architecture from CUDA C&amp;#43;&amp;#43; Programming Guide.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;GPU Architecture from CUDA C++ Programming Guide.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;gpus-and-supercomputing&#34;&gt;GPUs and Supercomputing&lt;/h3&gt;
&lt;p&gt;GPUs are featured in many of the top 500 supercomputers. This goes to show that they are a powerful and cost-efficient tool for solving problems. The table below shows the top 5 supercomputers as of November 2023. 4 of them utilize some form of GPU acceleration.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;CPUs&lt;/th&gt;
&lt;th&gt;GPUs&lt;/th&gt;
&lt;th&gt;Peak PFlop/s&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Frontier (Oak Ridge NL)&lt;/td&gt;
&lt;td&gt;606,208 cores&lt;/td&gt;
&lt;td&gt;37,888 AMD MI250X&lt;/td&gt;
&lt;td&gt;1,679.72&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Aurora (Argonne NL)&lt;/td&gt;
&lt;td&gt;1,100,000 cores (est.)&lt;/td&gt;
&lt;td&gt;63,744 Intel GPU Max&lt;/td&gt;
&lt;td&gt;1,059.33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Eagle (Microsoft Azure)&lt;/td&gt;
&lt;td&gt;1,123,200 cores (combined)&lt;/td&gt;
&lt;td&gt;Unknown Split (NVIDIA H100)&lt;/td&gt;
&lt;td&gt;846.74&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fugaku&lt;/td&gt;
&lt;td&gt;7,630,848 cores&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;537.21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LUMI&lt;/td&gt;
&lt;td&gt;362,496 cores&lt;/td&gt;
&lt;td&gt;11,712 AMD MI250X&lt;/td&gt;
&lt;td&gt;531.51&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The results are clear: heterogeneous parallel computing is a powerful tool for solving problems. Learning how to use these tools will be a valuable skill for the future.&lt;/p&gt;
&lt;h2 id=&#34;measuring-speedup&#34;&gt;Measuring Speedup&lt;/h2&gt;
&lt;p&gt;In general, if system A takes \(T_A\) time to complete a task and system B takes \(T_B\) time to complete the same task, then the speedup of system B over system A is given by \(S = \frac{T_A}{T_B}\).&lt;/p&gt;
&lt;p&gt;Amdahl&amp;rsquo;s law is defined as follows:&lt;/p&gt;
&lt;p&gt;\[S(s) = \frac{1}{(1 - p) + \frac{p}{s}}\]&lt;/p&gt;
&lt;p&gt;where \(p\) is the fraction of the task that can be parallelized and \(s\) is the speedup of the part of the task that can be parallelized.&lt;/p&gt;
&lt;p&gt;It is not common that 100% of a task can be parallelized. Amdah&amp;rsquo;s law takes this into account. Suppose that 40% of a given task can benefit from parallelization. If that part of the task can be sped up by a factor of 10, then the overall speedup is given by:&lt;/p&gt;
&lt;p&gt;\[S = \frac{1}{(1 - 0.4) + \frac{0.4}{10}} = 1.56\]&lt;/p&gt;
&lt;p&gt;In virtually every lab that you will do in this course, you will be asked to measure the speedup of your solution. This is a good way to verify that your solution is correct and that it is actually faster than the serial version. This will also be a critical part of your project, where you will first need to create a serial version of your solution and then parallelize it.&lt;/p&gt;
&lt;h2 id=&#34;gpu-programming-history&#34;&gt;GPU Programming History&lt;/h2&gt;
&lt;p&gt;Early GPU programming was done using OpenGL and DirectX. These were graphics APIs, so everything had to be done in terms of pixel shaders. Researchers found ways to use these APIs to do general purpose computing, but it was very difficult since one could not easily debug the code. Essentially, the input had to be encoded as a texture or color. The GPU would then process the texture and output the result as a texture. The output would then have to be decoded to get the result.&lt;/p&gt;
&lt;p&gt;In 2006, NVIDIA unveiled the GeForce 8800 GTX, which was the first DirectX 10 GPU. More importantly, it was the first GPU built using the CUDA architecture. CUDA also refers to the programming model that NVIDIA developed to facilitate general purpose GPU programming. A key piece of the CUDA architecture is the unified shader pipepline, which allows each ALU to be utilized for general purpose computations.&lt;/p&gt;
&lt;p&gt;The different ALUs have access to a global memory space as well as a shared memory space managed by software. We will explore the specifics of this architecture in part 1 of this course. Since that time, many major changes have been made to the CUDA architecture. Additionally, many other standards have been developed to facilitate GPU programming and parallel computing in general.&lt;/p&gt;
&lt;p&gt;One of the most important standards, which we also study in this course, is OpenCL. OpenCL is an open standard that allows for heterogeneous parallel computing. It is supported by many different vendors, including NVIDIA, AMD, and Intel. OpenCL is a C-like language that allows for the creation of kernels that can be executed on a variety of devices. The OpenCL standard is maintained by the Khronos Group, which also maintains the OpenGL standard.&lt;/p&gt;
&lt;h2 id=&#34;applications&#34;&gt;Applications&lt;/h2&gt;
&lt;p&gt;We are currently in the midst of a data explosion. Vertical scaling, the idea of improving a single system, cannot meet the demands of modern challenges. Horizontal scaling is the most sure solution for now. Distributed systems utilize cheap, commodity servers in lieu of complex supercomputers to distribute applications to mass markets. Parallel computation has applications in just about every field imaginable. We will try to cover a wide variety of applications, as many of them feature parallel solutions that are helpful in other domains.&lt;/p&gt;
&lt;h3 id=&#34;linear-algebra-libraries&#34;&gt;Linear Algebra Libraries&lt;/h3&gt;
&lt;p&gt;One of the most widely utilized applications of data parallelism is in linear algebra libraries. Common matrix operations such as matrix multiplication and matrix inversion are highly parallelizable. The &lt;a href=&#34;https://developer.nvidia.com/cublas&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;cuBLAS&lt;/a&gt; library is a highly optimized implementation of these operations.&lt;/p&gt;
&lt;p&gt;For a great overview of the evolution of linear algebra libraries and the impact of GPUs, see Jack Dongarra&amp;rsquo;s keynote speech at the &lt;a href=&#34;https://youtu.be/8TyyCWuquI0?si=DkPEDPWp7_n8GnVe&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;50 Years of Computing at UTA&lt;/a&gt; event.&lt;/p&gt;
&lt;h3 id=&#34;machine-learning&#34;&gt;Machine Learning&lt;/h3&gt;
&lt;p&gt;Model training and optimization in machine learning is a perfect candidate for data parallelism. Large models such as Llama2 require a massive amount of data to train (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Touvron et al. 2023&lt;/a&gt;). Deep learning models such as this are trained on many GPUs that can execute functions on independent data points in parallel.&lt;/p&gt;
&lt;p&gt;NVIDIA has developed a useful library, which we will study in this course, called &lt;a href=&#34;https://developer.nvidia.com/cudnn&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;cuDNN&lt;/a&gt; that implements highly optimized implementations of common functions used in a deep learning pipeline. High level frameworks build off of this library to provide easier development interfaces for machine learning practitioners. Popular examples include &lt;a href=&#34;https://pytorch.org&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;PyTorch&lt;/a&gt;, &lt;a href=&#34;https://www.tensorflow.org&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;TensorFlow&lt;/a&gt;, and &lt;a href=&#34;https://github.com/google/jax&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;JAX&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;computer-vision&#34;&gt;Computer Vision&lt;/h3&gt;
&lt;p&gt;Most of the current state-of-the-art computer vision methods are driven by deep learning, so they also benefit greatly from data parallelism. &lt;a href=&#34;https://ajdillhoff.github.io/notes/convolutional_neural_networks/&#34;&gt;Convolutional Neural Networks&lt;/a&gt; (CNN) have been the driving force behind machine-learning based computer vision methods. They are parameter efficient and take advantage of data parallelism. We will study the core operation behind this model, the convolutional opreator.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-12-21_15-02-18_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;2D Convolution on a 4x4 grid using a 3x3 filter with unit stride (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;2D Convolution on a 4x4 grid using a 3x3 filter with unit stride (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;computational-chemistry&#34;&gt;Computational Chemistry&lt;/h3&gt;
&lt;p&gt;CUDA has been utilized for computing heat transfer calculations efficiently (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Sosutha and Mohana 2015&lt;/a&gt;). The authors found that the computations could be computed independently, which is perfect for a parallel architecture like a GPU, where throughput is preferred to latency.&lt;/p&gt;
&lt;h3 id=&#34;other-applications&#34;&gt;Other Applications&lt;/h3&gt;
&lt;p&gt;There are many other applications of data parallelism, some of which we will explore and learn from in this course. Examples include the following.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Financial Analysis&lt;/li&gt;
&lt;li&gt;Scientific Simulation&lt;/li&gt;
&lt;li&gt;Engineering Simulation&lt;/li&gt;
&lt;li&gt;Data Intensive Analytics&lt;/li&gt;
&lt;li&gt;Medical Imaging&lt;/li&gt;
&lt;li&gt;Digital Audio Processing&lt;/li&gt;
&lt;li&gt;Digital Video Processing&lt;/li&gt;
&lt;li&gt;Biomedical Informatics&lt;/li&gt;
&lt;li&gt;Electronic Design Automation&lt;/li&gt;
&lt;li&gt;Statistical Modeling&lt;/li&gt;
&lt;li&gt;Numerical Methods&lt;/li&gt;
&lt;li&gt;Ray Tracing Rendering&lt;/li&gt;
&lt;li&gt;Interactive Physics&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-to-expect-from-this-course&#34;&gt;What to expect from this course&lt;/h2&gt;
&lt;p&gt;This course is extremely hands-on. Almost every topic we cover will have an associated programming exercise. Some of these exercises will be integrated into assignments, other will be presented as in-class demonstrations. The fact that there are so many applications means you will need to be able to adapt to new domains quickly. By the end of this course, you should have acquired the following skills:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Advanced familiarity with the CUDA programming model&lt;/li&gt;
&lt;li&gt;Ability to think in parallel&lt;/li&gt;
&lt;li&gt;Identify sections of code that can be parallelized&lt;/li&gt;
&lt;li&gt;Implementation of parallel solutions&lt;/li&gt;
&lt;li&gt;Debugging parallel code&lt;/li&gt;
&lt;li&gt;Measuring performance increase from parallelization&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Dumoulin, Vincent, and Francesco Visin. 2018. “A Guide to Convolution Arithmetic for Deep Learning.” &lt;i&gt;Arxiv:1603.07285 [Cs, Stat]&lt;/i&gt;, January. &lt;a href=&#34;http://arxiv.org/abs/1603.07285&#34;&gt;http://arxiv.org/abs/1603.07285&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Sosutha, S., and D. Mohana. 2015. “Heterogeneous Parallel Computing Using Cuda for Chemical Process.” &lt;i&gt;Procedia Computer Science&lt;/i&gt;, Graph Algorithms, High Performance Implementations and Its Applications ( ICGHIA 2014 ), 47 (January): 237–46. &lt;a href=&#34;https://doi.org/10.1016/j.procs.2015.03.203&#34;&gt;https://doi.org/10.1016/j.procs.2015.03.203&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_3&#34;&gt;&lt;/a&gt;Touvron, Hugo, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, et al. 2023. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” arXiv. &lt;a href=&#34;https://doi.org/10.48550/arXiv.2307.09288&#34;&gt;https://doi.org/10.48550/arXiv.2307.09288&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>MapReduce</title>
      <link>https://ajdillhoff.github.io/notes/mapreduce/</link>
      <pubDate>Fri, 24 Nov 2023 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/mapreduce/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-mapreduce&#34;&gt;What is MapReduce?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hadoop-distributed-file-system--hdfs&#34;&gt;Hadoop Distributed File System (HDFS)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mapreduce-overview&#34;&gt;MapReduce Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hadoop-v2-aka-yarn&#34;&gt;Hadoop v2 AKA YARN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;These are my personal notes from the book &lt;em&gt;Fundamentals of Database Systems&lt;/em&gt; by (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Elmasri and Navathe 2015&lt;/a&gt;). I highly recommend reading the original source material. The contents of the article should only serve as a brief overview of the topic.&lt;/p&gt;
&lt;h2 id=&#34;what-is-mapreduce&#34;&gt;What is MapReduce?&lt;/h2&gt;
&lt;p&gt;MapReduce is a programming model for processing large datasets in parallel. It was originally developed by Jeffrey Dean and Sanjay Ghemawat at Google in 2004 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dean and Ghemawat 2008&lt;/a&gt;). It is based on the functional programming paradigm and is inspired by the map and reduce functions in Lisp and other functional languages. The MapReduce programming model is implemented in the Hadoop framework.&lt;/p&gt;
&lt;p&gt;Hadoop is made up of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hadoop Distributed File System (HDFS)&lt;/li&gt;
&lt;li&gt;Yet Another Resource Negotiator (YARN)&lt;/li&gt;
&lt;li&gt;MapReduce&lt;/li&gt;
&lt;li&gt;Hadoop Common&lt;/li&gt;
&lt;/ul&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-11-26_20-32-30_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Diagram of MapReduce execution (Elmasri and Navathe).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Diagram of MapReduce execution (Elmasri and Navathe).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;example-word-count&#34;&gt;Example: Word Count&lt;/h3&gt;
&lt;p&gt;The classic introductory example for MapReduce is word count, as described in (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dean and Ghemawat 2008&lt;/a&gt;). Let&amp;rsquo;s say we have a collection of text documents that we want to preprocess for a Natural Language Processing pipeline. One of the first steps is to count the number of times each word appears in the corpus. This is a simple task that can be done in a single machine, but let&amp;rsquo;s assume that the corpus is too large to fit in memory on a single machine. We can use MapReduce to distribute the work across multiple machines.&lt;/p&gt;
&lt;p&gt;The problem is split into two steps: map and reduce. Each step is represented as a function that can run on any arbitrary number of nodes. For now, we will not worry about how the original data is split up efficiently. Instead, assume that each machine gets a single document.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;map&lt;/span&gt;(doc):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; word &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; doc:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        emit(word, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;reduce&lt;/span&gt;(word, counts):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    emit(word, sum(counts))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The map function takes a document and emits a key-value pair for each word in the document. The key is the word and the value is 1. The reduce function takes a word and a list of counts and emits a key-value pair with the word and the sum of the counts. The output of the map function is a list of key-value pairs that are grouped by key. The reduce function is then applied to each group of key-value pairs.&lt;/p&gt;
&lt;h2 id=&#34;hadoop-distributed-file-system--hdfs&#34;&gt;Hadoop Distributed File System (HDFS)&lt;/h2&gt;
&lt;p&gt;Store metadata and application data on different nodes.
Metadata is stored on the NameNode.
Application data is stored on DataNodes. This data is replicated across multiple DataNodes.
HDFS uses primary-secondary architecture. The NameNode is the primary and the DataNodes are the secondaries.
DataNodes are typically partitioned into 1 node per machine.
NameNodes maintain inodes about file and directories. These inodes are used to map file blocks to DataNodes.
NameNodes instruct the DataNodes to create, delete, and replicate blocks.
Data is retrieved by contacting the NameNode to get the block locations and then contacting the DataNodes directly.&lt;/p&gt;
&lt;h3 id=&#34;namenode&#34;&gt;NameNode&lt;/h3&gt;
&lt;p&gt;Maintain an image of the file system.
Maintain a journal of changes to the file system.&lt;/p&gt;
&lt;p&gt;Secondary NameNodes are used to create checkpoints of the NameNode&amp;rsquo;s state.&lt;/p&gt;
&lt;h3 id=&#34;datanode&#34;&gt;DataNode&lt;/h3&gt;
&lt;p&gt;Periodically send heartbeats to the NameNode to indicate their current state (BlockReport).
Block locations are not part of the namespace image.
BlockReports are used by services like the MapReduce JobTracker to determine where to schedule tasks.&lt;/p&gt;
&lt;h3 id=&#34;file-i-o-operations&#34;&gt;File I/O Operations&lt;/h3&gt;
&lt;p&gt;HDFS is single-writer, multiple-reader.
A file consists of blocks.
Data that is written on the last block becomes available after an hflush operation.&lt;/p&gt;
&lt;h2 id=&#34;mapreduce-overview&#34;&gt;MapReduce Overview&lt;/h2&gt;
&lt;p&gt;MapReduce is also a primary-secondary architecture.
The JobTracker is the primary process and the TaskTrackers are the secondaries.&lt;/p&gt;
&lt;h3 id=&#34;jobtracker&#34;&gt;JobTracker&lt;/h3&gt;
&lt;p&gt;Manages life cycles of jobs and schedules tasks on the cluster.&lt;/p&gt;
&lt;h4 id=&#34;job-submission&#34;&gt;Job Submission&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Gets a new ID from the job tracker.&lt;/li&gt;
&lt;li&gt;Verifies output specifications.&lt;/li&gt;
&lt;li&gt;Computes input splits for the job.&lt;/li&gt;
&lt;li&gt;Copies any resources needed to run the job.&lt;/li&gt;
&lt;li&gt;Informs the job tracker that it is ready for execution.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;job-initialization&#34;&gt;Job Initialization&lt;/h4&gt;
&lt;p&gt;When a job is initialized, it is placed in a queue with all information related to executing and tracking the job. A map task is created for each of the input splits. Further, a job setup and cleanup task are created.&lt;/p&gt;
&lt;h4 id=&#34;task-assignment&#34;&gt;Task Assignment&lt;/h4&gt;
&lt;p&gt;Each TaskTracker periodically sends a &lt;em&gt;heartbeat&lt;/em&gt; to the JobTracker with status updates. These inform the JobTracker that it is alive or is able to run a new task. When a TaskTracker is ready to run a new task, the JobTracker will allocate a new one by selecting it using some defined scheduler. There is a default scheduler that will pick based on priority, but a custom scheduler can be given to the job.&lt;/p&gt;
&lt;p&gt;Another consideration for assigning is a map task is &lt;strong&gt;data locality&lt;/strong&gt;. Map tasks are based on the input splits, so the JobTracker will try to run the task on the same node that the data is located. If this is not possible, it will prioritize based on the distance between the node and the data. For reduce tasks, there are no locality considerations.&lt;/p&gt;
&lt;h4 id=&#34;task-execution&#34;&gt;Task Execution&lt;/h4&gt;
&lt;p&gt;When a task is executed, any pertinent information is copied from the shared filesystem to a local filesystem. A JVM is launched to run each task so that any errors will only affect the JVM and not the entire TaskTracker. The JVM will run the task and then report back to the JobTracker with status updates.&lt;/p&gt;
&lt;h4 id=&#34;job-completion&#34;&gt;Job Completion&lt;/h4&gt;
&lt;p&gt;A job is completed once the last task is finished. In this case, a job cleanup task is run to clean up any resources used by the job.&lt;/p&gt;
&lt;h3 id=&#34;tasktracker&#34;&gt;TaskTracker&lt;/h3&gt;
&lt;p&gt;TaskTrackers run one per worker node on a cluster. Both map and reduce tasks run on Worker nodes. When a TaskTracker is started, it registers with the JobTracker so that the JobTracker can assign tasks to it. The actual task is run in a separate process on the Worker node which is managed by the TaskTracker.&lt;/p&gt;
&lt;h3 id=&#34;fault-tolerance&#34;&gt;Fault Tolerance&lt;/h3&gt;
&lt;p&gt;In Hadoop v1, three types of failures must be considered. The first two are the TaskTracker and JobTracker. The third is the spawned process that runs the task on the TaskTracker.&lt;/p&gt;
&lt;h4 id=&#34;task-failure&#34;&gt;Task Failure&lt;/h4&gt;
&lt;p&gt;Tasks can fail due to bad input, faulty code, hardware failure, or some other run time error. When an individual task fails, the error is logged and the failure is reported back to the parent TaskTracker. Since the TaskTracker is still running, it can notify the JobTracker that it is free to run another task.&lt;/p&gt;
&lt;p&gt;There is also a default timeout duration for tasks that are not making progress. If a task exceeds this timeout, it is killed and the TaskTracker is notified. The default timeout is 10 minutes but can be configured by the user. There are also settings dictating how many times a task can fail before the job is considered failed or the percentage of tasks that can fail before the job is considered failed. There may be circumstances in which task failures are acceptable as long as some of the work is completed.&lt;/p&gt;
&lt;h4 id=&#34;tasktracker-failure&#34;&gt;TaskTracker Failure&lt;/h4&gt;
&lt;p&gt;TaskTrackers that fail or are unresponsive past the heartbeat timeout are considered dead. The JobTracker will remove it from its pool of trackers to schedule tasks on. Tasks that were completed or in progress on the failed TaskTracker are rescheduled since the intermediate data is no longer available.&lt;/p&gt;
&lt;p&gt;TaskTrackers that fail repeatedly are added to a blacklist and are not used for scheduling tasks. This can occur if the TaskTracker is not configured correctly or if the TaskTracker is running on faulty hardware.&lt;/p&gt;
&lt;h4 id=&#34;jobtracker-failure&#34;&gt;JobTracker Failure&lt;/h4&gt;
&lt;p&gt;The JobTracker is a single point of failure in Hadoop v1. If the JobTracker fails, all jobs that are running or waiting to run are lost. The rate of failure is typically low since the chance that a single machine fails is low. If it does fail, a restart is attempted and all running jobs need to be restarted.&lt;/p&gt;
&lt;h3 id=&#34;shuffle-and-sort&#34;&gt;Shuffle and Sort&lt;/h3&gt;
&lt;p&gt;The shuffle and sort phase is a key process which defines how data is moved from the map tasks to the reduce tasks. Data may be split among many map tasks, but reducers get all rows for a given key together. The shuffle and sort phase is responsible for this. It is split up into three phases.&lt;/p&gt;
&lt;h4 id=&#34;map-phase&#34;&gt;Map Phase&lt;/h4&gt;
&lt;p&gt;Output from the map tasks is stored in memory in a circular buffer. Once that buffer becomes full, it spills over to the disk. Before going to the disk, it is partitioned based on the reducer that it will ultimately be sent to. This acts as a sort of pre-sorting to optimize the shuffle and sort phase.&lt;/p&gt;
&lt;p&gt;Depending on the setting for the size of spill files, the map phase may produce multiple spill files. These files are merged into a single file before being sent to the reducers. The final result is a single file per reducer that is sorted by key.&lt;/p&gt;
&lt;h4 id=&#34;copy-phase&#34;&gt;Copy Phase&lt;/h4&gt;
&lt;p&gt;The data needed by a particular reducer task is split up among many map tasks. The copy phase is responsible for copying the data from the map tasks to the reducer tasks. The reducer will begin copying as data is made available by the map tasks, even if all map tasks have not been completed.&lt;/p&gt;
&lt;p&gt;Copies can be executed in parallel via threading. The JobTracker is responsible for assigning the reducers to the map tasks so that the data is copied to the correct reducer. When all map tasks have completed, the reducer will begin the reduce phase.&lt;/p&gt;
&lt;h4 id=&#34;reduce-phase&#34;&gt;Reduce Phase&lt;/h4&gt;
&lt;p&gt;In this phase, data is merged while maintaining their sorted order. The reduce function is this executed for each key in the sorted output. The output is written directly to the &lt;em&gt;output filesystem&lt;/em&gt;, which is commonly HDFS.&lt;/p&gt;
&lt;h3 id=&#34;types-of-schedulers&#34;&gt;Types of Schedulers&lt;/h3&gt;
&lt;p&gt;Early versions of Hadoop used a simple FIFO scheduler. This scheduler would run jobs in the order that they were submitted. This is not ideal since it does not take into account the size of the job or the priority of the job. A job that is submitted after a large job will have to wait until the large job is completed. Considering longer running jobs like machine learning training, a better scheduler is needed.&lt;/p&gt;
&lt;h4 id=&#34;the-fair-scheduler&#34;&gt;The Fair Scheduler&lt;/h4&gt;
&lt;p&gt;The fair scheduler aims to give every user an equal amount of cluster capacity over time. This allows multiple jobs to be running simultaneously. The scheduler does this by placing jobs in &lt;em&gt;pools&lt;/em&gt; assigned to each user. This allows short jobs to run without waiting for long jobs to complete.&lt;/p&gt;
&lt;p&gt;The fair scheduler also supports &lt;em&gt;preemption&lt;/em&gt;. This allows the scheduler to kill tasks that are running for too long to make room for other jobs. This is useful for long running jobs that are not making progress. Note that this does not kill the entire job. The tasks that are killed are rescheduled on other TaskTrackers.&lt;/p&gt;
&lt;h4 id=&#34;the-capacity-scheduler&#34;&gt;The Capacity Scheduler&lt;/h4&gt;
&lt;p&gt;In capacity scheduling, a cluster is divided into multiple queues. Each queue is assigned some amount of cluster capacity. These queues are hierarchical, so a queue can be assigned to a user or a group of users. This allows for more fine grained control over the cluster capacity. In effect, this allows for multiple clusters to be managed by a single cluster.&lt;/p&gt;
&lt;h3 id=&#34;merging-database-functions-and-mapreduce&#34;&gt;Merging Database Functions and MapReduce&lt;/h3&gt;
&lt;p&gt;The power of MapReduce was apparent, but all operations had to be framed in the context of a mapping and reduce functions. This made it difficult or tedious to perform common database operations. Several projects were started to bridge the gap between MapReduce and databases.&lt;/p&gt;
&lt;h4 id=&#34;apache-pig&#34;&gt;Apache Pig&lt;/h4&gt;
&lt;p&gt;Developed by Yahoo, Pig Latin is a high level language that glues together SQL and MapReduce. It was not meant to replace SQL. In fact, the authors note that people in data analysis fine SQL to be unnatural for data analysis. Their work was mean to provide something that meets the declarative style of SQL and procedural style of MapReduce. Consider their opening example.&lt;/p&gt;
&lt;p&gt;Given a table of &lt;code&gt;urls&lt;/code&gt;: &lt;code&gt;(url, category, pagerank)&lt;/code&gt;, find the average pagerank of high-pagerank urls in that category. In SQL, a query might look like this.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; category, AV &lt;span style=&#34;color:#66d9ef&#34;&gt;G&lt;/span&gt;(pagerank)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; urls
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt; pagerank &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;GROUP&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; category
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;HAVING&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;COUNT&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;An equivalent query in Pig Latin would look like this.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-pig&#34; data-lang=&#34;pig&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;good_urls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;FILTER&lt;/span&gt; urls &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; pagerank &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;groups &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;GROUP&lt;/span&gt; good_urls &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; category;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;big_groups &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;FILTER&lt;/span&gt; groups &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; COUNT(good_urls) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;**6;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;output&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;FOREACH&lt;/span&gt; big_groups &lt;span style=&#34;color:#66d9ef&#34;&gt;GENERATE&lt;/span&gt; category, AVG(good_urls.pagerank);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Each statement in Pig Latin describes a data transformation. These statements are converted into an ensemble of MapReduce jobs, but each statement is not necessarily a single MapReduce job. The Pig Latin compiler is responsible for optimizing the statements.&lt;/p&gt;
&lt;h4 id=&#34;apache-hive&#34;&gt;Apache Hive&lt;/h4&gt;
&lt;p&gt;Hive was developed at Facebook to provide an SQL-like interface for processing queries on big data. In addition to providing a high-level language, it also treats Hadoop like a DBMS. This allows users to impose structure on the data and query it using as if it were a traditional database.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-11-26_16-11-09_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Hive Architecture (Elmasri and Navathe).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Hive Architecture (Elmasri and Navathe).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Hive comes with a query-based language called HiveQL that includes joints, aggregations, and other common SQL operations. The tables in Hive are linked directly to directories in HDFS. This allows for data to be loaded into Hive from HDFS and vice versa. Hive also supports partitioning and bucketing to improve performance. Bucketing stores the data physically in the same file, partitioned by the bucketing key.&lt;/p&gt;
&lt;h3 id=&#34;advantages-of-hadoop-mapreduce&#34;&gt;Advantages of Hadoop/MapReduce&lt;/h3&gt;
&lt;p&gt;Consider scanning a 100 TB dataset using a single machine. At a rate of 50 Mbps, this would take around 24 days. Using 1000 machines in parallel would reduce this to about 30 minutes. The resources available can be scaled easily by adding more machines to the cluster. In the event that a machine fails, the tasks can be reassigned to other machines without losing the job completely.&lt;/p&gt;
&lt;h2 id=&#34;hadoop-v2-aka-yarn&#34;&gt;Hadoop v2 AKA YARN&lt;/h2&gt;
&lt;p&gt;Hadoop v1 was well received and solved many of the problems efficiently through its MapReduce programming model. However, many problems arise naturally as the size of the cluster grows.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The JobTracker is a single point of failure. As the cluster size increases, this becomes more of an issue.&lt;/li&gt;
&lt;li&gt;Resources are allocated statically, resulting in a large amount of unused compute when processing jobs.&lt;/li&gt;
&lt;li&gt;Not every job fits cleanly into the MapReduce programming model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition to these issues, YARN aims to improve scalability, resource utilization, and flexibility. By increasing the availability of nodes at any given time, there are less wasted resources on a cluster. This is especially useful in enterprise-level clusters where there are many users and jobs running at any given time.&lt;/p&gt;
&lt;p&gt;Supporting multiple programming models also aids to this scalability. Consider a machine learning model being trained for long periods of time. In Hadoop v1, developers would frame these as MapReduce jobs. A major problem with this is that after the original update, the jobs would exchange data outside of the purview of the JobTracker. This also means that the fault tolerance features built into Hadoop v1 were not available for these jobs.&lt;/p&gt;
&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-11-26_18-08-55_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Hadoop v1 vs. YARN Architecture (Elmasri and Navathe).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Hadoop v1 vs. YARN Architecture (Elmasri and Navathe).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;resourcemanager&#34;&gt;ResourceManager&lt;/h4&gt;
&lt;p&gt;The ResourceManager is the master process in YARN. It is responsible for allocating resources to applications and scheduling tasks. Allocations are based on the chosen scheduler. ApplicationMasters will request resources from the ResourceManager. The ResourceManager will then allocate resources to the ApplicationMaster and notify the NodeManager to start the containers.&lt;/p&gt;
&lt;p&gt;Since the ResourceManager is only responsible for scheduling the available resources, different applications can make use of the same cluster at the same time. This is a major improvement over Hadoop v1 where the JobTracker was responsible for scheduling all jobs.&lt;/p&gt;
&lt;h4 id=&#34;nodemanager&#34;&gt;NodeManager&lt;/h4&gt;
&lt;p&gt;The NodeManager runs on every worker node in the cluster. It launches and monitors containers on the node as well as reports the resource utilization back to the ResourceManager. It additionally provides services to Containers such as security, logging, and local file management.&lt;/p&gt;
&lt;h4 id=&#34;applicationmaster&#34;&gt;ApplicationMaster&lt;/h4&gt;
&lt;p&gt;The ApplicationMaster manages the execution of an application&amp;rsquo;s processes. These applications can range from a traditional MapReduce job to a long-running machine learning model. The ApplicationMaster is responsible for negotiating resources with the ResourceManager and working with the NodeManager to execute and monitor the containers. It sends resource status updates to the ResourceManager as requirements change.&lt;/p&gt;
&lt;h4 id=&#34;container&#34;&gt;Container&lt;/h4&gt;
&lt;p&gt;A container is a collection of resources allocated to an application. These resources are allocated by the ResourceManager and managed by the NodeManager. These resources refer directly to the resources available on the node. This includes CPU, memory, disk, and network.&lt;/p&gt;
&lt;h3 id=&#34;fault-tolerance&#34;&gt;Fault Tolerance&lt;/h3&gt;
&lt;p&gt;The ResourceManager is a single point of failure in YARN. If it fails, it can restart and recover its state. Any containers in the cluster are killed and restarted. The ApplicationMaster is responsible for restarting any tasks that were running in the containers.&lt;/p&gt;
&lt;h3 id=&#34;execution-flow&#34;&gt;Execution Flow&lt;/h3&gt;
&lt;p&gt;The following gives an example of how a typical MapReduce job would be executed in YARN.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The client submits a job to the ResourceManager.&lt;/li&gt;
&lt;li&gt;The ResourceManager allocates a container for the ApplicationMaster and launches the ApplicationMaster on a NodeManager.&lt;/li&gt;
&lt;li&gt;The ApplicationMaster negotiates resources with the ResourceManager.&lt;/li&gt;
&lt;li&gt;The ResourceManager allocates containers for the MapReduce tasks and launches them on the NodeManagers.&lt;/li&gt;
&lt;li&gt;The MapReduce tasks are executed in the containers.&lt;/li&gt;
&lt;li&gt;The ApplicationMaster monitors the MapReduce tasks and reports status updates to the ResourceManager.&lt;/li&gt;
&lt;li&gt;When the job is complete, the ApplicationMaster is unregistered and the containers are released.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Dean, Jeffrey, and Sanjay Ghemawat. 2008. “MapReduce: Simplified Data Processing on Large Clusters.” &lt;i&gt;Communications of the Acm&lt;/i&gt; 51 (1): 107–13. &lt;a href=&#34;https://doi.org/10.1145/1327452.1327492&#34;&gt;https://doi.org/10.1145/1327452.1327492&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Elmasri, Ramez, and Shamkant B. Navathe. 2015. &lt;i&gt;Fundamentals of Database Systems&lt;/i&gt;. 7th ed. Pearson. &lt;a href=&#34;https://www.pearson.com/en-us/subject-catalog/p/fundamentals-of-database-systems/P200000003546/9780137502523&#34;&gt;https://www.pearson.com/en-us/subject-catalog/p/fundamentals-of-database-systems/P200000003546/9780137502523&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Pretraining Large Language Models</title>
      <link>https://ajdillhoff.github.io/notes/pretraining_large_language_models/</link>
      <pubDate>Thu, 16 Nov 2023 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/pretraining_large_language_models/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#unsupervised-pre-training&#34;&gt;Unsupervised Pre-training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#from-gpt-to-gpt2&#34;&gt;From GPT to GPT2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;These notes provide an overview of pre-training large language models like GPT and Llama.&lt;/p&gt;
&lt;h2 id=&#34;unsupervised-pre-training&#34;&gt;Unsupervised Pre-training&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start by reviewing the pre-training procedure detailed in the GPT paper (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Radford et al. 2020&lt;/a&gt;). The &lt;em&gt;Generative&lt;/em&gt; in Generative Pre-Training reveals much about how the network can be trained without direct supervision. It is analogous to how you might have studied definitions as a kid: create some flash cards with the term on the front and the definition on the back. Given the context of the word, you try and recite the definition. For a pre-training language model, it is given a series of tokens and is tasked with generating the next token in the sequence. Since we have access to the original documents, we can easily determine if it was correct.&lt;/p&gt;
&lt;p&gt;Given a sequence of tokens \(\mathcal{X} = \{x_1, x_2, \ldots, x_n\}\), the model is trained to predict the next token \(x_{n+1}\) in the sequence. The model is trained to maximize the log-likelihood of the next token:&lt;/p&gt;
&lt;p&gt;\[\mathcal{L}(\mathcal{X}) = \sum_{i=1}^{n} \log p(x_{i+1} \mid x_{i-k}, \ldots, x_i)\]&lt;/p&gt;
&lt;p&gt;where \(k\) is the size of the context window.&lt;/p&gt;
&lt;p&gt;Large language models are typically based on the &lt;a href=&#34;https://ajdillhoff.github.io/notes/transformers/&#34;&gt;Transformers&lt;/a&gt; model. The original model was trained for language translation. Depending on the task, different variants are employed. For GPT models, a decoder-only architecture is used, as see below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-11-16_15-56-14_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Decoder-only diagram from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_2&amp;#34;&amp;gt;Vaswani et al. 2017&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Decoder-only diagram from (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Vaswani et al. 2017&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The entire input pipeline for GPT can be expressed rather simply. First, the tokenized input is passed through an embedding layer \(W_{e}\). Embedding layers map the tokenized input into a lower-dimensional vector representation. A positional embedding matrix of the same size as \(\mathcal{X} W_{e}\) is added in order to preserve the order of the tokens.&lt;/p&gt;
&lt;p&gt;The embedded data \(h_0\) is then passed through \(n\) transformer blocks. The output of this is passed through the softmax function in order to produce an output distribution over target tokens.&lt;/p&gt;
&lt;h2 id=&#34;from-gpt-to-gpt2&#34;&gt;From GPT to GPT2&lt;/h2&gt;
&lt;p&gt;GPT2 is a larger version of GPT, with an increased context size of 1024 tokens and a vocabulary of 50,257 vocabulary. In this paper, they posit that a system should be able to perform many tasks on the same input. For example, we may want our models to summarize complex texts as well as provide answers to specific questions we have about the content. Instead of training multiple separate models to perform these tasks individually, the model should be able to adapt to these tasks based on the context. In short, it should model \(p(output \mid input, task)\) instead of \(p(output \mid input)\).&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2020. “Improving Language Understanding by Generative Pre-Training,” 12.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need,” 11.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Distributed Databases</title>
      <link>https://ajdillhoff.github.io/notes/distributed_databases/</link>
      <pubDate>Tue, 14 Nov 2023 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/distributed_databases/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-fragmentation&#34;&gt;Data Fragmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-replication&#34;&gt;Data Replication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-concurrency&#34;&gt;Data Concurrency&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Distributed systems excel at partitioning large problems into smaller chunks that can be processed in parallel. This requires parallel thinking instead of serial thinking. Many algorithms and solutions that run serially may be easier to adapt to parallel applications than others.&lt;/p&gt;
&lt;p&gt;Distributed solutions are the natural next step to scaling up a system. In the context of databases, the main challenges related to distribution, replication, distributed transactions, distributed metadata management, and distributed query processing.&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;According to Elmasri and Navathe (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Elmasri and Navathe 2015&lt;/a&gt;), a distributed database should satisfy &lt;em&gt;at least&lt;/em&gt; the following conditions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;database nodes should be connected by a network,&lt;/li&gt;
&lt;li&gt;the information on each node should be logically related,&lt;/li&gt;
&lt;li&gt;and each node does not necessarily need to be identicaly in terms of data, hardware, and software.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;transparency&#34;&gt;Transparency&lt;/h3&gt;
&lt;p&gt;Transparency is the concept of hiding the complex details of a distributed database from the user. There are several types of transparency:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Distribution transparency&lt;/strong&gt;&lt;/strong&gt; - the user does not need to know how the data is distributed across the nodes. This could refer to the location of the data, the replication of the data, or the fragmentation of the data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Replication transparency&lt;/strong&gt;&lt;/strong&gt; - data may be stored in multiple nodes. This type of transparency improves availability by allowing the system to continue operating even if a node goes down.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Fragmentation transparency&lt;/strong&gt;&lt;/strong&gt; - data is either horizontally or vertically fragmented across nodes. Horizontal fragmentation, also called &lt;strong&gt;&lt;strong&gt;sharding&lt;/strong&gt;&lt;/strong&gt;, refers to decomposing tuples of a table into multiple systems. For example, we could horizontally fragment our &lt;code&gt;Character&lt;/code&gt; table based on the &lt;code&gt;class_id&lt;/code&gt;. Vertical fragmentation refers to decomposing the columns of a table into multiple systems. For example, we could vertically fragment our &lt;code&gt;Character&lt;/code&gt; table into a &lt;code&gt;Character&lt;/code&gt; table and a &lt;code&gt;CharacterStats&lt;/code&gt; table.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;availability-and-reliability&#34;&gt;Availability and Reliability&lt;/h3&gt;
&lt;p&gt;Having more than one point of failure means that a distributed database is more &lt;strong&gt;reliable&lt;/strong&gt; than a centralized database. With technologies like replication, the &lt;strong&gt;availability&lt;/strong&gt; of the database also increases.&lt;/p&gt;
&lt;h3 id=&#34;scalability&#34;&gt;Scalability&lt;/h3&gt;
&lt;p&gt;Scalability in a database that is distributed over multiple nodes can be categorized into two types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Horizontal scalability&lt;/strong&gt;&lt;/strong&gt; - adding more nodes to the system.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Vertical scalability&lt;/strong&gt;&lt;/strong&gt; - adding more resources to the nodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A centralized database can only support vertical scalability. If it goes down or is fragmented from a portion of a broader network, the data is no longer accessible. In a distributed system, the nodes can be partitioned into smaller networks that can still operate independently depending on the type of failure. This is called &lt;strong&gt;&lt;strong&gt;partition tolerance&lt;/strong&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;autonomy&#34;&gt;Autonomy&lt;/h3&gt;
&lt;p&gt;Autonomy refers to the ability of a node to operate independently of other nodes. This is important for distributed systems because it allows for the system to continue operating even if a node goes down.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Design autonomy&lt;/strong&gt; - Data model usage and transaction managament are independent of other nodes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Communication autonomy&lt;/strong&gt; - Nodes can communicate with each other without the need for a central coordinator.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Execution autonomy&lt;/strong&gt; - Nodes can execute transactions independently of other nodes. While this type of autonomy leads to more availability and higher performance, it can also create problems with consistency since nodes may not be able to agree on the order of operations.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;data-fragmentation&#34;&gt;Data Fragmentation&lt;/h2&gt;
&lt;p&gt;As mentioned at the beginning of these notes, breaking up a problem into smaller chunks is the key to parallelism. In the context of databases, this means figuring out which nodes have which portions of the data. We will discuss fragmentation under the assumption that no data replication is being used.&lt;/p&gt;
&lt;h3 id=&#34;horizontal-fragmentation--sharding&#34;&gt;Horizontal Fragmentation (Sharding)&lt;/h3&gt;
&lt;p&gt;Imagine a scenario in which we shard our &lt;code&gt;Users&lt;/code&gt; table based on the geographic location of their IP address. If we have 3 nodes in (west coast, central, east coast), then we can separate our table into 3 tables, one for each region. This is called &lt;strong&gt;&lt;strong&gt;horizontal fragmentation&lt;/strong&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;strong&gt;sharding&lt;/strong&gt;&lt;/strong&gt;. The main advantage of sharding is that it allows us to scale horizontally. The main disadvantage is that it makes it more difficult to perform queries that require data from multiple regions.&lt;/p&gt;
&lt;h3 id=&#34;vertical-fragmentation&#34;&gt;Vertical Fragmentation&lt;/h3&gt;
&lt;p&gt;Vertical fragmentation can make sense when we have a table with a large number of columns. For example, we could vertically fragment our &lt;code&gt;Users&lt;/code&gt; table into a &lt;code&gt;Users&lt;/code&gt; table and a &lt;code&gt;UserStats&lt;/code&gt; table. When vertically fragmenting data, there should be a common attribute between the two tables. In this case, the &lt;code&gt;user_id&lt;/code&gt; would be the common attribute.&lt;/p&gt;
&lt;h2 id=&#34;data-replication&#34;&gt;Data Replication&lt;/h2&gt;
&lt;p&gt;Data replication is the process of storing the same data in multiple nodes. There are obvious tradeoffs when it comes to selecting a replication strategy. First, let&amp;rsquo;s consider the extreme cases. If no replication is used, then the system is more consistent since there is only one copy of the data. The availability suffers, however, since there is only a single copy of the data.&lt;/p&gt;
&lt;p&gt;If the data is replicated to every single node, then the availability and performance of the system increases. However, the consistency of the system suffers since there are multiple copies of the data that need to be kept in sync. Picking a replication strategy will largely depend on the needs of the application. Deciding how this data is fragmented is the process of &lt;strong&gt;data distribution&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;The following example is from Elmasri and Navathe (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Elmasri and Navathe 2015&lt;/a&gt;). In this example, a company has three nodes for each of its departments. Node 2 stores data for the &lt;code&gt;Research&lt;/code&gt; department and Node 3 stores data for the &lt;code&gt;Administration&lt;/code&gt; department. The idea behind this is that the &lt;code&gt;EMPLOYEE&lt;/code&gt; and &lt;code&gt;PROJECT&lt;/code&gt; information for each department will be frequently accessed by that department. This would be more efficient than having to access the data from a centralized database. Node 1 is located at the company&amp;rsquo;s headquarters and includes data for all departments.&lt;/p&gt;
&lt;p&gt;The data in the &lt;code&gt;DEPARTMENT&lt;/code&gt; table is horizontally fragmented using the department number &lt;code&gt;Dnumber&lt;/code&gt;. Since there are foreign key relationships in &lt;code&gt;EMPLOYEE&lt;/code&gt;, &lt;code&gt;PROJECT&lt;/code&gt;, and &lt;code&gt;DEPT_LOCATIONS&lt;/code&gt;, they are also fragmented. This is a special type of fragmentation called &lt;strong&gt;derived fragmentation&lt;/strong&gt;. These are easier to fragment since they have a direct foreign key relationship.&lt;/p&gt;
&lt;p&gt;A more difficulty decision comes with the &lt;code&gt;WORKS_ON&lt;/code&gt; table. It does not have an attribute that indicates which department each tuple belongs to. The authors choose to fragment based on the department that the employee works for. This is further fragmented based on the department that controls the projects that the employee is working on.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-11-14_18-41-42_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Fragmentation of `WORKS_ON` table for department 5. &amp;lt;@elmasri_fundamentals_2015&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Fragmentation of &lt;code&gt;WORKS_ON&lt;/code&gt; table for department 5. &amp;lt;@elmasri_fundamentals_2015&amp;gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the figure above, all of the fragments include employees of the research department. The first fragment includes employees that work on projects controlled by the research department. The second fragment includes employees that work on projects controlled by the administration department. The third fragment includes employees that work on projects controlled by headquarters.&lt;/p&gt;
&lt;h2 id=&#34;data-concurrency&#34;&gt;Data Concurrency&lt;/h2&gt;
&lt;p&gt;Distributed systems that employ data replication or allow for multiple users to access the same data at the same time need to be concerned with data concurrency. This is the process of ensuring that the data remains consistent when multiple users are accessing the same data at the same time. Several problems can occur in a DDBMS, such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inconsistency between multiple copies of the data,&lt;/li&gt;
&lt;li&gt;failure of a node,&lt;/li&gt;
&lt;li&gt;network outages that sever the connection between nodes,&lt;/li&gt;
&lt;li&gt;failure of a transaction that is applied to multiple nodes,&lt;/li&gt;
&lt;li&gt;and deadlocks between transactions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;concurrency-control&#34;&gt;Concurrency Control&lt;/h3&gt;
&lt;p&gt;Many control solutions for distributed systems are based on the idea of a centralized &lt;strong&gt;locking&lt;/strong&gt; authority. This authority is responsible for granting locks to transactions that request them. The authority is also responsible for granting access to data that is locked by other transactions. When an object is locked, it cannot be accessed by other transactions.&lt;/p&gt;
&lt;p&gt;In this case, the &lt;em&gt;central authority&lt;/em&gt; may be a &lt;strong&gt;distinguished copy&lt;/strong&gt; of the data. All requests to lock or unlock are sent to that copy.&lt;/p&gt;
&lt;h4 id=&#34;primary-site-technique&#34;&gt;Primary Site Technique&lt;/h4&gt;
&lt;p&gt;All locks are kept at a primary site. This site is responsible for granting locks to transactions that request them. The primary site is also responsible for granting access to data that is locked by other transactions. This is a simple technique that is easy to implement. However, it is not very scalable since all requests must go through the primary site. Note that this does not prevent transactions with read locks from accessing any copy of the item. If a transaction has a write lock, the primary site must update all copies of the data before releasing the lock.&lt;/p&gt;
&lt;h4 id=&#34;primary-site-with-backup&#34;&gt;Primary Site with Backup&lt;/h4&gt;
&lt;p&gt;If the primary site fails in the first approach, the system effectively becomes unavailable. To prevent this, we can have a backup primary site that takes over if the primary site fails. This is a simple solution that is easy to implement. If the primary site fails in this case, a backup takes over and becomes the new primary. A new backup is chosen so that the system can continue to operate. One downside to this approach is that locks must be recorded at both the primary and backup sites.&lt;/p&gt;
&lt;h4 id=&#34;primary-copy-technique&#34;&gt;Primary Copy Technique&lt;/h4&gt;
&lt;p&gt;Lock coordination is distributed among various sites. Distinguished copies for different items are distributed to different sites. A failure at one site would only affect the transactions that are accessing its distinguished copies. Other items not on the site would remain functional. In the case of a failure, the sites that are still running can choose a new coordinator based on some strategy. One such strategy is to have all running sites vote on a new coordinator. The site with the most votes becomes the new coordinator.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Elmasri, Ramez, and Shamkant B. Navathe. 2015. &lt;i&gt;Fundamentals of Database Systems&lt;/i&gt;. 7th ed. Pearson. &lt;a href=&#34;https://www.pearson.com/en-us/subject-catalog/p/fundamentals-of-database-systems/P200000003546/9780137502523&#34;&gt;https://www.pearson.com/en-us/subject-catalog/p/fundamentals-of-database-systems/P200000003546/9780137502523&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>NOSQL</title>
      <link>https://ajdillhoff.github.io/notes/nosql/</link>
      <pubDate>Tue, 14 Nov 2023 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/nosql/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#nosql-characteristics-for-distributed-systems&#34;&gt;NOSQL Characteristics for Distributed Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nosql-data-models&#34;&gt;NOSQL Data Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cap-theorem&#34;&gt;CAP Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#document-based-nosql-systems&#34;&gt;Document-Based NOSQL Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#key-value-nosql-systems&#34;&gt;Key-Value NOSQL Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#column-based-nosql-systems&#34;&gt;Column-Based NOSQL Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#graph-based-nosql-systems&#34;&gt;Graph-Based NOSQL Systems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;&lt;strong&gt;NOSQL&lt;/strong&gt; refers to Not Only SQL. A NOSQL system is commonly a distributed one that focuses on semi-structured data storage, high performance, availability, replication and scalability. These type of systems developed to meet the needs of large-scale internet applications where a traditional SQL database could not.&lt;/p&gt;
&lt;p&gt;Consider an application like Amazon which manages a high volume of data and user requests. The application needs to be able to store and retrieve this data quickly and reliably. They created their own database system called DynamoDB which is a key-value store. DynamoDB has been used for many applications that require high performance and availability such as video streaming through services like Disney+.&lt;/p&gt;
&lt;p&gt;The data that is used in these systems does not usually fit the mold of a traditional SQL database. For example, a relational database might store an object by disassembling it into its components and storing each component in a separate table. This is not ideal for a system that needs to store and retrieve data quickly. A NOSQL system will store the object as a whole and retrieve it as a whole.&lt;/p&gt;
&lt;h2 id=&#34;nosql-characteristics-for-distributed-systems&#34;&gt;NOSQL Characteristics for Distributed Systems&lt;/h2&gt;
&lt;p&gt;Given the nature of the applications that utilize NOSQL systems, the most important characteristic is high availability. Of course, performance is also important given the number of users that expect the service to remain responsive at all times.&lt;/p&gt;
&lt;h3 id=&#34;scalability&#34;&gt;Scalability&lt;/h3&gt;
&lt;p&gt;NOSQL systems typically aim for horizontal scalability. The applications that use these systems are expected to grow rapidly and the system needs to be able to handle the increased load. This sort of dynamic scaling means that implementations should not rely on a fixed number of nodes.&lt;/p&gt;
&lt;p&gt;For example, during the holiday season, Amazon will need to rapidly scale up their infrastructure to handle the increased load. Cloud technologies are capable of doing this automatically, but the database system needs to be able to handle the increased load as well.&lt;/p&gt;
&lt;h3 id=&#34;availability&#34;&gt;Availability&lt;/h3&gt;
&lt;p&gt;NOSQL systems are expected to be highly available. This means that the system should be able to handle failures and continue to operate. Data is typically replicated over multiple nodes. However, this replication comes with increased complexity for writing data. To deal with this, many NOSQL systems implement a relaxed version called &lt;strong&gt;eventual consistency&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;replication-models&#34;&gt;Replication Models&lt;/h3&gt;
&lt;p&gt;There are two main replication models for NOSQL systems: &lt;strong&gt;primary-replica&lt;/strong&gt; and &lt;strong&gt;primary-primary&lt;/strong&gt;. In primary-replica replication, only one copy is the primary for which all write operations are applied. The write is propagated asynchronously to the replicas.&lt;/p&gt;
&lt;p&gt;In primary-primary replication, all copies are equal and can accept write operations. This is more complex to implement, but it allows for better performance and availability. If multiple users write to the same object, the system needs to be able to handle the conflict through a reconciliation process.&lt;/p&gt;
&lt;h3 id=&#34;sharding&#34;&gt;Sharding&lt;/h3&gt;
&lt;p&gt;Depending on the application, a NOSQL collection could have millions of documents. These may need to be accessed simultaneously by a large number fo users. &lt;strong&gt;Sharding&lt;/strong&gt; is a technique that allows the data to be distributed across multiple nodes. In this way, multiple nodes can work in parallel to handle the load. This has an added benefit of ensuring that no single node is overloaded.&lt;/p&gt;
&lt;h3 id=&#34;high-performance-data-access&#34;&gt;High-Performance Data Access&lt;/h3&gt;
&lt;p&gt;In a distributed system with millions upon millions of objects distributed across many nodes, how do you find the object you are looking for? NOSQL systems typically use a &lt;strong&gt;hash-based&lt;/strong&gt; approach to find the object. This is done by hashing the key of the object and using the hash to determine which node the object is stored on. This is a very fast operation and allows for the system to scale to millions of objects.&lt;/p&gt;
&lt;p&gt;Another solution is called &lt;strong&gt;range partitioning&lt;/strong&gt; in which the location is determined based on a range of key values. Each node would handle a different partition of the keys.&lt;/p&gt;
&lt;h3 id=&#34;other-characteristics&#34;&gt;Other Characteristics&lt;/h3&gt;
&lt;p&gt;NOSQL systems do not require a schema. This means that the data does not need to be structured in a specific way. This is useful for applications that need to store a variety of data types. For example, a social media application might need to store user profiles, posts, comments, etc. These are all different types of data that would not fit well into a relational database. Instead of a schema, a language for describing the data is used. A common language is JSON.&lt;/p&gt;
&lt;p&gt;Given the common application of NOSQL systems, a complex query language is not required. Many of the requests are in the form of a simple read or write operation. This allows for the system to be optimized for these operations. These operations are typically provided by an API and are called &lt;strong&gt;CRUD operations&lt;/strong&gt; (Create, Read, Update, and Delete). Without the full power of SQL, complex operations such as &lt;code&gt;JOIN&lt;/code&gt; or &lt;code&gt;CONSTRAINTS&lt;/code&gt; must be handled by the application.&lt;/p&gt;
&lt;h2 id=&#34;nosql-data-models&#34;&gt;NOSQL Data Models&lt;/h2&gt;
&lt;p&gt;There are four main data models used by NOSQL systems: &lt;strong&gt;key-value&lt;/strong&gt;, &lt;strong&gt;column&lt;/strong&gt;, &lt;strong&gt;document&lt;/strong&gt;, and &lt;strong&gt;graph&lt;/strong&gt;. Each of these models has its own advantages and disadvantages. The model that is chosen depends on the application and the type of data that is being stored.&lt;/p&gt;
&lt;h3 id=&#34;key-value&#34;&gt;Key-Value&lt;/h3&gt;
&lt;p&gt;The key-value model is the simplest of the four. It is essentially a hash table where the key is used to retrieve the value. The value can be any type of data. This model is very fast and can scale to millions of objects.&lt;/p&gt;
&lt;h3 id=&#34;column&#34;&gt;Column&lt;/h3&gt;
&lt;p&gt;Tables are partitioned by columns into column families. Each column family is stored in its own files.&lt;/p&gt;
&lt;h3 id=&#34;document&#34;&gt;Document&lt;/h3&gt;
&lt;p&gt;Documents are stored in collections. Each document is stored as a JSON object. This model is very flexible and can store a variety of data types. It is also very fast and can scale to millions of objects. The documents are typically queried using their document ID, but other indices can be created to speed up queries.&lt;/p&gt;
&lt;h3 id=&#34;graph&#34;&gt;Graph&lt;/h3&gt;
&lt;p&gt;Graphs are used to represent relationships between objects. Each object is represented as a node and the relationships are represented as edges. This model is useful for applications that need to represent complex relationships between objects.&lt;/p&gt;
&lt;h2 id=&#34;cap-theorem&#34;&gt;CAP Theorem&lt;/h2&gt;
&lt;p&gt;The CAP theorem states that a distributed system can only guarantee two of the following three properties: &lt;strong&gt;consistency&lt;/strong&gt;, &lt;strong&gt;availability&lt;/strong&gt;, and &lt;strong&gt;partition tolerance&lt;/strong&gt;. Consistency means that all nodes see the same data at the same time. Availability means that every request receives a response. Partition tolerance means that the system continues to operate despite network failures.&lt;/p&gt;
&lt;h2 id=&#34;document-based-nosql-systems&#34;&gt;Document-Based NOSQL Systems&lt;/h2&gt;
&lt;p&gt;In document-based NOSQL systems, the data is &lt;strong&gt;self-describing&lt;/strong&gt; as there is no need for a schema. These sytems store &lt;strong&gt;documents&lt;/strong&gt; which are essentially JSON objects. The documents are stored in &lt;strong&gt;collections&lt;/strong&gt; which are similar to tables in a relational database. The documents are retrieved using their document ID.&lt;/p&gt;
&lt;h3 id=&#34;mongodb&#34;&gt;MongoDB&lt;/h3&gt;
&lt;p&gt;MongoDB is a document-based NOSQL database that is flexibile, scalable, and high-performance. It stores data in a JSON-like format called BSON (Binary JSON). Inidividual &lt;strong&gt;documents&lt;/strong&gt; are stored in a &lt;strong&gt;collection&lt;/strong&gt;. No schema is needed to begin storing data. The python code below will create a new collection for our RPG &lt;code&gt;Users&lt;/code&gt; with a simple command in &lt;code&gt;pymongo&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;db[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;users&amp;#39;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will create a new collection named &lt;code&gt;users&lt;/code&gt; with the default settings. If you want to specify additional options, call the &lt;code&gt;create_collection&lt;/code&gt; function. Common parameters include determining of a collection is capped by the storage size and maximum number of documents.&lt;/p&gt;
&lt;h4 id=&#34;representing-data&#34;&gt;Representing Data&lt;/h4&gt;
&lt;p&gt;Whenever a new item is inserted to a colletion, a unique &lt;code&gt;ObjectId&lt;/code&gt; is created and indexed. If the ID of a document should match a user-defined protocol, it can be set manually. Since there is no schema to specify a relationship, document relationships can be created by including the ~ObjectId~s of objects you wish to reference in your data.&lt;/p&gt;
&lt;p&gt;There are multiple ways to represent relationships between documents. Consider a &lt;code&gt;Character&lt;/code&gt; that holds multiple items in an &lt;code&gt;Inventory&lt;/code&gt;. The items could be referenced as an array of &lt;code&gt;Item&lt;/code&gt; objects within the &lt;code&gt;Character&lt;/code&gt; object itself. Alternatively, the &lt;code&gt;Character&lt;/code&gt; could hold an array of &lt;code&gt;ObjectId~s that reference the ~Item&lt;/code&gt; objects in the &lt;code&gt;Inventory&lt;/code&gt; collection. A third approach would have each &lt;code&gt;Item&lt;/code&gt; reference the &lt;code&gt;Character&lt;/code&gt; that owns it. The best approach depends on the application and the type of queries that will be performed.&lt;/p&gt;
&lt;h4 id=&#34;crud-operations&#34;&gt;CRUD Operations&lt;/h4&gt;
&lt;p&gt;CRUD stands for Create, Read, Update, and Delete. Single or multiple documents can be implemented with the &lt;code&gt;insert&lt;/code&gt; function. In &lt;code&gt;pymongo&lt;/code&gt;, you can use either &lt;code&gt;Collections.insert_one&lt;/code&gt; or &lt;code&gt;Collections.insert_many&lt;/code&gt;. The &lt;code&gt;insert_one&lt;/code&gt; function takes a single document as an argument and returns the &lt;code&gt;ObjectId&lt;/code&gt; of the inserted document. The &lt;code&gt;insert_many&lt;/code&gt; function takes a list of documents as an argument and returns a list of ~ObjectId~s.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;db[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;users&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;insert_one({&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Naomi&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;age&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;25&lt;/span&gt;})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;db[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;users&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;insert_many([{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Naomi&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;age&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;25&lt;/span&gt;}, {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;James&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;age&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;}])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Reading objects is done with the &lt;code&gt;find&lt;/code&gt; function. There are several variants of this available in &lt;code&gt;pymongo&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;find_one&lt;/code&gt; returns a single document that matches the query.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;find&lt;/code&gt; returns a cursor that can be iterated over to retrieve all documents that match the query.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;find_one_and_delete&lt;/code&gt; returns a single document that matches the query and deletes it.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;find_one_and_replace&lt;/code&gt; returns a single document that matches the query and replaces it with the specified document.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;find_one_and_update&lt;/code&gt; returns a single document that matches the query and updates it with the specified document.&lt;/li&gt;
&lt;/ul&gt;
&lt;!--listend--&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;val &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; db[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;users&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;find_one({&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Naomi&amp;#39;&lt;/span&gt;})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print the document&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(val)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print the name&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(val[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Updating documents is done with the &lt;code&gt;update&lt;/code&gt; function. We saw an updated combined with &lt;code&gt;find&lt;/code&gt; above, but &lt;code&gt;pymongo&lt;/code&gt; also implements &lt;code&gt;update_one&lt;/code&gt; and &lt;code&gt;update_many&lt;/code&gt;. The &lt;code&gt;update_one&lt;/code&gt; function takes a query and an update document as arguments. The &lt;code&gt;update_many&lt;/code&gt; function takes a query and an update document as arguments. Both functions return a &lt;code&gt;UpdateResult&lt;/code&gt; object that contains information about the operation.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;db[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;users&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;update_one({&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Naomi&amp;#39;&lt;/span&gt;}, {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;$set&amp;#39;&lt;/span&gt;: {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;age&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;26&lt;/span&gt;}})
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Deleting documents is done with the &lt;code&gt;delete_one&lt;/code&gt; and &lt;code&gt;delete_many&lt;/code&gt; functions. Both functions take a query as an argument and return a &lt;code&gt;DeleteResult&lt;/code&gt; object that contains information about the operation.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;db[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;users&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;delete_one({&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Naomi&amp;#39;&lt;/span&gt;})
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;characteristics&#34;&gt;Characteristics&lt;/h4&gt;
&lt;p&gt;MongoDB uses a &lt;strong&gt;two-phase commit&lt;/strong&gt; method to ensure transaction atomicity and consistency. In the first phase of the process, a coordinator sends a message to all nodes to prepare for the transaction. Each node then responds with an acknowledgement. If all nodes respond with an acknowledgement, the coordinator sends a commit message to all nodes. If any node fails to respond with an acknowledgement, the coordinator sends a message to roll back the transaction.&lt;/p&gt;
&lt;p&gt;For data replication, a variation on the &lt;strong&gt;primary-replica&lt;/strong&gt; model is used. A primary node is chosen with at least one replica. More nodes can be added at the cost of increased time for writes. The total number of nodes for a replica set is at least 3, so if only a primary and one replica are used, an &lt;strong&gt;arbiter&lt;/strong&gt; must be chosen to break ties. In fact, any replica set with an even number of nodes must have an arbiter.&lt;/p&gt;
&lt;p&gt;All write operations mus be performed on the primary copy before being propagated to the replicas. Users can determine the &lt;strong&gt;read preference&lt;/strong&gt; for their application. The default is to read from the primary copy, but users can choose to read from the nearest copy or a specific copy. If a copy other than the primary is chosen for the read preference, it is not guaranteed that the user will get the lastest version of the data.&lt;/p&gt;
&lt;h4 id=&#34;sharding&#34;&gt;Sharding&lt;/h4&gt;
&lt;p&gt;We previously discussed that having all of the data in a single collection can lead to performance issues. Sharding is a technique that allows the data to be distributed across multiple nodes. This allows for multiple nodes to work in parallel to handle the load. Sharding splits the data into disjoint partitions which can then be stored on different nodes.&lt;/p&gt;
&lt;p&gt;The partitions can be determined via &lt;strong&gt;hash partitioning&lt;/strong&gt; or &lt;strong&gt;range partitioning&lt;/strong&gt;. In either case, a document field must be chosen to determine the partition. This partition field is called the &lt;strong&gt;shard key&lt;/strong&gt;. It must exist in every document and be indexed.&lt;/p&gt;
&lt;p&gt;When using sharding on MongoDB, a &lt;strong&gt;query router&lt;/strong&gt; keeps tracks of which nodes contain which shards. The actual query is then routed to the node containing the shard. In the event that a query is sent to a node that does not contain the shard, the query router will forward the query to all nodes.&lt;/p&gt;
&lt;h2 id=&#34;key-value-nosql-systems&#34;&gt;Key-Value NOSQL Systems&lt;/h2&gt;
&lt;p&gt;Key-value systems use a simple data model and typically do not have a query language. The data is stored as a key-value pair. The key is used to retrieve the value. The value can be any type of data. This model is very fast and can scale to millions of objects. Popular key-value stores include DynamoDB, Voldemort, Redis, and Cassandra. We will briefly discuss each of them below.&lt;/p&gt;
&lt;h3 id=&#34;dynamodb&#34;&gt;DynamoDB&lt;/h3&gt;
&lt;p&gt;DynamoDB was developed by Amazon to meet the needs of their large-scale internet applications. It is a key-value store that is highly available and scalable. It is also a managed service which means that Amazon handles the scaling and replication for you. It uses tables, items, and attributes without the need for a schema. The table itself holds multiple items which are self-describing. That is, the items have &lt;code&gt;(attribute, value)&lt;/code&gt; pairs.&lt;/p&gt;
&lt;p&gt;Tables must have &lt;strong&gt;primary keys&lt;/strong&gt; which can be either a single attribute or pair of attributes. For single attributes, DynamoDB will build a hash index on this attribute. For pairs of attribute, a &lt;strong&gt;hash and range&lt;/strong&gt; primary key is used. The primary key is the pair of attributes and the hash index is built on the first attribute. This allows for fast retrieval of items based on the first attribute. The second attribute can be used to sort the items for which the first attribute is the same.&lt;/p&gt;
&lt;h3 id=&#34;voldemort&#34;&gt;Voldemort&lt;/h3&gt;
&lt;p&gt;Voldemort is a distributed key-value store based on DynamoDB and developed by LinkedIn and Microsoft. The distribution of data is handled via &lt;strong&gt;consistent hashing&lt;/strong&gt;. Since Voldemort is based on DynamoDB, many of the characteristics described below also apply to DynamoDB.&lt;/p&gt;
&lt;h4 id=&#34;operations&#34;&gt;Operations&lt;/h4&gt;
&lt;p&gt;Like DynamoDB, key-value pairs are the primary data structure. These are kept in a data &lt;code&gt;store&lt;/code&gt;. Three basic operations are implemented: &lt;code&gt;get&lt;/code&gt;, &lt;code&gt;put&lt;/code&gt;, and &lt;code&gt;delete&lt;/code&gt;. Data is stored as a byte array.&lt;/p&gt;
&lt;h4 id=&#34;formatted-data&#34;&gt;Formatted Data&lt;/h4&gt;
&lt;p&gt;Voldemort supports multiple formats for the data. The default format is a byte array, but other formats such as JSON and Protocol Buffers are supported. It provides default serializers for these formats, but users can also implement their own. As long as a &lt;code&gt;Serializer&lt;/code&gt; class is implemented, it can be used to serialize and deserialize data.&lt;/p&gt;
&lt;h4 id=&#34;consistent-hashing&#34;&gt;Consistent Hashing&lt;/h4&gt;
&lt;p&gt;Voldemort distributes data based on a hash function that is applied to each key. The range of values on which the key is mapped corresponds to a node. The figure below shows an example of 7 regions being mapped to 3 nodes (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Elmasri and Navathe 2015&lt;/a&gt;).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-11-20_10-49-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Consistent hashing in Voldemort.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Consistent hashing in Voldemort.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Consistent hashing naturally permits data replication and horizontal scaling. As new nodes are added, only a small subset of the data needs to be rehashed to the new node. Replicas are created by mapping the key to multiple nodes.&lt;/p&gt;
&lt;h4 id=&#34;consistency&#34;&gt;Consistency&lt;/h4&gt;
&lt;p&gt;Concurrent writes are allowed which means there can exist multiple versions of the same key at different nodes. Consistency occurs when an item is read. If the system can reconcile the different versions of the key to a single value, it will pass that final value on. Otherwise, multiple versions may be sent to the application to be resolved.&lt;/p&gt;
&lt;h3 id=&#34;redis&#34;&gt;Redis&lt;/h3&gt;
&lt;p&gt;Redis is an in-memory key-value store. This implies that is basic operations perform very quickly. However, it is not well suited for general purpose applications that require high volumes of data. A typical use-case for Redis would be caching, session management, or real-time analytics.&lt;/p&gt;
&lt;p&gt;For example, Twitter uses Redis to drive their timeline feature. The posts are indexed using an ID and stored in Redis. When a user requests their timeline, the IDs are retrieved from Redis as a chain of IDs.&lt;/p&gt;
&lt;h3 id=&#34;cassandra&#34;&gt;Cassandra&lt;/h3&gt;
&lt;p&gt;Cassandra can be used as a wide-column database (discussed below) or key-value database. It was originally developed at Facebook to handle large amounts of data across multiple commodity servers. It implements the Cassandra Query Language (CQL) which is similar to SQL. The data it partitioned similarly to other NOSQL datastores in that data is distributed in partitions across multiple nodes. CQL does not support cross-partition queries.&lt;/p&gt;
&lt;h2 id=&#34;column-based-nosql-systems&#34;&gt;Column-Based NOSQL Systems&lt;/h2&gt;
&lt;p&gt;The largest differentiator of a column-based system and key-value system is the way the key is defined. A popular implementation of this type of system is known as &lt;strong&gt;BigTable&lt;/strong&gt; which was developed by Google. It uses the &lt;strong&gt;Google File System (GFS)&lt;/strong&gt; to store data. There is an open source equivalent named &lt;strong&gt;Apache Hbase&lt;/strong&gt; which we will focus on below.&lt;/p&gt;
&lt;p&gt;Hbase organizes data using &lt;em&gt;namespaces, tables, column families, column qualifiers, columns, rows&lt;/em&gt;, and &lt;em&gt;data cells&lt;/em&gt;. A column is identified by a family and qualifier. It can store multiple versions of the same data, differentiating each version using a timestamp. Each data cell is identified by a unique key. Tables are associated with column families. When loading data, the column qualifiers must be specified.&lt;/p&gt;
&lt;p&gt;New column qualifiers can be created as needed, producing new rows of data. However, application developers must keep track of which qualifiers belnog to which family. This is a form of vertical partitioning. Since the columns belong to the same column family, they are stored in the same file.&lt;/p&gt;
&lt;p&gt;Cells are reference by their key which is a combination of the row key, column family, column qualifier, and timestamp. For relational semantics, namespaces are used to define a collection of tables.&lt;/p&gt;
&lt;p&gt;Hbase divides tables into &lt;strong&gt;regions&lt;/strong&gt; which hold a range of row keys into the table. It is for this reason that they keys must be sortable lexicographically. Each region has a number of &lt;strong&gt;stores&lt;/strong&gt; for which a column family is assigned. These regions of data are assigned to nodes in the cluster. To manage splitting and merging of regions, a &lt;strong&gt;primary server&lt;/strong&gt; is used.&lt;/p&gt;
&lt;h2 id=&#34;graph-based-nosql-systems&#34;&gt;Graph-Based NOSQL Systems&lt;/h2&gt;
&lt;p&gt;The last category of NOSQL databases discussed in these notes are Graph Databases. These databases are used to represent relationships between objects. Each object is represented as a node and the relationships are represented as edges. This model is useful for applications that need to represent complex relationships between objects. A popular implementation of this type of system is known as &lt;strong&gt;Neo4j&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Nodes and relationships can have a unique collection of properties to describe them. Nodes are labeled, and nodes with the same label are grouped into collections for querying. Relationship types are useful for grouping relationships based on a common property.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Paths&lt;/strong&gt; specify a traversal of a subgraph. They are used to specify a query and consist of nodes and relationships. The subgraph is used as a pattern to find other subgraphs that match the pattern. The query can be further refined by specifying constraints on the nodes and relationships.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Elmasri, Ramez, and Shamkant B. Navathe. 2015. &lt;i&gt;Fundamentals of Database Systems&lt;/i&gt;. 7th ed. Pearson. &lt;a href=&#34;https://www.pearson.com/en-us/subject-catalog/p/fundamentals-of-database-systems/P200000003546/9780137502523&#34;&gt;https://www.pearson.com/en-us/subject-catalog/p/fundamentals-of-database-systems/P200000003546/9780137502523&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Policy Gradient Methods</title>
      <link>https://ajdillhoff.github.io/notes/policy_gradient_methods/</link>
      <pubDate>Sun, 12 Nov 2023 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/policy_gradient_methods/</guid>
      <description>&lt;p&gt;When we had full knowledge of the states, we could use &lt;a href=&#34;https://ajdillhoff.github.io/notes/markov_decision_processes/&#34;&gt;Markov Decision Processes&lt;/a&gt; to find the optimal policy. When this assumption breaks down, we need to come up with our best approximation. This is not a far stretch from how we might handle new scenarios in our own lives. When we begin a new task, we are certainly not experts. We may learn from a teacher or set off to explore on our own. As we practice and churn out the seemingly endless variations of our endeavour, we begin to develop a sense of what works and what doesn&amp;rsquo;t. We may not be able to articulate the exact rules that we follow, but we can certainly tell when we are doing well or poorly.&lt;/p&gt;
&lt;p&gt;In lieu of a conscious agent with human intelligence, we can approximate the policy using a gradient-based approach. We will use the gradient of the expected reward with respect to the policy parameters to update the policy. Methods that use this approach are called &lt;strong&gt;&lt;strong&gt;policy gradient methods&lt;/strong&gt;&lt;/strong&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Structured Query Language</title>
      <link>https://ajdillhoff.github.io/notes/structured_query_language/</link>
      <pubDate>Mon, 30 Oct 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/structured_query_language/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#history-and-development&#34;&gt;History and Development&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#schemas&#34;&gt;Schemas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-types&#34;&gt;Data Types&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#creation&#34;&gt;Creation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#constraints&#34;&gt;Constraints&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#retrieving-data&#34;&gt;Retrieving Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modifying-data&#34;&gt;Modifying Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nested-queries&#34;&gt;Nested Queries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#joined-tables&#34;&gt;Joined Tables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aggregate-functions&#34;&gt;Aggregate Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#grouping&#34;&gt;Grouping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#with-clause&#34;&gt;&lt;code&gt;WITH&lt;/code&gt; Clause&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modifying-tables&#34;&gt;Modifying Tables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;history-and-development&#34;&gt;History and Development&lt;/h2&gt;
&lt;p&gt;Structured Query Language (SQL) is a database language for managing data in a relation DBMS. Its original inception was based on a paper by Edgar F. Codd in 1970 titled &lt;em&gt;A Relational Model of Data for Large Shared Data Banks&lt;/em&gt; (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Codd 1970&lt;/a&gt;). Two employees working at IBM in the 1970s, Donald D. Chamberlin and Raymond F. Boyce, developed the first version of SQL in 1974 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Chamberlin and Boyce 1974&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The first official standard of SQL was SQL-86, or SQL1, which was published in 1986 by the American National Standards Institute (ANSI). The following table shows the release dates of major SQL standards along with a brief description of the changes made in each version.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Standard&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SQL-86&lt;/td&gt;
&lt;td&gt;SQL1&lt;/td&gt;
&lt;td&gt;First official standard of SQL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SQL-89&lt;/td&gt;
&lt;td&gt;SQL2&lt;/td&gt;
&lt;td&gt;Added support for integrity constraints, views, and assertions&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SQL-92&lt;/td&gt;
&lt;td&gt;SQL2&lt;/td&gt;
&lt;td&gt;Added support for triggers, recursive queries, and support for procedural programming&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SQL:1999&lt;/td&gt;
&lt;td&gt;SQL3&lt;/td&gt;
&lt;td&gt;Added support for object-relational features&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SQL:2003&lt;/td&gt;
&lt;td&gt;SQL3&lt;/td&gt;
&lt;td&gt;Added support for XML, window functions, and support for regular expressions&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SQL:2006&lt;/td&gt;
&lt;td&gt;SQL3&lt;/td&gt;
&lt;td&gt;Added more XML storage features and XQuery support&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SQL:2008&lt;/td&gt;
&lt;td&gt;SQL3&lt;/td&gt;
&lt;td&gt;Added support for TRUNCATE TABLE and enhanced MERGE statements&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SQL:2011&lt;/td&gt;
&lt;td&gt;SQL3&lt;/td&gt;
&lt;td&gt;Added support for temporal data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SQL:2016&lt;/td&gt;
&lt;td&gt;SQL3&lt;/td&gt;
&lt;td&gt;Added support for JSON&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SQL:2023&lt;/td&gt;
&lt;td&gt;SQL3&lt;/td&gt;
&lt;td&gt;Added support for Propery Graph Queries and new JSON features&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;schemas&#34;&gt;Schemas&lt;/h2&gt;
&lt;p&gt;In our &lt;a href=&#34;https://ajdillhoff.github.io/notes/introduction_to_databases/&#34;&gt;Introduction to Databases&lt;/a&gt; we discussed the concept of a schema as a definition of the structure of a database. In SQL, a schema is a collection of database objects, such as tables, views, and indexes. A schema is owned by a database user and has the same name as the user. A database user can own multiple schemas, and a schema can be owned by multiple users. A schema can also be owned by a role, which is a collection of users. A role can own multiple schemas, and a schema can be owned by multiple roles.&lt;/p&gt;
&lt;p&gt;There are several practical reasons for which we would want to create multiple schemas. For example, a database might be used by both a Human Resources and Healthcare Management application. Creating two separate schemas would ensure that data for each application is kept secure from unauthorized users. Multiple schemas are also used for testing and development processes. Large structural changes to an application may require a new scheme to be created. New features can be developed in the new schema while the old schema is still being used by the application.&lt;/p&gt;
&lt;p&gt;The following command creates a new schema named &lt;code&gt;MedApp&lt;/code&gt; and assigns it to the user &lt;code&gt;MedAdmin&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;SCHEMA&lt;/span&gt; MedApp &lt;span style=&#34;color:#66d9ef&#34;&gt;AUTHORIZATION&lt;/span&gt; MedAdmin;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;data-types&#34;&gt;Data Types&lt;/h2&gt;
&lt;p&gt;SQL supports a wide variety of data types. The following table shows the most common data types supported by SQL.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Data Type&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CHAR(n)&lt;/td&gt;
&lt;td&gt;Fixed-length character string. The maximum length is &lt;code&gt;n&lt;/code&gt; characters.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VARCHAR(n)&lt;/td&gt;
&lt;td&gt;Variable-length character string. The maximum length is &lt;code&gt;n&lt;/code&gt; characters.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;INT&lt;/td&gt;
&lt;td&gt;Integer value. The maximum value is &lt;code&gt;2^31 - 1&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SMALLINT&lt;/td&gt;
&lt;td&gt;Integer value. The maximum value is &lt;code&gt;2^15 - 1&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DECIMAL(i,j)&lt;/td&gt;
&lt;td&gt;Fixed-point number. The maximum precision is &lt;code&gt;38&lt;/code&gt; digits. The maximum scale is &lt;code&gt;38&lt;/code&gt; digits.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NUMERIC(i,j)&lt;/td&gt;
&lt;td&gt;Fixed-point number. The maximum precision is &lt;code&gt;38&lt;/code&gt; digits. The maximum scale is &lt;code&gt;38&lt;/code&gt; digits.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;REAL&lt;/td&gt;
&lt;td&gt;Floating-point number. The maximum precision is &lt;code&gt;6&lt;/code&gt; digits.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DOUBLE&lt;/td&gt;
&lt;td&gt;Floating-point number. The maximum precision is &lt;code&gt;15&lt;/code&gt; digits.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DATE&lt;/td&gt;
&lt;td&gt;Date value. The range is &lt;code&gt;1000-01-01&lt;/code&gt; to &lt;code&gt;9999-12-31&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TIME&lt;/td&gt;
&lt;td&gt;Time value. The range is &lt;code&gt;00:00:00&lt;/code&gt; to &lt;code&gt;23:59:59&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TIMESTAMP&lt;/td&gt;
&lt;td&gt;Date and time value. The range is &lt;code&gt;1000-01-01 00:00:00&lt;/code&gt; to &lt;code&gt;9999-12-31 23:59:59&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CLOB(n)&lt;/td&gt;
&lt;td&gt;Specifies columns with large text values. Maximum length specified in kilobytes (K), megabytes (M), or gigabytes (G)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BIT(n)&lt;/td&gt;
&lt;td&gt;Fixed-length bit string.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BIT VARYING(n)&lt;/td&gt;
&lt;td&gt;Variable-length bit string.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BLOB(n)&lt;/td&gt;
&lt;td&gt;Binary Large Object - used for images, video, and other large items.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;creation&#34;&gt;Creation&lt;/h2&gt;
&lt;p&gt;Creating schemas, databases, and tables is done with the &lt;code&gt;CREATE&lt;/code&gt; command. The following command creates a new database named &lt;code&gt;RPG&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;DATABASE&lt;/span&gt; RPG;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;When creating a new table, we must specify the name of the table and the attributes of the table. The following command creates a new table named &lt;code&gt;Users&lt;/code&gt; with four attributes.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; Users (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    user_id INT,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    username VARCHAR(&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    email VARCHAR(&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    created_at &lt;span style=&#34;color:#66d9ef&#34;&gt;TIMESTAMP&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;constraints&#34;&gt;Constraints&lt;/h2&gt;
&lt;p&gt;Constraints allow us to add rules to our database that ensure the integrity of our data. There are several types of constraints that can be added to a table. For example, if a user is deleted, we may want to delete all of the user&amp;rsquo;s posts as well. This can be accomplished by adding a &lt;code&gt;CASCADE&lt;/code&gt; constraint to the &lt;code&gt;DELETE&lt;/code&gt; statement. We can also set a default value to each attribute. Constraints such as &lt;code&gt;CHECK&lt;/code&gt; and &lt;code&gt;UNIQUE&lt;/code&gt; can be added to ensure that the data is valid and unique. The following table shows the most common constraints supported by SQL.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Constraint&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;NOT NULL&lt;/td&gt;
&lt;td&gt;Ensures that a column cannot have a NULL value.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UNIQUE&lt;/td&gt;
&lt;td&gt;Ensures that all values in a column are unique.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PRIMARY KEY&lt;/td&gt;
&lt;td&gt;A combination of a NOT NULL and UNIQUE.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FOREIGN KEY&lt;/td&gt;
&lt;td&gt;Ensures that values in a column match values in another table&amp;rsquo;s column.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CHECK&lt;/td&gt;
&lt;td&gt;Ensures that all values in a column satisfy a specific condition.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DEFAULT&lt;/td&gt;
&lt;td&gt;Sets a default value for a column when no value is specified.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;INDEX&lt;/td&gt;
&lt;td&gt;Used to create and retrieve data from the database very quickly.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AUTO INCREMENT&lt;/td&gt;
&lt;td&gt;Automatically generates a unique number when a new record is inserted into a table.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;When creating the &lt;code&gt;Users&lt;/code&gt; table above, we may want to ensure that the &lt;code&gt;user_id&lt;/code&gt; attribute is unique. We can do this by adding a &lt;code&gt;UNIQUE&lt;/code&gt; constraint to the &lt;code&gt;user_id&lt;/code&gt; attribute. It is also possible to have it auto increment so that we do not have to specify a value for it when inserting a new user.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; Users (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    user_id INT &lt;span style=&#34;color:#66d9ef&#34;&gt;UNIQUE&lt;/span&gt; AUTO_INCREMENT,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    username VARCHAR(&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    email VARCHAR(&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    created_at &lt;span style=&#34;color:#66d9ef&#34;&gt;TIMESTAMP&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The following command creates a new table named &lt;code&gt;Characters&lt;/code&gt; with a &lt;code&gt;VARCHAR&lt;/code&gt; attribute named &lt;code&gt;Name&lt;/code&gt; which is set to &lt;code&gt;NOT NULL&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; Characters (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Name VARCHAR(&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;NOT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;NULL&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Constraints can also be added after the initial attribute declaration. When creating the &lt;code&gt;Characters&lt;/code&gt; table, if we want to state that the &lt;code&gt;user_id&lt;/code&gt; field should be a foreign key, we can add a &lt;code&gt;FOREIGN KEY&lt;/code&gt; constraint to the &lt;code&gt;user_id&lt;/code&gt; attribute.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; Characters (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    id INT &lt;span style=&#34;color:#66d9ef&#34;&gt;UNIQUE&lt;/span&gt; AUTO_INCREMENT,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Name VARCHAR(&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;NOT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;NULL&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    user_id INT,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;CONSTRAINT&lt;/span&gt; fk_user_id
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;     &lt;span style=&#34;color:#66d9ef&#34;&gt;FOREIGN&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;KEY&lt;/span&gt; (user_id) &lt;span style=&#34;color:#66d9ef&#34;&gt;REFERENCES&lt;/span&gt; Users(user_id)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The constraint is given the name &lt;code&gt;fk_user_id&lt;/code&gt; and is added to the &lt;code&gt;user_id&lt;/code&gt; attribute. The &lt;code&gt;FOREIGN KEY&lt;/code&gt; constraint states that the &lt;code&gt;user_id&lt;/code&gt; attribute references the &lt;code&gt;user_id&lt;/code&gt; attribute in the &lt;code&gt;Users&lt;/code&gt; table.&lt;/p&gt;
&lt;h2 id=&#34;retrieving-data&#34;&gt;Retrieving Data&lt;/h2&gt;
&lt;p&gt;Retrieving data from an SQL database is done with an &lt;code&gt;SFW&lt;/code&gt; query, &lt;code&gt;SELECT-FROM-WHERE&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;attribute list&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;table&lt;/span&gt; list&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;condition&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For example, we can get the experience and level of a character named &lt;code&gt;Atticus&lt;/code&gt; from the &lt;code&gt;Characters&lt;/code&gt; table with the following query.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; experience, &lt;span style=&#34;color:#66d9ef&#34;&gt;level&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Characters
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt; Name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Atticus&amp;#39;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The attributes we retrieve in a query are referred to as the &lt;code&gt;projection attributes&lt;/code&gt;. This query &lt;code&gt;SELECT~s a ~Character&lt;/code&gt; from all rows of the &lt;code&gt;Character&lt;/code&gt; table which satisfy the &lt;strong&gt;selection condition&lt;/strong&gt; of the &lt;code&gt;WHERE&lt;/code&gt; clause.
We can also query the e-mail addresses of all users who have a character that is a human.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; email
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Users, Characters, Races
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt; Users.user_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Characters.user_id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; Characters.race_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Races.id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; Races.name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Human&amp;#39;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The &lt;code&gt;WHERE&lt;/code&gt; clause in this example is an example of a &lt;strong&gt;join condition&lt;/strong&gt; since it combines attributes from multiple tables. Note that there are two tables which have a &lt;code&gt;user_id&lt;/code&gt; attribute, so we must differentiate them by prepending the table name before the attribute name. This is how ambiguities are solved in SQL.&lt;/p&gt;
&lt;p&gt;You can also use the &lt;code&gt;AS&lt;/code&gt; keyword to shorthand the table names in your query. The previous query can be rewritten as&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; U.username
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;   Users &lt;span style=&#34;color:#66d9ef&#34;&gt;AS&lt;/span&gt; U, Characters &lt;span style=&#34;color:#66d9ef&#34;&gt;AS&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;, Races &lt;span style=&#34;color:#66d9ef&#34;&gt;AS&lt;/span&gt; R
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt;  U.user_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.user_id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; R.id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.race_id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; R.name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Human&amp;#39;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;duplicate-return-values&#34;&gt;Duplicate Return Values&lt;/h3&gt;
&lt;p&gt;The previous query returns the names of all users who have a &lt;code&gt;Human&lt;/code&gt; character. If a user has multiple characters that are &lt;code&gt;Human&lt;/code&gt;, it will return their name multiple times. If we are instead only interested in the names of users who have a &lt;code&gt;Human&lt;/code&gt; character, we can use the &lt;code&gt;DISTINCT&lt;/code&gt; keyword to remove duplicate values.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;DISTINCT&lt;/span&gt; U.username
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;   Users &lt;span style=&#34;color:#66d9ef&#34;&gt;AS&lt;/span&gt; U, Characters &lt;span style=&#34;color:#66d9ef&#34;&gt;AS&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;, Races &lt;span style=&#34;color:#66d9ef&#34;&gt;AS&lt;/span&gt; R
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt;  U.user_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.user_id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; R.id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.race_id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; R.name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Human&amp;#39;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;tables-as-sets&#34;&gt;Tables as Sets&lt;/h3&gt;
&lt;p&gt;SQL uses some set operations from set theory. It supports the &lt;code&gt;UNION&lt;/code&gt;, set difference &lt;code&gt;EXCEPT&lt;/code&gt;, and set intersection &lt;code&gt;INTERSECT&lt;/code&gt; operations. The following query returns the names of all users who have a &lt;code&gt;Human&lt;/code&gt; character or a &lt;code&gt;Gnome&lt;/code&gt; character.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;DISTINCT&lt;/span&gt; U.username
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Users &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; U, Characters &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;, Races &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; R
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt; U.user_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.user_id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.race_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; R.id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; R.name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Human&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;UNION&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;DISTINCT&lt;/span&gt; U.username
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Users &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; U, Characters &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;, Races &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; R
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt; U.user_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.user_id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.race_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; R.id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; R.name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Gnome&amp;#39;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If we wanted to find the users who had both a &lt;code&gt;Human&lt;/code&gt; character and a &lt;code&gt;Gnome&lt;/code&gt; character, we could use the &lt;code&gt;INTERSECT&lt;/code&gt; operator instead.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;DISTINCT&lt;/span&gt; U.username
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Users &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; U, Characters &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;, Races &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; R
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt; U.user_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.user_id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.race_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; R.id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; R.name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Human&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;INTERSECT&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;DISTINCT&lt;/span&gt; U.username
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Users &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; U, Characters &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;, Races &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; R
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt; U.user_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.user_id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.race_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; R.id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; R.name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Gnome&amp;#39;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can also use the &lt;code&gt;EXCEPT&lt;/code&gt; operator to find the users who have a &lt;code&gt;Human&lt;/code&gt; character but not a &lt;code&gt;Gnome&lt;/code&gt; character.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;DISTINCT&lt;/span&gt; U.username
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Users &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; U, Characters &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;, Races &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; R
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt; U.user_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.user_id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.race_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; R.id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; R.name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Human&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;EXCEPT&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;DISTINCT&lt;/span&gt; U.username
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Users &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; U, Characters &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;, Races &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; R
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt; U.user_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.user_id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.race_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; R.id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; R.name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Gnome&amp;#39;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;pattern-matching&#34;&gt;Pattern Matching&lt;/h3&gt;
&lt;p&gt;SQL supports pattern matching with the &lt;code&gt;LIKE&lt;/code&gt; operator. The &lt;code&gt;LIKE&lt;/code&gt; operator is used in the &lt;code&gt;WHERE&lt;/code&gt; clause to search for a specified pattern in a column. This is different from equality operators since it allows us to search for patterns rather than exact matches. The following table shows the most common wildcards used in SQL.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Wildcard&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;%&lt;/td&gt;
&lt;td&gt;Matches any string of zero or more characters.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;Matches any single character.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[]&lt;/td&gt;
&lt;td&gt;Matches any single character within the brackets.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[^]&lt;/td&gt;
&lt;td&gt;Matches any single character not within the brackets.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The following query returns the names of all simple items in the &lt;code&gt;Items&lt;/code&gt; table. These can be found based on their description, since the term &lt;code&gt;simple&lt;/code&gt; is not explicitly mentioned in the name.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt;  name
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;    Items
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt;   description &lt;span style=&#34;color:#66d9ef&#34;&gt;LIKE&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;%simple%&amp;#39;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can also query based on arithmetic ranges. For example, we might be interested in the items that are less than 100 gold.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt;  name
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;    Items
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt;   value &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;ordering&#34;&gt;Ordering&lt;/h3&gt;
&lt;p&gt;SQL allows us to order the results of our query with the &lt;code&gt;ORDER BY&lt;/code&gt; clause. The following query returns the names of all items in the &lt;code&gt;Items&lt;/code&gt; table ordered by their value.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt;  name
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;    Items
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;ORDER&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; value;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can also order by multiple attributes. The following query returns the names of all items in the &lt;code&gt;Items&lt;/code&gt; table ordered by their value and then their name.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt;  name
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;    Items
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;ORDER&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; value, name;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;modifying-data&#34;&gt;Modifying Data&lt;/h2&gt;
&lt;h3 id=&#34;inserting-data&#34;&gt;Inserting Data&lt;/h3&gt;
&lt;p&gt;We previously saw an example of inserting new data. Let&amp;rsquo;s insert a new user account to our table. If we are inserting a value for every attribute, we can omit the attribute list.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;INSERT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;INTO&lt;/span&gt; Users
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;VALUES&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Alex&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;alex.dillhoff@uta.edu&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2023-10-31 15:26:17&amp;#39;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If we are only inserting values for some attributes, we must specify the attribute list.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;INSERT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;INTO&lt;/span&gt; Users (user_id, username, email)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;VALUES&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Alex&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;alex.dillhoff@uta.edu&amp;#39;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If we attempt to leave out a value for an attribute that is not nullable, we will get an error. While working on our database, we may have realized that some of these important attributes should always be specified. We can add a &lt;code&gt;NOT NULL&lt;/code&gt; constraint to these attributes to ensure that they are always specified. We will look at ways of modifying tables in the next section.&lt;/p&gt;
&lt;h3 id=&#34;updating-data&#34;&gt;Updating Data&lt;/h3&gt;
&lt;p&gt;Updating data is a common task and is easily supported by the &lt;code&gt;UPDATE&lt;/code&gt; command. In an RPG, players will use items, gain experience, and level up. All of these will require modifications to existing tables. For example, if we wish to update the experience of a character, we can use the following query.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;UPDATE&lt;/span&gt; Characters
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SET&lt;/span&gt; experience &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; experience &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt; name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Atticus&amp;#39;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;deleting-data&#34;&gt;Deleting Data&lt;/h3&gt;
&lt;p&gt;Deleting a tuple or several tuples is straightforward in SQL. The following query deletes the user with the &lt;code&gt;user_id&lt;/code&gt; of &lt;code&gt;7&lt;/code&gt; from the &lt;code&gt;Users&lt;/code&gt; table.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;DELETE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Users
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt; user_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If we want to delete all tuples from a table, we can use the &lt;code&gt;TRUNCATE TABLE&lt;/code&gt; command. This command is faster than deleting all tuples with the &lt;code&gt;DELETE&lt;/code&gt; command since it does not log each deletion. However, it cannot be used if the table is referenced by a foreign key constraint.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;TRUNCATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; Users;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;When deleting tuples from a database, it&amp;rsquo;s important to consider any foreign key constraints that the table may have. If we delete a tuple from a table that is referenced by a foreign key constraint, we may end up with orphaned tuples. For example, if we delete a user from the &lt;code&gt;Users&lt;/code&gt; table, we may end up with a character that has no user. We can avoid this by adding a &lt;code&gt;CASCADE&lt;/code&gt; constraint to the &lt;code&gt;DELETE&lt;/code&gt; statement. This will delete all tuples that reference the tuple we are deleting.&lt;/p&gt;
&lt;h2 id=&#34;nested-queries&#34;&gt;Nested Queries&lt;/h2&gt;
&lt;p&gt;Nested queries allow us to make more complex queries on subsets of data returned from an original query. A nested query can be placed in any of the &lt;code&gt;SELECT&lt;/code&gt;, &lt;code&gt;FROM&lt;/code&gt;, or &lt;code&gt;WHERE&lt;/code&gt; clauses. The query that uses the results of the nested query is called the &lt;strong&gt;outer query&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;What if we wanted a list of users who had at least 1 character whose class was the least represented class across all characters? We could first make a query to identify which class is the least represented before using that to find the users who have a character with that class.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt;  username
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;    Users
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt;   user_id &lt;span style=&#34;color:#66d9ef&#34;&gt;IN&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; user_id
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Characters
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    &lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt; class_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; class_id
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                      &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Characters
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                      &lt;span style=&#34;color:#66d9ef&#34;&gt;GROUP&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; class_id
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                      &lt;span style=&#34;color:#66d9ef&#34;&gt;ORDER&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;COUNT&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;ASC&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                      &lt;span style=&#34;color:#66d9ef&#34;&gt;LIMIT&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The innert-most query returns the &lt;code&gt;class_id&lt;/code&gt; of the least represented class by counting the number of characters in each class and ordering them in ascending order. The middle query returns the &lt;code&gt;user_id&lt;/code&gt; of all characters whose class is the least represented class. The outer query returns the &lt;code&gt;username&lt;/code&gt; of all users who have a character with the least represented class.&lt;/p&gt;
&lt;p&gt;Pay attention to the second &lt;code&gt;WHERE&lt;/code&gt; clause that uses the &lt;code&gt;=&lt;/code&gt; operator instead of &lt;code&gt;IN&lt;/code&gt;. This is because the nested query returns a single value and single tuple. If we used the &lt;code&gt;IN&lt;/code&gt; operator, we would get an error since MySQL does not support the &lt;code&gt;LIMIT&lt;/code&gt; and &lt;code&gt;IN/ALL/ANY/SOME&lt;/code&gt; operators together.&lt;/p&gt;
&lt;h3 id=&#34;correlated-nested-queries&#34;&gt;Correlated Nested Queries&lt;/h3&gt;
&lt;p&gt;Some nested queries would have to execute for each tuple in the outer query. Consider the following query which returns the name and ID of all Human characters.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.id, &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.name
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;    Characters &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt;   &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.race_id &lt;span style=&#34;color:#66d9ef&#34;&gt;IN&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; R.id
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                      &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Races &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; R
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                      &lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt; R.name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Human&amp;#39;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This query is an example of a &lt;strong&gt;correlated nested query&lt;/strong&gt; since the inner query is dependent on the outer query. The inner query must be executed for each tuple in the outer query. This can be inefficient if the outer query returns a large number of tuples.&lt;/p&gt;
&lt;p&gt;Since this query uses only an &lt;code&gt;IN&lt;/code&gt; operator, we can rewrite it as a single block query.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.id, &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.name
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;    Characters &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;, Races &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; R
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt;   &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.race_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; R.id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; R.name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Human&amp;#39;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s revisit an earlier query to introduce the &lt;code&gt;EXISTS&lt;/code&gt; operator. If we want to query the user names and IDs of all users who have an elf character, we can use the following query.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt;  U.id, U.username
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;    Users &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; U
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt;   &lt;span style=&#34;color:#66d9ef&#34;&gt;EXISTS&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;   Characters &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;, Races &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; R
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.user_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; U.id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.race_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; R.id
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                       &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; R.name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Elf&amp;#39;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can also use the &lt;code&gt;NOT EXISTS&lt;/code&gt; operator to find the users who do not have an elf character. This works opposite to the &lt;code&gt;EXISTS&lt;/code&gt; operator.&lt;/p&gt;
&lt;h2 id=&#34;joined-tables&#34;&gt;Joined Tables&lt;/h2&gt;
&lt;p&gt;Joined tables are the result of a query that combines rows from two or more tables. The syntax itself may be easier to understand as compared to the nested queries written above. The following query returns the names of all users who have a &lt;code&gt;Human&lt;/code&gt; character.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; U.username, &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.name
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;   (Users &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; U &lt;span style=&#34;color:#66d9ef&#34;&gt;JOIN&lt;/span&gt; Characters &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;ON&lt;/span&gt; U.user_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.user_id), Races &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; R
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.race_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; R.id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; R.name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Human&amp;#39;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here, the &lt;code&gt;JOIN&lt;/code&gt; function is used to combine both tables on the condition that the &lt;code&gt;user_id&lt;/code&gt; of the &lt;code&gt;Users&lt;/code&gt; table is equal to the &lt;code&gt;user_id&lt;/code&gt; of the &lt;code&gt;Characters&lt;/code&gt; table. The &lt;code&gt;WHERE&lt;/code&gt; clause is used to filter the results to only those that have a &lt;code&gt;Human&lt;/code&gt; character.&lt;/p&gt;
&lt;p&gt;This type of join is also referred to as an &lt;strong&gt;inner join&lt;/strong&gt; since it only returns rows that satisfy the join condition. There are several other types of joins that we can use to combine tables. If we wanted to return the same information along with all of the users who do not have a &lt;code&gt;Human&lt;/code&gt; character, we can use a &lt;strong&gt;left outer join&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; U.username, &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.name
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;   (Users &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; U &lt;span style=&#34;color:#66d9ef&#34;&gt;LEFT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;OUTER&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;JOIN&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        (Characters &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;JOIN&lt;/span&gt; Races &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; R &lt;span style=&#34;color:#66d9ef&#34;&gt;ON&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.race_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; R.id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; R.name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Human&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;ON&lt;/span&gt; U.user_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.user_id);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;aggregate-functions&#34;&gt;Aggregate Functions&lt;/h2&gt;
&lt;p&gt;Aggregate functions are used to perform calculations on a set of values and return a single value. The following table shows the most common aggregate functions supported by SQL.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Function&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;AVG()&lt;/td&gt;
&lt;td&gt;Returns the average value of a numeric column.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;COUNT()&lt;/td&gt;
&lt;td&gt;Returns the number of rows that match a specified criteria.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MAX()&lt;/td&gt;
&lt;td&gt;Returns the maximum value of a column.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MIN()&lt;/td&gt;
&lt;td&gt;Returns the minimum value of a column.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SUM()&lt;/td&gt;
&lt;td&gt;Returns the sum of all values in a column.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;These can be used in the &lt;code&gt;SELECT&lt;/code&gt; clause. The following query returns the average level of all characters.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;AVG&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;level&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;   Characters;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;These statistics can be used with more complex queries. The following query returns the name of the user with the highest level character.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; U.username
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;   Users &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; U, Characters &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt;  U.user_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.user_id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.&lt;span style=&#34;color:#66d9ef&#34;&gt;level&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;MAX&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;level&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                            &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Characters);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the next example, the query returns the tuple of the highest level Human character.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;    (Characters &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;JOIN&lt;/span&gt; Races &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; R &lt;span style=&#34;color:#66d9ef&#34;&gt;ON&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.race_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; R.id &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; R.name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Human&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt;   &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.&lt;span style=&#34;color:#66d9ef&#34;&gt;level&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;MAX&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;level&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                   &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Characters);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;grouping&#34;&gt;Grouping&lt;/h2&gt;
&lt;p&gt;As we just saw, aggregate functions permit some preliminary data analysis. We can take this further using &lt;strong&gt;grouping&lt;/strong&gt;. For example, we calculated above the average level of all characters. What if we wanted to compute the average level of each class? We can use the &lt;code&gt;GROUP BY&lt;/code&gt; clause to group the tuples by class.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt;   &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.class_id, &lt;span style=&#34;color:#66d9ef&#34;&gt;AVG&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.&lt;span style=&#34;color:#66d9ef&#34;&gt;level&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;     Characters &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;GROUP&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.class_id;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the next example, we will run a similar query except we will also include the number of distinct users who have a character of that class.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt;   &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.class_id, &lt;span style=&#34;color:#66d9ef&#34;&gt;AVG&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.&lt;span style=&#34;color:#66d9ef&#34;&gt;level&lt;/span&gt;), &lt;span style=&#34;color:#66d9ef&#34;&gt;COUNT&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;DISTINCT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.user_id)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;     Characters &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;GROUP&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.class_id;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;with-clause&#34;&gt;&lt;code&gt;WITH&lt;/code&gt; Clause&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s say we wanted to get the actual class name along with the users that had a character with the least represented class. We can use a nested query in the &lt;code&gt;FROM&lt;/code&gt; clause to get the class name.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt;  username, Classes.name
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;    Users, Classes
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt;   user_id &lt;span style=&#34;color:#66d9ef&#34;&gt;IN&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; user_id
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Characters
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    &lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt; class_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; class_id
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                      &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Characters
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                      &lt;span style=&#34;color:#66d9ef&#34;&gt;GROUP&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; class_id
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                      &lt;span style=&#34;color:#66d9ef&#34;&gt;ORDER&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;COUNT&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;ASC&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                      &lt;span style=&#34;color:#66d9ef&#34;&gt;LIMIT&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;AND&lt;/span&gt; Classes.id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; class_id
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                          &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Characters
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                          &lt;span style=&#34;color:#66d9ef&#34;&gt;GROUP&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; class_id
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                          &lt;span style=&#34;color:#66d9ef&#34;&gt;ORDER&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;COUNT&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;ASC&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                          &lt;span style=&#34;color:#66d9ef&#34;&gt;LIMIT&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This query might immediately come across as inefficient since we are making the same query multiple times. If you agree, your intuition would be right. We can use the &lt;code&gt;WITH&lt;/code&gt; clause to make this query more efficient.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WITH&lt;/span&gt; LeastUsedClass &lt;span style=&#34;color:#66d9ef&#34;&gt;AS&lt;/span&gt; (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; class_id
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Characters
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;GROUP&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; class_id
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;ORDER&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;COUNT&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;ASC&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;LIMIT&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; U.username, &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.name
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Users U
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;JOIN&lt;/span&gt; Characters CH &lt;span style=&#34;color:#66d9ef&#34;&gt;ON&lt;/span&gt; U.user_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CH.user_id
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;JOIN&lt;/span&gt; Classes &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;ON&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;C&lt;/span&gt;.id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CH.class_id
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;WHERE&lt;/span&gt; CH.class_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; class_id &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; LeastUsedClass);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;modifying-tables&#34;&gt;Modifying Tables&lt;/h2&gt;
&lt;p&gt;Modifying databases used in production is inevitable. Typically, you will modify an offline test version before deploying it, but the process is the same. The &lt;code&gt;ALTER TABLE&lt;/code&gt; command is used to modify tables. The following table shows the most common modifications that can be made to a table.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Command&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ADD&lt;/td&gt;
&lt;td&gt;Adds a new column to the table.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DROP&lt;/td&gt;
&lt;td&gt;Deletes a column from the table.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MODIFY&lt;/td&gt;
&lt;td&gt;Changes the data type of a column.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RENAME&lt;/td&gt;
&lt;td&gt;Changes the name of a column.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;When we originally created the &lt;code&gt;Users&lt;/code&gt; table, we did not specify a &lt;code&gt;NOT NULL&lt;/code&gt; constraint for the &lt;code&gt;username&lt;/code&gt; attribute. We can add this constraint with the following command.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;ALTER&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; Users
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;MODIFY&lt;/span&gt; username VARCHAR(&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;NOT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;NULL&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Another mistake that was made was with the name of the ID column. We can rename this column with the following command.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;ALTER&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; Users
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RENAME&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;COLUMN&lt;/span&gt; user_id &lt;span style=&#34;color:#66d9ef&#34;&gt;TO&lt;/span&gt; id;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;scenario-updating-foreign-keys&#34;&gt;Scenario: Updating Foreign Keys&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s say we want to delete anything related to a user if that user is deleted from the &lt;code&gt;Users&lt;/code&gt; table. That means all characters and inventories associated with those characters should be deleted. Currently, attempting to delete a user will fail with the following error:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;SQL Error &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1451&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;23000&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;: Cannot delete or update a parent row: a foreign key constraint fails &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;rpg&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;.&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;Inventory&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;, CONSTRAINT &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;Iventory_ibfk_1&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt; FOREIGN KEY &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;character_id&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; REFERENCES &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;Characters&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;id&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It looks like the foreign key also has a typo. Let&amp;rsquo;s recreate this foreign key so that it will delete all characters and inventories associated with a user. First we drop the current key.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;ALTER&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; Inventory
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;DROP&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;FOREIGN&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;KEY&lt;/span&gt; Iventory_ibfk_1;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then we add a new one.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;ALTER&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; Inventory
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;ADD&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;CONSTRAINT&lt;/span&gt; Inventory_ibfk_1
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FOREIGN&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;KEY&lt;/span&gt; (character_id) &lt;span style=&#34;color:#66d9ef&#34;&gt;REFERENCES&lt;/span&gt; Characters(id) &lt;span style=&#34;color:#66d9ef&#34;&gt;ON&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;DELETE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;CASCADE&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;A typical SQL query consists of a &lt;code&gt;SELECT&lt;/code&gt; clause, a &lt;code&gt;FROM&lt;/code&gt; clause, and a &lt;code&gt;WHERE&lt;/code&gt; clause. The &lt;code&gt;SELECT&lt;/code&gt; clause specifies the attributes to be returned. The &lt;code&gt;FROM&lt;/code&gt; clause specifies the tables to be queried. The &lt;code&gt;WHERE&lt;/code&gt; clause specifies the conditions that must be satisfied for a tuple to be returned.&lt;/p&gt;
&lt;p&gt;As we saw in the examples above, there are up to 6 clauses that can be used in a query. The following table shows the clauses that can be used in a query and the order in which they must appear.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Clause&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SELECT&lt;/td&gt;
&lt;td&gt;Specifies the attributes to be returned.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FROM&lt;/td&gt;
&lt;td&gt;Specifies the tables to be queried.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;WHERE&lt;/td&gt;
&lt;td&gt;Specifies the conditions that must be satisfied for a tuple to be returned.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GROUP BY&lt;/td&gt;
&lt;td&gt;Groups the tuples by a specified attribute.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HAVING&lt;/td&gt;
&lt;td&gt;Specifies the conditions that must be satisfied for a group to be returned.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ORDER BY&lt;/td&gt;
&lt;td&gt;Specifies the order in which the tuples are returned.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Chamberlin, Donald D., and Raymond F. Boyce. 1974. “SEQUEL: A Structured English Query Language.” In &lt;i&gt;Proceedings of the 1974 ACM SIGFIDET (Now SIGMOD) Workshop on Data Description, Access and Control&lt;/i&gt;, 249–64. SIGFIDET ’74. New York, NY, USA: Association for Computing Machinery. &lt;a href=&#34;https://doi.org/10.1145/800296.811515&#34;&gt;https://doi.org/10.1145/800296.811515&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Codd, E. F. 1970. “A Relational Model of Data for Large Shared Data Banks.” &lt;i&gt;Communications of the Acm&lt;/i&gt; 13 (6): 377–87. &lt;a href=&#34;https://doi.org/10.1145/362384.362685&#34;&gt;https://doi.org/10.1145/362384.362685&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Databases</title>
      <link>https://ajdillhoff.github.io/notes/introduction_to_databases/</link>
      <pubDate>Sat, 28 Oct 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/introduction_to_databases/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#an-online-rpg-database&#34;&gt;An Online RPG Database&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#from-schema-to-database&#34;&gt;From Schema to Database&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#database-management-systems&#34;&gt;Database Management Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#creating-our-rpg-database&#34;&gt;Creating our RPG Database&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;&lt;strong&gt;Recommended Reading: Chapters 1 and 2 from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Elmasri and Navathe 2015&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Databases allow us to store, retrieve, and edit different types of data. They should be scalable, secure, and reliable. They should also be able to handle concurrent access and be able to recover from failures. There are multiple types of databases that are optimized for different use cases. Tabular data, for example, is typically stored in a &lt;strong&gt;relational&lt;/strong&gt; database. Large format data such as images, videos, and audio are typically stored in a &lt;strong&gt;non-relational&lt;/strong&gt; database.&lt;/p&gt;
&lt;p&gt;Creating, deploying, and maintaining databases is facilitated through a &lt;strong&gt;database management system&lt;/strong&gt; (DBMS). A DBMS is a software system that allows us to interact with a database. It provides an interface for us to create, read, update, and delete data. It also provides a way for us to define the structure of our data and the relationships between different pieces of data. Examples of DBMSs include MySQL, PostgreSQL, and MongoDB.&lt;/p&gt;
&lt;p&gt;Once a database is deployed, we can interact with it a number of ways. Most DBMSs include a client which allows us to interact with the database through a command line interface. We can also interact with the database through a programming language such as Python or Java.&lt;/p&gt;
&lt;p&gt;It is important to emphasize that a database is not the same thing as a file system. A file system is a way to store data on a disk, whereas a database is a way to store data in a file system. File systems are good at managing unstructured data with little regard to the relationships inherit in the data itself. What if multiple people working on the same document try to save their changes at the same time? What if a user tries to delete a file that is currently being used by another user? These are problems that a file system is not designed to handle.&lt;/p&gt;
&lt;h2 id=&#34;an-online-rpg-database&#34;&gt;An Online RPG Database&lt;/h2&gt;
&lt;p&gt;To introduce some foundational terms and concepts of databases, let&amp;rsquo;s design and create a database for an online RPG. In this game, users can create accounts, make multiple characters, store items for their characters, and embark on quests to level up their characters. Even from this simple description, we can start separating our data into different &lt;strong&gt;entities&lt;/strong&gt; and &lt;strong&gt;relationships&lt;/strong&gt;. Each logical entity in our game will be represented by a &lt;strong&gt;table&lt;/strong&gt; in our database. The &lt;strong&gt;attributes&lt;/strong&gt; of each table will be represented by columns in our database. For this database, we will need &lt;em&gt;at least&lt;/em&gt; the following tables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Users&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Characters&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Items&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Inventory&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Quests&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We may add or modify these depending on the finer details. If you are not familiar with online RPG games, don&amp;rsquo;t worry. We will be sure to include the necessities to get us started. Let&amp;rsquo;s start with the first table, &lt;code&gt;Users&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;users&#34;&gt;Users&lt;/h3&gt;
&lt;p&gt;A &lt;code&gt;User&lt;/code&gt; represents an online account that is unique to each person who plays the game. It should contain the &lt;code&gt;username&lt;/code&gt;, &lt;code&gt;email&lt;/code&gt;, and date that it was created, which we will call &lt;code&gt;created_at&lt;/code&gt;. This is enough information for now. Using this, we can create our first &lt;strong&gt;table&lt;/strong&gt;. There is one more attribute that wasn&amp;rsquo;t explicitly mentioned. Each &lt;code&gt;User&lt;/code&gt; in our table should have a unique identifier. This is called a &lt;strong&gt;primary key&lt;/strong&gt;. We will use a sequentially increasing number starting at 1 for our primary key. This is a common practice, but it is not the only way to do it. We will call this column &lt;code&gt;user_id&lt;/code&gt;. The full table is showing below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Users&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;user_id&lt;/code&gt;: primary key&lt;/li&gt;
&lt;li&gt;&lt;code&gt;username&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;email&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;created_at&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;characters&#34;&gt;Characters&lt;/h3&gt;
&lt;p&gt;It is common for users to have multiple characters so they can experience the full range of our game. This table will have more attributes than the &lt;code&gt;Users&lt;/code&gt; table since there are a wide range of stats that our characters can have, such as their name, level, experience, and health. We will also need to know which user each character belongs to. We can do this by adding a column called &lt;code&gt;user_id&lt;/code&gt; which will be a &lt;strong&gt;foreign key&lt;/strong&gt; to the &lt;code&gt;Users&lt;/code&gt; table. This will allow us to link each character to the user that created it. The full table is shown below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Characters&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;character_id&lt;/code&gt;: primary key&lt;/li&gt;
&lt;li&gt;&lt;code&gt;user_id&lt;/code&gt;: foreign key&lt;/li&gt;
&lt;li&gt;&lt;code&gt;name&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;level&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;experience&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;health&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;created_at&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;items&#34;&gt;Items&lt;/h3&gt;
&lt;p&gt;As our user&amp;rsquo;s play, they will collect items such as weapons, armor, and potions. As our game evolves, our game designers will add more items to the game. A table for our items is shown below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Items&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;item_id&lt;/code&gt;: primary key&lt;/li&gt;
&lt;li&gt;&lt;code&gt;name&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;value&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;inventory&#34;&gt;Inventory&lt;/h3&gt;
&lt;p&gt;Our users will need a way to store their items. We can do this by creating a table called &lt;code&gt;Inventory&lt;/code&gt;. This table will have a foreign key to the &lt;code&gt;Characters&lt;/code&gt; table so we can link each item to the character that owns it. It will also have a foreign key to the &lt;code&gt;Items&lt;/code&gt; table so we can link each item to the item that it represents. We will also need to know how many of each item our users have. We can do this by adding a column called &lt;code&gt;quantity&lt;/code&gt;. The full table is shown below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inventory&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;inventory_id&lt;/code&gt;: primary key&lt;/li&gt;
&lt;li&gt;&lt;code&gt;character_id&lt;/code&gt;: foreign key&lt;/li&gt;
&lt;li&gt;&lt;code&gt;item_id&lt;/code&gt;: foreign key&lt;/li&gt;
&lt;li&gt;&lt;code&gt;quantity&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;quests&#34;&gt;Quests&lt;/h3&gt;
&lt;p&gt;No RPG would be complete without quests that our player&amp;rsquo;s could embark upon. The &lt;code&gt;Quests&lt;/code&gt; table will have a name, description, and a reward. In the case of multiple rewards, we can create a separate table called &lt;code&gt;QuestRewards&lt;/code&gt; that will have a foreign key to the &lt;code&gt;Quests&lt;/code&gt; table and a foreign key to the &lt;code&gt;Items&lt;/code&gt; table. This will allow us to link each quest to the items that it rewards. This means that the &lt;code&gt;Quests&lt;/code&gt; table does not need an explicit reference to the reward item. We can look those up separately. The full table is shown below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quests&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;quest_id&lt;/code&gt;: primary key&lt;/li&gt;
&lt;li&gt;&lt;code&gt;name&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;description&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;reward_experience&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;min_level&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;QuestRewards&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;quest_reward_id&lt;/code&gt;: primary key&lt;/li&gt;
&lt;li&gt;&lt;code&gt;quest_id&lt;/code&gt;: foreign key&lt;/li&gt;
&lt;li&gt;&lt;code&gt;item_id&lt;/code&gt;: foreign key&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;a-few-extras&#34;&gt;A Few Extras&lt;/h3&gt;
&lt;p&gt;There are a few more tables we should add to round out our characters. Most RPGs allow the users to create characters of different &lt;strong&gt;races&lt;/strong&gt;, such as a human, orc, or elf, as well as the characters &lt;strong&gt;class&lt;/strong&gt;, which defines what sort of abilities the character will have.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Race&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;race_id&lt;/code&gt;: primary key&lt;/li&gt;
&lt;li&gt;&lt;code&gt;name&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Class&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class_id&lt;/code&gt;: primary key&lt;/li&gt;
&lt;li&gt;&lt;code&gt;name&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With the addition of these two tables, let&amp;rsquo;s add &lt;em&gt;foreign keys&lt;/em&gt; to our original &lt;code&gt;Characters&lt;/code&gt; table. We will add a &lt;code&gt;race_id&lt;/code&gt; and a &lt;code&gt;class_id&lt;/code&gt;. The full table is shown below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Characters&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;character_id&lt;/code&gt;: primary key&lt;/li&gt;
&lt;li&gt;&lt;code&gt;user_id&lt;/code&gt;: foreign key&lt;/li&gt;
&lt;li&gt;&lt;code&gt;name&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;level&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;experience&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;health&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;race_id&lt;/code&gt;: foreign key&lt;/li&gt;
&lt;li&gt;&lt;code&gt;class_id&lt;/code&gt;: foreign key&lt;/li&gt;
&lt;li&gt;&lt;code&gt;created_at&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That&amp;rsquo;s it! We have all the tables we need to get us started. All tables with example data are shown below. You&amp;rsquo;ll notice that each of the primary IDs in the tables below have been renamed to &lt;code&gt;id&lt;/code&gt;. Besides giving us extra room to display the table, the primary key is always unique to the table, so we don&amp;rsquo;t need to include the table name in the column name.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Users&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\begin{array}{|r|l|l|l|}
\hline
\text{id} &amp;amp; \text{username} &amp;amp; \text{email} &amp;amp; \text{created_at} \\
\hline
1 &amp;amp; \text{Naomi} &amp;amp; \text{player1@example.com} &amp;amp; \text{2023-01-01 10:00:00} \\
2 &amp;amp; \text{Clarissa} &amp;amp; \text{player2@example.com} &amp;amp; \text{2023-01-02 11:00:00} \\
3 &amp;amp; \text{Avasarala} &amp;amp; \text{player3@example.com} &amp;amp; \text{2023-01-03 12:00:00} \\
\hline
\end{array}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Characters&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\begin{array}{|r|r|l|r|r|r|r|r|l|}
\hline
\text{id} &amp;amp; \text{user_id} &amp;amp; \text{name} &amp;amp; \text{class_id} &amp;amp; \text{race_id} &amp;amp; \text{level} &amp;amp; \text{experience} &amp;amp; \text{health} &amp;amp; \text{created_at} \\
\hline
1 &amp;amp; 1 &amp;amp; \text{Atticus} &amp;amp; 1 &amp;amp; 1 &amp;amp; 10 &amp;amp; 1000 &amp;amp; 100 &amp;amp; \text{2023-01-01 10:10:00} \\
2 &amp;amp; 1 &amp;amp; \text{Bobbie} &amp;amp; 2 &amp;amp; 2 &amp;amp; 15 &amp;amp; 1500 &amp;amp; 200 &amp;amp; \text{2023-01-01 10:20:00} \\
3 &amp;amp; 2 &amp;amp; \text{Raimi} &amp;amp; 3 &amp;amp; 3 &amp;amp; 8 &amp;amp; 800 &amp;amp; 90 &amp;amp; \text{2023-01-02 11:10:00} \\
4 &amp;amp; 3 &amp;amp; \text{Beef} &amp;amp; 4 &amp;amp; 4 &amp;amp; 12 &amp;amp; 1200 &amp;amp; 110 &amp;amp; \text{2023-01-03 12:10:00} \\
5 &amp;amp; 2 &amp;amp; \text{Demon} &amp;amp; 4 &amp;amp; 4 &amp;amp; 12 &amp;amp; 1200 &amp;amp; 110 &amp;amp; \text{2023-01-05 12:10:00} \\
\hline
\end{array}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Items&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\begin{array}{|r|l|r|r|}
\hline
\text{id} &amp;amp; \text{name} &amp;amp; \text{value} \\
\hline
1 &amp;amp; \text{Sword} &amp;amp; 100 \\
2 &amp;amp; \text{Shield} &amp;amp; 150 \\
3 &amp;amp; \text{Staff} &amp;amp; 200 \\
4 &amp;amp; \text{Bow} &amp;amp; 250 \\
\hline
\end{array}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inventory&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\begin{array}{|r|r|r|r|}
\hline
\text{id} &amp;amp; \text{character_id} &amp;amp; \text{item_id} &amp;amp; \text{quantity} \\
\hline
1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 \\
2 &amp;amp; 2 &amp;amp; 2 &amp;amp; 1 \\
3 &amp;amp; 3 &amp;amp; 3 &amp;amp; 1 \\
4 &amp;amp; 4 &amp;amp; 4 &amp;amp; 1 \\
\hline
\end{array}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quests&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\begin{array}{|r|l|l|r|l|r|}
\hline
\text{id} &amp;amp; \text{name} &amp;amp; \text{description} &amp;amp; \text{reward_experience} &amp;amp; \text{min_level} \\
\hline
1 &amp;amp; \text{Linken&amp;rsquo;s Sword} &amp;amp; \text{Desc1} &amp;amp; 100 &amp;amp; 5 \\
2 &amp;amp; \text{Mankrik&amp;rsquo;s Wife} &amp;amp; \text{Desc2} &amp;amp; 200 &amp;amp; 10 \\
3 &amp;amp; \text{The Hermit} &amp;amp; \text{Desc3} &amp;amp; 300 &amp;amp; 15 \\
4 &amp;amp; \text{The Great Masquerade} &amp;amp; \text{Desc4} &amp;amp; 400 &amp;amp; 20 \\
\hline
\end{array}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;QuestRewards&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\begin{array}{|r|r|r|}
\hline
\text{id} &amp;amp; \text{quest_id} &amp;amp; \text{item_id} \\
\hline
1 &amp;amp; 1 &amp;amp; 1 \\
2 &amp;amp; 2 &amp;amp; 2 \\
3 &amp;amp; 3 &amp;amp; 3 \\
4 &amp;amp; 4 &amp;amp; 4 \\
\hline
\end{array}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Races&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\begin{array}{|r|l|}
\hline
\text{race_id} &amp;amp; \text{name} \\
\hline
1 &amp;amp; \text{Human} \\
2 &amp;amp; \text{Elf} \\
3 &amp;amp; \text{Dwarf} \\
4 &amp;amp; \text{Orc} \\
\hline
\end{array}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\begin{array}{|r|l|}
\hline
\text{class_id} &amp;amp; \text{name} \\
\hline
1 &amp;amp; \text{Warrior} \\
2 &amp;amp; \text{Mage} \\
3 &amp;amp; \text{Rogue} \\
4 &amp;amp; \text{Paladin} \\
\hline
\end{array}&lt;/p&gt;
&lt;h2 id=&#34;from-schema-to-database&#34;&gt;From Schema to Database&lt;/h2&gt;
&lt;p&gt;What we did in the previous example is created a database &lt;strong&gt;schema&lt;/strong&gt; based on our entities. A schema does not represent the entire picture of our &lt;strong&gt;data model&lt;/strong&gt;. Relationships and other constraints are not represented in the schema. The data model itself defines the structure of a database, including data types, relationships, constraints, and a set of operations for performing basic functions like retrieving and updating data.&lt;/p&gt;
&lt;h3 id=&#34;the-three-schema-architecture&#34;&gt;The Three-Schema Architecture&lt;/h3&gt;
&lt;p&gt;The three-schema architecture is a way to separate the different aspects of a database. The three schemas are the &lt;strong&gt;external schema&lt;/strong&gt;, the &lt;strong&gt;conceptual schema&lt;/strong&gt;, and the &lt;strong&gt;internal schema&lt;/strong&gt;. The internal schema describes how the data is stored on disk. Unless we are working on the backend of the database, we typically do not need to worry about the internal level. The external schema describes how the data is viewed by the user. This is the level that we interact with when we use a DBMS. The conceptual schema is the middle layer that describes the logical structure of the data. This is the level that we are working with when we create a schema.&lt;/p&gt;
&lt;p&gt;Under this architecture, we can modify the internal schema without affecting the external schema. This is important because it allows us to change the way that the data is stored without affecting the applications that use it. We can also modify the external schema without affecting the internal schema. This allows us to change the way that the data is viewed without affecting the applications that use it. This concept of &lt;strong&gt;data independence&lt;/strong&gt; is one of the most important features of a DBMS.&lt;/p&gt;
&lt;h2 id=&#34;database-management-systems&#34;&gt;Database Management Systems&lt;/h2&gt;
&lt;p&gt;With our database defined, we can use it to make &lt;strong&gt;queries&lt;/strong&gt; about the records that it stores. How we access that database depends on the DBMS that we are using. The database itself is can be modified and changed without affecting the applications that use it. We can also create multiple &lt;strong&gt;views&lt;/strong&gt; of our data dynamically. For example, we can create a view that shows all of the items that a user has in their inventory, or show all of the characters that belong to a specific user. This is all done without modifying the underlying data. This is a powerful feature of databases that allows us to create complex applications that can be easily modified and updated.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;transaction&lt;/strong&gt; is a set of operations that are performed on a database. Transactions are typically used to ensure that the database is in a consistent state. For example, if we want to transfer money from one account to another, we need to make sure that the money is removed from one account and added to the other. If we fail to do this, we could end up with money that is neither in the original account nor the destination account. Transactions allow us to perform these operations in a way that guarantees that the database is in a consistent state.&lt;/p&gt;
&lt;p&gt;A DBMS must ensure transactional properties such as &lt;strong&gt;isolation&lt;/strong&gt;, which ensure that each transaction executes in isolation from others, and &lt;strong&gt;atomicity&lt;/strong&gt;, which ensures that either all operations in a transaction are executed or none are.&lt;/p&gt;
&lt;h3 id=&#34;dbms-languages&#34;&gt;DBMS Languages&lt;/h3&gt;
&lt;p&gt;A DBMS provides a way for us to interact with the database. Depending on the level of abstraction and the DBMS itself, a specific language is used to perform basic operations on the database. The most common languages are &lt;strong&gt;data definition languages&lt;/strong&gt; (DDLs) and &lt;strong&gt;data manipulation languages&lt;/strong&gt; (DMLs). A DDL is used to define the structure of the database, such as creating tables and defining relationships between them. A DML is used to perform operations on the data itself, such as inserting, updating, and deleting records.&lt;/p&gt;
&lt;p&gt;A common query language called Structured Query Language (SQL) defines both DDLs and DMLs. For example, to create our &lt;code&gt;User&lt;/code&gt; table from above, we can use the following SQL statement:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; Users (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  user_id INT &lt;span style=&#34;color:#66d9ef&#34;&gt;PRIMARY&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;KEY&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  username VARCHAR(&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  email VARCHAR(&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  created_at DATETIME
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that we must specify a type for each attribute in our table. SQL also provides a DML, we can use to insert records into our table:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;INSERT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;INTO&lt;/span&gt; Users (user_id, username, email, created_at)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;VALUES&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Naomi&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;player1@example.com&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2023-01-01 10:00:00&amp;#39;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;dbms-interfaces&#34;&gt;DBMS Interfaces&lt;/h3&gt;
&lt;p&gt;A DBMS provides an interface for us to interact with the database. This interface can be a command line interface, a graphical user interface, or a programming language interface. Other interfaces using natural language or voice can also be found in the wild. With the rapid advancement of machine learning, these interfaces are becoming more and more common. &lt;a href=&#34;https://github.com/kulltc/chatgpt-sql&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Here&lt;/a&gt; is an example of a chatbot that can be used to query a database.&lt;/p&gt;
&lt;h2 id=&#34;creating-our-rpg-database&#34;&gt;Creating our RPG Database&lt;/h2&gt;
&lt;p&gt;For this example, we will be using MySQL. We only want to make sure that we have MySQL installed and are able to interface with the command line. You can find a thorough installation guide &lt;a href=&#34;https://dev.mysql.com/doc/mysql-installation-excerpt/5.7/en/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;. Once it is installed and configured, start the MySQL server and log in using the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mysql -u root -p
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You should be prompted for a password. If you have not set a password, you can leave it blank. Once you are logged in, you should see a prompt that looks like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mysql&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s create a database for our RPG. We can do this with the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;DATABASE&lt;/span&gt; rpg;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can verify that the database was created by listing all of the databases on the server:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SHOW&lt;/span&gt; DATABASES;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You should see the &lt;code&gt;rpg&lt;/code&gt; database in the list. We can now use this database to create our tables. We can do this with the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;USE rpg;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will tell MySQL to use the &lt;code&gt;rpg&lt;/code&gt; database for all subsequent commands. We can now create our &lt;code&gt;Users&lt;/code&gt; table:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; Users (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  user_id INT &lt;span style=&#34;color:#66d9ef&#34;&gt;PRIMARY&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;KEY&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  username VARCHAR(&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  email VARCHAR(&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  created_at DATETIME
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can verify that the table was created by listing all of the tables in the database:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SHOW&lt;/span&gt; TABLES;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You should see the &lt;code&gt;Users&lt;/code&gt; table in the list. We can now insert some data into the table:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;INSERT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;INTO&lt;/span&gt; Users (user_id, username, email, created_at)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;VALUES&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;         (&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Naomi&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;player1@example.com&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2023-01-01 10:00:00&amp;#39;&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;         (&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Clarissa&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;player2@example.com&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2023-01-02 11:00:00&amp;#39;&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;         (&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Avasarala&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;player3@example.com&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2023-01-03 12:00:00&amp;#39;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can verify that the data was inserted by querying the table:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; Users;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You should see the data that we inserted in the table. We can now create the rest of our tables:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; Characters (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  character_id INT &lt;span style=&#34;color:#66d9ef&#34;&gt;PRIMARY&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;KEY&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  user_id INT,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  name VARCHAR(&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;level&lt;/span&gt; INT,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  experience INT,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  health INT,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  created_at DATETIME
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; Items (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  item_id INT &lt;span style=&#34;color:#66d9ef&#34;&gt;PRIMARY&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;KEY&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  name VARCHAR(&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  value INT
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; Inventory (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  inventory_id INT &lt;span style=&#34;color:#66d9ef&#34;&gt;PRIMARY&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;KEY&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  character_id INT,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  item_id INT,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  quantity INT
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; Quests (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  quest_id INT &lt;span style=&#34;color:#66d9ef&#34;&gt;PRIMARY&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;KEY&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  name VARCHAR(&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  description VARCHAR(&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  reward_experience INT,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  min_level INT
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; QuestRewards (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  quest_reward_id INT &lt;span style=&#34;color:#66d9ef&#34;&gt;PRIMARY&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;KEY&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  quest_id INT,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  item_id INT
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Try creating the tables for the &lt;code&gt;Races&lt;/code&gt; and &lt;code&gt;Classes&lt;/code&gt; yourself. Once you are done, you can insert some data into the tables. Use the samples from above or create your own. Once you are done, you can query the tables to verify that the data was inserted correctly.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Elmasri, Ramez, and Shamkant B. Navathe. 2015. &lt;i&gt;Fundamentals of Database Systems&lt;/i&gt;. 7th ed. Pearson. &lt;a href=&#34;https://www.pearson.com/en-us/subject-catalog/p/fundamentals-of-database-systems/P200000003546/9780137502523&#34;&gt;https://www.pearson.com/en-us/subject-catalog/p/fundamentals-of-database-systems/P200000003546/9780137502523&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Minimum Spanning Trees</title>
      <link>https://ajdillhoff.github.io/notes/minimum_spanning_trees/</link>
      <pubDate>Sat, 21 Oct 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/minimum_spanning_trees/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#finding-the-minimum-spanning-tree&#34;&gt;Finding the Minimum Spanning Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kruskal-s-algorithm&#34;&gt;Kruskal&amp;rsquo;s Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#prim-s-algorithm&#34;&gt;Prim&amp;rsquo;s Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Minimum spanning trees are undirected graphs that connect all of the vertices such that there are no redundant edges and the total weight is minimized. They are useful for finding the shortest path between two points in a graph. Useful application of MSTs include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;network design&lt;/strong&gt;: it is useful to know the least expensive path with respect to either latency or resource cost for telecommunications networks, transportation networks, or electrical grids.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;approximation algorithms&lt;/strong&gt;: MSTs can be used to approximate the solution to the traveling salesman problem.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;clustering&lt;/strong&gt;: MSTs can be used to cluster data points in a graph.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;image segmentation&lt;/strong&gt;: MSTs can be used to segment images into smaller regions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;Let \(G\) be a connected, undirected graph with edges \(E\), vertices \(V\), and edge weights \(w\). A &lt;strong&gt;minimum spanning tree&lt;/strong&gt; is a subset \(T \subseteq E\) that connects all of the vertices such that the total weight is minimized. The original graph \(G\) is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-21_18-46-30_undirected_original.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;An undirected graph with redundant edges.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;An undirected graph with redundant edges.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The minimum spanning tree of the above graph is show below. All of the redundant edges have been removed, but there is still a path between each pair of nodes.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-21_18-56-15_mst.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;The minimum spanning tree of (G).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;The minimum spanning tree of (G).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;As described in &lt;em&gt;Introduction to Algorithms&lt;/em&gt; there are two greedy algorithms for finding the minimum spanning tree of a graph (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;). These notes will review both of these, but first let&amp;rsquo;s look at a general algorithm for finding the minimum spanning tree of a graph.&lt;/p&gt;
&lt;h2 id=&#34;finding-the-minimum-spanning-tree&#34;&gt;Finding the Minimum Spanning Tree&lt;/h2&gt;
&lt;p&gt;The general algorithm for finding the minimum spanning tree of a graph grows a set of edges \(T\) from an empty set. At each step, the algorithm adds the edge with the smallest weight that does not create a cycle. The algorithm terminates when \(T\) is a complete tree.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;T = {}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;while T is not a spaning tree
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    find the edge e with the smallest weight that does not create a cycle
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    T = T union {e}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Each edge \(e\) that is added must result in a tree that is a subset of the minimum spanning tree. The challenge of this algorithm is actually finding such an edge. How would we know such an edge if we saw it? We first need to define a few properties which will shine light on this.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;cut&lt;/strong&gt; of a graph \(G\) is a partition of the vertices \(V\) into two disjoint sets \(S\) and \(V - S\). An edge \(e\) &lt;strong&gt;crosses&lt;/strong&gt; the cut if one of its endpoints is in \(S\) and the other is in \(V - S\). If no edge in a given set \(E\) crosses the cut, then that cut &lt;strong&gt;respects&lt;/strong&gt; \(E\). An edge that is the minimum weight edge that crosses a cut is called a &lt;strong&gt;light edge&lt;/strong&gt;. With these definitions, we can now formally define how to find a &lt;strong&gt;safe edge&lt;/strong&gt;, which is an edge that can be added to the current set of edges \(T\) without creating a cycle.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Theorem 21.1 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;)&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let \(G = (V, E)\) be a connected, undirected graph with a real-valued weight function \(w\) defined on \(E\). Let \(A\) be a subset of \(E\) that is included in some minimum spanning tree for \(G\), let \((S, V - S)\) be any cut of \(G\) that respects \(A\), and let \(e\) be a light edge crossing \((S, V - S)\). Then, edge \(e\) is safe for \(A\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-22_13-10-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Visual proof of Theorem 21.1 (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Visual proof of Theorem 21.1 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The two sets in the figure above represent vertices in \(S\) (orange) and vertices in \(V - S\) (tan). \(T\) is the original MST depicted in the figure. The dotted line is the new edge \((u, v)\) to consider. \(A\) is a subset of edges in \(T\) represented by the blue lines. If the safe edge \((u, v)\) is already in the original MST \(T\), then we are done.&lt;/p&gt;
&lt;p&gt;The vertices \(u\) and \(v\) lie on opposite sides of the cut. The edge \((u, v)\) would introduce a cycle since there is already a path from \(u\) to \(v\) in \(T\) that crosses the cut via \((x, y)\). Since both \((u, v)\) and \((x, y)\) are light edges that cross the cut, then it must be that \(w(u, v) \leq w(x, y)\).&lt;/p&gt;
&lt;p&gt;Let \(T&amp;rsquo;\) be the minimum spanning tree with \((x, y)\) replaced by \((u, v)\). That is \(T&amp;rsquo; = T - \{(x, y)\} \cup \{(u, v)\}\). Since \(T\) is a minimum spanning tree, then \(w(T) \leq w(T&amp;rsquo;)\). Since \(w(T) = w(T&amp;rsquo;)\), then \(T&amp;rsquo;\) is also a minimum spanning tree. Therefore, \((u, v)\) is safe for \(A\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Corollary 21.2 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;)&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We can also view this in terms of &lt;strong&gt;connected components&lt;/strong&gt;, which are subsets of vertices that are connected by a path. If \(C\) and \(C&amp;rsquo;\) are two connected components in \(T\) and \((u, v)\) is a light edge connecting \(C\) and \(C&amp;rsquo;\), then \((u, v)\) is safe for \(T\).&lt;/p&gt;
&lt;p&gt;The figure below shows a graph with two individual components. If the edge \((u, v)\) is a light edge, then it is safe to add it to the set of edges \(T\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-22_15-39-37_connected_components.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Two connected components from a graph (left). Adding a safe edge (right).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Two connected components from a graph (left). Adding a safe edge (right).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;kruskal-s-algorithm&#34;&gt;Kruskal&amp;rsquo;s Algorithm&lt;/h2&gt;
&lt;p&gt;The first solution to the minimum spanning tree that we will study is called &lt;strong&gt;Kruskal&amp;rsquo;s algorithm&lt;/strong&gt;. This algorithm grows a forest of trees from an empty set. At each step, the algorithm adds the lightest edge that does not create a cycle. The algorithm terminates when the forest is a single tree. This can be viewed as an agglomerative clustering algorithm. The algorithm starts with each vertex in its own cluster. At each step, the algorithm merges the two clusters that are closest together. The algorithm terminates when there is only one cluster.&lt;/p&gt;
&lt;p&gt;The algorithm is given below (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Kruskal&amp;rsquo;s Algorithm&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A = {}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;for each vertex v in G.V
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    MAKE-SET(v)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sort the edges of G.E into nondecreasing order by weight w
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;for each edge (u, v) in G.E, taken in nondecreasing order by weight
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    if FIND-SET(u) != FIND-SET(v)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A = A union {(u, v)}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        UNION(u, v)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;return A
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;A step-by-step example of an implementation in Python is available &lt;a href=&#34;https://github.com/ajdillhoff/python-examples/blob/main/data_structures/graphs/kruskals_algorithm.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;The running time is dependent on how the disjoint-set of vertices is implemented. In the best known case, a &lt;em&gt;disjoint-set-forest&lt;/em&gt; implementation should be used (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;). Creating a list of edges takes \(O(E)\) time. Sorting the edges takes \(O(E \log E)\) time. The &lt;code&gt;for&lt;/code&gt; loop iterates over each edge, which is \(O(E)\). All disjoin-set operations take \(O((V + E)\alpha(V))\) time. Since the graph is connected, \(E \geq V - 1\), so the total running time is \(O(E \log E + E + E \alpha(V)) = O(E \log E + E \alpha(V)) = O(E \log V)\).&lt;/p&gt;
&lt;h2 id=&#34;prim-s-algorithm&#34;&gt;Prim&amp;rsquo;s Algorithm&lt;/h2&gt;
&lt;p&gt;The second solution starts at an arbitrary vertex in a set \(A\) and adds a new vertex to \(A\) in a greedy fashion. To efficiently select a new edge to add, Prim&amp;rsquo;s algorithm uses a priority queue to keep track of the lightest edge that crosses the cut. The algorithm terminates when \(A\) is a complete tree. The full algorithm is given below. We will step through it in more detail after.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Prim&amp;rsquo;s Algorithm&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A = {}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;for each vertex v in G.V
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    key[v] = infinity
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    pi[v] = NIL
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;key[r] = 0
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q = G.V
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;while Q is not empty
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    u = EXTRACT-MIN(Q)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A = A union {u}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    for each vertex v in G.Adj[u]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        if v in Q and w(u, v) &amp;lt; key[v]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            pi[v] = u
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            key[v] = w(u, v)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You might look at this and wonder how the MST is represented. Prim&amp;rsquo;s algorithm implicitly maintains the set \(A = \{(v, v.\pi) : v \in V - \{r\} - Q\}\). When the &lt;code&gt;while&lt;/code&gt; loop terminates, \(A = \{(v, v.\pi) : v \in V - \{r\}\}\), since the queue is empty. The critical part of this is to understand how the algorith changes the key values.&lt;/p&gt;
&lt;p&gt;A step-by-step example of an implementation in Python is available &lt;a href=&#34;https://github.com/ajdillhoff/python-examples/blob/main/data_structures/graphs/prims_algorithm.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;Prim&amp;rsquo;s algorithm uses a priority queue to keep track of the lightest edge that crosses the cut. If the priority queue is implemented as a &lt;a href=&#34;https://www.cs.cmu.edu/~tcortina/15-121sp10/Unit06B.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;min-heap&lt;/a&gt;, which has a worst-case running time of \(O(\log V)\) for both &lt;code&gt;EXTRACT-MIN&lt;/code&gt; and &lt;code&gt;DECREASE-KEY&lt;/code&gt;. The algorithm calls &lt;code&gt;EXTRACT-MIN&lt;/code&gt; once for each vertex, which is \(O(V \log V)\). The algorithm calls &lt;code&gt;DECREASE-KEY&lt;/code&gt; once for each edge, which is \(O(E \log V)\). The total running time is \(O(V \log V + E \log V) = O(E \log V)\).&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Single-Source Shortest Paths</title>
      <link>https://ajdillhoff.github.io/notes/single_source_shortest_paths/</link>
      <pubDate>Sat, 21 Oct 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/single_source_shortest_paths/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bellman-ford&#34;&gt;Bellman-Ford&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#shortest-paths-on-a-dag&#34;&gt;Shortest Paths on a DAG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dijkstra-s-algorithm&#34;&gt;Dijkstra&amp;rsquo;s Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;When you hear the term &lt;em&gt;shortest path&lt;/em&gt;, you may think of the shortest physical distance between your current location and wherever it is you&amp;rsquo;re going. Finding the most optimal route via GPS is one of the most widely used mobile applications. Physical paths are not the only types we may wish to find a shortest path for. Other examples include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Network Routing&lt;/strong&gt;: To improve network performance, it is critical to know the shortest path from one system to another in terms of latency.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Puzzle Solving&lt;/strong&gt;: For puzzles such as a Rubik&amp;rsquo;s cube, the vertices could represents states of the cube and edges could correspond to a single move.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Robotics&lt;/strong&gt;: Shortest paths in terms of robotics have a lot to do with physical distances, but it could also relate the completing a task efficiently.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These notes will cover classical single-source shortest path algorithms, but first we must formally define the problem.&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;Given a weighted, directed graph \(G = (V, E)\) with weight function \(w: E \rightarrow \mathbb{R}\), a source vertex \(s \in V\), and a destination vertex \(t \in V\), find the shortest path from \(s\) to \(t\). The weight of a path is defined as the sum of the weights of its edges:&lt;/p&gt;
&lt;p&gt;\[
w(p) = \sum_{e \in p} w(e).
\]&lt;/p&gt;
&lt;p&gt;The shortest-path weight between two vertices \(u\) and \(v\) is given by&lt;/p&gt;
&lt;p&gt;\[
\delta(u, v) = \begin{cases}
\min_{p \in P(u, v)} w(p) &amp;amp; \text{if } P(u, v) \neq \emptyset \\
\infty &amp;amp; \text{otherwise}
\end{cases}
\]&lt;/p&gt;
&lt;p&gt;where \(P(u, v)\) is the set of all paths from \(u\) to \(v\). The shortest-path weight from \(s\) to \(t\) is given by \(\delta(s, t)\).&lt;/p&gt;
&lt;p&gt;The output of a shortest-path algorithm will produce, for each vertex \(v \in V\):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(v.d\): The shortest-path estimate from \(s\) to \(v\).&lt;/li&gt;
&lt;li&gt;\(v.\pi\): The predecessor of \(v\) in the shortest path from \(s\) to \(v\).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Shortest-path algorithms rely on an optimal substructure property that is defined by Lemma 22.1 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 22.1&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Given a weighted, directed graph \(G = (V,E)\) with weight function \(w: E \rightarrow \mathbb{R}\), let \(p = \langle v_0, v_1, \dots, v_k \rangle\) be a shortest path from vertex \(v_0\) to vertex \(v_k\). For any \(i\) and \(j\) such that \(0 \leq i \leq j \leq k\), let \(p_{ij} = \langle v_i, v_{i+1}, \dots, v_j \rangle\) be the subpath of \(p\) from vertex \(v_i\) to vertex \(v_j\). Then, \(p_{ij}\) is a shortest path from \(v_i\) to \(v_j\).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is also important to note here that a shortest path should contain no cycles. Some shortest-path algorithms require that the edge weights be strictly positive. For those that do not, they may have some mechanism for detecting negative-weight cycles. In any case, a cycle of any kind cannot be included in a shortest path. This is because if a cycle were included, we could simply traverse the cycle as many times as we wanted to reduce the weight of the path. For positive-weight cycles, if a shortest path included a cycle, then surely we could remove the cycle to get a lower weight.&lt;/p&gt;
&lt;p&gt;As we build a shortest path, we need to keep track of which vertices lead us from the source to the destination. Some algorithms maintain this by keeping a &lt;strong&gt;predecessor&lt;/strong&gt; attribute for each vertex in the path. Solutions such as the Viterbi algorithm keep an array of indices that correspond to the vertices in the path. In any case, we will need to keep track of the vertices in the path as we build it.&lt;/p&gt;
&lt;h3 id=&#34;relaxation&#34;&gt;Relaxation&lt;/h3&gt;
&lt;p&gt;There is one more important property to define before discussing specific algorithms: &lt;strong&gt;relaxation&lt;/strong&gt;. Relaxing an edge \((u, v)\) is to test whether going through vertex \(u\) improves the shortest path to \(v\). If so, we update the shortest-path estimate and predecessor of \(v\) to reflect the new shortest path. Relaxation requires that we maintain the shortest-path estimate and processor for each vertex. This is initialized as follows.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;initialize_single_source&lt;/span&gt;(G, s):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;V:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;inf&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    s&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;When the values are changed, we say that the vertex has been &lt;strong&gt;relaxed&lt;/strong&gt;. Relaxing an edge \((u, v)\) is done as follows.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;relax&lt;/span&gt;(u, v, w):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; w(u, v):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; w(u, v)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; u
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;properties&#34;&gt;Properties&lt;/h4&gt;
&lt;p&gt;Relaxation has the following properties.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If the shortest-path estimate of a vertex is not \(\infty\), then it is always an upper bound on the weight of a shortest path from the source to that vertex.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The shortest-path estimate of a vertex will either stay the same or decrease as the algorithm progresses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once a vertex&amp;rsquo;s shortest-path estimate is finalized, it will never change.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The shortest-path estimate of a vertex is always greater than or equal to the actual shortest-path weight.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After \(i\) iterations of relaxing on all \((u, v)\), if the shortest path to \(v\) has \(i\) edges, then \(v.d = \delta(s, v)\).&lt;/p&gt;
&lt;p&gt;Following &lt;em&gt;Introduction to Algorithms&lt;/em&gt;, we will first discuss the Bellman-Ford algorithm, which has a higher runtime but works with graphs that have negative edge weights. Then, we will discuss Dijkstra&amp;rsquo;s algorithm, which has a lower runtime but only works with graphs that have non-negative edge weights.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;bellman-ford&#34;&gt;Bellman-Ford&lt;/h2&gt;
&lt;p&gt;The Bellman-Ford algorithm is a dynamic programming algorithm that solves the single-source shortest-paths problem in the general case in which edge weights may be negative. If a negative-weight cycle is reachable from the source, then the algorithm will report its existence. Otherwise, it will report the shortest-path weights and predecessors. It works by relaxing edges, decreasing the shortest-path estimate on the weight of a shortest path from \(s\) to each vertex \(v\) until it reaches the shortest-path weight.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bellman_ford&lt;/span&gt;(G, w, s):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    initialize_single_source(G, s)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, len(G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;V)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (u, v) &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;E:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            relax(u, v, w)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (u, v) &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;E:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; w(u, v):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;In the figure below, graph (a) shows the original graph before iterating over the edges. Graphs (b)-(e) show the result of looping over both edges originating from \(s\). Depending on the implementation, the first iteration of the vertices would result directly in graph (c). You can find a Python implementation of this example &lt;a href=&#34;https://github.com/ajdillhoff/python-examples/blob/main/data_structures/graphs/bellman_ford_algorithm.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-24_21-05-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Step-by-step execution of Bellman-Ford on a graph with negative-weight edges (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Step-by-step execution of Bellman-Ford on a graph with negative-weight edges (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;correctness&#34;&gt;Correctness&lt;/h3&gt;
&lt;p&gt;Bellman-Ford is guaranteed to converge after \(|V| - 1\) iterations, assuming no negative-weight cycles.&lt;/p&gt;
&lt;h4 id=&#34;proof&#34;&gt;Proof&lt;/h4&gt;
&lt;p&gt;The first iteration relaxes \((v_0, v_1)\). The second iteration relaxes \((v_1, v_2)\), and so on. The &lt;strong&gt;path-relaxation&lt;/strong&gt; property from before implies that \(v.d = v_k.d = \delta(s, v_k) = \delta(s, v)\). If there is a negative-weight cycle, then the shortest path to \(v_k\) is not well-defined. This is verified in the final loop over the edges.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (u, v) &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;E:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; w(u, v):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If there exists a negative-weight cycle \(c = \langle v_0, v_1, \dots, v_k \rangle\), where \(v_0 = v_k\) that can be reached from \(s\), then&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^{k} w(v_{i-1}, v_i) &amp;lt; 0.
\]&lt;/p&gt;
&lt;p&gt;To complete the proof by contradiction, assume that Bellman-Ford returns &lt;code&gt;True&lt;/code&gt;. Then we would have that \(v_i.d \leq v_{i-1}.d + w(v_{i-1}, v_i)\) for \(i = 1, 2, \dots, k\) by the &lt;strong&gt;triangle inequality&lt;/strong&gt; property. If we sum around the cycle, we get&lt;/p&gt;
&lt;p&gt;\begin{align*}
\sum_{i=1}^k v_i.d &amp;amp;\leq \sum_{i=1}^k (v_{i-1}.d + w(v_{i-1}, v_i))\\
&amp;amp;= \sum_{i=1}^k v_{i-1}.d + \sum_{i=1}^k w(v_{i-1}, v_i)\\
\end{align*}&lt;/p&gt;
&lt;p&gt;Since the vertices are in a cycle, each vertex appears only once in each summation \(\sum_{i=1}^k v_{i}.d\) and \(\sum_{i=1}^k v_{i-1}.d\). Subtracting this from both sides of the inequality, we get&lt;/p&gt;
&lt;p&gt;\[
0 \leq \sum_{i=1}^k w(v_{i-1}, v_i).
\]&lt;/p&gt;
&lt;p&gt;This contradicts the assumption that there is a negative-weight cycle. Therefore, if Bellman-Ford returns &lt;code&gt;True&lt;/code&gt;, then there are no negative-weight cycles.&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;Using an adjacency list representation, the runtime of Bellman-Ford is \(O(V^2 + VE)\). The initialization takes \(\Theta(V)\). Each of the \(|V| - 1\) iterations over the edges takes \(\Theta(V + E)\), and the final check for negative-weight cycles takes \(\Theta(V + E)\). If the number of edges and vertices is such that the number of vertices are a lower bound on the edges, then the runtime is \(O(VE)\).&lt;/p&gt;
&lt;h3 id=&#34;example-22-dot-1-1&#34;&gt;Example 22.1-1&lt;/h3&gt;
&lt;p&gt;Run Bellman-Ford on the given path using \(z\) as the source. Then change the weight of \((z, x)\) to 4 and run it again with \(s\) as the source.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-22_11-13-14_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Figure 22.4 from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Figure 22.4 from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;shortest-paths-on-a-dag&#34;&gt;Shortest Paths on a DAG&lt;/h2&gt;
&lt;p&gt;If we are given a directed acyclic graph (DAG), we can solve the single-source shortest path problem in \(O(V + E)\) time. By definition, the graph has no cycles and thus no negative-weight cycles.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dag_shortest_paths&lt;/span&gt;(G, w, s):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    initialize_single_source(G, s)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; u &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; topological_sort(G):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;adj[u]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            relax(u, v, w)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Run &lt;code&gt;dag_shortest_paths&lt;/code&gt; on the graph given below with \(s\) as the source.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-22_11-23-05_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Figure 22.5 from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Figure 22.5 from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;The runtime of &lt;code&gt;dag_shortest_paths&lt;/code&gt; is \(O(V + E)\), where \(V\) is the number of vertices and \(E\) is the number of edges. The topological sort takes \(O(V + E)\) time. Initializing the vertices takes \(O(V)\) time. The first &lt;code&gt;for&lt;/code&gt; loop makes on iteration per vertex, and the inner loop relaxes each edge only once.&lt;/p&gt;
&lt;h2 id=&#34;dijkstra-s-algorithm&#34;&gt;Dijkstra&amp;rsquo;s Algorithm&lt;/h2&gt;
&lt;p&gt;Dijkstra&amp;rsquo;s algorithm also solves the single-source shortest path problem on a weighted, directed graph \(G = (V,E)\) but requires nonnegative weights on all edges. It works in a breadth-first manner. A minimum priority queue is utilized to keep track of the vertices that have not been visited based on their current minimum shortest-path estimate. The algorithm works by relaxing edges, decreasing the shortest-path estimate on the weight of a shortest path from \(s\) to each vertex \(v\) until it reaches the shortest-path weight.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dijkstra&lt;/span&gt;(G, w, s):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    initialize_single_source(G, s)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    S &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;V
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; Q:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        u &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; extract_min(Q)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        S&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(u)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;adj[u]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            prev_d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            relax(u, v, w)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; prev_d:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                decrease_key(Q, v)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;A Python example of the figure below is available &lt;a href=&#34;https://github.com/ajdillhoff/python-examples/blob/main/data_structures/graphs/dijkstras_algorithm.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-25_08-21-04_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;A step-by-step execution of Dijkstra&amp;#39;s algorithm on a graph with non-negative edge weights (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;A step-by-step execution of Dijkstra&amp;rsquo;s algorithm on a graph with non-negative edge weights (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;See Chapter 22 of &lt;em&gt;Introduction to Algorithms&lt;/em&gt; for a detailed analysis of Dijkstra&amp;rsquo;s algorithm. Inserting the nodes and then extracting them from the queue yields \(O(V \log V)\). After extracting a node, its edges are iterated with a possible update to the queue. This takes \(O(E \log V)\). The total runtime is \(O((V + E) \log V)\).&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Graph Theory</title>
      <link>https://ajdillhoff.github.io/notes/introduction_to_graph_theory/</link>
      <pubDate>Tue, 17 Oct 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/introduction_to_graph_theory/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-are-graphs&#34;&gt;What are Graphs?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#graph-traversal-algorithms&#34;&gt;Graph Traversal Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#breadth-first-search&#34;&gt;Breadth First Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#depth-first-search&#34;&gt;Depth First Search&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;what-are-graphs&#34;&gt;What are Graphs?&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;graph&lt;/strong&gt; is a data structure that is used to represent pairwise relationships between objects. Graphs are used in many applications, such as social networks, maps, and routing algorithms. These notes accompany the series of lectures on graphs for my &lt;em&gt;Foundations of Computing&lt;/em&gt; course at the University of Texas - Arlington.&lt;/p&gt;
&lt;h3 id=&#34;definitions&#34;&gt;Definitions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;directed graph&lt;/strong&gt; \(G\) is represented as a pair \((V, E)\) of a set of vertices \(V\) and edges \(E\). Edges are represented as ordered pairs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An &lt;strong&gt;undirected graph&lt;/strong&gt; \(G\) is represented as a pair \((V, E)\) of a set of vertices \(V\) and edges \(E\). The edges are represented as unordered pairs, as it does not matter which direction the edge is going.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Let \((u, v)\) be an edge in a graph \(G\). If \(G\) is a directed graph, then the edge is &lt;strong&gt;incident from&lt;/strong&gt; \(u\) and is &lt;strong&gt;incident to&lt;/strong&gt; \(v\). In this case, \(v\) is also &lt;strong&gt;adjacent&lt;/strong&gt; to \(u\). If \(G\) is an undirected graph, then the edge is &lt;strong&gt;incident on&lt;/strong&gt; \(u\) and \(v\). For undirected graphs, the &lt;strong&gt;adjacency&lt;/strong&gt; relation is symmetric.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;degree&lt;/strong&gt; is a graph is the number of edges incident on a vertex. For directed graphs, the &lt;strong&gt;in-degree&lt;/strong&gt; is the number of edges incident to a vertex, and the &lt;strong&gt;out-degree&lt;/strong&gt; is the number of edges incident from a vertex.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;path&lt;/strong&gt; from a vertex \(u\) to another vertex \(v\) is a sequence of edges that starts at \(u\) and ends at \(v\). This definition can include duplicates. A &lt;strong&gt;simple path&lt;/strong&gt; is a path that does not repeat any vertices. A &lt;strong&gt;cycle&lt;/strong&gt; is a path that starts and ends at the same vertex. If a path exists from \(u\) to \(v\), then \(u\) is &lt;strong&gt;reachable&lt;/strong&gt; from \(v\).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;connected graph&lt;/strong&gt; is a graph where there is a path between every pair of vertices. A &lt;strong&gt;strongly connected graph&lt;/strong&gt; is a directed graph where there is a path between every pair of vertices. The &lt;strong&gt;connected components&lt;/strong&gt; of a graph are the subgraphs in which each pair of nodes is connected by a path. In image processing, connected-component labeling is used to find regions of connected pixels in a binary image.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Let \(G = (V, E)\) and \(G&amp;rsquo; = (V&amp;rsquo;, E&amp;rsquo;)\). \(G\) and \(G&amp;rsquo;\) are &lt;strong&gt;isomorphic&lt;/strong&gt; if there is a bijection between their vertices such that \((u, v) \in E\) if and only if \((f(u), f(v)) \in E&amp;rsquo;\).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;complete graph&lt;/strong&gt; is an undirected graph in which every pair of vertices is adjacent. A &lt;strong&gt;bipartite graph&lt;/strong&gt; is an undirected graph in which the vertices can be partitioned into two sets such that every edge connects a vertex in one set to a vertex in the other set.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;multi-graph&lt;/strong&gt; is a graph that allows multiple edges between the same pair of vertices. These are commonly in social network analysis, where multiple edges between two people can represent different types of relationships.&lt;/p&gt;
&lt;p&gt;TODO: Add figures demonstrating the above definitions&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;representations&#34;&gt;Representations&lt;/h3&gt;
&lt;p&gt;Graphs can be represented in many different ways. The most common representations are adjacency lists and adjacency matrices. Adjacency lists are more space-efficient for sparse graphs, while adjacency matrices are more space-efficient for dense graphs. Adjacency lists are also more efficient for finding the neighbors of a vertex, while adjacency matrices are more efficient for checking if an edge exists between two vertices.&lt;/p&gt;
&lt;h4 id=&#34;example-adjacency-matrix-and-reachability&#34;&gt;Example: Adjacency Matrix and Reachability&lt;/h4&gt;
&lt;p&gt;Consider the graph in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-17_21-08-29_directed_graph.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;A directed graph&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;A directed graph
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The adjacency matrix for this graph is:&lt;/p&gt;
&lt;p&gt;\begin{bmatrix}
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
\end{bmatrix}&lt;/p&gt;
&lt;p&gt;The rows and columns represent the vertices in the graph. The value at row \(i\) and column \(j\) is 1 if there is an edge from vertex \(i\) to vertex \(j\). Otherwise, the value is 0. Let \(A\) be the adjacency matrix for a graph \(G\). The matrix \(A^k\) represents the number of paths of length \(k\) between each pair of vertices. For example, \(A^2\) for the above graph is:&lt;/p&gt;
&lt;p&gt;\begin{bmatrix}
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
\end{bmatrix}&lt;/p&gt;
&lt;p&gt;The value at row \(i\) and column \(j\) is the number of paths of length 2 from vertex \(i\) to vertex \(j\). For example, is a path from vertex 0 to vertex 6 via 0 -&amp;gt; 3 -&amp;gt; 6.&lt;/p&gt;
&lt;h2 id=&#34;graph-traversal-algorithms&#34;&gt;Graph Traversal Algorithms&lt;/h2&gt;
&lt;p&gt;Graph traversal algorithms are used to explore the structure of a graph. You might initially find this a useless endeavour. If we have defined our own graph, what about it would we need to explore? In scenarios with highly complex and large datasets, the search takes on slightly different meanings. Perhaps we are searching for specific trends based on a range of values such as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Six_Degrees_of_Kevin_Bacon&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Bacon number&lt;/a&gt; of a particular celebrity.&lt;/p&gt;
&lt;h2 id=&#34;breadth-first-search&#34;&gt;Breadth First Search&lt;/h2&gt;
&lt;p&gt;Breadth First search explores the graph broadly, ensuring one level has been exhausted before moving onto the next. We previously studied breadth-first search in the context of binary search trees. The algorithm is the same when applied on general graphs, but our perspective is slightly different now. The function studied before did not use node coloring. Let&amp;rsquo;s investigate the algorithm given by Cormen et al. in &lt;em&gt;Introduction to Algorithms&lt;/em&gt; (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The algorithm adds a color to each node to keep track of its state. The colors are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;WHITE: The node has not been discovered yet.&lt;/li&gt;
&lt;li&gt;GRAY: The node has been discovered, but not all of its neighbors have been discovered.&lt;/li&gt;
&lt;li&gt;BLACK: The node has been discovered, and all of its neighbors have been discovered.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, every vertex is painted white and the distance is set to \(\infty\). The first node &lt;code&gt;s&lt;/code&gt; is immediately set to have 0 distance. The queue then starts with &lt;code&gt;s&lt;/code&gt;. While there are any grey vertices, dequeue the next available node and add its adjacent vertices to the queue. The distance of each adjacent vertex is set to the distance of the current vertex plus one. Once all of its neighbors have been discovered, the current vertex is painted black.&lt;/p&gt;
&lt;p&gt;The algorithm and an example run from Cormen et al. are shown below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bfs&lt;/span&gt;(G, s):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; u &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;V:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; WHITE
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; inf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    s&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; GRAY
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    s&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    s&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Queue()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Q&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;enqueue(s)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; Q&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;empty():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        u &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Q&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dequeue()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;adj[u]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; WHITE:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; GRAY
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; u
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                Q&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;enqueue(v)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;





&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-17_20-11-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Breadth First Search from Cormen et al.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Breadth First Search from Cormen et al.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;The running time of BFS is \(O(V + E)\), where \(V\) is the number of vertices and \(E\) is the number of edges. Each vertex is queued and dequeued once, so the queue operations take \(O(V)\) time. Each edge is examined once, so the edge operations take \(O(E)\) time. The total running time is \(O(V + E)\).&lt;/p&gt;
&lt;h3 id=&#34;breath-first-trees&#34;&gt;Breath-first Trees&lt;/h3&gt;
&lt;p&gt;The blue lines in the previous example depict a &lt;strong&gt;breadth-first tree&lt;/strong&gt; which was built by BFS. The tree is defined by the \(\pi\) values updated throughout the course of the algorithm. These can also be used to reconstruct the shortest path from \(s\) to any other vertex \(v\).&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;predecessor subgraph&lt;/strong&gt; is a graph \(G_{\pi} = (V_{\pi}, E_{\pi})\), where&lt;/p&gt;
&lt;p&gt;\begin{align*}
V_{\pi} &amp;amp;= \{v \in V : v.\pi \neq \text{None}\} \cup \{s\} \text{ and}\\
E_{\pi} &amp;amp;= \{(v.\pi, v) : v \in V_{\pi} - \{s\}\}.
\end{align*}&lt;/p&gt;
&lt;p&gt;Such a graph is a breadth-first tree if \(V_{\pi}\) consists of the vertices reachable from \(s\) and, for all \(v \in V_{\pi}\), the subgraph \(G_{\pi}\) contains a unique simple path from \(s\) to \(v\) that is also a shortest path from \(s\) to \(v\) in \(G\).&lt;/p&gt;
&lt;p&gt;To print the vertices on a shortest path from \(s\) to \(v\):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;print_path&lt;/span&gt;(G, s, v):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; s:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(s)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pi &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;No path from&amp;#34;&lt;/span&gt;, s, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;to&amp;#34;&lt;/span&gt;, v, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;exists.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print_path(G, s, v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pi)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(v)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;exercise&#34;&gt;Exercise&lt;/h3&gt;
&lt;p&gt;Run BFS on the following graph, starting from \(s\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-16_16-08-11_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Graph for BFS exercise from Cormen et al.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Graph for BFS exercise from Cormen et al.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;depth-first-search&#34;&gt;Depth First Search&lt;/h2&gt;
&lt;p&gt;Like the BFS algorithm presented in &lt;em&gt;Introduction to Algorithms&lt;/em&gt; by Cormen et al., the DFS algorithm also uses colors to keep track of the state of each node. The colors are similar to the BFS algorithm, but the meaning is slightly different:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;WHITE: The node has not been discovered yet.&lt;/li&gt;
&lt;li&gt;GRAY: The node has been visited for the first time.&lt;/li&gt;
&lt;li&gt;BLACK: The adjacency list of the node has been examined completely.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, all vertices are colored white. The time is set to 0. The function &lt;code&gt;dfs_visit&lt;/code&gt; is called on each vertex. The function is defined as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dfs&lt;/span&gt;(G):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; u &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;V:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; WHITE
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    time &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; u &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;V:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; WHITE:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            dfs_visit(G, u)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dfs_visit&lt;/span&gt;(G, u):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    time &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; time
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; GRAY
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;adj[u]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; WHITE:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; u
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            dfs_visit(G, v)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    time &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; time
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;When a node is discovered via &lt;code&gt;dfs_visit&lt;/code&gt;, the time is recorded and the color is changed to gray. The start and finish times are useful in understanding the structure of the graph. After all of the node&amp;rsquo;s neighbors have been discovered, the color is changed to black and the finish time is recorded. That is, the depth from the current node must be fully explored before it is considered finished. The figure below shows the progress of DFS on a directed graph.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-17_20-25-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Depth First Search from Cormen et al.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Depth First Search from Cormen et al.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;Each vertex is added and removed to the queue once. Since these operations are performed in \(O(1)\) time, the total time for these operations in \(O(V)\). When a vertex is dequeued, its adjacency list is scanned. The total number of entries in all adjacency lists is equal to the number of edges, so the time spent scanning these lists of \(O(V + E)\). In summary, the operation is linear in terms of the adjacency-list representation.&lt;/p&gt;
&lt;h3 id=&#34;properties-of-dfs&#34;&gt;Properties of DFS&lt;/h3&gt;
&lt;p&gt;The predecessor subgraph \(G_{\pi}\) is a forest of trees. That is, it creates a collection of depth first trees.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;What is a forest?&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How does DFS generate a forest?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A vertex \(u = v.{\pi}\) if and only if \(\text{DFS-VISIT}(G, v)\) was called during a search of \(u\)&amp;rsquo;s adjacency list. Vertex \(v\) is a descendant of vertex \(u\) in the depth-first forest if and only if \(v\) is discovered during the time in which \(u\) is gray (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Essentially, each call to &lt;code&gt;dfs_visit&lt;/code&gt; from &lt;code&gt;dfs&lt;/code&gt; finds a new tree. Let&amp;rsquo;s consider this on the example above.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exercise: Draw the DFS forest using the graph above.&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;parenthesis-theorem&#34;&gt;Parenthesis Theorem&lt;/h4&gt;
&lt;p&gt;In a DFS, the discovery and finish times have &lt;strong&gt;parenthesis structure&lt;/strong&gt;. For all \(u, v\), exactly only of the following holds:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the intervals \([u.d, u.f]\) and \([v.d, v.f]\) are entirely disjoint and neither \(u\) nor \(v\) is a descendant of the other in the depth-first forest.&lt;/li&gt;
&lt;li&gt;the interval \([u.d, u.f]\) is entirely contained within the interval \([v.d, v.f]\) and \(u\) is a descendant of \(v\) in the depth-first forest.&lt;/li&gt;
&lt;li&gt;the interval \([v.d, v.f]\) is entirely contained within the interval \([u.d, u.f]\) and \(v\) is a descendant of \(u\) in the depth-first forest.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It is called the &lt;strong&gt;parenthesis theorem&lt;/strong&gt; because if &lt;code&gt;dfs_visit&lt;/code&gt; printed &amp;ldquo;\((u\)&amp;rdquo; when it first encountered \(u\) and printed &amp;ldquo;\(u)\)&amp;rdquo; when it finished, the expression would be well formed.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-16_17-14-36_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Example of parenthesis structure from Cormen et al.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Example of parenthesis structure from Cormen et al.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;white-path-theorem&#34;&gt;White-path Theorem&lt;/h4&gt;
&lt;p&gt;In a depth-first forest of a graph \(G\), vertex \(v\) is a descendant of vertex \(u\) if and only if at the time \(u.d\) that &lt;code&gt;dfs_visit&lt;/code&gt; is called on \(u\), there is a path of white vertices from \(u\) to \(v\) in \(G\).&lt;/p&gt;
&lt;h4 id=&#34;more-on-dfs-forests&#34;&gt;More on DFS Forests&lt;/h4&gt;
&lt;p&gt;Consider the result of DFS on the graph below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-16_17-30-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;DFS forest from Cormen et al.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;DFS forest from Cormen et al.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The edges are labeled as either&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;tree edges&lt;/strong&gt;: edges in the depth-first forest.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;back edges&lt;/strong&gt;: edges that point from a vertex to an ancestor in the depth-first forest.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;forward edges&lt;/strong&gt;: edges that point from a vertex to a descendant in the depth-first forest.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The above graph can be visualized as a DFS forest, as shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-16_17-33-21_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;DFS forest from Cormen et al.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;DFS forest from Cormen et al.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Red-Black Trees</title>
      <link>https://ajdillhoff.github.io/notes/red_black_trees/</link>
      <pubDate>Sun, 15 Oct 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/red_black_trees/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#operations&#34;&gt;Operations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exercises&#34;&gt;Exercises&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Red-Black Trees are modified &lt;a href=&#34;https://ajdillhoff.github.io/notes/binary_search_trees/&#34;&gt;Binary Search Trees&lt;/a&gt; that maintain a balanced structure in order to guarantee that operations like search, insert, and delete run in \(O(\log n)\) time.&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;A red-black tree is a binary search tree with the following properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Every node is either red or black.&lt;/li&gt;
&lt;li&gt;The root is black.&lt;/li&gt;
&lt;li&gt;Every &lt;code&gt;NULL&lt;/code&gt; leaf is black.&lt;/li&gt;
&lt;li&gt;If a node is red, then both its children are black.&lt;/li&gt;
&lt;li&gt;For each node, all simple paths from the node to descendant leaves contain the same number of black nodes.&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-15_11-50-31_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Red-Black Tree from CLRS Chapter 13.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Red-Black Tree from CLRS Chapter 13.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The only structural addition we need to make over a BST is the addition of a &lt;code&gt;color&lt;/code&gt; attribute to each node. This attribute can be either &lt;code&gt;RED&lt;/code&gt; or &lt;code&gt;BLACK&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Property 5 implies that the &lt;em&gt;black-height&lt;/em&gt; of a tree is an important property. This property is used to prove that the height of a red-black tree with \(n\) internal nodes is at most \(2 \log(n + 1)\).&lt;/p&gt;
&lt;h2 id=&#34;operations&#34;&gt;Operations&lt;/h2&gt;
&lt;h3 id=&#34;rotate&#34;&gt;Rotate&lt;/h3&gt;
&lt;p&gt;If a &lt;Binary Search Tree&gt; is balanced, then searching for a node takes \(O(\log n)\) time. However, if the tree is unbalanced, then searching can take \(O(n)\) time. When items are inserted or deleted from a tree, it can become unbalanced. Without any way to correct for this, a BST is less desirable unless the data will not change.&lt;/p&gt;
&lt;p&gt;When nodes are inserted or deleted into a red-black tree, the &lt;strong&gt;&lt;strong&gt;rotation&lt;/strong&gt;&lt;/strong&gt; operation is used in functions that maintain the red-black properties. This ensures that the tree remains balanced and that operations like search, insert, and delete run in \(O(\log n)\) time. The figure below shows the two types of rotations that can be performed on a red-black tree.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-15_17-28-19_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Rotations in a red-black tree (CLRS Figure 13.2).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Rotations in a red-black tree (CLRS Figure 13.2).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Python implementations of both left and right rotations are given below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;left_rotate&lt;/span&gt;(self, x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nil:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nil:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;root &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;right_rotate&lt;/span&gt;(self, y):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nil:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nil:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;root &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Cormen et al. Figure 13.3 (below) shows the result of performing a left rotation on node \(x\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-15_18-07-53_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Left rotation on node (x) (CLRS Figure 13.3).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Left rotation on node (x) (CLRS Figure 13.3).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;A rotation only changes pointer assignments, so it takes \(O(1)\) time.&lt;/p&gt;
&lt;h3 id=&#34;insert&#34;&gt;Insert&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;insert&lt;/code&gt; operation in a red-black tree starts off identically to the &lt;code&gt;insert&lt;/code&gt; operation in a BST. The new node is inserted into the tree as a leaf node. Since the &lt;code&gt;NULL&lt;/code&gt; leaf nodes must be black by definition, the added node is colored red. The function in Python is shown below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;insert&lt;/span&gt;(self, z):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nil
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;root
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nil:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nil:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;root &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nil
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nil
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RED
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;insert_fixup(z)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;By adding the node and setting its color to red, we have possibly violated properties 2 and 4. Property 2 is violated if &lt;code&gt;z&lt;/code&gt; is the root. Property 4 is violated if the parent of the new node is also red. The final line of the function calls &lt;code&gt;insert_fixup&lt;/code&gt; to restore the red-black properties. It is defined as follows.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;insert_fixup&lt;/span&gt;(self, z):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; RED:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; RED:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RED
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                z &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; z &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    z &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left_rotate(z)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RED
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right_rotate(z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; RED:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RED
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                z &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; z &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    z &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right_rotate(z)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RED
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left_rotate(z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;root&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;case-1&#34;&gt;Case 1&lt;/h4&gt;
&lt;p&gt;Inside the &lt;code&gt;while&lt;/code&gt; loop, the first and second conditions are symmetric. One considers the case where &lt;code&gt;z&lt;/code&gt;&amp;rsquo;s parent is a left child, and the other considers the case where &lt;code&gt;z&lt;/code&gt;&amp;rsquo;s parent is a right child. Further, if &lt;code&gt;z&lt;/code&gt;&amp;rsquo;s parent is a left child, then we start by setting &lt;code&gt;y&lt;/code&gt; to &lt;code&gt;z&lt;/code&gt;&amp;rsquo;s &lt;em&gt;aunt&lt;/em&gt;. Let&amp;rsquo;s investigate the first &lt;code&gt;if&lt;/code&gt; statement, where &lt;code&gt;y&lt;/code&gt; is &lt;code&gt;RED&lt;/code&gt;. In this case, both &lt;code&gt;z&lt;/code&gt;&amp;rsquo;s parent and aunt are &lt;code&gt;RED&lt;/code&gt;. We can fix this by setting both to &lt;code&gt;BLACK&lt;/code&gt; and setting &lt;code&gt;z&lt;/code&gt;&amp;rsquo;s grandparent to &lt;code&gt;RED&lt;/code&gt;. This may violate property 2, so we set &lt;code&gt;z&lt;/code&gt; to its grandparent and repeat the loop.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; RED:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RED
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    z &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;case-2&#34;&gt;Case 2&lt;/h4&gt;
&lt;p&gt;If &lt;code&gt;y&lt;/code&gt; is &lt;code&gt;BLACK&lt;/code&gt;, then we need to consider the case where &lt;code&gt;z&lt;/code&gt; is a right child. In this case, we set &lt;code&gt;z&lt;/code&gt; to its parent and perform a left rotation. This automatically results in the third case, where &lt;code&gt;z&lt;/code&gt; is a left child.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; z &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    z &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left_rotate(z)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;case-3&#34;&gt;Case 3&lt;/h4&gt;
&lt;p&gt;If &lt;code&gt;z&lt;/code&gt; is a left child, then we set &lt;code&gt;z&lt;/code&gt;&amp;rsquo;s parent to &lt;code&gt;BLACK&lt;/code&gt; and its grandparent to &lt;code&gt;RED&lt;/code&gt;. Then we perform a right rotation on the grandparent.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RED
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right_rotate(z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Figure 13.4 from Cormen et al. demonstrates the addition of a node to a red-black tree. The node is inserted as a leaf node and colored red. Then &lt;code&gt;insert_fixup&lt;/code&gt; is called to restore the red-black properties.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-15_20-41-14_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Inserting a node into a red-black tree (CLRS Figure 13.4).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Inserting a node into a red-black tree (CLRS Figure 13.4).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The &lt;code&gt;insert&lt;/code&gt; operation takes \(O(\log n)\) time since it performs a constant number of rotations.&lt;/p&gt;
&lt;h3 id=&#34;delete&#34;&gt;Delete&lt;/h3&gt;
&lt;p&gt;Like the &lt;code&gt;delete&lt;/code&gt; operation of a BST, the &lt;code&gt;delete&lt;/code&gt; operation of a RBT uses a &lt;code&gt;transplant&lt;/code&gt; operation to replace the deleted node with its child. The &lt;code&gt;transplant&lt;/code&gt; operation is defined as follows.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;transplant&lt;/span&gt;(self, u, v):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nil:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;root &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; u &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The full &lt;code&gt;delete&lt;/code&gt; operation follows a similar structure to that of its BST counterpart. There are a few distinct differences based on the color of the node being deleted. The function begins as follows.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;delete&lt;/span&gt;(self, z):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y_original_color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first line sets &lt;code&gt;y&lt;/code&gt; to the node to be deleted. The second line saves the color of &lt;code&gt;y&lt;/code&gt;. This is necessary because &lt;code&gt;y&lt;/code&gt; will be replaced by another node, and we need to know the color of the replacement node. The first two conditionals check if &lt;code&gt;z&lt;/code&gt; has any children. If there is right child, then the &lt;code&gt;z&lt;/code&gt; is replaced by the left child. If there is a left child, then &lt;code&gt;z&lt;/code&gt; is replaced by the right child. If &lt;code&gt;z&lt;/code&gt; has no children, then &lt;code&gt;z&lt;/code&gt; is replaced by &lt;code&gt;NULL&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transplant(z, z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transplant(z, z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If &lt;code&gt;z&lt;/code&gt; has two children, then we find the successor of &lt;code&gt;z&lt;/code&gt; and set &lt;code&gt;y&lt;/code&gt; to it. The successor is the node with the smallest key in the right subtree of &lt;code&gt;z&lt;/code&gt;. The successor is guaranteed to have at most one child, so we can use the code above to replace &lt;code&gt;y&lt;/code&gt; with its child. Then we replace &lt;code&gt;z&lt;/code&gt; with &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;minimum(z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y_original_color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right: &lt;span style=&#34;color:#75715e&#34;&gt;# y is farther down the tree&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transplant(y, y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transplant(z, y)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The procedure kept track of &lt;code&gt;y_original_color&lt;/code&gt; to see if any violations occurred. This would happen if &lt;code&gt;y&lt;/code&gt; was originally &lt;code&gt;BLACK&lt;/code&gt; because the &lt;code&gt;transplant&lt;/code&gt; operation, or the deletion itself, could have violated the red-black properties. If &lt;code&gt;y_original_color&lt;/code&gt; is &lt;code&gt;BLACK&lt;/code&gt;, then we call &lt;code&gt;delete_fixup&lt;/code&gt; to restore the properties.&lt;/p&gt;
&lt;h3 id=&#34;delete-fixup&#34;&gt;Delete Fixup&lt;/h3&gt;
&lt;p&gt;If the node being deleted is &lt;code&gt;BLACK&lt;/code&gt;, then the following scenarios can occur. If &lt;code&gt;y&lt;/code&gt; is the root and a &lt;code&gt;RED&lt;/code&gt; child of &lt;code&gt;y&lt;/code&gt; becomes the new root, property 2 is violated. Let &lt;code&gt;x&lt;/code&gt; be a &lt;code&gt;RED&lt;/code&gt; child of &lt;code&gt;y&lt;/code&gt;, if a new parent of &lt;code&gt;x&lt;/code&gt; is &lt;code&gt;RED&lt;/code&gt;, then property 4 is violated. Lastly, removing &lt;code&gt;y&lt;/code&gt; may have caused a violation of property 5, since any path containing &lt;code&gt;y&lt;/code&gt; has 1 less &lt;code&gt;BLACK&lt;/code&gt; node in it.&lt;/p&gt;
&lt;p&gt;Correcting violation 5 can be done by &lt;em&gt;transferring&lt;/em&gt; the &lt;code&gt;BLACK&lt;/code&gt; property from &lt;code&gt;y&lt;/code&gt; to &lt;code&gt;x&lt;/code&gt;, the node that moves into &lt;code&gt;y&lt;/code&gt;&amp;rsquo;s original position. This requires us to allow nodes to take on multiple counts of colors. That is, if &lt;code&gt;x&lt;/code&gt; was already &lt;code&gt;BLACK&lt;/code&gt;, it becomes double &lt;code&gt;BLACK&lt;/code&gt;. If it was &lt;code&gt;RED&lt;/code&gt;, it becomes &lt;code&gt;RED-AND-BLACK&lt;/code&gt;. There is a good reason to this extension, as it will help us decide which case of &lt;code&gt;delete_fixup&lt;/code&gt; to use.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;delete_fixup&lt;/code&gt; function will restore violations of properties 1, 2, and 4. It is called after the &lt;code&gt;delete&lt;/code&gt; operation, and it takes a single argument, &lt;code&gt;x&lt;/code&gt;, which is the node that replaced the deleted node. It performs a series of rotations and color changes to restore the violated properties.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s look at the &lt;code&gt;delete_fixup&lt;/code&gt; function from the ground up. It is a little more complex than &lt;code&gt;insert_fixup&lt;/code&gt; because it has to handle the case where the node being deleted is &lt;code&gt;BLACK&lt;/code&gt;. In total, there are 4 distinct cases per side. Like &lt;code&gt;insert_fixup&lt;/code&gt;, it is enough to understand the first half, as the second is symmetric. The function begins as follows, where &lt;code&gt;x&lt;/code&gt; is a left child.&lt;/p&gt;
&lt;h4 id=&#34;case-1&#34;&gt;Case 1&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;delete_fixup&lt;/span&gt;(self, x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;root &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; BLACK:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; RED:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RED
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left_rotate(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the first case, &lt;code&gt;x&lt;/code&gt;&amp;rsquo;s sibling &lt;code&gt;w&lt;/code&gt; is &lt;code&gt;RED&lt;/code&gt;. If this is true, then &lt;code&gt;w&lt;/code&gt; must have two &lt;code&gt;BLACK&lt;/code&gt; subnodes. The colors of &lt;code&gt;w&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt;&amp;rsquo;s parent are then switched, and a left rotation is performed on &lt;code&gt;x&lt;/code&gt;&amp;rsquo;s parent. The result of case 1 converts to one of cases 2, 3, or 4. The figure below shows the result of the first case.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-15_22-07-04_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Case 1 of `delete_fixup` (CLRS Figure 13.7).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Case 1 of &lt;code&gt;delete_fixup&lt;/code&gt; (CLRS Figure 13.7).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;case-2&#34;&gt;Case 2&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; BLACK &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; BLACK:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RED
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If both of &lt;code&gt;w&lt;/code&gt;&amp;rsquo;s subnodes are &lt;code&gt;BLACK&lt;/code&gt; and both &lt;code&gt;w&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt; are also black (actually, &lt;code&gt;x&lt;/code&gt; is doubly &lt;code&gt;BLACK&lt;/code&gt;), then there is an extra &lt;code&gt;BLACK&lt;/code&gt; node on the path from &lt;code&gt;w&lt;/code&gt; to the leaves. The colors of both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;w&lt;/code&gt; are switched, which leaves &lt;code&gt;x&lt;/code&gt; with a single &lt;code&gt;BLACK&lt;/code&gt; count and &lt;code&gt;w&lt;/code&gt; as &lt;code&gt;RED&lt;/code&gt;. The extra &lt;code&gt;BLACK&lt;/code&gt; property is transferred to &lt;code&gt;x&lt;/code&gt;&amp;rsquo;s parent. The figure below shows the result of the second case.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-16_07-57-36_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Case 2 of `delete_fixup` (CLRS Figure 13.7).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Case 2 of &lt;code&gt;delete_fixup&lt;/code&gt; (CLRS Figure 13.7).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;case-3&#34;&gt;Case 3&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; BLACK:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RED
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right_rotate(w)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If &lt;code&gt;w&lt;/code&gt; is &lt;code&gt;BLACK&lt;/code&gt;, its left child is &lt;code&gt;RED&lt;/code&gt;, and its right child is &lt;code&gt;BLACK&lt;/code&gt;, then the colors of &lt;code&gt;w&lt;/code&gt; and its left child are switched. Then a right rotation is performed on &lt;code&gt;w&lt;/code&gt;. This rotation moves the &lt;code&gt;BLACK&lt;/code&gt; node to &lt;code&gt;w&lt;/code&gt;&amp;rsquo;s position, which is now the new sibling of &lt;code&gt;x&lt;/code&gt;. This leads directly to case 4. A visualization of case 3 is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-16_08-01-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Case 3 of `delete_fixup` (CLRS Figure 13.7).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Case 3 of &lt;code&gt;delete_fixup&lt;/code&gt; (CLRS Figure 13.7).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;case-4&#34;&gt;Case 4&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left_rotate(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;root
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;At this point, &lt;code&gt;w&lt;/code&gt; is &lt;code&gt;BLACK&lt;/code&gt; and its right child is &lt;code&gt;RED&lt;/code&gt;. Also remember that &lt;code&gt;x&lt;/code&gt; still holds an extra &lt;code&gt;BLACK&lt;/code&gt; count. This last case performs color changes and a left rotation which remedy the extra &lt;code&gt;BLACK&lt;/code&gt; count. The figure below shows the result of case 4.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-16_08-12-46_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Case 4 of `delete_fixup` (CLRS Figure 13.7).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Case 4 of &lt;code&gt;delete_fixup&lt;/code&gt; (CLRS Figure 13.7).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The full &lt;code&gt;delete_fixup&lt;/code&gt; function is shown below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;delete_fixup&lt;/span&gt;(self, x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;root &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; BLACK:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; RED:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RED
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left_rotate(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; BLACK &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; BLACK:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RED
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; BLACK:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RED
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right_rotate(w)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left_rotate(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;root
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; RED:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RED
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right_rotate(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; BLACK &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; BLACK:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RED
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; BLACK:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RED
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left_rotate(w)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right_rotate(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;p)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;root
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BLACK
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The &lt;code&gt;delete&lt;/code&gt; operation takes \(O(\log n)\) time since it performs a constant number of rotations. The &lt;code&gt;delete_fixup&lt;/code&gt; operation also takes \(O(\log n)\) time since it performs a constant number of color changes and at most 3 rotations. Case 2 of &lt;code&gt;delete_fixup&lt;/code&gt; could move the violation up the tree, but this would happen no more than \(O(\log n)\) times. In total, the &lt;code&gt;delete&lt;/code&gt; operation takes \(O(\log n)\) time.&lt;/p&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Create a red-black tree class in Python that supports the operations discussed in these notes.&lt;/li&gt;
&lt;li&gt;Using the created class from exercise 1, implement a Hash Map class that uses a red-black tree for collision resolution via chaining.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Binary Search Trees</title>
      <link>https://ajdillhoff.github.io/notes/binary_search_trees/</link>
      <pubDate>Tue, 10 Oct 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/binary_search_trees/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#binary-search-trees&#34;&gt;Binary Search Trees&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#operations&#34;&gt;Operations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#analysis&#34;&gt;Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;A $n$-ary tree is a graph-based data structure in which each node has up to \(n\) subnodes. It is supported by the following operations (not exclusive):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Search&lt;/li&gt;
&lt;li&gt;Insert&lt;/li&gt;
&lt;li&gt;Delete&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Tree-based data structures are defined by the following properties.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;&lt;strong&gt;size&lt;/strong&gt;&lt;/strong&gt; of a tree \(T\) is determined by the total number of nodes in \(T\).&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;&lt;strong&gt;root&lt;/strong&gt;&lt;/strong&gt; of a tree \(T\) is the starting point of \(T\).&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;&lt;strong&gt;leaf node&lt;/strong&gt;&lt;/strong&gt; of a tree \(T\) is a node that has no sub-nodes.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;&lt;strong&gt;height&lt;/strong&gt;&lt;/strong&gt; of a tree is determined by the length of the shortest path between the root of \(T\) and the lowest leaf node of \(T\).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we limit the number of subnodes each node may have to 2, the structure becomes known as a &lt;strong&gt;&lt;strong&gt;binary tree&lt;/strong&gt;&lt;/strong&gt;. Limiting the structure in this way is of interest to us because of the efficiency benefits seen in operations applied to binary trees. If we narrow the scope of these trees further, we can define a &lt;strong&gt;&lt;strong&gt;binary search tree&lt;/strong&gt;&lt;/strong&gt; whose search operation, as the name might suggest, runs in \(\Theta(lg n)\) worst-case time.&lt;/p&gt;
&lt;h2 id=&#34;binary-search-trees&#34;&gt;Binary Search Trees&lt;/h2&gt;
&lt;p&gt;A binary search tree is a regular binary tree with references to the left, right, and parent nodes, defined by the following property:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Let \(x\) be a node in a binary search tree. If \(y\) is a node in the left subtree of \(x\), then \(y.key \leq x.key\). If \(y\) is a node in the right subtree of \(x\), then \(y.key \geq x.key\).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Under this definition, operations such as search, insert, and delete can be performed in \(\Theta(lg n)\) worst-case time assuming that the tree is balanced. Later, we will explore a variant of the binary search tree that guarantees a balanced tree.&lt;/p&gt;
&lt;p&gt;A tree node implemented in Python might look like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Node&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, key):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;operations&#34;&gt;Operations&lt;/h2&gt;
&lt;h3 id=&#34;traversals&#34;&gt;Traversals&lt;/h3&gt;
&lt;p&gt;Like any other graph-based structure, a tree can be traversed using either depth-first or breadth-first search. Only an inorder depth-first search is of interest for a binary search tree, as we will see below. Consider the given tree in the figure below. Performing an inorder traversal on this tree yields the keys in sorted order from smallest to largest.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;inorder_tree_walk&lt;/span&gt;(x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        inorder_tree_walk(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        inorder_tree_walk(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Traversing the entire tree takes \(\Theta(n)\) time, as each node must be visited once. &lt;em&gt;Searching&lt;/em&gt; a tree, however, only takes \(\Theta(lg n)\) time. The search algorithm is defined recursively as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tree_search&lt;/span&gt;(x, k):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; tree_search(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left, k)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; tree_search(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right, k)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Consider the balanced tree shown in the figure below. If we search for the key 15, notice that after the first comparison with the root, the search space goes from 15 nodes to 7 nodes. After the second comparison, the search space goes from 7 nodes to 3 nodes. After the third comparison, the search space goes from 3 nodes to 1 node. This is the essence of binary search, and it is why the search operation runs in \(\Theta(lg n)\) time.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-10_20-05-13_binary_tree_full.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;A balanced binary search tree&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;A balanced binary search tree
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;minimum&#34;&gt;Minimum&lt;/h3&gt;
&lt;p&gt;In a BST, the minimum value is the leftmost node. Finding the minimum is as easy as traversing down the left branch until a leaf node is reached.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tree_minimum&lt;/span&gt;(x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;maximum&#34;&gt;Maximum&lt;/h3&gt;
&lt;p&gt;In a BST, the maximum value is the rightmost node. Finding the maximum is as easy as traversing down the right branch until a leaf node is reached.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tree_maximum&lt;/span&gt;(x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;successor&#34;&gt;Successor&lt;/h3&gt;
&lt;p&gt;The successor and predecessor operations are useful for the delete operation defined below. The successor of a node \(x\) is the node with the smallest key greater than \(x.key\). If \(x\) has a right subtree, then the successor of \(x\) is the minimum of the right subtree. If \(x\) has no right subtree, then the successor of \(x\) is the lowest ancestor of \(x\) whose left child is also an ancestor of \(x\).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tree_successor&lt;/span&gt;(x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; tree_minimum(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;predecessor&#34;&gt;Predecessor&lt;/h3&gt;
&lt;p&gt;The predecessor of a node \(x\) is the node with the largest key less than \(x.key\). If \(x\) has a left subtree, then the predecessor of \(x\) is the maximum of the left subtree. If \(x\) has no left subtree, then the predecessor of \(x\) is the lowest ancestor of \(x\) whose right child is also an ancestor of \(x\).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tree_predecessor&lt;/span&gt;(x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; tree_maximum(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;insert&#34;&gt;Insert&lt;/h3&gt;
&lt;p&gt;Inserting an item into a binary search tree follows the same logic as traversal. Starting at the root, the key is compared to see if it is greater than the root&amp;rsquo;s key. If so, recursively traverse down the right branch. If not, recursively traverse down the left branch. This process continues until a leaf node is reached, at which point the new node is inserted as a child of the leaf node.&lt;/p&gt;
&lt;p&gt;This process will not necessarily result in a balanced tree. In fact, if the keys are inserted in sorted order, the tree will be a linked list. This is the worst-case scenario for a binary search tree, as the search operation will then run in \(\Theta(n)\) time.&lt;/p&gt;
&lt;p&gt;The full algorithm is given below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tree_insert&lt;/span&gt;(T, z):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;root
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        T&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;root &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;delete&#34;&gt;Delete&lt;/h3&gt;
&lt;p&gt;Deleting a node is not a straightforward as insert. Depending on the structure of the subtree, one of three cases must be considered.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If \(z\) has no subnodes, simply remove \(z\) from the tree.&lt;/li&gt;
&lt;li&gt;If \(z\) has one subnode, replace \(z\) with its subnode.&lt;/li&gt;
&lt;li&gt;If \(z\) has two subnodes, replace \(z\) with its successor. It is a bit more complicated than this, as we explore below.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In case 3, node \(z\) has both a left and right subnode. The first step is to find the successor of \(z\), \(y\). Since \(z\) has 2 subnodes, its successor has no left subnode (&lt;strong&gt;&lt;strong&gt;convince yourself of this&lt;/strong&gt;&lt;/strong&gt;). Likewise, its predecessor has no right subnode. If \(y\) is the right subnode of \(z\), replace \(z\) by \(y\).&lt;/p&gt;
&lt;p&gt;If \(y\) is not the right subnode of \(z\), it is somewhere further down the right branch. In this case, replace \(y\) by its right subnode before replacing \(z\) by \(y\). The figure below shows the removal of node 12 from the tree in the figure above.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-10_23-03-23_binary_tree_delete.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Deleting node 12 from the tree&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Deleting node 12 from the tree
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Even though only 1 node was moved (13 to 12&amp;rsquo;s old position), the process of deleting a node actually involves &lt;strong&gt;&lt;strong&gt;transplanting&lt;/strong&gt;&lt;/strong&gt; a subtree to a new position. This is defined algorithmically as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;transplant&lt;/span&gt;(T, u, v):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        T&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;root &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; u &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the code above, &lt;code&gt;u&lt;/code&gt; is the node to be replaced, and &lt;code&gt;v&lt;/code&gt; is the node to replace it. Updating &lt;code&gt;v&lt;/code&gt;&amp;rsquo;s left and right subnodes are done in the calling function &lt;code&gt;tree_delete&lt;/code&gt;, as seen below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tree_delete&lt;/span&gt;(T, z):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:  &lt;span style=&#34;color:#75715e&#34;&gt;# Case 1 and 2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        transplant(T, z, z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;: &lt;span style=&#34;color:#75715e&#34;&gt;# Also case 1 and 2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        transplant(T, z, z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;: &lt;span style=&#34;color:#75715e&#34;&gt;# Case 3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tree_minimum(z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right) &lt;span style=&#34;color:#75715e&#34;&gt;# get successor&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            transplant(T, y, y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        transplant(T, z, y)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;analysis&#34;&gt;Analysis&lt;/h2&gt;
&lt;p&gt;Insert, delete, and search all run in \(\Theta(h)\) time, where \(h\) is the height of the tree. If the tree is balanced, \(h = \Theta(lg n)\), and all operations run in \(\Theta(lg n)\) time. If the tree is not balanced, \(h = \Theta(n)\), and all operations run in \(\Theta(n)\) time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Data Structures</title>
      <link>https://ajdillhoff.github.io/notes/introduction_to_data_structures/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/introduction_to_data_structures/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-to-data-structures&#34;&gt;Introduction to Data Structures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#review-pointers&#34;&gt;Review: Pointers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#arrays&#34;&gt;Arrays&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrices&#34;&gt;Matrices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multi-dimensional-arrays&#34;&gt;Multi-Dimensional Arrays&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#stacks&#34;&gt;Stacks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#queues&#34;&gt;Queues&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction-to-data-structures&#34;&gt;Introduction to Data Structures&lt;/h2&gt;
&lt;p&gt;Data structures are fundamental concepts in computer science that allow us to organize and store data in a way that enables efficient access and modification. They are essential building blocks for creating efficient and sophisticated computer programs and databases. Different types of data structures include arrays, linked lists, stacks, queues, trees, graphs, and many more, each serving a specific purpose and suited to specific applications.&lt;/p&gt;
&lt;p&gt;Understanding data structures is therefore important because they are used in almost every software application. For instance, social media applications use data structures to store user information and their connections, while search engines use them to index and retrieve web pages quickly. The choice of the right data structure significantly impacts the performance, scalability, and resource utilization of software applications.&lt;/p&gt;
&lt;p&gt;Having a strong grasp of data structures and their properties is critical for anyone working with software or data in general. Through studying the benefits and limitations of each data structure, you will be equipped to analyze the efficacy of existing systems as well as make the right choice when developing new ones.&lt;/p&gt;
&lt;h3 id=&#34;why-do-we-need-so-many&#34;&gt;Why Do We Need So Many?&lt;/h3&gt;
&lt;p&gt;There are many data structures available because no single dataset works best for all cases. Each data structure has its unique characteristics, advantages, and disadvantages. These differences can often be evaluated quantitatively, providing rigorous backing when selecting the appropriate one for the task at hand.&lt;/p&gt;
&lt;p&gt;For example, arrays are excellent when the size of the data is known and constant, but they are not efficient when it comes to frequent insertions and deletions. Linked lists, on the other hand, allow for efficient insertions and deletions but are not as quick as arrays when it comes to accessing elements. Trees are invaluable when we need to maintain a sorted list of data and perform quick searches, insertions, and deletions, while hash tables are optimal for scenarios where we need to perform fast lookups.&lt;/p&gt;
&lt;h2 id=&#34;review-pointers&#34;&gt;Review: Pointers&lt;/h2&gt;
&lt;h3 id=&#34;what-are-pointers&#34;&gt;What are Pointers?&lt;/h3&gt;
&lt;p&gt;Pointers are variables in programming that store the memory address of another variable. They are a powerful feature in many programming languages, including C and C++, allowing programmers to directly access memory locations and manipulate data efficiently. Pointers are crucial for implementing dynamic data structures like linked lists, trees, and graphs.&lt;/p&gt;
&lt;p&gt;In Python, pointers are not exposed explicitly as in languages like C, but references, which are similar to pointers, are used to hold the memory address of objects. Understanding the concept of pointers and references is essential for managing memory effectively and avoiding issues like memory leaks and dangling pointers in languages that allow direct memory manipulation. Even if we are not dealing with pointers directly, studying them is beneficial for understanding algorithms and data structures in general.&lt;/p&gt;
&lt;h3 id=&#34;how-are-they-represented&#34;&gt;How are They Represented?&lt;/h3&gt;
&lt;p&gt;In languages like C, pointers are represented using the asterisk (*) symbol, and the address operator (&amp;amp;) is used to retrieve the memory address of a variable. For example, &lt;code&gt;int *p;&lt;/code&gt; declares a pointer to an integer, and &lt;code&gt;p = &amp;amp;x;&lt;/code&gt; assigns the address of the variable &lt;code&gt;x&lt;/code&gt; to the pointer &lt;code&gt;p&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In Python, pointers are not explicitly represented, but references to objects are used to achieve similar functionality. For instance, when a list is assigned to a new variable, the new variable holds a reference to the same list object, not a copy of the list. Any modifications made through one variable are reflected in the other.&lt;/p&gt;
&lt;h2 id=&#34;arrays&#34;&gt;Arrays&lt;/h2&gt;
&lt;h3 id=&#34;how-are-arrays-represented-in-memory&#34;&gt;How are Arrays Represented in Memory?&lt;/h3&gt;
&lt;p&gt;Arrays are fundamental data structures that store elements of the same type in contiguous memory locations. The elements can be accessed randomly by indexing into the array. In memory, an array is represented as a block of memory cells, each holding an element of the array placed side by side. The size of each cell is determined by the size of the array&amp;rsquo;s element type.&lt;/p&gt;
&lt;p&gt;The base address of the array is the memory address of the first element (index 0), and it is used, along with the index and the size of each element, to calculate the address of any element in the array. For example, if the base address is `B`, the size of each element is `S`, and the index of the element is `i`, the address of the element can be calculated as `B + (i * S)`.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-02_17-50-19_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Memory layout of an integer array of size 8.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Memory layout of an integer array of size 8.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;how-many-bytes-does-each-element-use&#34;&gt;How Many Bytes Does Each Element Use?&lt;/h3&gt;
&lt;p&gt;The number of bytes used by each element in an array depends on the data type of the elements. For example, in most systems, an &lt;code&gt;int&lt;/code&gt; uses 4 bytes, a &lt;code&gt;char&lt;/code&gt; uses 1 byte, and a &lt;code&gt;double&lt;/code&gt; uses 8 bytes. When an array is declared, the total memory allocated for the array is the product of the number of elements and the size of each element.&lt;/p&gt;
&lt;p&gt;In Python, the &lt;code&gt;sys&lt;/code&gt; module can be used to find the size of an object in bytes. However, Python’s dynamic typing and object-oriented nature mean that the size of an array element can vary significantly, as each element is an object that can have additional overhead and can hold references to other objects.&lt;/p&gt;
&lt;p&gt;Since Python does not expose pointers explicitly, we can safely program efficient programs without worrying about making common mistakes related to pointers and memory management.&lt;/p&gt;
&lt;h3 id=&#34;how-are-arrays-indexed&#34;&gt;How are Arrays Indexed?&lt;/h3&gt;
&lt;p&gt;Arrays are indexed using a zero-based indexing system, where the first element is accessed using index 0, the second element with index 1, and so on. To access an element at a specific index, the address of the element is calculated using the base address of the array, the size of each element, and the index of the element.&lt;/p&gt;
&lt;p&gt;In languages that support pointers, the address of an element in an array can be calculated using pointer arithmetic. If &lt;code&gt;p&lt;/code&gt; is a pointer to the base address of the array, and &lt;code&gt;i&lt;/code&gt; is the index of the element, the address of the element can be calculated as &lt;code&gt;p + i&lt;/code&gt;, where &lt;code&gt;i&lt;/code&gt; is automatically scaled by the size of the array&amp;rsquo;s element type.&lt;/p&gt;
&lt;h2 id=&#34;matrices&#34;&gt;Matrices&lt;/h2&gt;
&lt;h3 id=&#34;fixed-sized-arrays-vs-dot-ragged-arrays&#34;&gt;Fixed-sized Arrays vs. Ragged Arrays&lt;/h3&gt;
&lt;p&gt;Matrices are two-dimensional arrays that can be represented using fixed-sized arrays or ragged arrays. A fixed-sized array is a regular matrix where each row has the same number of columns, and it is represented in memory as a contiguous block. It allows for efficient access to elements using row and column indices but can waste memory if the matrix is sparse.&lt;/p&gt;
&lt;p&gt;A ragged array, on the other hand, is an irregular matrix where each row can have a different number of columns. It is represented using an array of pointers, where each pointer points to a one-dimensional array representing a row of the matrix. Ragged arrays are more memory-efficient for sparse matrices but can be more complex to manage and traverse.&lt;/p&gt;
&lt;p&gt;Choosing between fixed-sized and ragged arrays depends on the requirements of the application, the characteristics of the matrix, and the trade-offs between memory efficiency and complexity. Understanding the differences between the two representations is crucial for implementing matrices effectively and optimizing memory usage.&lt;/p&gt;
&lt;h3 id=&#34;flat-indexing-back-and-forth&#34;&gt;Flat Indexing, Back and Forth&lt;/h3&gt;
&lt;p&gt;Flat indexing is a technique used to represent a two-dimensional array or matrix using a one-dimensional array. In this representation, the elements of the matrix are stored in a single array in row-major or column-major order, and the two-dimensional indices (row and column) are mapped to a single index in the one-dimensional array.&lt;/p&gt;
&lt;p&gt;For a matrix with &lt;code&gt;M&lt;/code&gt; rows and &lt;code&gt;N&lt;/code&gt; columns, the mapping from two-dimensional indices to a one-dimensional index in row-major order is done using the formula &lt;code&gt;index = (row * N) + column&lt;/code&gt;, and in column-major order using the formula &lt;code&gt;index = (column * M) + row&lt;/code&gt;. Flat indexing allows for efficient memory utilization and easy serialization of matrices but requires conversion between one-dimensional and two-dimensional indices.&lt;/p&gt;
&lt;h3 id=&#34;python-example-of-matrices&#34;&gt;Python Example of Matrices&lt;/h3&gt;
&lt;p&gt;In Python, matrices can be represented using lists of lists, where each inner list represents a row of the matrix. Elements can be accessed and modified using two indices, one for the row and one for the column. For example, to create a 2x3 matrix and access the element in the second row and third column, you can do the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;matrix &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;element &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; matrix[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]  &lt;span style=&#34;color:#75715e&#34;&gt;# access value 6&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;NumPy also provides support for multi-dimensional arrays and matrices along with a host of functions to perform operations on them. Using Numpy, you can create a matrix and access its elements as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;matrix &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;element &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; matrix[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]  &lt;span style=&#34;color:#75715e&#34;&gt;# access value 6&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;multi-dimensional-arrays&#34;&gt;Multi-Dimensional Arrays&lt;/h2&gt;
&lt;h3 id=&#34;introduction-to-multi-dimensional-arrays&#34;&gt;Introduction to Multi-Dimensional Arrays&lt;/h3&gt;
&lt;p&gt;Multi-dimensional arrays are arrays of more than 1 dimension. They are used to represent complex data structures like matrices, tensors, and tables. They are crucial for various applications, including scientific computing, image processing, and machine learning, where data is often multi-dimensional.&lt;/p&gt;
&lt;p&gt;In a multi-dimensional array, each element is identified by multiple indices, one for each dimension of the array. For example, in a two-dimensional array representing a matrix, each element is identified by two indices representing the row and column of the element. The number of dimensions and the size of each dimension determine the structure and capacity of the array.&lt;/p&gt;
&lt;p&gt;Different languages have different approaches to handling multi-dimensional arrays. C, for example, arranges the memory of any array contiguously. Java and Python use jagged arrays, where the row lengths can differ in size. The data in each row might be contiguous, but the entire array is not.&lt;/p&gt;
&lt;h3 id=&#34;numpy-arrays-and-their-representation&#34;&gt;NumPy Arrays and Their Representation&lt;/h3&gt;
&lt;p&gt;A NumPy array is represented in memory as a contiguous block, and it allows for efficient access and manipulation of elements using multiple indices. The shape of the array, represented as a tuple of integers, determines the number of dimensions and the size of each dimension of the array.&lt;/p&gt;
&lt;p&gt;To determine how to index the contiguous stream of values represented as an $n$-dimensional array, each &lt;code&gt;np.array&lt;/code&gt; specifies the &lt;code&gt;strides&lt;/code&gt; for each dimension. Consider the following array with shape &lt;code&gt;(2, 3, 4)&lt;/code&gt; and data type &lt;code&gt;int32&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;               [&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;               [&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;11&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;]],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              [[&lt;span style=&#34;color:#ae81ff&#34;&gt;13&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;               [&lt;span style=&#34;color:#ae81ff&#34;&gt;17&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;18&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;19&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;               [&lt;span style=&#34;color:#ae81ff&#34;&gt;21&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;22&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;23&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;24&lt;/span&gt;]]], dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;int32)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The &lt;code&gt;strides&lt;/code&gt; of the array are &lt;code&gt;(48, 16, 4)&lt;/code&gt;, which means that to move to the next depth, we need to move 48 bytes, to move to the next row, we need to move 16 bytes, and to move to the next column, we need to move 4 bytes. The &lt;code&gt;strides&lt;/code&gt; are calculated based on the size of each element and the shape of the array.&lt;/p&gt;
&lt;h3 id=&#34;python-example-with-numpy&#34;&gt;Python Example with NumPy&lt;/h3&gt;
&lt;p&gt;Here’s an example of how to create a two-dimensional array (matrix) using Numpy and how to access its elements:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Creating a 2x3 Numpy array&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;array &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Accessing the element in the second row and third column&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;element &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; array[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]  &lt;span style=&#34;color:#75715e&#34;&gt;# element will be 6&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;NumPy also provides various functions to perform operations on arrays, such as reshaping, transposing, and aggregating. For example, to calculate the sum of all elements in the array, you can use the &lt;code&gt;np.sum&lt;/code&gt; function:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;total &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(array)  &lt;span style=&#34;color:#75715e&#34;&gt;# total will be 21&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;stacks&#34;&gt;Stacks&lt;/h2&gt;
&lt;h3 id=&#34;how-are-stacks-represented-with-an-array&#34;&gt;How are Stacks Represented with an Array?&lt;/h3&gt;
&lt;p&gt;A stack is a linear data structure that follows the Last In First Out (LIFO) principle, meaning the last element added to the stack is the first one to be removed. Stacks can be easily implemented using arrays, where elements are added and removed from one end of the array, known as the top of the stack.&lt;/p&gt;
&lt;p&gt;When representing a stack with an array, one must keep track of the index of the top of the stack. Elements are added to the stack by placing them at the position indicated by the top index and then incrementing the top index. Elements are removed from the stack by decrementing the top index and then accessing the element at that position.&lt;/p&gt;
&lt;h3 id=&#34;difference-between-using-the-beginning-or-the-end-of-the-array-as-the-top&#34;&gt;Difference Between Using the Beginning or the End of the Array as the Top&lt;/h3&gt;
&lt;p&gt;When implementing a stack using an array, one can choose to use either the beginning or the end of the array as the top of the stack. The choice affects the implementation of the push and pop operations and the way the top index is managed.&lt;/p&gt;
&lt;p&gt;If the beginning of the array is used as the top, elements are added and removed from the first position of the array, and the other elements must be shifted to make room or fill the gap. This can lead to higher time complexity for push and pop operations. If the end of the array is used as the top, elements are added and removed from the last position of the array, allowing for constant-time push and pop operations without the need to shift other elements.&lt;/p&gt;
&lt;h3 id=&#34;python-example-of-stack-with-array&#34;&gt;Python Example of Stack with Array&lt;/h3&gt;
&lt;p&gt;Here’s an example of how to implement a stack using a Python list, with the end of the list as the top of the stack:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;stack &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []  &lt;span style=&#34;color:#75715e&#34;&gt;# Initializing an empty stack&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;stack&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)  &lt;span style=&#34;color:#75715e&#34;&gt;# Pushing an element onto the stack&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;stack&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)  &lt;span style=&#34;color:#75715e&#34;&gt;# Pushing another element onto the stack&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;top_element &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; stack&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pop()  &lt;span style=&#34;color:#75715e&#34;&gt;# Popping the top element from the stack, top_element will be 2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This implementation allows for efficient and simple push and pop operations, with constant time complexity. However, the size of the stack is limited by the available memory, and care must be taken to handle underflow and overflow conditions.&lt;/p&gt;
&lt;p&gt;Understanding how to implement and use stacks is crucial for solving problems that involve reversing, balancing, or processing data in a LIFO manner. Stacks are a versatile and fundamental data structure used in various applications, including expression evaluation, syntax parsing, and undo mechanisms.&lt;/p&gt;
&lt;h2 id=&#34;queues&#34;&gt;Queues&lt;/h2&gt;
&lt;h3 id=&#34;how-are-queues-represented-with-an-array&#34;&gt;How are Queues Represented with an Array?&lt;/h3&gt;
&lt;p&gt;A queue is a linear data structure that follows the First In First Out (FIFO) principle, meaning the first element added to the queue is the first one to be removed. Queues can be implemented using arrays, where elements are added at the rear and removed from the front.&lt;/p&gt;
&lt;p&gt;When representing a queue with an array, two indices are maintained, one for the front and one for the rear of the queue. Elements are enqueued by placing them at the position indicated by the rear index and then incrementing the rear index. Elements are dequeued by accessing the element at the front index and then incrementing the front index.&lt;/p&gt;
&lt;h3 id=&#34;downside-to-using-one-side-of-the-array-as-the-front-and-the-other-as-the-rear&#34;&gt;Downside to Using One Side of the Array as the Front and the Other as the Rear&lt;/h3&gt;
&lt;p&gt;When implementing a queue using an array, using one side of the array as the front and the other as the rear can lead to inefficient use of space. Once elements are dequeued from the front, the space they occupied cannot be reused, and overflow can occur even if there is free space at the front of the array.&lt;/p&gt;
&lt;p&gt;To overcome this limitation, a circular queue can be implemented, where the front and rear indices wrap around to the beginning of the array when they reach the end. This allows for efficient use of space and avoids overflow as long as there is free space in the array. However, it requires more complex index management and can be harder to implement correctly.&lt;/p&gt;
&lt;h3 id=&#34;more-efficient-approach-by-using-a-reference-to-a-head-and-tail&#34;&gt;More Efficient Approach by Using a Reference to a Head and Tail&lt;/h3&gt;
&lt;p&gt;A more efficient approach to implementing a queue is to use a linked list, where each element holds a reference to the next element in the queue. This allows for dynamic resizing of the queue and efficient enqueue and dequeue operations, without the need for complex index management or wasted space.&lt;/p&gt;
&lt;p&gt;In this approach, two pointers are maintained, one for the head (front) and one for the tail (rear) of the queue. Elements are enqueued by adding them at the position pointed to by the tail pointer and updating the tail pointer to the new element. Elements are dequeued by accessing the element pointed to by the head pointer and updating the head pointer to the next element.&lt;/p&gt;
&lt;p&gt;This same approach can be done by using indices for the head and tail. The data itself is &amp;ldquo;circular&amp;rdquo; in the sense that the indices wrap around to the beginning of the array when they reach the end. This allows for efficient use of space and avoids overflow as long as there is free space in the array.&lt;/p&gt;
&lt;h3 id=&#34;python-example-of-a-queue-using-an-array&#34;&gt;Python Example of a Queue using an Array&lt;/h3&gt;
&lt;p&gt;Here’s an example of how to implement a simple queue using a Python list, with the front at the beginning of the list and the rear at the end of the list:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; collections &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; deque
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;queue &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; deque()  &lt;span style=&#34;color:#75715e&#34;&gt;# Initializing an empty queue&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;queue&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)  &lt;span style=&#34;color:#75715e&#34;&gt;# Enqueueing an element&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;queue&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)  &lt;span style=&#34;color:#75715e&#34;&gt;# Enqueueing another element&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;front_element &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; queue&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;popleft()  &lt;span style=&#34;color:#75715e&#34;&gt;# Dequeueing the front element from the queue, front_element will be 1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This implementation uses the &lt;code&gt;deque&lt;/code&gt; class from the &lt;code&gt;collections&lt;/code&gt; module, which allows for efficient appending and popping from both ends of the list. It provides a simple and versatile way to implement a queue in Python, with dynamic resizing and constant-time enqueue and dequeue operations.&lt;/p&gt;
&lt;p&gt;Understanding how to implement and use queues is essential for solving problems that involve processing data in a FIFO manner. Queues are a fundamental and versatile data structure used in various applications, including task scheduling, order processing, and breadth-first search.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linked Lists</title>
      <link>https://ajdillhoff.github.io/notes/linked_lists/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/linked_lists/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#singly-linked-lists&#34;&gt;Singly-Linked Lists&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#doubly-linked-lists&#34;&gt;Doubly-Linked Lists&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#operations&#34;&gt;Operations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exercises&#34;&gt;Exercises&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;A linked list is a &lt;strong&gt;dynamic&lt;/strong&gt; and &lt;strong&gt;aggregate&lt;/strong&gt; data structure made up of a collection of nodes. The nodes of a linked list can store any data type and are not enforced to contain the same data type. A basic &lt;code&gt;node&lt;/code&gt; structure may be defined as&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; node {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;data;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; node &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;next;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;singly-linked-lists&#34;&gt;Singly-Linked Lists&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;singly-linked list&lt;/strong&gt;, the most basic form, has a reference to the first node in the list, called the head, and a single link between each node.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-03_16-52-48_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Diagram of a linked list with 3 nodes. The top sections contain data and the bottom sections contain pointers to the next node.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Diagram of a linked list with 3 nodes. The top sections contain data and the bottom sections contain pointers to the next node.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The definition of a linked list node allows the list to grow dynamically. Nodes can be added at any time to any position in the node, as long as a reference to the node before the new one is known. The last node in a list points to &lt;code&gt;NULL&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;doubly-linked-lists&#34;&gt;Doubly-Linked Lists&lt;/h2&gt;
&lt;p&gt;More commonly, linked lists are &lt;strong&gt;doubly-linked&lt;/strong&gt; in that there is a link to the next node and a link to the previous node. This allows for more flexibility in traversing the list, but requires more memory to store the extra link. A standard implementation will also keep a reference to both the head and the tail of the list. This permits efficient insertion and deletion at both ends of the list.&lt;/p&gt;
&lt;h2 id=&#34;operations&#34;&gt;Operations&lt;/h2&gt;
&lt;h3 id=&#34;insertion&#34;&gt;Insertion&lt;/h3&gt;
&lt;p&gt;A new node can be inserted either at the beginning, the end, or somewhere in between. Inserting at the beginning or end is a constant time operation, but inserting in the middle requires traversing the list to find the correct position. To insert at the beginning, the new node&amp;rsquo;s &lt;code&gt;next&lt;/code&gt; reference is updated to the old head and the head is updated to the new node.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;insert_at_beginning&lt;/span&gt;(head, data):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    new_node &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Node(data)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    new_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; head
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    head &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; new_node
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To insert at the end without a reference to the tail, the list must be traversed to find the last node. The new node&amp;rsquo;s &lt;code&gt;next&lt;/code&gt; reference is set to &lt;code&gt;NULL&lt;/code&gt; and the last node&amp;rsquo;s &lt;code&gt;next&lt;/code&gt; reference is set to the new node.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;insert_at_end&lt;/span&gt;(head, data):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    new_node &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Node(data)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    new_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; head &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        head &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; new_node
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        last_node &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; head
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; last_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            last_node &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; last_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        last_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; new_node
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To insert at the end with a reference to the tail, the new node&amp;rsquo;s &lt;code&gt;next&lt;/code&gt; reference is set to &lt;code&gt;NULL&lt;/code&gt; and the tail&amp;rsquo;s &lt;code&gt;next&lt;/code&gt; reference is set to the new node. The tail is then updated to the new node.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;insert_at_end&lt;/span&gt;(head, tail, data):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    new_node &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Node(data)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    new_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; head &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        head &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; new_node
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        tail &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; new_node
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        tail&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; new_node
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        tail &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; new_node
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To insert in the middle, the list must be traversed to find the correct position. The new node&amp;rsquo;s &lt;code&gt;next&lt;/code&gt; reference is set to the next node and the previous node&amp;rsquo;s &lt;code&gt;next&lt;/code&gt; reference is set to the new node.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;insert_in_middle&lt;/span&gt;(head, data, position):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    new_node &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Node(data)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; head &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        head &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; new_node
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        current_node &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; head
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(position &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            current_node &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; current_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        new_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; current_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        current_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; new_node
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;searching&#34;&gt;Searching&lt;/h3&gt;
&lt;p&gt;Searching a linked list is a linear time operation. The list is traversed until the desired node is found or the end of the list is reached.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;search&lt;/span&gt;(head, data):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    current_node &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; head
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; current_node &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; current_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; data:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; current_node
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        current_node &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; current_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;deletion&#34;&gt;Deletion&lt;/h3&gt;
&lt;p&gt;Deletion is similar to insertion. A node can be deleted from the beginning, the end, or somewhere in between. Deleting at the beginning or end is a constant time operation, but deleting in the middle requires traversing the list to find the correct position. To delete at the beginning, the head is updated to the second node and the first node is deleted.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;delete_at_beginning&lt;/span&gt;(head):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; head &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        head &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; head&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To delete at the end without a reference to the tail, the list must be traversed to find the last node. The second to last node&amp;rsquo;s &lt;code&gt;next&lt;/code&gt; reference is set to &lt;code&gt;NULL&lt;/code&gt; and the last node is deleted.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;delete_at_end&lt;/span&gt;(head):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; head &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; head&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            head &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            last_node &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; head
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; last_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                last_node &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; last_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            last_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To delete at the end with a reference to the tail, the tail is updated to the second to last node and the last node is deleted.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;delete_at_end&lt;/span&gt;(head, tail):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; head &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; head&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            head &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            tail &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            last_node &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; head
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; last_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                last_node &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; last_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            last_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            tail &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; last_node
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To delete in the middle, the list must be traversed to find the correct position. The previous node&amp;rsquo;s &lt;code&gt;next&lt;/code&gt; reference is set to the next node and the current node is deleted.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;delete_in_middle&lt;/span&gt;(head, position):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; head &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        current_node &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; head
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(position &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            current_node &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; current_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        current_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; current_node&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;next
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Reverse a singly-linked list in linear time and constant space.&lt;/li&gt;
&lt;li&gt;Implement a queue using a linked list.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Complexity Analysis</title>
      <link>https://ajdillhoff.github.io/notes/complexity_analysis/</link>
      <pubDate>Mon, 25 Sep 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/complexity_analysis/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-notation-of-complexity-analysis&#34;&gt;The notation of complexity analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#formal-definition-of-asymptotic-notation&#34;&gt;Formal Definition of Asymptotic Notation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#common-functions&#34;&gt;Common Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;the-notation-of-complexity-analysis&#34;&gt;The notation of complexity analysis&lt;/h2&gt;
&lt;h3 id=&#34;o-notation&#34;&gt;$O$-notation&lt;/h3&gt;
&lt;p&gt;$O$-notation, often referred to as &amp;ldquo;Big Oh&amp;rdquo; notation, describes an upper bound on the behavior of a function. It really means that the function &lt;em&gt;will not grow faster&lt;/em&gt; than the a given rate. This rate is typically the highest-order term in the function, and is often referred to as the &amp;ldquo;dominant term&amp;rdquo; or &amp;ldquo;dominant function&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;For example, the function \(f(n) = 3n^2 + 2n + 1\) has a dominant term of \(n^2\), and so we would say that \(f(n) = O(n^2)\). We could also accurately describe \(f(n)\) as \(O(n^3)\) since it technically does not grow at a faster rate than \(n^3\), but this is less common as it misleads the reader into thinking that the function is bounded at \(n^3\).&lt;/p&gt;
&lt;h3 id=&#34;and-omega-notation&#34;&gt;$Ω$-notation&lt;/h3&gt;
&lt;p&gt;$Ω$-notation is used to describe the lower bound on the asymptotic behavior of a function. Specifically, it means that the function grows &lt;em&gt;at least as fast&lt;/em&gt; as the given rate. The function \(f(n) = 3n^2 + 2n + 1\) grows at least as fast as \(n^2\), so we would say that \(f(n) = \Omega(n^2)\). It does not grow as fast as \(n^3\), however.&lt;/p&gt;
&lt;p&gt;Just like $O$-notation, we can abuse this definition and say that something that grows at least as fast as \(n^2\) also grows as fast as \(n\). This would lead the reader to believe that the function is bounded at \(n\), which is not true. For this reason, we typically use the tightest bound possible.&lt;/p&gt;
&lt;h3 id=&#34;and-theta-notation&#34;&gt;$Θ$-notation&lt;/h3&gt;
&lt;p&gt;$Θ$-notation gives a tightly bound characterization of a function&amp;rsquo;s behavior. It gives the rate of growth within a constant factor bounded above as well as constant factor bounded below.&lt;/p&gt;
&lt;p&gt;To show that a function is \(\Theta(f(n))\), we must show that it is both \(O(f(n))\) and \(\Omega(f(n))\). Taking our example from above, the function \(f(n) = 3n^2 + 2n + 1\) is \(\Theta(n^2)\).&lt;/p&gt;
&lt;h3 id=&#34;example-insertion-sort&#34;&gt;Example: Insertion Sort&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s put this notation to work and characterize the running time of insertion sort. We&amp;rsquo;ll start by writing out the pseudocode for the algorithm:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;insertion_sort&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, len(A)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; A[j] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            A[j &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[j]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A[j &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;From our &lt;a href=&#34;https://ajdillhoff.github.io/notes/introduction_to_complexity_analysis/&#34;&gt;Introduction to Algorithms&lt;/a&gt; lecture, we already know that the outer loop runs \((n-1)\) times (although the loop is checked \(n\) times). This is not dependent on the order of the \(n\) inputs either. The inner loop is dependent on the values of our input. It could run anywhere between 0 and \(i-1\) times. In the worst case, we saw that it would run \(n-1\) times as well. With this, we concluded that the running time of insertion sort is \(O(n^2)\). Since this was derived for the worst-case, it is reasonable to say that insertion sort is \(O(n^2)\) for all cases.&lt;/p&gt;
&lt;p&gt;The key to the number of operations that the inner loop takes is &lt;code&gt;A[j + 1] = A[j]&lt;/code&gt;, or the number of times a value is shifted to the right. Given an input of \(n\) elements in the worst-case scenario, we can split the input into 3 partitions where the largest \(\lfloor\frac{n}{4}\rfloor\) values are in the first partition. The second partition has size \(\lceil\frac{n}{2}\rceil\), and the last partition has size \(\lfloor\frac{n}{4}\rfloor\). By using the floor and ceiling functions, we can accommodate for odd values of \(n\).&lt;/p&gt;
&lt;p&gt;When the array is finally sorted, the largest \(\lfloor\frac{n}{4}\rfloor\) values will be in the last partition. That means that they would have passed through the middle \(\lceil\frac{n}{2}\rceil\) values one at a time. Therefore, we can state that the worst-case is proportional to&lt;/p&gt;
&lt;p&gt;\[
\left(\left\lfloor\frac{n}{4}\right\rfloor\right)\left(\left\lceil\frac{n}{2}\right\rceil\right) \leq \frac{n^2}{8}.
\]&lt;/p&gt;
&lt;p&gt;This is \(\Omega(n^2)\), so we can conclude that insertion sort is \(\Theta(n^2)\).&lt;/p&gt;
&lt;h3 id=&#34;bonus-example-selection-sort&#34;&gt;Bonus Example: Selection Sort&lt;/h3&gt;
&lt;p&gt;Use a similar analysis to show that the worst-case for selection sort is \(\Theta(n^2)\). As a reminder, selection sort is defined as&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;selection_sort&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, len(A)&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        min_j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, len(A)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; A[j] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; A[min_j]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                min_j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; j
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A[i], A[min_j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[min_j], A[i]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We have already observed that the outer loop iterates \(n-1\) times. Even in the best case, the inner loop runs proportional to \(n\) times. This is sufficient to conclude that the running time is \(O(n^2)\) for all cases.&lt;/p&gt;
&lt;p&gt;For showing that the worst case is \(\Omega(n^2)\), we could use the same argument as insertion sort. However, that isn&amp;rsquo;t necessary. In &lt;em&gt;any&lt;/em&gt; case, the inner loop will run proportional to \(n\) times. It is not dependent on any specific arrangement of the input as selection sort is. Therefore, we can conclude that the worst-case is \(\Omega(n^2)\), and so selection sort is \(\Theta(n^2)\).&lt;/p&gt;
&lt;h2 id=&#34;formal-definition-of-asymptotic-notation&#34;&gt;Formal Definition of Asymptotic Notation&lt;/h2&gt;
&lt;p&gt;Now that we have established some understanding of the notation, let&amp;rsquo;s define it formally. We typically use functions whose domains are over the set of natural or real numbers.&lt;/p&gt;
&lt;h3 id=&#34;o-notation&#34;&gt;$O$-notation&lt;/h3&gt;
&lt;p&gt;We previously established that $O$-notation described as &lt;strong&gt;asymptotic upper bound&lt;/strong&gt;. It was briefly mentioned that this bound holds within a constant factor, which we will now define more thoroughly. For a function \(g(n)\), \(O(g(n)) = \{f(n) : \exists c &amp;gt; 0, n_0 &amp;gt; 0 \text{ such that } 0 \leq f(n) \leq cg(n) \text{ for all } n \geq n_0\}\). It might make more sense to visualize this definition.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-09-26_17-43-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Visualization of $O$-notation (source: Cormen et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Visualization of $O$-notation (source: Cormen et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Notice that the function \(f(n)\) is bounded above by \(cg(n)\) for all \(n \geq n_0\) in the figure above.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s put this definition to the test with an example. Given a function \(f(n) = 3n^2 + 200n + 1000\), show that \(f(n) = O(n^2)\). The goal is to find positive constants \(c\) and \(n_0\) such that \(3n^2 + 200n + 1000 \leq cn^2\) for all \(n \geq n_0\). Dividing both sides by \(n^2\) yields&lt;/p&gt;
&lt;p&gt;\[
3 + \frac{200}{n} + \frac{1000}{n^2} \leq c.
\]&lt;/p&gt;
&lt;p&gt;This equation has many possible solutions. Let&amp;rsquo;s choose \(n_0 = 2\), then&lt;/p&gt;
&lt;p&gt;\[
3 + \frac{200}{2} + \frac{1000}{2^2} = 3 + 100 + 250 = 353 \leq c.
\]&lt;/p&gt;
&lt;p&gt;Therefore, we can conclude that \(f(n) = O(n^2)\).&lt;/p&gt;
&lt;h3 id=&#34;and-omega-notation&#34;&gt;$Ω$-notation&lt;/h3&gt;
&lt;p&gt;The notation used to describe an &lt;strong&gt;asymptotic lower bound&lt;/strong&gt; is formally defined as \(\Omega(g(n)) = \{f(n) : \exists c &amp;gt; 0, n_0 &amp;gt; 0 \text{ such that } 0 \leq cg(n) \leq f(n) \text{ for all } n \geq n_0\}\). Again, it is helpful to visualize this.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-09-26_18-17-07_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Visualization of $&amp;amp;Omega;$-notation (source: Cormen et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Visualization of $Ω$-notation (source: Cormen et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Notice that the function \(f(n)\) is bounded below by \(cg(n)\) for all \(n \geq n_0\) in the figure above.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s revisit our function from above and show that \(f(n) = \Omega(n^2)\). The goal is to find positive constants \(c\) and \(n_0\) such that \(3n^2 + 200n + 1000 \geq cn^2\) for all \(n \geq n_0\). Dividing both sides by \(n^2\) yields&lt;/p&gt;
&lt;p&gt;\[
3 + \frac{200}{n} + \frac{1000}{n^2} \geq c.
\]&lt;/p&gt;
&lt;p&gt;This holds when \(c = 3\) and \(n_0\) is any positive integer. To see this, think about what happens to this function as \(n\) approaches infinity. The first term will always be 3, and the second and third terms will approach 0. Therefore, we can conclude that \(f(n) = \Omega(n^2)\).&lt;/p&gt;
&lt;h3 id=&#34;and-theta-notation&#34;&gt;$Θ$-notation&lt;/h3&gt;
&lt;p&gt;Lastly, the notation used for an &lt;strong&gt;asymptotically tight bound&lt;/strong&gt; is \(\Theta(g(n)) = \{f(n) : \exists c_1, c_2 &amp;gt; 0, n_0 &amp;gt; 0 \text{ such that } 0 \leq c_1g(n) \leq f(n) \leq c_2g(n) \text{ for all } n \geq n_0\}\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-09-26_18-23-25_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Visualization of $&amp;amp;Theta;$-notation (source: Cormen et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Visualization of $Θ$-notation (source: Cormen et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;We had mentioned previously that if \(f(n) = \Omega(g(n))\) and \(f(n) = O(g(n))\), then \(f(n) = \Theta(g(n))\). This is formalized in the following theorem, as stated in Cormen et al.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For any two functions \(f(n)\) and \(g(n)\), we have \(f(n) = \Theta(g(n))\) if and only if \(f(n) = O(g(n))\) and \(f(n) = \Omega(g(n))\).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;function-properties&#34;&gt;Function Properties&lt;/h3&gt;
&lt;p&gt;The following properties are useful when analyzing the asymptotic behavior of functions.&lt;/p&gt;
&lt;h4 id=&#34;transitivity&#34;&gt;Transitivity&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;If \(f(n) = O(g(n))\) and \(g(n) = O(h(n))\), then \(f(n) = O(h(n))\).&lt;/li&gt;
&lt;li&gt;If \(f(n) = \Omega(g(n))\) and \(g(n) = \Omega(h(n))\), then \(f(n) = \Omega(h(n))\).&lt;/li&gt;
&lt;li&gt;If \(f(n) = \Theta(g(n))\) and \(g(n) = \Theta(h(n))\), then \(f(n) = \Theta(h(n))\).&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;reflexivity&#34;&gt;Reflexivity&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;\(f(n) = O(f(n))\)&lt;/li&gt;
&lt;li&gt;\(f(n) = \Omega(f(n))\)&lt;/li&gt;
&lt;li&gt;\(f(n) = \Theta(f(n))\)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;symmetry&#34;&gt;Symmetry&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;\(f(n) = \Theta(g(n))\) if and only if \(g(n) = \Theta(f(n))\).&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;transpose-symmetry&#34;&gt;Transpose Symmetry&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;\(f(n) = O(g(n))\) if and only if \(g(n) = \Omega(f(n))\).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;common-functions&#34;&gt;Common Functions&lt;/h2&gt;
&lt;p&gt;The functions used to describe both time and space complexity are visualized below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-09-26_19-11-32_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Common functions used in complexity analysis (source: Wikipedia)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Common functions used in complexity analysis (source: Wikipedia)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Introduction to Algorithms</title>
      <link>https://ajdillhoff.github.io/notes/introduction_to_complexity_analysis/</link>
      <pubDate>Tue, 19 Sep 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/introduction_to_complexity_analysis/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-to-algorithms&#34;&gt;Introduction to Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#insertion-sort&#34;&gt;Insertion Sort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-sorting-numbers&#34;&gt;Example: Sorting Numbers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#worst-case-analysis&#34;&gt;Worst-Case Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#best-case-analysis&#34;&gt;Best-Case Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rate-of-growth&#34;&gt;Rate of Growth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-analysis-of-selection-sort&#34;&gt;Example: Analysis of Selection Sort&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction-to-algorithms&#34;&gt;Introduction to Algorithms&lt;/h2&gt;
&lt;p&gt;One of the major goals of computer science is to solve important problems. In order to do that, we must be able to express those solutions both mathematically and in a way that can be executed by a computer. Further, those solutions need to be aware of the resources that are available to them. It does us no good to come up with a solution that could never be run by current hardware or executed in a reasonable amount of time.&lt;/p&gt;
&lt;p&gt;There are of course other considerations besides runtime. How much memory does the solution require? Does it require a lot of data to be stored on disk? What about distributed solutions that can be run on multiple machines? Some solutions can be so complex, that we must also consider their environmental impact. For example, Meta&amp;rsquo;s Llama 2 large language models required 3,311,616 combined GPU hours to train. They report that their total carbon emissions from training were 539 tons of CO2 equivalent (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Touvron et al. 2023&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;We begin our algorithmic journey by studying a simple sorting algorithm, insertion sort. First, we need to formally define the problem of sorting. Given a sequence of \(n\) objects \(A = \langle a_1, a_2, \ldots, a_n \rangle\), we want to rearrange the elements such that \(a_1&amp;rsquo; \leq a_2&amp;rsquo; \leq \ldots \leq a_n&amp;rsquo;\). We will assume that the elements are comparable, meaning that we can use the operators \(&amp;lt;\) and \(&amp;gt;\) to compare them. Some sets, such as the set of all real numbers, have a natural ordering. A useful programming language would provide the required comparison operators. For other types of elements, such as strings, this may not be the case. For example, how would you compare the strings &amp;ldquo;apple&amp;rdquo; and &amp;ldquo;banana&amp;rdquo;? In these cases, we will need to define our own comparison operators. Either way, we will assume that the comparison operators are available to us.&lt;/p&gt;
&lt;p&gt;This example follows the one given in Chapter 2 of Cormen et al. (2009).&lt;/p&gt;
&lt;h2 id=&#34;insertion-sort&#34;&gt;Insertion Sort&lt;/h2&gt;
&lt;p&gt;Insertion sort is defined as&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;insertion_sort&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, len(A)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; A[j] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            A[j &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[j]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A[j &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;example-sorting-numbers&#34;&gt;Example: Sorting Numbers&lt;/h2&gt;
&lt;p&gt;TODO: Add a step-by-step example of sorting a list of numbers.&lt;/p&gt;
&lt;h2 id=&#34;worst-case-analysis&#34;&gt;Worst-Case Analysis&lt;/h2&gt;
&lt;p&gt;Given the definition from above, we can compute \(T(n)\), the running time of the algorithm on an input of size \(n\). To do this, we need to sum the products of the cost of each statement and the number of times each statement is executed.&lt;/p&gt;
&lt;p&gt;At first glance, the first statement &lt;code&gt;for i in range(1, len(A))&lt;/code&gt; appears to execute \(n-1\) times since it starts at 1 and only goes up to, but not including, \(n\). Remember that the &lt;code&gt;for&lt;/code&gt; statement must be checked to see if it should exit, so the test is executed one more time than the number of iterations. Therefore, the first statement is executed \(n\) times. If we say that the cost to execute each check is \(c_1\), then the total cost of the first statement is \(c_1 n\).&lt;/p&gt;
&lt;p&gt;With the exception of the &lt;code&gt;while&lt;/code&gt; loop, the statement inside the &lt;code&gt;for&lt;/code&gt; loop is executed once per iteration. The cost of executing statement \(i\) is \(c_i\). Therefore, the total cost of the second statement is \(c_2 n\). The costs are updated in the code below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;insertion_sort&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, len(A)): &lt;span style=&#34;color:#75715e&#34;&gt;# c_1 n&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[i] &lt;span style=&#34;color:#75715e&#34;&gt;# c_2 n&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# c_3 n&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; A[j] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            A[j &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[j]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A[j &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key &lt;span style=&#34;color:#75715e&#34;&gt;# c_7 n&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For the &lt;code&gt;while&lt;/code&gt; loop, we can denote the number of times it runs by \(t_i\), where \(i\) is the iteration of the &lt;code&gt;for&lt;/code&gt; loop. If the &lt;code&gt;while&lt;/code&gt; condition check costs \(c_4\) and is executed \(t_i\) times for each &lt;code&gt;for&lt;/code&gt; loop iteration, the total cost is given as \(c_4 \sum_{i=1}^{n-1} t_i\).&lt;/p&gt;
&lt;p&gt;The statement inside the &lt;code&gt;while&lt;/code&gt; loop are executed 1 fewer times than the number of times the condition check is executed. Therefore, the total cost of the statements inside the &lt;code&gt;while&lt;/code&gt; loop is \(c_5 \sum_{i=1}^{n-1} (t_i - 1) + c_5 \sum_{i=1}^{n-1} (t_i - 1)\). The cost of the &lt;code&gt;while&lt;/code&gt; loop is updated in the code below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;insertion_sort&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, len(A)): &lt;span style=&#34;color:#75715e&#34;&gt;# c_1 * n&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[i] &lt;span style=&#34;color:#75715e&#34;&gt;# c_2 * (n-1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# c_3 * (n-1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; A[j] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; key: &lt;span style=&#34;color:#75715e&#34;&gt;# c_4 * [t_i for i in range(1, len(A))]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            A[j &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[j] &lt;span style=&#34;color:#75715e&#34;&gt;# c_5 * [t_i - 1 for i in range(1, len(A))]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# c_6 * [t_i - 1 for i in range(1, len(A))]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A[j &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key &lt;span style=&#34;color:#75715e&#34;&gt;# c_7 * (n-1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To get the total running time \(T(n)\), we sum up all of the costs.&lt;/p&gt;
&lt;p&gt;\begin{align}
T(n) &amp;amp;= c_1 n + c_2 (n-1) + c_3 (n-1) + c_4 \sum_{i=1}^{n-1} t_i + c_5 \sum_{i=1}^{n-1} (t_i - 1) + c_6 \sum_{i=1}^{n-1} (t_i - 1) + c_7 (n-1) \\
\end{align}&lt;/p&gt;
&lt;p&gt;This analysis is a good start, but it doesn&amp;rsquo;t paint the whole picture. The number of actual executions will depend on the input that is given. For example, what if the input is already sorted, or given in reverse order? It is common to express the worst-case runtime for a particular algorithm. For insertion sort, that is when the input is in reverse order. In this case, each element \(A[i]\) is compared to every other element in the sorted subarray. This means that \(t_i = i\) for every iteration of the &lt;code&gt;for&lt;/code&gt; loop. Therefore, the worst-case runtime is given as&lt;/p&gt;
&lt;p&gt;\begin{align}
T(n) &amp;amp;= c_1 n + c_2 (n-1) + c_3 (n-1) + c_4 \sum_{i=1}^{n-1} i + c_5 \sum_{i=1}^{n-1} (i - 1) + c_6 \sum_{i=1}^{n-1} (i - 1) + c_7 (n-1) \\
\end{align}&lt;/p&gt;
&lt;p&gt;To express this runtime solely in terms of \(n\), we can use the fact that \(\sum_{i=1}^{n-1} i = (\sum_{i=0}^{n-1} i) - 1 =  \frac{n(n-1)}{2} - 1\) and \(\sum_{i=1}^{n-1} (i - 1) = \sum_{i=0}^{n-2} i = \frac{n(n-1)}{2}\). This gives us&lt;/p&gt;
&lt;p&gt;\begin{align}
T(n) &amp;amp;= c_1 n + c_2 (n-1) + c_3 (n-1) + c_4 \left(\frac{n(n-1)}{2} - 1\right)\\
&amp;amp;+ c_5 \left(\frac{n(n-1)}{2}\right) + c_6 \left(\frac{n(n-1)}{2}\right) + c_7 (n-1) \\
&amp;amp;= \left(\frac{c_4}{2} + \frac{c_5}{2} + \frac{c_6}{2}\right)n^2 + \left(c_1 + c_2 + c_3 + \frac{c_4}{2} - \frac{c_5}{2} - \frac{c_6}{2} + c_7\right)n - (c_2 + c_3 + c_4 + c_7) \\
\end{align}&lt;/p&gt;
&lt;p&gt;With the appropriate choice of constants, we can express this as a quadratic function \(an^2 + bn + c\).&lt;/p&gt;
&lt;h2 id=&#34;best-case-analysis&#34;&gt;Best-Case Analysis&lt;/h2&gt;
&lt;p&gt;The best-case runtime for insertion sort is when the input is already sorted. In this case, the &lt;code&gt;while&lt;/code&gt; check is executed only once per iteration of the &lt;code&gt;for&lt;/code&gt; loop. That is, \(t_i = 1\) for every iteration of the &lt;code&gt;for&lt;/code&gt; loop. Therefore, the best-case runtime is given as&lt;/p&gt;
&lt;p&gt;\begin{align}
T(n) &amp;amp;= c_1 n + c_2 (n-1) + c_3 (n-1) + c_4 (n-1) + c_7 (n-1) \\
&amp;amp;= (c_1 + c_2 + c_3 + c_4 + c_7)n - (c_2 + c_3 + c_4 + c_7) \\
\end{align}&lt;/p&gt;
&lt;p&gt;Let \(a = c_1 + c_2 + c_3 + c_4 + c_7\) and $b = -(c_2 + c_3 + c_4 + c_7)$Then the best-case runtime is given as \(an + b\), a linear function of \(n\).&lt;/p&gt;
&lt;h2 id=&#34;rate-of-growth&#34;&gt;Rate of Growth&lt;/h2&gt;
&lt;p&gt;We can simplify how we express the runtime of both these cases by considering only the highest-order term. Consider the worst-case, \(T(n) = an^2 + bn + c\). As \(n\) grows, the term \(an^2\) will dominate the runtime, rendering the others insignificant by comparison. This simplification is typically expressed using \(\Theta\) notation. For the worst-case, we say that \(T(n) = \Theta(n^2)\). It is a compact way of stating that the runtime is proportional to \(n^2\) for large values of \(n\).&lt;/p&gt;
&lt;h2 id=&#34;example-analysis-of-selection-sort&#34;&gt;Example: Analysis of Selection Sort&lt;/h2&gt;
&lt;p&gt;Based on the analysis above, let&amp;rsquo;s check our understanding and see if we can characterize the runtime of another sorting algorithm, selection sort. Selection sort is defined as&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;selection_sort&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, len(A) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        min_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, len(A)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; A[j] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; A[min_index]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                min_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; j
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A[i], A[min_index] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[min_index], A[i]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first statement &lt;code&gt;for i in range(0, len(A) - 1)&lt;/code&gt; will be evaluated \(n\) times. With the exception of the inner &lt;code&gt;for&lt;/code&gt; loop, the rest of the statements in the scope of the first &lt;code&gt;for&lt;/code&gt; loop are executed once per iteration. Their costs are \(c_2\) and \(c_6\), respectively.&lt;/p&gt;
&lt;p&gt;The inner &lt;code&gt;for&lt;/code&gt; loop will be checked \(n-i\) times for each iteration of the outer &lt;code&gt;for&lt;/code&gt; loop. The cost of the condition check is \(c_3\). The cost of the statements inside the &lt;code&gt;for&lt;/code&gt; loop are \(c_4\) and \(c_5\). The &lt;code&gt;if&lt;/code&gt; check is evaluated for every iteration of the inner loop, but the statements inside the &lt;code&gt;if&lt;/code&gt; are only executed when the condition is true. We can denote this as \(t_i\), the number of times the &lt;code&gt;if&lt;/code&gt; condition is true for each iteration of the inner &lt;code&gt;for&lt;/code&gt; loop. The cost of the inner loop is given as&lt;/p&gt;
&lt;p&gt;\begin{align}
c_3 \sum_{i=1}^{n-1} (n-i) + c_4 \sum_{i=0}^{n-1} (n-i-1) + c_5 \sum_{i=0}^{n-1} t_i\\
\end{align}&lt;/p&gt;
&lt;p&gt;Combining this with the cost of the outer &lt;code&gt;for&lt;/code&gt; loop, we get&lt;/p&gt;
&lt;p&gt;\begin{align}
T(n) &amp;amp;= c_1 n + c_2 (n-1) + c_6 (n-1) + c_3 \sum_{i=0}^{n-1} (n-i) + c_4 \sum_{i=0}^{n-1} (n-i-1) + c_5 \sum_{i=0}^{n-1} t_i\\
\end{align}&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Touvron, Hugo, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, et al. 2023. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” arXiv. &lt;a href=&#34;https://doi.org/10.48550/arXiv.2307.09288&#34;&gt;https://doi.org/10.48550/arXiv.2307.09288&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Python Review Questions</title>
      <link>https://ajdillhoff.github.io/notes/python_exam_review/</link>
      <pubDate>Tue, 12 Sep 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/python_exam_review/</guid>
      <description>&lt;h1 id=&#34;python-review-questions&#34;&gt;Python Review Questions&lt;/h1&gt;
&lt;p&gt;The following questions are meant to help you review introductory concepts in Python. They are based on the &lt;a href=&#34;https://docs.python.org/3/tutorial/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Python 3 Tutorial&lt;/a&gt; and &lt;a href=&#34;https://docs.python.org/3/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Python 3 Documentation&lt;/a&gt; and were written to accompany a 5 lecture series on Python.&lt;/p&gt;
&lt;p&gt;There are three types of questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Verify the Code:&lt;/strong&gt; Determine the output of a code snippet.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fill in the Code:&lt;/strong&gt; Fill in the code to complete a code snippet.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create the Function:&lt;/strong&gt; Create a function that satisfies the given requirements.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;verify-the-code&#34;&gt;Verify the Code&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Operations on Strings&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Hello&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;World&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What will the output be?&lt;/p&gt;
&lt;details&gt;
 &lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Hello World World
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;+&lt;/code&gt; operator concatenates strings, and the &lt;code&gt;*&lt;/code&gt; operator repeats strings.&lt;/p&gt;
 &lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Looping Over Lists&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(i &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What will the output be?&lt;/p&gt;
&lt;details&gt;
 &lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;for&lt;/code&gt; loop iterates over the elements of the list.&lt;/p&gt;
 &lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Mutability of Strings&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;hello&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;s[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;H&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Is this code valid? If not, why?&lt;/p&gt;
&lt;details&gt;
 &lt;summary&gt;Solution&lt;/summary&gt;
&lt;p&gt;This code is not valid. Strings are immutable, so you cannot change their elements.&lt;/p&gt;
 &lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Static Methods vs. Instance Methods&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;MyClass&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;instance_method&lt;/span&gt;(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;instance method&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;@staticmethod&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;static_method&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;static method&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;obj &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; MyClass()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(obj&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;instance_method())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(MyClass&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;static_method())
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What will the output be?&lt;/p&gt;
&lt;details&gt;
 &lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;instance method
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;static method
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; Instance methods are called on an instance of a class, whereas static methods are called on the class itself.&lt;/p&gt;
 &lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;yield&lt;/code&gt; Keyword Example&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;my_gen&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;yield&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;A&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;yield&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;B&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;gen &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; my_gen()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(next(gen))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(next(gen))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(next(gen))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What will the output be?&lt;/p&gt;
 &lt;details&gt;
  &lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;B
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Traceback &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;most recent call last&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   File &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;&lt;/span&gt;, line 1, in &amp;lt;module&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;StopIteration
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;yield&lt;/code&gt; keyword is used to create generators. Generators are iterators that can be iterated over only once.&lt;/p&gt;
  &lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Type Validation&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;42&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(isinstance(x, int))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What will the output be?&lt;/p&gt;
&lt;details&gt;
 &lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;True
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;isinstance&lt;/code&gt; function checks if an object is of a certain type.&lt;/p&gt;
 &lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;input&lt;/code&gt; Always Returns a &lt;code&gt;str&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; input(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Enter a number: &amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(type(x) &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; int)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If the user enters &lt;code&gt;42&lt;/code&gt;, what will the output be?&lt;/p&gt;
 &lt;details&gt;
  &lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;False
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;input&lt;/code&gt; function always returns a &lt;code&gt;str&lt;/code&gt;, even if the user enters a number.&lt;/p&gt;
  &lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Search in List Example&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;my_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; my_list)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What will the output be?&lt;/p&gt;
 &lt;details&gt;
  &lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;True
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;in&lt;/code&gt; operator checks if an element is in a list.&lt;/p&gt;
  &lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Formatted Printing&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Alice&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;age &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;name&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;age&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; years old.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What will the output be?&lt;/p&gt;
 &lt;details&gt;
  &lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Alice is &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt; years old.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;f&lt;/code&gt; prefix allows you to use formatted strings.&lt;/p&gt;
  &lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Equality Versus &lt;code&gt;is&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(x &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; y)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(x &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; y)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What will the output be?&lt;/p&gt;
&lt;details&gt;
 &lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;True
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;False
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;==&lt;/code&gt; operator checks if two objects are equal, whereas the &lt;code&gt;is&lt;/code&gt; operator checks if two objects are the same object.&lt;/p&gt;
 &lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CSV Line to List Using &lt;code&gt;split&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;line &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;apple,banana,cherry&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;fruits &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; line&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;,&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(fruits)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What will the output be?&lt;/p&gt;
&lt;details&gt;
 &lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;apple&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;banana&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;cherry&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;split&lt;/code&gt; method splits a string into a list of strings using a delimiter.&lt;/p&gt;
 &lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep vs. Shallow Copy&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; copy
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, [&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; copy&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy(a)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; copy&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;deepcopy(a)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;99&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(b)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(c)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What will the output be?&lt;/p&gt;
&lt;details&gt;
 &lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1, &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;99, 3&lt;span style=&#34;color:#f92672&#34;&gt;]]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1, &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;2, 3&lt;span style=&#34;color:#f92672&#34;&gt;]]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;copy&lt;/code&gt; function creates a shallow copy, whereas the &lt;code&gt;deepcopy&lt;/code&gt; function creates a deep copy.&lt;/p&gt;
 &lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Access Class Variable vs. Instance Variable&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Dog&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    kind &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;canine&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, name):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; name
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Dog(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Fido&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(d&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;kind)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(d&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What will the output be?&lt;/p&gt;
&lt;details&gt;
 &lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;canine
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Fido
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;kind&lt;/code&gt; variable is a class variable, whereas the &lt;code&gt;name&lt;/code&gt; variable is an instance variable.&lt;/p&gt;
 &lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Accessing &lt;code&gt;global&lt;/code&gt; Variable&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;foo&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;global&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(x)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;foo()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What will the output be?&lt;/p&gt;
&lt;details&gt;
 &lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;global&lt;/code&gt; keyword allows you to access a global variable inside a function.&lt;/p&gt;
 &lt;/details&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fill-in-the-code&#34;&gt;Fill in the Code&lt;/h2&gt;
&lt;ol start=&#34;15&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Looping Over 2D Lists&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;matrix &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Fill in the code to print each element in the 2D list.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; matrix:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; element &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; row:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(element)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The outer loop iterates over the rows, and the inner loop iterates over the elements in each row.&lt;/p&gt;
&lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Static Methods vs. Instance Methods&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Calculator&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;@staticmethod&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;add&lt;/span&gt;(a, b):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; a &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Fill in the code to create an instance method that multiplies two numbers.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Calculator&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;@staticmethod&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;add&lt;/span&gt;(a, b):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; a &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;multiply&lt;/span&gt;(self, a, b):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; a &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; b
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; Instance methods take &lt;code&gt;self&lt;/code&gt; as the first argument.&lt;/p&gt;
&lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;List Comprehension for 2D List&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Fill in the code to create a 2D list with list comprehension.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# The 2D list should contain rows from 0 to 4 and columns from 0 to 4, &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# where each element is the sum of its row and column index.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;matrix &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[row &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; col &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; col &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The outer loop iterates over the rows, and the inner loop iterates over the columns.&lt;/p&gt;
&lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dictionary Add, Iterating Over Keys and Values&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;my_dict &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;apple&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;banana&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Fill in the code to add a key-value pair (&amp;#39;cherry&amp;#39;, 3) to my_dict and print all keys and values.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;my_dict[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;cherry&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; key &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; my_dict:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(key, my_dict[key])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;for&lt;/code&gt; loop iterates over the keys of the dictionary.&lt;/p&gt;
&lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Add List to Existing List (Zip vs. Extend)&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;list1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;list2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Fill in the code to append the elements of list2 to list1.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;list1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extend(list2)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;extend&lt;/code&gt; method appends the elements of a list to another list.&lt;/p&gt;
&lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Override Special Method So Class Can Be Sorted&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Person&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, name, age):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; name
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;age &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; age
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Fill in the code to make instances of this class sortable by age.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Person&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, name, age):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; name
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;age &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; age
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __lt__(self, other):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;age &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; other&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;age
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;__lt__&lt;/code&gt; method overrides the &lt;code&gt;&amp;lt;&lt;/code&gt; operator. Either this method or the &lt;code&gt;__gt__&lt;/code&gt; method must be defined for the class to be sortable.&lt;/p&gt;
&lt;/details&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;create-the-function&#34;&gt;Create the Function&lt;/h2&gt;
&lt;ol start=&#34;21&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reading Input From the User&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; read_numbers_from_user(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The sum is:&amp;#34;&lt;/span&gt;, sum(numbers))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Create the function &lt;code&gt;read_numbers_from_user&lt;/code&gt; that takes an integer ( n ) and reads ( n ) numbers from the user.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;read_numbers_from_user&lt;/span&gt;(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        numbers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(int(input(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Enter a number: &amp;#34;&lt;/span&gt;)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; numbers
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;input&lt;/code&gt; function reads a string from the user. The &lt;code&gt;int&lt;/code&gt; function converts a string to an integer.&lt;/p&gt;
&lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Match Statement vs. If-Elif-Else&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(match_fruit_color(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;apple&amp;#34;&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Create the function &lt;code&gt;match_fruit_color&lt;/code&gt; that takes a fruit name and returns its color using a match statement.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;match_fruit_color&lt;/span&gt;(fruit):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;match&lt;/span&gt; fruit:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;apple&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;red&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;banana&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;yellow&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;cherry&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;red&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; _:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;unknown&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;match&lt;/code&gt; statement is used to compare a value against a number of patterns. It is similar to the &lt;code&gt;switch&lt;/code&gt; statement in other languages.&lt;/p&gt;
&lt;details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Formatted Printing&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print_formatted_string(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;John&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;25&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Create the function &lt;code&gt;print_formatted_string&lt;/code&gt; that takes a name and an age and prints them in a formatted string.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;print_formatted_string&lt;/span&gt;(name, age):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;name&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;age&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; years old.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;f&lt;/code&gt; prefix allows you to use formatted strings.&lt;/p&gt;
&lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Type Validation&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(is_valid_number(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;42&amp;#34;&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Create the function &lt;code&gt;is_valid_number&lt;/code&gt; that takes a string and returns True if it can be converted to an integer or a float, otherwise False.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;is_valid_number&lt;/span&gt;(s):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        float(s)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ValueError&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;try&lt;/code&gt; statement allows you to handle exceptions. The &lt;code&gt;float&lt;/code&gt; function converts a string to a float. If a string could be converted to a &lt;code&gt;float&lt;/code&gt;, it can also be converted to an &lt;code&gt;int&lt;/code&gt;.&lt;/p&gt;
&lt;/details&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Accessing &lt;code&gt;global&lt;/code&gt; Variable&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;increment_global_x()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(x)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Create the function &lt;code&gt;increment_global_x&lt;/code&gt; that increments the global variable ( x ) by 1.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Solution&lt;/summary&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;increment_global_x&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;global&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt; The &lt;code&gt;global&lt;/code&gt; keyword allows you to access a global variable inside a function.&lt;/p&gt;
&lt;/details&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>NumPy: Basics</title>
      <link>https://ajdillhoff.github.io/notes/numpy_basics/</link>
      <pubDate>Sun, 10 Sep 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/numpy_basics/</guid>
      <description>&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/ajdillhoff/python-examples/blob/main/numpy_quickstart/basics.ipynb&#34;&gt;
  &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;/&gt;
&lt;/a&gt;
&lt;h1 id=&#34;numpy-quickstart&#34;&gt;&lt;code&gt;NumPy&lt;/code&gt; Quickstart&lt;/h1&gt;
&lt;p&gt;This notebook is a quick introduction to &lt;code&gt;NumPy&lt;/code&gt;. It is an interactive version of the &lt;a href=&#34;https://docs.scipy.org/doc/numpy/user/quickstart.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;NumPy Quickstart Tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All credits go to the original authors of the tutorial © Copyright 2008-2023, NumPy Developers.&lt;/p&gt;
&lt;h1 id=&#34;the-basics&#34;&gt;The Basics&lt;/h1&gt;
&lt;p&gt;NumPy’s main object is the homogeneous multidimensional array. It is a table of elements (usually numbers), all of the same type, indexed by a tuple of non-negative integers. In NumPy dimensions are called axes.&lt;/p&gt;
&lt;p&gt;For example, the array for the coordinates of a point in 3D space, &lt;code&gt;[1, 2, 1]&lt;/code&gt;, has one axis. That axis has 3 elements in it, so we say it has a length of 3. In the example pictured below, the array has 2 axes. The first axis has a length of 2, the second axis has a length of 3.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[[ 1., 0., 0.],
 [ 0., 1., 2.]]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;NumPy’s array class is called &lt;code&gt;ndarray&lt;/code&gt;. It is also known by the alias &lt;code&gt;array&lt;/code&gt;. Note that &lt;code&gt;numpy.array&lt;/code&gt; is not the same as the Standard Python Library class &lt;code&gt;array.array&lt;/code&gt;, which only handles one-dimensional arrays and offers less functionality. The more important attributes of an &lt;code&gt;ndarray&lt;/code&gt; object are:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ndarray.ndim&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;the number of axes (dimensions) of the array.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ndarray.shape&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;the dimensions of the array. This is a tuple of integers indicating the size of the array in each dimension. For a matrix with n rows and m columns, &lt;code&gt;shape&lt;/code&gt; will be &lt;code&gt;(n,m)&lt;/code&gt;. The length of the &lt;code&gt;shape&lt;/code&gt; tuple is therefore the number of axes, &lt;code&gt;ndim&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ndarray.size&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;the total number of elements of the array. This is equal to the product of the elements of &lt;code&gt;shape&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ndarray.dtype&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;an object describing the type of the elements in the array. One can create or specify dtype’s using standard Python types. Additionally NumPy provides types of its own. numpy.int32, numpy.int16, and numpy.float64 are some examples.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ndarray.itemsize&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;the size in bytes of each element of the array. For example, an array of elements of type &lt;code&gt;float64&lt;/code&gt; has &lt;code&gt;itemsize 8&lt;/code&gt; (=64/8), while one of type &lt;code&gt;complex32&lt;/code&gt; has &lt;code&gt;itemsize&lt;/code&gt; 4 (=32/8). It is equivalent to &lt;code&gt;ndarray.dtype.itemsize&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ndarray.data&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;the buffer containing the actual elements of the array. Normally, we won’t need to use this attribute because we will access the elements in an array using indexing facilities.&lt;/p&gt;
&lt;h2 id=&#34;an-example&#34;&gt;An example&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a.shape = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a.ndim = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ndim))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a.dtype.name = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dtype&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a.itemsize = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;itemsize))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a.size = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;size))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;type(a) = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(type(a)))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;a = 
[[ 0  1  2  3  4]
 [ 5  6  7  8  9]
 [10 11 12 13 14]]
a.shape = (3, 5)
a.ndim = 2
a.dtype.name = int64
a.itemsize = 8
a.size = 15
type(a) = &amp;lt;class &#39;numpy.ndarray&#39;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;array-creation&#34;&gt;Array Creation&lt;/h2&gt;
&lt;p&gt;There are several ways to create arrays.&lt;/p&gt;
&lt;p&gt;For example, you can create an array from a regular Python list or tuple using the &lt;code&gt;array&lt;/code&gt; function. The type of the resulting array is deduced from the type of the elements in the sequences.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a.dtype = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dtype))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;1.2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3.5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5.1&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(b))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b.dtype = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(b&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dtype))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;a = [2 3 4]
a.dtype = int64
b = [1.2 3.5 5.1]
b.dtype = float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A frequent error consists in calling array with multiple numeric arguments, rather than providing a single list of numbers as an argument.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# WRONG&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

/home/alex/dev/teaching/python-examples/numpy/quickstart.ipynb Cell 6 line 1
----&amp;gt; &amp;lt;a href=&#39;vscode-notebook-cell:/home/alex/dev/teaching/python-examples/numpy/quickstart.ipynb#X14sZmlsZQ%3D%3D?line=0&#39;&amp;gt;1&amp;lt;/a&amp;gt; a = np.array(1, 2, 3, 4)


TypeError: array() takes from 1 to 2 positional arguments but 4 were given
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]) &lt;span style=&#34;color:#75715e&#34;&gt;# RIGHT&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;array&lt;/code&gt; transforms sequences of sequences into two-dimensional arrays, sequences of sequences of sequences into three-dimensional arrays, and so on.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([(&lt;span style=&#34;color:#ae81ff&#34;&gt;1.5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), (&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;)])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(b))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;b = [[1.5 2.  3. ]
 [4.  5.  6. ]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The type of the array can also be explicitly specified at creation time:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]], dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;complex)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;c =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(c))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;c =
[[1.+0.j 2.+0.j]
 [3.+0.j 4.+0.j]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Often, the elements of an array are originally unknown, but its size is known. Hence, NumPy offers several functions to create arrays with initial placeholder content. These minimize the necessity of growing arrays, an expensive operation.&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;zeros&lt;/code&gt; creates an array full of zeros, the function &lt;code&gt;ones&lt;/code&gt; creates an array full of ones, and the function &lt;code&gt;empty&lt;/code&gt; creates an array whose initial content is random and depends on the state of the memory. By default, the dtype of the created array is &lt;code&gt;float64&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.zeros((3, 4)) =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros((&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;))))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.ones((2, 3, 4), dtype=np.int16) =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ones((&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;), dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;int16)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.empty((2, 3)) =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;empty((&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;))))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;np.zeros((3, 4)) =
[[0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]]
np.ones((2, 3, 4), dtype=np.int16) =
[[[1 1 1 1]
  [1 1 1 1]
  [1 1 1 1]]

 [[1 1 1 1]
  [1 1 1 1]
  [1 1 1 1]]]
np.empty((2, 3)) =
[[1.39069238e-309 1.39069238e-309 1.39069238e-309]
 [1.39069238e-309 1.39069238e-309 1.39069238e-309]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create sequences of numbers, NumPy provides the &lt;code&gt;arange&lt;/code&gt; function which is analogous to the Python built-in &lt;code&gt;range&lt;/code&gt;, but returns an array.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.arange(10, 30, 5) = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.arange(0, 2, 0.3) = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.3&lt;/span&gt;))) &lt;span style=&#34;color:#75715e&#34;&gt;# it accepts float arguments&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;np.arange(10, 30, 5) = [10 15 20 25]
np.arange(0, 2, 0.3) = [0.  0.3 0.6 0.9 1.2 1.5 1.8]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When &lt;code&gt;arange&lt;/code&gt; is used with floating point arguments, it is generally not possible to predict the number of elements obtained, due to the finite floating point precision. For this reason, it is usually better to use the function &lt;code&gt;linspace&lt;/code&gt; that receives as an argument the number of elements that we want, instead of the step:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pi
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.linspace(0, 2, 9) = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linspace(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;))) &lt;span style=&#34;color:#75715e&#34;&gt;# 9 numbers from 0 to 2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linspace(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;pi, &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# useful to evaluate function at lots of points&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sin(x)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;np.linspace(0, 2, 9) = [0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;print-arrays&#34;&gt;Print Arrays&lt;/h2&gt;
&lt;p&gt;When you print an array, NumPy displays it in a similar way to nested lists, but with the following layout:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the last axis is printed from left to right,&lt;/li&gt;
&lt;li&gt;the second-to-last is printed from top to bottom,&lt;/li&gt;
&lt;li&gt;the rest are also printed from top to bottom, with each slice separated from the next by an empty line.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One-dimensional arrays are then printed as rows, bidimensionals as matrices and tridimensionals as lists of matrices.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# 1d array&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# 2d array&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(b))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;24&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# 3d array&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;c =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(c))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;a =
[0 1 2 3 4 5]
b =
[[ 0  1  2]
 [ 3  4  5]
 [ 6  7  8]
 [ 9 10 11]]
c =
[[[ 0  1  2  3]
  [ 4  5  6  7]
  [ 8  9 10 11]]

 [[12 13 14 15]
  [16 17 18 19]
  [20 21 22 23]]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If an array is too large to be printed, NumPy automatically skips the central part of the array and only prints the corners:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.arange(10000) = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;10000&lt;/span&gt;)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.arange(10000).reshape(100, 100) =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;10000&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;)))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;np.arange(10000) = [   0    1    2 ... 9997 9998 9999]
np.arange(10000).reshape(100, 100) =
[[   0    1    2 ...   97   98   99]
 [ 100  101  102 ...  197  198  199]
 [ 200  201  202 ...  297  298  299]
 ...
 [9700 9701 9702 ... 9797 9798 9799]
 [9800 9801 9802 ... 9897 9898 9899]
 [9900 9901 9902 ... 9997 9998 9999]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To disable this behaviour and force NumPy to print the entire array, you can change the printing options using &lt;code&gt;set_printoptions&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sys
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_printoptions(threshold&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;maxsize) &lt;span style=&#34;color:#75715e&#34;&gt;# force NumPy to print the entire array&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;basic-operations&#34;&gt;Basic Operations&lt;/h2&gt;
&lt;p&gt;Arithmetic operators on arrays apply elementwise. A new array is created and filled with the result.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;40&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(b))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; a &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; b
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;c =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(c))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b**2 =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(b&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;10*np.sin(a) =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sin(a)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a &amp;lt; 35 =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;35&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;b =
[0 1 2 3]
c =
[20 29 38 47]
b**2 =
[0 1 4 9]
10*np.sin(a) =
[ 9.12945251 -9.88031624  7.4511316  -2.62374854]
a &amp;lt; 35 =
[ True  True False False]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unlike in many matrix languages, the product operator &lt;code&gt;*&lt;/code&gt; operates elementwise in NumPy arrays. The matrix product can be performed using the &lt;code&gt;@&lt;/code&gt; operator (in python &amp;gt;=3.5) or the dot function or method:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;B &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;A *  B =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(A &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; B)) &lt;span style=&#34;color:#75715e&#34;&gt;# elementwise product&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;A @ B =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(A &lt;span style=&#34;color:#f92672&#34;&gt;@&lt;/span&gt; B)) &lt;span style=&#34;color:#75715e&#34;&gt;# matrix product&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;A.dot(B) =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(A&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dot(B))) &lt;span style=&#34;color:#75715e&#34;&gt;# another matrix product&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;A *  B =
[[2 0]
 [0 4]]
A @ B =
[[5 4]
 [3 4]]
A.dot(B) =
[[5 4]
 [3 4]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some operations, such as &lt;code&gt;+=&lt;/code&gt; and &lt;code&gt;*=&lt;/code&gt;, act in place to modify an existing array rather than create a new one.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rg &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;default_rng(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# create instance of default random number generator&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ones((&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;int)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random((&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;*=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; a
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(b))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; b &lt;span style=&#34;color:#75715e&#34;&gt;# b is not automatically converted to integer type&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;a =
[[3 3 3]
 [3 3 3]]
b =
[[3.51182162 3.9504637  3.14415961]
 [3.94864945 3.31183145 3.42332645]]



---------------------------------------------------------------------------

UFuncTypeError                            Traceback (most recent call last)

/home/alex/dev/teaching/python-examples/numpy/quickstart.ipynb Cell 29 line 1
      &amp;lt;a href=&#39;vscode-notebook-cell:/home/alex/dev/teaching/python-examples/numpy/quickstart.ipynb#Y110sZmlsZQ%3D%3D?line=7&#39;&amp;gt;8&amp;lt;/a&amp;gt; b += a
     &amp;lt;a href=&#39;vscode-notebook-cell:/home/alex/dev/teaching/python-examples/numpy/quickstart.ipynb#Y110sZmlsZQ%3D%3D?line=9&#39;&amp;gt;10&amp;lt;/a&amp;gt; print(&amp;quot;b =\n{}&amp;quot;.format(b))
---&amp;gt; &amp;lt;a href=&#39;vscode-notebook-cell:/home/alex/dev/teaching/python-examples/numpy/quickstart.ipynb#Y110sZmlsZQ%3D%3D?line=11&#39;&amp;gt;12&amp;lt;/a&amp;gt; a += b


UFuncTypeError: Cannot cast ufunc &#39;add&#39; output from dtype(&#39;float64&#39;) to dtype(&#39;int64&#39;) with casting rule &#39;same_kind&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When operating with arrays of different types, the type of the resulting array corresponds to the more general or precise one (a behavior known as upcasting).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ones(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;int32)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linspace(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, pi, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b.dtype.name = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(b&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dtype&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; a &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;c =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(c))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;c.dtype.name = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(c&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dtype&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(c&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;j)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;d =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(d))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;d.dtype.name = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(d&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dtype&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;b.dtype.name = float64
c =
[1.         2.57079633 4.14159265]
c.dtype.name = float64
d =
[ 0.54030231+0.84147098j -0.84147098+0.54030231j -0.54030231-0.84147098j]
d.dtype.name = complex128
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Many unary operations, such as computing the sum of all the elements in the array, are implemented as methods of the &lt;code&gt;ndarray&lt;/code&gt; class.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random((&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a.sum() = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum()))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a.min() = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;min()))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a.max() = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max()))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;a =
[[0.82770259 0.40919914 0.54959369]
 [0.02755911 0.75351311 0.53814331]]
a.sum() = 3.1057109529998157
a.min() = 0.027559113243068367
a.max() = 0.8277025938204418
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, these operations apply to the array as though it were a list of numbers, regardless of its shape. However, by specifying the &lt;code&gt;axis&lt;/code&gt; parameter you can apply an operation along the specified axis of an array:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(b))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b.sum(axis=0) = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(b&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;))) &lt;span style=&#34;color:#75715e&#34;&gt;# sum of each column&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b.min(axis=1) = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(b&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;min(axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))) &lt;span style=&#34;color:#75715e&#34;&gt;# min of each row&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b.cumsum(axis=1) =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(b&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cumsum(axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))) &lt;span style=&#34;color:#75715e&#34;&gt;# cumulative sum along each row&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;b =
[[ 0  1  2  3]
 [ 4  5  6  7]
 [ 8  9 10 11]]
b.sum(axis=0) = [12 15 18 21]
b.min(axis=1) = [0 4 8]
b.cumsum(axis=1) =
[[ 0  1  3  6]
 [ 4  9 15 22]
 [ 8 17 27 38]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;universal-functions&#34;&gt;Universal Functions&lt;/h2&gt;
&lt;p&gt;NumPy provides familiar mathematical functions such as sin, cos, and exp. In NumPy, these are called “universal functions”(&lt;code&gt;ufunc&lt;/code&gt;). Within NumPy, these functions operate elementwise on an array, producing an array as output.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;B &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;B = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(B))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.exp(B) = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(B)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.sqrt(B) = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt(B)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;C &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;2.&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1.&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4.&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.add(B, C) = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add(B, C)))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;B = [0 1 2]
np.exp(B) = [1.         2.71828183 7.3890561 ]
np.sqrt(B) = [0.         1.         1.41421356]
np.add(B, C) = [2. 0. 6.]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;indexing-slicing-and-iterating&#34;&gt;Indexing, Slicing and Iterating&lt;/h2&gt;
&lt;p&gt;One-dimensional arrays can be indexed, sliced and iterated over, much like lists and other Python sequences.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a[2] = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a[2:5] = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a[:&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# equivalent to a[0:6:2] = -1000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#75715e&#34;&gt;# from start to position 6, exclusive, set every 2nd element to -1000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a[ : :-1] = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a[ : :&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])) &lt;span style=&#34;color:#75715e&#34;&gt;# reversed a&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; a:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(i&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3.&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;a = [  0   1   8  27  64 125 216 343 512 729]
a[2] = 8
a[2:5] = [ 8 27 64]
a = [-1000     1 -1000    27 -1000   125   216   343   512   729]
a[ : :-1] = [  729   512   343   216   125 -1000    27 -1000     1 -1000]
nan
1.0
nan
3.0
nan
4.999999999999999
5.999999999999999
6.999999999999999
7.999999999999999
8.999999999999998


/tmp/ipykernel_15700/4153428083.py:14: RuntimeWarning: invalid value encountered in power
  print(i**(1/3.))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Multidimensional arrays can have one index per axis. These indices are given in a tuple separated by commas:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;f&lt;/span&gt;(x, y):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fromfunction(f, (&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;), dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;int)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(b))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b[2, 3] = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(b[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b[0:5, 1] = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(b[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])) &lt;span style=&#34;color:#75715e&#34;&gt;# each row in the second column of b&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b[ : , 1] = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(b[ : , &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])) &lt;span style=&#34;color:#75715e&#34;&gt;# equivalent to the previous example&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b[1:3, : ] =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(b[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, : ])) &lt;span style=&#34;color:#75715e&#34;&gt;# each column in the second and third row of b&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;b =
[[ 0  1  2  3]
 [10 11 12 13]
 [20 21 22 23]
 [30 31 32 33]
 [40 41 42 43]]
b[2, 3] = 23
b[0:5, 1] = [ 1 11 21 31 41]
b[ : , 1] = [ 1 11 21 31 41]
b[1:3, : ] =
[[10 11 12 13]
 [20 21 22 23]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When fewer indices are provided than the number of axes, the missing indices are considered complete slices:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b[-1] = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(b[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])) &lt;span style=&#34;color:#75715e&#34;&gt;# the last row. Equivalent to b[-1, : ]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;b[-1] = [40 41 42 43]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The expression within brackets in &lt;code&gt;b[i]&lt;/code&gt; is treated as an &lt;code&gt;i&lt;/code&gt; followed by as many instances of &lt;code&gt;:&lt;/code&gt; as needed to represent the remaining axes. NumPy also allows you to write this using dots as &lt;code&gt;b[i,...]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;dots&lt;/code&gt; (&lt;code&gt;...&lt;/code&gt;) represent as many colons as needed to produce a complete indexing tuple. For example, if &lt;code&gt;x&lt;/code&gt; is an array with 5 axes, then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x[1,2,...]&lt;/code&gt; is equivalent to &lt;code&gt;x[1,2,:,:,:]&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x[...,3]&lt;/code&gt; to &lt;code&gt;x[:,:,:,:,3]&lt;/code&gt; and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x[4,...,5,:]&lt;/code&gt; to &lt;code&gt;x[4,:,:,5,:]&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[[  &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,  &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,  &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], &lt;span style=&#34;color:#75715e&#34;&gt;# a 3D array (two stacked 2D arrays)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                [ &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;13&lt;/span&gt;]],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                [[&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;101&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;102&lt;/span&gt;],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                [&lt;span style=&#34;color:#ae81ff&#34;&gt;110&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;112&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;113&lt;/span&gt;]]])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;c.shape = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(c&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;c[1,...] =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(c[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;])) &lt;span style=&#34;color:#75715e&#34;&gt;# same as c[1,:,:] or c[1]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;c[...,2] =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(c[&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;])) &lt;span style=&#34;color:#75715e&#34;&gt;# same as c[:,:,2]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;c.shape = (2, 2, 3)
c[1,...] =
[[100 101 102]
 [110 112 113]]
c[...,2] =
[[  2  13]
 [102 113]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Iterating over multidimensional arrays is done with respect to the first axis:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; b:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(row)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[0 1 2 3]
[10 11 12 13]
[20 21 22 23]
[30 31 32 33]
[40 41 42 43]
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>NumPy: Copies and Views</title>
      <link>https://ajdillhoff.github.io/notes/numpy_copies_and_views/</link>
      <pubDate>Sun, 10 Sep 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/numpy_copies_and_views/</guid>
      <description>&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/ajdillhoff/python-examples/blob/main/numpy_quickstart/copies_and_views.ipynb&#34;&gt;
  &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;/&gt;
&lt;/a&gt;
&lt;h1 id=&#34;numpy-quickstart&#34;&gt;&lt;code&gt;NumPy&lt;/code&gt; Quickstart&lt;/h1&gt;
&lt;p&gt;This notebook is a quick introduction to &lt;code&gt;NumPy&lt;/code&gt;. It is an interactive version of the &lt;a href=&#34;https://docs.scipy.org/doc/numpy/user/quickstart.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;NumPy Quickstart Tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All credits go to the original authors of the tutorial © Copyright 2008-2023, NumPy Developers.&lt;/p&gt;
&lt;h1 id=&#34;copies-and-views&#34;&gt;Copies and Views&lt;/h1&gt;
&lt;p&gt;When operating and manipulating arrays, their data is sometimes copied into a new array and sometimes not. This is often a source of confusion for beginners. There are three cases:&lt;/p&gt;
&lt;h2 id=&#34;no-copy-at-all&#34;&gt;No Copy at All&lt;/h2&gt;
&lt;p&gt;Simple assignments make no copy of array objects or of their data.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              [&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              [&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;11&lt;/span&gt;]])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; a &lt;span style=&#34;color:#75715e&#34;&gt;# no new object is created&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(b &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; a) &lt;span style=&#34;color:#75715e&#34;&gt;# a and b are two names for the same ndarray object&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Python passes mutable objects as references, so function calls make no copy.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;f&lt;/span&gt;(x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(id(x))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(id(a)) &lt;span style=&#34;color:#75715e&#34;&gt;# id is a unique identifier of an object&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;f(a) &lt;span style=&#34;color:#75715e&#34;&gt;# a is passed to the function under the name x&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;139846954913200
139846954913200
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;view-or-shallow-copy&#34;&gt;View or Shallow Copy&lt;/h2&gt;
&lt;p&gt;Different array objects can share the same data. The &lt;code&gt;view&lt;/code&gt; method creates a new array object that looks at the same data.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;view() &lt;span style=&#34;color:#75715e&#34;&gt;# c is a view of the data owned by a&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;c is a = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(c &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; a))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;c.base is a = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(c&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;base &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; a)) &lt;span style=&#34;color:#75715e&#34;&gt;# c is a view of the data owned by a&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;c.flags.owndata = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(c&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;flags&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;owndata)) &lt;span style=&#34;color:#75715e&#34;&gt;# c does not own the data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; c&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape((&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;)) &lt;span style=&#34;color:#75715e&#34;&gt;# a&amp;#39;s shape doesn&amp;#39;t change&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a.shape = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;c[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1234&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# a&amp;#39;s data changes&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;c is a = False
c.base is a = True
c.flags.owndata = False
a.shape = (3, 4)
a =
[[   0    1    2    3]
 [1234    5    6    7]
 [   8    9   10   11]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Slicing an array returns a view of it:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; a[:, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;s[:] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# s[:] is a view of s. Note the difference between s=10 and s[:]=10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;a =
[[   0   10   10    3]
 [1234   10   10    7]
 [   8   10   10   11]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;deep-copy&#34;&gt;Deep Copy&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;copy&lt;/code&gt; method makes a complete copy of the array and its data.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy() &lt;span style=&#34;color:#75715e&#34;&gt;# a new array object with new data is created&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;d is a = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(d &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; a))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;d.base is a = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(d&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;base &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; a)) &lt;span style=&#34;color:#75715e&#34;&gt;# d doesn&amp;#39;t share anything with a&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;d[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;9999&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a =&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;d is a = False
d.base is a = False
a =
[[   0   10   10    3]
 [1234   10   10    7]
 [   8   10   10   11]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sometimes &lt;code&gt;copy&lt;/code&gt; should be called after slicing if the original array is not required anymore. For example, suppose a is a huge intermediate result and the final result b only contains a small fraction of a, a deep copy should be made when constructing b with slicing:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(int(&lt;span style=&#34;color:#ae81ff&#34;&gt;1e8&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; a[:&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;del&lt;/span&gt; a &lt;span style=&#34;color:#75715e&#34;&gt;# the memory of ``a`` can be released.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If &lt;code&gt;b = a[:100]&lt;/code&gt; is used instead, &lt;code&gt;a&lt;/code&gt; is referenced by &lt;code&gt;b&lt;/code&gt; and will persist in memory even if &lt;code&gt;del a&lt;/code&gt; is executed.&lt;/p&gt;
&lt;h2 id=&#34;functions-and-methods-overview&#34;&gt;Functions and Methods Overview&lt;/h2&gt;
&lt;p&gt;See &lt;a href=&#34;https://docs.scipy.org/doc/numpy/reference/routines.html#routines&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Routines&lt;/a&gt; for the full list of routines available in NumPy.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NumPy: Shape Manipulation</title>
      <link>https://ajdillhoff.github.io/notes/numpy_shape_manipulation/</link>
      <pubDate>Sun, 10 Sep 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/numpy_shape_manipulation/</guid>
      <description>&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/ajdillhoff/python-examples/blob/main/numpy_quickstart/shape_manipulation.ipynb&#34;&gt;
  &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;/&gt;
&lt;/a&gt;
# `NumPy` Quickstart
&lt;p&gt;This notebook is a quick introduction to &lt;code&gt;NumPy&lt;/code&gt;. It is an interactive version of the &lt;a href=&#34;https://docs.scipy.org/doc/numpy/user/quickstart.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;NumPy Quickstart Tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All credits go to the original authors of the tutorial © Copyright 2008-2023, NumPy Developers.&lt;/p&gt;
&lt;h1 id=&#34;shape-manipulation&#34;&gt;Shape Manipulation&lt;/h1&gt;
&lt;h2 id=&#34;changing-the-shape-of-an-array&#34;&gt;Changing the shape of an array&lt;/h2&gt;
&lt;p&gt;An array has a shape given by the number of elements along each axis:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;floor(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random((&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a.shape = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;a = 
[[6. 7. 2. 1.]
 [5. 9. 4. 9.]
 [3. 5. 3. 2.]]
a.shape = (3, 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The shape of an array can be changed with various commands. Note that the following three commands all return a modified array, but do not change the original array:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a.ravel() = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ravel())) &lt;span style=&#34;color:#75715e&#34;&gt;# flatten the array&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a.reshape(6,2) = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))) &lt;span style=&#34;color:#75715e&#34;&gt;# reshape the array&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a.T = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;T)) &lt;span style=&#34;color:#75715e&#34;&gt;# transpose the array&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a.T.shape = &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;T&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;a.ravel() = [6. 7. 2. 1. 5. 9. 4. 9. 3. 5. 3. 2.]
a.reshape(6,2) = 
[[6. 7.]
 [2. 1.]
 [5. 9.]
 [4. 9.]
 [3. 5.]
 [3. 2.]]
a.T = 
[[6. 5. 3.]
 [7. 9. 5.]
 [2. 4. 3.]
 [1. 9. 2.]]
a.T.shape = (4, 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The order of the elements in the array resulting from &lt;code&gt;ravel&lt;/code&gt; is normally “C-style”, that is, the rightmost index “changes the fastest”, so the element after &lt;code&gt;a[0,0]&lt;/code&gt; is &lt;code&gt;a[0,1]&lt;/code&gt;. If the array is reshaped to some other shape, again the array is treated as “C-style”. NumPy normally creates arrays stored in this order, so &lt;code&gt;ravel&lt;/code&gt; will usually not need to copy its argument, but if the array was made by taking slices of another array or created with unusual options, it may need to be copied. The functions &lt;code&gt;ravel&lt;/code&gt; and &lt;code&gt;reshape&lt;/code&gt; can also be instructed, using an optional argument, to use FORTRAN-style arrays, in which the leftmost index changes the fastest.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;reshape&lt;/code&gt; function returns its argument with a modified shape, whereas the &lt;code&gt;ndarray.resize&lt;/code&gt; method modifies the array itself:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;resize((&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;)) &lt;span style=&#34;color:#75715e&#34;&gt;# resize the array&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a.resize((2,6)) = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;a.resize((2,6)) = 
[[6. 7. 2. 1. 5. 9.]
 [4. 9. 3. 5. 3. 2.]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If a dimension is given as -1 in a reshaping operation, the other dimensions are automatically calculated:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a.reshape(3, -1) = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;a.reshape(3, -1) = 
[[6. 7. 2. 1.]
 [5. 9. 4. 9.]
 [3. 5. 3. 2.]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;stacking-together-different-arrays&#34;&gt;Stacking together different arrays&lt;/h2&gt;
&lt;p&gt;Several arrays can be stacked together along different axes:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;floor(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random((&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;floor(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random((&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(b))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.vstack((a,b)) = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;vstack((a,b))))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.hstack((a,b)) = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;hstack((a,b))))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;a = 
[[8. 0.]
 [5. 1.]]
b = 
[[7. 7.]
 [0. 1.]]
np.vstack((a,b)) = 
[[8. 0.]
 [5. 1.]
 [7. 7.]
 [0. 1.]]
np.hstack((a,b)) = 
[[8. 0. 7. 7.]
 [5. 1. 0. 1.]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;column_stack&lt;/code&gt; stacks 1D arrays as columns into a 2D array. It is equivalent to &lt;code&gt;hstack&lt;/code&gt; only for 2D arrays:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; newaxis
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.column_stack((a,b)) = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;column_stack((a,b)))) &lt;span style=&#34;color:#75715e&#34;&gt;# with 2D arrays&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;4.&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2.&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;3.&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;8.&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.column_stack((a,b)) = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;column_stack((a,b)))) &lt;span style=&#34;color:#75715e&#34;&gt;# returns a 2D array&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.hstack((a,b)) = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;hstack((a,b)))) &lt;span style=&#34;color:#75715e&#34;&gt;# returns a 1D array&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a[:,newaxis] = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a[:,newaxis])) &lt;span style=&#34;color:#75715e&#34;&gt;# view a as a 2D column vector&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.column_stack((a[:,newaxis],b[:,newaxis])) = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;column_stack((a[:,newaxis],b[:,newaxis]))))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.hstack((a[:,newaxis],b[:,newaxis])) = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;hstack((a[:,newaxis],b[:,newaxis]))))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;np.column_stack((a,b)) = 
[[4. 3.]
 [2. 8.]]
np.column_stack((a,b)) = 
[[4. 3.]
 [2. 8.]]
np.hstack((a,b)) = 
[4. 2. 3. 8.]
a[:,newaxis] = 
[[4.]
 [2.]]
np.column_stack((a[:,newaxis],b[:,newaxis])) = 
[[4. 3.]
 [2. 8.]]
np.hstack((a[:,newaxis],b[:,newaxis])) = 
[[4. 3.]
 [2. 8.]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In general, for arrays of with more than two dimensions, &lt;code&gt;hstack&lt;/code&gt; stacks along their second axes, &lt;code&gt;vstack&lt;/code&gt; stacks along their first axes, and &lt;code&gt;concatenate&lt;/code&gt; allows for an optional arguments giving the number of the axis along which the concatenation should happen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In complex cases, &lt;code&gt;r_&lt;/code&gt; and &lt;code&gt;c_&lt;/code&gt; are useful for creating arrays by stacking numbers along one axis. They allow the use of range literals &lt;code&gt;:&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.r_[1:4,0,4] = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;r_[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;])) &lt;span style=&#34;color:#75715e&#34;&gt;# concatenate along the first axis&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;np.r_[1:4,0,4] = 
[1 2 3 0 4]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When used with arrays as arguments, &lt;code&gt;r_&lt;/code&gt; and &lt;code&gt;c_&lt;/code&gt; are similar to &lt;code&gt;vstack&lt;/code&gt; and &lt;code&gt;hstack&lt;/code&gt; in their default behavior, but allow for an optional argument giving the number of the axis along which to concatenate.&lt;/p&gt;
&lt;h2 id=&#34;splitting-one-array-into-several-smaller-ones&#34;&gt;Splitting one array into several smaller ones&lt;/h2&gt;
&lt;p&gt;Using &lt;code&gt;hsplit&lt;/code&gt;, you can split an array along its horizontal axis, either by specifying the number of equally shaped arrays to return, or by specifying the columns after which the division should occur:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;floor(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random((&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(a))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# split a into 3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.hsplit(a,3) = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;hsplit(a,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# split a after the third and the fourth column&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;np.hsplit(a,(3,4)) = &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;hsplit(a,(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;))))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;a = 
[[7. 0. 8. 1. 4. 7. 8. 1. 1. 8. 1. 2.]
 [6. 7. 7. 5. 7. 5. 0. 7. 7. 4. 6. 1.]]
np.hsplit(a,3) = 
[array([[7., 0., 8., 1.],
       [6., 7., 7., 5.]]), array([[4., 7., 8., 1.],
       [7., 5., 0., 7.]]), array([[1., 8., 1., 2.],
       [7., 4., 6., 1.]])]
np.hsplit(a,(3,4)) = 
[array([[7., 0., 8.],
       [6., 7., 7.]]), array([[1.],
       [5.]]), array([[4., 7., 8., 1., 1., 8., 1., 2.],
       [7., 5., 0., 7., 7., 4., 6., 1.]])]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;vsplit&lt;/code&gt; splits along the vertical axis, and &lt;code&gt;array_split&lt;/code&gt; allows one to specify along which axis to split.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example: Writing a Data Loader in Python</title>
      <link>https://ajdillhoff.github.io/notes/dataloader/</link>
      <pubDate>Mon, 04 Sep 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/dataloader/</guid>
      <description>&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/ajdillhoff/python-examples/blob/main/examples/dataloader.ipynb&#34;&gt;
  &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;/&gt;
&lt;/a&gt;
&lt;h1 id=&#34;loading-data-for-ml-applications&#34;&gt;Loading Data for ML Applications&lt;/h1&gt;
&lt;p&gt;In this notebook, we will implement code to load data for ML applications. Following the approach used by &lt;code&gt;PyTorch&lt;/code&gt;, we will implement a &lt;code&gt;Dataset&lt;/code&gt; class and a &lt;code&gt;DataLoader&lt;/code&gt; class. The &lt;code&gt;Dataset&lt;/code&gt; class will be used to load the data and the &lt;code&gt;DataLoader&lt;/code&gt; class will be used to iterate over the data in batches.&lt;/p&gt;
&lt;p&gt;We will test it on a simple image dataset.&lt;/p&gt;
&lt;h2 id=&#34;the-dataset-class&#34;&gt;The &lt;code&gt;Dataset&lt;/code&gt; Class&lt;/h2&gt;
&lt;p&gt;First, we need some way of representing how each individual item will be pre-processed as the dataloader iterates over the data. We will do this by creating a &lt;code&gt;Dataset&lt;/code&gt; class. Since this class represents multiple items in our dataset, we will need to define the following special methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;__len__&lt;/code&gt;: returns the length of the dataset&lt;/li&gt;
&lt;li&gt;&lt;code&gt;__getitem__&lt;/code&gt;: returns the item at a given index&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-intel-image-classification-dataset&#34;&gt;The Intel Image Classification Dataset&lt;/h2&gt;
&lt;p&gt;We will use the &lt;a href=&#34;https://www.kaggle.com/datasets/puneet6060/intel-image-classification&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Intel Image Classification Dataset&lt;/a&gt; from Kaggle. This dataset contains images of natural scenes around the world. The dataset contains 25,000 images of size 150x150 distributed under 6 categories. The dataset is divided into a training set and a test set. The training set contains 14k images. The images are organized under folder representing each category. When initializing our dataset, it should iterate through the folders to enumerate all the images and their corresponding labels.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; os
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;IntelDataset&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, data_path):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data_path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data_path
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;classes, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;class_to_idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_find_classes(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data_path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;samples &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_make_dataset(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data_path, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;class_to_idx)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __len__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; len(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;samples)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __getitem__(self, idx):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;NotImplementedError&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_find_classes&lt;/span&gt;(self, path):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Summarized from torchvision.datasets.ImageFolder&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        classes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [d&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; d &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scandir(path) &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; d&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;is_dir()]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        classes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sort()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        class_to_idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {cls_name: i &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i, cls_name &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate(classes)}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; classes, class_to_idx
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_make_dataset&lt;/span&gt;(self, dir, class_to_idx):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Summarized from torchvision.datasets.ImageFolder&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        images &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        dir &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;expanduser(dir)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; target &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sorted(os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;listdir(dir)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(dir, target)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isdir(d):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; root, _, fnames &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sorted(os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;walk(d)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; fname &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sorted(fnames):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(root, fname)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    item &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (path, class_to_idx[target])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    images&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(item)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; images
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here is the first iteration of our dataset. Let&amp;rsquo;s test it on the Intel Image Classification dataset available here: &lt;a href=&#34;https://www.kaggle.com/datasets/puneet6060/intel-image-classification&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.kaggle.com/datasets/puneet6060/intel-image-classification&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We will use the training samples to test our &lt;code&gt;Dataset&lt;/code&gt; class. There are 14,000 images in the training set over 6 categories. Each category is contained in its own subfolder.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; IntelDataset(data_path&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;data/seg_train&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;classes)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;class_to_idx)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(len(dataset))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Sample the first 5 items&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;samples[i])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[&#39;buildings&#39;, &#39;forest&#39;, &#39;glacier&#39;, &#39;mountain&#39;, &#39;sea&#39;, &#39;street&#39;]
{&#39;buildings&#39;: 0, &#39;forest&#39;: 1, &#39;glacier&#39;: 2, &#39;mountain&#39;: 3, &#39;sea&#39;: 4, &#39;street&#39;: 5}
14034
(&#39;data/seg_train/buildings/0.jpg&#39;, 0)
(&#39;data/seg_train/buildings/10006.jpg&#39;, 0)
(&#39;data/seg_train/buildings/1001.jpg&#39;, 0)
(&#39;data/seg_train/buildings/10014.jpg&#39;, 0)
(&#39;data/seg_train/buildings/10018.jpg&#39;, 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;loading-the-images&#34;&gt;Loading the Images&lt;/h3&gt;
&lt;p&gt;Now that we can successfully initialize the dataset, we can implement &lt;code&gt;__getitem__&lt;/code&gt; to load the actual image and return a tuple of the image and its label. We will use the &lt;code&gt;PIL&lt;/code&gt; library to load the image and convert it to a &lt;code&gt;numpy&lt;/code&gt; array.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; os
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; PIL &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Image
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;IntelDataset&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, data_path):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data_path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data_path
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;classes, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;class_to_idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_find_classes(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data_path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;samples &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_make_dataset(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data_path, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;class_to_idx)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __len__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; len(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;samples)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __getitem__(self, idx):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        path, target &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;samples[idx]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(path)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;convert(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;RGB&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array(image)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; image, target
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_find_classes&lt;/span&gt;(self, path):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Summarized from torchvision.datasets.ImageFolder&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        classes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [d&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; d &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scandir(path) &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; d&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;is_dir()]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        classes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sort()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        class_to_idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {cls_name: i &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i, cls_name &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate(classes)}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; classes, class_to_idx
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_make_dataset&lt;/span&gt;(self, dir, class_to_idx):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Summarized from torchvision.datasets.ImageFolder&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        images &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        dir &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;expanduser(dir)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; target &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sorted(os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;listdir(dir)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(dir, target)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isdir(d):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; root, _, fnames &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sorted(os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;walk(d)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; fname &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sorted(fnames):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(root, fname)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    item &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (path, class_to_idx[target])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    images&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(item)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; images
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Sample 9 random images and display them with their labels&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; IntelDataset(data_path&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;data/seg_train&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;fig, axes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplots(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(len(dataset))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        image, target &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dataset[idx]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        axes[i, j]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(image)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        axes[i, j]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_title(dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;classes[target])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        axes[i, j]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;axis(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;off&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;
  &lt;img src=&#34;dataloader_files/dataloader_5_0.png&#34; alt=&#34;png&#34;&gt;
&lt;/p&gt;
&lt;h2 id=&#34;the-dataloader-class&#34;&gt;The &lt;code&gt;DataLoader&lt;/code&gt; Class&lt;/h2&gt;
&lt;p&gt;Now that we have a &lt;code&gt;Dataset&lt;/code&gt; to manage loading each individual item, we need to create a &lt;code&gt;DataLoader&lt;/code&gt; that is responsible for iterating over the dataset in batches.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;DataLoader&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, dataset, batch_size, shuffle&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dataset
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;batch_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch_size
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shuffle &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; shuffle
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __len__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; len(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataset) &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;batch_size
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __iter__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shuffle:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            indices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;permutation(len(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataset))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            indices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(len(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataset))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(len(self)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            batch_indices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; indices[i&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;batch_size:(i&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;batch_size]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            batch &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataset[idx] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; idx &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; batch_indices]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            images, targets &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; zip(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;batch)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            images &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stack(images, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            targets &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stack(targets, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;yield&lt;/span&gt; images, targets
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In this example, the &lt;code&gt;__iter__&lt;/code&gt; uses Python generators to yield a batch of data instead of overriding the &lt;code&gt;__next__&lt;/code&gt; method.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s test this with our dataset.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; IntelDataset(data_path&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;data/seg_train&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dataloader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataLoader(dataset, batch_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;, shuffle&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print the first 5 batches&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i, (images, targets) &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate(dataloader):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Batch &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;i&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; images shape: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;images&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Batch &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;i&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; targets shape: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;targets&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Batch 0 images shape: (32, 150, 150, 3)
Batch 0 targets shape: (32,)
Batch 1 images shape: (32, 150, 150, 3)
Batch 1 targets shape: (32,)
Batch 2 images shape: (32, 150, 150, 3)
Batch 2 targets shape: (32,)
Batch 3 images shape: (32, 150, 150, 3)
Batch 3 targets shape: (32,)
Batch 4 images shape: (32, 150, 150, 3)
Batch 4 targets shape: (32,)
Batch 5 images shape: (32, 150, 150, 3)
Batch 5 targets shape: (32,)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;cleaning-the-data&#34;&gt;Cleaning the Data&lt;/h2&gt;
&lt;p&gt;Our data loader failed because there are some images that are not $150 \times 150$. We need to figure out exactly what to do with those images. We can either remove them or resize them so that they are all the same size. We will resize them to $150 \times 150$. This will be easiest to do in the &lt;code&gt;__getitem__&lt;/code&gt; method of our &lt;code&gt;Dataset&lt;/code&gt; class.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; os
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; PIL &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Image
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;IntelDataset&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, data_path):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data_path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data_path
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;classes, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;class_to_idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_find_classes(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data_path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;samples &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_make_dataset(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data_path, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;class_to_idx)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __len__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; len(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;samples)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __getitem__(self, idx):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        path, target &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;samples[idx]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(path)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;convert(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;RGB&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Resize if the image is not 150x150&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;size &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;150&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;150&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;resize((&lt;span style=&#34;color:#ae81ff&#34;&gt;150&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;150&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array(image)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; image, target
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_find_classes&lt;/span&gt;(self, path):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Summarized from torchvision.datasets.ImageFolder&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        classes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [d&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; d &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scandir(path) &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; d&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;is_dir()]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        classes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sort()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        class_to_idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {cls_name: i &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i, cls_name &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate(classes)}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; classes, class_to_idx
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_make_dataset&lt;/span&gt;(self, dir, class_to_idx):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Summarized from torchvision.datasets.ImageFolder&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        images &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        dir &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;expanduser(dir)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; target &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sorted(os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;listdir(dir)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(dir, target)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isdir(d):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; root, _, fnames &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sorted(os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;walk(d)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; fname &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sorted(fnames):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(root, fname)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    item &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (path, class_to_idx[target])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    images&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(item)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; images
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; IntelDataset(data_path&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;data/seg_train&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dataloader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataLoader(dataset, batch_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;, shuffle&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Verify that our fix works!&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Only print the first 5 batches&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i, (images, targets) &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate(dataloader):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Batch &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;i&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; images shape: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;images&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Batch &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;i&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; targets shape: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;targets&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Batch 0 images shape: (32, 150, 150, 3)
Batch 0 targets shape: (32,)
Batch 1 images shape: (32, 150, 150, 3)
Batch 1 targets shape: (32,)
Batch 2 images shape: (32, 150, 150, 3)
Batch 2 targets shape: (32,)
Batch 3 images shape: (32, 150, 150, 3)
Batch 3 targets shape: (32,)
Batch 4 images shape: (32, 150, 150, 3)
Batch 4 targets shape: (32,)
Batch 5 images shape: (32, 150, 150, 3)
Batch 5 targets shape: (32,)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Object-Oriented Programming with Python</title>
      <link>https://ajdillhoff.github.io/notes/oop/</link>
      <pubDate>Mon, 04 Sep 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/oop/</guid>
      <description>&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/ajdillhoff/python-examples/blob/main/basics/oop.ipynb&#34;&gt;
  &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;/&gt;
&lt;/a&gt;
&lt;h1 id=&#34;classes&#34;&gt;Classes&lt;/h1&gt;
&lt;p&gt;When a class is defined, a namespace is created for it. All assignments to local variables are part of this namespace. The code below defines a class, creates an instance of the class, and calls a method on the instance.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Shape&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Represents any shape.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, color):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; color
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;orientation &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rotate&lt;/span&gt;(self, angle):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;orientation &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; angle
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Shape(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;red&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;s&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rotate(&lt;span style=&#34;color:#ae81ff&#34;&gt;45.0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(s&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;orientation)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;45.0
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;class-and-instance-variables&#34;&gt;Class and Instance Variables&lt;/h2&gt;
&lt;p&gt;The class above has two &lt;em&gt;instance&lt;/em&gt; variables, &lt;code&gt;color&lt;/code&gt; and &lt;code&gt;orientation&lt;/code&gt;. These variables are accessed using the &lt;code&gt;self&lt;/code&gt; keyword. The &lt;code&gt;self&lt;/code&gt; keyword is used to access instance variables and methods.&lt;/p&gt;
&lt;p&gt;Classes can also have &lt;em&gt;class&lt;/em&gt; variables that are accessible, and shared, by all instances of the class. Let&amp;rsquo;s add a class variable to the &lt;code&gt;Shape&lt;/code&gt; class.&lt;/p&gt;
&lt;h3 id=&#34;private-variables&#34;&gt;Private Variables&lt;/h3&gt;
&lt;p&gt;Python does not have a formal mechanism for describing a private variable. You can still create them using naming conventions. A common approach to creating private variables is to prefix each identifier with double underscores. If we wanted to make the &lt;code&gt;orientation&lt;/code&gt; variable private, we would rename it to &lt;code&gt;__orientation&lt;/code&gt;, for example.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Shape&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Represents any shape.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    max_area &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, color):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; color
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;orientation &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rotate&lt;/span&gt;(self, angle):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;orientation &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; angle
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Shape(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;red&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;s&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rotate(&lt;span style=&#34;color:#ae81ff&#34;&gt;45.0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;r &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Shape(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;blue&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(s&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;orientation)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Maximum area for a shape:&amp;#34;&lt;/span&gt;, Shape&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max_area)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;45.0
Maximum area for a shape: 100.0
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;special-methods&#34;&gt;Special Methods&lt;/h1&gt;
&lt;p&gt;We already saw one special method, &lt;code&gt;__init__()&lt;/code&gt;, that serves as our constructor for a class. There are several others that are useful for customizing our classes. They are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;__str__()&lt;/code&gt;: called when &lt;code&gt;str()&lt;/code&gt; is called on an instance of the class&lt;/li&gt;
&lt;li&gt;&lt;code&gt;__repr__()&lt;/code&gt;: called when &lt;code&gt;repr()&lt;/code&gt; is called on an instance of the class&lt;/li&gt;
&lt;li&gt;&lt;code&gt;__len__()&lt;/code&gt;: called when &lt;code&gt;len()&lt;/code&gt; is called on an instance of the class&lt;/li&gt;
&lt;li&gt;&lt;code&gt;__add__()&lt;/code&gt;: called when &lt;code&gt;+&lt;/code&gt; is used on two instances of the class&lt;/li&gt;
&lt;li&gt;&lt;code&gt;__eq__()&lt;/code&gt;: called when &lt;code&gt;==&lt;/code&gt; is used on two instances of the class&lt;/li&gt;
&lt;li&gt;&lt;code&gt;__lt__()&lt;/code&gt;: called when &lt;code&gt;&amp;lt;&lt;/code&gt; is used on two instances of the class&lt;/li&gt;
&lt;li&gt;&lt;code&gt;__gt__()&lt;/code&gt;: called when &lt;code&gt;&amp;gt;&lt;/code&gt; is used on two instances of the class&lt;/li&gt;
&lt;li&gt;&lt;code&gt;__le__()&lt;/code&gt;: called when &lt;code&gt;&amp;lt;=&lt;/code&gt; is used on two instances of the class&lt;/li&gt;
&lt;li&gt;&lt;code&gt;__ge__()&lt;/code&gt;: called when &lt;code&gt;&amp;gt;=&lt;/code&gt; is used on two instances of the class&lt;/li&gt;
&lt;li&gt;&lt;code&gt;__ne__()&lt;/code&gt;: called when &lt;code&gt;!=&lt;/code&gt; is used on two instances of the class&lt;/li&gt;
&lt;li&gt;&lt;code&gt;__hash__()&lt;/code&gt;: called when &lt;code&gt;hash()&lt;/code&gt; is called on an instance of the class&lt;/li&gt;
&lt;li&gt;&lt;code&gt;__bool__()&lt;/code&gt;: called when &lt;code&gt;bool()&lt;/code&gt; is called on an instance of the class&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s modify the &lt;code&gt;Shape&lt;/code&gt; class to add a few of these methods. We will also add an &lt;code&gt;area&lt;/code&gt; attribute so that we can override the comparison operators.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Shape&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Represents any shape.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    max_area &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, color, area):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; color
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;orientation &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; area
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rotate&lt;/span&gt;(self, angle):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;orientation &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; angle
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __eq__(self, other):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; other&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __lt__(self, other):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; other&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __gt__(self, other):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; other&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __le__(self, other):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; other&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __ge__(self, other):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; other&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __ne__(self, other):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; other&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __str__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Shape, color: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{0}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;, area: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{1}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    s1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Shape(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;red&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10.0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    s2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Shape(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;blue&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20.0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;s1 == s2:&amp;#34;&lt;/span&gt;, s1 &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; s2)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;s1 != s2:&amp;#34;&lt;/span&gt;, s1 &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; s2)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;s1 &amp;lt; s2:&amp;#34;&lt;/span&gt;, s1 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; s2)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;s1 &amp;gt; s2:&amp;#34;&lt;/span&gt;, s1 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; s2)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;s1 &amp;lt;= s2:&amp;#34;&lt;/span&gt;, s1 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; s2)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;s1 &amp;gt;= s2:&amp;#34;&lt;/span&gt;, s1 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; s2)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(s1)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(s2)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;s1 == s2: False
s1 != s2: True
s1 &amp;lt; s2: True
s1 &amp;gt; s2: False
s1 &amp;lt;= s2: True
s1 &amp;gt;= s2: False
Shape, color: red, area: 10.0
Shape, color: blue, area: 20.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we have defined the &lt;code&gt;&amp;lt;&lt;/code&gt; operator, &lt;code&gt;list.sort()&lt;/code&gt; can sort our shapes. If the &lt;code&gt;__lt__()&lt;/code&gt; operator was not defined, &lt;code&gt;list.sort()&lt;/code&gt; would use the &lt;code&gt;__gt__()&lt;/code&gt; operator. If neither are defined, attemping to sort would result in an error. Let&amp;rsquo;s add a few more and verify this.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; random
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;colors &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;red&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;blue&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;green&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;yellow&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;black&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;white&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Generate 10 shapes with random colors and areas&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;shapes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;choice(colors)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    area &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;uniform(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;100.0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    shapes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(Shape(color, area))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print the shapes, sorted by area&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; shape &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sorted(shapes):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(shape)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Shape, color: red, area: 13.697816464863
Shape, color: white, area: 42.56718610585648
Shape, color: white, area: 47.443134198872464
Shape, color: white, area: 53.85070279838825
Shape, color: yellow, area: 66.78631236791435
Shape, color: green, area: 70.55065950752918
Shape, color: blue, area: 73.19818592952365
Shape, color: white, area: 74.03228452807117
Shape, color: black, area: 86.72544463003362
Shape, color: red, area: 94.59245601130148
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;inheritance&#34;&gt;Inheritance&lt;/h1&gt;
&lt;p&gt;Inheritance allows us to create a specialized version of another class. Generally, this means that our specialized class has access to the methods and instance variables of the parent class. Let&amp;rsquo;s create a &lt;code&gt;Circle&lt;/code&gt; and &lt;code&gt;Square&lt;/code&gt; that inherit from shape. Their areas will be calculated based on their properties.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; math
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Shape&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Represents any shape.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    max_area &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, color):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; color
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;orientation &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rotate&lt;/span&gt;(self, angle):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;orientation &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; angle
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __eq__(self, other):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; other&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __lt__(self, other):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; other&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __gt__(self, other):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; other&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __le__(self, other):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; other&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __ge__(self, other):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; other&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __ne__(self, other):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; other&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __str__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Shape, color: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{0}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;, area: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{1}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Circle&lt;/span&gt;(Shape):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Represents a circle.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, color, radius):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        Shape&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__init__(self, color)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;radius &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; radius
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_area()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __str__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Circle, color: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{0}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;, area: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{1}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;, radius: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{2}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;radius)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_area&lt;/span&gt;(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pi &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;radius &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Rectangle&lt;/span&gt;(Shape):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Represents a rectangle.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, color, width, height):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        Shape&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__init__(self, color)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;width &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; width
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;height &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; height
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_area()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __str__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Rectangle, color: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{0}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;, area: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{1}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;, width: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{2}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;, height: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{3}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;color, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;area, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;width, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;height)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_area&lt;/span&gt;(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;width &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;height
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;shape_classes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [Rectangle, Circle]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;colors &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;red&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;blue&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;green&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;yellow&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;black&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;white&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Generate 10 shapes with random colors and areas&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;shapes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;choice(colors)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    shape_class &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;choice(shape_classes)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; shape_class &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; Rectangle:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        width &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;uniform(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;, math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt(Shape&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max_area))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        height &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;uniform(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;, math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt(Shape&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max_area))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        shape &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Rectangle(color, width, height)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        radius &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;uniform(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;, math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt(Shape&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max_area &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pi)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        shape &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Circle(color, radius)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    shapes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(shape)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print the shapes, sorted by area&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; shape &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sorted(shapes):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(shape)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Circle, color: blue, area: 0.5110628296555956, radius: 0.2851984845159934
Rectangle, color: yellow, area: 0.6186484099066036, width: 0.2740689854907352, height: 2.25727259433927
Circle, color: white, area: 5.867493292628993, radius: 0.9663542627217231
Circle, color: green, area: 18.28116762721217, radius: 1.7057368476298893
Rectangle, color: blue, area: 20.114023003818147, width: 5.075190544004695, height: 3.963205485472614
Rectangle, color: blue, area: 23.171307446004665, width: 2.586024349725701, height: 8.960204666465124
Circle, color: red, area: 42.43901187799147, radius: 2.598918721375873
Rectangle, color: white, area: 45.198912747710224, width: 9.793600740960953, height: 4.615147578833736
Circle, color: yellow, area: 47.991651978311594, radius: 2.7637128359318064
Rectangle, color: green, area: 83.54726788281138, width: 9.643204828660805, height: 8.66384872739595
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;global-and-nonlocal-keywords&#34;&gt;&lt;code&gt;global&lt;/code&gt; and &lt;code&gt;nonlocal&lt;/code&gt; keywords&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;global&lt;/code&gt; keyword is used to declare an identifier that can be used for the entire code block. This is useful when we want to use a variable in a function that is defined outside of the function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;f&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;global&lt;/span&gt; x &lt;span style=&#34;color:#75715e&#34;&gt;# global keyword is used to access a global variable from a function&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;f()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(x)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;nonlocal&lt;/code&gt; keyword is used to declare an identifier that is defined in the nearest enclosing scope. This is useful when we want to use a variable in a nested function that is defined outside of the nested function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;f&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;g&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;nonlocal&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    g()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(x)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;f()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following example from &lt;a href=&#34;https://docs.python.org/3/tutorial/classes.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;the official Python docs&lt;/a&gt; shows the relationship between global, local, and nonlocal variables.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;scope_test&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;do_local&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        spam &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;local spam&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;do_nonlocal&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;nonlocal&lt;/span&gt; spam
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        spam &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nonlocal spam&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;do_global&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;global&lt;/span&gt; spam
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        spam &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;global spam&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    spam &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;test spam&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    do_local()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;After local assignment:&amp;#34;&lt;/span&gt;, spam)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    do_nonlocal()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;After nonlocal assignment:&amp;#34;&lt;/span&gt;, spam)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    do_global()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;After global assignment:&amp;#34;&lt;/span&gt;, spam)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;scope_test()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;In global scope:&amp;#34;&lt;/span&gt;, spam)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;After local assignment: test spam
After nonlocal assignment: nonlocal spam
After global assignment: nonlocal spam
In global scope: global spam
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The unexpected result here is that &lt;code&gt;spam&lt;/code&gt; is still equal to &lt;code&gt;nonlocal&lt;/code&gt; even though it was changed in &lt;code&gt;do_global&lt;/code&gt; by declaring &lt;code&gt;global spam&lt;/code&gt;. When declaring something as &lt;code&gt;nonlocal&lt;/code&gt;, the variable must already exist in the enclosing namespace. The declaration of &lt;code&gt;global spam&lt;/code&gt; created a new instance of &lt;code&gt;spam&lt;/code&gt; in the &lt;code&gt;global&lt;/code&gt; namespace.&lt;/p&gt;
&lt;p&gt;The example below shows how local, nonlocal, and global variables work in the context of classes.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;User&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Represents a user.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, id, name, password):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; id
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; name
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;password &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; password
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;domain &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;unknown&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __str__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;User: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{0}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;, id: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{1}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;id)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;global_login&lt;/span&gt;(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;global&lt;/span&gt; domain
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;domain &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; domain
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# def nonlocal_login(self):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;#     nonlocal domain&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;#     domain = &amp;#34;compuserve.net&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;nonlocal_login&lt;/span&gt;(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        domain &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;compuserve.net&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;set_domain&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;nonlocal&lt;/span&gt; domain
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;domain &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; domain
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        set_domain()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;local_login&lt;/span&gt;(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;domain &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tx.rr.com&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;domain &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gmail.com&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;u &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; User(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;John&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;password&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;global_login()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;domain)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nonlocal_login()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;domain)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;local_login()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;domain)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;gmail.com
compuserve.net
tx.rr.com
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;data-classes&#34;&gt;Data Classes&lt;/h1&gt;
&lt;p&gt;Sometimes in our work, we may want to represent a simple class consisting only of attributes, similar to a &lt;code&gt;struct&lt;/code&gt; in C. Python provides a way to do this using the &lt;code&gt;dataclass&lt;/code&gt; decorator. The &lt;code&gt;dataclass&lt;/code&gt; decorator will automatically generate a constructor, &lt;code&gt;__repr__()&lt;/code&gt;, and &lt;code&gt;__eq__()&lt;/code&gt; method for us. The follow example shows how to implement such a class.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; dataclasses &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; dataclass
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;@dataclass&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Product&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Represents a product.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    id: int
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    name: str
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    price: float
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    quantity: int &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __str__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Product: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{0}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;, id: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{1}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;id)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Let&amp;#39;s create a list of graphics cards and list them&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;products &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;products&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(Product(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;GeForce RTX 2080 Ti&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1200.0&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;products&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(Product(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;GeForce RTX 2080&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;800.0&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;products&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(Product(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;GeForce RTX 2070&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;600.0&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;products&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(Product(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;GeForce RTX 2060&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;350.0&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;products&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(Product(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;GeForce GTX 1660 Ti&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;275.0&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;products&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(Product(&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;GeForce GTX 1660&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;200.0&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;products&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(Product(&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;GeForce GTX 1650&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;150.0&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;products&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(Product(&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;GeForce GTX 1080 Ti&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;800.0&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;products&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(Product(&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;GeForce GTX 1080&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;500.0&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;products&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(Product(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;GeForce GTX 1070 Ti&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;450.0&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; product &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; products:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(product)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Product: GeForce RTX 2080 Ti, id: 1
Product: GeForce RTX 2080, id: 2
Product: GeForce RTX 2070, id: 3
Product: GeForce RTX 2060, id: 4
Product: GeForce GTX 1660 Ti, id: 5
Product: GeForce GTX 1660, id: 6
Product: GeForce GTX 1650, id: 7
Product: GeForce GTX 1080 Ti, id: 8
Product: GeForce GTX 1080, id: 9
Product: GeForce GTX 1070 Ti, id: 10
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;iterators&#34;&gt;Iterators&lt;/h1&gt;
&lt;p&gt;We have used iterator objects, like a list, in previous examples. When defining our own custom classes, we can also define them as iterators. To do this, we need to implement the &lt;code&gt;__iter__()&lt;/code&gt; and &lt;code&gt;__next__()&lt;/code&gt; methods. The &lt;code&gt;__iter__()&lt;/code&gt; method should return the iterator object itself. The &lt;code&gt;__next__()&lt;/code&gt; method should return the next item in the sequence. When there are no more items in the sequence, &lt;code&gt;__next__()&lt;/code&gt; should raise a &lt;code&gt;StopIteration&lt;/code&gt; exception.&lt;/p&gt;
&lt;p&gt;This will be useful for our final example, where we will implement a dataloader for a machine learning application. To illusterate iterators with a simpler example, we need to justify the need to implement our own iterator. If we create something simple that iterates over a simple list of objects, why not just use the list itself?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s create a class that represents a 3D object. A 3D object has a list of vertices and a list of faces. Each face is a list of indices into the list of vertices. We will create a class that represents a 3D object. Our iterator for this class will iterator over the faces of the object, returning the vertices that make up each face.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Model&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Represents a 3D model.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, vertices, faces):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;vertices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; vertices
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;faces &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; faces
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __str__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; vertices, &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; faces&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(len(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;vertices), len(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;faces))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __len__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; len(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;faces)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __iter__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; self
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __next__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;index &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; len(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;faces):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;StopIteration&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        face &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;faces[self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;index]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        vertices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; vertex_index &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; face:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            vertices&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;vertices[vertex_index])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;index &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; vertices
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __getitem__(self, key):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        vertices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; vertex_index &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;faces[key]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            vertices&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;vertices[vertex_index])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; vertices
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create a cube model&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vertices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;), (&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;), (&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;), (&lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            (&lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;), (&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;), (&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;), (&lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;faces &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), (&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;), (&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;), (&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;), (&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;), (&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cube &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Model(vertices, faces)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Iterator over the model&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; face &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; cube:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Face &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(cube&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;index, face))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Face 1: [(0.0, 0.0, 0.0), (1.0, 0.0, 0.0), (1.0, 1.0, 0.0), (0.0, 1.0, 0.0)]
Face 2: [(1.0, 0.0, 0.0), (1.0, 0.0, 1.0), (1.0, 1.0, 1.0), (1.0, 1.0, 0.0)]
Face 3: [(1.0, 0.0, 1.0), (0.0, 0.0, 1.0), (0.0, 1.0, 1.0), (1.0, 1.0, 1.0)]
Face 4: [(0.0, 0.0, 1.0), (0.0, 0.0, 0.0), (0.0, 1.0, 0.0), (0.0, 1.0, 1.0)]
Face 5: [(0.0, 1.0, 0.0), (1.0, 1.0, 0.0), (1.0, 1.0, 1.0), (0.0, 1.0, 1.0)]
Face 6: [(0.0, 0.0, 1.0), (1.0, 0.0, 1.0), (1.0, 0.0, 0.0), (0.0, 0.0, 0.0)]
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;generators&#34;&gt;Generators&lt;/h1&gt;
&lt;p&gt;Generators provide a much cleaner way to implement iterators. Instead of implementing the &lt;code&gt;__iter__()&lt;/code&gt; and &lt;code&gt;__next__()&lt;/code&gt; methods, we can use the &lt;code&gt;yield&lt;/code&gt; keyword. The &lt;code&gt;yield&lt;/code&gt; keyword is used to return a value from a generator. The generator will remember its place in the sequence and return the next value when &lt;code&gt;next()&lt;/code&gt; is called on it.&lt;/p&gt;
&lt;p&gt;Since our class already included &lt;code&gt;__getitem__()&lt;/code&gt;, we can use the &lt;code&gt;yield&lt;/code&gt; keyword to implement our iterator.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_next_face&lt;/span&gt;(model):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; face &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(len(model)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;yield&lt;/span&gt; model[face]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Generator function&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; face &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; get_next_face(cube):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(face)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[(0.0, 0.0, 0.0), (1.0, 0.0, 0.0), (1.0, 1.0, 0.0), (0.0, 1.0, 0.0)]
[(1.0, 0.0, 0.0), (1.0, 0.0, 1.0), (1.0, 1.0, 1.0), (1.0, 1.0, 0.0)]
[(1.0, 0.0, 1.0), (0.0, 0.0, 1.0), (0.0, 1.0, 1.0), (1.0, 1.0, 1.0)]
[(0.0, 0.0, 1.0), (0.0, 0.0, 0.0), (0.0, 1.0, 0.0), (0.0, 1.0, 1.0)]
[(0.0, 1.0, 0.0), (1.0, 1.0, 0.0), (1.0, 1.0, 1.0), (0.0, 1.0, 1.0)]
[(0.0, 0.0, 1.0), (1.0, 0.0, 1.0), (1.0, 0.0, 0.0), (0.0, 0.0, 0.0)]
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Python File I/O</title>
      <link>https://ajdillhoff.github.io/notes/file_io/</link>
      <pubDate>Wed, 30 Aug 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/file_io/</guid>
      <description>&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/ajdillhoff/python-examples/blob/main/basics/file_io.ipynb&#34;&gt;
  &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;/&gt;
&lt;/a&gt;
&lt;h1 id=&#34;file-io-in-python&#34;&gt;File I/O in Python&lt;/h1&gt;
&lt;p&gt;This notebook will cover the following topics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Opening and closing files&lt;/li&gt;
&lt;li&gt;Reading and writing text files&lt;/li&gt;
&lt;li&gt;Reading and writing binary files&lt;/li&gt;
&lt;li&gt;Using the &lt;code&gt;with&lt;/code&gt; statement&lt;/li&gt;
&lt;li&gt;Using the &lt;code&gt;pickle&lt;/code&gt; module&lt;/li&gt;
&lt;li&gt;Serializing objects with &lt;code&gt;pickle&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Reading and writing JSON files&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;opening-and-closing-files&#34;&gt;Opening and closing files&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with the basics. To open a file, we use the built-in &lt;code&gt;open()&lt;/code&gt; function. The &lt;code&gt;open()&lt;/code&gt; function takes two arguments: the name of the file to open and the mode in which to open it. The mode can be one of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&#39;r&#39;&lt;/code&gt; - open for reading (default)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;w&#39;&lt;/code&gt; - open for writing, truncating the file first&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;x&#39;&lt;/code&gt; - open for exclusive creation, failing if the file already exists&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;a&#39;&lt;/code&gt; - open for writing, appending to the end of the file if it exists&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;b&#39;&lt;/code&gt; - binary mode&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;t&#39;&lt;/code&gt; - text mode (default)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;+&#39;&lt;/code&gt; - open a disk file for updating (reading and writing)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To make this tutorial more interesting, let&amp;rsquo;s create a purpose for our code. Our goal is to create a logging system for players and their rolls. Every time a player rolls, we will add it to our log. We will also add a timestamp to each roll. We will store the log in a file called &lt;code&gt;log.txt&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; random
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; datetime
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# First, we&amp;#39;ll open a file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;fp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;log.txt&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;w&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Let&amp;#39;s create a list of players&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;players &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Naomi&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;James&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Amos&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bobbie&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Let&amp;#39;s roll a d20 for each player and write it in the log, including a timestamp&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; player &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; players:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    roll &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    timestamp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; datetime&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;datetime&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;now()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    fp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;write(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;timestamp&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; - &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;player&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; rolled a &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;roll&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Finally, let&amp;#39;s close the file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;fp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;close()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It isn&amp;rsquo;t recommended to open and close the file manually like this. Instead, we will use the &lt;code&gt;with&lt;/code&gt; statement. This will ensure that the file is closed properly even if an exception is raised.&lt;/p&gt;
&lt;p&gt;Additionally, it is possible that calling &lt;code&gt;write&lt;/code&gt; without using &lt;code&gt;with&lt;/code&gt; will not write to the file immediately. Instead, it will be buffered and written later. Using &lt;code&gt;with&lt;/code&gt; will ensure that the file is written to immediately.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; datetime
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;log.txt&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;a&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; f:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    f&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;write(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;datetime&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;datetime&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;now()&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;player&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; rolled &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;roll&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;reading-and-writing-binary-files&#34;&gt;Reading and writing binary files&lt;/h2&gt;
&lt;p&gt;For the next example, we will write the user&amp;rsquo;s rolls as binary. Each user will get their own file so that we can easily read their rolls later.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; player &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; players:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    rolls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rolls/&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; player &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;.bin&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;wb&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; fp:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        fp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;write(bytes(rolls))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# To verify, let&amp;#39;s open the files and print the contents&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; player &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; players:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rolls/&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; player &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;.bin&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rb&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; fp:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(player, list(fp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read()))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Naomi [2, 13, 18, 8, 16]
James [7, 10, 17, 15, 15]
Amos [19, 9, 11, 6, 3]
Bobbie [16, 5, 18, 13, 1]
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;reading-from-csv-files&#34;&gt;Reading from CSV files&lt;/h1&gt;
&lt;p&gt;Python has a built-in module for reading and writing CSV files. CSV stands for comma-separated values. It is a common format for storing tabular data.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Load and parse CSV&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; csv
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Store the data in a list of dictionaries&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;../data/musicnet_metadata.csv&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;r&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; fp:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    reader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; csv&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reader(fp)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    keys &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; next(reader)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; reader:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(dict(zip(keys, row)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Let&amp;#39;s print the first 5 rows&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; data[:&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(row)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Count the number of Violin pieces are in the dataset&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;count &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; data:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Violin&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ensemble&amp;#34;&lt;/span&gt;]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        count &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;There are &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;count&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; Violin pieces in the dataset&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;{&#39;id&#39;: &#39;1727&#39;, &#39;composer&#39;: &#39;Schubert&#39;, &#39;composition&#39;: &#39;Piano Quintet in A major&#39;, &#39;movement&#39;: &#39;2. Andante&#39;, &#39;ensemble&#39;: &#39;Piano Quintet&#39;, &#39;source&#39;: &#39;European Archive&#39;, &#39;transcriber&#39;: &#39;http://tirolmusic.blogspot.com/&#39;, &#39;catalog_name&#39;: &#39;OP114&#39;, &#39;seconds&#39;: &#39;447&#39;}
{&#39;id&#39;: &#39;1728&#39;, &#39;composer&#39;: &#39;Schubert&#39;, &#39;composition&#39;: &#39;Piano Quintet in A major&#39;, &#39;movement&#39;: &#39;3. Scherzo: Presto&#39;, &#39;ensemble&#39;: &#39;Piano Quintet&#39;, &#39;source&#39;: &#39;European Archive&#39;, &#39;transcriber&#39;: &#39;http://tirolmusic.blogspot.com/&#39;, &#39;catalog_name&#39;: &#39;OP114&#39;, &#39;seconds&#39;: &#39;251&#39;}
{&#39;id&#39;: &#39;1729&#39;, &#39;composer&#39;: &#39;Schubert&#39;, &#39;composition&#39;: &#39;Piano Quintet in A major&#39;, &#39;movement&#39;: &#39;4. Andantino - Allegretto&#39;, &#39;ensemble&#39;: &#39;Piano Quintet&#39;, &#39;source&#39;: &#39;European Archive&#39;, &#39;transcriber&#39;: &#39;http://tirolmusic.blogspot.com/&#39;, &#39;catalog_name&#39;: &#39;OP114&#39;, &#39;seconds&#39;: &#39;444&#39;}
{&#39;id&#39;: &#39;1730&#39;, &#39;composer&#39;: &#39;Schubert&#39;, &#39;composition&#39;: &#39;Piano Quintet in A major&#39;, &#39;movement&#39;: &#39;5. Allegro giusto&#39;, &#39;ensemble&#39;: &#39;Piano Quintet&#39;, &#39;source&#39;: &#39;European Archive&#39;, &#39;transcriber&#39;: &#39;http://tirolmusic.blogspot.com/&#39;, &#39;catalog_name&#39;: &#39;OP114&#39;, &#39;seconds&#39;: &#39;368&#39;}
{&#39;id&#39;: &#39;1733&#39;, &#39;composer&#39;: &#39;Schubert&#39;, &#39;composition&#39;: &#39;Piano Sonata in A major&#39;, &#39;movement&#39;: &#39;2. Andantino&#39;, &#39;ensemble&#39;: &#39;Solo Piano&#39;, &#39;source&#39;: &#39;Museopen&#39;, &#39;transcriber&#39;: &#39;Segundo G. Yogore&#39;, &#39;catalog_name&#39;: &#39;D959&#39;, &#39;seconds&#39;: &#39;546&#39;}
There are 35 Violin pieces in the dataset
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;application-list-all-works-in-the-dataset-by-bach-for-solo-violin&#34;&gt;Application: List all works in the dataset by Bach for Solo Violin&lt;/h1&gt;
&lt;p&gt;Now that we have our data loaded. Let&amp;rsquo;s use list comprehensions to print out all the works by Bach for solo violin. Our formatted table should show the following columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;id&lt;/li&gt;
&lt;li&gt;composition name&lt;/li&gt;
&lt;li&gt;seconds&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Filter all lines that are written by Bach&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;bach &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [row &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; data &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bach&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composer&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Solo Violin&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ensemble&amp;#34;&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print the number of Bach pieces&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;There are &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;len(bach)&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; Bach pieces in the dataset&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print them all out&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; bach:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(row)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;There are 9 Bach pieces in the dataset
{&#39;id&#39;: &#39;2186&#39;, &#39;composer&#39;: &#39;Bach&#39;, &#39;composition&#39;: &#39;Violin Partita No 3 in E major&#39;, &#39;movement&#39;: &#39;1. Preludio&#39;, &#39;ensemble&#39;: &#39;Solo Violin&#39;, &#39;source&#39;: &#39;Oliver Colbentston&#39;, &#39;transcriber&#39;: &#39;suzumidi&#39;, &#39;catalog_name&#39;: &#39;BWV1006&#39;, &#39;seconds&#39;: &#39;214&#39;}
{&#39;id&#39;: &#39;2191&#39;, &#39;composer&#39;: &#39;Bach&#39;, &#39;composition&#39;: &#39;Violin Partita No 3 in E major&#39;, &#39;movement&#39;: &#39;6. Bourree&#39;, &#39;ensemble&#39;: &#39;Solo Violin&#39;, &#39;source&#39;: &#39;Oliver Colbentston&#39;, &#39;transcriber&#39;: &#39;suzumidi&#39;, &#39;catalog_name&#39;: &#39;BWV1006&#39;, &#39;seconds&#39;: &#39;102&#39;}
{&#39;id&#39;: &#39;2241&#39;, &#39;composer&#39;: &#39;Bach&#39;, &#39;composition&#39;: &#39;Violin Sonata No 1 in G minor&#39;, &#39;movement&#39;: &#39;1. Adagio&#39;, &#39;ensemble&#39;: &#39;Solo Violin&#39;, &#39;source&#39;: &#39;European Archive&#39;, &#39;transcriber&#39;: &#39;suzumidi&#39;, &#39;catalog_name&#39;: &#39;BWV1001&#39;, &#39;seconds&#39;: &#39;242&#39;}
{&#39;id&#39;: &#39;2242&#39;, &#39;composer&#39;: &#39;Bach&#39;, &#39;composition&#39;: &#39;Violin Sonata No 1 in G minor&#39;, &#39;movement&#39;: &#39;2. Fuga&#39;, &#39;ensemble&#39;: &#39;Solo Violin&#39;, &#39;source&#39;: &#39;European Archive&#39;, &#39;transcriber&#39;: &#39;suzumidi&#39;, &#39;catalog_name&#39;: &#39;BWV1001&#39;, &#39;seconds&#39;: &#39;312&#39;}
{&#39;id&#39;: &#39;2243&#39;, &#39;composer&#39;: &#39;Bach&#39;, &#39;composition&#39;: &#39;Violin Sonata No 1 in G minor&#39;, &#39;movement&#39;: &#39;3. Siciliana&#39;, &#39;ensemble&#39;: &#39;Solo Violin&#39;, &#39;source&#39;: &#39;European Archive&#39;, &#39;transcriber&#39;: &#39;suzumidi&#39;, &#39;catalog_name&#39;: &#39;BWV1001&#39;, &#39;seconds&#39;: &#39;193&#39;}
{&#39;id&#39;: &#39;2244&#39;, &#39;composer&#39;: &#39;Bach&#39;, &#39;composition&#39;: &#39;Violin Sonata No 1 in G minor&#39;, &#39;movement&#39;: &#39;4. Presto&#39;, &#39;ensemble&#39;: &#39;Solo Violin&#39;, &#39;source&#39;: &#39;European Archive&#39;, &#39;transcriber&#39;: &#39;suzumidi&#39;, &#39;catalog_name&#39;: &#39;BWV1001&#39;, &#39;seconds&#39;: &#39;214&#39;}
{&#39;id&#39;: &#39;2288&#39;, &#39;composer&#39;: &#39;Bach&#39;, &#39;composition&#39;: &#39;Violin Partita No 1 in B minor&#39;, &#39;movement&#39;: &#39;2. Corrente&#39;, &#39;ensemble&#39;: &#39;Solo Violin&#39;, &#39;source&#39;: &#39;John Garner&#39;, &#39;transcriber&#39;: &#39;suzumidi&#39;, &#39;catalog_name&#39;: &#39;BWV1002&#39;, &#39;seconds&#39;: &#39;191&#39;}
{&#39;id&#39;: &#39;2289&#39;, &#39;composer&#39;: &#39;Bach&#39;, &#39;composition&#39;: &#39;Violin Partita No 1 in B minor&#39;, &#39;movement&#39;: &#39;3. Sarabande&#39;, &#39;ensemble&#39;: &#39;Solo Violin&#39;, &#39;source&#39;: &#39;John Garner&#39;, &#39;transcriber&#39;: &#39;suzumidi&#39;, &#39;catalog_name&#39;: &#39;BWV1002&#39;, &#39;seconds&#39;: &#39;203&#39;}
{&#39;id&#39;: &#39;2659&#39;, &#39;composer&#39;: &#39;Bach&#39;, &#39;composition&#39;: &#39;Violin Partita No 1 in B minor&#39;, &#39;movement&#39;: &#39;6. Double&#39;, &#39;ensemble&#39;: &#39;Solo Violin&#39;, &#39;source&#39;: &#39;John Garner&#39;, &#39;transcriber&#39;: &#39;suzumidi&#39;, &#39;catalog_name&#39;: &#39;BWV1002&#39;, &#39;seconds&#39;: &#39;108&#39;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like this dataset is missing quite a few pieces. Bach wrote 6 sonatas and partitas for solo violin. We only have 3 of them in this dataset. Of the 3 that are included, partitas 1 and 3 are incomplete as well.&lt;/p&gt;
&lt;p&gt;Based on the order of the ids, it looks like this data should be present. Let&amp;rsquo;s see if we can fill some of this in.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# The times are based on Hilary Hahn&amp;#39;s recordings&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;missing_pieces &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2679&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composer&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bach&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composition&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Violin Partita No 3 in E major&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;movement&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2. Loure&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ensemble&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Solo Violin&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;source&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;DASC 5300 Fall 2023&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;transcriber&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;none&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;catalog_name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;BWV 1006&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;seconds&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;287&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2680&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composer&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bach&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composition&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Violin Partita No 3 in E major&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;movement&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;3. Gavotte en Rondeau&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ensemble&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Solo Violin&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;source&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;DASC 5300 Fall 2023&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;transcriber&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;none&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;catalog_name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;BWV 1006&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;seconds&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;196&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2681&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composer&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bach&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composition&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Violin Partita No 3 in E major&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;movement&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;4. Menuet I&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ensemble&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Solo Violin&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;source&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;DASC 5300 Fall 2023&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;transcriber&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;none&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;catalog_name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;BWV 1006&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;seconds&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;113&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2682&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composer&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bach&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composition&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Violin Partita No 3 in E major&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;movement&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;5. Menuet II&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ensemble&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Solo Violin&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;source&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;DASC 5300 Fall 2023&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;transcriber&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;none&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;catalog_name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;BWV 1006&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;seconds&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;183&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;formatting-our-output&#34;&gt;Formatting Our Output&lt;/h1&gt;
&lt;p&gt;Viewing raw lines of a dictionary or CSV file is less than ideal. Let&amp;rsquo;s format our output to make it easier to read. We will format the filtered Bach data. Since we know that every piece was written by him, we don&amp;rsquo;t need to show that column.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Let&amp;#39;s add the missing pieces to our dataset&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extend(missing_pieces)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Refilter the Bach pieces&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;bach &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [row &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; data &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bach&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composer&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Solo Violin&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ensemble&amp;#34;&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;There are &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;len(bach)&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; Bach pieces in the dataset&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print them all out, sorted by composition then movement&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;composition_width &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(len(row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composition&amp;#34;&lt;/span&gt;]) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; bach)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;movement_width &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(len(row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;movement&amp;#34;&lt;/span&gt;]) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; bach)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;id_width &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(len(row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;]) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; bach)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;{:&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;} | {:&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;} | {:&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Composition&amp;#34;&lt;/span&gt;, composition_width, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Movement&amp;#34;&lt;/span&gt;, movement_width, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ID&amp;#34;&lt;/span&gt;, id_width))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (composition_width &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; movement_width &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; id_width &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sorted(bach, key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: (x[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composition&amp;#34;&lt;/span&gt;], x[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;movement&amp;#34;&lt;/span&gt;])):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;{:&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;} | {:&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;} | {:&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composition&amp;#34;&lt;/span&gt;], composition_width, row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;movement&amp;#34;&lt;/span&gt;], movement_width, row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;], id_width))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;There are 13 Bach pieces in the dataset
Composition                    | Movement              | ID  
-------------------------------------------------------------
Violin Partita No 1 in B minor | 2. Corrente           | 2288
Violin Partita No 1 in B minor | 3. Sarabande          | 2289
Violin Partita No 1 in B minor | 6. Double             | 2659
Violin Partita No 3 in E major | 1. Preludio           | 2186
Violin Partita No 3 in E major | 2. Loure              | 2679
Violin Partita No 3 in E major | 3. Gavotte en Rondeau | 2680
Violin Partita No 3 in E major | 4. Menuet I           | 2681
Violin Partita No 3 in E major | 5. Menuet II          | 2682
Violin Partita No 3 in E major | 6. Bourree            | 2191
Violin Sonata No 1 in G minor  | 1. Adagio             | 2241
Violin Sonata No 1 in G minor  | 2. Fuga               | 2242
Violin Sonata No 1 in G minor  | 3. Siciliana          | 2243
Violin Sonata No 1 in G minor  | 4. Presto             | 2244
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Now that it looks good, let&amp;#39;s write the updated dataset to a new file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;../data/musicnet_metadata_updated.csv&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;w&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; fp:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    writer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; csv&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DictWriter(fp, fieldnames&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;keys)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    writer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;writeheader()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    writer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;writerows(data)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;reading-and-writing-json&#34;&gt;Reading and Writing JSON&lt;/h1&gt;
&lt;p&gt;JSON stands for JavaScript Object Notation. It is a common format for storing and transmitting data. It is often used for web APIs. Python has a built-in module for reading and writing JSON files.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s open the CSV file from the previous example and write it to JSON.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Load and parse CSV&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; csv
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; json
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Store the data in a list of dictionaries&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;../data/musicnet_metadata.csv&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;r&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; fp:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    reader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; csv&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reader(fp)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    keys &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; next(reader)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; reader:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(dict(zip(keys, row)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Write the data to a JSON file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;musicnet_metadata.json&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;w&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; fp:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    json&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dump(data, fp)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;reading-and-writing-pickle-files&#34;&gt;Reading and Writing &lt;code&gt;pickle&lt;/code&gt; Files&lt;/h1&gt;
&lt;p&gt;Python has a built-in module for reading and writing &lt;code&gt;pickle&lt;/code&gt; files. &lt;code&gt;pickle&lt;/code&gt; is a binary format for serializing Python objects. It is not human-readable, but it is very useful for storing and transmitting data. Note that &lt;code&gt;pickle&lt;/code&gt; is not secure. It is possible to create malicious &lt;code&gt;pickle&lt;/code&gt; files that can execute arbitrary code when loaded. Also, other languages cannot natively read &lt;code&gt;pickle&lt;/code&gt; files. However, there are usually libraries available for reading &lt;code&gt;pickle&lt;/code&gt; files in other languages.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start by writing the list from the previous example to a &lt;code&gt;pickle&lt;/code&gt; file.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Write the data to a pickle file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pickle
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;musicnet_metadata.pkl&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;wb&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; fp:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    pickle&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dump(data, fp)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Get the file size of the pickle file we just wrote&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; os
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;getsize(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;musicnet_metadata.pkl&amp;#34;&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;52353
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;serializing-class-objects&#34;&gt;Serializing Class Objects&lt;/h1&gt;
&lt;p&gt;Serializing objects is simple with Python. Let&amp;rsquo;s create a simple class to represent the attributes of our dataset. We can then convert our previous list of dictionary data to a list of objects. Finally, we can serialize the list of objects to a &lt;code&gt;pickle&lt;/code&gt; file.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create a class for our data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Piece&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, data):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;composer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composer&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;composition &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;composition&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;movement &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;movement&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ensemble &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ensemble&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;source &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;source&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transcriber &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;transcriber&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;catalog_name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;catalog_name&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;seconds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;seconds&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __repr__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;lt;Piece &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;id&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;composer&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; - &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;composition&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __str__(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;composer&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; - &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;composition&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Convert our data into a list of objects and write it to a pickle file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pieces &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [Piece(d) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; d &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; data]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;musicnet_metadata.pkl&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;wb&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; fp:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    pickle&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dump(pieces, fp)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Print the size of this new file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;getsize(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;musicnet_metadata.pkl&amp;#34;&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;54352
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;bonus-write-the-class-objects-to-json&#34;&gt;Bonus: Write the class objects to JSON&lt;/h1&gt;
&lt;p&gt;We can&amp;rsquo;t write the class objects to JSON directly. We need to convert them to a dictionary first. We can do this by implementing the &lt;code&gt;__dict__&lt;/code&gt; method. Without explicitly defining &lt;code&gt;__dict__&lt;/code&gt;, it will return the default dictionary for the class. However, we can override it to return a custom dictionary. The default version will include all of the class attributes. We can use this to our advantage to convert the class to a dictionary.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Bonus: Save the class data as JSON&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; json
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Write the list of class objects to a JSON file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;musicnet_metadata.json&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;w&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; fp:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    json&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dump(pieces, fp, default&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; o: o&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__dict__)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Python Functions</title>
      <link>https://ajdillhoff.github.io/notes/functions/</link>
      <pubDate>Wed, 30 Aug 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/functions/</guid>
      <description>&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/ajdillhoff/python-examples/blob/main/basics/functions.ipynb&#34;&gt;
  &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;/&gt;
&lt;/a&gt;
&lt;h1 id=&#34;functions&#34;&gt;Functions&lt;/h1&gt;
&lt;p&gt;We learned the importance of functions early on in mathematics. It is a compact way of represented a complex process dependent on a set of variables. In programming, functions are used to encapsulate a set of instructions that are used repeatedly. Functions are also used to make code more readable and easier to debug.&lt;/p&gt;
&lt;p&gt;In this notebook, we will look at a few important built-in functions in Python as well as how to define our own functions.&lt;/p&gt;
&lt;h2 id=&#34;built-in-functions&#34;&gt;Built-in Functions&lt;/h2&gt;
&lt;p&gt;Python has a good number of built-in functions that cover a general range of tasks.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Find the max of a list of numbers&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;max_val &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The max value is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;max_val&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Find the min of a list of numbers&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;min_val &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The min value is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;min_val&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Get the length of a string&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; input(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Enter some text: &amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The length of &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;text&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;len(text)&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;The max value is 3
The min value is 1
The length of &amp;quot;test&amp;quot; is 4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;len&lt;/code&gt; function returns the length of a string, list, or other iterable object. Since these functions are built-in, we should treat them as keywords. However, we can still overwrite them if we want to.&lt;/p&gt;
&lt;h2 id=&#34;type-conversion-functions&#34;&gt;Type Conversion Functions&lt;/h2&gt;
&lt;p&gt;Python has built-in functions to convert between data types. These are &lt;code&gt;int&lt;/code&gt;, &lt;code&gt;float&lt;/code&gt;, &lt;code&gt;str&lt;/code&gt;, &lt;code&gt;bool&lt;/code&gt;, and &lt;code&gt;list&lt;/code&gt;. Converting between incompatible types will result in an error.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Try entering a number and text&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;val &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; input(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Enter a number: &amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;val_int &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int(val)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The value of &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;val&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;val_int&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Converting an integer type to float will truncate the decimal value&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;float_val &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3.14&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;int_val &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int(float_val)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The value of &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;float_val&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;int_val&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Converting a number to a string also has its uses&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;big_number &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;184759372934&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;big_number_str &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; str(big_number)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;There are &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;len(big_number_str)&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; digits in &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;big_number_str&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;The value of 10 is 10
The value of 3.14 is 3
There are 12 digits in 184759372934
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;math-functions&#34;&gt;Math Functions&lt;/h2&gt;
&lt;p&gt;Python has many more functions that are available through named modules. For example, the &lt;code&gt;math&lt;/code&gt; module contains many useful functions for mathematical operations. To use these functions, we need to import the module first.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; math
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;signal_power &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;noise_power &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ratio &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; signal_power &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; noise_power
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;decibels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;log10(ratio)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The decibel value is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;decibels&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# The math module also has a function to convert from radians to degrees&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;radians &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.7&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;degrees &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;degrees(radians)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;radians&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; radians is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;degrees&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; degrees&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# The math module also has a function to convert from degrees to radians&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;degrees &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;45&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;radians &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;radians(degrees)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;degrees&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; degrees is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;radians&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; radians&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;The decibel value is 3.010299956639812
0.7 radians is 40.10704565915762 degrees
45 degrees is 0.7853981633974483 radians
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;example-getting-the-number-of-digits-without-len&#34;&gt;Example: Getting the number of digits without &lt;code&gt;len&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;We can use the &lt;code&gt;math.log10&lt;/code&gt; function to get the number of digits in a number.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;big_number &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;184759372934&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;big_number_log &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;log10(big_number)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The log of &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;big_number&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;big_number_log&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can use this to get the number of digits in a number&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;big_number_log_int &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int(big_number_log)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;There are &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;big_number_log_int &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; digits in &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;big_number&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;The log of 184759372934 is 11.266606479598726
There are 12 digits in 184759372934
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;random-numbers&#34;&gt;Random Numbers&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;random&lt;/code&gt; module contains functions for generating random numbers. The &lt;code&gt;random&lt;/code&gt; function returns a random number between 0 and 1. The &lt;code&gt;randint&lt;/code&gt; function returns a random integer between two numbers. Other functions in the &lt;code&gt;random&lt;/code&gt; module include &lt;code&gt;randrange&lt;/code&gt;, &lt;code&gt;choice&lt;/code&gt;, &lt;code&gt;choices&lt;/code&gt;, &lt;code&gt;shuffle&lt;/code&gt;, and &lt;code&gt;sample&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; random
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Generate 10 random numbers between 0 and 1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Generate 10 random integers between 4 and 10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Randomly select a value from a list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;outcomes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rock&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;paper&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;scissors&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;choice(outcomes))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;0.5791115192498043
0.25376108559783617
0.3024994224981705
0.09072083021401978
0.42037740159252324
0.6617409553852582
0.4379309412534801
0.2475132325137145
0.8452657960508129
0.9268148237541722
4
5
7
5
7
10
8
6
7
4
scissors
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Third-party modules such as &lt;code&gt;numpy&lt;/code&gt; and &lt;code&gt;scipy&lt;/code&gt; contain many more functions for generating random numbers.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Generate 10 random numbers between 0 and 1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rand(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(numbers)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Sample 10 random numbers from a normal distribution&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(numbers)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[0.47426298 0.22698408 0.42633457 0.97584666 0.68835279 0.57067918
 0.56958053 0.86689698 0.54039587 0.59397872]
[ 0.97783258 -0.4043045   0.05416324 -2.21162364 -0.60327265 -0.39077797
 -0.83294774  0.56285811 -0.28047169 -0.60044315]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;defining-functions&#34;&gt;Defining Functions&lt;/h2&gt;
&lt;p&gt;We can define our own functions using the &lt;code&gt;def&lt;/code&gt; keyword. The syntax for defining a function is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function_name&lt;/span&gt;(parameters):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# function body&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; value
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Python functions can have multiple parameters and return multiple values. The &lt;code&gt;return&lt;/code&gt; keyword is optional. If it is not used, the function will return &lt;code&gt;None&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; math
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;calculate_stats&lt;/span&gt;(numbers):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Calculate the mean and standard deviation of a list of numbers&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    mean &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sum(numbers) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; len(numbers)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    variance &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sum((x &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; mean) &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; numbers) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; len(numbers)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    std_dev &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt(variance)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; mean, std_dev
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the mean and standard deviation of a list of numbers&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mean, std_dev &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; calculate_stats(numbers)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The mean is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;mean&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; and the standard deviation is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;std_dev&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;The mean is 3.0 and the standard deviation is 1.4142135623730951
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;default-argument-values&#34;&gt;Default Argument Values&lt;/h1&gt;
&lt;p&gt;Python functions can have default values for their arguments. This allows us to call the function without specifying the value for that argument. If we do not specify a value for an argument, the default value will be used.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# To demonstrate default argument values, let&amp;#39;s make a function that&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# allows the user to select the axis along which to calculate the mean&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;calculate_mean&lt;/span&gt;(numbers, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Calculates the mean of a 2D array along a given axis&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Check that the input is a 2D python list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; isinstance(numbers, list) &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; isinstance(numbers[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], list):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ValueError&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Input must be a 2D list&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; axis &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Here, zip(*numbers) will return a list of tuples,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# where each tuple is a column of our 2D list.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; [sum(col) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; len(col) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; col &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; zip(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;numbers)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; axis &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; [sum(row) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; len(row) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; numbers]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ValueError&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Invalid axis value&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the mean along the rows&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mean &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; calculate_mean(numbers, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Mean of each row:&amp;#34;&lt;/span&gt;, mean)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the mean along the columns&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mean &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; calculate_mean(numbers, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Mean of each column&amp;#34;&lt;/span&gt;, mean)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Mean of each row: [2.0, 5.0]
Mean of each column [2.5, 3.5, 4.5]
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;keyword-arguments&#34;&gt;Keyword Arguments&lt;/h1&gt;
&lt;p&gt;Python functions can also have keyword arguments. This allows us to specify the name of the argument when calling the function. This is useful when a function has many arguments and we only want to specify a few of them.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;matrix_max&lt;/span&gt;(matrix, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, return_indices&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Calculates the maximum of a 2D array along a given axis&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Check that the input is a 2D python list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; isinstance(matrix, list) &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; isinstance(matrix[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], list):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ValueError&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Input must be a 2D list&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; axis &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Here, zip(*matrix) will return a list of tuples,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# where each tuple is a column of our 2D list.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        max_vals &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [max(col) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; col &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; zip(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;matrix)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; return_indices:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            max_indices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [col&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;index(max(col)) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; col &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; zip(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;matrix)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; max_vals, max_indices
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; max_vals
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; axis &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        max_vals &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [max(row) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; matrix]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; return_indices:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            max_indices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [row&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;index(max(row)) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; matrix]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; max_vals, max_indices
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; max_vals
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ValueError&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Invalid axis value&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the max along the rows&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;max_vals &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; matrix_max(numbers, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Max of each row:&amp;#34;&lt;/span&gt;, max_vals)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the max along the columns&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;max_vals &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; matrix_max(numbers, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Max of each column&amp;#34;&lt;/span&gt;, max_vals)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the max along the rows and return the indices&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;max_vals, max_indices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; matrix_max(numbers, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, return_indices&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Max of each row:&amp;#34;&lt;/span&gt;, max_vals)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Indices of max values:&amp;#34;&lt;/span&gt;, max_indices)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Max of each row: [3, 6]
Max of each column [4, 5, 6]
Max of each row: [3, 6]
Indices of max values: [2, 2]
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;list-unpacking-and-variable-arguments&#34;&gt;List Unpacking and Variable Arguments&lt;/h1&gt;
&lt;p&gt;Python functions can have variable arguments. This allows us to pass in a variable number of arguments to a function. The &lt;code&gt;*&lt;/code&gt; operator is used to unpack a list or tuple into separate arguments.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# The range function expects up to 3 arguments: start, stop, and step&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can use list unpacking to put our arguments in a list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;args &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;args):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(i)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;1
3
5
7
9
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can create a function that support multiple arguments as well as keyword arguments&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;print_args&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;args, &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;kwargs):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Positional arguments:&amp;#34;&lt;/span&gt;, args)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Keyword arguments:&amp;#34;&lt;/span&gt;, kwargs)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print_args(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, a&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, b&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Positional arguments: (1, 2, 3)
Keyword arguments: {&#39;a&#39;: 4, &#39;b&#39;: 5}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Git</title>
      <link>https://ajdillhoff.github.io/notes/introduction_to_git/</link>
      <pubDate>Sat, 26 Aug 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/introduction_to_git/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-version-control&#34;&gt;What is Version Control?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-git&#34;&gt;What is Git?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-a-repository&#34;&gt;What is a Repository?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#configuring-git&#34;&gt;Configuring Git&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#creating-a-repository&#34;&gt;Creating a Repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#staging-files&#34;&gt;Staging Files&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#committing-changes&#34;&gt;Committing Changes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ignoring-files&#34;&gt;Ignoring Files&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#branching&#34;&gt;Branching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#merging&#34;&gt;Merging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#remotes&#34;&gt;Remotes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cloning-an-existing-repository&#34;&gt;Cloning an Existing Repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;what-is-version-control&#34;&gt;What is Version Control?&lt;/h2&gt;
&lt;p&gt;Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later. This is useful not just for team projects, for for individual projects as well.&lt;/p&gt;
&lt;p&gt;With version control, you can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;revert files back to a previous state&lt;/li&gt;
&lt;li&gt;revert the entire project back to a previous state&lt;/li&gt;
&lt;li&gt;compare changes over time&lt;/li&gt;
&lt;li&gt;see who last modified something that might be causing a problem&lt;/li&gt;
&lt;li&gt;who introduced an issue and when&lt;/li&gt;
&lt;li&gt;and much more&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When tracking changes to a project over time, the simplest approach is one that you might recognize if you&amp;rsquo;ve ever worked on an essay for class. Imagine you&amp;rsquo;ve just finished the first draft of an assignment. You decide to save this document as &lt;code&gt;essay_first_draft.docx&lt;/code&gt;. After working on it a bit more, you choose to save the updated copy to a new file so that you can compare the initial and final draft. This one is then named &lt;code&gt;essay_first_draft_COMPLETE.docx&lt;/code&gt;. You end up reading some new information and realize you missed a key requirement of the assignment. After adding in the new information you save it as &lt;code&gt;essay_first_draft_COMPLETE_v2.docx&lt;/code&gt;. After many such iterations you end up with a collection of ill-named files.&lt;/p&gt;
&lt;p&gt;Maybe you&amp;rsquo;ve never done this yourself, but this example actually depicts a version control system. Luckily for us, there have been many improvements to this naive method. A more ideal choice, especially in a team environment, would be a Centralized VCS. Project files would be hosted on a server that keeps track of the different changes. Team members can download the latest versions, modify them, and update the server once they are done.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_18-53-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Centralized VCS ([source](&amp;lt;https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control&amp;gt;))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Centralized VCS (&lt;a href=&#34;https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;source&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This is a welcome improvement over the naive version, but it still has its downsides. What if the server or connection goes down? There are many scenarios that would lead to a catastrophic loss of data. For important projects, you would not want to keep all of your eggs in once basket. A more ideal solution would be a Distributed VCS, this is what Git is.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_18-58-25_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Distributed VCS ([source](&amp;lt;https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control&amp;gt;))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Distributed VCS (&lt;a href=&#34;https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;source&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In a DVCS, every user has a complete copy of the project. If the server goes down, or a connection is lost, you can still work on the project. Since every user has a complete copy of the project, there is no single point of failure. Another huge advantage is speed. The operations are performed locally. It is only when you want to share your changes that you need to connect to the server. This means that you can commit changes, create branches, and perform other operations without an internet connection.&lt;/p&gt;
&lt;h2 id=&#34;what-is-git&#34;&gt;What is Git?&lt;/h2&gt;
&lt;p&gt;There are two primary ways of thinking about versioning in general: snapshots and differences. The first starts with your original files and records each change as a delta between the latest version and the previous.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_19-03-32_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Difference-based Version Control ([source](&amp;lt;https://git-scm.com/book/en/v2/Getting-Started-What-is-Git%3F&amp;gt;))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Difference-based Version Control (&lt;a href=&#34;https://git-scm.com/book/en/v2/Getting-Started-What-is-Git%3F&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;source&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The second starts with your original files and records each change as a snapshot of the entire project. Files that have not changed will not be duplicated. Instead, Git will create a reference to the previous version of the file. This is the approach that Git uses, and it comes with a great benefit that we will see when we get to branching.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_19-05-52_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Snapshot-based Version Control ([source](&amp;lt;https://git-scm.com/book/en/v2/Getting-Started-What-is-Git%3F&amp;gt;))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Snapshot-based Version Control (&lt;a href=&#34;https://git-scm.com/book/en/v2/Getting-Started-What-is-Git%3F&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;source&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;what-is-a-repository&#34;&gt;What is a Repository?&lt;/h2&gt;
&lt;p&gt;A repository is a collection of files and folders that are tracked by Git. It is the project folder that you will be working in. You can create a repository from scratch, or you can clone an existing repository. We will cover examples of both during class. Cloning is the process of copying an existing repository to your local machine. We will start with the first approach: creating a repository from scratch.&lt;/p&gt;
&lt;p&gt;Before starting, it is important to at least know the three major states of Git. Files can be &lt;code&gt;modified&lt;/code&gt;, &lt;code&gt;staged&lt;/code&gt;, or &lt;code&gt;committed&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;code&gt;modified&lt;/code&gt; file has been changed locally, but has not been committed to the repository.&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;staged&lt;/code&gt; file is a modified file that has been marked to be included in the next commit.&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;committed&lt;/code&gt; file is a staged file that has been saved to the repository.&lt;/li&gt;
&lt;/ul&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_19-16-09_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;The main sections of Git. ([source](&amp;lt;https://git-scm.com/book/en/v2/Getting-Started-What-is-Git%3F&amp;gt;))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;The main sections of Git. (&lt;a href=&#34;https://git-scm.com/book/en/v2/Getting-Started-What-is-Git%3F&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;source&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The figure above depicts the three major sections of working with a Git repository. Each repository has a &lt;code&gt;.git&lt;/code&gt; directory that contains all of the information about the project. The &lt;code&gt;working directory&lt;/code&gt; is the root directory where the latest versions of the files exist. Once modifications are made, these changes are sent to the &lt;code&gt;staging area&lt;/code&gt;. This is where you can choose which changes to include in the next commit. Once you are happy with the changes, you can commit them to the repository. This will save the changes to the &lt;code&gt;.git&lt;/code&gt; directory.&lt;/p&gt;
&lt;h2 id=&#34;configuring-git&#34;&gt;Configuring Git&lt;/h2&gt;
&lt;p&gt;Once you have installed Git, there are a few important configuration options to get started. If you have already been using Git, you can skip this section. If you are using Git for the first time, you will need to set your name and email address. This information will be used to identify you as the author of the commits that you make.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git config --global user.name &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Naomi Nagata&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git config --global user.email &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;naomi@rocinante.exp&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you have already used a service like GitHub, note that this name and email does not need to match the one you used to log into that service.&lt;/p&gt;
&lt;p&gt;You can view your current configuration at any time by running the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git config --list --show-origin
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;creating-a-repository&#34;&gt;Creating a Repository&lt;/h2&gt;
&lt;p&gt;For this example, our first project will be a Python program that resizes images to a specified width. This is to ensure that the aspect ratio is maintained.&lt;/p&gt;
&lt;p&gt;Now that we have Git installed and configured, we can create our first repository. First, create a new directory for the project. I will use &lt;code&gt;pyresize&lt;/code&gt; in this document. Then, navigate to that directory and run the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mkdir pyresize &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; cd pyresize
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git init
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You may see the following warning when creating a new repository:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint: Using &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;master&amp;#39;&lt;/span&gt; as the name &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; the initial branch. This default branch name
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint: is subject to change. To configure the initial branch name to use in all
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint: of your new repositories, which will suppress this warning, call:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint: 	git config --global init.defaultBranch &amp;lt;name&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint: Names commonly chosen instead of &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;master&amp;#39;&lt;/span&gt; are &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;main&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;trunk&amp;#39;&lt;/span&gt; and
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;development&amp;#39;&lt;/span&gt;. The just-created branch can be renamed via this command:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hint: 	git branch -m &amp;lt;name&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Initialized empty Git repository in /home/alex/dev/pyresize/.git/
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s first set the default branch name to &lt;code&gt;main&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git config --global init.defaultBranch main
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next, we will change the name of the current branch to &lt;code&gt;main&lt;/code&gt;. We could also delete the &lt;code&gt;.git&lt;/code&gt; directory and start over, but this is a good opportunity to learn how to rename a branch.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git branch -m main
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can view the status of your repository at any time by using the &lt;code&gt;git status&lt;/code&gt; command. This will show you the current branch, the files that have been modified, and the files that have been staged. Our newly created repository looks like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ git status
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;On branch main
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;No commits yet
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;nothing to commit &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;create/copy files and use &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git add&amp;#34;&lt;/span&gt; to track&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;staging-files&#34;&gt;Staging Files&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s create our first file and add it to the repository. We will create a file called &lt;code&gt;pyresize.py&lt;/code&gt; that contains the following code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sys
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; PIL &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Image
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;resize_image&lt;/span&gt;(image_path, width):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(image_path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    wpercent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (width &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; float(image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;size[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    hsize &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int((float(image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;size[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; float(wpercent)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;resize((width, hsize), Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;LANCZOS)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;save(image_path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; __name__ &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;__main__&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    resize_image(sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argv[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], int(sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argv[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;At this point, we have a local change that our repository is not aware of. We can see this by running the &lt;code&gt;git status&lt;/code&gt; command again.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ git status
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;On branch main
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;No commits yet
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Untracked files:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;use &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git add &amp;lt;file&amp;gt;...&amp;#34;&lt;/span&gt; to include in what will be committed&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    pyresize.py
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;nothing added to commit but untracked files present &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;use &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git add&amp;#34;&lt;/span&gt; to track&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s add our file with &lt;code&gt;git add pyresize.py&lt;/code&gt; and check the status again.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ git add pyresize.py
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ git status
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;On branch main
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;No commits yet
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Changes to be committed:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;use &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git rm --cached &amp;lt;file&amp;gt;...&amp;#34;&lt;/span&gt; to unstage&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    new file:   pyresize.py
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;committing-changes&#34;&gt;Committing Changes&lt;/h2&gt;
&lt;p&gt;Finally, we will &lt;code&gt;commit&lt;/code&gt; this change via &lt;code&gt;git commit&lt;/code&gt;. There are a few things to note about this command. If you haven&amp;rsquo;t configured your default editor, it might be set to something like &lt;code&gt;nano&lt;/code&gt; or &lt;code&gt;vim&lt;/code&gt; by default. If you are not familiar with these editors, you can set your default editor to something else by running the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git config --global core.editor &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;code --wait&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will set your default editor to Visual Studio Code. Obviously, this should be installed on your system if you are using it. If you are using a different editor, you can replace &lt;code&gt;code&lt;/code&gt; with the command that you would use to open a file in that editor. For example, if you are using &lt;code&gt;vim&lt;/code&gt;, you would use &lt;code&gt;vim&lt;/code&gt;. The &lt;code&gt;--wait&lt;/code&gt; flag above will wait for the editor to close before continuing. This is important for Git to know when you are done writing your commit message. &lt;strong&gt;Note that not every application supports this flag.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once you have set your default editor, you can run &lt;code&gt;git commit&lt;/code&gt; to open the editor and write your commit message. The first line should be a short description of the change. The following lines should be a more detailed description of the change. You can see an example below:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-git&#34; data-lang=&#34;git&#34;&gt;Added our first file.
# Please enter the commit message for your changes. Lines starting
# with &amp;#39;#&amp;#39; will be ignored, and an empty message aborts the commit.
#
# On branch main
#
# Initial commit
#
# Changes to be committed:
#	new file:   pyresize.py
#
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once we save this message, the commit will be complete. You can view the commit history by running &lt;code&gt;git log&lt;/code&gt;. This will show you the commit hash, the author, the date, and the commit message. You can also commit changes and add a message in one command.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git commit -m &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Added our first file.&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We do not yet have a remote repository to &lt;code&gt;push&lt;/code&gt; to, so we will save that for later. For now, we will continue to work locally. Let&amp;rsquo;s add an image to our repository so that we can test it. I am going to use the &lt;a href=&#34;![]%28https://resources.uta.edu/mme/identity/_images/new-logos/new-initials-logo.jpg%29&#34;&gt;UTA Logo&lt;/a&gt; for this example. You can download this and variations from the &lt;a href=&#34;https://resources.uta.edu/mme/identity/brand/index.php&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;UTA Branding Resources&lt;/a&gt; page.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_20-04-35_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;UTA Logo&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;UTA Logo
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Create a new &lt;code&gt;imgs&lt;/code&gt; folder and add your image(s) to it. We can then add the directory along with all of its contents using &lt;code&gt;git add imgs&lt;/code&gt;. You can see the status of your repository by running &lt;code&gt;git status&lt;/code&gt; again. Let&amp;rsquo;s go ahead and &lt;code&gt;commit&lt;/code&gt; these changes.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git commit -am &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Added the UTA logo.&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;making-a-change&#34;&gt;Making a Change&lt;/h3&gt;
&lt;p&gt;For this project, we don&amp;rsquo;t really need to have a bunch of test images. It is sufficient to have one or two. The name of our image folder should probably change to reflect its purpose. Let&amp;rsquo;s start by renaming &lt;code&gt;imgs&lt;/code&gt; to &lt;code&gt;test_imgs&lt;/code&gt;. We can do this with the &lt;code&gt;mv&lt;/code&gt; command in bash. Our repository will now look like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ git status
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;On branch main
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Changes not staged &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; commit:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;use &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git add/rm &amp;lt;file&amp;gt;...&amp;#34;&lt;/span&gt; to update what will be committed&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;use &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git restore &amp;lt;file&amp;gt;...&amp;#34;&lt;/span&gt; to discard changes in working directory&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	deleted:    imgs/uta.png
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Untracked files:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;use &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git add &amp;lt;file&amp;gt;...&amp;#34;&lt;/span&gt; to include in what will be committed&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	test_imgs/
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;no changes added to commit &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;use &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git add&amp;#34;&lt;/span&gt; and/or &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git commit -a&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This might be a tad unexpected. We renamed the folder, but Git is telling us that we deleted a file. This is because Git is tracking the file &lt;code&gt;imgs/uta.png&lt;/code&gt;. When we renamed the folder, Git no longer knew where to find the file. We can fix this by running &lt;code&gt;git rm imgs/uta.png&lt;/code&gt;. This will remove the file from the repository. We can then add the new folder with &lt;code&gt;git add test_imgs&lt;/code&gt;. However, if we simply use &lt;code&gt;git add test_imgs&lt;/code&gt;, Git will not know that we renamed the folder. We can fix this by using the &lt;code&gt;-A&lt;/code&gt; flag. This will tell Git to add all changes, including renames. Our repository will now look like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ git status
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;On branch main
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Changes to be committed:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;use &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git restore --staged &amp;lt;file&amp;gt;...&amp;#34;&lt;/span&gt; to unstage&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    renamed:    imgs/uta.png -&amp;gt; test_imgs/uta.png
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Go ahead and &lt;code&gt;commit&lt;/code&gt; these changes.&lt;/p&gt;
&lt;h2 id=&#34;ignoring-files&#34;&gt;Ignoring Files&lt;/h2&gt;
&lt;p&gt;There are certain files and directories that will end up in our project folder that we do not want to track. For example, we may want to resize images and save them in a local &lt;code&gt;output&lt;/code&gt; directory. However, we do not want to track any of these images. We can have Git remember what we want or do &lt;em&gt;not&lt;/em&gt; want using an ignore file. This file will contain a list of files and directories that we want to ignore. Let&amp;rsquo;s create a &lt;code&gt;.gitignore&lt;/code&gt; file and add the following line:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;output/
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Make sure you add and commit the &lt;code&gt;.gitignore&lt;/code&gt; file.&lt;/p&gt;
&lt;h2 id=&#34;branching&#34;&gt;Branching&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s go ahead and test our program by resizing one of our images. I&amp;rsquo;m going to to resize &lt;code&gt;uta.png&lt;/code&gt; to have a width of 500 pixels using the following command.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;python pyresize.py test_imgs/uta.png &lt;span style=&#34;color:#ae81ff&#34;&gt;500&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Our program doesn&amp;rsquo;t support creating a new file when resizing. Instead, it resizes the file and overwrites the original. We should add this feature and modify it without messing up our current code base. This is where branching comes in. We can create a new branch that will contain our new feature. We can then test it and merge it back into the main branch once we are happy with it.&lt;/p&gt;
&lt;p&gt;Every &lt;code&gt;commit&lt;/code&gt; that we make is a snapshot of the entire project up to that point. There is a unique identifier attached to each commit. If we want to work on a specific bug or new feature without affecting the current code base, we can create a branch to track those changes independently of the other branches. The main benefits are that we can potentially break the code base without affecting the production-ready code. We can also work on multiple features at the same time without affecting each other.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s create a new branch called &lt;code&gt;output_write&lt;/code&gt;. We can do this with the &lt;code&gt;git branch&lt;/code&gt; command.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git branch output_write
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;





&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_20-38-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;The result of creating a new branch named `testing`. ([source](&amp;lt;https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell&amp;gt;))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;The result of creating a new branch named &lt;code&gt;testing&lt;/code&gt;. (&lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;source&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This only creates a new branch, but we are still on the &lt;code&gt;main&lt;/code&gt; branch. We can see this by running &lt;code&gt;git branch&lt;/code&gt;. The current branch will be highlighted with an asterisk. The current branch is pointed to by the &lt;code&gt;HEAD&lt;/code&gt; pointer. We can switch to the new branch using &lt;code&gt;git checkout&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git checkout output_write
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;





&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_20-40-25_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;The `HEAD` pointer after switching to a new branch. ([source](&amp;lt;https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell&amp;gt;))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;The &lt;code&gt;HEAD&lt;/code&gt; pointer after switching to a new branch. (&lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;source&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;modifying-the-code&#34;&gt;Modifying the Code&lt;/h3&gt;
&lt;p&gt;Now that we are on the &lt;code&gt;output_write&lt;/code&gt; branch, we can modify the code without affecting the &lt;code&gt;main&lt;/code&gt; branch. Let&amp;rsquo;s modify our original function to take in an additional argument: the output path. We can then use this path to save the resized image to a new file. Our new code will look like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sys
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; PIL &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Image
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;resize_image&lt;/span&gt;(image_path, width, output_path):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(image_path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    wpercent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (width &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; float(image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;size[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    hsize &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int((float(image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;size[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; float(wpercent)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;resize((width, hsize), Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;LANCZOS)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;save(output_path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; __name__ &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;__main__&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    resize_image(sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argv[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], int(sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argv[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]), sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argv[&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Go ahead and commit these changes. Since we have already moved the &lt;code&gt;HEAD&lt;/code&gt; pointer to the new branch, this change will not affect our &lt;code&gt;main&lt;/code&gt; branch. The figure below is analagous to this scenario.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_20-45-17_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;The result of committing changes to a new branch. ([source](&amp;lt;https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell&amp;gt;))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;The result of committing changes to a new branch. (&lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;source&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Let&amp;rsquo;s test our program once more by resizing an image and saving it to the &lt;code&gt;output&lt;/code&gt; directory. We can do this with the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;python pyresize.py test_imgs/uta.png &lt;span style=&#34;color:#ae81ff&#34;&gt;500&lt;/span&gt; output/uta.png
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Notice that when you run &lt;code&gt;git status&lt;/code&gt; after resizing the image and saving to the &lt;code&gt;output&lt;/code&gt; directory, it does not report any changes. Our ignore file is working as intended!&lt;/p&gt;
&lt;h2 id=&#34;merging&#34;&gt;Merging&lt;/h2&gt;
&lt;p&gt;Now that we have completed our new feature and tested it, we should merge these changes back to the &lt;code&gt;main&lt;/code&gt; branch. We can do this with the &lt;code&gt;git merge&lt;/code&gt; command. First, we need to switch back to the &lt;code&gt;main&lt;/code&gt; branch.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git checkout main
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can then merge the &lt;code&gt;output_write&lt;/code&gt; branch into the &lt;code&gt;main&lt;/code&gt; branch.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git merge output_write
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will merge the changes from the &lt;code&gt;output_write&lt;/code&gt; branch into the &lt;code&gt;main&lt;/code&gt; branch. If there are any conflicts, Git will let you know and you can resolve them manually. Once the merge is complete, you can delete the &lt;code&gt;output_write&lt;/code&gt; branch.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git branch -d output_write
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;





&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-27_20-51-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;The result of merging a branch into the `master` branch. ([source](&amp;lt;https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell&amp;gt;))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;The result of merging a branch into the &lt;code&gt;master&lt;/code&gt; branch. (&lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;source&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The figure above shows the history of a repository in which a branch named &lt;code&gt;iss53&lt;/code&gt; was created, modified with new commits, and eventually merged back into the &lt;code&gt;master&lt;/code&gt; branch.&lt;/p&gt;
&lt;h2 id=&#34;remotes&#34;&gt;Remotes&lt;/h2&gt;
&lt;p&gt;We have now covered the basics of using Git locally. Eventually, we will want our changes to be backed up on a remote server. This will allow us to collaborate with others and work on our projects from multiple machines. There are many services that provide this functionality. We will use GitHub for this example, but the process is similar for other services.&lt;/p&gt;
&lt;h3 id=&#34;creating-a-repository&#34;&gt;Creating a Repository&lt;/h3&gt;
&lt;p&gt;First, we need to create a new repository on GitHub. You can do this by clicking the &lt;code&gt;New&lt;/code&gt; button on the &lt;a href=&#34;https://github.com&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;GitHub homepage&lt;/a&gt;. I am only going to add a short description of this program. Go ahead and click &lt;code&gt;Create repository&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;GitHub supports both SSH and HTTPS. I already have an SSH key set up. If you haven&amp;rsquo;t configured one yet, check out &lt;a href=&#34;https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Adding a new SSH key to your GitHub account&lt;/a&gt; for instructions on how to do so. You can also use HTTPS, this requires a personal access token. More information can be found at &lt;a href=&#34;https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Creating a personal access token&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Once created, we will have the option to use either our HTTPS or SSH URL. Mine is &lt;code&gt;git@github.com:ajdillhoff/pyresize.git&lt;/code&gt;. We can add this as a remote repository using the &lt;code&gt;git remote add&lt;/code&gt; command.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git remote add origin git@github.com:ajdillhoff/pyresize.git
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;pushing-to-a-remote&#34;&gt;Pushing to a Remote&lt;/h3&gt;
&lt;p&gt;Now that we have a remote repository, we can push our changes to it. We can do this with the &lt;code&gt;git push&lt;/code&gt; command. However, we need to specify the remote repository and the branch that we want to push. We can do this with the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git push -u origin main
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;That&amp;rsquo;s it! Our changes are now backed up on GitHub. We can view our repository by navigating to the URL in our browser. We can also view the commit history by clicking the &lt;code&gt;Commits&lt;/code&gt; link.&lt;/p&gt;
&lt;h2 id=&#34;cloning-an-existing-repository&#34;&gt;Cloning an Existing Repository&lt;/h2&gt;
&lt;p&gt;If our repository already exists on GitHub, we can clone it to our local machine. This will create a new directory with the same name as the repository. We can do this with the &lt;code&gt;git clone&lt;/code&gt; command. Let&amp;rsquo;s clone the &lt;code&gt;pyresize&lt;/code&gt; repository that we just created.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git clone git@github.com:ajdillhoff/pyresize.git
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can clone using either the HTTPS or SSH URLs. Make sure you have the appropriate key or access token to do so.&lt;/p&gt;
&lt;h3 id=&#34;pulling-from-a-remote&#34;&gt;Pulling from a Remote&lt;/h3&gt;
&lt;p&gt;Now that we have cloned the repository, we can make changes and push them to the remote. However, if someone else makes changes to the remote repository, we will need to pull those changes to our local repository. We can do this with the &lt;code&gt;git pull&lt;/code&gt; command.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git pull origin main
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Git will always require that we are up-to-date with the remote before we can push our changes. If someone else has made changes to the remote, we will need to pull those changes before we can push our own. This is to prevent conflicts.&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;We have covered the basics of using Git. We have created a repository, staged and committed changes, created branches, merged branches, and pushed our changes to a remote repository. There are many more features that we have not covered, but this should be enough to get you started. If you are interested in learning more, check out the &lt;a href=&#34;https://git-scm.com/book/en/v2&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Git Book&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Command Reference&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Command&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git init&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Create a new repository&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git config&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Configure Git&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git status&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;View the status of your repository&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git add&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Add files to the staging area&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git commit&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Commit changes to the repository&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git branch&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Create, list, or delete branches&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git checkout&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Switch branches or restore working tree files&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git merge&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Join two or more development histories together&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git remote&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Manage set of tracked repositories&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git push&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Update remote refs along with associated objects&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git clone&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Clone a repository into a new directory&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git pull&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fetch from and integrate with another repository or a local branch&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git log&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Show commit logs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git rm&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Remove files from the working tree and from the index&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git mv&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Move or rename a file, a directory, or a symlink&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git branch -d&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Delete a branch&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;git remote add&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Add a remote repository&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Python List Comprehensions</title>
      <link>https://ajdillhoff.github.io/notes/list_comprehensions/</link>
      <pubDate>Sat, 26 Aug 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/list_comprehensions/</guid>
      <description>&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/ajdillhoff/python-examples/blob/main/basics/list_comprehensions.ipynb&#34;&gt;
  &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;/&gt;
&lt;/a&gt;
&lt;h1 id=&#34;lists-and-list-comprehensions&#34;&gt;Lists and List Comprehensions&lt;/h1&gt;
&lt;p&gt;List comprehensions provide a concise way to create lists, and are often faster than using a for-loop. They are inspired by set-builder notation in mathematics.&lt;/p&gt;
&lt;p&gt;This notebook demonstrates common list functions as well as the syntax and basic usage of list comprehensions.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Naomi&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;James&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Amos&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bobbie&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Append &amp;#34;Miller&amp;#34; to the end of the list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Miller&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# This can also be accomplished using the + operator or the extend() method&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# These are similar, where `+=` is shorthand for `extend`&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extend([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Chrisjen&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Alex&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# `extend()` works on any iterable&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extend([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Fred&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Dawes&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Ashford&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# This can also be done using slicing&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# names[len(names):] = [&amp;#34;Fred&amp;#34;, &amp;#34;Dawes&amp;#34;, &amp;#34;Ashford&amp;#34;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Insert &amp;#34;Holden&amp;#34; at the beginning of the list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;insert(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Holden&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can easily remove items&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;remove(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Alex&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can also remove items by value&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;remove(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Ashford&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can also remove items by index. This will return the removed item&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;eros_passenger &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pop(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(eros_passenger &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; is on his way to Venus&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# If your list has duplicates, you can count the number of times a value appears&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Naomi&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; appears &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; str(names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Naomi&amp;#34;&lt;/span&gt;)) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; time(s)&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Reversing a list is easy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reverse()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can also create a shallow copy of a list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names_copy &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Miller is on his way to Venus
Naomi appears 1 time(s)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sorting&#34;&gt;Sorting&lt;/h2&gt;
&lt;p&gt;A list can be sorted by calling the &lt;code&gt;sort&lt;/code&gt; function. The list is sorted in-place, meaning that the original list is modified. Since a list can contain any type of object, the objects must be comparable to each other. For example, a list of strings can be sorted alphabetically, but a list of strings and integers cannot be sorted.&lt;/p&gt;
&lt;p&gt;In cases with mixed types, a custom &lt;code&gt;key&lt;/code&gt; function can be passed to the &lt;code&gt;sort&lt;/code&gt; function. The &lt;code&gt;key&lt;/code&gt; function is called on each element of the list, and the return value is used to sort the list. For example, to sort a list of strings and integers by the length of the string, the &lt;code&gt;key&lt;/code&gt; function would transform the integers into strings.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; l &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;abc&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ab&amp;#39;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; l&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sort(key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;str)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; l
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;abc&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ab&amp;#39;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Sorting is a very common operation, and Python gives us some level of control over how the items are sorted.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# The default is to sort in ascending order&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Sort the list in ascending order&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sort()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(names)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Sort the list in descending order&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sort(reverse&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(names)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can also sort by a key function, such as the length of each name&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sort(key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;len)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(names)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[&#39;Amos&#39;, &#39;Bobbie&#39;, &#39;Chrisjen&#39;, &#39;Dawes&#39;, &#39;Fred&#39;, &#39;Holden&#39;, &#39;James&#39;, &#39;Naomi&#39;]
[&#39;Naomi&#39;, &#39;James&#39;, &#39;Holden&#39;, &#39;Fred&#39;, &#39;Dawes&#39;, &#39;Chrisjen&#39;, &#39;Bobbie&#39;, &#39;Amos&#39;]
[&#39;Fred&#39;, &#39;Amos&#39;, &#39;Naomi&#39;, &#39;James&#39;, &#39;Dawes&#39;, &#39;Holden&#39;, &#39;Bobbie&#39;, &#39;Chrisjen&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s introduce a slightly more complex scenario in which each person in the list rolls a 20-sided die. We&amp;rsquo;ll use the random module to generate a random number between 1 and 20 for each person. Using those values, we can then sort the list by the roll of the die, in descending order.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; random
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Roll a 20-sided die for each person using a for loop&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# rolls = []&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# for name in names:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#     rolls.append(random.randint(1, 20))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# The above code is commented out by default because it can be written more succinctly using a list comprehension&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rolls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; names]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(names)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;index(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Fred&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Sort the names by the roll of the die, in descending order&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# First, create a function that returns the roll based on the name&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_roll&lt;/span&gt;(name):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# print(names) # reveals an empty list!&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; rolls[names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;index(name)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# names.sort(key=get_roll, reverse=True)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# The above will not work because `names` does not exist within the scope of the function.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can instead combine the rolls with the names using the zip() function&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names_and_rolls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(zip(names, rolls))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names_and_rolls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sort(key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; roll: roll[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], reverse&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(names_and_rolls)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[&#39;Fred&#39;, &#39;Amos&#39;, &#39;Naomi&#39;, &#39;James&#39;, &#39;Dawes&#39;, &#39;Holden&#39;, &#39;Bobbie&#39;, &#39;Chrisjen&#39;]
[(&#39;Dawes&#39;, 19), (&#39;Fred&#39;, 18), (&#39;Amos&#39;, 12), (&#39;Naomi&#39;, 11), (&#39;Bobbie&#39;, 10), (&#39;Holden&#39;, 8), (&#39;Chrisjen&#39;, 7), (&#39;James&#39;, 5)]
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;list-comprehensions&#34;&gt;List Comprehensions&lt;/h1&gt;
&lt;p&gt;We can also create nested list comprehensions, which is equivalent to nested for loops. For example, let&amp;rsquo;s create a 3x4 matrix using a nested list comprehension.&lt;/p&gt;
&lt;h2 id=&#34;nested-list-comprehension&#34;&gt;Nested list comprehension&lt;/h2&gt;
&lt;h2 id=&#34;advanced-list-comprehension&#34;&gt;Advanced list comprehension&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Nested list comprehension&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create a 2D list where each row represents the top 5 rolls for each person&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;top_rolls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; names]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(top_rolls)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can create a similar 2D list where each row is a tuple of the name and the top 5 rolls&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;top_rolls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [(name, [random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)]) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; name &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; names]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(top_rolls)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# If we wrote this with loops, it would look like this:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;top_rolls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; name &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; names:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    rolls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        rolls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    top_rolls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append((name, rolls))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[[7, 1, 11, 15, 14], [3, 14, 15, 10, 9], [7, 16, 17, 15, 19], [12, 6, 10, 17, 19], [10, 4, 7, 17, 11], [16, 1, 14, 2, 12], [3, 10, 2, 12, 13], [7, 17, 16, 3, 6]]
[(&#39;Fred&#39;, [15, 7, 1, 3, 16]), (&#39;Amos&#39;, [7, 15, 9, 4, 6]), (&#39;Naomi&#39;, [19, 7, 12, 5, 6]), (&#39;James&#39;, [6, 11, 4, 10, 17]), (&#39;Dawes&#39;, [13, 5, 15, 19, 8]), (&#39;Holden&#39;, [14, 18, 16, 5, 15]), (&#39;Bobbie&#39;, [12, 15, 8, 18, 20]), (&#39;Chrisjen&#39;, [18, 16, 19, 10, 11])]
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;performance-of-list-comprehensions-versus-for-loops&#34;&gt;Performance of list comprehensions versus &lt;code&gt;for&lt;/code&gt; loops&lt;/h1&gt;
&lt;p&gt;A strong argument for list comprehensions is that they are more elegant and easier to read. However, they are also faster than for loops. Let&amp;rsquo;s compare the performance of list comprehensions and for loops.&lt;/p&gt;
&lt;p&gt;Two common benchmarks to test their performance are to append numbers to a list and to square numbers. Let&amp;rsquo;s compare the performance of list comprehensions and for loops for these two tasks.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; timeit
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Benchmark #1: Append vs. List Comprehension&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Append&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;for_append&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    names &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1000000&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        names&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(i)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# print the average out of 10 runs (in milliseconds)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Append: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; str(timeit&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;timeit(for_append, number&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# List Comprehension&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;list_comprehension&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    names &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [i &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1000000&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# print the average out of 10 runs (in milliseconds)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Append: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; str(timeit&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;timeit(for_append, number&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Append: 371.44755799999984
Append: 351.0218179999356
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Benchmark 2: Squaring Numbers&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# For loop&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;for_loop&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    squares &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1000000&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        squares&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(i&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# print the average out of 10 runs (in milliseconds)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;For Loop: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; str(timeit&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;timeit(for_loop, number&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# List Comprehension&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;list_comprehension&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    squares &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [i&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1000000&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# print the average out of 10 runs (in milliseconds)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;List Comprehension: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; str(timeit&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;timeit(list_comprehension, number&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;For Loop: 593.4785219997138
List Comprehension: 572.2285600004398
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Control Flow in Python</title>
      <link>https://ajdillhoff.github.io/notes/control_flow_in_python/</link>
      <pubDate>Tue, 22 Aug 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/control_flow_in_python/</guid>
      <description>&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/ajdillhoff/python-examples/blob/main/basics/control_flow.ipynb&#34;&gt;
  &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;/&gt;
&lt;/a&gt;
&lt;h1 id=&#34;control-flow&#34;&gt;Control Flow&lt;/h1&gt;
&lt;p&gt;Control flow allows us to build programs that react to some pre-determined condition. For example, what happens when a user logs in with the correct credentials? What if they don&amp;rsquo;t give valid credentials?&lt;/p&gt;
&lt;p&gt;This notebook covers the basic tools to writing conditional statements in Python. It follows Chapter 3 in &lt;a href=&#34;https://www.py4e.com/html3/03-conditional&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Python for Everyone&lt;/a&gt; by Charles Severance along with my own examples.&lt;/p&gt;
&lt;h2 id=&#34;boolean-expressions&#34;&gt;Boolean Expressions&lt;/h2&gt;
&lt;p&gt;A boolean expression evaluates to either &lt;code&gt;True&lt;/code&gt; or &lt;code&gt;False&lt;/code&gt;. This type of expression would be used to check status codes or to check if a user has entered the correct password, for example.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1==1 is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1==2 is &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;1==1 is True
1==2 is False
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Operators like &lt;code&gt;==&lt;/code&gt; are called &lt;em&gt;relational operators&lt;/em&gt; and they compare two operands and return either &lt;code&gt;True&lt;/code&gt; or &lt;code&gt;False&lt;/code&gt;. Other relational operators include:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# x is not equal to y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# x is greater than y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# x is less than y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# x is greater than or equal to y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# x is less than or equal to y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# x is the same as y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# x is not the same as y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The last two operators, &lt;code&gt;is&lt;/code&gt; and &lt;code&gt;is not&lt;/code&gt;, are used to check if two variables are referencing the same object. The fact that the operands must be &lt;em&gt;objects&lt;/em&gt; is important here. You should avoid comparing a value with a variable. Python will let you do this, but it will also output a warning.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# True, but not recommended&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# False, but not recommended&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;gt;:3: SyntaxWarning: &amp;quot;is&amp;quot; with a literal. Did you mean &amp;quot;==&amp;quot;?
&amp;lt;&amp;gt;:3: SyntaxWarning: &amp;quot;is&amp;quot; with a literal. Did you mean &amp;quot;==&amp;quot;?
/var/folders/vd/wbzsx0g538nfr96xq81fp7k40000gn/T/ipykernel_24914/759655086.py:3: SyntaxWarning: &amp;quot;is&amp;quot; with a literal. Did you mean &amp;quot;==&amp;quot;?
  x is 5





True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Python includes three logical operators that are verbose compared to other languages.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# True if both x and y are True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; y &lt;span style=&#34;color:#75715e&#34;&gt;# True if either x or y are True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; x &lt;span style=&#34;color:#75715e&#34;&gt;# True if x is False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Example: FizzBuzz&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Consider two possible solutions to the FizzBuzz problem&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Solution 1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;FizzBuzz&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Solution 2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Fizz&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Buzz&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;FizzBuzz
Fizz
Buzz
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the second solution, the output was separated to two separate lines since the &lt;code&gt;print&lt;/code&gt; function automatically adds a newline. We can change this behavior by adding a second argument to the &lt;code&gt;print&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Fizz&amp;#34;&lt;/span&gt;, end&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Buzz&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;FizzBuzz
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;conditional-execution&#34;&gt;Conditional Execution&lt;/h2&gt;
&lt;p&gt;We have already used a key conditional execution tool: the &lt;code&gt;if&lt;/code&gt; statement. The &lt;code&gt;if&lt;/code&gt; statement allows us to execute a block of code if a condition is met. The general syntax is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; condition:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if condition is True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Also note that Python is particular about indentation. The code that is executed if the condition is met must be indented. The standard is to use four spaces for each level of indentation.&lt;/p&gt;
&lt;p&gt;We can also chain conditional statements together using &lt;code&gt;elif&lt;/code&gt; and &lt;code&gt;else&lt;/code&gt;. The &lt;code&gt;elif&lt;/code&gt; statement is short for &amp;ldquo;else if&amp;rdquo; and allows us to check another condition if the previous condition was not met. The &lt;code&gt;else&lt;/code&gt; statement is used to execute code if none of the previous conditions were met. The general syntax is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; condition:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if condition is True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; condition:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if the first condition is False and this condition is True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if all other conditions are False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;switch-statements&#34;&gt;Switch Statements&lt;/h3&gt;
&lt;p&gt;Until version 3.10, Python did not have a &lt;code&gt;switch&lt;/code&gt; statement. This is a conditional statement that allows us to check a variable against a series of values.&lt;/p&gt;
&lt;p&gt;With version 3.10 comes the &lt;code&gt;match&lt;/code&gt; statement. This statement is similar to the &lt;code&gt;switch&lt;/code&gt; statement in other languages. The general syntax is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;match&lt;/span&gt; variable:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; value1:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if variable == value1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; value2:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if variable == value2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; value3:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if variable == value3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; _:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if none of the previous conditions were met&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;language &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; input(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;What is your favorite programming language? &amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;match&lt;/span&gt; language:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Python&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;You&amp;#39;re in the right place.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Java&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Do you despise C++ as much as the creator of Java?&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;C++&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;You probably like game development.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;C&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Speed is your thing.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; _:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;You like something else!&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;You like something else!
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unlike other languages that implement a &lt;code&gt;switch&lt;/code&gt; statement, Python&amp;rsquo;s &lt;code&gt;match&lt;/code&gt; statement does not have a &lt;code&gt;break&lt;/code&gt; statement. We can still utilize fall-through behavior by including multiple values in a single case separated by &lt;code&gt;|&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;match&lt;/span&gt; variable:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; value1 &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; value2:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if variable == value1 or variable == value2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; value3:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if variable == value3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; _:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute if none of the previous conditions were met&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;language &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; input(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;What is your favorite programming language? &amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;match&lt;/span&gt; language:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Python&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;python&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;You&amp;#39;re in the right place.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Java&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Do you despise C++ as much as the creator of Java?&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;C++&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;You probably like game development.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;C&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Speed is your thing.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;case&lt;/span&gt; _:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;You like something else!&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;You like something else!
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;iterations&#34;&gt;Iterations&lt;/h2&gt;
&lt;p&gt;Iterations allow us to execute a block of code multiple times. This is useful for iterating over a list of items or for executing a block of code until a condition is met.&lt;/p&gt;
&lt;p&gt;Python supports both a &lt;code&gt;while&lt;/code&gt; loop and a &lt;code&gt;for&lt;/code&gt; loop. The &lt;code&gt;while&lt;/code&gt; loop will execute a block of code until a condition is met. The &lt;code&gt;for&lt;/code&gt; loop will iterate over a sequence of items.&lt;/p&gt;
&lt;h3 id=&#34;for-loops&#34;&gt;For Loops&lt;/h3&gt;
&lt;p&gt;As opposed to something like C, Python&amp;rsquo;s &lt;code&gt;for&lt;/code&gt; loop is more like a &lt;code&gt;foreach&lt;/code&gt; loop. The &lt;code&gt;for&lt;/code&gt; loop will iterate over a sequence of items. The general syntax is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; item &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sequence:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute for each item in the sequence&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It is commonly used with the &lt;code&gt;range&lt;/code&gt; function to iterate over a sequence of numbers. The &lt;code&gt;range&lt;/code&gt; function takes three arguments: &lt;code&gt;start&lt;/code&gt;, &lt;code&gt;stop&lt;/code&gt;, and &lt;code&gt;step&lt;/code&gt;. The &lt;code&gt;start&lt;/code&gt; argument is the first number in the sequence. The &lt;code&gt;stop&lt;/code&gt; argument is the last number in the sequence. The &lt;code&gt;step&lt;/code&gt; argument is the amount to increment the sequence by. The &lt;code&gt;step&lt;/code&gt; argument is optional and defaults to &lt;code&gt;1&lt;/code&gt;. The &lt;code&gt;stop&lt;/code&gt; argument is required. The &lt;code&gt;start&lt;/code&gt; argument is optional and defaults to &lt;code&gt;0&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(i)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;while-loops&#34;&gt;While Loops&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;while&lt;/code&gt; loop will execute a block of code until a condition is met. The general syntax is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; condition:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute while condition is True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;lists&#34;&gt;Lists&lt;/h2&gt;
&lt;p&gt;Lists are a sequence of values. They are similar to arrays in other languages. The values in a list are called &lt;em&gt;elements&lt;/em&gt; or &lt;em&gt;items&lt;/em&gt;. Lists are mutable, meaning that we can change the values in a list. Lists are also ordered, meaning that the order of the elements in a list is important.&lt;/p&gt;
&lt;p&gt;We can create a list by separating the elements with commas and surrounding the list with square brackets.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Naomi&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bobbie&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;James&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Amos&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Chrisjen&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Alex&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Clarissa&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;names_and_numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Naomi&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bobbie&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;James&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Amos&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;11&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Chrisjen&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;13&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Alex&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Clarissa&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;17&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can even include lists in our lists&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;nested_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;]]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;iterating-over-lists&#34;&gt;Iterating Over Lists&lt;/h3&gt;
&lt;p&gt;We can iterate over a list using a &lt;code&gt;for&lt;/code&gt; loop. The &lt;code&gt;for&lt;/code&gt; loop will iterate over each element in the list. The general syntax is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; item &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; list:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# code to execute for each item in the list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If the list contains tuples, we can use tuple unpacking to assign the values in the tuple to multiple variables.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;), (&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;), (&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; x, y &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; numbers:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(x, y)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;combining-lists-with-zip&#34;&gt;Combining Lists with &lt;code&gt;zip&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;zip&lt;/code&gt; function allows us to combine two lists into a single list of tuples. The first element in the first list will be paired with the first element in the second list, the second element in the first list will be paired with the second element in the second list, and so on. The general syntax is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;user_ids &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;usernames &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;alice&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;bob&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;charlie&amp;#39;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;users &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; zip(user_ids, usernames)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;user_ids &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;user_names &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Naomi&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bobbie&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;James&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Amos&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Chrisjen&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can combine these lists into a single list of tuples&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;user_ids_and_names &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; zip(user_ids, user_names)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We can also convert the zip object into a list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;user_ids_and_names &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(user_ids_and_names)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; users &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; user_ids_and_names:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(users)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;(1, &#39;Naomi&#39;)
(2, &#39;Bobbie&#39;)
(3, &#39;James&#39;)
(4, &#39;Amos&#39;)
(5, &#39;Chrisjen&#39;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Python</title>
      <link>https://ajdillhoff.github.io/notes/introduction_to_python/</link>
      <pubDate>Sun, 20 Aug 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/introduction_to_python/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#programming-with-python&#34;&gt;Programming with Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variables-values-and-data-types&#34;&gt;Variables, Values, and Data Types&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#basic-operators&#34;&gt;Basic Operators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#statements-and-expressions&#34;&gt;Statements and Expressions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#basic-i-o&#34;&gt;Basic I/O&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#commenting-code&#34;&gt;Commenting Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;These notes are focused on introducing programming with Python for those without a technical background.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://docs.python.org/3/faq/general.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;official website&lt;/a&gt; provides the following description of Python.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Python is an interpreted, interactive, object-oriented programming language. It incorporates modules, exceptions, dynamic typing, very high level dynamic data types, and classes. It supports multiple programming paradigms beyond object-oriented programming, such as procedural and functional programming. Python combines remarkable power with very clear syntax. It has interfaces to many system calls and libraries, as well as to various window systems, and is extensible in C or C++. It is also usable as an extension language for applications that need a programmable interface. Finally, Python is portable: it runs on many Unix variants including Linux and macOS, and on Windows.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you have never studied any programming languages before, much of this description will be useless to you. There is no context for the types of programming (object-oriented, functional, etc.), and the fact that is extensible in C or C++ may mean absolutely nothing.&lt;/p&gt;
&lt;h3 id=&#34;why-python&#34;&gt;Why Python?&lt;/h3&gt;
&lt;p&gt;Given that this course is for prospective data science practitioners, and the fact that we have less than a month to cover a programming language, Python is a natural choice. It is widely used in the field of Machine Learning and is gaining more and more ground over R (or so I think) for statistics. There are many third-party libraries for data analysis, visualization, and just about any other data science application we can think of.&lt;/p&gt;
&lt;h3 id=&#34;how-to-use-these-notes&#34;&gt;How to use these notes&lt;/h3&gt;
&lt;p&gt;These notes are organized to follow each major topic in Python. They will also follow the free online book &lt;a href=&#34;https://www.py4e.com/book&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Python for Everybody&lt;/a&gt;. These particular lecture notes will start with Chapter 2: Variables, expressions, and statements. It is highly recommended that you run the examples on your own machine, and you are encouraged to make changes. Try to break the code and fix it again. Have it output something different and change the program&amp;rsquo;s purpose entirely.&lt;/p&gt;
&lt;p&gt;A Python notebook will accompany each lecture and will be accessible on &lt;a href=&#34;https://github.com/ajdillhoff/python-examples&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;my GitHub page&lt;/a&gt;. I will also include code snippets directly in this article to highlight a particular example or point.&lt;/p&gt;
&lt;h3 id=&#34;programming-is-hard&#34;&gt;Programming is Hard&lt;/h3&gt;
&lt;p&gt;Before we dive into the language itself, there are a few things that are important to keep in mind. First, &lt;strong&gt;programming is hard&lt;/strong&gt;. It is a skill that requires practice. The tools that you use to program are constantly evolving to keep up with the use-cases of the day. A processor is a complex calculator which means we have to be extremely explicit about the instructions we provide. If you are picking this up for the first time, remember to be patient and be kind to yourself. You will be able to work on big projects that are important to you, but we all have to start somewhere.&lt;/p&gt;
&lt;h3 id=&#34;resources-are-finite&#34;&gt;Resources are Finite&lt;/h3&gt;
&lt;p&gt;Another important thing to remember is that we are working with limited resources. There is only so much memory and storage space that we can access. These notes are not meant to accompany a full course on hardware architectures, so we will use the following diagram to visualize this point.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-20_13-52-14_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Figure 1.3 from Python for Everybody.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Figure 1.3 from Python for Everybody.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The closer the memory is to the CPU, the quicker it can be accessed. Most of the algorithms and data structures we will study in this course will be used with data in &lt;strong&gt;main memory&lt;/strong&gt;. The trade off is that memory that is closer to the CPU is more expensive and has reduced capacity when compared with secondary memory. When we start working with larger sources of data, we will need to adapt our solutions to work with memory that is not directly accessible through a local machine. For now, keep this picture in mind as we dive into Python.&lt;/p&gt;
&lt;h2 id=&#34;programming-with-python&#34;&gt;Programming with Python&lt;/h2&gt;
&lt;p&gt;Programming languages provide the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a way to write instructions (&lt;strong&gt;syntax, statements, expressions&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;a way to execute a complex series of instructions (&lt;strong&gt;functions&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;a way to store the results of computations and represent data (&lt;strong&gt;variables, data structures&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They mostly differ in how the language is written, the &lt;strong&gt;syntax&lt;/strong&gt;. Consider the following snippet of Python code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;username &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;test&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;password &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;securePass1&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;user_logged_in &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; login(username, password) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    user_logged_in &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Even if you have never seen any Python code before, you could probably figure out what this block of code is doing. First, 3 &lt;strong&gt;variables&lt;/strong&gt; are defined which store the username, password, and a &lt;em&gt;flag&lt;/em&gt; that represents whether or not the user is logged into the system.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;control statement&lt;/strong&gt; &lt;code&gt;if&lt;/code&gt; is declared to evaluate if an &lt;strong&gt;expression&lt;/strong&gt; is true. This expression calls a function named &lt;code&gt;login&lt;/code&gt; and passes the user&amp;rsquo;s credentials as its arguments. We can assume that the call to &lt;code&gt;login&lt;/code&gt; is doing something like validating the user&amp;rsquo;s information and registering their request with a server. If the user was successfully logged in, the function will &lt;code&gt;return True&lt;/code&gt;. In this case, we can updated our variable &lt;code&gt;user_logged_in&lt;/code&gt; to reflect this.&lt;/p&gt;
&lt;h2 id=&#34;variables-values-and-data-types&#34;&gt;Variables, Values, and Data Types&lt;/h2&gt;
&lt;p&gt;The first 3 lines in the example above are variable initializations. The first line is &lt;code&gt;username = &amp;quot;test&amp;quot;&lt;/code&gt; which instructs our machines to create a new &lt;strong&gt;variable&lt;/strong&gt; named &lt;code&gt;username&lt;/code&gt; and assign it the value &lt;code&gt;&amp;quot;test&amp;quot;&lt;/code&gt;. All variables require memory to store their &lt;strong&gt;values&lt;/strong&gt;. When a variable is created, our machine will assign it an address so that it knows where to access that variable&amp;rsquo;s value. This concept is rather simple: &lt;strong&gt;in order for something to exist, there must be space for it.&lt;/strong&gt; As Python developers, we will rarely think about where and how these values are being stored.&lt;/p&gt;
&lt;p&gt;Most languages have rules about what names we can give to variables, and Python is no exception. A variable can use any combination of letters, numbers, and underscores, as long as it does not start with a number and is not the same as a &lt;strong&gt;reserved word&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reserved words in Python&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;and     continue    finally     is          raise
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;as      def         for         lambda      return
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;assert  del         from        None        True
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;async   elif        global      nonlocal    try
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;await   else        if          not         while
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;break   except      import      or          with
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;class   False       in          pass        yield
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;data-types&#34;&gt;Data Types&lt;/h3&gt;
&lt;p&gt;Different &lt;strong&gt;values&lt;/strong&gt; are represented differently depending on their &lt;strong&gt;type&lt;/strong&gt;. An integer can be represented in binary in a very straightforward manner. &lt;code&gt;10&lt;/code&gt; in base 10 is represented as &lt;code&gt;1010&lt;/code&gt; in binary, for example. Characters in a &lt;code&gt;string&lt;/code&gt; like &lt;code&gt;&amp;quot;securePass1&amp;quot;&lt;/code&gt; are represented using an encoding such as &lt;a href=&#34;https://en.wikipedia.org/wiki/ASCII&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;ASCII&lt;/a&gt; or &lt;a href=&#34;https://en.wikipedia.org/wiki/Unicode&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Unicode&lt;/a&gt;. Real numbers are typically represented as floating-point types using the &lt;a href=&#34;https://en.wikipedia.org/wiki/IEEE_754&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;IEEE 754 Standard for Floating-Point Arithmetic&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When we create a variable in Python, we do not need to explicitly declare what type that variable it is. That is what makes Python &lt;strong&gt;dynamically typed&lt;/strong&gt;. Instead, it will infer the type based on the value. We can always ask Python how it is representing each variable, as seen in the following code.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt; type&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;securePass1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;lt;class &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;str&amp;#39;&lt;/span&gt;&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt; type&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;10&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;lt;class &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;int&amp;#39;&lt;/span&gt;&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt; type&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;3.14&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;lt;class &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;float&amp;#39;&lt;/span&gt;&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;basic-operators&#34;&gt;Basic Operators&lt;/h2&gt;
&lt;p&gt;A programming language would be pretty useless if it did not offer some way to do basic arithmetic. Python supports the following arithmetic operators: &lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt;, &lt;code&gt;//&lt;/code&gt;, &lt;code&gt;**&lt;/code&gt; and &lt;code&gt;%&lt;/code&gt;. You may instantly recognize the first 4, but the last 3 may not be so familiar. Let us start with &lt;code&gt;//&lt;/code&gt;, integer division.&lt;/p&gt;
&lt;p&gt;An integer data type cannot represent decimal values. So what happens if you try to execute something like &lt;code&gt;1 / 2&lt;/code&gt;? We recognize this to be &lt;code&gt;0.5&lt;/code&gt;, but that is not the case with every programming language. In C, for example, &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;2&lt;/code&gt; are treated as integer types by default. If you attempt to evaluate &lt;code&gt;1 / 2&lt;/code&gt;, the result is &lt;code&gt;0&lt;/code&gt; since there is no way to store the decimal information. Essentially, the decimal portion of the result is &lt;strong&gt;truncated&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Python is a little more forgiving. &lt;code&gt;1 / 2&lt;/code&gt; evaluates to &lt;code&gt;0.5&lt;/code&gt;, as we may expect. However, if you want to perform division between these operands as if they were both integers, you can use &lt;code&gt;//&lt;/code&gt;. Indeed, &lt;code&gt;1 // 2&lt;/code&gt; evalutes to &lt;code&gt;0&lt;/code&gt; in Python.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;**&lt;/code&gt; operator is more straightforward: it raises the left-hand operand to whatever value is provided on the right side. For example, &lt;code&gt;2**4&lt;/code&gt; evaluates to &lt;code&gt;16&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally, the modulus operator &lt;code&gt;%&lt;/code&gt; provides the remainder of integer division. So something like &lt;code&gt;5 % 2&lt;/code&gt; would return &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;statements-and-expressions&#34;&gt;Statements and Expressions&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;statement&lt;/strong&gt; is anything that can be executed. This could be a simple variable assignment or a function call.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;username &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;user1&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(username &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;user1&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;An &lt;strong&gt;expression&lt;/strong&gt; is a statement that evaluates into some result. This could be the result of a function call, an assignment, or a complex computation.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;get_user_by_id(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;basic-i-o&#34;&gt;Basic I/O&lt;/h2&gt;
&lt;p&gt;Python provides functions to read input from the user&amp;rsquo;s keyboard as well as print information back to the terminal using &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;print&lt;/code&gt;. When &lt;code&gt;input&lt;/code&gt; is evaluated, it will wait for the user to press &lt;code&gt;Enter&lt;/code&gt; before processing the input. If you would like to provide a text prompt to the user before entering, you can pass the prompt as a string.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; input(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Enter some text: &amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(text)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;An example run of this program may look like the following.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Enter some text: OK here it is
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;OK here it is
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Notice that the result of the &lt;code&gt;input&lt;/code&gt; function is the actual data entered by the user. This is immediately assigned to the &lt;code&gt;text&lt;/code&gt; variable. We can easily print out the value of &lt;code&gt;text&lt;/code&gt; by passing it as an argument to the &lt;code&gt;print&lt;/code&gt; function.&lt;/p&gt;
&lt;h3 id=&#34;formatted-output&#34;&gt;Formatted Output&lt;/h3&gt;
&lt;p&gt;We can work with more detailed output using formatted strings, as demonstrated in the following example.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3.14159265359&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The value of pi is approximately &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;pi&lt;span style=&#34;color:#e6db74&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;.3f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;The value of pi is approximately 3.141.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;More details about the different ways of using formatting strings is documented &lt;a href=&#34;https://docs.python.org/3/tutorial/inputoutput.html?highlight=formatted%20print#formatted-string-literals&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;commenting-code&#34;&gt;Commenting Code&lt;/h2&gt;
&lt;p&gt;The last topic of this introduction is about commenting. Communicating the purpose of your program is not only important when working with others, but you will find it to be extremely helpful as you build larger and larger projects. It is a common trap to dive into an idea with absolute focus, quickly hacking away as your program takes shape. This sort of approach is like a house of cards. As soon as your attention is diverted, it takes time to build that model up in your head again.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-20_21-52-05_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Daily experiences with programming.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Daily experiences with programming.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Writing down your program&amp;rsquo;s purpose and design while documenting its function is paramount for a product that is both robust and maintainable. The simplest way to communicate ideas is to leave &lt;strong&gt;comments&lt;/strong&gt; in the code itself. There are two ways to leave basic comments in Python: single-line and multi-line. The code below demonstrates both.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;This small code example shows how to comment in Python.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;By the way, this is a multi-line comment.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;user1&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# stores the user&amp;#39;s name&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As see above, multi-line comments are wrapped in &lt;code&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;. These are typically reserved for things like function documentation (more on that later). Single-line comments start with &lt;code&gt;#&lt;/code&gt; and can be placed on the same line as a statement.&lt;/p&gt;
&lt;p&gt;There is a third type of commenting called &lt;strong&gt;self-commenting&lt;/strong&gt;. The same example above will motivate this type of commenting. There is nothing invalid about the statement &lt;code&gt;a = &amp;quot;user1&amp;quot;&lt;/code&gt;. It defines a variable named &lt;code&gt;a&lt;/code&gt; whose value is the string &lt;code&gt;&amp;quot;user1&amp;quot;&lt;/code&gt;. However, if there wasn&amp;rsquo;t a comment on the same line describing its purpose, it might not be so clear. There is an easier way to communicate this without commenting at all. We could instead write something like &lt;code&gt;username = &amp;quot;user1&amp;quot;&lt;/code&gt;. The variable name itself resolves any ambiguity about its purpose and obviates the need for an additional comment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Markov Decision Processes</title>
      <link>https://ajdillhoff.github.io/notes/markov_decision_processes/</link>
      <pubDate>Mon, 24 Jul 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/markov_decision_processes/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#key-terms&#34;&gt;Key Terms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#defining-goals&#34;&gt;Defining Goals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#policies-and-values&#34;&gt;Policies and Values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bellman-equations&#34;&gt;Bellman Equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimality&#34;&gt;Optimality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimizing-the-policy&#34;&gt;Optimizing the Policy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;key-terms&#34;&gt;Key Terms&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Agent&lt;/strong&gt;: The learner or decision maker.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt;: The world that the agent can interact with.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;State&lt;/strong&gt;: A representation of the agent and environment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: The agent can take an action in the environment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reward&lt;/strong&gt;: Given to the agent based on actions taken.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Maximize rewards earned over time.&lt;/p&gt;
&lt;p&gt;At time \(t\), the agent observes the state of the environment \(S_t \in \mathcal{S}\) and can select an action \(A_t \in \mathcal{A}(s)\), where \(\mathcal{A}(s)\) suggests that the available actions are dependent on the current state.
At time \(t + 1\), the agent receives a reward \(R_{t+1} \in \mathcal{R}\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-11-15_19-01-30_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;The agent-environment in a Markov decision process (Credit: Sutton &amp;amp;amp; Barto).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;The agent-environment in a Markov decision process (Credit: Sutton &amp;amp; Barto).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To improve its knowledge about an environment or increase its performance on a task, an agent must first be able to interpret or make sense of that environment in some way.
Second, there must be a well defined goal. For an agent playing Super Mario, for example, the goal would be to complete each level while maximizing the score.
Third, the agent must be able to interact with its environment by taking actions.
If the Mario-playing agent could not move Mario around, it would never be able to improve.
If the agent makes a decision which leads to Mario&amp;rsquo;s untimely demise, it would update its knowledge of the world so that it would tend towards a more favorable action.
These three requirements: sensations, actions, and goals, are encapsulated by &lt;strong&gt;Markov Decision Processes&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;A Markov decision process is defined by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(\mathcal{S}\) - a set of states,&lt;/li&gt;
&lt;li&gt;\(\mathcal{A}\) - a set of actions,&lt;/li&gt;
&lt;li&gt;\(\mathcal{R}\) - a set of rewards,&lt;/li&gt;
&lt;li&gt;\(P\) - the transition probability function to determine transition between states,&lt;/li&gt;
&lt;li&gt;\(\gamma\) - discount factor for future rewards.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At time \(t\), an agent in state \(S_t\) selects an action \(A_t\).
At \(t+1\), it receives a reward \(R_{t+1}\) based on that action.&lt;/p&gt;
&lt;p&gt;In a finite MDP, the states, actions, and rewards have a finite number of elements.
Random variables \(R_t\) and \(S_t\) have discrete probability distributions dependent on the preceding state and action.&lt;/p&gt;
&lt;p&gt;\[
p(s&amp;rsquo;, r|s, a) = P\{S_t = s&amp;rsquo;, R_t = r|S_{t-1} = s, A_{t-1} = a\}
\]&lt;/p&gt;
&lt;p&gt;If we want the state transition probabilities, we can sum over the above distribution:&lt;/p&gt;
&lt;p&gt;\[
p(s&amp;rsquo;|s, a) = P\{S_t = s&amp;rsquo;|S_{t-1} = s, A_{t-1}=a\} = \sum_{r\in\mathcal{R}}p(s&amp;rsquo;, r|s, a).
\]&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;reward function&lt;/strong&gt; \(r\) gives the expected &lt;em&gt;next&lt;/em&gt; reward given some state and action:&lt;/p&gt;
&lt;p&gt;\[
r(s, a) = \mathbb{E}[R_{t+1}|S_t=s, A_t=a] = \sum_{r}r \sum_{s&amp;rsquo;}p(s&amp;rsquo;, r|s, a).
\]&lt;/p&gt;
&lt;h2 id=&#34;defining-goals&#34;&gt;Defining Goals&lt;/h2&gt;
&lt;p&gt;In reinforcement learning, the goal is encoded in the form of a &lt;strong&gt;&lt;strong&gt;reward signal&lt;/strong&gt;&lt;/strong&gt;. The agent sets out to &lt;em&gt;maximize&lt;/em&gt; the total amount of reward it receives over an &lt;strong&gt;&lt;strong&gt;episode&lt;/strong&gt;&lt;/strong&gt;. An &lt;strong&gt;&lt;strong&gt;episode&lt;/strong&gt;&lt;/strong&gt; is defined dependent on the problem context and ends in a &lt;strong&gt;&lt;strong&gt;terminal state&lt;/strong&gt;&lt;/strong&gt;. It could be a round of game, a single play, or the result of moving a robot. Typically, the rewards come as a single scalar value at teach time step. This implies that an agent might take an action that results in a negative reward if it is optimal in the long run.&lt;/p&gt;
&lt;p&gt;Formally, the &lt;strong&gt;expected return&lt;/strong&gt; includes a &lt;strong&gt;discount factor&lt;/strong&gt; that allows us to control the trade-off between short-term and long-term rewards:&lt;/p&gt;
&lt;p&gt;\[
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1},
\]&lt;/p&gt;
&lt;p&gt;where \(0 \leq \gamma \leq 1\). This can be written in terms of the expected return itself as well:&lt;/p&gt;
&lt;p&gt;\[
G_t = R_{t+1} + \gamma G_{t+1}.
\]&lt;/p&gt;
&lt;h2 id=&#34;policies-and-values&#34;&gt;Policies and Values&lt;/h2&gt;
&lt;p&gt;Two important concepts that help our agent make decisions are the policy and value functions. A &lt;strong&gt;policy&lt;/strong&gt;, typically denoted by \(\pi\), maps states to actions. Such a function can be deterministic, \(\pi(s) = a\), or stochastic, \(\pi(a|s)\).&lt;/p&gt;
&lt;p&gt;The value of a particular state under a policy \(\pi\) is defined as&lt;/p&gt;
&lt;p&gt;\[
v_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s] = \mathbb{E}_{\pi}\Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}\Bigg|S_t=s\Bigg].
\]&lt;/p&gt;
&lt;p&gt;We also must define the value of taking an action \(a\) in state \(s\) following policy \(\pi\):&lt;/p&gt;
&lt;p&gt;\[
q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t|S_t=s, A_t=a] = \mathbb{E}_{\pi}\Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}\Bigg|S_t=s, A_t=a\Bigg].
\]&lt;/p&gt;
&lt;p&gt;This function defines the expected return of following a particular policy and starting in state \(s\).
Both the &lt;strong&gt;state-value&lt;/strong&gt; and &lt;strong&gt;action-value&lt;/strong&gt; functions can be updated as a result of the agent&amp;rsquo;s experience. How it is updated is method-dependent. Certain methods will also dictate how the policy itself can be updated.&lt;/p&gt;
&lt;h2 id=&#34;bellman-equations&#34;&gt;Bellman Equations&lt;/h2&gt;
&lt;p&gt;The recursive relationship between the value of a state and its future states can be represented using &lt;strong&gt;&lt;strong&gt;Bellman equations&lt;/strong&gt;&lt;/strong&gt;. In RL, we are interested in the equations for both the state-value and action-value. Given the diagram of an MDP, we can see that they are related to each other. To make the following equations easier to understand, it is important to remember the flow of a Markov decision process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;take an action,&lt;/li&gt;
&lt;li&gt;arrive at a state and sense the reward,&lt;/li&gt;
&lt;li&gt;consult the policy for the next action.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With that in mind, let&amp;rsquo;s look at the state-value function first. This function considers the expected value of starting in a state \(s\) and following policy \(\pi\). In other words, we must consider all possible actions and their future rewards.&lt;/p&gt;
&lt;p&gt;\begin{align*}
v_{\pi}(s) &amp;amp;= \mathbb{E}_{\pi}[G_t | S_t = s]\\
&amp;amp;= \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]\\
&amp;amp;= \sum_{a} \pi(a|s) \sum_{s&amp;rsquo;}\sum_{r}p(s&amp;rsquo;, r|s, a)\Big[r + \gamma \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]\Big]\\
&amp;amp;= \sum_{a} \pi(a|s) \sum_{s&amp;rsquo;, r}p(s&amp;rsquo;, r|s, a)\Big[r + \gamma v_{\pi}(s&amp;rsquo;)\Big]\\
&amp;amp;= \sum_{a} \pi(a | s)\big[r(s, a) + \gamma \sum_{s&amp;rsquo;}p(s&amp;rsquo;|s, a)v_{\pi}(s&amp;rsquo;)\big]\\
\end{align*}&lt;/p&gt;
&lt;p&gt;The first sum over actions considers &lt;em&gt;all possible actions&lt;/em&gt;. This is followed by a transition to possible states \(s&amp;rsquo;\) conditioned on taking each action multiplied by the expected value of being at the new state.&lt;/p&gt;
&lt;p&gt;The action-value function follows a similar derivation:&lt;/p&gt;
&lt;p&gt;\begin{align*}
q_{\pi}(s, a) &amp;amp;= \mathbb{E}_{\pi}[G_t|S_t = s, A_t = a]\\
&amp;amp;= \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a]\\
&amp;amp;= r(s, a) + \sum_{s&amp;rsquo;}p(s&amp;rsquo;|s, a) v_{\pi}(s&amp;rsquo;)
\end{align*}&lt;/p&gt;
&lt;p&gt;There is a very similar looking set of terms in the state-value function above, and we should expect that! If we want to evaluate the current state, we need to look ahead at the possible actions and their resulting rewards. Similarly, evaluating the current action requires us to look head at the value of future states.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s expand \(q_{\pi}(s,a)\) once more so that it is written in terms of itself.&lt;/p&gt;
&lt;p&gt;\begin{align*}
q_{\pi}(s, a) &amp;amp;= r(s, a) + \gamma \sum_{s&amp;rsquo;}p(s&amp;rsquo;|s, a) v_{\pi}(s&amp;rsquo;)\\
&amp;amp;= r(s, a) + \gamma \sum_{s&amp;rsquo;}p(s&amp;rsquo;|s, a) \sum_{a&amp;rsquo;} \pi(s&amp;rsquo;, a&amp;rsquo;) \big[r(s&amp;rsquo;, a&amp;rsquo;) + \gamma \sum_{s&amp;rsquo;&amp;rsquo;} p(s&amp;rsquo;&amp;rsquo;|s&amp;rsquo;, a&amp;rsquo;)v_{\pi}(s&amp;rsquo;&amp;rsquo;)]\\
&amp;amp;= r(s, a) + \gamma \sum_{s&amp;rsquo;}p(s&amp;rsquo;|s, a) \sum_{a&amp;rsquo;} \pi(s&amp;rsquo;, a&amp;rsquo;) q_{\pi}(s&amp;rsquo;, a&amp;rsquo;)
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;optimality&#34;&gt;Optimality&lt;/h2&gt;
&lt;p&gt;To solve a reinforcement learning problem, we are interested in finding the policy \(\pi_{*}\) whose expected return is greater than all other possible policies over all states.
An &lt;strong&gt;&lt;strong&gt;optimal policy&lt;/strong&gt;&lt;/strong&gt; will use an &lt;strong&gt;*optimal state-value function&lt;/strong&gt; and &lt;strong&gt;&lt;strong&gt;optimal action-value function&lt;/strong&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;\begin{align*}
v_{*}(s) &amp;amp;= \max_{\pi}v_{\pi}(s)\\
q_{*}(s, a) &amp;amp;= \max_{\pi}q_{\pi}(s, a).
\end{align*}&lt;/p&gt;
&lt;p&gt;The optimal state-value function would select the best possible action instead of summing over all possibley actions starting in state \(s\):&lt;/p&gt;
&lt;p&gt;\begin{align*}
v_{*}(s) &amp;amp;= \max_{a} q_{\pi_*}(s, a)\\
&amp;amp;= \max_{a}\big[r(s, a) + \gamma \sum_{s&amp;rsquo;} p(s&amp;rsquo;|s, a) v_{*}(s&amp;rsquo;)\big]
\end{align*}&lt;/p&gt;
&lt;p&gt;Similarly, the optimal action-value function selects the best possible action from the next state \(s&amp;rsquo;\):&lt;/p&gt;
&lt;p&gt;\[
q_{*}(s) = r(s, a) + \gamma \sum_{s&amp;rsquo;} p(s&amp;rsquo;|s, a) \max_{a} q_{*}(s&amp;rsquo;, a&amp;rsquo;).
\]&lt;/p&gt;
&lt;h2 id=&#34;optimizing-the-policy&#34;&gt;Optimizing the Policy&lt;/h2&gt;
&lt;p&gt;For smaller problems with reasonably small state and action spaces, we can use Dynamic Programming to compute the optimal policy. These methods quickly become intractable as the complexity of our problem increases. As is common in machine learning, we would resort to approximation methods for complex spaces.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;In fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ndash; Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Imagine if you had a set policy that dictated the actions you would take from work to home. In this example, assume the policy is not an optimal policy. One day, you decide to take a left at a particular intersection rather than going forward. After that, you follow your policy as described. If this decision ultimately resulted in you arriving home sooner, you would probably update your policy to always take that left. This intuition describes a result of the &lt;strong&gt;policy improvement theorem&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let \(\pi\) and \(\pi&amp;rsquo;\) be two deterministic policies where&lt;/p&gt;
&lt;p&gt;\[
q_{\pi}(s, \pi&amp;rsquo;(s)) \geq v_{\pi}(s),\quad \forall s.
\]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gradient Boosting</title>
      <link>https://ajdillhoff.github.io/notes/gradient_boosting/</link>
      <pubDate>Mon, 17 Jul 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/gradient_boosting/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#notes-from--no-item-data-friedmangreedyfunctionapproximation2001&#34;&gt;Notes from (NO_ITEM_DATA:friedmanGreedyFunctionApproximation2001)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;notes-from--no-item-data-friedmangreedyfunctionapproximation2001&#34;&gt;Notes from (NO_ITEM_DATA:friedmanGreedyFunctionApproximation2001)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Many machine learning methods are parameterized functions that are optimized using some numerical optimization techniques, notably steepest-descent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Initial learner is a stump, subsequent learners are trees with depth as some power of 2 (commonly).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Numerical optimization in function space&lt;/strong&gt;
\[
g_m(\mathbf{x}) = E_y\Big[\frac{\partial L(y, F(\mathbf{x}))}{\partial F(\mathbf{x})}|\mathbf{x}\Big]_{F(\mathbf{x})=F_{m-1}(\mathbf{x})}
\]
The optimal step size found by solving&lt;/p&gt;
&lt;p&gt;\[
\rho_m = \mathop{\arg \min}_{\rho} E_{y,\mathbf{x}}L(y,F_{m-1}(\mathbf{x})-\rho g_m(\mathbf{x}))
\]
Then the function \(m\) is updated:
\[
f_m(\mathbf{x}) = -\rho_m g_m(\mathbf{x})
\]&lt;/p&gt;
&lt;p&gt;Walking through it&amp;hellip;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Make an initial guess with \(f_0(\mathbf{x})\)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Evaluate \(L(y, f_0(\mathbf{x}))\)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Improve model by boosting \(f_1(\mathbf{x}) = -\rho_1 g_1(\mathbf{x})\), where \[ g_1(\mathbf{x}) = \frac{\partial L(y, f_0(\mathbf{x}))}{\partial f_0(\mathbf{x})}. \]
This implies that \(f_1\) is predicting the gradient of the previous function.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the model is nonparametric, the expected value of the function conditioned on the input cannot be estimated accurately because we cannot sample the entire distribution of \(\mathbf{x}\). The author&amp;rsquo;s note that &amp;ldquo;&amp;hellip;even if it could, one would like to estimate \(F^*(\mathbf{x})\) at \(\mathbf{x}\) values other than the training sample points.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Smoothness is imposed by approximating the function with a parametric model. I think this means that the distribution is approximated as well.&lt;/p&gt;
&lt;p&gt;\begin{equation}
(\beta_m, \mathbf{a}_m) = \mathop{\arg \min}_{\beta, \mathbf{a}}\sum_{i=1}^N L(y_i, F_{m-1}(\mathbf{x}_i) + \beta h(\mathbf{x}_i; \mathbf{a}))
\end{equation}&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What if a solution to the above equation is difficult to obtain? Instead, view \(\beta_m h(\mathbf{x};\mathbf{a}_m)\) as the best greedy step toward \(F^*(\mathbf{x})\), under the constraint that the step direction, in this case \(h(\mathbf{x};\mathbf{a}_m)\), is a member of the class of functions \(h(\mathbf{x};\mathbf{a})\). The negative gradient can be evaluated at each data point:
\[
-g_m(\mathbf{x}_i) = -\frac{\partial L(y_i, F_{m-1}(\mathbf{x}_i))}{\partial F_{m-1}(\mathbf{x}_i)}.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This gradient is evaluated at every data point. However, we cannot generalize to new values not in our dataset. The proposed solution comes via \(\mathbf{h}_m = \{h(\mathbf{x}_i;\mathbf{a}_m)\}_{1}^N\) &amp;ldquo;most parallel to&amp;rdquo; \(-\mathbf{g}_m \in \mathbb{R}^N\).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As long as we can compute a derivative for the original loss function, our subsequent boosting problems are solved via least-squared error:
\[
\mathbf{a}_m = \mathop{\arg \min}_{\mathbf{a}, \beta} \sum_{i=1}^N \Big(-g_m(\mathbf{x}_i)-\beta h(\mathbf{x}_i;\mathbf{a})\Big)^2
\]&lt;/p&gt;

        
        
        
        
        
        &lt;figure&gt;
        
        &lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-07-18_19-43-31_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Original generic algorithm from (NO_ITEM_DATA:friedmanGreedyFunctionApproximation2001).&#34; &gt;
        
        
        
        &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
          
          &lt;p&gt;
            &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Original generic algorithm from (NO_ITEM_DATA:friedmanGreedyFunctionApproximation2001).
            
            
            
          &lt;/p&gt; 
        &lt;/figcaption&gt;
        
        &lt;/figure&gt;

&lt;p&gt;Check out a basic implementation in Python &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/blob/main/boosting/intro_to_gradient_boosting.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;NO_ITEM_DATA:friedmanGreedyFunctionApproximation2001&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An Introduction to Hidden Markov Models for Gesture Recognition</title>
      <link>https://ajdillhoff.github.io/articles/intro_to_hmms/</link>
      <pubDate>Sat, 15 Jul 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/articles/intro_to_hmms/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Hidden Markov Models provide a way of modeling the dynamics of sequential information. They have been used for speech recognition, part-of-speech tagging, machine translation, handwriting recognition, and, as we will see in this article, gesture recognition.&lt;/p&gt;
&lt;p&gt;Consider a somewhat practical use-case: you are going to throw a party with a meticulously curated playlist. You would rather not let anyone have the remote as it might get lost, and letting anyone interrupt the playlist with their own selections may derail the entire event. However, you still want to give your guests the ability to control the volume and skip back and forth between tracks in the playlist. We will also assume that guests will use change tracks and control the volume responsibly.&lt;/p&gt;
&lt;p&gt;The solution to this problem is to implement a gesture recognition system to identify simple hand motions. In this case, we only have to model 4 separate gestures: VolumeUp, VolumeDown, PrevTrack, NextTrack. Since the motions are temporal in nature, we can model each gesture using Hidden Markov Models. First, we need to cover a bit of background on what a Hidden Markov Model actually is.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;First, introduce Markov Chains&lt;/li&gt;
&lt;li&gt;Then the Markov assumption&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the core of our problem, we want to model a distribution over a sequence of states. Consider a sequence of only 3 states \(p(x_1, x_2, x_3)\). The full computation of this can be done using the chain rule of probability:&lt;/p&gt;
&lt;p&gt;\[
p(x_1, x_2, x_3) = p(x_1) p(x_2 | x_1) p(x_3 | x_1, x_2).
\]&lt;/p&gt;
&lt;p&gt;If the random variables of our problem are not conditionally independent, the complexity of calculating this is exponential in the number of random variables.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Markov&lt;/strong&gt; in Hidden Markov Models addresses this complexity. The &lt;strong&gt;Markov Assumption&lt;/strong&gt; states that the probability of an event at time \(t\) is conditioned &lt;em&gt;only&lt;/em&gt; on the previously observed event: \(p(x_t | x_{t-1})\). This is compactly represented with a graphical model, as seen in figure &lt;strong&gt;TODO&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TODO: Figure of basic Markov Chain&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;hidden&lt;/strong&gt; qualifier comes from the fact that the data we wish to model was generated from some underlying process that is not directly observable. A classic example for HMMs uses the weather. Imagine you had a log which had the number of water bottles a person had drank per day over the entire year. To make the problem slightly more difficult, the log entries were not associated with a date. It is reasonable to say that the amount of water a person drinks is influenced by how hot or cold it is on a particular day. So, the &lt;strong&gt;hidden state&lt;/strong&gt; in this case is the weather: hot or cold. We can model this with an HMM by establishing that the amount of water (&lt;strong&gt;observed state&lt;/strong&gt;) is conditioned on the weather (&lt;strong&gt;hidden state&lt;/strong&gt;). Figure &lt;strong&gt;TODO&lt;/strong&gt; shows this HMM graphically.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-07-20_19-12-54_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;An HMM with 4 states and 2 observation symbols (y_1) or (y_2).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;An HMM with 4 states and 2 observation symbols (y_1) or (y_2).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Formally, a Hidden Markov Model is defined by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The number of hidden states \(N\).&lt;/li&gt;
&lt;li&gt;A transition probability matrix \(A \in \mathbb{R}^{N \times N}\), where \(a_{ij} = p(z_t = j | z_{t-1} = i)\).&lt;/li&gt;
&lt;li&gt;An observation symbol probability distribution \(B = \{b_j(k)\} = p(\mathbf{x}_t = k | z_t = j)\).&lt;/li&gt;
&lt;li&gt;An initial state distribution \(\pi_i = p(z_t = i)\).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The trainable parameters of our model are \(\lambda = (A, B, \pi)\).&lt;/p&gt;
&lt;h2 id=&#34;functions-of-an-hmm&#34;&gt;Functions of an HMM&lt;/h2&gt;
&lt;p&gt;Given the basic definition of what an HMM is, how can we train the parameters defined in \(\lambda\). If we somehow already knew the parameters, how can we extract useful information from the model? Depending on our task, we can use HMMs to answer many important questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Filtering&lt;/strong&gt; computes \(p(z_t | \mathbf{x}_{1:t})\). That is, we are computing this probability as new samples come in up to time \(t\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smoothing&lt;/strong&gt; is accomplished when we have all the data in the sequence.
This is expressed as \(p(z_t|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fixed lag smoothing&lt;/strong&gt; allows for a trade off between accuracy and delay. It is useful in cases where we might not have the full sequence, but we wish to compute \(p(z_{t-l}|\mathbf{x}_{1:t})\) for some \(l &amp;gt; 0\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Predictions&lt;/strong&gt; are represented as \(p(z_{t+h}|\mathbf{x}_{1:t})\), where \(h &amp;gt; 0\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MAP estimation&lt;/strong&gt; yields the most probably state sequence \(\text{arg}\max_{\mathbf{z}_{1:T}}p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;We can sample the &lt;strong&gt;posterior&lt;/strong&gt; \(p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;We can also compute \(p(\mathbf{x}_{1:T})\) by summing up over all hidden paths. This is useful for classification tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course not all of these functions make sense for every possible task, more on that later. This article is not meant to be an exhaustive resource for all HMM functions; we will only look at the tasks necessary to train and use HMMs for isolated gesture recognition &lt;strong&gt;TODO: offer additional reading suggestions&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;data-processing&#34;&gt;Data Processing&lt;/h2&gt;
&lt;p&gt;As far as the efficacy of our model goes, how we process the data is the most important. Our system will start with a camera that records our guests performing one of the four simple motions. For simplicity, let&amp;rsquo;s pretend that the camera has an onboard chip that detects the 2D centroids of the left hand for each frame. That helps a lot, but there is still the problem of isolating a group of frames based on when the user wanted to start and finish the command. Assuming we have a solution for both of these problems, we still need to take into account that users will gesture at different speeds. Since all of these problems are challenging in their own right, we will assume the computer vision fairy has taken care of this for us.&lt;/p&gt;
&lt;p&gt;Each gesture in our dataset consists of 30 \((x, y)\) locations of the center of the left hand with respect to image coordinates. Even with this simplified data, we have another problem: different users may gesture from different locations. The hand locations for one user performing the &lt;code&gt;VolumeUp&lt;/code&gt; gesture may be vastly different from another. This isn&amp;rsquo;t too bad to deal with. We could normalize or training data by subtracting the location of the hand in the first frame from the gesture. That way every input would start at \((0, 0)\). We can simplify this even further by using &lt;strong&gt;relative motion states&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;relative-motion-states&#34;&gt;Relative Motion States&lt;/h3&gt;
&lt;p&gt;Relative motion states discretize our data, thus simplifying the input space. The idea is quite simple: if the hand moved to the right relative to the previous frame, we assign \(x = 1\) for that frame. If it moved to the left, assign \(x = -1\). If it didn&amp;rsquo;t move at all, or did not move a significant amount, assign \(x = 0\). We apply similar rules for the \(y\) locations as well. The &lt;strong&gt;TODO: figure&lt;/strong&gt; below shows the relative motion grid.&lt;/p&gt;
&lt;p&gt;Besides greatly simplifying our input space, meaning we can use a simple categorical distribution to model these observations, we no longer have to worry about the discrepency between where each user performed the gesture.&lt;/p&gt;
&lt;h2 id=&#34;modeling-a-gesture&#34;&gt;Modeling a Gesture&lt;/h2&gt;
&lt;p&gt;Our system will consist of 4 HMM models to model the dynamics of each gesture. To determine which gesture was performed, we will given our input sequence to each one and have it compute \(p(\mathbf{x}_{1:T}; \lambda_i)\), the probability of the observation given the parameters of model \(i\). Whichever model gives the high probability wins.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TODO&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Describe EM at a high level, show the breakdown of probabilities that need to be known&lt;/li&gt;
&lt;li&gt;Go into forward-backwards&lt;/li&gt;
&lt;li&gt;Go back to EM and plug them in&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;training-expectation-maximization&#34;&gt;Training: Expectation-Maximization&lt;/h3&gt;
&lt;p&gt;If we cannot observe the hidden states directly, how are we supposed to update the model parameters \(\lambda = (A, B, \pi)\)? We may not have all of the information, but we do have &lt;em&gt;some&lt;/em&gt; information. We can use that to fill in the missing values with what we would expect them to be given what we already know. Then, we can update our parameters using those expected values. This is accomplished through a two-stage algorithm called &lt;strong&gt;Expectation-Maximization&lt;/strong&gt;. Those familiar with k-Nearest Neighbors should already be familiar with this process.&lt;/p&gt;
&lt;h4 id=&#34;updating-with-perfect-information&#34;&gt;Updating with Perfect Information&lt;/h4&gt;
&lt;p&gt;It is useful to know how we would update our parameters assuming we had perfect information. If the hidden states were fully observable, then updating our model parameters would be as straightforward as computing the maximum likelihood estimates.
For \(A\) and \(\pi\), we first tally up the following counts:&lt;/p&gt;
&lt;p&gt;\[
\hat{a}_{ij} = \frac{N_{ij}}{\sum_j N_{ij}},
\]&lt;/p&gt;
&lt;p&gt;the number of times we expect to transition from \(i\) to \(j\) divided by the number of times we transition from \(i\) to any other state. Put simply, this computes the expected transitions from \(i\) to \(j\) normalized by all the times we expect to start in state \(i\).&lt;/p&gt;
&lt;p&gt;For \(\pi\), we have&lt;/p&gt;
&lt;p&gt;\[
\hat{\pi_i} = \frac{N_i}{\sum_i N_i},
\]&lt;/p&gt;
&lt;p&gt;the number of times we expect to start in state \(i\) divided by the number of times we start in any other state.&lt;/p&gt;
&lt;p&gt;Estimating the parameters for \(B\) depends on which distribution we are using for our observation probabilities.
For a multinomial distribution, we would compute the number of times we are in state \(j\) and observe a symbol \(k\) divided by the number of times we are in state \(j\):&lt;/p&gt;
&lt;p&gt;\[
\hat{b}_{jk} = \frac{N_{jk}}{N_k},
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
N_{jk} = \sum_{i=1}^N \sum_{t=1}^T \mathbb{1} (z_{i, t}=j, x_{i, t}=k).
\]&lt;/p&gt;
&lt;p&gt;It is also common to model our emission probability using a Normal distribution. We can even use a parameterized model like a neural network. &lt;strong&gt;TODO: provide links to examples of these&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;updating-with-missing-information&#34;&gt;Updating with Missing Information&lt;/h4&gt;
&lt;p&gt;Now to the real problem: fill in our missing information using our observable data and the current parameter estimates. There are two important statistics that we need to compute, called the &lt;strong&gt;sufficient statistics&lt;/strong&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The expected number of transitions from \(i\) to \(j\).&lt;/li&gt;
&lt;li&gt;The expected number of times we are transitioning from \(i\) to any other state.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both of these can be computed starting with the same probability &lt;em&gt;conditioned&lt;/em&gt; on our observable data:&lt;/p&gt;
&lt;p&gt;\[
p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T}).
\]&lt;/p&gt;
&lt;h3 id=&#34;forwards-backwards-algorithm&#34;&gt;Forwards-Backwards Algorithm&lt;/h3&gt;
&lt;p&gt;Computing joint distribution can be very computationally expensive. Fortunately for us, the Markov assumption along with operations on graphs open the door to a dynamic programming approach named the Forward-Backward algorithm.&lt;/p&gt;
&lt;p&gt;The Forwards-Backwards Algorithm, also known as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Baum-Welch algorithm&lt;/a&gt;, provides an effective solution to computing the joint described above. In fact, there are many useful distributions that can be computed with this algorithm such as the &lt;strong&gt;filtering&lt;/strong&gt; and &lt;strong&gt;smoothing&lt;/strong&gt; tasks.&lt;/p&gt;
&lt;h4 id=&#34;forward-probability&#34;&gt;Forward Probability&lt;/h4&gt;
&lt;p&gt;The forward probability, often denoted as \(\alpha\), represents the probability of ending up at a particular hidden state \(i\) at time \(t\) having seen the observations up to that time:&lt;/p&gt;
&lt;p&gt;\[
\alpha_t(i) = p(z_t = i, \mathbf{x}_{1:t} | \lambda).
\]&lt;/p&gt;
&lt;p&gt;This value is computed recursively starting from \(t=1\) and going forwards to \(t=T\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Initialization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For \(t=1\), we calculate:&lt;/p&gt;
&lt;p&gt;\[
\alpha_1(i) = \pi_i b_i(x_1),\quad 1 \leq i \leq N,
\]&lt;/p&gt;
&lt;p&gt;where \(\pi_i\) is the initial probability of state \(i\) and \(b_i(x_1)\) is the emission probability of the first observation \(x_1\) given that we are in state \(i\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recursion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;After that, we calculate the remaining \(\alpha_t(i)\) as follows:&lt;/p&gt;
&lt;p&gt;\[
\alpha_{t+1}(j) = b_j(x_{t+1}) \sum_{i=1}^{N} \alpha_{t}(i)a_{ij},
\]&lt;/p&gt;
&lt;p&gt;where \(N\) is the number of hidden states, and \(a_{ij}\) is the transition probability from state \(i\) to state \(j\).&lt;/p&gt;
&lt;h4 id=&#34;backward-probability&#34;&gt;Backward Probability&lt;/h4&gt;
&lt;p&gt;The backward probability, denoted as \(\beta\), gives the probability of observing the remaining observations from time \(t+1\) to \(T\) given that we are in state \(i\) at time \(t\):&lt;/p&gt;
&lt;p&gt;\[
\beta_t(i) = p(\mathbf{x}_{t+1:T} | z_t = i, \lambda).
\]&lt;/p&gt;
&lt;p&gt;Again, this is calculated recursively but this time starting from \(t=T\) and going backwards to \(t=1\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Initialization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For \(t=T\), we initialize:&lt;/p&gt;
&lt;p&gt;\[
\beta_T(i) = 1, \forall i.
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recursion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Then we calculate the remaining \(\beta_t(i)\) as:&lt;/p&gt;
&lt;p&gt;\[
\beta_{t}(i) = \sum_{j=1}^{N} a_{ij}b_j(x_{t+1})\beta_{t+1}(j).
\]&lt;/p&gt;
&lt;h4 id=&#34;calculating-the-sufficient-statistics&#34;&gt;Calculating the Sufficient Statistics&lt;/h4&gt;
&lt;p&gt;With these two sets of probabilities, we can calculate the two required sufficient statistics as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The expected number of transitions from \(i\) to \(j\):&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
\frac{\sum_{t=1}^{T-1} \alpha_t(i) a_{ij} b_j(x_{t+1}) \beta_{t+1}(j)}{P(X|\lambda)}
\]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The expected number of times we are transitioning from \(i\) to any other state:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
\frac{\sum_{t=1}^{T-1} \alpha_t(i) \beta_t(i)}{P(X|\lambda)}
\]&lt;/p&gt;
&lt;p&gt;Where \(P(X|\lambda)\) is the total probability of the observations, calculated as:&lt;/p&gt;
&lt;p&gt;\[
P(X|\lambda) = \sum_{i=1}^{N} \alpha_T(i)
\]&lt;/p&gt;
&lt;h4 id=&#34;how-does-this-give-us-p--z-t-i-z-t-plus-1-j-mathbf-x-1-t&#34;&gt;How does this give us \(p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T})\)?&lt;/h4&gt;
&lt;p&gt;To understand how the variables of the Forwards-Backwards algorithm relate to the original probabilities, we can express the term \(p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T})\) in terms of the original probability distributions in the HMM:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(\pi_i\) - the probability of starting in state \(i\),&lt;/li&gt;
&lt;li&gt;\(a_{ij}\) - the probability of transitioning from state \(i\) to state \(j\),&lt;/li&gt;
&lt;li&gt;\(b_j(x_t)\) - the probability that state \(j\) will emit observation \(x_t\).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The joint probability \(p(z_t = i, z_{t+1} = j, \mathbf{x}_{1:T})\) would represent the probability of being in state \(i\) at time \(t\), moving to state \(j\) at time \(t+1\), and observing the sequence of emissions \(\mathbf{x}_{1:T}\). This can be factored as follows due to the Markov property:&lt;/p&gt;
&lt;p&gt;\[
p(z_t = i, z_{t+1} = j, \mathbf{x}_{1:T}) = p(\mathbf{x}_{1:t}, z_t = i)p(z_{t+1} = j| z_t = i)p(\mathbf{x}_{t+1:T} | z_{t+1} = j, \mathbf{x}_{1:t}).
\]&lt;/p&gt;
&lt;p&gt;Using our definitions of \(\alpha\) and \(\beta\), we can rewrite this in terms of our HMM quantities:&lt;/p&gt;
&lt;p&gt;\[
p(z_t = i, z_{t+1} = j, \mathbf{x}_{1:T}) = \alpha_t(i)a_{ij}b_j(x_{t+1})\beta_{t+1}(j).
\]&lt;/p&gt;
&lt;p&gt;Here, \(\alpha_t(i)\) represents \(p(\mathbf{x}_{1:t}, z_t = i)\), the joint probability of the observations until time \(t\) and being in state \(i\) at time \(t\), and \(\beta_{t+1}(j)\) represents \(p(\mathbf{x}_{t+1:T} | z_{t+1} = j)\), the probability of the observations from time \(t+1\) to \(T\) given we&amp;rsquo;re in state \(j\) at time \(t+1\).&lt;/p&gt;
&lt;p&gt;Then, to obtain \(p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T})\), we divide by \(p(\mathbf{x}_{1:T})\) to normalize the probabilities, which is the sum over all states of \(\alpha_T(i)\), or equivalently, the sum over all states of \(\beta_1(i)\pi_i b_i(x_1)\).&lt;/p&gt;
&lt;p&gt;This gives us:&lt;/p&gt;
&lt;p&gt;\[
p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T}) = \frac{\alpha_t(i)a_{ij}b_j(x_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^{N}\alpha_T(i)}.
\]&lt;/p&gt;
&lt;p&gt;This is the same expression as before, but broken down in terms of the original HMM quantities and the forward and backward variables. This can also be explained through graph properties and operations. See &lt;a href=&#34;https://cedar.buffalo.edu/~srihari/CSE574/Chap13/13.2.2-ForwardBackward.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Sargur Srihari&amp;rsquo;s excellent lecture slides&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h2 id=&#34;implementation-in-python&#34;&gt;Implementation in Python&lt;/h2&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Bias and Variance</title>
      <link>https://ajdillhoff.github.io/notes/bias_and_variance/</link>
      <pubDate>Tue, 04 Jul 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/bias_and_variance/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#generalization&#34;&gt;Generalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bias&#34;&gt;Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variance&#34;&gt;Variance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bias-variance-tradeoff&#34;&gt;Bias-Variance Tradeoff&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;generalization&#34;&gt;Generalization&lt;/h2&gt;
&lt;p&gt;When fitting machine learning models to data, we want them to &lt;strong&gt;generalize&lt;/strong&gt; well to the distribution that we have sampled from. We can measure a model&amp;rsquo;s ability to generalize by evaluating it on previously unseen data that is sampled from the same distribution as the training set. However, we often do not know the true underlying distribution. So we must fit the models to empirical distributions derived from observed data.&lt;/p&gt;
&lt;p&gt;Measuring bias and variance is crucial for determining the quality of a model. &lt;strong&gt;Bias&lt;/strong&gt; refers to the difference between the average prediction of a model and the correct value we are trying to predict. A model with high bias oversimplifies the problem and leads to high error on both training and test data. &lt;strong&gt;Variance&lt;/strong&gt; refers to the sensitivity of a model to fluctuations in the training set. High variance suggests that the model&amp;rsquo;s performance changes significantly when it is fit on different samplings of the training data, which can lead to overfitting.&lt;/p&gt;
&lt;p&gt;To achieve good generalization, it is essential to find a balance between bias and variance, minimizing the total error. This can be done by selecting appropriate model complexity and using regularization techniques to prevent overfitting or underfitting. Additionally, model validation techniques, such as hold-out validation and cross-validation, can be employed to assess a model&amp;rsquo;s ability to generalize to unseen data.&lt;/p&gt;
&lt;h2 id=&#34;bias&#34;&gt;Bias&lt;/h2&gt;
&lt;p&gt;Consider fitting a simple linear model to nonlinear data. The model will not be able to generalize well, regardless of the size of the training set. In fact, it would also exhibit poor performance when evaluated on the training set as well. When a model has not learned the patterns in the training data and is likewise unable to generalize to new data, it is known as &lt;strong&gt;underfitting&lt;/strong&gt;. In this case, such a model has &lt;strong&gt;high bias&lt;/strong&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-07-04_22-51-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Regardless of the dataset sampled, a linear model exhibits high bias.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Regardless of the dataset sampled, a linear model exhibits high bias.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;variance&#34;&gt;Variance&lt;/h2&gt;
&lt;p&gt;Variance is described in terms of the model fitting procedure and the training data. In terms of data, variance measures dispersion. It could also be interpreted as a measure of diversity. Sets with low variance contain samples that are close to the mean, and sampling from such a set would produce rather consistent data points.&lt;/p&gt;
&lt;p&gt;In terms of model fitting, a model that fits the training data well but not the test data describes &lt;strong&gt;overfitting&lt;/strong&gt;. This is because the training data is only an empirical sample of the true underlying distribution. A different sampling of the distribution may yield a set that more closely resembles the test set. Due to the &lt;strong&gt;variance&lt;/strong&gt; of the underlying distribution, our model overfits the patterns that exist in the training set.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-07-04_17-54-36_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A 5th degree polynomial trained on 3 different samplings of the distribution.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A 5th degree polynomial trained on 3 different samplings of the distribution.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;bias-variance-tradeoff&#34;&gt;Bias-Variance Tradeoff&lt;/h2&gt;
&lt;p&gt;If a model is not complex enough to capture the underlying distribution, it will perform poorly on both the training and test sets. Indeed, the model has low bias. If the model is too complex, it will exhibit low bias and high variance, overfitting the training set while failing to generalize well to unseen data. The solution then is to find a tradeoff between bias and variance with respect to the model complexity.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Natural Language Processing</title>
      <link>https://ajdillhoff.github.io/notes/natural_language_processing/</link>
      <pubDate>Sun, 23 Apr 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/natural_language_processing/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#text-preprocessing&#34;&gt;Text Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tasks&#34;&gt;Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#models&#34;&gt;Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#perplexity&#34;&gt;Perplexity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Text Preprocessing
&lt;ul&gt;
&lt;li&gt;Character-level tokenization&lt;/li&gt;
&lt;li&gt;Word-level tokenization&lt;/li&gt;
&lt;li&gt;Subword tokenization&lt;/li&gt;
&lt;li&gt;Stopwords&lt;/li&gt;
&lt;li&gt;Batching&lt;/li&gt;
&lt;li&gt;Padding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Unsupervised Pre-Training
&lt;ul&gt;
&lt;li&gt;Autoregression&lt;/li&gt;
&lt;li&gt;BERT loss&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tasks
&lt;ul&gt;
&lt;li&gt;Text Classification&lt;/li&gt;
&lt;li&gt;Named Entity Recognition&lt;/li&gt;
&lt;li&gt;Question Answering&lt;/li&gt;
&lt;li&gt;Summarization&lt;/li&gt;
&lt;li&gt;Translation&lt;/li&gt;
&lt;li&gt;Text Generation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;text-preprocessing&#34;&gt;Text Preprocessing&lt;/h2&gt;
&lt;p&gt;Text preprocessing is an essential step in NLP that involves cleaning and transforming unstructured text data to prepare it for analysis. Some common text preprocessing techniques include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Expanding contractions (e.g., &amp;ldquo;don&amp;rsquo;t&amp;rdquo; to &amp;ldquo;do not&amp;rdquo;) [7]&lt;/li&gt;
&lt;li&gt;Lowercasing text[7]&lt;/li&gt;
&lt;li&gt;Removing punctuations[7]&lt;/li&gt;
&lt;li&gt;Removing words and digits containing digits[7]&lt;/li&gt;
&lt;li&gt;Removing stopwords (common words that do not carry much meaning) [7]&lt;/li&gt;
&lt;li&gt;Rephrasing text[7]&lt;/li&gt;
&lt;li&gt;Stemming and Lemmatization (reducing words to their root forms) [7]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;common-tokenizers&#34;&gt;Common Tokenizers&lt;/h3&gt;
&lt;p&gt;Tokenization is the process of breaking a stream of textual data into words, terms, sentences, symbols, or other meaningful elements called tokens. Some common tokenizers used in NLP include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Whitespace Tokenizer&lt;/strong&gt;&lt;/strong&gt;: Splits text based on whitespace characters (e.g., spaces, tabs, and newlines) [2].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;NLTK Tokenizer&lt;/strong&gt;&lt;/strong&gt;: A popular Python library that provides various tokenization functions, including word and sentence tokenization[1].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;SpaCy Tokenizer&lt;/strong&gt;&lt;/strong&gt;: Another popular Python library for NLP that offers a fast and efficient tokenizer, which can handle large documents and is customizable[5].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;WordPiece Tokenizer&lt;/strong&gt;&lt;/strong&gt;: A subword tokenizer used in models like BERT, which breaks text into smaller subword units to handle out-of-vocabulary words more effectively[3].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Byte Pair Encoding (BPE) Tokenizer&lt;/strong&gt;&lt;/strong&gt;: A subword tokenizer that iteratively merges the most frequent character pairs in the text, resulting in a vocabulary of subword units[12].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;SentencePiece Tokenizer&lt;/strong&gt;&lt;/strong&gt;: A library that provides both BPE and unigram-based subword tokenization, which can handle multiple languages and does not rely on whitespace for tokenization[6].&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These tokenizers differ in the way they split text into tokens and handle language-specific considerations, such as handling out-of-vocabulary words, dealing with punctuation, and managing whitespace characters. The choice of tokenizer depends on the specific NLP task and the characteristics of the text data being processed.&lt;/p&gt;
&lt;p&gt;Citations:
[1] &lt;a href=&#34;https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/&lt;/a&gt;
[2] &lt;a href=&#34;https://neptune.ai/blog/tokenization-in-nlp&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://neptune.ai/blog/tokenization-in-nlp&lt;/a&gt;
[3] &lt;a href=&#34;https://towardsdatascience.com/comparing-transformer-tokenizers-686307856955&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://towardsdatascience.com/comparing-transformer-tokenizers-686307856955&lt;/a&gt;
[4] &lt;a href=&#34;https://www.analyticsvidhya.com/blog/2021/09/essential-text-pre-processing-techniques-for-nlp/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.analyticsvidhya.com/blog/2021/09/essential-text-pre-processing-techniques-for-nlp/&lt;/a&gt;
[5] &lt;a href=&#34;https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/&lt;/a&gt;
[6] &lt;a href=&#34;https://www.reddit.com/r/MachineLearning/comments/rprmq3/d_sentencepiece_wordpiece_bpe_which_tokenizer_is/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.reddit.com/r/MachineLearning/comments/rprmq3/d_sentencepiece_wordpiece_bpe_which_tokenizer_is/&lt;/a&gt;
[7] &lt;a href=&#34;https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/&lt;/a&gt;
[8] &lt;a href=&#34;https://towardsdatascience.com/top-5-word-tokenizers-that-every-nlp-data-scientist-should-know-45cc31f8e8b9&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://towardsdatascience.com/top-5-word-tokenizers-that-every-nlp-data-scientist-should-know-45cc31f8e8b9&lt;/a&gt;
[9] &lt;a href=&#34;https://www.projectpro.io/recipes/explain-difference-between-word-tokenizer&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.projectpro.io/recipes/explain-difference-between-word-tokenizer&lt;/a&gt;
[10] &lt;a href=&#34;https://www.telusinternational.com/insights/ai-data/article/what-is-text-mining&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.telusinternational.com/insights/ai-data/article/what-is-text-mining&lt;/a&gt;
[11] &lt;a href=&#34;https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4&lt;/a&gt;
[12] &lt;a href=&#34;https://towardsdatascience.com/a-comprehensive-guide-to-subword-tokenisers-4bbd3bad9a7c&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://towardsdatascience.com/a-comprehensive-guide-to-subword-tokenisers-4bbd3bad9a7c&lt;/a&gt;
[13] &lt;a href=&#34;https://towardsdatascience.com/text-preprocessing-in-natural-language-processing-using-python-6113ff5decd8&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://towardsdatascience.com/text-preprocessing-in-natural-language-processing-using-python-6113ff5decd8&lt;/a&gt;
[14] &lt;a href=&#34;https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/&lt;/a&gt;
[15] &lt;a href=&#34;https://docs.tamr.com/new/docs/tokenizers-and-similarity-functions&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://docs.tamr.com/new/docs/tokenizers-and-similarity-functions&lt;/a&gt;
[16] &lt;a href=&#34;https://pitt.libguides.com/textmining/preprocessing&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://pitt.libguides.com/textmining/preprocessing&lt;/a&gt;
[17] &lt;a href=&#34;https://medium.com/@ajay_khanna/tokenization-techniques-in-natural-language-processing-67bb22088c75&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://medium.com/@ajay_khanna/tokenization-techniques-in-natural-language-processing-67bb22088c75&lt;/a&gt;
[18] &lt;a href=&#34;https://datascience.stackexchange.com/questions/75304/bpe-vs-wordpiece-tokenization-when-to-use-which&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://datascience.stackexchange.com/questions/75304/bpe-vs-wordpiece-tokenization-when-to-use-which&lt;/a&gt;
[19] &lt;a href=&#34;https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing&lt;/a&gt;
[20] &lt;a href=&#34;https://www.tokenex.com/blog/ab-what-is-nlp-natural-language-processing-tokenization/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.tokenex.com/blog/ab-what-is-nlp-natural-language-processing-tokenization/&lt;/a&gt;
[21] &lt;a href=&#34;https://hungsblog.de/en/technology/learnings/difference-between-the-tokenizer-and-the-pretrainedtokenizer-class/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://hungsblog.de/en/technology/learnings/difference-between-the-tokenizer-and-the-pretrainedtokenizer-class/&lt;/a&gt;
[22] &lt;a href=&#34;https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html&lt;/a&gt;
[23] &lt;a href=&#34;https://medium.com/nlplanet/two-minutes-nlp-a-taxonomy-of-tokenization-methods-60e330aacad3&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://medium.com/nlplanet/two-minutes-nlp-a-taxonomy-of-tokenization-methods-60e330aacad3&lt;/a&gt;
[24] &lt;a href=&#34;https://www.geeksforgeeks.org/text-preprocessing-in-python-set-1/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.geeksforgeeks.org/text-preprocessing-in-python-set-1/&lt;/a&gt;
[25] &lt;a href=&#34;https://medium.com/@utkarsh.kant/tokenization-a-complete-guide-3f2dd56c0682&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://medium.com/@utkarsh.kant/tokenization-a-complete-guide-3f2dd56c0682&lt;/a&gt;
[26] &lt;a href=&#34;https://stackoverflow.com/questions/380455/looking-for-a-clear-definition-of-what-a-tokenizer-parser-and-lexers-are&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://stackoverflow.com/questions/380455/looking-for-a-clear-definition-of-what-a-tokenizer-parser-and-lexers-are&lt;/a&gt;
[27] &lt;a href=&#34;https://blog.floydhub.com/tokenization-nlp/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://blog.floydhub.com/tokenization-nlp/&lt;/a&gt;
[28] &lt;a href=&#34;https://medium.com/product-ai/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://medium.com/product-ai/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908&lt;/a&gt;
[29] &lt;a href=&#34;https://pub.towardsai.net/in-depth-tokenization-methods-of-14-nlp-libraries-with-python-example-297ecdd14c1&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://pub.towardsai.net/in-depth-tokenization-methods-of-14-nlp-libraries-with-python-example-297ecdd14c1&lt;/a&gt;
[30] &lt;a href=&#34;https://datascience.stackexchange.com/questions/88680/what-is-the-difference-between-countvectorizer-and-tokenizer-or-are-they-the&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://datascience.stackexchange.com/questions/88680/what-is-the-difference-between-countvectorizer-and-tokenizer-or-are-they-the&lt;/a&gt;
[31] &lt;a href=&#34;https://www.freecodecamp.org/news/train-algorithms-from-scratch-with-hugging-face/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.freecodecamp.org/news/train-algorithms-from-scratch-with-hugging-face/&lt;/a&gt;
[32] &lt;a href=&#34;https://exchange.scale.com/public/blogs/preprocessing-techniques-in-nlp-a-guide&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://exchange.scale.com/public/blogs/preprocessing-techniques-in-nlp-a-guide&lt;/a&gt;
[33] &lt;a href=&#34;https://huggingface.co/docs/transformers/tokenizer_summary&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://huggingface.co/docs/transformers/tokenizer_summary&lt;/a&gt;
[34] &lt;a href=&#34;https://blog.octanove.org/guide-to-subword-tokenization/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://blog.octanove.org/guide-to-subword-tokenization/&lt;/a&gt;
[35] &lt;a href=&#34;https://www.enjoyalgorithms.com/blog/text-data-pre-processing-techniques-in-ml/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.enjoyalgorithms.com/blog/text-data-pre-processing-techniques-in-ml/&lt;/a&gt;
[36] &lt;a href=&#34;https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;tasks&#34;&gt;Tasks&lt;/h2&gt;
&lt;h3 id=&#34;text-classification&#34;&gt;Text Classification&lt;/h3&gt;
&lt;p&gt;Commonly seen in the form of sentiment analysis, where the objective is to classify whether some input text is positive or negative. Document classification, in which documents are identified by their content, is also useful.&lt;/p&gt;
&lt;h3 id=&#34;named-entity-recognition&#34;&gt;Named Entity Recognition&lt;/h3&gt;
&lt;p&gt;Extract important nouns from a body of text.&lt;/p&gt;
&lt;h3 id=&#34;question-answering&#34;&gt;Question Answering&lt;/h3&gt;
&lt;h3 id=&#34;summarization&#34;&gt;Summarization&lt;/h3&gt;
&lt;h3 id=&#34;translation&#34;&gt;Translation&lt;/h3&gt;
&lt;h3 id=&#34;text-generation&#34;&gt;Text Generation&lt;/h3&gt;
&lt;p&gt;Generate text from a prompt. This could be in the form of a simple question or some initial dialog. This is also seen in tools like GitHub Co-Pilot to generate code based on contextual code in the same project.&lt;/p&gt;
&lt;h2 id=&#34;models&#34;&gt;Models&lt;/h2&gt;
&lt;p&gt;Discuss GPT2&lt;/p&gt;
&lt;h2 id=&#34;perplexity&#34;&gt;Perplexity&lt;/h2&gt;
&lt;p&gt;A measure of confidence of a language model. A naive model may predict a word by randomly selecting any of the \(N\) words in its vocabulary. As the model is optimized and the distribution of possible sequences is acquired, the perplexity decreases.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Transformers</title>
      <link>https://ajdillhoff.github.io/notes/transformers/</link>
      <pubDate>Sun, 06 Nov 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/transformers/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#attention&#34;&gt;Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#key-value-store&#34;&gt;Key-value Store&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scaled-dot-product-attention&#34;&gt;Scaled Dot Product Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multi-head-attention&#34;&gt;Multi-Head Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#encoder-decoder-architecture&#34;&gt;Encoder-Decoder Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#encoder&#34;&gt;Encoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#decoder&#34;&gt;Decoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The story of Transformers begins with &amp;ldquo;Attention Is All You Need&amp;rdquo; (Vaswani et al., n.d.). In this seminal work, the authors describe the current landscape of sequential models, their shortcomings, and the novel ideas that result in their successful application.&lt;/p&gt;
&lt;p&gt;Their first point highlights a fundamental flaw in how &lt;a href=&#34;https://ajdillhoff.github.io/notes/recurrent_neural_networks/&#34;&gt;Recurrent Neural Networks&lt;/a&gt; process sequential data: their output is a function of the previous time step. Given the hindsight of 2022, where large language models are crossing the &lt;a href=&#34;https://arxiv.org/pdf/2101.03961.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;trillion parameter milestone&lt;/a&gt;, a model requiring recurrent computation dependent on previous time steps without the possibility of parallelization would be virtually intractable.&lt;/p&gt;
&lt;p&gt;The second observation refers to attention mechanisms, a useful addition to sequential models that enable long-range dependencies focused on specific contextual information. When added to translation models, attention allows the model to focus on particular words (Bahdanau, Cho, and Bengio 2016).&lt;/p&gt;
&lt;p&gt;The Transformer architecture considers the entire sequence using only attention mechanisms.
There are no recurrence computations in the model, allowing for higher efficiency through parallelization.&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;The original architecture consists of an encoder and decoder, each containing one or more attention mechanisms.
Not every type of model uses both encoders and decoders. This is discussed later [TODO: discuss model types].
Before diving into the architecture itself, it is important to understand what an attention mechanism is and how it functions.&lt;/p&gt;
&lt;h2 id=&#34;attention&#34;&gt;Attention&lt;/h2&gt;
&lt;p&gt;Attention mechanisms produce relationships between sequences. When we look at an image of a dog running in a field with the intent of figuring out what the dog is doing in the picture, we pay greater attention to the dog and look at contextual cues in the image that might inform us of their task. This is an automatic process which allows us to efficiently process information.&lt;/p&gt;
&lt;p&gt;Attention mechanisms follow the same concept. Consider a machine translation task in which a sentence in English is translated to French. Certain words between the input and output will have stronger correlations than others.&lt;/p&gt;
&lt;h3 id=&#34;soft-attention&#34;&gt;Soft Attention&lt;/h3&gt;
&lt;p&gt;Use of context vector that is dependent on a sequence of annotations. These contain information about the input sequence with a focus on the parts surrounding the $i$-th word.&lt;/p&gt;
&lt;p&gt;\[
c_i = \sum_{j=1}^{T_x}\alpha_{ij}h_j
\]&lt;/p&gt;
&lt;p&gt;What is \(\alpha_{ij}\) and how is it computed? This comes from an alignment model which assigns a score reflecting how well the inputs around position \(j\) and output at position \(i\) match, given by&lt;/p&gt;
&lt;p&gt;\[
e_{ij} = a(s_{i-1}, h_j),
\]&lt;/p&gt;
&lt;p&gt;where \(a\) is a feed-forward neural network and \(h_j\) is an annotation produced by the hidden layer of a BRNN.
These scores are passed to the softmax function so that \(\alpha_{ij}\) represents the weight of annotation \(h_j\):&lt;/p&gt;
&lt;p&gt;\[
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp (e_{ik})}.
\]&lt;/p&gt;
&lt;p&gt;This weight reflects how important \(h_j\) is at deciding the next state \(s_i\) and generating \(y_i\).&lt;/p&gt;
&lt;h3 id=&#34;soft-vs-dot-hard-attention&#34;&gt;Soft vs. Hard Attention&lt;/h3&gt;
&lt;p&gt;This mechanism was also described in the context of visual attention as &amp;ldquo;soft&amp;rdquo; attention (Xu et al. 2016).
The authors also describe an alternative version they call &amp;ldquo;hard&amp;rdquo; attention.
Instead of providing a probability of where the model should look, hard attention provides a single location that is sampled from a multinoulli distribution parameterized by \(\alpha_i\).&lt;/p&gt;
&lt;p&gt;\[
p(s_{t,i} = 1 | s_{j&amp;lt;t}, \mathbf{a}) = \alpha_{t,i}
\]&lt;/p&gt;
&lt;p&gt;Here, \(s_{t,i}\) represents the location \(i\) at time \(t\), \(s_{j&amp;lt;t}\) are the location variables prior to \(t\), and \(\mathbf{a}\) is an image feature vector.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-11-10_12-07-42_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Hard attention for &amp;#34;A man and a woman playing frisbee in a field.&amp;#34; (Xu et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Hard attention for &amp;ldquo;A man and a woman playing frisbee in a field.&amp;rdquo; (Xu et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-11-10_12-08-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Soft attention for &amp;#34;A woman is throwing a frisbee in a park.&amp;#34; (Xu et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Soft attention for &amp;ldquo;A woman is throwing a frisbee in a park.&amp;rdquo; (Xu et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The two figures above show the difference between soft and hard attention.
Hard attention, while faster at inference time, is non-differentiable and requires more complex methods to train (TODO: cite Luong).&lt;/p&gt;
&lt;h3 id=&#34;self-attention&#34;&gt;Self-Attention&lt;/h3&gt;
&lt;p&gt;Self attention is particularly useful for determining the relationship between different parts of an input sequence. The figure below demonstrates self-attention given an input sentence (Cheng, Dong, and Lapata 2016).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-11-10_13-11-31_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Line thickness indicates stronger self-attention (Cheng et al.).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Line thickness indicates stronger self-attention (Cheng et al.).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;How aligned the two vectors are.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cross-attention&#34;&gt;Cross Attention&lt;/h3&gt;
&lt;p&gt;TODO&lt;/p&gt;
&lt;h2 id=&#34;key-value-store&#34;&gt;Key-value Store&lt;/h2&gt;
&lt;p&gt;Query, key, and value come from the same input (self-attention).&lt;/p&gt;
&lt;p&gt;Check query against all possible keys in the dictionary. They have the same size.
The value is the result stored there, not necessarily the same size.
Each item in the sequence will generate a query, key, and value.&lt;/p&gt;
&lt;p&gt;The attention vector is a function of they keys and the query.&lt;/p&gt;
&lt;p&gt;Hidden representation is a function of the values and the attention vector.&lt;/p&gt;
&lt;p&gt;The Transformer paper talks about queries, keys, and values. This idea comes from retrieval systems.
If you are searching for something (a video, book, song, etc.), you present a system your query. That system will compare your query against the keys in its database. If there is a key that matches your query, the value is returned.&lt;/p&gt;
&lt;p&gt;\[
att(q, \mathbf{k}, \mathbf{v}) = \sum_i v_i f(q, k_i),
\]
where \(f\) is a similarity function.&lt;/p&gt;
&lt;p&gt;This is an interesting and convenient representation of attention.
To implement this idea, we need some measure of similarity.
Why not orthogonality? Two vectors that are orthogonal produce a scalar value of 0.
The maximum value two vectors will produce as a result of the dot product occurs when the two vectors have the exact same direction.
This is convenient because the dot product is simple and efficient and we are already performing these calculations in our deep networks in the form of matrix multiplication.&lt;/p&gt;
&lt;h2 id=&#34;scaled-dot-product-attention&#34;&gt;Scaled Dot Product Attention&lt;/h2&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-11-21_18-39-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Scaled dot-product attention ((Vaswani et al., n.d.))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Scaled dot-product attention ((Vaswani et al., n.d.))
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Each &lt;strong&gt;query&lt;/strong&gt; vector is multiplied with each &lt;strong&gt;key&lt;/strong&gt; using the dot product.
This is implemented more efficiently via matrix multiplication.
A few other things are added here to control the output.
The first is &lt;strong&gt;scaling&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;multi-head-attention&#34;&gt;Multi-Head Attention&lt;/h2&gt;
&lt;p&gt;A single attention head can transform the input into a single representation. Is this analagous to using a single convolutional filter? The benefit of having multiple filters is to create multiple possible representations from the same input.&lt;/p&gt;
&lt;h2 id=&#34;encoder-decoder-architecture&#34;&gt;Encoder-Decoder Architecture&lt;/h2&gt;
&lt;p&gt;The original architecture of a transformer was defined in the context of sequence transduction tasks, where both the input and output are sequences. The most common task of this type is machine translation.&lt;/p&gt;
&lt;h2 id=&#34;encoder&#34;&gt;Encoder&lt;/h2&gt;
&lt;p&gt;The encoder layer takes an input sequence \(\{\mathbf{x}_t\}_{t=0}^T\) and transforms it into another sequence \(\{\mathbf{z}_t\}_{t=0}^T\).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;What is \(\mathbf{z}_t\)?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How is it used?
Input as key and value into second multi-head attention layer of the &lt;strong&gt;decoder&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Could you create an encoder only model?
Yes. Suitable for classification tasks &amp;ndash; classify the representation produced by the encoder.
&lt;strong&gt;How does this representation relate to understanding?&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It&amp;rsquo;s a transformation to another representation.&lt;/p&gt;
&lt;p&gt;Generated representation also considers the context of other parts of the same sequence (bi-directional).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;decoder&#34;&gt;Decoder&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Generates an output sequence.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Decoder-only models?
Suitable for text generation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What does the input represent?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What does the output represent?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What if we don&amp;rsquo;t use an encoder, what information is added in lieu of the encoder output?&lt;/p&gt;
&lt;!-- This HTML table template is generated by emacs/table.el --&gt;
&lt;table border=&#34;1&#34;&gt;
  &lt;tr&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Model&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Examples&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Tasks&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Encoder&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;ALBERT,&amp;nbsp;BERT,&amp;nbsp;DistilBERT,&lt;br /&gt;
      ELECTRA,&amp;nbsp;RoBERTa&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Sentence&amp;nbsp;classification,&amp;nbsp;named&amp;nbsp;entity&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      recognition,&amp;nbsp;extractive&amp;nbsp;question&amp;nbsp;answering&amp;nbsp;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Decoder&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;CTRL,&amp;nbsp;GPT,&amp;nbsp;GPT-2,&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      Transformer&amp;nbsp;XL&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Text&amp;nbsp;generation&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Encoder-decoder&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;BART,&amp;nbsp;T5,&amp;nbsp;Marian,&amp;nbsp;mBART&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Summarization,&amp;nbsp;translation,&amp;nbsp;generative&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      question&amp;nbsp;answering&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;p&gt;TODO&lt;/p&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/labmlai/status/1543159412940242945?s=20&amp;amp;t=EDu5FzDWl92EqnJlWvfAxA&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://twitter.com/labmlai/status/1543159412940242945?s=20&amp;t=EDu5FzDWl92EqnJlWvfAxA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Transduction_%28machine_learning%29&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://en.wikipedia.org/wiki/Transduction_(machine_learning)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.apronus.com/math/transformer-language-model-definition&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.apronus.com/math/transformer-language-model-definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2018-06-24-attention/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://lilianweng.github.io/posts/2018-06-24-attention/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://nlp.seas.harvard.edu/annotated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;http://nlp.seas.harvard.edu/annotated-transformer/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Sequential Minimal Optimization</title>
      <link>https://ajdillhoff.github.io/notes/sequential_minimal_optimization/</link>
      <pubDate>Mon, 04 Jul 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/sequential_minimal_optimization/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#box-constraints&#34;&gt;Box Constraints&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#updating-the-lagrangians&#34;&gt;Updating the Lagrangians&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-algorithm&#34;&gt;The Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementation&#34;&gt;Implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Paper link:&lt;/strong&gt; &lt;a href=&#34;https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Sequential Minimal Optimization (SMO) is an algorithm to solve the SVM Quadratic Programming (QP) problem efficiently (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Platt, n.d.&lt;/a&gt;). Developed by John Platt at Microsoft Research, SMO deals with the constraints of the SVM objective by breaking it down into a smaller optimization problem at each step.&lt;/p&gt;
&lt;p&gt;The two key components of SMO are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;an analytic method to solving for two Lagrange multipliers at a time&lt;/li&gt;
&lt;li&gt;and a heuristic for choosing which multipliers to optimize.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The original objective is to maximize the margin between the nearest positive and negative examples.
For the linear case, if the output is given as&lt;/p&gt;
&lt;p&gt;\[
u = \mathbf{w}^T \mathbf{x} - b,
\]&lt;/p&gt;
&lt;p&gt;where \(\mathbf{w}\) is the normal vector to the hyperplane separating the classes, then the margin is given as&lt;/p&gt;
&lt;p&gt;\[
m = \frac{1}{\|w\|_2}.
\]&lt;/p&gt;
&lt;p&gt;Maximizing this margin yielded the primal optimization problem&lt;/p&gt;
&lt;p&gt;\begin{align*}
\min_{\mathbf{w},b} \frac{1}{2} \|\mathbf{w}\|^2\\
\textrm{s.t.} \quad &amp;amp; y_i(\mathbf{w}^T \mathbf{x} - b) \geq 1, \forall i\\
\end{align*}&lt;/p&gt;
&lt;p&gt;The dual form of the objective function for a &lt;a href=&#34;https://ajdillhoff.github.io/notes/support_vector_machine/&#34;&gt;Support Vector Machine&lt;/a&gt; is&lt;/p&gt;
&lt;p&gt;\[
\min_{\vec\alpha} \Psi(\vec{\alpha}) = \min_{\vec{\alpha}} \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N y_i y_j K(\mathbf{x}_i, \mathbf{x}_j)\alpha_i\alpha_j - \sum_{i=1}^N \alpha_i
\]&lt;/p&gt;
&lt;p&gt;with inequality constraints&lt;/p&gt;
&lt;p&gt;\[
\alpha_i \geq 0, \forall i,
\]&lt;/p&gt;
&lt;p&gt;and a linear equality constraint&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^N y_i \alpha_i = 0.
\]&lt;/p&gt;
&lt;p&gt;For a linear SVM, the output is dependent on a weight vector \(\mathbf{w}\) and threshold \(b\):&lt;/p&gt;
&lt;p&gt;\[
\mathbf{w} = \sum_{i=1}^N y_i \alpha_i \mathbf{x}_i, \quad b = \mathbf{w}^T \mathbf{x}_k - y_k.
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;The threshold is also dependent on the weight vector?&lt;/strong&gt;&lt;/strong&gt; The weight vector \(\mathbf{w}\) is computed using the training data. The threshold is only dependent on non-zero support vectors, \(\alpha_k &amp;gt; 0\).&lt;/p&gt;
&lt;h3 id=&#34;overlapping-distributions&#34;&gt;Overlapping Distributions&lt;/h3&gt;
&lt;p&gt;Slack variables were introduced to allow misclassifications at the cost of a linear penalty.
This is useful for datasets that are not linearly separable.
In practice, this is accomplished with a slight modification of the original objective function:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\min_{\mathbf{w},b} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^N \xi_i\\
\textrm{s.t.} \quad &amp;amp; y_i(\mathbf{w}^T \mathbf{x} - b) \geq 1 - \xi_i, \forall i\\
\end{align*}&lt;/p&gt;
&lt;p&gt;The convenience of this formulation is that the parameters \(\xi_i\) do not appear in the dual formulation at all.
The only added constraint is&lt;/p&gt;
&lt;p&gt;\[
0 \leq \alpha_i \leq C, \forall i.
\]&lt;/p&gt;
&lt;p&gt;This is referred to as the box constraint for reasons we shall see shortly.&lt;/p&gt;
&lt;h2 id=&#34;box-constraints&#34;&gt;Box Constraints&lt;/h2&gt;
&lt;p&gt;The smallest optimization step that SMO solves is that of two variables.
Given the constraints above, the solution lies on a diagonal line \(\sum_{i=1}^N y_i \alpha_i = 0\) bounded within a box \(0 \leq \alpha_i \leq C, \forall i\).&lt;/p&gt;
&lt;p&gt;Isolating for two samples with alphas \(\alpha_1\) and \(\alpha_2\), the constraint \(\sum_{i=1}^n y_i \alpha_i = 0\) suggests that&lt;/p&gt;
&lt;p&gt;\[
y_1 \alpha_1 + y_2 \alpha_2 = w.
\]&lt;/p&gt;
&lt;p&gt;We first consider the case when \(y_1 \neq y_2\).
Let \(y_1 = 1\) and \(y_2 = -1\), then \(a_1 - a_2 = w\).
As \(\alpha_1\) increases, \(\alpha_2\) must also increase to satisfy the constraint.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-07-10_22-48-56_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Equality constraint for case 1 (from Platt&amp;#39;s SMO paper).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Equality constraint for case 1 (from Platt&amp;rsquo;s SMO paper).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The other case is when \(y_1 = y_2\), then \(\alpha_1 + \alpha_2 = w\).
As \(\alpha_1\) is increased, \(\alpha_2\) is decreased to satisfy the constraint.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-07-10_22-51-53_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Box constraint for samples of the same class (from Platt&amp;#39;s SMO paper).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Box constraint for samples of the same class (from Platt&amp;rsquo;s SMO paper).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;updating-the-lagrangians&#34;&gt;Updating the Lagrangians&lt;/h2&gt;
&lt;p&gt;SMO solves for only two Lagrange multipliers at a time.
Solving for only 1 at a time would be impossible under the constraint \(\sum_{i=1}^N y_i \alpha_i = 0\).
The first step is to compute \(\alpha_2\) and constrain it between the ends of the diagonal line segment from the box constraints.&lt;/p&gt;
&lt;p&gt;If \(y_1 \neq y_2\), then the following bounds are applied to \(\alpha_2\):&lt;/p&gt;
&lt;p&gt;\begin{equation*}
L = \max(0, \alpha_2 - \alpha_1), \quad H = \min(C, C + \alpha_2 - \alpha_1)
\end{equation*}&lt;/p&gt;
&lt;p&gt;otherwise, the bounds are computed as:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
L = \max(0, \alpha_2 + \alpha_1 - C), \quad H = \min(C, \alpha_2 + \alpha_1)
\end{equation*}&lt;/p&gt;
&lt;p&gt;Updating the actual parameter is done following the update rule of gradient descent:&lt;/p&gt;
&lt;p&gt;\[
\alpha_2^{\text{new}} = \alpha_2 + \frac{y_2(E_1 - E_2)}{\eta}.
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do we arrive at this update rule?&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;second-derivative-of-the-objective-function&#34;&gt;Second Derivative of the Objective Function&lt;/h3&gt;
&lt;p&gt;Here, \(\eta\) represents the step size and direction. It is computed from the second derivative of the objective function along the diagonal line. To see that this is the case, consider the original objective function&lt;/p&gt;
&lt;p&gt;\begin{align*}
\min_{\mathbf{\alpha}} \quad &amp;amp; \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N y_i y_j K(\mathbf{x}_i, \mathbf{x}_j) \mathbf{\alpha}_1 \mathbf{\alpha}_2 - \sum_{i=1}^N \alpha_i\\
\textrm{s.t.} \quad &amp;amp; 0 \leq \alpha_i \leq C, \forall i\\
&amp;amp; \sum_{i=1}^N y_i \alpha_i = 0\\
\end{align*}&lt;/p&gt;
&lt;p&gt;Since we are optimizing with respect to only 2 Lagrangian multipliers at a time, we can write the Lagrangian function as&lt;/p&gt;
&lt;p&gt;\[
\frac{1}{2} y_1^2 K_{11} \alpha_1^2 + \frac{1}{2} y_2^2 K_{22} \alpha_2^2 + y_1 \alpha_1 \sum_{j=3}^N y_j \alpha_j K_{1j} + y_2 \alpha_2 \sum_{j=3}^N y_j \alpha_j K_{2j} - \alpha_1 - \alpha_2 + \sum_{j=3}^N \alpha_j
\]&lt;/p&gt;
&lt;p&gt;We are only optimizing with respect to \(\alpha_1\) and \(\alpha_2\), the next step is to extract those terms from the sum.
This is simplified further by noting that \(\sum_{j=3}^N y_j \alpha_j K_{ij}\) looks very similar to the output of an SVM:&lt;/p&gt;
&lt;p&gt;\[
u = \sum_{j=1}^N y_j \alpha_j K(\mathbf{x}_j, \mathbf{x}) - b.
\]&lt;/p&gt;
&lt;p&gt;This allows us to introduce a variable \(v_i\) based on \(u_i\), the output of an SVM given sample \(\mathbf{x}_i\):&lt;/p&gt;
&lt;p&gt;\[
v_i = \sum_{j=3}^N y_j \alpha_j K_{ij} = u_i + b - y_1 \alpha_1 K_{1i} - y_2 \alpha_2 K_{2i}.
\]&lt;/p&gt;
&lt;p&gt;The objective function is then written as&lt;/p&gt;
&lt;p&gt;\[
\frac{1}{2} y_1^2 K_{11} \alpha_1^2 + \frac{1}{2} y_2^2 K_{22} \alpha_2^2 + y_1 \alpha_1 v_1 + y_2 \alpha_2 v_2 - \alpha_1 - \alpha_2 + \sum_{j=3}^N \alpha_j.
\]&lt;/p&gt;
&lt;p&gt;Note that the trailing sum \(\sum_{j=3}^N \alpha_j\) is treated as a constant since those values are not considered when optimizing for \(\alpha_1\) and \(\alpha_2\).&lt;/p&gt;
&lt;p&gt;Given the box constraints from above, we must update \(\alpha_1\) and \(\alpha_2\) such that&lt;/p&gt;
&lt;p&gt;\[
\alpha_1 + s \alpha_2 = \alpha_1^* + s \alpha_2^* = w.
\]&lt;/p&gt;
&lt;p&gt;This linear relationship allows us to express the objective function in terms of α_2:&lt;/p&gt;
&lt;p&gt;\[
\Psi = \frac{1}{2} y_1^2 K_{11} (w - s \alpha_2)^2 + \frac{1}{2} y_2^2 K_{22} \alpha_2^2 + y_1 (w - s \alpha_2) v_1 + y_2 \alpha_2 v_2 - \alpha_1 - \alpha_2 + \sum_{j=3}^N \alpha_j.
\]&lt;/p&gt;
&lt;p&gt;The extremum of the function is given by the first derivative with respect to \(\alpha_2\):&lt;/p&gt;
&lt;p&gt;\[
\frac{d\Psi}{d\alpha_2} = -sK_{11}(w - s\alpha_2) + K_{22}\alpha_2 - K_{12}\alpha_2 + s K_{12} (w - s \alpha_2) - y_2 v_2 + s + y_2 v_2 - 1 = 0.
\]&lt;/p&gt;
&lt;p&gt;In most cases, the second derivative will be positive.
The minimum of \(\alpha_2\) is where&lt;/p&gt;
&lt;p&gt;\begin{align*}
\alpha_2 (K_{11} + K_{22} - 2 K_{12}) &amp;amp;= s(K_{11} - K_{12})w + y_2(v_1 - v_2) + 1 - s\\
&amp;amp;= s(K_{11} - K_{12})(s\alpha_2^*+\alpha_1^*)\\
&amp;amp;+ y_2(u_1-u_2+y_1\alpha_1^*(K_{12} - K_{11}) + y_2 \alpha_2^* (K_{22} - K_{21})) + y_2^2 - s\\
&amp;amp;= \alpha_2^*(K_{11}+K_{22} - 2K_{12}) + y_2(u_1 - u_2 + y_2 - y_1).
\end{align*}&lt;/p&gt;
&lt;p&gt;If we let \(E_1 = u_1 - y_1\), \(E_2 = u_2 - y_2\), and \(\eta = K_{11} + K_{22} - 2K_{12}\), then&lt;/p&gt;
&lt;p&gt;\[
\alpha_2^{\text{new}} = \alpha_2 + \frac{y_2(E_1 - E_2)}{\eta}.
\]&lt;/p&gt;
&lt;h2 id=&#34;the-algorithm&#34;&gt;The Algorithm&lt;/h2&gt;
&lt;p&gt;Sequential Minimal Optimization (SMO) solves the SVM problem which usually requires a Quadratic Programming (QP) solution.
It does this by breaking down the larger optimization problem into a small and simple form: solving for two Lagrangians.
Solving for one would not be possible without violating KKT conditions.
There are two components to Sequential Minimal Optimization: the first is how the Lagrangians are selected and the second is the actual optimization step.&lt;/p&gt;
&lt;h3 id=&#34;choosing-the-first-lagrangian&#34;&gt;Choosing the First Lagrangian&lt;/h3&gt;
&lt;p&gt;The algorithm first determines which samples in the dataset violate the given KKT conditions. Only those violating the conditions are eligible for optimization. A solution is found when the following are true for all \(i\):&lt;/p&gt;
&lt;p&gt;\begin{align*}
\alpha_i = 0 \iff y_i u_i \geq 1,\\
0 &amp;lt; \alpha_i &amp;lt; C \iff y_i u_i = 1,\\
\alpha_i = C \iff y_i u_i \leq 1.
\end{align*}&lt;/p&gt;
&lt;p&gt;Additionally, samples that are not on the bounds are selected (those with \(\alpha_i \neq 0\) and \(\alpha_i \neq C\)).
This continues through the dataset until no sample violates the KKT constraints within \(\epsilon\).&lt;/p&gt;
&lt;p&gt;As a last step, SMO searches the entire dataset to look for any bound samples that violate KKT conditions. It is possible that updating a non-bound sample would cause a bound sample to violate the KKT conditions.&lt;/p&gt;
&lt;h3 id=&#34;choosing-the-second-lagrangian&#34;&gt;Choosing the Second Lagrangian&lt;/h3&gt;
&lt;p&gt;The second Lagrangian is chosen to maximize the size of the step taken during joint optimization.
Noting that the step size is based on&lt;/p&gt;
&lt;p&gt;\[
\alpha_2^{\text{new}} = \alpha_2 + \frac{y_2(E_1 - E_2)}{\eta},
\]&lt;/p&gt;
&lt;p&gt;it is approximated by computing \(|E_1 - E_2|\).&lt;/p&gt;
&lt;p&gt;If positive progress cannot be made given the choice of Lagrangian, SMO will begin iterating through non-bound examples.
If no eligible candidates are found in the non-bound samples, the entire dataset is searched.&lt;/p&gt;
&lt;h3 id=&#34;updating-the-parameters&#34;&gt;Updating the Parameters&lt;/h3&gt;
&lt;p&gt;With the second derivative of the objective function, we can take an optimization step along the diagonal line.
To ensure that this step adheres to the box constraints defined above, the new value of \(\alpha_2\) is clipped:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\alpha_2^{\text{new,clipped}} =
\begin{cases}
H &amp;amp;\text{if} \quad \alpha_2^{\text{new}} \geq H;\\
\alpha_2^{\text{new}} &amp;amp;\text{if} \quad L &amp;lt; \alpha_2^{\text{new}} &amp;lt; H;\\
L &amp;amp;\text{if} \quad \alpha_2^{\text{new}} \geq L.\\
\end{cases}
\end{equation*}&lt;/p&gt;
&lt;p&gt;With the new value of \(\alpha_2\), \(\alpha_1\) is computed such that the original KKT condition is preserved:&lt;/p&gt;
&lt;p&gt;\[
\alpha_1^{\text{new}} = \alpha_1 + s(\alpha_2 - \alpha_2^{\text{new,clipped}}),
\]&lt;/p&gt;
&lt;p&gt;where \(s = y_1y_2\).&lt;/p&gt;
&lt;p&gt;Points that are beyond the margin are given an alpha of 0: \(\alpha_i = 0\).
Points that are on the margin satisfy \(0 &amp;lt; \alpha_i &amp;lt; C\). These are the support vectors.
Points inside the margin satisfy \(\alpha_i = C\).&lt;/p&gt;
&lt;h4 id=&#34;linear-svms&#34;&gt;Linear SVMs&lt;/h4&gt;
&lt;p&gt;In the case of linear SVMs, the parameters can be stored as a single weight vector&lt;/p&gt;
&lt;p&gt;\[
\mathbf{w}^{\text{new}} = \mathbf{w} + y_1 (\alpha_1^{\text{new}} - \alpha_1)\mathbf{x}_1 + y_2(\alpha_2^{\text{new,clipped}} - \alpha_2)\mathbf{x}_2.
\]&lt;/p&gt;
&lt;p&gt;The output of a linear SVM is computed as&lt;/p&gt;
&lt;p&gt;\[
u = \mathbf{w}^T \mathbf{x} - b.
\]&lt;/p&gt;
&lt;h4 id=&#34;nonlinear-svms&#34;&gt;Nonlinear SVMs&lt;/h4&gt;
&lt;p&gt;In the nonlinear case, the output of the model is computed as&lt;/p&gt;
&lt;p&gt;\[
u = \sum_{i=1}^N y_i \alpha_i K(\mathbf{x}_i, \mathbf{x}) - b.
\]&lt;/p&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;An implementation of SMO in Python is available at &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/blob/main/svm/smo.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://github.com/ajdillhoff/CSE6363/blob/main/svm/smo.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Platt, John C. n.d. “Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines,” 21.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Discriminant Functions</title>
      <link>https://ajdillhoff.github.io/notes/discriminant_functions/</link>
      <pubDate>Tue, 07 Jun 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/discriminant_functions/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#binary-classification&#34;&gt;Binary Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plotting-a-decision-boundary&#34;&gt;Plotting a Decision Boundary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiple-classes&#34;&gt;Multiple Classes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sensitivity-to-outliers&#34;&gt;Sensitivity to Outliers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The notebook for this lesson can be found &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/blob/main/logistic_regression/least_squares_classification.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_regression/&#34;&gt;linear regression&lt;/a&gt;, we were able to fit a model to our data in order to make inferences on unseen data points. In the examples, both the input features and observation were continuous. With discriminant functions, we will use similar models to classify the data points based on their input features. We start out with the simplest approach: we assume that the data is linearly separable and can be assigned one of \(K\) discrete classes.&lt;/p&gt;
&lt;p&gt;For classification with linear discriminant functions, we will use a \(K\) dimensional vector that has a 1 corresponding to the class encoding for that input and a 0 for all other positions. For example, if our possible target classes were \(\{\text{car, truck, person}\}\), then a target vector for \(\text{person}\) would be \(\mathbf{y} = [0, 0, 1]^T\).&lt;/p&gt;
&lt;p&gt;This article will stick to a discriminative approach to classification. That is, we define a discriminant function which assigns each data input \(\mathbf{x}\) to a class. For a probabilistic perspective, see &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_discriminant_analysis/&#34;&gt;Linear Discriminant Analysis&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We will again start with a linear model \(y = f(\mathbf{x}; \mathbf{w})\). Unlike the model used with &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_regression/&#34;&gt;linear regression&lt;/a&gt;, ours will need to predict a discrete class label. In other words, we need to predict a vector with a 1 corresponding to the class encoding.&lt;/p&gt;
&lt;h2 id=&#34;binary-classification&#34;&gt;Binary Classification&lt;/h2&gt;
&lt;p&gt;Consider a simple dataset with 2 features per data sample. Our goal is to classify the data as being one of two possible classes.
This only requires a single function which classifies the sample as being in class 0 if \(f(\mathbf{x};\mathbf{w}) \geq 0\) and class 1 otherwise.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-23_18-10-03_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Two groups of data that are very clearly linearly separable.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Two groups of data that are very clearly linearly separable.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The model output is such that \(f(\mathbf{x};\mathbf{w}) = [1, 0]\) when \(\mathbf{x}\) is predicted as class 1. If \(f(\mathbf{x};\mathbf{w}) = [0, 1]\) then \(\mathbf{x}\) is assigned to class 2.
In practice, the actual output will not be a one-hot vector.
There will be some values in all positions of the vector.&lt;/p&gt;
&lt;p&gt;For example, a model trained on a binary classification task outputs the following vector given a randomly selected input sample:&lt;/p&gt;
&lt;p&gt;\[
[0.1224, 0.8776]
\]&lt;/p&gt;
&lt;p&gt;A class would be assigned by taking the argmax of this output vector.
That is, the model predicts that this sample belongs to class 1.&lt;/p&gt;
&lt;h3 id=&#34;measuring-classifier-performance&#34;&gt;Measuring Classifier Performance&lt;/h3&gt;
&lt;p&gt;L1 loss can be used to measure classifier performance for linear discriminant function models.&lt;/p&gt;
&lt;p&gt;\[
E = \sum_{i=1}^N \sum_{j=1}^M |\hat{y}_{ij} - y_{ij}|
\]&lt;/p&gt;
&lt;h2 id=&#34;plotting-a-decision-boundary&#34;&gt;Plotting a Decision Boundary&lt;/h2&gt;
&lt;p&gt;In the case of binary classification, a sample is predicted as class 1 if the output vector has the highest value at index 0.
Otherwise, it is classified as class 2.
If we were to plot the decision regions, we would see that the boundary is at the point when the output for both classes is equal.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-10_19-03-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Binary classification with decision regions shown.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Binary classification with decision regions shown.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;multiple-classes&#34;&gt;Multiple Classes&lt;/h2&gt;
&lt;p&gt;Extending this to multiple classes is as easy as encoding the classes in a one-hot vector whose length equals the number of classes.
The parameters of the model can be obtained using gradient descent, the normal equations, or any other method that optimizes the least squares criterion.&lt;/p&gt;
&lt;p&gt;The figure below shows an example of a linear discriminant function model fit to a dataset with 3 classes.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-10_19-08-29_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Multiclass classification using linear discriminant functions.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Multiclass classification using linear discriminant functions.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;sensitivity-to-outliers&#34;&gt;Sensitivity to Outliers&lt;/h2&gt;
&lt;p&gt;One major flaw with least squares models is their sensitivity to outliers in the data.
Consider the dataset shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-11_11-28-25_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Linearly separable dataset&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Linearly separable dataset
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This dataset is clearly linearly separable. This will be no problem for our linear classifier, as seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-11_11-29-36_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Linear classifier fit to data using least squares.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Linear classifier fit to data using least squares.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This dataset has a convenient property that the samples from each class are tightly clustered.
What happens if our data is slightly more diverse?&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-11_11-32-10_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;2-class dataset in which one class is not as tightly clustered as the other.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;2-class dataset in which one class is not as tightly clustered as the other.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the dataset above, we can still clearly see that it should be linearly separable.
Unfortunately, our least squares model will be very sensitive to the 20 points at the top left of the plot.
Training a linear discriminant function using least squares results in the following decision boundary.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-11_11-33-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;The model misclassifies samples that should be linearly separable.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;The model misclassifies samples that should be linearly separable.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;If we determine that a linear classifier is adequate for a given dataset, we may wish to use a slightly more robust model such as &lt;a href=&#34;https://ajdillhoff.github.io/notes/logistic_regression/&#34;&gt;Logistic Regression&lt;/a&gt; instead of linear discriminant functions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Long Short-Term Memory</title>
      <link>https://ajdillhoff.github.io/notes/long_short_term_memory/</link>
      <pubDate>Tue, 12 Apr 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/long_short_term_memory/</guid>
      <description>&lt;p&gt;The recurrent nature of RNNs means that gradients get smaller and smaller as the timesteps increase.
This is known as the &lt;strong&gt;vanishing gradient problem&lt;/strong&gt;.
One of the first popular solutions to this problem is called &lt;strong&gt;Long Short-Term Memory&lt;/strong&gt;, a recurrent network architecture by Hochreiter and Schmidhuber.&lt;/p&gt;
&lt;p&gt;An LSTM is made up of memory blocks as opposed to simple hidden units.
Each block is differentiable and contains a memory cell along with 3 gates: the input, output, and forget gates.
These components allow the blocks to maintain some history of information over longer range dependencies.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_13-36-14_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;LSTM memory block with a single cell (adapted from Andrew Ng&amp;#39;s diagram).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;LSTM memory block with a single cell (adapted from Andrew Ng&amp;rsquo;s diagram).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The original LSTM only had an input and output gate.
Forget gates were added in 2000 by Gers et al. to control the amount of context that could be reset, if the task called for it.
Peephole connections were proposed by Gers et al. in 2002.
These are weights that combine the previous cell state to the gates in order to learn tasks that require precise timing.&lt;/p&gt;
&lt;p&gt;By controlling when information can either enter or leave the memory cell, LSTM blocks are able to maintain more historical context than RNNs.&lt;/p&gt;
&lt;h2 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h2&gt;
&lt;p&gt;The equations listed here follow notation and description from Alex Graves&amp;rsquo; thesis.&lt;/p&gt;
&lt;p&gt;The weight from unit \(i\) to \(j\) is given as \(w_{ij}\).
The input to unit \(j\) at time \(t\) is \(a_j^t\) and the result of its activation function is \(b_j^t\).
Let \(\psi\), \(\phi\), and \(\omega\) be the input, forget, and output gates.
A memory cell is denoted by \(c \in C\), where \(C\) is the set of cells in the network.
The activation, or state, of a given cell \(c\) at time \(t\) is \(s_c^t\).
The output of each gate passes through an activation function \(f\), while the input and output activation functions of a memory block are given by \(g\) and \(h\).&lt;/p&gt;
&lt;p&gt;The forward pass for the input gates is&lt;/p&gt;
&lt;p&gt;\[
a_{\psi}^t = \sum_{i=1}^I w_{i\psi}x_i^t + \sum_{h=1}^H w_{h\psi}b_h^{t-1} + \sum_{c=1}^C w_{c\psi}s_c^{t-1}.
\]&lt;/p&gt;
&lt;p&gt;The output of the forget gates is&lt;/p&gt;
&lt;p&gt;\[
a_{\phi}^t = \sum_{i=1}^I w_{i\phi}x_i^t + \sum_{h=1}^H w_{h\phi}b_h^{t-1} + \sum_{c=1}^C w_{c\phi}s_c^{t-1}.
\]&lt;/p&gt;
&lt;p&gt;The output of the output gates is&lt;/p&gt;
&lt;p&gt;\[
a_{\omega}^t = \sum_{i=1}^I w_{i\omega}x_i^t + \sum_{h=1}^H w_{h\omega}b_h^{t-1} + \sum_{c=1}^C w_{c\omega}s_c^{t-1}.
\]&lt;/p&gt;
&lt;p&gt;Each of the outputs above is passed through an activation function \(f\).&lt;/p&gt;
&lt;p&gt;The output of each cell is computed as&lt;/p&gt;
&lt;p&gt;\[
a_c^t = \sum_{i=1}^I w_{ic}x_i^t + \sum_{i=1}^H w_{hc}b_h^{t-1}
\]&lt;/p&gt;
&lt;p&gt;and the internal state is updated via&lt;/p&gt;
&lt;p&gt;\[
s_c^t = b_{\phi}^t s_c^{t-1} + b_{\psi}^t g(a_c^t).
\]&lt;/p&gt;
&lt;p&gt;The state update considers the state at the previous timestep multiplied by the output of the forget gate.
That is, it controls how much of the current memory to keep.&lt;/p&gt;
&lt;p&gt;The final cell output is given as&lt;/p&gt;
&lt;p&gt;\[
b_c^t = b_{\omega}^t h(s_c^t).
\]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recurrent Neural Networks</title>
      <link>https://ajdillhoff.github.io/notes/recurrent_neural_networks/</link>
      <pubDate>Sun, 10 Apr 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/recurrent_neural_networks/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bidirectional-recurrent-neural-networks&#34;&gt;Bidirectional Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Neural networks are an effective tool for regression and classification tasks, but they do not consider the dependencies of information over time.
Many tasks have implicit information that is dependent on input that may have already been processed or may not be seen until the future.&lt;/p&gt;
&lt;p&gt;Recurrent Neural Networks (RNN) consider the historical context of time-series data.
Bi-directional Recurrent Neural Networks (BRNN) consider both historical and future context. This is necessary for tasks like language tanslation.&lt;/p&gt;
&lt;p&gt;Parameter sharing across different parts of the model is key for sequence models.
Different instances of a particular feature may appear at different time steps.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I see Naomi there.&amp;rdquo; and &amp;ldquo;Naomi is right there&amp;rdquo; both convey that Naomi is present, but we would not require the model to have separate parameters just because the word position is different between the two.&lt;/p&gt;
&lt;p&gt;Recurrent connections provide a memory of sorts.
This enables important contextual information to be &amp;ldquo;remembered&amp;rdquo; throughout time.
These models are not without their limitations.
When trained with gradient descent, the gradient information passed throughout multiple time steps can become insignificant.
There are several ways to address the &lt;strong&gt;vanishing gradient&lt;/strong&gt; problem which are explored in alternative models such as &lt;a href=&#34;https://ajdillhoff.github.io/notes/long_short_term_memory/&#34;&gt;Long Short-Term Memory&lt;/a&gt; and &lt;a href=&#34;https://ajdillhoff.github.io/notes/transformers/&#34;&gt;Transformers&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;The definition of RNNs start with that of &lt;a href=&#34;https://ajdillhoff.github.io/notes/neural_networks/&#34;&gt;Neural Networks&lt;/a&gt;.
One layer of an RNN has some number of hidden units that transforms the input into an intermediate representation.
In addition to transforming the input, another set of parameters is used to transform the hidden context over time.
The difference is that the hidden layer is shared over time, as seen in the equation below.&lt;/p&gt;
&lt;p&gt;\[
\mathbf{h}^{(t)} = f(\mathbf{h}^{(t-1)}, \mathbf{x}^{(t)}; \mathbf{\theta})
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-09_16-36-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Computation graph of an RNN (By fdeloche - Own work, CC BY-SA 4.0, &amp;lt;https://commons.wikimedia.org/w/index.php?curid=60109157&amp;gt;)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Computation graph of an RNN (By fdeloche - Own work, CC BY-SA 4.0, &lt;a href=&#34;https://commons.wikimedia.org/w/index.php?curid=60109157&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://commons.wikimedia.org/w/index.php?curid=60109157&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the computation graph above, a recurrent network has three weight matrices associated with its forward pass.
An input weight matrix \(U \in \mathbb{R}^{H \times D}\) processes the features for each &lt;em&gt;frame&lt;/em&gt; of the input sequence.
The hidden layer has a weight matrix \(V \in \mathbb{R}^{H \times H}\), where \(H\) is the number of hidden nodes.
The output layer will have a weight matrix \(W \in \mathbb{R}^{O \times H}\).&lt;/p&gt;
&lt;h3 id=&#34;forwards-pass&#34;&gt;Forwards Pass&lt;/h3&gt;
&lt;p&gt;To understand the computation graph of an RNN, consider an input of length \(T\) with \(D\) features. That is, each input sample is a sequence of features. This could be represented as encoded video data, text data, or any other sequence signals.
To compute the output of a hidden layer \(\mathbf{h}\) at time \(t\), take a linear combination of all input feature \(x_i^t\) at time \(t\) in addition to the output of the previous hidden layer and then add the linear combination of output activations for each node in the hidden layer:&lt;/p&gt;
&lt;p&gt;\[
a_h^t = \sum_{d=1}^D w_{dh} x_d^t + \sum_{h&amp;rsquo;=1}^H w_{h&amp;rsquo;h} b_{h&amp;rsquo;}^{t-1},
\]&lt;/p&gt;
&lt;p&gt;where \(b_h^t = \theta_h(a_h^t)\) and we assume the bias term is concatenated with the weights.&lt;/p&gt;
&lt;p&gt;Weights in the hidden layer are crucial for RNNs to adapt to contextual features based on their occurrence relative to time.
For example, a character-based language model based on a traditioinal network would produce similar output for consecutive letters that are the same.
In an RNN, the hidden weights would produce a different output for each consecutive character even if it were the same.&lt;/p&gt;
&lt;p&gt;The hidden layer outputs are used in both the subsequent computations through time as well as the output node for each instance \(t\). The inputs to the output node are computed from the hidden node at the same time as the output to the hidden activation:&lt;/p&gt;
&lt;p&gt;\[
a_k^t = \sum_{h=1}^H w_{hk}b_h^t.
\]&lt;/p&gt;
&lt;h3 id=&#34;backwards-pass&#34;&gt;Backwards Pass&lt;/h3&gt;
&lt;p&gt;The gradients of a recurrent network are computed using backpropagation, similar to neural networks.
Since the forward pass is over \(t\) time step, the backward pass must consider them as well.
This variant of backpropagation for recurrent models is calling backpropagation through time (BPTT).&lt;/p&gt;
&lt;p&gt;Like a feed forward network, the output is dependent on the activation of the hidden layer.
For a recurrent model, its dependence is through the output of the hidden layer as well as the pass to the next hidden time step.&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathcal{L}}{\partial a_h^t} = \frac{\partial \mathcal{L}}{\partial b_h^t} \frac{\partial b_h^t}{\partial a_h^t}
\]&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathcal{L}}{\partial b_h^t} = \sum_{k=1}^K \frac{\partial \mathcal{L}}{\partial a_k^t} \frac{\partial a_k^t}{\partial b_h^t} + \sum_{h&amp;rsquo;=1}^H \frac{\partial \mathcal{L}}{\partial a_{h&amp;rsquo;}^{t+1}} \frac{\partial a_{h&amp;rsquo;}^{t+1}}{\partial a_{h}^t}
\]&lt;/p&gt;
&lt;p&gt;The derivatives with respect to the weights are given as&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathcal{L}}{\partial w_{ij}} = \sum_{t=1}^T \frac{\partial \mathcal{L}}{\partial a_j^t} \frac{\partial a_j^t}{\partial w_{ij}}.
\]&lt;/p&gt;
&lt;h2 id=&#34;bidirectional-recurrent-neural-networks&#34;&gt;Bidirectional Recurrent Neural Networks&lt;/h2&gt;
&lt;p&gt;Standard RNNs work for many problems with sequential input.
Training such a model would consider the full input through time \(T\), but inference may only be able to consider the data up to time \(t &amp;lt; T\).
There are sequential tasks which could leverage from both past and future context, such as language translation.
For this case, BRNNs were proposed &amp;lt;&amp;amp;schusterBidirectionalRecurrentNeural1997&amp;gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-11-06_15-18-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Diagram of BRNN from Graves et al.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Diagram of BRNN from Graves et al.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://karpathy.github.io/2015/05/21/rnn-effectiveness/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;The Unreasonable Effectiveness of RNNs&lt;/a&gt; by Andrej Karpathy&lt;/li&gt;
&lt;li&gt;&amp;lt;&amp;amp;gravesSupervisedSequenceLabelling2012&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Understanding LSTM Networks&lt;/a&gt; by Christopher Colah&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Optimization for Deep Learning</title>
      <link>https://ajdillhoff.github.io/notes/optimization_for_deep_learning/</link>
      <pubDate>Thu, 07 Apr 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/optimization_for_deep_learning/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gradient-descent-and-its-variants&#34;&gt;Gradient Descent and its Variants&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adaptive-learning-rate-methods&#34;&gt;Adaptive Learning Rate Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parameter-initialization&#34;&gt;Parameter Initialization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ruder.io/optimizing-gradient-descent/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://ruder.io/optimizing-gradient-descent/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.deeplearningbook.org/contents/optimization.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.deeplearningbook.org/contents/optimization.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;empirical risk minimization&lt;/strong&gt; - minimizing over an empirical distribution. Differs from risk minimization which is minimizing over the true distribution. We typically do not know the true distribution.&lt;/p&gt;
&lt;p&gt;Complex models are able to memorize the dataset.&lt;/p&gt;
&lt;p&gt;In many applications for training, what we want to optimize is different from what we actually optimize since we need to have useful derivatives for gradient descent. For example, the 0-1 loss&lt;/p&gt;
&lt;p&gt;\begin{equation*}
L(i, j) =
\begin{cases}
0 \qquad i = j \\
1 \qquad i \ne j
\end{cases}
\qquad i,j \in M
\end{equation*}&lt;/p&gt;
&lt;p&gt;is what we would really want to minimize for classification tasks.
In practice we use something like &lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Cross Entropy Loss&lt;/a&gt;.
As pointed out by (Goodfellow et al.), there are sometimes advantages with using &lt;strong&gt;surrogate loss functions&lt;/strong&gt;.
A 0-1 loss may eventually fit the training set with 100% accuracy.
At this point, no further optimization could take place as the error would be 0.
With losses like negative log likelihood, optimization could continue which may result in increasing the margin between classes.&lt;/p&gt;
&lt;p&gt;Larger batch sizes provide a more accurate estimate of the gradient.&lt;/p&gt;
&lt;p&gt;Randomly selecting samples is crucial for learning.
Datasets may be arranged in such a strong bias is present.
Shuffling once isn&amp;rsquo;t enough because the data is biased after the first iteration.
We could only get around this if we had the true distribution to generate new samples.&lt;/p&gt;
&lt;p&gt;If our training set is extremely large, we may converge to a solution without ever having gone through all samples.
Typically, models are able to train on multiple passes of the dataset to increase their generalization error.
Each subsequent pass may increase the bias, but not enough to decrease generalization performance.&lt;/p&gt;
&lt;p&gt;The gradient norm can be monitored while training to see if the issue is local minima or any other critical point (&lt;a href=&#34;#citeproc_bib_item_5&#34;&gt;Zhao, Zhang, and Hu, n.d.&lt;/a&gt;).
If the parameters were to get stuck at a critical point, the gradient norm should shrink over time.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-21_09-21-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;The gradient norm decreases as it settles into some minima (Zhao et al.).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;The gradient norm decreases as it settles into some minima (Zhao et al.).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;gradient-descent-and-its-variants&#34;&gt;Gradient Descent and its Variants&lt;/h2&gt;
&lt;p&gt;Original gradient descent update:&lt;/p&gt;
&lt;p&gt;\[
\theta = \theta - \eta \nabla_{\theta}J(\theta)
\]&lt;/p&gt;
&lt;p&gt;Having a constant value for \(\eta\) means that the network will usually be unable to converge to a local minimum.
As the parameters reach a minimum, the constant learning update means that it will jump around the true minimum point. This is usually remedied in part by setting up a decreasing learning rate schedule.
This necessarily requires more manual guess work as to what the best annealing schedule would be.&lt;/p&gt;
&lt;h3 id=&#34;momentum&#34;&gt;Momentum&lt;/h3&gt;
&lt;p&gt;When the loss surface is more steep in one dimension than others, SGD will move back and forth in the directions of greatest descent while only slowly moving in the direction with a smaller decline. The figure below gives an example.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-07_16-35-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;SGD moves slower towards covergence for non-uniform surfaces.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;SGD moves slower towards covergence for non-uniform surfaces.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;If the gradient had some &lt;em&gt;momentum&lt;/em&gt; which built up over time, it would take fewer iterations to converge.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-07_16-38-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;SGD with momentum converges in fewer iterations.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;SGD with momentum converges in fewer iterations.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In practice, this can be implemented by adding some fraction of the previous update to the current step:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{g}_t &amp;amp;= \alpha \mathbf{g}_{t-1} + \eta \nabla_{\theta}J(\theta)\\
\theta &amp;amp;= \theta - \mathbf{g}_t
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;nesterov-momentum&#34;&gt;Nesterov Momentum&lt;/h3&gt;
&lt;p&gt;If we allow the momentum to keep increasing, the steps become greater and greater. This could lead to the parameters &amp;ldquo;rolling&amp;rdquo; out of the minimum up a steep incline.
If our algorithm knew that it was coming up to an incline, it would be smarter to slow down.
This is essentially what Nesterov momentum does.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-07_17-01-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Nesterov momentum computes the gradient after applying momentum.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Nesterov momentum computes the gradient after applying momentum.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;\begin{align*}
\mathbf{g}_t &amp;amp;= \alpha \mathbf{g}_{t-1} + \eta \nabla_{\theta}J(\theta - \alpha \mathbf{g}_{t-1})\\
\theta &amp;amp;= \theta - \mathbf{g}_t
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;adaptive-learning-rate-methods&#34;&gt;Adaptive Learning Rate Methods&lt;/h2&gt;
&lt;p&gt;The rate at which a model converges to some solution is dependent on many factors. One that we can control is the learning rate.
If the learning rate is too large, the model may never converge because it jumps too far in each iteration.
If the learning rate is too small, it may take much longer to converge to any solution.&lt;/p&gt;
&lt;p&gt;It would be ideal if the optimization algorithm could adapt its learning rate to local changes in the loss landscape.
In that way, the algorithm would be less dependent on the initial learning rate.&lt;/p&gt;
&lt;h3 id=&#34;adagrad&#34;&gt;Adagrad&lt;/h3&gt;
&lt;p&gt;Adagrad adapts the learning rate to the parameters following the idea that parameters associated with salient features should be updated less frequently (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Duchi, Hazan, and Singer 2011&lt;/a&gt;). If they occur often, updating them with a larger step would result in a solution that is more dependent on them at the expense of other features.&lt;/p&gt;
&lt;p&gt;Adagrad uses a different learning rate for every parameter:&lt;/p&gt;
&lt;p&gt;\[
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}}g_t
\]&lt;/p&gt;
&lt;p&gt;Here, \(g_t = \frac{\partial J(\theta)}{\partial \theta}\). This provides a partial derivative for every parameter \(\theta_i\).
A history of gradient changes are accumulated in a matrix \(G_t \in \mathbb{R}^{d \times d}\) which is a diagonal matrix containing the sum of squares of the gradients with respect to each \(\theta_i\) up to the current step.&lt;/p&gt;
&lt;p&gt;In effect, the parameters with larger partial derivatives have a sharper decrease in learning rate.
The downside to this method is that, as squared gradients are accumulated in \(G_t\), the sum increases causing the learning rate to eventually be too small to learn.&lt;/p&gt;
&lt;h3 id=&#34;rmsprop&#34;&gt;RMSProp&lt;/h3&gt;
&lt;p&gt;To remedy the long term issues of Adagrad, Geoffrey Hinton proposed RMSProp.
There was no formal publication for this. It was discussed and taught in Coursera course on &lt;a href=&#34;https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Neural Networks&lt;/a&gt;.
Instead of accumulating gradients, RMSProp uses an exponentially weighted moving average:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{s}_t = \rho \mathbf{s} + (1 - \rho)\mathbf{g}_{t-1} \odot \mathbf{g}_t
\]&lt;/p&gt;
&lt;p&gt;A new parameter \(\rho\) controls how much of the historical gradient is used. The update is&lt;/p&gt;
&lt;p&gt;\[
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\mathbf{s}_t + \epsilon}}\mathbf{g}_t.
\]&lt;/p&gt;
&lt;p&gt;Hinton proposed that \(\rho=0.9\) in the original lectures.&lt;/p&gt;
&lt;h3 id=&#34;adam&#34;&gt;Adam&lt;/h3&gt;
&lt;p&gt;One of the most popular gradient descent variants in used today is Adam (&lt;a href=&#34;#citeproc_bib_item_4&#34;&gt;Kingma and Ba 2017&lt;/a&gt;).
Short for Adaptive Moment Estimation, Adam adapts the learning rate to each parameter.
Similar to RMSProp, it stores an exponentially moving average of past squared gradients.
Adam additionally stores first-order moments of the gradients.&lt;/p&gt;
&lt;p&gt;After calculating the gradients \(g_t\) at time \(t\) the first and second moment estimates are updated as&lt;/p&gt;
&lt;p&gt;\begin{align*}
m_t &amp;amp;= \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t\\
v_t &amp;amp;= \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2
\end{align*}&lt;/p&gt;
&lt;p&gt;The estimates \(m_t\) and \(v_t\) are initialized to zero leading to updated estimates that are biased to zero.
The authors counteract this by computing &lt;em&gt;bias-corrected estimates&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\hat{m}_t &amp;amp;= \frac{m_t}{1 - \beta_1^t}\\
\hat{v}_t &amp;amp;= \frac{v_t}{1 - \beta_2^t}
\end{align*}&lt;/p&gt;
&lt;p&gt;The final update rule step is&lt;/p&gt;
&lt;p&gt;\[
\theta_t = \theta_{t-1} - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}} + \epsilon}.
\]&lt;/p&gt;
&lt;p&gt;There are several other varients. A good overview of these can be found on &lt;a href=&#34;https://ruder.io/optimizing-gradient-descent/index.html#fn9&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Sebastian Ruder&amp;rsquo;s&lt;/a&gt; blog.
The figures below provide some visual intuition of the behavior of common gradient descent variants.
These visualizations were provided by &lt;a href=&#34;https://twitter.com/alecrad&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Alec Radford&lt;/a&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-07_22-14-25_opt1.gif&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Behavior of algorithms at a saddle point (Credit: Alec Radford).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Behavior of algorithms at a saddle point (Credit: Alec Radford).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-07_22-17-24_opt2.gif&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Behavior of each algorithm on a loss surface (Credit: Alec Radford).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Behavior of each algorithm on a loss surface (Credit: Alec Radford).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Additional visualizations can be found &lt;a href=&#34;https://github.com/Jaewan-Yun/optimizer-visualization&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;parameter-initialization&#34;&gt;Parameter Initialization&lt;/h2&gt;
&lt;p&gt;Due to the complexity of their loss landscapes, the choice of initialization can have a significant impact on the solution. This affects how quickly the model converges. Although &lt;a href=&#34;https://ai.googleblog.com/2022/04/reproducibility-in-deep-learning-and.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;recent work&lt;/a&gt; aims to smooth loss surfaces so that models are easier to train, deep learning models can be tricky to reproduce.&lt;/p&gt;
&lt;p&gt;There is not much known about what makes the most optimal initialization strategy, but one property is that of weight symmetry. If all weights are initialized to the same value, their update will also be uniform. If two nodes are connected to the same input, there update will be uniform as well. Understanding this, a reasonable initialization strategy would be to ensure that the weights to not permit any symmetry in nodes connected to the same input.&lt;/p&gt;
&lt;p&gt;Small weights during initialization may lead to vanishing gradients.
Large weights may lead to exploding gradients as successive multiplications are applied.
The parameter values should be large enough to propagate information effectively through the network.&lt;/p&gt;
&lt;h3 id=&#34;normalized-initialization--xavier&#34;&gt;Normalized Initialization (Xavier)&lt;/h3&gt;
&lt;p&gt;Normalized initialization chooses an initial scale of the weights of a fully connected layer based on the number input and output nodes:&lt;/p&gt;
&lt;p&gt;\[
W_{i,j} \sim U\Bigg(-\sqrt{\frac{6}{m + n}}, \sqrt{\frac{6}{m+n}}\Bigg),
\]&lt;/p&gt;
&lt;p&gt;where \(m\) and \(n\) are the number of input and output nodes, respectively.
This initialization was empirically validated by (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Glorot and Bengio, n.d.&lt;/a&gt;) with the goal that all layers have the same activation variance and back-propagated gradient variance.&lt;/p&gt;
&lt;h3 id=&#34;he-initialization&#34;&gt;He Initialization&lt;/h3&gt;
&lt;p&gt;Xavier initialization is based on successive matrix multiplications without any non-linearities.
Any deep learning model will surely break this assumption.
He et al. derive another initialization strategy while considering rectified linear units (ReLU) and parametric rectified linear units (PReLU) (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;He et al. 2015&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://spell.ml/blog/lr-schedulers-and-adaptive-optimizers-YHmwMhAAACYADm6F&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://spell.ml/blog/lr-schedulers-and-adaptive-optimizers-YHmwMhAAACYADm6F&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Duchi, John, Elad Hazan, and Yoram Singer. 2011. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.” &lt;i&gt;Journal of Machine Learning Research&lt;/i&gt; 12 (61): 2121–59. &lt;a href=&#34;http://jmlr.org/papers/v12/duchi11a.html&#34;&gt;http://jmlr.org/papers/v12/duchi11a.html&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Glorot, Xavier, and Yoshua Bengio. n.d. “Understanding the Difﬁculty of Training Deep Feedforward Neural Networks,” 8.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_3&#34;&gt;&lt;/a&gt;He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.” &lt;i&gt;Arxiv:1502.01852 [Cs]&lt;/i&gt;, February. &lt;a href=&#34;http://arxiv.org/abs/1502.01852&#34;&gt;http://arxiv.org/abs/1502.01852&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_4&#34;&gt;&lt;/a&gt;Kingma, Diederik P., and Jimmy Ba. 2017. “Adam: A Method for Stochastic Optimization.” &lt;i&gt;Arxiv:1412.6980 [Cs]&lt;/i&gt;, January. &lt;a href=&#34;http://arxiv.org/abs/1412.6980&#34;&gt;http://arxiv.org/abs/1412.6980&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_5&#34;&gt;&lt;/a&gt;Zhao, Yang, Hao Zhang, and Xiuyuan Hu. n.d. “Penalizing Gradient Norm for Efﬁciently Improving Generalization in Deep Learning,” 11.&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Convolutional Neural Networks</title>
      <link>https://ajdillhoff.github.io/notes/convolutional_neural_networks/</link>
      <pubDate>Sat, 02 Apr 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/convolutional_neural_networks/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#convolution-operator&#34;&gt;Convolution Operator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#properties-of-convolutions&#34;&gt;Properties of Convolutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parameter-sharing&#34;&gt;Parameter Sharing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pooling&#34;&gt;Pooling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backwards-pass&#34;&gt;Backwards Pass&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example&#34;&gt;Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#neural-networks-for-image-classification&#34;&gt;Neural Networks for Image Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#useful-resources&#34;&gt;Useful Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;&lt;strong&gt;Key Concepts&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Invariance and Equivariance&lt;/li&gt;
&lt;li&gt;Definition&lt;/li&gt;
&lt;li&gt;Padding, Stride, Kernel size, dilation&lt;/li&gt;
&lt;li&gt;Purpose of multiple feature maps&lt;/li&gt;
&lt;li&gt;Receptive fields and hierarchies of features&lt;/li&gt;
&lt;li&gt;Downsampling, Upsampling, Examples in research&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Dense neural networks made up of linear layers and a chosen activation function are not practical for image data. Consider an image of size \(224\times224\times3\). The first layer of a dense network would require a \(150,528\times n\) parameter matrix, where \(n\) is the number of nodes in the first layer. It is common to build dense networks where the first layer has more nodes than input features. In this case, we would need a minimum of \(150,528^2\) parameters in the first layer. Even if we chose something much smaller like \(n=1024\), this would require \(154,140,672\) parameters for just the first layer. This is clearly impractical.&lt;/p&gt;
&lt;p&gt;Aside from requiring a large number of parameters, we might ask whether it is beneficial to feed raw pixel values into a dense network. The network itself would be learning pixel-wise features with no regard to their spatial relationship. This makes our network&amp;rsquo;s job much more difficult because the spatial arrangement of features tells us so much about what we see. In practice, this means that the network would have to learn the same features at every location in the image. We would instead prefer this network to learn features that are &lt;strong&gt;invariant&lt;/strong&gt; to translation. That is, the network should learn features that are the same regardless of where they appear in the image.&lt;/p&gt;
&lt;p&gt;Invariance to translation is very convenient and can save our network a lot of work in learning the same feature at every point in the input. It is also desirable that our network is invariant to other transformations such as rotation, scaling, skewing, and warping. Formally, a function \(f(\mathbf{x})\) of an image \(\mathbf{x}\) is invariant to a transformation \(t(\mathbf{x})\) if&lt;/p&gt;
&lt;p&gt;\[
f(t(\mathbf{x})) = f(\mathbf{x}).
\]&lt;/p&gt;
&lt;p&gt;Aside from invariance, some models should be &lt;strong&gt;equivariant&lt;/strong&gt; to certain transformations. That is, the output of the model should change in the same way as the input. Image segmentation models should be equivariant to translation. If we were to shift an image by a few pixels, the output segmentation mask should also shift by the same amount. Convolutional neural networks are equivariant to &lt;em&gt;translation&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;convolution-operator&#34;&gt;Convolution Operator&lt;/h2&gt;
&lt;p&gt;A convolution is a function that takes two functions as input and produces a third function as output. The first function is the input and the second function is the kernel. The output is called the feature map. The kernel is also sometimes called the filter.&lt;/p&gt;
&lt;p&gt;\[
(f * g)(t) = \int f(t-a)g(a)da
\]&lt;/p&gt;
&lt;p&gt;We can view them more concretely by considering the functions to be vectors. For example, let the function \(f\) be an input vector \(x\) and \(w\) be a kernel representing a filter. The convolution operator is then&lt;/p&gt;
&lt;p&gt;\[
(x * w)(t) = \int x(t-a)w(a)da.
\]&lt;/p&gt;
&lt;p&gt;The result the &lt;strong&gt;feature map&lt;/strong&gt; representing the response of the kernel at each location in the input.&lt;/p&gt;
&lt;p&gt;In the case of discrete values, the operator is written as&lt;/p&gt;
&lt;p&gt;\[
(x * w)(t) = \sum_{a}x(t-a)w(a).
\]&lt;/p&gt;
&lt;p&gt;In machine learning, the kernel \(w\) is usually represented by some set of parameters that is optimized.&lt;/p&gt;
&lt;p&gt;CNNs for images use a 2D convolution defined as&lt;/p&gt;
&lt;p&gt;\[
(I * K)(i, j) = \sum_m \sum_n I(i-m, j-n)K(m, n).
\]&lt;/p&gt;
&lt;p&gt;In this formulation, the kernel is effectively flipped across the vertical and horizontal axis.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-05_19-09-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;2D Convolution (Image Credit: Song Ho Ahn (linked above)).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;2D Convolution (Image Credit: Song Ho Ahn (linked above)).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In practice, most deep learning APIs implement &lt;strong&gt;cross-correlation&lt;/strong&gt;.
Whether the function is implemented as true convolution makes no difference when it comes to optimizing a deep model since filter weights that are produced with cross-correlation would be produced, albeit flipped, with convolution.&lt;/p&gt;
&lt;p&gt;\[
(K * I)(i, j) = \sum_m \sum_n I(i+m, j+n)K(m, n).
\]&lt;/p&gt;
&lt;h2 id=&#34;properties-of-convolutions&#34;&gt;Properties of Convolutions&lt;/h2&gt;
&lt;p&gt;Convolutional networks are commonly built on &lt;em&gt;full&lt;/em&gt; or &lt;em&gt;valid&lt;/em&gt; convolutions. Other variants have also been explored. Here, we will briefly discuss the different properties of this operator. A more detailed treatment can be found in (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;padding&#34;&gt;Padding&lt;/h3&gt;
&lt;p&gt;By definition, a convolution of an input with a filter of size \(n\times n\) will produce an output of size \((m-n+1)\times(m-n+1)\), where \(m\) is the size of the input. This means that the output will be smaller than the input. This is often referred to as a &lt;strong&gt;valid&lt;/strong&gt; convolution. The figure below shows a convolution between a \(3\times3\) kernel and a \(5\times5\) input.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-26_16-31-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A valid convolution (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A valid convolution (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The output of this convolution is a \(3\times3\) feature map. This is a problem if we want to build a deep network. Each convolution will reduce the size of the input. If we were to stack multiple convolutional layers, the output would eventually be too small to be useful. If we want our output to be same size as the input, we can add padding to the original input image before convolving it. This is often known as a &lt;strong&gt;full&lt;/strong&gt; convolution. An example is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-26_16-34-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;A full convolution (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;A full convolution (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;stride&#34;&gt;Stride&lt;/h3&gt;
&lt;p&gt;So far, we have only looked at convolutions which step by 1 unit as they shift over the image. We can control the size of this step, or &lt;strong&gt;stride&lt;/strong&gt;, to produce different outcomes. Picking a non-unit stride has a number of effects on the features that are learned in a convolutional neural network.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dimensionality reduction&lt;/strong&gt;: Skipping over pixels reduces the size of the output feature map. This provides another way of downsampling the input.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Less computation&lt;/strong&gt;: Fewer computations are required to produce the output feature map.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Increased field of view&lt;/strong&gt;: A larger stride increases the field of view of the kernel, leading to larger receptive fields in deeper layers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given an input of size \(m\times m\) and a kernel of size \(n\times n\), the output size of a convolution with stride \(s\) is given by&lt;/p&gt;
&lt;p&gt;\[
\left\lfloor\frac{m-n}{s}\right\rfloor + 1.
\]&lt;/p&gt;
&lt;p&gt;The figure below shows a convolution with stride 2 on a \(5\times5\) input.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-26_16-45-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;A convolution with stride 2 (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;A convolution with stride 2 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;kernel-size&#34;&gt;Kernel Size&lt;/h3&gt;
&lt;p&gt;The size of the kernel has a large impact on the features that are learned. A larger kernel will have a larger receptive field. This means that the kernel will be able to capture more information about the input. However, this comes at the cost of increased computation. Common kernel sizes in most CNNs are \(3\times3\), \(5\times5\), and \(7\times7\). It is also convenient to pick an odd kernel size so that the kernel has a center pixel.&lt;/p&gt;
&lt;h3 id=&#34;dilation&#34;&gt;Dilation&lt;/h3&gt;
&lt;p&gt;Around 2015, a research trend for CNNs was to find a way to increase the receptive field without adding more parameters. The result is a &lt;strong&gt;dilated&lt;/strong&gt; convolution. The output of a dilated convolution is computed by skipping over pixels in the input. The figure below shows a \(3\times3\) kernel with a dilation of 2.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-27_08-19-10_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;A dilated convolution (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;A dilated convolution (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The output size is computed as&lt;/p&gt;
&lt;p&gt;\[
\left\lfloor\frac{m + 2p - n - (n-1)(d-1)}{s}\right\rfloor + 1,
\]&lt;/p&gt;
&lt;p&gt;where \(p\) is the amount of padding and \(d\) is the dilation factor.&lt;/p&gt;
&lt;h2 id=&#34;parameter-sharing&#34;&gt;Parameter Sharing&lt;/h2&gt;
&lt;p&gt;In a densely connected layer, each input has a corresponding weight attached to it.
For example, we ran a few &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/tree/main/deep_learning&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;introductory experiments&lt;/a&gt; on the CIFAR10 dataset using a deep, densely connected network.
To reduce the amount of parameters in the first layer, we converted each image to grayscale.
The input also had to be vectorized in order to be processed.
With a processed image of size \(32 \times 32\), this resulted in a \(1024\) dimensional vector for each input.
Our first layer had \(512\) nodes resulting in a parameter matrix of size \(1024 \times 512\).&lt;/p&gt;
&lt;p&gt;Convolution layers have &lt;strong&gt;shared parameters&lt;/strong&gt;, meaning the same parameters are used for each region on the input.
A single channel 2D filter of size \(n \times n\) only requires \(n \times n\) parameters.
Each kernel is applied to every location in the original input using the same parameters.&lt;/p&gt;
&lt;p&gt;Kernels are &lt;strong&gt;equivariant&lt;/strong&gt; to translation because of their shared parameters.
That is, as the input changes, the output will change in the same way.
Formally, two functions \(f\) and \(g\) are equivarient if&lt;/p&gt;
&lt;p&gt;\[
f(g(x)) = g(f(x)).
\]&lt;/p&gt;
&lt;p&gt;In the context of image features, a kernel applied across an image will produce strong responses in regions that exhibit the same local features.
For example, a kernel that detects horizontal lines will produce strong responses across all parts of the image that show a large contrast between vertical pixels.&lt;/p&gt;
&lt;h2 id=&#34;pooling&#34;&gt;Pooling&lt;/h2&gt;
&lt;p&gt;When a convolution is applied to some input image, the resulting output feature map represents the responses of the kernel applied to each location in the image.
If this original image were to be shifted by a few pixels, the reponses would also be shifted.
In order to increase the robustness of a model to small perturbations such as translation, a pooling layer was historically employed after each non-linear activation following a convolutional layer.&lt;/p&gt;
&lt;p&gt;They effectively provide a summary statistic of a local region by selecting the average or maximum responses in a small window. This provides translation invariance since the maximum response will be the same for a region even if it is translated by a small amount.
It also acts as a quick way to downsample the image, leading to fewer parameters in the model.&lt;/p&gt;
&lt;p&gt;Modern works do not employ pooling operations as often. For example (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;He et al. 2016&lt;/a&gt;) perform dimensionality reduction with \(1 \times 1\) convolutions.
(&lt;a href=&#34;#citeproc_bib_item_6&#34;&gt;Springenberg et al. 2015&lt;/a&gt;) argue that fully convolutional networks can achieve the same performance without max pooling.&lt;/p&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster.&amp;rdquo; - Geoffrey Hinton&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;h2 id=&#34;backwards-pass&#34;&gt;Backwards Pass&lt;/h2&gt;
&lt;p&gt;The parameters of a convolutional layer are updated via backpropagation like any other layer with trainable parameters.
Given a kernel \(w\), it is necessary to compute \(\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}}\), where \(w_{m&amp;rsquo;, n&amp;rsquo;}\) is the \((m&amp;rsquo;, n&amp;rsquo;)th\) entry of the kernel.
This entry affects all entries in the feature map, so \(\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}}\) will sum over all such entries.&lt;/p&gt;
&lt;p&gt;To show the gradient calculation, we will assume a convolutional layer with zero padding and unit stride with a square \(2 \times 2\) kernel applied to a square \(3 \times 3\) input.
The output map is then \((3 - 2 + 1) \times (3 - 2 + 1) = 2 \times 2\).&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}} = \sum_{i=0}^2 \sum_{j=0}^2 \frac{\partial \mathcal{L}}{\partial x_{i, j}} \frac{\partial x_{i,j}}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}}
\]&lt;/p&gt;
&lt;p&gt;If \(\mathbf{z}^{(l-1)}\) is the output from the previous layer, then&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{\partial x_{i, j}}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}} &amp;amp;= \frac{\partial}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}} \sum_{m} \sum_{n} w_{m, n} z_{i+m, j+n}^{(l-1)} + b\\
&amp;amp;= \frac{\partial}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}} w_{m&amp;rsquo;, n&amp;rsquo;}z_{i+m&amp;rsquo;, j+n&amp;rsquo;}^{(l-1)}\\
&amp;amp;= z_{i+m&amp;rsquo;, j+n&amp;rsquo;}^{(l-1)}
\end{align*}&lt;/p&gt;
&lt;p&gt;Then \(\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}}\) becomes&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}} &amp;amp;= \sum_{i=0}^2 \sum_{j=0}^2 \frac{\partial \mathcal{L}}{\partial x_{i, j}} z_{i+m&amp;rsquo;, j+n&amp;rsquo;}^{(l-1)}\\
&amp;amp;= \frac{\partial \mathcal{L}}{\partial x_{i, j}} * z_{m&amp;rsquo;, n&amp;rsquo;}^{(l-1)}.
\end{align*}&lt;/p&gt;
&lt;p&gt;\(\frac{\partial \mathcal{L}}{\partial x_{i, j}}\) represent the gradients with respect to the feature maps. To match the flipped kernel used in the forward pass, they are flipped in an opposite manner.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s train and evaluate a convolutional neural network on the OG network: LeNet5 (&lt;a href=&#34;#citeproc_bib_item_4&#34;&gt;LeCun et al. 1989&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;neural-networks-for-image-classification&#34;&gt;Neural Networks for Image Classification&lt;/h2&gt;
&lt;h3 id=&#34;ilsvrc&#34;&gt;ILSVRC&lt;/h3&gt;
&lt;p&gt;The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is the most popular image classification and object detection challenge starting in 2010. It now exists as the ILSVRC 2012-2017 challenge on &lt;a href=&#34;https://www.kaggle.com/c/imagenet-object-localization-challenge/overview/description&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Kaggle&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;alexnet&#34;&gt;AlexNet&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://code.google.com/archive/p/cuda-convnet/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://code.google.com/archive/p/cuda-convnet/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The network that arguably popuarlized deep learning by achieving a 37.5% top-1 and 17% top-5 error rate on the ILSVRC-2010 test set. This model performed significantly better than leading competitors (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Krizhevsky, Sutskever, and Hinton 2017&lt;/a&gt;).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-12_18-25-11_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;ILSVRC-2010 results reported by Krizhevsky et al.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;ILSVRC-2010 results reported by Krizhevsky et al.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This performance was based on many different insights and techniques including ReLU activations and dropout.
The authors stated in their original publication that the large capacity of the model is necessary to fully describe the diversity of objects in ImageNet.&lt;/p&gt;
&lt;h4 id=&#34;architecture&#34;&gt;Architecture&lt;/h4&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-12_18-35-38_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;AlexNet architecture (from Krizhevsky et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;AlexNet architecture (from Krizhevsky et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;AlexNet is made up of 5 convolutional layers followed by 3 fully-connected layers.
The outputs of the last layer are used as input to the softmax function.&lt;/p&gt;
&lt;p&gt;Each layer uses the ReLU activation function.&lt;/p&gt;
&lt;p&gt;\[
f(x) = \max(0, x)
\]&lt;/p&gt;
&lt;p&gt;The justification for switching to ReLU as opposed to sigmoid or tanh is the faster training times.
Experiments on smaller CNNs show that networks with ReLU reach 25% training error on CIFAR-10 six times faster than those with tanh activations.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-12_18-49-42_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Training loss over time using ReLU (solid) versus tanh (dotted) (from Krizhevsky et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Training loss over time using ReLU (solid) versus tanh (dotted) (from Krizhevsky et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Another benefit of ReLU activations is that they are less reliant on input normalization.
In a saturating activation function like tanh, large absolute values in the inputs will be clamped to either -1 or 1.
ReLU is unbounded above 0.
Networks can still train as long as some input is positive.
Local response normalization (LRN) is used after the first and second convolutional layers.&lt;/p&gt;
&lt;p&gt;The motivation behind LRN is taken from &lt;em&gt;lateral inhibition&lt;/em&gt; in neurobiology.
An overly excited neuron (one with a high response) can subdue or dampen the responses from local neighbors.
If all responses in a local region are uniformly large, which can happend since ReLU is unbounded, it will dampen them all.&lt;/p&gt;
&lt;p&gt;In practice, they showed that applying LRNs to their model reduced the top-1 and top-5 error rates by 1.4% and 1.2%, respectively.&lt;/p&gt;
&lt;h4 id=&#34;regularization&#34;&gt;Regularization&lt;/h4&gt;
&lt;p&gt;The entire network has 60 million parameters.
Even with so many parameters and training on a dataset with over 8 million images, their model overfits the training data quickly without the aid of regularization.
They employ both image translations and horizontal reflections.&lt;/p&gt;
&lt;p&gt;The use of translations is where the popular \(224 \times 224\) training size originated.
The original size of the images in the dataset is \(256 \times 256\).
To work with random translations without worrying about padding, they crop the final output to \(224 \times 224\).
The final output of the network extracts 5 \(224 \times 224\) patches from the test input and averages the network prediction made on each patch.&lt;/p&gt;
&lt;p&gt;Additionally, they alter the RGB intensities so that the network is less reliant on specific intensities and illumination for each object.
The intuition is that the identity of an object is invariant to lighting conditions.&lt;/p&gt;
&lt;p&gt;As a last form of regularization, they employ dropout in the first two fully-connected layers.&lt;/p&gt;
&lt;h4 id=&#34;training&#34;&gt;Training&lt;/h4&gt;
&lt;p&gt;They trained their model on a training set of 1.2 million images using two NVIDIA GTX 580 3GB GPUs.
They had to write their own optimized CUDA code for this since deep learning frameworks such as Tensorflow and PyTorch did not exist yet.
The training took ~6 days to pass 90 epochs.&lt;/p&gt;
&lt;h3 id=&#34;vgg&#34;&gt;VGG&lt;/h3&gt;
&lt;p&gt;Published in 2015, (&lt;a href=&#34;#citeproc_bib_item_5&#34;&gt;Simonyan and Zisserman 2015&lt;/a&gt;) explore how depth plays a role in convolutional neural networks.
They systematically increase the depth of the network while keep other hyperparameters fixed.
The filter sizes are also kept at \(3 \times 3\).&lt;/p&gt;
&lt;p&gt;Similar to (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Krizhevsky, Sutskever, and Hinton 2017&lt;/a&gt;), they use ReLU activations and in only one of their models to they employ Local Response Normalization.
They found that adding LRN to their model did not increase performance.
Instead, it only increased computation time and memory consumption.
Their models are summarized in the table below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-13_07-48-12_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Model configurations used (Simonyan and Zisserman).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Model configurations used (Simonyan and Zisserman).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The number of parameters for each network is 133 million, 133 million, 134 million, 138 million, and 144 million starting from A to E.&lt;/p&gt;
&lt;h3 id=&#34;googlenet&#34;&gt;GoogLeNet&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_14-18-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;The network-in-network architecture pairs perfectly with the Inception meme.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;The network-in-network architecture pairs perfectly with the Inception meme.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Proposed a 22-layer network architecture that has \(12 \times\) fewer parameters than (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Krizhevsky, Sutskever, and Hinton 2017&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The authors were already thinking about applications in mobile computing, where hardware limitations would require smaller networks that still perform well.&lt;/p&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;In this paper, we will focus on an efficient deep neural network architecture for computer vision, codenamed Inception, which derives its name from the Network in network paper by Lin et al. in conjunction with the famous “we need to go deeper” internet meme.&amp;rdquo; - Szegedy et al.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;It was apparent at the time that building larger networks would generally lead to better performance.
Adding more parameters leads to easier overfitting.
Bigger networks also mean more computation. If the goal is to adapt high quality networks into mobile computing, solutions would have to include more sophistication than simply adding more components.&lt;/p&gt;
&lt;h4 id=&#34;hebbian-learning&#34;&gt;Hebbian Learning&lt;/h4&gt;
&lt;p&gt;A linear increase in filters leads to a quadratic increase in computation.
If most filter parameters end up being close to 0, then this increase in model capacity is wasted.
One solution is to include sparsity in the network instead of having dense connections.
Szegedy et al. were motivated by the work of Arora et al., which they summarized as follows.&lt;/p&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Their main result states that if the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs.&amp;rdquo; - Szegedy et al.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;This result relates with &lt;a href=&#34;https://en.wikipedia.org/wiki/Hebbian_theory&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Hebbian theory&lt;/a&gt; on synaptic plasticity which is summarized as &amp;ldquo;neurons that fire together, wire together.&amp;rdquo;&lt;/p&gt;
&lt;h4 id=&#34;from-theory-to-architecture&#34;&gt;From Theory to Architecture&lt;/h4&gt;
&lt;p&gt;Motivated by sparse connections, the architecture is designed to approximate sparsity given current dense components like convolutional layers.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_14-52-12_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 11: &amp;lt;/span&amp;gt;Naive version of the Inception module (Szegedy et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;Naive version of the Inception module (Szegedy et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The Inception module design as seen above is motivated as follows.
In layers closer to the raw input, filters would be grouped into local regions.
In this case, a \(1 \times 1\) convolution would summarize these groups.&lt;/p&gt;
&lt;p&gt;For clusters that are spread out, a larger filter would be needed to cover the larger regions.
This motivates the use of \(3 \times 3\) and \(5 \times 5\) filters.&lt;/p&gt;
&lt;p&gt;The choice to include a max pooling function in each module is based on previous successes of using max pooling.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-01-46_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 12: &amp;lt;/span&amp;gt;Description of layers from Szegedy et al.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 12: &lt;/span&gt;Description of layers from Szegedy et al.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;vanishing-gradients&#34;&gt;Vanishing Gradients&lt;/h4&gt;
&lt;p&gt;Creating a deeper network means that training is more susceptible to the vanishing gradient problem.
They noted that shallower networks that perform well on image classification would surely provide strong disciminative features.
They leverage this idea by computing 2 additional intermediate outputs: one in the middle of the network and an additional output 3 layers beyond that one.
This permits the gradients to be strengthened by intermediate losses when combined with the original gradients.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-07-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 13: &amp;lt;/span&amp;gt;GoogLeNet model (Szegedy et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 13: &lt;/span&gt;GoogLeNet model (Szegedy et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;results&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;GoogLeNet took 1st place in the 2014 ILSVRC with a 6.67% top-5 error rate.&lt;/p&gt;
&lt;h3 id=&#34;resnet&#34;&gt;ResNet&lt;/h3&gt;
&lt;p&gt;By 2016, it was clear that deeper models could build a richer hierarchy of features leading to better performance on a wide range of computer vision tasks.
However, with deeper networks comes the vanishing gradient problem.
Training them remained difficult for a time, but initialization and other normalization techniques found ways to resolve this issue.&lt;/p&gt;
&lt;p&gt;With deeper networks, a new problem appeared.
Adding more layers generally results in higher accuracy.
At a certain point, adding additional layers leads to a decrease in accuracy.
Many experiments ruled out the possibility of overfitting by observing that the training error was increasing as well.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-19-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 14: &amp;lt;/span&amp;gt;Result of experiments showing that decreased accuracy was not a result of overfitting.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 14: &lt;/span&gt;Result of experiments showing that decreased accuracy was not a result of overfitting.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;identity-mappings&#34;&gt;Identity Mappings&lt;/h4&gt;
&lt;p&gt;Consider a shallow network with some measure performance on a task.
If we were to add additional layers to make this network deeper, but those layers were simply identity mappings, then we should expect an error no greater than the original shallow network.
However, current solvers are unable to find such a solution in a reasonable amount of time on an equally deep network optimized from a random initialization.&lt;/p&gt;
&lt;h4 id=&#34;residual-functions&#34;&gt;Residual Functions&lt;/h4&gt;
&lt;p&gt;The main idea of this paper is to attempt to learn a residual function \(\mathcal{F}(\mathbf{x}) := \mathcal{H}(\mathbf{x}) - \mathbf{x}\) of the desired mapping \(\mathcal{H}(\mathbf{x})\) rather than attempting to learn the mapping directly.
The desired mapping then given by \(\mathcal{H}(\mathbf{x}) = \mathcal{F}(\mathbf{x}) + \mathbf{x}\).
If it were optimal to learn an identity mapping, the idea is that it would be simpler to learn by moving towards a 0 residual.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-45-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 15: &amp;lt;/span&amp;gt;Residual unit (He et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 15: &lt;/span&gt;Residual unit (He et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The function can be implemented into neural networks by using skip connections, as seen in the figure above.
Adding these identity mappings does not require any additional parameters, as the input is simply passed to the end of the stack.&lt;/p&gt;
&lt;h4 id=&#34;architecture-complexity&#34;&gt;Architecture Complexity&lt;/h4&gt;
&lt;p&gt;They compare a 34-layer plain network based on the VGG-19 architecture with a 34-layer residual network.
They note that VGG-19 has more filters and higher complexity than their residual network.
Specifically, VGG-19 requires 19.6 billion FLOPs compared to only 3.6 billion for their 34-layer residual network.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-49-39_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 16: &amp;lt;/span&amp;gt;Comparison of architectures and their complexity (He et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 16: &lt;/span&gt;Comparison of architectures and their complexity (He et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;results&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;They evaluate how well the residual networks generalize when adding more layers.
As mentioned in the introduction, typical models would see an increase in training error as the number of layers were increased.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-53-18_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 17: &amp;lt;/span&amp;gt;Training comparisons between plain and residual networks (He et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 17: &lt;/span&gt;Training comparisons between plain and residual networks (He et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Their ensemble of models achieved 3.57% top-5 error on the ImageNet test set, achieving 1st place in the ILSVRC 2015 classification challenge.
It additionally was adapted to other challenges and won first place on ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in both the ILSVRC and COCO 2015 competitions.&lt;/p&gt;
&lt;h2 id=&#34;useful-resources&#34;&gt;Useful Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.songho.ca/dsp/convolution/convolution.html#convolution_2d&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.songho.ca/dsp/convolution/convolution.html#convolution_2d&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vdumoulin/conv_arithmetic&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://github.com/vdumoulin/conv_arithmetic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cs231n.github.io/convolutional-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://cs231n.github.io/convolutional-networks/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://grzegorzgwardys.wordpress.com/2016/04/22/8/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://grzegorzgwardys.wordpress.com/2016/04/22/8/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Dumoulin, Vincent, and Francesco Visin. 2018. “A Guide to Convolution Arithmetic for Deep Learning.” &lt;i&gt;Arxiv:1603.07285 [Cs, Stat]&lt;/i&gt;, January. &lt;a href=&#34;http://arxiv.org/abs/1603.07285&#34;&gt;http://arxiv.org/abs/1603.07285&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “Deep Residual Learning for Image Recognition.” In &lt;i&gt;2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/i&gt;, 770–78. Las Vegas, NV, USA: IEEE. &lt;a href=&#34;https://doi.org/10.1109/CVPR.2016.90&#34;&gt;https://doi.org/10.1109/CVPR.2016.90&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_3&#34;&gt;&lt;/a&gt;Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. “ImageNet Classification with Deep Convolutional Neural Networks.” &lt;i&gt;Communications of the Acm&lt;/i&gt; 60 (6): 84–90. &lt;a href=&#34;https://doi.org/10.1145/3065386&#34;&gt;https://doi.org/10.1145/3065386&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_4&#34;&gt;&lt;/a&gt;LeCun, Yann, Bernhard Boser, John Denker, Donnie Henderson, R. Howard, Wayne Hubbard, and Lawrence Jackel. 1989. “Handwritten Digit Recognition with a Back-Propagation Network.” In &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;. Vol. 2. Morgan-Kaufmann. &lt;a href=&#34;https://papers.nips.cc/paper/1989/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html&#34;&gt;https://papers.nips.cc/paper/1989/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_5&#34;&gt;&lt;/a&gt;Simonyan, Karen, and Andrew Zisserman. 2015. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” &lt;i&gt;Arxiv:1409.1556 [Cs]&lt;/i&gt;, April. &lt;a href=&#34;http://arxiv.org/abs/1409.1556&#34;&gt;http://arxiv.org/abs/1409.1556&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_6&#34;&gt;&lt;/a&gt;Springenberg, Jost Tobias, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. 2015. “Striving for Simplicity: The All Convolutional Net.” &lt;i&gt;Arxiv:1412.6806 [Cs]&lt;/i&gt;, April. &lt;a href=&#34;http://arxiv.org/abs/1412.6806&#34;&gt;http://arxiv.org/abs/1412.6806&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning</title>
      <link>https://ajdillhoff.github.io/notes/deep_learning/</link>
      <pubDate>Tue, 29 Mar 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/deep_learning/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-makes-a-model-deep&#34;&gt;What makes a model deep?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deep-networks&#34;&gt;Deep Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deep-vs-dot-shallow-networks&#34;&gt;Deep vs. Shallow Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#high-dimensional-structured-data&#34;&gt;High Dimensional Structured Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#activation-functions&#34;&gt;Activation Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#loss-functions&#34;&gt;Loss Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-typical-training-pipeline&#34;&gt;A Typical Training Pipeline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#useful-links&#34;&gt;Useful Links&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Deep learning is a term that you&amp;rsquo;ve probably heard of a million times by now in different contexts. It is an umbrella term that encompasses techniques for computer vision, bioinformatics, natural language processing, and much more. It almost always involves a neural network of some kind that was trained on a large corpus of data.&lt;/p&gt;
&lt;p&gt;The existence of the word &amp;ldquo;deep&amp;rdquo; implies a contrast to &amp;ldquo;shallow&amp;rdquo; learning. Some definition define a deep network as an artificial neural network with more than 1 layer. Another definition is that a deep model will include a hierarchy of features that are learned from the data. These features are learned as part of the optimization process as opposed to being manually engineered as is required in other machine learning techniques.&lt;/p&gt;
&lt;p&gt;If you are not yet familiar with &lt;a href=&#34;https://ajdillhoff.github.io/notes/neural_networks/&#34;&gt;neural networks&lt;/a&gt;, follow the link to learn about their basics as they are the foundation of deep learning systems.&lt;/p&gt;
&lt;p&gt;We will cover how to implement an array of deep learning models for different tasks.
Different layers and activation functions will be explored as well as the effect of regularization.
There will also be a focus on best practices for organizing a machine learning project.&lt;/p&gt;
&lt;h2 id=&#34;what-makes-a-model-deep&#34;&gt;What makes a model deep?&lt;/h2&gt;
&lt;p&gt;We begin by comparing &lt;em&gt;shallow&lt;/em&gt; networks with &lt;em&gt;deep&lt;/em&gt; networks.
What defines a deep network? Is it as simple as crossing a threshold into \(n\) layers?
As evidenced by (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Zeiler and Fergus 2013&lt;/a&gt;) deeper networks allow for a more robust hierarchy of image features.&lt;/p&gt;
&lt;p&gt;There is work by (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Montúfar et al. 2014&lt;/a&gt;) which suggests that shallow networks require an exponential amount of nodes as compared to deeper networks.
Additionally, there are many individual results which suggest that deeper networks provide better task generalization.&lt;/p&gt;
&lt;p&gt;As we will later see when studying Convolutional Neural Networks, the optimization of such deep networks produces features that maximize the performance of a task. That is, the network is not only optimizing the overall performance of task, but it produces features from the data that may be useful in other contexts.
This is particularly useful for transfer learning, where large pre-trained models can be used as starting points for novel tasks.
The benefit being that a complete retraining of the model is not necessary.&lt;/p&gt;
&lt;h2 id=&#34;deep-networks&#34;&gt;Deep Networks&lt;/h2&gt;
&lt;p&gt;Like &lt;a href=&#34;https://ajdillhoff.github.io/notes/neural_networks/&#34;&gt;neural networks&lt;/a&gt;, deep networks are defined by the number of layers, nodes per layer, activation functions, and loss functions.
We now review the forward and backward pass, providing more insight into the structure and usage of deep networks along the way.&lt;/p&gt;
&lt;p&gt;Consider a deep network with \(L\) layers. Layer \(l\) has \(n_{l-1}\) input connections and \(n_l\) output nodes and activation function \(g^{(l)}\).
The final output is evaluated with some ground truth using a loss function \(\mathcal{L}\).&lt;/p&gt;
&lt;h3 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h3&gt;
&lt;p&gt;\begin{align*}
\mathbf{a}^{(l)} &amp;amp;= W^{(l)}\mathbf{z}^{(l-1)} + \mathbf{b}^{(l)}\\
\mathbf{z}^{(l)} &amp;amp;= g^{(l)}(\mathbf{a}^{(l)})\\
\end{align*}&lt;/p&gt;
&lt;p&gt;This is repeated from the input to the last layer.
For the first layer \(l=1\), the input \(\mathbf{z}^{(0)} = \mathbf{x}\).
In practice, the output \(\mathbf{a}^{(l)}\) is cached since it is required for the backward pass.
This prevents the values from needing to be computed twice.&lt;/p&gt;
&lt;p&gt;It is also worth it to study the sizes of the matrices while performing a forward pass.
For a layer \(l\), \(W^{(l)} \in \mathbb{R}^{n_l \times n_{l-1}}\) and the input \(\mathbf{z}^{(l-1)} \in \mathbb{R}^{n_{l-1} \times 1}\).
When training, it is common to perform batch gradient descent with batches of input of size \(B\).
Then, \(\mathbf{z}^{(l-1)} \in \mathbb{R}^{n_{l-1}\times B}\) and \(\mathbf{a}^{(l)}, \mathbf{b}^{(l)} \in \mathbb{R}^{n_l \times B}\).&lt;/p&gt;
&lt;h3 id=&#34;backward-pass&#34;&gt;Backward Pass&lt;/h3&gt;
&lt;p&gt;During the backward pass, the gradient is propagated from the last layer to the first.
Each layer that contains trainable parameters must also compute the gradient of the network output with respect to the weights and biases.
This can be done in a modular way, as shown next.&lt;/p&gt;
&lt;p&gt;Consider the last layer. The gradients with respect to the weights and biases are&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{d\mathcal{L}}{dW^{(L)}} &amp;amp;= \frac{d\mathcal{L}}{d\mathbf{z}^{(L)}} \frac{d\mathbf{z}^{(L)}}{d\mathbf{a}^{(L)}} \frac{d\mathbf{a}^{(L)}}{dW^{(L)}}\\
\frac{d\mathcal{L}}{d\mathbf{b}^{(L)}} &amp;amp;= \frac{d\mathcal{L}}{d\mathbf{z}^{(L)}} \frac{d\mathbf{z}^{(L)}}{d\mathbf{a}^{(L)}} \frac{d\mathbf{a}^{(L)}}{d\mathbf{b}^{(L)}}.
\end{align*}&lt;/p&gt;
&lt;p&gt;To see how the gradient continues to be propagated backward, compute the same thing for layer \(L-1\)&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{d\mathcal{L}}{dW^{(L-1)}} &amp;amp;= \frac{d\mathcal{L}}{d\mathbf{z}^{(L)}} \frac{d\mathbf{z}^{(L)}}{d\mathbf{a}^{(L)}} \frac{d\mathbf{a}^{(L)}}{d\mathbf{z}^{(L-1)}} \frac{d\mathbf{z}^{(L-1)}}{d\mathbf{a}^{(L-1)}} \frac{d\mathbf{a}^{(L-1)}}{dW^{(L-1)}}\\
\frac{d\mathcal{L}}{d\mathbf{b}^{(L-1)}} &amp;amp;= \frac{d\mathcal{L}}{d\mathbf{z}^{(L)}} \frac{d\mathbf{z}^{(L)}}{d\mathbf{a}^{(L)}} \frac{d\mathbf{a}^{(L)}}{d\mathbf{z}^{(L-1)}} \frac{d\mathbf{z}^{(L-1)}}{d\mathbf{a}^{(L-1)}} \frac{d\mathbf{a}^{(L-1)}}{d\mathbf{b}^{(L-1)}}.
\end{align*}&lt;/p&gt;
&lt;p&gt;As seen above, to continue propagating the gradient backward, each layer \(l\) must also compute&lt;/p&gt;
&lt;p&gt;\[
\frac{d\mathbf{a}^{(l)}}{d\mathbf{z}^{(l-1)}}.
\]&lt;/p&gt;
&lt;p&gt;To summarize, every layer with trainable parameters will compute&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{d\mathcal{L}}{dW^{(l)}} = \frac{d\mathbf{a}^{(l+1)}}{d\mathbf{z}^{(l)}} \frac{d\mathbf{z}^{(l)}}{d\mathbf{a}^{(l)}} \frac{d\mathbf{a}^{(l)}}{dW^{(l)}}\\
\frac{d\mathcal{L}}{d\mathbf{b}^{(l)}} = \frac{d\mathbf{a}^{(l+1)}}{d\mathbf{z}^{(l)}} \frac{d\mathbf{z}^{(l)}}{d\mathbf{a}^{(l)}} \frac{d\mathbf{a}^{(l)}}{d\mathbf{b}^{(l)}}.
\end{align*}&lt;/p&gt;
&lt;p&gt;The term \(\frac{d\mathbf{a}^{(l+1)}}{d\mathbf{z}^{(l)}}\) is the gradient that is propagated from layer \(l+1\).&lt;/p&gt;
&lt;h2 id=&#34;deep-vs-dot-shallow-networks&#34;&gt;Deep vs. Shallow Networks&lt;/h2&gt;
&lt;p&gt;As mentioned above, a shallow network can approximate any continuous function to arbitrary precision. If a deep network can represent the composition of two shallow networks, then it can also approximate any continuous function to arbitrary precision. Then why are deep networks better than shallow networks when both can approximate any function? There are a few compelling reasons as to why, starting with the &lt;strong&gt;capacity&lt;/strong&gt; of the network and the number of linear regions it can represent per parameter.&lt;/p&gt;
&lt;p&gt;As discussed in &lt;em&gt;Understanding Deep Learning&lt;/em&gt; (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Prince 2023&lt;/a&gt;), a shallow network with 1 input, 1 output, and \(D &amp;gt; 2\) hidden units can create up to \(D + 1\) linear regions using \(3D+1\) parameters. The \(3D + 1\) comes from the fact that the hidden layer requires \(D\) parameters for the weights with an extra \(D\) parameters for the bias terms. To convert from the hidden layer to the output layer, there are \(D\) parameters for the weights and 1 parameter for the bias term. The figure below shows the maximum number of linear regions as a function of the number of parameters for networks that map one input to one output.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-22_21-30-14_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Maximum number of linear regions as a function of the number of parameters for networks that map one input to one output (&amp;lt;a href=&amp;#34;#citeproc_bib_item_2&amp;#34;&amp;gt;Prince 2023&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Maximum number of linear regions as a function of the number of parameters for networks that map one input to one output (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Prince 2023&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;high-dimensional-structured-data&#34;&gt;High Dimensional Structured Data&lt;/h2&gt;
&lt;p&gt;For high dimensional structured data, such as images, deep networks are able to learn a hierarchy of features that are useful for the task at hand while requiring a significantly smaller number of parameters than a shallow network. Consider a \(100\times100\) image used as input to a shallow network with 1 hidden layer. This would require \(10,001\) parameters to represent the weights and biases. If we instead use a deep network with with convolutional layers, we can use significantly fewer parameters. We will see this more closely when we study &lt;a href=&#34;https://ajdillhoff.github.io/notes/convolutional_neural_networks/&#34;&gt;Convolutional Neural Networks&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;activation-functions&#34;&gt;Activation Functions&lt;/h2&gt;
&lt;h3 id=&#34;sigmoid&#34;&gt;Sigmoid&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\[
\sigma(\mathbf{x}) = \frac{1}{1 + \exp(-\mathbf{x})}
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Derivative&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\[
\sigma(\mathbf{x})(1 - \sigma(\mathbf{x}))
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-31_10-04-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Sigmoid non-linearity (Wikipedia)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Sigmoid non-linearity (Wikipedia)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;loss-functions&#34;&gt;Loss Functions&lt;/h2&gt;
&lt;p&gt;Loss functions are used to evaluate the performance of a model. In the context of gradient descent, their gradient with respect to the model parameters is used to update the parameters. Loss functions can be constructed using maximum likelihood estimation over a probability distribution or by using a distance metric between the model output and the ground truth. The table below from (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Prince 2023&lt;/a&gt;) shows some common loss functions and their use cases.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Data Type&lt;/th&gt;
&lt;th&gt;Domain&lt;/th&gt;
&lt;th&gt;Distribution&lt;/th&gt;
&lt;th&gt;Use&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Univariate, continuous, unbounded&lt;/td&gt;
&lt;td&gt;\(\mathbb{R}\)&lt;/td&gt;
&lt;td&gt;univariate normal&lt;/td&gt;
&lt;td&gt;Regression&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Univariate, discrete, binary&lt;/td&gt;
&lt;td&gt;\(\{0, 1\}\)&lt;/td&gt;
&lt;td&gt;Bernoulli&lt;/td&gt;
&lt;td&gt;Binary Classification&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Univariate, discrete, bounded&lt;/td&gt;
&lt;td&gt;\(\{0, 1\}^K\)&lt;/td&gt;
&lt;td&gt;Multinoulli&lt;/td&gt;
&lt;td&gt;Multiclass Classification&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;a-typical-training-pipeline&#34;&gt;A Typical Training Pipeline&lt;/h2&gt;
&lt;p&gt;When training and evaluating models, especially on benchmark datasets, it is important to properly test their generalization performance.
This test is crucial when comparing the efficacy of your ideas versus baseline evaluations or competing methods.&lt;/p&gt;
&lt;p&gt;To ensure that your model is evaluated in a fair way, it is common to set aside a set of test data that is only used during the final comparison.
This data is typically annotated so that some metric can be used.&lt;/p&gt;
&lt;p&gt;It is true that the training data drives the parameter tuning during optimization.
This is most commonly done with gradient descent.
However, we will also change the hyperparamers such as learning rate, batch size, and data augmentation.
In this case, we want to evaluate the relative performance of each change.&lt;/p&gt;
&lt;p&gt;If we use the test set to do this, then we are necessarily using the test set for training.
Our biases and intuitions about the model&amp;rsquo;s performance would be implicitly influenced by that set.
To track our relative changes without using the test set, we can take a portion of the original training set and label it as our &lt;strong&gt;validation set&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The split between training, validation, and test data is relatively small.
Most modern datasets are large, with millions of samples.
Consider &lt;a href=&#34;https://www.image-net.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;ImageNet&lt;/a&gt;, an image classification dataset with over 14 million samples.
Taking 10,000 samples to serve as a validation set is only \(~.07\%\) of the dataset.&lt;/p&gt;
&lt;p&gt;Most modern machine learning frameworks have an easy way to split the dataset.
We can do this in PyTorch using &lt;code&gt;torch.utils.data.random_split&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;train_dataset, val_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;utils&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random_split(dataset, [train_size, val_size])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;useful-links&#34;&gt;Useful Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=D_jt-xO_RmI&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Deep learning Bootcamp: Kaiming He (author of ResNet)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Montúfar, Guido, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. 2014. “On the Number of Linear Regions of Deep Neural Networks.” &lt;i&gt;Arxiv:1402.1869 [Cs, Stat]&lt;/i&gt;, June. &lt;a href=&#34;http://arxiv.org/abs/1402.1869&#34;&gt;http://arxiv.org/abs/1402.1869&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Prince, Simon J.D. 2023. &lt;i&gt;Understanding Deep Learning&lt;/i&gt;. MIT Press. &lt;a href=&#34;https://udlbook.github.io/udlbook/&#34;&gt;https://udlbook.github.io/udlbook/&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_3&#34;&gt;&lt;/a&gt;Zeiler, Matthew D., and Rob Fergus. 2013. “Visualizing and Understanding Convolutional Networks.” &lt;i&gt;Arxiv:1311.2901 [Cs]&lt;/i&gt;, November. &lt;a href=&#34;http://arxiv.org/abs/1311.2901&#34;&gt;http://arxiv.org/abs/1311.2901&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Boosting</title>
      <link>https://ajdillhoff.github.io/notes/boosting/</link>
      <pubDate>Wed, 23 Mar 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/boosting/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adaboost&#34;&gt;AdaBoost&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Combining predictions from multiple sources is usually preferred to a single source.
For example, a medical diagnosis would carry much more weight if it was the result of a consensus of several experts.
This idea of prediction by consensus is a powerful way to improve classification and regression models.
In fact, good performance of a committee of models can be achieved even if each individual model is conceptually very simple.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Boosting&lt;/strong&gt; is one such way of building a committee of models for classification or regression and is popularly implemented by an algorithm called &lt;strong&gt;AdaBoost&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;adaboost&#34;&gt;AdaBoost&lt;/h2&gt;
&lt;p&gt;Given a dataset \(\{\mathbf{x}_i\}\) and target variables \(\{\mathbf{y}_i\}\), AdaBoost first initializes a set of weights corresponding to each data sample as \(w_i = \frac{1}{N}\).
At each step of the algorithm, a simple classifier, called a &lt;strong&gt;weak learner&lt;/strong&gt; is fit to the data.
The weights for each sample are adjusted based on the individual classifier&amp;rsquo;s performance.
If the sample was misclassified, the relative weight for that sample is increased.
After all classifiers have been fit, they are combined to form an ensemble model.&lt;/p&gt;
&lt;h3 id=&#34;the-algorithm&#34;&gt;The Algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Initialize data weights \({w_i}\) as \(w_i^{(1)} = \frac{1}{n}\) for \(i = 1, \dots, n\).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fit each weak learner \(j\) to the training data by minimizing the misclassification cost:&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^n w_i^{(j)} \mathbb{1}(f_j(\mathbf{x}_i) \neq \mathbf{y}_i)
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute a weighted error rate&lt;/p&gt;
&lt;p&gt;\[
\epsilon_j = \frac{\sum_{i=1}^n w_i^{(j)} \mathbb{1}(f_j(\mathbf{x}_i) \neq \mathbf{y}_i)}{\sum_{i=1}^n w_i^{(j)}}
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the weighted error rate to compute a weight for each classifier such that misclassified samples are given higher weight:&lt;/p&gt;
&lt;p&gt;\[
\alpha_j = \ln \bigg\{\frac{1 - \epsilon_j}{\epsilon_j}\bigg\}.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update the data weights for the next model in the sequence:&lt;/p&gt;
&lt;p&gt;\[
w_i^{j+1} = w_i^{j} \exp\{\alpha_j \mathbb{1}(f_j(\mathbf{x}_i \neq \mathbf{y}_i)\}.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once all weak learners are trained, the final model predictions are given by&lt;/p&gt;
&lt;p&gt;\[
Y_M(\mathbf{x}) = \text{sign} \Bigg(\sum_{j=1}^M \alpha_j f_j(\mathbf{x})\Bigg).
\]&lt;/p&gt;
&lt;h3 id=&#34;weak-learners&#34;&gt;Weak Learners&lt;/h3&gt;
&lt;p&gt;The weak learners can be any classification or regression model.
However, they are typically chosen to be very simple to account for training time.
For example, a complex deep learning model would be a poor choice for a weak learner.&lt;/p&gt;
&lt;p&gt;One example of a weak learner is a simple linear model like a &lt;a href=&#34;https://ajdillhoff.github.io/notes/perceptron/&#34;&gt;Perceptron&lt;/a&gt; or decision stump.
A standard implementation of AdaBoost uses a decision tree with depth 1, as observed in &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html?highlight=boost#sklearn.ensemble.AdaBoostClassifier&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;sklearn&amp;rsquo;s implementation.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s put this together and walk through the first few steps of training an AdaBoost model using a decision stump as the weak learner. We will use a very simple dataset to keep the values easy to compute by hand.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Initial Data&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Weak Learner 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first learner is trained on the initial data and picks \(x_1 = 2.5\) as the split threshold.
Input where \(x_1 \leq 2.5\) is assigned to class 0 and all other samples are assigned class 1.
The data with this learner&amp;rsquo;s predictions are shown below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;th&gt;prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Error and weight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The error is simple enough to compute as all samples are currently weighted equally. Since two of the samples were misclassified, the error is the sum of their weights.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Total error&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(e_1 = 0.2 + 0.2 = 0.4\).&lt;/p&gt;
&lt;p&gt;The weight of the classifier can then be computed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classifier weight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(\alpha_1 = \frac{1}{2} \ln \big(\frac{1 - e_1}{e_1}\big) = 0.2027\).&lt;/p&gt;
&lt;p&gt;The weights of our data can now be updated using this value of \(\alpha_1\).
The weight of each example is updated by multiplying each correctly classifed sample by \(\exp\{-\alpha_1\}\) and each incorrectly classified sample by \(\exp\{\alpha\}\):&lt;/p&gt;
&lt;p&gt;\[
w_i^{j+1} = w_i^{j} \exp\{\alpha_j \mathbb{1}(f_j(\mathbf{x}_i \neq \mathbf{y}_i)\}.
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; You will notice that the equation above is different from the actual update rule that was applied to the weights in this example. In the original publication &lt;strong&gt;(TODO: reference Fruend)&lt;/strong&gt;, the weights are renormalized at the end of the loop. In this example, the normalization is combined with the update. In either case, the updated weights are shown below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Weak Learner 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The algorithm now moves to the next weak learner, which classifies the data given a threshold of \(x_1 = 3.5\). Its predictions are shown below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;th&gt;prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.250&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.250&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Only a single sample is misclassified, and the error is computed as before.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Total error&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(e_2 = 0.250\)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classifier weight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(\alpha_2 = \frac{1}{2} \ln \big(\frac{1 - e_2}{e_2}\big) = 0.5493\)&lt;/p&gt;
&lt;p&gt;The weights are updated for each sample, yielding the following data:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.500&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The second sample has been misclassified twice at this point, leading to a relatively high weight. This will hopefully be addressed by the third learner.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Weak Learner 3&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The final weak learner splits the data on \(x_2 = 6.5\), yielding the following output for each sample.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;th&gt;prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.500&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Unfortunately, sample 2 is too tricky for any of our weak learners. The total error is shown below. Since this is a binary classification problem, the error suggests that our weak learner performs worse than random guessing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Total error&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(e_3 = 0.667\)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classifier weight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(\alpha_3 = \frac{1}{2} \ln \big(\frac{1 - e_3}{e_3}\big) = -0.3473\)&lt;/p&gt;
&lt;p&gt;The negative value of the classifier weight suggests that its predictions will be reversed when evaluated. The updated weights of each data sample are given below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.375&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.125&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Final Classifier&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The final classifier is a weighted vote of the three weak learners, with the weights being the classifier weights we calculated (0.2027, 0.5493, and -0.3473). The negative weight means that the third learner&amp;rsquo;s predictions are reversed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stereo Vision</title>
      <link>https://ajdillhoff.github.io/notes/stereo_vision/</link>
      <pubDate>Wed, 23 Mar 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/stereo_vision/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#epipolar-geometry&#34;&gt;Epipolar Geometry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#calibration-with-known-intrinsic-parameters-and-world-points&#34;&gt;Calibration with Known Intrinsic Parameters and World Points&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#estimating-depth&#34;&gt;Estimating Depth&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Binocular vision permits depth perception.
It is an important part of many tasks such as robotic vision, pose estimation, and scene understanding.
The goal of steropsis is to reconstruct a 3D representation of the world given correspondences between two or more cameras.&lt;/p&gt;
&lt;p&gt;The process of computing these correspondences assumes two or more cameras with known intrinsic and extrinsic parameters.
Methods exist to estimate the required transformation parameters using points based on matching image features.
If some set of points which a fixed coordinate system is known, such as a calibration pattern, the problem becomes even simpler.
Knowing the exact world point as it is projected in all image planes is essentially a ground truth.&lt;/p&gt;
&lt;h2 id=&#34;epipolar-geometry&#34;&gt;Epipolar Geometry&lt;/h2&gt;
&lt;p&gt;Epipolar geometry is the backbone of stereopsis.
We will first define what epipolar geometry is, how it is used in the stereo vision problem, and the core constraint that limits our search space of point correspondences.
It is defined visually below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-24_20-23-56_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Overview of epipolar geometry for stereopsis (Source: Szeliski).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Overview of epipolar geometry for stereopsis (Source: Szeliski).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Consider the point \(\mathbf{p}\) whose projection is \(\mathbf{x}_0\) with respect to camera \(\mathbf{c}_0\) and \(\mathbf{x}_1\) with respect to camera \(\mathbf{c}_1\).
All 5 of these points lie on the &lt;strong&gt;epipolar plane&lt;/strong&gt;.
Additionally, points \(\mathbf{e}_0\) and \(\mathbf{e}_1\) lie on the line defined by \(\mathbf{c}_0\) and \(\mathbf{c}_1\) as it intersects the image plane of each camera, respectively.
These are called the &lt;strong&gt;epipoles&lt;/strong&gt;.
These are also the projections of the other camera centers.&lt;/p&gt;
&lt;p&gt;Fundamental to establishing the correspondences between the two cameras is the &lt;strong&gt;epipolar constraint&lt;/strong&gt;.
If \(\mathbf{x}_0\) and \(\mathbf{x}_1\) represent projections of the same point, then \(\mathbf{x}_1\) must like on the epipolar line \(l_1\) associated with \(\mathbf{x}_0\) and vice versa.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-24_21-07-55_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;The epipolar constraint restricts the search space for matching correspondences. (Source: Szeliski)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;The epipolar constraint restricts the search space for matching correspondences. (Source: Szeliski)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;As seen in the figure above, potentional matches for \(\mathbf{x}_0\) must lie on the epipolar line defined by \(\mathbf{e}_1\) and \(\mathbf{x}_1\).&lt;/p&gt;
&lt;p&gt;Our goal is to compute translation \(\mathbf{t}\) and rotation \(R\).
To find these, we start by mathematically defining the epipolar constraint.
The epipolar plane is defined by the lines \(\overrightarrow{\mathbf{c}_0 \mathbf{p}}\), \(\overrightarrow{\mathbf{c}_1 \mathbf{p}}\), and \(\overrightarrow{\mathbf{c}_0 \mathbf{c}_1}\).
This can be written as&lt;/p&gt;
&lt;p&gt;\[
\overrightarrow{\mathbf{c}_0 \mathbf{p}} \cdot [\overrightarrow{\mathbf{c}_0 \mathbf{c}_1} \times \overrightarrow{\mathbf{c}_1 \mathbf{p}}] = 0.
\]&lt;/p&gt;
&lt;p&gt;We do not know \(\mathbf{p}\), but we do have \(\mathbf{x}_0\) and \(\mathbf{x}_1\), the projections of \(\mathbf{p}\) onto the image plane of each respective camera.
Assuming that both camera are calibrated, we have a set of known intrinsic matrices \(\mathbf{K}_j\).&lt;/p&gt;
&lt;p&gt;Although we do not have the exact \(z\) value of the point, we do know the point with respect to the camera calculcated as&lt;/p&gt;
&lt;p&gt;\[
\mathbf{p}_0 = d_0 \hat{\mathbf{x}}_0,
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\hat{\mathbf{x}}_0 = \mathbf{K}_0^{-1} \mathbf{x}_0.
\]&lt;/p&gt;
&lt;p&gt;The relationship between points \(\mathbf{p}_0\) and \(\mathbf{p}_1\) is&lt;/p&gt;
&lt;p&gt;\[
d_1 \hat{\mathbf{x}}_1 = R(d_0 \hat{\mathbf{x}}_0) + \mathbf{t},
\]&lt;/p&gt;
&lt;p&gt;where \(R\) is a rotation matrix and \(\mathbf{t}\) is an offset vector.
These are the parameters that are solved from stereo calibration.&lt;/p&gt;
&lt;p&gt;Since the vectors \(\hat{\mathbf{x}}_0\), \(\hat{\mathbf{x}}_1\), and \(\mathbf{t}\) are coplanar, the plane can be represented by a normal vector.
That is, the vector that is orthogonal to all points in the plane.
Such a vector can be calculated by taking the cross product of both sides of the above equation with \(\mathbf{t}\):&lt;/p&gt;
&lt;p&gt;\[
d_1 [\mathbf{t}]_{\times} \hat{\mathbf{x}}_1 = d_0 [\mathbf{t}]_{\times} R \hat{\mathbf{x}}_0,
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
[\mathbf{t}]_{\times}=\begin{bmatrix}
0 &amp;amp; -t_z &amp;amp; t_y\\
t_z &amp;amp; 0 &amp;amp; -t_x\\
-t_y &amp;amp; t_x &amp;amp; 0
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;It is true that, since the normal vector is orthogonal to \(\hat{\mathbf{x}}_0\), \(\hat{\mathbf{x}}_1\), and \(\mathbf{t}\), taking the dot product of any of these vectors and the normal vector yields 0:&lt;/p&gt;
&lt;p&gt;\[
d_1 \hat{\mathbf{x}}_1^T [\mathbf{t}]_{\times} \hat{\mathbf{x}}_1 = d_0 \hat{\mathbf{x}}_1^T [\mathbf{t}]_{\times} R \hat{\mathbf{x}}_0 = 0.
\]&lt;/p&gt;
&lt;p&gt;We have now established the epipolar constraint in terms of the rotation matrix \(R\) and translation \(\mathbf{t}\).
These are the parameters that relate the points between both cameras.
This constraint is more compactly written as&lt;/p&gt;
&lt;p&gt;\[
\hat{\mathbf{x}}_1^T E \hat{\mathbf{x}}_0 = 0,
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
E = [\mathbf{t}_{\times}]R.
\]&lt;/p&gt;
&lt;p&gt;\(E\) is called the &lt;strong&gt;essential matrix&lt;/strong&gt; which relates the projected points between the two cameras.
Once we have the essential matrix, we can compute \(\mathbf{t}\) and \(R\).&lt;/p&gt;
&lt;h2 id=&#34;calibration-with-known-intrinsic-parameters-and-world-points&#34;&gt;Calibration with Known Intrinsic Parameters and World Points&lt;/h2&gt;
&lt;p&gt;We first look at the simplest case of stereo calibration.
The intrinsic parameters of both cameras have been computed using a standard calibration technique.
Additionally, we have a fixed calibration pattern used to establish a correspondence between
the world points and the points with respect to each camera.&lt;/p&gt;
&lt;p&gt;Given \(N\) measured correspondences \(\{(\mathbf{x}_{i0}, \mathbf{x}_{i1})\}\), we can form a linear system with equations of the form&lt;/p&gt;
&lt;p&gt;\begin{alignat*}{3}
x_{i0}x_{i1}e_{00} &amp;amp; {}+{} &amp;amp; y_{i0}x_{i1}e_{01} &amp;amp; {}+{} &amp;amp; x_{i1}e_{02} &amp;amp; {}+{} &amp;amp;\\
x_{i0}y_{i1}e_{10} &amp;amp; {}+{} &amp;amp; y_{i0}y_{i1}e_{11} &amp;amp; {}+{} &amp;amp; y_{i1}e_{12} &amp;amp; {}+{} &amp;amp;\\
x_{i0}e_{20} &amp;amp; {}+{} &amp;amp; y_{i0}e_{21} &amp;amp; {}+{} &amp;amp; e_{22} &amp;amp; {}={} &amp;amp; 0
\end{alignat*}&lt;/p&gt;
&lt;p&gt;Given at least 8 equations corresponding to the 8 unknowns of \(E\), we can use SVD to solve for \(E\).&lt;/p&gt;
&lt;p&gt;\[
E = [\mathbf{t}]_{\times}R = \mathbf{U \Sigma V^T} = \begin{bmatrix}
\mathbf{u}_0 &amp;amp; \mathbf{u}_1 &amp;amp; \mathbf{t}
\end{bmatrix}\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 1\\
\end{bmatrix}\begin{bmatrix}
\mathbf{v}_0^T\\
\mathbf{v}_1^T\\
\mathbf{v}_2^T\\
\end{bmatrix}
\]&lt;/p&gt;
&lt;h2 id=&#34;estimating-depth&#34;&gt;Estimating Depth&lt;/h2&gt;
&lt;p&gt;Given the intrinsic parameters and parameters relating both calibrated cameras, we can estimate the depth of a point that is seen by both cameras.&lt;/p&gt;
&lt;p&gt;We know that \(\mathbf{x}_0 = K_0 \hat{\mathbf{x}}_0\) and \(\mathbf{x}_1 = K_1 \hat{\mathbf{x}}_1\).
With the stereo calibration complete, we also know \(A = [R\quad \mathbf{t}]\) and&lt;/p&gt;
&lt;p&gt;\[
\hat{\mathbf{x}}_0 = A\hat{\mathbf{x}}_1.
\]&lt;/p&gt;
&lt;p&gt;Plugging into the projection equation for the first camera yields&lt;/p&gt;
&lt;p&gt;\[
\mathbf{x}_0 = K_0 A\hat{\mathbf{x}}_1.
\]&lt;/p&gt;
&lt;p&gt;Our knowns are \(\mathbf{x}_0, \mathbf{x}_1, K_0, K_1, \text{ and } A\).
The only unknown is \(\hat{\mathbf{x}}_1\).&lt;/p&gt;
&lt;p&gt;We are left with 2 equations&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{x}_0 &amp;amp;= K_0 A\hat{\mathbf{x}}_1\\
\mathbf{x}_1 &amp;amp;= K_1 \hat{\mathbf{x}}_1.
\end{align*}&lt;/p&gt;
&lt;p&gt;If we let \(P = K_0 A\), we write the \(\mathbf{x}_0 = \begin{bmatrix}u_1\\v_1\\1\end{bmatrix}\) and&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\begin{bmatrix}
u_1\\
v_1\\
1
\end{bmatrix}=\begin{bmatrix}
p_{11} &amp;amp; p_{12} &amp;amp; p_{13} &amp;amp; p_{14}\\
p_{21} &amp;amp; p_{22} &amp;amp; p_{23} &amp;amp; p_{24}\\
p_{31} &amp;amp; p_{32} &amp;amp; p_{33} &amp;amp; p_{34}\\
\end{bmatrix}\begin{bmatrix}
X\\
Y\\
Z\\
W
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;This gives two equations for \(x_1\) and \(y_1\), the measured 2D feature locations:&lt;/p&gt;
&lt;p&gt;\begin{align*}
x_1 &amp;amp;= \frac{p_{11}X + p_{12}Y + p_{13}Z + p_{14}W}{p_{31}X + p_{32}Y + p_{33}Z + p_{34}W}\\
y_1 &amp;amp;= \frac{p_{21}X + p_{22}Y + p_{23}Z + p_{24}W}{p_{31}X + p_{32}Y + p_{33}Z + p_{34}W}.
\end{align*}&lt;/p&gt;
&lt;p&gt;Multiplying both equations by the denominator yields a set of equations that can be solved via linear least squares or SVD.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Decision Trees</title>
      <link>https://ajdillhoff.github.io/notes/decision_trees/</link>
      <pubDate>Fri, 18 Mar 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/decision_trees/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-iris-dataset&#34;&gt;Example: Iris Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#growing-a-tree&#34;&gt;Growing a Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#examining-the-iris-classification-tree&#34;&gt;Examining the Iris Classification Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pruning-a-tree&#34;&gt;Pruning a Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-algorithm&#34;&gt;The Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/dmilla/introduction-to-decision-trees-titanic-dataset&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.kaggle.com/dmilla/introduction-to-decision-trees-titanic-dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;decision tree&lt;/strong&gt;, or Classification and Regression Trees (CART), is a model that recursively partitions the input space based on a collection of features.
The partitions are split based on very simple binary choices.
If yes, branch to the left; if no, branch to the right.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-22_11-11-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Regression tree (left) and its piecewise constant surface (right) (Source: _Machine Learning: A Probabilistic Perspective_ by Kevin P. Murphy).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Regression tree (left) and its piecewise constant surface (right) (Source: &lt;em&gt;Machine Learning: A Probabilistic Perspective&lt;/em&gt; by Kevin P. Murphy).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To compute the response, we represent each individual decision as a function \(\phi\) and sum the responses:&lt;/p&gt;
&lt;p&gt;\[
f(\mathbf{x}) = \mathbb{E}[y | \mathbf{x}] = \sum_{m=1}^M w_m \mathbb{1} (\mathbf{x} \in R_m) = \sum_{m=1}^M w_m \phi(\mathbf{x};\mathbf{v}_m),
\]&lt;/p&gt;
&lt;p&gt;where \(R_m\) is the \(m^{\text{th}}\) region, \(w_m\) is the mean response, and \(\mathbf{v}_m\) is the choice of variable to split on along with its threshold value. Note that this is &lt;strong&gt;not&lt;/strong&gt; a differentiable function due to the indicator function.&lt;/p&gt;
&lt;h2 id=&#34;example-iris-dataset&#34;&gt;Example: Iris Dataset&lt;/h2&gt;
&lt;p&gt;To see this on real data, consider the Iris flower dataset.
For example, we will look at a decision tree model that classifies each flower into either &lt;strong&gt;setosa&lt;/strong&gt;, &lt;strong&gt;versicolor&lt;/strong&gt;, or &lt;strong&gt;virginica&lt;/strong&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-20_21-04-19_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Our initial Iris classifier.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Our initial Iris classifier.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;We are given data about a new iris and want to classify it using this tree.
Our sample has a sepal length of 6.1cm, a sepal width of 2.8cm, a petal length of 4.7cm, and a petal width of 1.2cm.
The first decision considers the petal width.
Our sample has a width of 1.2, so it continues down the right branch.&lt;/p&gt;
&lt;p&gt;The second decision consideres petal width again.
Since our sample does not have a width greater than 1.75, we continue down the left branch.&lt;/p&gt;
&lt;p&gt;At this point, the model was optimized to now consider the petal length.
Our length comes in at 4.7, just shy of going down the right path.&lt;/p&gt;
&lt;p&gt;We now arrive at the last decision.
Since our petal length is not greater than 1.65, the model classifies this sample as &lt;strong&gt;versicolor&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;growing-a-tree&#34;&gt;Growing a Tree&lt;/h2&gt;
&lt;p&gt;To grow a tree, a decision needs to be made as to whether or not the current set of data can be split based on some feature.
As such, there should be a reliable way of determining if a feature provided a good split.
This is evaluated using a cost function and selecting the feature and value corresponding to the minimum cost:&lt;/p&gt;
&lt;p&gt;\[
(j^*, t^*) = \text{arg} \min_{j\in\{1, \dots, D\}} \min_{t \in \mathcal{T}_j} \text{cost}(\{\mathbf{x}_i, y_i : x_{ij} \leq t\}) + \text{cost}(\{\mathbf{x}_i, y_i : x_{ij} &amp;gt; t\}).
\]&lt;/p&gt;
&lt;p&gt;In words, this function finds a value \(t\) such that groups the data with the lowest cost.
For a regression task, the cost function is typically defined as&lt;/p&gt;
&lt;p&gt;\[
\text{cost}(\mathcal{D}) = \sum_{i \in \mathcal{D}}(y_i - \bar{y})^2,
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\bar{y} = \frac{1}{|\mathcal{D}|}\sum_{i \in \mathcal{D}} y_i.
\]&lt;/p&gt;
&lt;p&gt;Splits that result in clusters with high variance may still see a higher cost, even though they are the minimum.&lt;/p&gt;
&lt;p&gt;As their alternative name implies, decision trees can also be used for classification.
The splits are still based on features and threshold values at each branch.
When a split is considered, a class-conditional probability is estimated for that data.&lt;/p&gt;
&lt;h3 id=&#34;splitting-the-data&#34;&gt;Splitting the Data&lt;/h3&gt;
&lt;p&gt;Given a set of data that makes it to node \(i\), denoted \(\mathcal{D}_i\), a fitting procedure must select the feature and threshold value that minimizes the cost. If the feature is continuous, the range of search values is selected by sorting a list of unique values from the subset of data. For each unique value, the cost is computed by splitting the data into two groups based on the threshold value. The threshold value that minimizes the cost is selected.&lt;/p&gt;
&lt;p&gt;In the case of categorical data, we would intuitively split the data into a set of data that contains the category and a set that does not. The cost is then computed for each category. The category that minimizes the cost is selected.&lt;/p&gt;
&lt;p&gt;Given data satisfying \(X_j &amp;lt; t\), the class-conditional probability is&lt;/p&gt;
&lt;p&gt;\[
\hat{\pi}_c = \frac{1}{|\mathcal{D}|}\sum_{i \in \mathcal{D}} \mathbb{1}(y_i = c).
\]&lt;/p&gt;
&lt;h3 id=&#34;error-functions&#34;&gt;Error Functions&lt;/h3&gt;
&lt;p&gt;The common error functions used for classification are &lt;strong&gt;misclassification rate&lt;/strong&gt;, &lt;strong&gt;entropy&lt;/strong&gt;, and &lt;strong&gt;Gini index&lt;/strong&gt;.
Misclassification rate is computed by summing the number of misclassifications:&lt;/p&gt;
&lt;p&gt;\[
\frac{1}{|\mathcal{D}|} \sum_{i \in \mathcal{D}} \mathbb{1}(y_i \neq \hat{y}) = 1 - \hat{\pi}_{\hat{y}}.
\]&lt;/p&gt;
&lt;p&gt;Entropy is computed as&lt;/p&gt;
&lt;p&gt;\[
\mathbb{H}(\mathbb{\hat{\pi}}) = -\sum_{c=1}^C \hat{\pi}_c \log \hat{\pi}_c.
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gini index&lt;/strong&gt; computes the expected error rate.&lt;/p&gt;
&lt;p&gt;\[
G = \sum_{c=1}^C \hat{\pi}_c (1 - \hat{\pi}_c) = 1 - \sum_{c=1}^C \hat{\pi}_c^2
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-19_17-32-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Impurity measured for binary classification (Source: _Machine Learning: A Probabilistic Perspective_ by Kevin P. Murphy)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Impurity measured for binary classification (Source: &lt;em&gt;Machine Learning: A Probabilistic Perspective&lt;/em&gt; by Kevin P. Murphy)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Like entropy, it promotes an equal number of observations across all classes in a node.
For small values of \(\hat{\pi}\), the error is smaller than that of entropy.
If the dataset is imbalanced, entropy is typically favored as it penalizes imbalanced datasets more than Gini will.
Both will favor splits that result in one node being pure.&lt;/p&gt;
&lt;h3 id=&#34;stopping-growth&#34;&gt;Stopping Growth&lt;/h3&gt;
&lt;p&gt;If left unchecked, the algorithm to grow a tree will continue until the data can no longer be split.
In the trivial case, this will be when every data point represents a leaf node.
In order to prevent overfitting, there are several criteria that are considered.&lt;/p&gt;
&lt;h4 id=&#34;does-the-split-reduce-the-cost-enough&#34;&gt;Does the split reduce the cost enough?&lt;/h4&gt;
&lt;p&gt;It may be ideal to only split the data if the cost is reduced by some acceptable value.
The reduction can be computed by&lt;/p&gt;
&lt;p&gt;\[
\Delta = \text{cost}(\mathcal{D}) - \bigg(\frac{|\mathcal{D}_L|}{|\mathcal{D}|}\text{cost}(\mathcal{D}_L) + \frac{|\mathcal{D}_R|}{|\mathcal{D}|} \text{cost}(\mathcal{D}_R)\bigg).
\]&lt;/p&gt;
&lt;h4 id=&#34;has-the-tree-reached-some-maximum-depth&#34;&gt;Has the tree reached some maximum depth?&lt;/h4&gt;
&lt;p&gt;The depth of a tree is set as a hyperparameter.
Later, when we look at an example, we will use cross validation to select the best depth parameter for our model.&lt;/p&gt;
&lt;h4 id=&#34;is-the-distribution-of-the-split-pure&#34;&gt;Is the distribution of the split &lt;strong&gt;pure&lt;/strong&gt;?&lt;/h4&gt;
&lt;p&gt;If either of the splits is fully made up of data with the same label, there is no need to split it any further.&lt;/p&gt;
&lt;h4 id=&#34;is-the-split-too-small&#34;&gt;Is the split too small?&lt;/h4&gt;
&lt;p&gt;A split that is too small may lead to overfitting.&lt;/p&gt;
&lt;h2 id=&#34;examining-the-iris-classification-tree&#34;&gt;Examining the Iris Classification Tree&lt;/h2&gt;
&lt;p&gt;How exactly does the earlier example model make its decision at each node?
The full tree is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-20_21-11-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;A detailed view of our Iris classifier.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;A detailed view of our Iris classifier.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The first split is visually very simple to intuit.
Using petal width can perfectly split all setosa samples into a single leaf node.&lt;/p&gt;
&lt;h2 id=&#34;pruning-a-tree&#34;&gt;Pruning a Tree&lt;/h2&gt;
&lt;p&gt;Depending on the data, stopping growth based on measuring the relative decrease in error may not result in a model that performs well.
Image a dataset that requires multiple features to provide a sufficient classification.
If only one of the features is considered in isolation, it may provide no decrease in error.
A practical example of this is the XOR problem.
Splitting on \(x_1\) or \(x_2\) in isolation does not provide any indication about the true output.
It is only when \(x_1 \neq x_2\) does the output equal to 1.&lt;/p&gt;
&lt;p&gt;To rectify this, a tree can be grown until it is completely full before &lt;strong&gt;pruning&lt;/strong&gt; the branches
that result in the smallest increase in error.&lt;/p&gt;
&lt;h2 id=&#34;the-algorithm&#34;&gt;The Algorithm&lt;/h2&gt;
&lt;p&gt;The general algorithm is shown in MATLAB below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;% node = fitTree(node, D, depth)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;% Recursive function to learn a decision tree&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;% Returns the index of the current node.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;%&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;%   node  - The node index in obj.Nodes.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;%   D     - Indices to the current data.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;%   depth - Current depth of the tree.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; node = &lt;span style=&#34;color:#a6e22e&#34;&gt;fitTree&lt;/span&gt;(obj, node, D, depth)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;% Determine best split for the data and return the split&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    [j, t, dSplit, classDist] = obj.split(D, obj.Nodes(node).features);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    obj.Nodes(node).prediction = classDist;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    disp(classDist);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;% Use heuristic to determine if node is worth splitting&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; obj.splitNode(depth, classDist) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; true
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;% set the node test, the function that determines the branch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        obj.Nodes(node .test = {j, t};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        newFeatures = obj.Nodes(node).features(obj.Nodes(node).features &lt;span style=&#34;color:#f92672&#34;&gt;~=&lt;/span&gt; j);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;% set the child nodes to the left and right splits&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        obj.Nodes(node).children = zeros(size(dSplit, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        numNewNodes = size(dSplit, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i = &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; : numNewNodes
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            obj.Nodes(&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) = struct(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;prediction&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;test&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#75715e&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;features&amp;#39;&lt;/span&gt;, newFeatures, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;children&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;parent&amp;#39;&lt;/span&gt;, node);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            obj.Nodes(node).children(i)  = obj.fitTree(length(obj.Nodes), dSplit{i}, depth &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Camera Models</title>
      <link>https://ajdillhoff.github.io/notes/camera_models/</link>
      <pubDate>Fri, 11 Mar 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/camera_models/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#reading&#34;&gt;Reading&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#outline&#34;&gt;Outline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pinhole-model&#34;&gt;Pinhole Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#from-world-space-to-image-space&#34;&gt;From World Space to Image Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#camera-parameters&#34;&gt;Camera Parameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#camera-calibration&#34;&gt;Camera Calibration&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;reading&#34;&gt;Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Chapters 1 and 7 (Forsyth and Ponce)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.scratchapixel.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.scratchapixel.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1RMyNQR9jGdJm64FCiuvoyJiL6r3H18jeNIyocIO9sfA/edit#slide=id.p&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://docs.google.com/presentation/d/1RMyNQR9jGdJm64FCiuvoyJiL6r3H18jeNIyocIO9sfA/edit#slide=id.p&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://vision.stanford.edu/teaching/cs131_fall1617/lectures/lecture8_camera_models_cs131_2016.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;http://vision.stanford.edu/teaching/cs131_fall1617/lectures/lecture8_camera_models_cs131_2016.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://vlm1.uta.edu/~athitsos/courses/cse4310_spring2021/lectures/11_geometry.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;http://vlm1.uta.edu/~athitsos/courses/cse4310_spring2021/lectures/11_geometry.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;outline&#34;&gt;Outline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Pinhole model&lt;/li&gt;
&lt;li&gt;Coordinates of a pinhole model&lt;/li&gt;
&lt;li&gt;Perspective projections&lt;/li&gt;
&lt;li&gt;Homogeneous coordinates&lt;/li&gt;
&lt;li&gt;Computer graphics perspective&lt;/li&gt;
&lt;li&gt;Lenses&lt;/li&gt;
&lt;li&gt;Intrinsic and extrensic parameters&lt;/li&gt;
&lt;li&gt;From world to camera to image space&lt;/li&gt;
&lt;li&gt;Camera calibration&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;pinhole-model&#34;&gt;Pinhole Model&lt;/h2&gt;
&lt;p&gt;Imagine piercing a small hole into a plate and placing it in front of a black screen.
The light that enters through the pinhole will show an inverted image against the back plane.
If we place a virtual screen in front of the pinhole plate, we can project the image onto it.
This is the basic idea behind a &lt;strong&gt;pinhole camera model&lt;/strong&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-14_17-02-08_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Pinhole camera model (Forsyth and Ponce).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Pinhole camera model (Forsyth and Ponce).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;virtual-camera-coordinates&#34;&gt;Virtual Camera Coordinates&lt;/h3&gt;
&lt;p&gt;There are two sets of coordinates for each object in the pinhole camera model.
First are the actual coordinates of the visible points as seen from the camera.
For real images, these points are continuous. For virtual images, these are discrete.
To produce an actual image, a set of 3D coordinates are projected onto the image plane.
&lt;strong&gt;These 3D coordinates are with respect to the camera&amp;rsquo;s coordinate system.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We can represent the transformation from camera coordinates to image coordinates with a projection matrix.
There are many properties of the camera that must be considered, such as the lens, resolution, etc.
We will start with a simple projection matrix.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-14_19-06-36_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Derivation of simple perspective projection.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Derivation of simple perspective projection.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the figure above, the camera is located at point \(A\) with its viewing plane defined by \(BC\).
The \(y\) value of the world space coordinate is given by \(DE\).
Notice that there are two right triangles (\(\Delta ABC\) and \(\Delta ADE\)).
As such, the ratio between their sides is constant:&lt;/p&gt;
&lt;p&gt;\[
\frac{BC}{DE} = \frac{AB}{AD}.
\]&lt;/p&gt;
&lt;p&gt;Solving for \(y&amp;rsquo; = BC\) yields \(BC = \frac{AB * DE}{AD}\).
A similar relationship follows for the \(x\) value.
In a more common notation, this is written as&lt;/p&gt;
&lt;p&gt;\begin{align*}
x&amp;rsquo; &amp;amp;= \frac{x}{z}\\
y&amp;rsquo; &amp;amp;= \frac{y}{z}
\end{align*}&lt;/p&gt;
&lt;p&gt;This can be represented as a transformation matrix and point vector by using &lt;strong&gt;homogeneous coordinates&lt;/strong&gt;.
These coordinate are to projective geometry as Cartesian coordinates are to Euclidean geometry.
For a 3D point \(\mathbf{p} = (X, Y, Z)\), its corresponding homogeneous coordinate is \(\mathbf{p}_w = (x, y, z, w)\).
The relationship between the original coordinate and its homogeneous coordinate is given by&lt;/p&gt;
&lt;p&gt;\[
X = \frac{x}{w},\quad Y = \frac{y}{w}, \quad Z = \frac{z}{w}.
\]&lt;/p&gt;
&lt;p&gt;With homogeneous coordinates, we can easily project 3D points to an image plane.
First, let the projection matrix be defined as&lt;/p&gt;
&lt;p&gt;\[
M = \begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0\\
\end{bmatrix}
\]&lt;/p&gt;
&lt;p&gt;and a general homogeneous coordinate is given by&lt;/p&gt;
&lt;p&gt;\[
\mathbf{p} = \begin{bmatrix}
x\\
y\\
z\\
w
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;Then,&lt;/p&gt;
&lt;p&gt;\[
M\mathbf{p} = \begin{bmatrix}
x\\
y\\
z\\
z
\end{bmatrix}
\]&lt;/p&gt;
&lt;p&gt;and the perspective divide is applied to finish the transformation.
That is, divide each component by \(w\) and dropping the last component.&lt;/p&gt;
&lt;p&gt;\[
\mathbf{p}&amp;rsquo; = \begin{bmatrix}
\frac{x}{z}\\
\frac{y}{z}\\
1
\end{bmatrix}
\]&lt;/p&gt;
&lt;h3 id=&#34;physical-camera-coordinates&#34;&gt;Physical Camera Coordinates&lt;/h3&gt;
&lt;p&gt;If we are simulating a physical camera, then the sensor should be &lt;em&gt;behind&lt;/em&gt; the lens.
In that case, the points will be inverted as compared to the virtual model.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-15_12-10-04_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Visualization of the projection equation (Forsyth and Ponce).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Visualization of the projection equation (Forsyth and Ponce).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Remember that this is not taking optics into account.
This is a simple projective model.
The figure below shows the relationship between the virtual image plane and sensor image plane.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-16_12-36-32_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Sensor plane versus virtual plane.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Sensor plane versus virtual plane.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To calculate the points projected to the sensor plane using our simple matrix from above, we only need to negate the \(z\) and \(w\) values:&lt;/p&gt;
&lt;p&gt;\[
M = \begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0\\
\end{bmatrix}
\]&lt;/p&gt;
&lt;p&gt;Then the transformed point becomes&lt;/p&gt;
&lt;p&gt;\[
M\mathbf{p} = \begin{bmatrix}
x\\
y\\
-z\\
-z
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;Applying the perspective divide yields the 3D point \((-\frac{x}{z}, -\frac{y}{z}, 1)\).&lt;/p&gt;
&lt;h4 id=&#34;considering-the-focal-length&#34;&gt;Considering the Focal Length&lt;/h4&gt;
&lt;p&gt;An important property that all cameras have is the focal length \(f\).
That is the distance between the lens and the camera&amp;rsquo;s sensor.
In computer graphics terms, the focal length may also refer to the distance between the virtual image plane and camera.
This is then called the &lt;strong&gt;near plane&lt;/strong&gt; instead.&lt;/p&gt;
&lt;p&gt;For the following examples, we will follow OpenGL&amp;rsquo;s projective matrix in which the \(x\) and \(y\) values are scaled by a relationship between the focal length and image window size:&lt;/p&gt;
&lt;p&gt;\[
\frac{2n}{r - l},
\]&lt;/p&gt;
&lt;p&gt;where \(n\) is the near plane distance (related to \(f\)), \(r\) is the value of the right side of the window, and \(l\) is the value of the left side of the window.
For a square window of size \([-1, 1]\), this reduces to \(n\) (or \(f\)).
Then the projection matrix is given as&lt;/p&gt;
&lt;p&gt;\[
M = \begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; -\frac{1}{f} &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0\\
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why does \(M(3, 3) = -\frac{1}{f}\)?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The projected point should be \(f\) units away from the lens.
Projecting a point using the matrix above yields&lt;/p&gt;
&lt;p&gt;\[
M\mathbf{p} = \begin{bmatrix}
x\\
y\\
-\frac{z}{f}\\
-z
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;Then applying the perspective divide to compute the projected coordinate results in&lt;/p&gt;
&lt;p&gt;\[
\mathbf{p&amp;rsquo;} = \begin{bmatrix}
-\frac{fx}{z}\\
-\frac{fy}{z}\\
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;We first computed the 3D homogeneous coordinate and then applied the relationship to 2D homogeneous coordinates using the resulting \((x, y, z)\) triplet.
Applying the final divide which establishes the relationship between 2D Cartesian and homogeneous coordinates results in \(\mathbf{p&amp;rsquo;}\).&lt;/p&gt;
&lt;p&gt;Be aware that computer graphics implementations may apply this process differently using a matrix that looks something like this&lt;/p&gt;
&lt;p&gt;\[
M = \begin{bmatrix}
f &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; f &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0\\
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;If we apply this to a point in 3D space using homogeneous coordinates, it results in the point&lt;/p&gt;
&lt;p&gt;\[
M\mathbf{p} = \begin{bmatrix}
fx\\
fy\\
-z\\
-z
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;Then the resulting 3D point is&lt;/p&gt;
&lt;p&gt;\[
\mathbf{p&amp;rsquo;} = \begin{bmatrix}
-\frac{fx}{z}\\
-\frac{fy}{z}\\
1\\
\end{bmatrix},
\]&lt;/p&gt;
&lt;p&gt;which gives us the same \(x\) and \(y\) values of the projected point as before.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-16_13-44-25_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;3D points (green) projected onto the virtual plane (red) and sensor plane (blue).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;3D points (green) projected onto the virtual plane (red) and sensor plane (blue).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;from-world-space-to-image-space&#34;&gt;From World Space to Image Space&lt;/h2&gt;
&lt;p&gt;The process of projecting points in camera space onto an image plane is fairly simple, but gets slightly more complicated
when considering each entity from the &lt;em&gt;world&lt;/em&gt; perspective.
Objects in the world appear differently depending on the viewpoint of the camera.
To model this mathematically, we need to be able to describe the points in 3 different spaces:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;World space&lt;/li&gt;
&lt;li&gt;Camera space&lt;/li&gt;
&lt;li&gt;Image space&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To fully describe the possibilities that our camera&amp;rsquo;s perspective can take, we need to define scaling, translation, and rotations.
These allow us to describe the different positions and orientations are camera may be in with respect to the world around it.&lt;/p&gt;
&lt;h3 id=&#34;translation&#34;&gt;Translation&lt;/h3&gt;
&lt;p&gt;Consider the points of a pyramid observed by a virtual camera, as seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-16_16-11-03_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Points of a pyramid as seen from our virtual camera.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Points of a pyramid as seen from our virtual camera.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;If we move the camera away from the points, the will appear smaller (assuming a perspective projection).
This movement is described by a translation with respect to the origin in world space.
In the above figure, the camera is position at the origin.
The figure below shows the camera moved down the $z$-axis by 2 units.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-16_16-14-47_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Translating the camera backwards by 2 units. The project points appear smaller.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Translating the camera backwards by 2 units. The project points appear smaller.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This is described with a matrix as&lt;/p&gt;
&lt;p&gt;\[
T = \begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; -2\\
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1\\
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;In general, a translation in 3D can be described with the following matrix&lt;/p&gt;
&lt;p&gt;\[
T = \begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; T_x\\
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; T_y\\
0 &amp;amp; 0 &amp;amp; 1 &amp;amp; T_z\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1\\
\end{bmatrix}.
\]&lt;/p&gt;
&lt;h3 id=&#34;scaling&#34;&gt;Scaling&lt;/h3&gt;
&lt;p&gt;Projected objects can appear larger or smaller depending on the &lt;strong&gt;focal length&lt;/strong&gt; and &lt;strong&gt;field of view&lt;/strong&gt;.
Recall the perspective projection matrix from the previous section:&lt;/p&gt;
&lt;p&gt;\[
P = \begin{bmatrix}
f &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; f &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0\\
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;The projections we have seen so far are with a focal length of 1.
We can make the objects in the image appear larger by increasing the focal length, as seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-16_16-23-25_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Focal length doubled. The projected image appears larger.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Focal length doubled. The projected image appears larger.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;rotation&#34;&gt;Rotation&lt;/h3&gt;
&lt;p&gt;Finally, we can rotate our camera about the \(x\), \(y\), or \(z\) axis.
Following a &lt;a href=&#34;https://en.wikipedia.org/wiki/Right-hand_rule&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;right-handed coordinate system&lt;/a&gt;, these rotations are given by&lt;/p&gt;
&lt;p&gt;\begin{align*}
R_x(\theta) &amp;amp;= \begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; \cos \theta &amp;amp; -\sin \theta\\
0 &amp;amp; \sin \theta &amp;amp; \cos \theta
\end{bmatrix}\\
R_y(\theta) &amp;amp;= \begin{bmatrix}
\cos \theta &amp;amp; 0 &amp;amp; \sin \theta\\
0 &amp;amp; 1 &amp;amp; 0\\
-\sin \theta &amp;amp; 0 &amp;amp; \cos \theta
\end{bmatrix}\\
R_z(\theta) &amp;amp;= \begin{bmatrix}
\cos \theta &amp;amp; -\sin \theta &amp;amp; 0\\
\sin \theta &amp;amp; \cos \theta &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}\\
\end{align*}&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-16_16-42-17_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Rotating the camera about the y-axis. The points now appear on the right side of the image plane.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Rotating the camera about the y-axis. The points now appear on the right side of the image plane.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;camera-parameters&#34;&gt;Camera Parameters&lt;/h2&gt;
&lt;p&gt;When relating points in the world to those of the image space, two sets of parameters are typically used:
the &lt;strong&gt;intrinsic&lt;/strong&gt; and &lt;strong&gt;extrinsic&lt;/strong&gt; parameters of a camera.
Intrinsic parameters relate the points in the camera&amp;rsquo;s reference frame to those of the image plane.
Extrinsic parameters relate the camera&amp;rsquo;s coordinate system to a world coordinate system as well as describing its position and orientation.&lt;/p&gt;
&lt;h3 id=&#34;intrinsic-parameters&#34;&gt;Intrinsic Parameters&lt;/h3&gt;
&lt;p&gt;Which parameters are required to project a point to the image plane?
The intrinsic parameters are those which convert the points on the physical sensor into a normalized image frame.
We start with a basic projection matrix&lt;/p&gt;
&lt;p&gt;\[
M = \begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0\\
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;A physical sensor may not have square pixels, so there are two additional scale parameters \(k\) and \(l\).
Along with the focal length \(f\), the projected values become&lt;/p&gt;
&lt;p&gt;\begin{align*}
x &amp;amp;= kf\frac{X}{Z} = kf\hat{x}\\
y &amp;amp;= lf\frac{Y}{Z} = lf\hat{y}\\
\end{align*}&lt;/p&gt;
&lt;p&gt;The focal length and scale factors \(k\) and \(l\) are dependent and can be written as \(\alpha = kf\) and \(\beta = lf\).&lt;/p&gt;
&lt;p&gt;The physical sensor has an origin at the corner whereas the normalized image frame has an origin at the center.
To account for this offset, a translation parameter for both \(x\) and \(y\) is necessary.
The pixel conversion is now represented as&lt;/p&gt;
&lt;p&gt;\begin{align*}
x &amp;amp;= \alpha \hat{x} + x_0\\
y &amp;amp;= \beta \hat{y} + y_0\\
\end{align*}&lt;/p&gt;
&lt;p&gt;Finally, the sensor may not be perfectly center.
There may also be additional defects that cause the sensor to be warped or skewed.
This is error that can be accounted for by a rotation.&lt;/p&gt;
&lt;p&gt;\begin{align*}
x &amp;amp;= \alpha \hat{x} - \alpha \cot \theta \hat{y} + x_0\\
y &amp;amp;= \frac{\beta}{\sin \theta} \hat{y} + y_0\\
\end{align*}&lt;/p&gt;
&lt;p&gt;To recap, the 5 intrinsic parameters are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;\(\alpha\) - scale parameter for \(x\)&lt;/li&gt;
&lt;li&gt;\(\frac{\beta}{\sin \theta}\) - scale parameter for \(y\), also accounts for skew&lt;/li&gt;
&lt;li&gt;\(-\alpha \cot \theta\) - skew coefficient between the \(x\) and \(y\) axis&lt;/li&gt;
&lt;li&gt;\(x_0\) - the principal point for \(x\)&lt;/li&gt;
&lt;li&gt;\(y_0\) - the principal point for \(y\)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The third parameter \(-\alpha \cot \theta\) is sometimes represented as \(\gamma\) and will be set to 0 in most cases.
A typical intrinsic matrix is then&lt;/p&gt;
&lt;p&gt;\[
K = \begin{bmatrix}
\alpha &amp;amp; \gamma &amp;amp; x_0\\
0 &amp;amp; \beta &amp;amp; y_0\\
0 &amp;amp; 0 &amp;amp; 1\\
\end{bmatrix}.
\]&lt;/p&gt;
&lt;h3 id=&#34;extrinsic-parameters&#34;&gt;Extrinsic Parameters&lt;/h3&gt;
&lt;p&gt;Given a point with respect to the world frame, \([\mathbf{p}]_W\), we are interested in a change of basis matrix which transforms the world space coordinate to the camera&amp;rsquo;s basis.
This is represented by a change of basis matrix which converts a point in world space to the camera&amp;rsquo;s coordinate frame.&lt;/p&gt;
&lt;p&gt;The matrix is defined as a rigid transformation involving a rotation and translation.
The rotation components \(R\) are composed following the 3 rotation matrices described earlier.
This only requires 3 parameters if using something like &lt;a href=&#34;https://en.wikipedia.org/wiki/Euler_angles&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Euler angles&lt;/a&gt;.
Euler angles represent the rotational angle around each axis by individual parameters \(\phi\), \(\theta\), and \(\psi\).&lt;/p&gt;
&lt;p&gt;\[
R = \begin{bmatrix}
r_{11} &amp;amp; r_{12} &amp;amp; r_{13}\\
r_{21} &amp;amp; r_{22} &amp;amp; r_{23}\\
r_{31} &amp;amp; r_{32} &amp;amp; r_{33}\\
\end{bmatrix}
\]&lt;/p&gt;
&lt;p&gt;The translational component is represented using an additional 3 parameters \(t_x\), \(t_y\), and \(t_z\).
This can be represented as a matrix&lt;/p&gt;
&lt;p&gt;\[
T = \begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; t_x\\
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; t_y\\
0 &amp;amp; 0 &amp;amp; 1 &amp;amp; t_z\\
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;For convenience, the extrinsic parameters can be represented as a single matrix&lt;/p&gt;
&lt;p&gt;\[
M = \begin{bmatrix}
r_{11} &amp;amp; r_{12} &amp;amp; r_{13} &amp;amp; t_x\\
r_{21} &amp;amp; r_{22} &amp;amp; r_{23} &amp;amp; t_y\\
r_{31} &amp;amp; r_{32} &amp;amp; r_{33} &amp;amp; t_z\\
\end{bmatrix}
\]&lt;/p&gt;
&lt;p&gt;The full transformation from world space to image space is then represented as&lt;/p&gt;
&lt;p&gt;\[
\mathbf{p} = KM[\mathbf{p}]_W,
\]&lt;/p&gt;
&lt;p&gt;where \([\mathbf{p}]_W = \begin{bmatrix}x\\y\\z\\1\end{bmatrix}\).&lt;/p&gt;
&lt;h2 id=&#34;camera-calibration&#34;&gt;Camera Calibration&lt;/h2&gt;
&lt;p&gt;There are many applications which require that we know the exact camera parameters, including augmented reality, inpainting techniques, and depth estimation.
We now investigate how, given a fixed world coordinate system with known structure, we can approximate the camera parameters.&lt;/p&gt;
&lt;p&gt;Popular software solutions for calibrating cameras are provided by &lt;a href=&#34;http://www.vision.caltech.edu/bouguetj/calib_doc/htmls/example.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;MATLAB&lt;/a&gt; and &lt;a href=&#34;https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;OpenCV&lt;/a&gt;.
These are based off of work by &lt;a href=&#34;https://www.microsoft.com/en-us/research/project/a-flexible-new-technique-for-camera-calibration-2/?from=https%3A%2F%2Fresearch.microsoft.com%2F%7Ezhang%2FCalib%2F&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Zhengyou Zhang&lt;/a&gt;.
We will follow the original publication to explain the solutions to camera calibration.
As such, the notation will change a bit.&lt;/p&gt;
&lt;p&gt;The first goal is to relate the model points \(\mathbf{M}\) to the image points \(\mathbf{m}\) by a homography \(\mathbf{H}\) such that&lt;/p&gt;
&lt;p&gt;\[
s\hat{\mathbf{m}} = \mathbf{H} \hat{\mathbf{M}},
\]&lt;/p&gt;
&lt;p&gt;where \(\hat{\mathbf{m}}\) is the homogeneous coordinate of \(\mathbf{m}\) (likewise for \(\mathbf{M}\)) and \(s\) is a scale factor.&lt;/p&gt;
&lt;p&gt;To relate this to the notation in the previous section,&lt;/p&gt;
&lt;p&gt;\[
s\tilde{\mathbf{m}} = \mathbf{H} \tilde{\mathbf{M}} \equiv \mathbf{p} = KM[\mathbf{p}]_W.
\]&lt;/p&gt;
&lt;p&gt;In the paper, \(\mathbf{H} = \mathbf{A}[\mathbf{R}\quad \mathbf{t}]\), where&lt;/p&gt;
&lt;p&gt;\[
\mathbf{A} = \begin{bmatrix}
\alpha &amp;amp; \gamma &amp;amp; u_0\\
0 &amp;amp; \beta &amp;amp; v_0\\
0 &amp;amp; 0 &amp;amp; 1\\
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;\([\mathbf{R}\quad \mathbf{t}]\) is the matrix \(M\) from the previous section.&lt;/p&gt;
&lt;h3 id=&#34;estimating-the-homography&#34;&gt;Estimating the Homography&lt;/h3&gt;
&lt;p&gt;If we have an image of the model plan, we can estimate \(\mathbf{H}\) using maximum-likelihood estimation as follows.
Assume that \(\mathbf{m}_i\) is affected by Gaussian noise with zero mean and covariance \(\mathbf{\Lambda}_{\mathbf{m}_i}\).
The objective is then to minimize&lt;/p&gt;
&lt;p&gt;\[
\sum_i (\mathbf{m}_i - \hat{\mathbf{m}_i})^T \mathbf{\Lambda_{\mathbf{m}_i}^{-1}}(\mathbf{m}_i - \hat{\mathbf{m}_i}),
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\hat{\mathbf{m}_i} = \frac{1}{\bar{\mathbf{h}}_3^T \mathbf{M}_i} \begin{bmatrix}
\bar{\mathbf{h}}_1^T \mathbf{M}_i\\
\bar{\mathbf{h}}_2^T \mathbf{M}_i\\
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;\(\bar{\mathbf{h}}_i\) represents the $i$th row of \(\mathbf{H}\).&lt;/p&gt;
&lt;p&gt;The approach now constructs a matrix of model points similar to the approach used by &lt;a href=&#34;https://ajdillhoff.github.io/notes/random_sample_consensus/&#34;&gt;RANSAC&lt;/a&gt;.
First, let \(\mathbf{x} = [\bar{\mathbf{h}}_1^T\  \bar{\mathbf{h}}_2^T\ \bar{\mathbf{h}}_3^T]\).
Recalling that \(\widetilde{\mathbf{M}}^T \in [X, Y, 1]^T\),&lt;/p&gt;
&lt;p&gt;\[
\begin{bmatrix}
\widetilde{\mathbf{M}}^T &amp;amp; \mathbf{0}^T &amp;amp; -u \widetilde{\mathbf{M}}^T\\
\mathbf{0}^T &amp;amp; \widetilde{\mathbf{M}}^T &amp;amp; -v \widetilde{\mathbf{M}}^T\\
\end{bmatrix}\mathbf{x} = \mathbf{0}.
\]&lt;/p&gt;
&lt;p&gt;Writing this as \(\mathbf{L}\mathbf{x} = \mathbf{0}\), where \(\mathbf{L} \in \mathbb{R}^{2n \times 9}\) with \(n\) being the number of points, the solution the eigenvector of \(\mathbf{L}^T\mathbf{L}\) corresponding to the smallest eigenvalue.
This can be computed using &lt;a href=&#34;https://en.wikipedia.org/wiki/Singular_value_decomposition&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Singular Value Decomposition&lt;/a&gt; without computing \(\mathbf{L}^T\mathbf{L}\) directly.
Factorize \(\mathbf{L} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T\) using SVD.
Then the parameters that minimizes the objective is the column vector in \(\mathbf{V}\) corresponding to the smallest eigenvalue.&lt;/p&gt;
&lt;p&gt;It is noted that since the columns of the rotation matrix are orthonormal, the following constraints are observed:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{h}_1^T \mathbf{A}^{-T} \mathbf{A}^{-1} \mathbf{h}_2 &amp;amp;= 0\\
\mathbf{h}_1^T \mathbf{A}^{-T} \mathbf{A}^{-1} \mathbf{h}_1 &amp;amp;= \mathbf{h}_2^T \mathbf{A}^{-T} \mathbf{A}^{-1} \mathbf{h}_2\\
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;solving-for-the-camera-parameters&#34;&gt;Solving for the Camera Parameters&lt;/h3&gt;
&lt;p&gt;With the homography \(\mathbf{H}\) and constraints given in the last section, Zhang proposes a two step solution that first estimates the parameters using a closed form solution.
These serve as a starting point for a non-linear optimization using &lt;a href=&#34;https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Levenberg-Marquardt&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, let&lt;/p&gt;
&lt;p&gt;\[
\mathbf{B} = \mathbf{A}^{-T} \mathbf{A}^{-1} = \begin{bmatrix}
\frac{1}{\alpha^2} &amp;amp; -\frac{\gamma}{\alpha^2\beta} &amp;amp; \frac{v_0\gamma - u_0\beta}{\alpha^2 \beta}\\
-\frac{\gamma}{\alpha^2\beta} &amp;amp; \frac{\gamma^2}{\alpha^2\beta^2} + \frac{1}{\beta^2} &amp;amp; -\frac{\gamma(v_0\gamma-u_0\beta)}{\alpha^2\beta^2}-\frac{v_0}{\beta^2}\\
\frac{v_0\gamma - u_0\beta}{\alpha^2 \beta} &amp;amp; -\frac{\gamma(v_0\gamma-u_0\beta)}{\alpha^2\beta^2}-\frac{v_0}{\beta^2} &amp;amp; \frac{(v_0\gamma-u_0\beta)^2}{\alpha^2\beta^2}+\frac{v_0^2}{\beta^2}+1
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;This is a symmetric matrix which can be represented more compactly as&lt;/p&gt;
&lt;p&gt;\[
\mathbf{b} = [B_{11}\ B_{12}\ B_{22}\ B_{13}\ B_{23}\ B_{33}]^T.
\]&lt;/p&gt;
&lt;p&gt;We can then write \(\mathbf{h}_1^T \mathbf{A}^{-T} \mathbf{A}^{-1} \mathbf{h}_2 = 0\) as&lt;/p&gt;
&lt;p&gt;\[
\mathbf{h}_i^T \mathbf{B} \mathbf{h}_j = \mathbf{v}_{ij}^T \mathbf{b},
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\mathbf{v}_{ij} = [h_{i1}h_{j1}, h_{i1}h_{j2} + h_{i2}h_{j1}, h_{i2}h_{j2}, h_{i3}h_{j1} + h_{i1}h_{j3}, h_{i3}h_{j2}+h_{i2}h_{j3}, h_{i3}h_{j3}]^T.
\]&lt;/p&gt;
&lt;p&gt;This permits us to write the constraints from the previous section as two homogeneous equations in \(\mathbf{b}\)&lt;/p&gt;
&lt;p&gt;\[
\begin{bmatrix}
\mathbf{v}_{12}^T\\
(\mathbf{v}_{11} - \mathbf{v}_{22})^T
\end{bmatrix} \mathbf{b} = \mathbf{0}.
\]&lt;/p&gt;
&lt;p&gt;If we compute the homography, as above, on \(n\) images, we end up with \(n\) equations like the one directly above.
Stacking this yields&lt;/p&gt;
&lt;p&gt;\[
\mathbf{V}\mathbf{b} = \mathbf{0},
\]&lt;/p&gt;
&lt;p&gt;where \(\mathbf{V} \in \mathbb{R}^{2n \times 6}\).&lt;/p&gt;
&lt;p&gt;The initial set of parameters is then solved following the solution for each homography \(\mathbf{H}\).
With \(\mathbf{b}\) estimated, the intrinsic parameters are&lt;/p&gt;
&lt;p&gt;\begin{align*}
v_0 &amp;amp;= (B_{12}B_{13} - B_{11}B_{23})/(B_{11}B_{22} - B_{12}^2)\\
\lambda &amp;amp;= B_{33} - [B_{13}^2 + v_0(B_{12}B_{13} - B_{11}B_{23})]/B_{11}\\
\alpha &amp;amp;= \sqrt{\lambda / B_{11}}\\
\beta &amp;amp;= \sqrt{\lambda B_{11}/(B_{11}B_{22} - B_{12}^2)}\\
\gamma &amp;amp;= -B_{12}\alpha^2\beta / \lambda\\
u_0 &amp;amp;= \gamma v_0 / \alpha - B_{13} \alpha^2 / \lambda.
\end{align*}&lt;/p&gt;
&lt;p&gt;The parameters \(\alpha, \beta, \gamma, u_0, v_0\) make up our intrinsic parameter matrix \(\mathbf{A}\).
These can be used to compute the extrinsic parameters following&lt;/p&gt;
&lt;p&gt;\begin{align*}
\lambda &amp;amp;= 1 / \|\mathbf{A}^{-1}\mathbf{h}_1\|\\
\mathbf{r}_1 &amp;amp;= \lambda \mathbf{A}^{-1}\mathbf{h}_1\\
\mathbf{r}_2 &amp;amp;= \lambda \mathbf{A}^{-1}\mathbf{h}_2\\
\mathbf{r}_3 &amp;amp;= \mathbf{r}_1 \times \mathbf{r}_2\\
\mathbf{t} &amp;amp;= \lambda \mathbf{A}^{-1}\mathbf{h}_3\\
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;refining-the-estimates&#34;&gt;Refining the Estimates&lt;/h3&gt;
&lt;p&gt;The approach in the last section is produced by minimizing algebraic distances.
These values are further refined using maximum-likelihood estimation by way of a nonlinear optimizer.
If there are \(m\) points on each of the \(n\) images, the objective function to minimize is given by&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^n \sum_{j=1}^m \|\mathbf{m}_{ij} - \hat{\mathbf{m}}(\mathbf{A}, \mathbf{R}_i, \mathbf{t}_i, \mathbf{M}_j)\|^2,
\]&lt;/p&gt;
&lt;p&gt;where \(\hat{\mathbf{m}}(\mathbf{A}, \mathbf{R}_i, \mathbf{t}_i, \mathbf{M}_j)\) is a projection of point \(\mathbf{M}_j\) in image \(i\).
This function is minimized using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Levenberg-Marquardt&lt;/a&gt; algorithm.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tracking</title>
      <link>https://ajdillhoff.github.io/notes/tracking/</link>
      <pubDate>Mon, 07 Mar 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/tracking/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tracking-with-optical-flow&#34;&gt;Tracking with Optical Flow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kalman-filters&#34;&gt;Kalman Filters&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Tracking features and objects is required in many applications ranging from autonomous driving to security. Vision tracking systems are often used for live sports broadcasts to keep track of players, the ball, and other visual queues related to the game.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-22_18-19-53_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Source: &amp;lt;https://azbigmedia.com/lifestyle/ball-tracking-technology-changes-way-fans-consume-practice-sport-of-golf/&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Source: &lt;a href=&#34;https://azbigmedia.com/lifestyle/ball-tracking-technology-changes-way-fans-consume-practice-sport-of-golf/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://azbigmedia.com/lifestyle/ball-tracking-technology-changes-way-fans-consume-practice-sport-of-golf/&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Naive tracking will detect an object per frame without any regard for prior information.
More sophisticated trackers will consider the previous frame as a starting point to their search space.
However, even these trackers many need to initialize after a certain amount of time if their estimate drifts too far away from the object&amp;rsquo;s actual location.&lt;/p&gt;
&lt;p&gt;Example of the importance of reliable tracking for driving assistance:
&lt;a href=&#34;https://youtu.be/NSDTZQdo6H8?t=898&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://youtu.be/NSDTZQdo6H8?t=898&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;tracking-with-optical-flow&#34;&gt;Tracking with Optical Flow&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Image motion&lt;/strong&gt; can be described as complex changes in image intensity from one time to another,
displaced by \(\delta\).&lt;/p&gt;
&lt;p&gt;The displacement can be modeled as an &lt;em&gt;affine motion field&lt;/em&gt; where the point is warped and translated by \(d\):&lt;/p&gt;
&lt;p&gt;\[
\delta = D\mathbf{x} + \mathbf{x},
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
D = \begin{bmatrix}
d_{xx} &amp;amp; d_{xy}\\
d_{yx} &amp;amp; d_{yy}
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;When comparing features between an image at \(t\) and \(t-1\), the feature centered at \(\mathbf{x}\) is transformed by&lt;/p&gt;
&lt;p&gt;\[
\textbf{I}_{t}(A\mathbf{x} + \mathbf{d}) = \textbf{I}_{t-1}(\mathbf{x}).
\]&lt;/p&gt;
&lt;p&gt;Here, \(A = I_2 + D\), where \(I_2\) is the \(2 \times 2\) identity matrix.
This addition will be explained later.&lt;/p&gt;
&lt;p&gt;Smaller variations between frames are less reliable for parameter estimation, so a pure translational model is better in these cases. That is&lt;/p&gt;
&lt;p&gt;\[
\delta = \mathbf{d}.
\]&lt;/p&gt;
&lt;h3 id=&#34;computing-image-motion&#34;&gt;Computing Image Motion&lt;/h3&gt;
&lt;p&gt;Computing image motion then becomes a minimization problem.&lt;/p&gt;
&lt;p&gt;\[
\epsilon = \int \int_{W} \big[\mathbf{I}_{t}(A\mathbf{x} + \mathbf{d}) - \mathbf{I}_{t-1}(\mathbf{x})\big]^2 w(\mathbf{x}) d\mathbf{x}
\]&lt;/p&gt;
&lt;p&gt;This is used in a minimization problem which will minimize the dissimilarity between the tracked features between frames.
The point is weighted by a function \(w\) over a window \(W\).&lt;/p&gt;
&lt;p&gt;Minimization of this error involves taking the derivative of \(\epsilon\) with respect to the unknowns in \(D\) and displacement vector \(\mathbf{d}\):&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{1}{2}\frac{\partial \epsilon}{\partial D} &amp;amp;= \int \int_W \Big[\mathbf{I}_t(A\mathbf{x}+\mathbf{d}) - \mathbf{I}_{t-1}(\mathbf{x})\Big]\mathbf{g}\mathbf{x}^T w d\mathbf{x} = 0\\
\frac{1}{2}\frac{\partial \epsilon}{\partial \mathbf{d}} &amp;amp;= \int \int_W \Big[\mathbf{I}_t(A\mathbf{x}+\mathbf{d}) - \mathbf{I}_{t-1}(\mathbf{x})\Big]\mathbf{g} w d\mathbf{x} = 0\\
\end{align*}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\mathbf{g} = \bigg(\frac{\partial \mathbf{I}_t}{\partial x}, \frac{\partial \mathbf{I}_t}{\partial y}\bigg)^T
\]&lt;/p&gt;
&lt;p&gt;is this spatial gradient of the image intensity.
We have computed these image gradients before!&lt;/p&gt;
&lt;p&gt;The transformation of a point from \(t\) to \(t+1\) is complex because of the potential rotation, scaling, and translation that can occur. Specifically, \(A\mathbf{x} + \mathbf{d}\) is a nonlinear transformation. To solve this with less friction, the term can be approximated.&lt;/p&gt;
&lt;p&gt;To linearlize \(\mathbf{I}_{t}\), a Taylor series expansion of it can be used, taking just the linear term:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{I}_t(A\mathbf{x}+\mathbf{d}) = \mathbf{I}_t(\mathbf{x})+\mathbf{g}^T(\mathbf{u}),
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\mathbf{u} = D\mathbf{x}+\mathbf{d}.
\]&lt;/p&gt;
&lt;p&gt;The authors argue that this approximation is reasonable assuming the motion in the images is small.&lt;/p&gt;
&lt;p&gt;Plugging this back into the derivatives above yields&lt;/p&gt;
&lt;p&gt;\begin{align*}
\int \int_W \mathbf{g}\mathbf{x}^T(\mathbf{g}^T\mathbf{u})w d\mathbf{x} &amp;amp;= \int \int_W \big[\mathbf{I}_{t-1}(\mathbf{x}) - \mathbf{I}_t\big]\mathbf{g}\mathbf{x}^T w d\mathbf{x}\\
\int \int_W \mathbf{g}(\mathbf{g}^T\mathbf{u})w d\mathbf{x} &amp;amp;= \int \int_W \big[\mathbf{I}_{t-1}(\mathbf{x}) - \mathbf{I}_t\big]\mathbf{g} w d\mathbf{x}\\
\end{align*}&lt;/p&gt;
&lt;p&gt;This is solved iteratively, following the Newton method, starting with the following values at \(t=0\):&lt;/p&gt;
&lt;p&gt;\begin{align*}
D_0 &amp;amp;= I\\
\mathbf{d}_0 &amp;amp;= \mathbf{0}\\
\mathbf{I}_0 &amp;amp;= \mathbf{I}(\mathbf{x}).
\end{align*}&lt;/p&gt;
&lt;p&gt;At step \(i\) the values are updated to&lt;/p&gt;
&lt;p&gt;\begin{align*}
D_i\\
\mathbf{d}_i\\
\mathbf{I}_i &amp;amp;= \mathbf{I}_{i-1}(A_i \mathbf{x} + \mathbf{d}_i).
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;a-more-compact-representation&#34;&gt;A More Compact Representation&lt;/h3&gt;
&lt;p&gt;At this point, the authors convert this representation into a more compact form in which the unknowns in \(D\) and the values of \(\mathbf{d}\) are separated from the function. We start with our current system of equations.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\int \int_W \mathbf{g}\mathbf{x}^T(\mathbf{g}^T\mathbf{u})w d\mathbf{x} &amp;amp;= \int \int_W \big[\mathbf{I}_{t-1}(\mathbf{x}) - \mathbf{I}_t(\mathbf{x})\big]\mathbf{g}\mathbf{x}^T w d\mathbf{x}\\
\int \int_W \mathbf{g}(\mathbf{g}^T\mathbf{u})w d\mathbf{x} &amp;amp;= \int \int_W \big[\mathbf{I}_{t-1}(\mathbf{x}) - \mathbf{I}_t(\mathbf{x})\big]\mathbf{g} w d\mathbf{x}\\
\end{align*}&lt;/p&gt;
&lt;p&gt;To achieve this, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Kronecker_product&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Kronecker product&lt;/a&gt; is used. This is a generalization of the outer product from vectors to matrices.
For two matrices \(A \in \mathbb{R}^{p \times q}\) and \(B \in \mathbf{R}^{m \times n}\), \(A \otimes B\) is a \(p \times q\) block matrix&lt;/p&gt;
&lt;p&gt;\[
A \otimes B = \begin{bmatrix}
a_{11}B &amp;amp; \cdots &amp;amp; a_{1q}\\
\vdots &amp;amp; \ddots &amp;amp; \vdots\\
a_{p1}B &amp;amp; \cdots &amp;amp; a_{pq}B
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;It has two particularly useful properties for this problem:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;\(A^T \otimes B^T = (A \otimes B)^T\)&lt;/li&gt;
&lt;li&gt;\(v(AXB) = (B^T \otimes A)v(X)\), where \(v\) is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Vectorization_%28mathematics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;vectorization&lt;/a&gt; operator.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Using this product and the properties just listed, we can extract the unknowns \(D\) and \(\mathbf{d}\) from the equations above.&lt;/p&gt;
&lt;p&gt;First, note that \(\mathbf{g}^T \mathbf{u}\) appear in both of the equations. These can then be rewritten as follows.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{g}^T \mathbf{u} &amp;amp;= \mathbf{g}^T(D\mathbf{x} + \mathbf{d})\\
&amp;amp;= \mathbf{g}^T D \mathbf{x} + \mathbf{g}^T \mathbf{d}\\
&amp;amp;= v(\mathbf{g}^T D \mathbf{x}) + \mathbf{g}^T \mathbf{d} &amp;amp;\text{ vectorization }\\
&amp;amp;= (\mathbf{x}^T \otimes \mathbf{g}^T)v(D) + \mathbf{g}^T \mathbf{d} &amp;amp;\text{ property 2 }\\
&amp;amp;= (\mathbf{x} \otimes \mathbf{g})^T v(D) + \mathbf{g}^T \mathbf{d} &amp;amp;\text{ property 1 }
\end{align*}&lt;/p&gt;
&lt;p&gt;Likewise, the term \(\mathbf{g}\mathbf{x}^T\) that appears on the right side of the first equation in our set to solve can be written as&lt;/p&gt;
&lt;p&gt;\begin{align*}
v(\mathbf{g}\mathbf{x}^T) &amp;amp;= v(\mathbf{g}1\mathbf{x}^T)\\
&amp;amp;= \mathbf{x} \otimes \mathbf{g}.
\end{align*}&lt;/p&gt;
&lt;p&gt;Plugging this into the first equation to solve from above yields&lt;/p&gt;
&lt;p&gt;\[
\int \int_W (\mathbf{x} \otimes \mathbf{g})((\mathbf{x} \otimes \mathbf{g})^T v(D) + \mathbf{g}^T\mathbf{d})w d\mathbf{x} = \int \int_W \big[\mathbf{I}_{t-1}(\mathbf{x}) - \mathbf{I}_t(\mathbf{x})\big](\mathbf{x} \otimes \mathbf{g}) w d\mathbf{x}.
\]&lt;/p&gt;
&lt;p&gt;Expanding these terms out produces&lt;/p&gt;
&lt;p&gt;\begin{align*}
\bigg(\int \int_W (\mathbf{x} \otimes \mathbf{g})(\mathbf{x} \otimes \mathbf{g})^T w d\mathbf{x}\bigg) v(D) + \bigg(\int \int_W (\mathbf{x} \otimes \mathbf{g}) \mathbf{g}^T w d\mathbf{x}\bigg) \mathbf{d}\\
= \int \int_W \big[\mathbf{I}_{t-1}(\mathbf{x}) - \mathbf{I}_t(\mathbf{x})\big](\mathbf{x} \otimes \mathbf{g}) w d\mathbf{x}.
\end{align*}&lt;/p&gt;
&lt;p&gt;The authors further simplify this equation using the following variables:&lt;/p&gt;
&lt;p&gt;\begin{align*}
U(\mathbf{x}) &amp;amp;= (\mathbf{x} \otimes \mathbf{g})(\mathbf{x} \otimes \mathbf{g})^T\\
V(\mathbf{x}) &amp;amp;= (\mathbf{x} \otimes \mathbf{g})\mathbf{g}^T\\
\mathbf{b}(\mathbf{x}) &amp;amp;= \big[\mathbf{I}_{t-1}(\mathbf{x}) - \mathbf{I}_{t}(\mathbf{x})\big]v(\mathbf{g}\mathbf{x}^T).
\end{align*}&lt;/p&gt;
&lt;p&gt;Then the above equation can be written as&lt;/p&gt;
&lt;p&gt;\[
\bigg(\int \int_W U(\mathbf{x}) w d\mathbf{x}\bigg) v(D) + \bigg(\int \int_W V(\mathbf{x}) w d\mathbf{x}\bigg) \mathbf{d} = \int \int_W \mathbf{b}(\mathbf{x}) w d\mathbf{x}.
\]&lt;/p&gt;
&lt;p&gt;To write the second equation in a similar way, the authors introduce two additional variables&lt;/p&gt;
&lt;p&gt;\begin{align*}
Z(\mathbf{x}) &amp;amp;= \mathbf{g}\mathbf{g}^T\\
\mathbf{x}(\mathbf{x}) &amp;amp;= \big[\mathbf{I}_{t-1}(\mathbf{x}) - \mathbf{I}_t(\mathbf{x})\big]\mathbf{g}.
\end{align*}&lt;/p&gt;
&lt;p&gt;Then,&lt;/p&gt;
&lt;p&gt;\[
\bigg(\int \int_W V^T(\mathbf{x}) w d\mathbf{x}\bigg) v(D) + \bigg(\int \int_W Z(\mathbf{x}) w d\mathbf{x}\bigg) \mathbf{d} = \int \int_W \mathbf{c}(\mathbf{x}) w d\mathbf{x}.
\]&lt;/p&gt;
&lt;p&gt;These equations can be written in a simple form: \(A\mathbf{x} + B\mathbf{y} = \mathbf{z}\).
A symmetric block matrix \(T \in \mathbb{R}^{6 \times 6}\) is then introduced:&lt;/p&gt;
&lt;p&gt;\begin{align*}
T &amp;amp;= \int \int_W \begin{bmatrix}
U &amp;amp; V\\
V^T &amp;amp; Z
\end{bmatrix} w d\mathbf{x}\\
&amp;amp;= \int \int_W \begin{bmatrix}
x^2g_x^2 &amp;amp; x^2 g_x g_y &amp;amp; x y g_x^2 &amp;amp; x y g_x g_y &amp;amp; x g_x^2 &amp;amp; x g_x g_y\\
x^2 g_x g_y &amp;amp; x^2 g_y^2 &amp;amp; x y g_x g_y &amp;amp; x y g_y^2 &amp;amp; x g_x g_y &amp;amp; x g_y^2\\
x y g_x^2 &amp;amp; x y g_x g_y &amp;amp; y^2 g_x^2 &amp;amp; y^2 g_x g_y &amp;amp; y g_x^2 &amp;amp; y g_x g_y\\
x y g_x g_y &amp;amp; x y g_y^2 &amp;amp; y^2 g_x g_y &amp;amp; y^2 g_y^2 &amp;amp; y g_x g_y &amp;amp; y g_y^2\\
x g_x^2 &amp;amp; x g_x g_y &amp;amp; y g_x^2 &amp;amp; y g_x g_y &amp;amp; g_x^2 &amp;amp; g_x g_y\\
x g_x g_y &amp;amp; x g_y^2 &amp;amp; y g_x g_y &amp;amp; y g_y^2 &amp;amp; g_x g_y &amp;amp; g_y^2
\end{bmatrix}w d \mathbf{x}.
\end{align*}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Look very closely at \(Z\)&lt;/strong&gt;. Does that remind you of anything?
Harris corner detection!&lt;/p&gt;
&lt;p&gt;The unknowns are vectorized as&lt;/p&gt;
&lt;p&gt;\[
\mathbf{z} = \begin{bmatrix}
v(D)\\
\mathbf{d}
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;The product vector is defined as&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{a} &amp;amp;= \int \int_W \begin{bmatrix}
\mathbf{b}\\
\mathbf{c}
\end{bmatrix} w d \mathbf{x}\\
&amp;amp;= \int \int_W \Big[\mathbf{I}_{t-1}(\mathbf{x}) - \mathbf{I}_t(\mathbf{x})\Big]\begin{bmatrix}
x g_x\\
x g_y\\
y g_x\\
y g_y\\
g_x\\
g_y
\end{bmatrix} w d \mathbf{x}.
\end{align*}&lt;/p&gt;
&lt;p&gt;Thus, the iterative solution requires solving the \(6 \times 6\) linear system&lt;/p&gt;
&lt;p&gt;\[
T \mathbf{z} = \mathbf{a}.
\]&lt;/p&gt;
&lt;h3 id=&#34;back-to-computing-image-motion&#34;&gt;Back to Computing Image Motion&lt;/h3&gt;
&lt;p&gt;With this more compact representation, the iterative solution is easier to achieve.
The authors conveniently note that the deformation of the feature window between frames will be relatively small, so \(D\) could be set to 0 for tracking.
This leads to the solution of a much smaller system for each time step:&lt;/p&gt;
&lt;p&gt;\[
Z \mathbf{d} = \begin{bmatrix}
g_x\\
g_y
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;Note that this is only true for small steps between frames.
It is also important to measure the dissimilarity between the feature at the initial frame as it changes over time with the iterative estimates.
If it changes too much, the dissimilarity will be high, indicating that it is no longer a reliable feature to track.&lt;/p&gt;
&lt;h3 id=&#34;picking-the-best-feature&#34;&gt;Picking the Best Feature&lt;/h3&gt;
&lt;p&gt;Shi and Tomasi posit that the best feature is one that can be tracked well.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We can track a window from frame to frame if this system represents good measurements, and if it can be solved reliably.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;They analyze the basic equation \(Z \mathbf{d} = \mathbf{e}\), which is solved during tracking.
If both eigenvalues of \(Z\) are large and do not differ by several orders of magnitude, the feature can be tracked reliably.
That is, they accept a window if the eigenvalues of \(Z\) satisfy&lt;/p&gt;
&lt;p&gt;\[
\min(\lambda_1, \lambda_2) &amp;gt; \lambda.
\]&lt;/p&gt;
&lt;p&gt;In practice, \(\lambda\) is determined by selecting a lower bound based on a region of uniform brightness in the image as well as an upper bound based on features such as corners.
The selected value of \(\lambda\) is somewhere in between.&lt;/p&gt;
&lt;h3 id=&#34;measuring-dissimilarity&#34;&gt;Measuring dissimilarity&lt;/h3&gt;
&lt;p&gt;To determine if a feature is still reliable over a longer time period, a measure of dissimilarity is used to measure the original feature versus its warped version at the current frame.
Consider the sequence below over 21 frames.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-08_20-53-48_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Source: Shi and Tomasi.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Source: Shi and Tomasi.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Their method successfully tracks the speed limit sign, as seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-08_20-54-43_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Source: Shi and Tomasi.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Source: Shi and Tomasi.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-08_20-57-23_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Source: Shi and Tomasi.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Source: Shi and Tomasi.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;They note the importance of using an affine deformation to track reliable features.
The figure below plots the dissimilarity using translation versus the deformation matrix over time.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-08_20-56-09_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Dissimilarity over time using translation (dashed) versus affine (solid) (Shi and Tomasi).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Dissimilarity over time using translation (dashed) versus affine (solid) (Shi and Tomasi).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;They also present a case when the feature is lost to occlusion, thus the dissimilarity of both approaches increases greatly over time.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-08_20-59-27_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Source: Shi and Tomasi.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Source: Shi and Tomasi.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-08_20-59-09_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Source: Shi and Tomasi.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Source: Shi and Tomasi.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-08_20-59-52_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Sign tracking (plusses) versus window tracking (circles) (Shi and Tomasi).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Sign tracking (plusses) versus window tracking (circles) (Shi and Tomasi).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;kalman-filters&#34;&gt;Kalman Filters&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;Kalman filter&lt;/strong&gt; is a linear dynamic model that models conditional probabilities following normal distributions.
It is an simple and effective model for tracking motion even in the presence of noise from measurements.
Kalman filters keep an estimate of the state and can update their estimates based on the given observations.&lt;/p&gt;
&lt;p&gt;\(\mathbf{X}_i\) - State of object at step \(i\)&lt;/p&gt;
&lt;p&gt;\(\mathbf{Y}_i\) - Measurement at step \(i\)&lt;/p&gt;
&lt;p&gt;There are two primary tasks to deal with, the real-time tracking task and the offline smoothing task.
We are more interested in the tracking task, so we will focus on that.&lt;/p&gt;
&lt;p&gt;Tracking task: \(P(X_k|Y_0, \dots, Y_k)\)&lt;/p&gt;
&lt;p&gt;Smoothing task: \(P(X_k|X_0, \dots, Y_n)\)&lt;/p&gt;
&lt;p&gt;Assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(P(Y_k|X_0, \dots, X_N, Y_0, \dots, Y_N) = P(Y_k|X_k)\)&lt;/li&gt;
&lt;li&gt;\(P(X_k|X_0, \dots, X_{k-1}) = P(X_k|X_{k-1})\)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;prediction-task&#34;&gt;Prediction Task&lt;/h3&gt;
&lt;p&gt;When tracking an object, we use the model to first predict a state and then update its current parameters given some measurement.
We want to predict the state given measurements: \(P(\mathbf{X}_i|\mathbf{Y}_0 = \mathbf{y}_0, \dots, \mathbf{Y}_{k-1}=\mathbf{y}_{k-1})\).&lt;/p&gt;
&lt;p&gt;Given the previous observations up to \(k-1\), what is our model&amp;rsquo;s estimate of the current state?
This can help establish a search location if we were looking to narrow our detection algorithm.&lt;/p&gt;
&lt;h3 id=&#34;correction&#34;&gt;Correction&lt;/h3&gt;
&lt;p&gt;\(P(\mathbf{X}_i|\mathbf{Y}_0 = \mathbf{y}_0, \dots, \mathbf{Y}_{i}=\mathbf{y}_{i})\) is the cur  rent distribution.
This is the estimate given the actual observation at \(i\).
Note that this observation could be given from a noisy measurement.&lt;/p&gt;
&lt;h3 id=&#34;linear-dynamics&#34;&gt;Linear Dynamics&lt;/h3&gt;
&lt;p&gt;Assuming linear models, the problem becomes much simpler.
We can model the observations and state using normal distributions.&lt;/p&gt;
&lt;p&gt;\[
\mathbf{x} \sim \mathcal{N}(\mathbf{\mu}, \Sigma)
\]&lt;/p&gt;
&lt;p&gt;The measurements themselves can be modeled as&lt;/p&gt;
&lt;p&gt;\[
\mathbf{y}_k \sim \mathcal{N}(\mathcal{B}_k \mathbf{x}_k, \Sigma_k),
\]&lt;/p&gt;
&lt;p&gt;where \(k\) is the current step.&lt;/p&gt;
&lt;p&gt;In other words, our model does not know the true position and velocity of objects at all times. If this were the case, tracking would be a trivial pursuit. This uncertainty is captured using Gaussian distributions. This is a reasonable assumption to make in many cases. It becomes more complex if we think about acceleration, but we could always model that as well.&lt;/p&gt;
&lt;p&gt;By the way, position and velocity are correlated. If the velocity of the current object is 0, you would probably bet that its position in the next frame will be the same. A covariance matrix tracks this correlation between the variables. The problem is starting to take shape. We have input and observation variables as well as a covariance matrix that defines the relationship between our position and velocity.&lt;/p&gt;
&lt;p&gt;For the future, the model will be represented as&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{x}_i &amp;amp;\sim \mathcal{N}(\mathcal{D}_i \mathbf{x}_{i-1}; \Sigma_{d_i})\\
\mathbf{y}_i &amp;amp;\sim \mathcal{N}(\mathcal{M}_i \mathbf{x}_i; \Sigma_{m_i}).
\end{align*}&lt;/p&gt;
&lt;p&gt;The mean parameter in both of these look a little strange; we usually see \(\mu\) here. The matrix \(\mathcal{D}_i\), for example, transforms the center of the Gaussian to an updated location. Consider a bird flying through the sky in a relatively straight pattern. Of course, there will be some variation due to external factors or the bird&amp;rsquo;s desire to move about. Modeling this motion with a Gaussian centered at the mean \(\mathbf{x}_{i-1}\) would be insufficient.&lt;/p&gt;
&lt;p&gt;For one, the likelihood that the bird would instantly change course is physically impossible. It could start to turn, but the resolution of our update steps would track this over a sequence of frames. Instead, we transform the value \(\mathbf{x}_{i-1}\) based on our current perception of the bird&amp;rsquo;s movement dynamics. This translates the Gaussian to a new location. It also has the possibility of changing its shape.&lt;/p&gt;
&lt;p&gt;For covering the algorithm, we will use notation following Forsyth and Ponce.
\(\bar{\mathbf{x}}_i^-\) is the mean of \(P(\mathbf{x}_i|y_0, \dots, y_{i-1})\) and \(\bar{\mathbf{x}}_i^+\) is the mean of \(P(\mathbf{x}_i|y_0, \dots, y_{i})\).
\(\Sigma_i^-\) and \(\Sigma_i^+\) are the covariances of those distributions.&lt;/p&gt;
&lt;p&gt;However, by making a convenient assumption about the observation model, we will mainly need to focus on the state model.
That is, the matrix \(\mathcal{M}_i\) is defined so that the mean is simply the state position at step \(i\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What does our state represent?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In a simple model, \(\mathbf{x} = \begin{bmatrix}p_x\\p_y\\v_x\\v_y\end{bmatrix}\).
That is the 2D position and velocity of the object being tracked.
The corresponding covariance matrix is \(\mathbf{x}\mathbf{x}^T\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do we predict the position and velocity of the next time step?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{p}_k &amp;amp;= \mathbf{p}_{k-1} + \Delta t \mathbf{v}_{k-1}\\
\mathbf{v}_k &amp;amp;= \mathbf{v}_{k-1}
\end{align*}&lt;/p&gt;
&lt;p&gt;This is making a simple, yet surprisingly effective, assumption that the velocity is constant.
We can write this as a matrix vector product:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{x}_k = \begin{bmatrix}
1 &amp;amp; 0 &amp;amp; \Delta t &amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \Delta t\\
0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\mathbf{x}_{k-1}
\]&lt;/p&gt;
&lt;p&gt;Compactly, the prediction for step \(t\) is \(\bar{\mathbf{x}}_k^- = \mathcal{D}_i \bar{\mathbf{x}}_{k-1}^+\).&lt;/p&gt;
&lt;p&gt;Since we updated every point \(\mathbf{x}_{k-1}\), we also need to make a prediction about the covariance matrix.
This is also achieved by multiplying every point by \(\mathcal{D}_i\)&lt;/p&gt;
&lt;p&gt;\[
\Sigma_{i}^- = \mathcal{D}_i\Sigma_{i-1}^+\mathcal{D}_i^T
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if we want to add additional knowledge like acceleration?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Our position prediction then follows&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{p}_i &amp;amp;= \mathbf{p}_{i-1} + \Delta t \mathbf{v}_{i-1} + \frac{1}{2} \Delta t^2 \mathbf{a}_{i-1}\\
\mathbf{v}_i &amp;amp;= \mathbf{v}_{i-1} + \Delta t \mathbf{a}_{i-1}
\end{align*}&lt;/p&gt;
&lt;p&gt;We can simplify this and assume constant acceleration, then \(\mathbf{a}_i = \mathbf{a}_{i-1}\).
The resulting update equation for \(\mathbf{x}_k\) becomes&lt;/p&gt;
&lt;p&gt;\[
\mathbf{x}_k = \begin{bmatrix}
1 &amp;amp; 0 &amp;amp; \Delta t &amp;amp; 0 &amp;amp; \frac{1}{2}\Delta t^2 \mathbf{a} &amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \Delta t &amp;amp; 0 &amp;amp; \frac{1}{2}\Delta t^2 \mathbf{a}\\
0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \Delta t &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \Delta t\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1\\
\end{bmatrix}
\mathbf{x}_{k-1}
\]&lt;/p&gt;
&lt;p&gt;However, if something like acceleration is a known &lt;strong&gt;control&lt;/strong&gt; factor in our system, then it does not need to be part of the state.
In this case, we could separate the update vector into&lt;/p&gt;
&lt;p&gt;\[
\mathbf{x}_k = \mathcal{D}_i\mathbf{x}_{k-1} + B_k \mathbf{u}_{k},
\]&lt;/p&gt;
&lt;p&gt;where \(B_k\) is the &lt;strong&gt;control matrix&lt;/strong&gt; and \(\mathbf{u}_k\) is the &lt;strong&gt;control vector&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;One last consideration is that of uncertainty due to factors outside of our system.
This can also be modeled using a Gaussian with zero mean and covariance \(\Sigma_d\):&lt;/p&gt;
&lt;p&gt;\[
\xi_k \sim \mathcal{N}(\mathbf{0}, \Sigma_d).
\]&lt;/p&gt;
&lt;p&gt;With this added noise, the prediction step becomes&lt;/p&gt;
&lt;p&gt;\begin{align*}
\bar{\mathbf{x}}_k^- &amp;amp;= \mathcal{D}_k\bar{\mathbf{x}}_{k-1}^+ + B_k \mathbf{u}_k\\
\Sigma_k^- &amp;amp;= \mathcal{D}_k \Sigma_{k-1}^+\mathcal{D}_k^T + \xi_k.
\end{align*}&lt;/p&gt;
&lt;p&gt;In words, \(\bar{\mathbf{x}}_k^-\) is the prediction of our current state based on the previous best estimate with an added correction term based on known factors (acceleration).
\(\Sigma_k^-\) is the updated uncertainty based on the old uncertainty with added Gaussian noise to reflect unknown factors.&lt;/p&gt;
&lt;h3 id=&#34;making-corrections&#34;&gt;Making Corrections&lt;/h3&gt;
&lt;p&gt;As we are tracking an object, we may have some way of getting measurements.
This could be through an object detector or other physical sensor.
The new measurement can refine our current set of parameters.
Kalman filters work well even if the measurement is noisy.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-10_22-34-57_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;1D Kalman Filter. Source: Forsyth and Ponce&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;1D Kalman Filter. Source: Forsyth and Ponce
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the figure above, the initial prediction has a large amount of uncertainty.
After updating with the current measurement, the uncertainty is reduced.&lt;/p&gt;
&lt;p&gt;The goal here is to reconcile the uncertainty of our predicted state with the uncertainty of the measurement.
That is, we want to know the distribution over the union of these two distributions.
This is achieved by multiplying the Gaussians together.&lt;/p&gt;
&lt;p&gt;\[
\mathcal{N}(\bar{\mathbf{x}}_i^+, \Sigma_i^+) = \mathcal{N}(\bar{\mathbf{x}}_i^-, \Sigma_i^-) * \mathcal{N}(\mathbf{y}_i, \Sigma_{m_i})
\]&lt;/p&gt;
&lt;p&gt;Solving for \(\bar{\mathbf{x}}_i^+\) and \(\Sigma_i^+\) yields&lt;/p&gt;
&lt;p&gt;\begin{align*}
\bar{\mathbf{x}}_i^+ &amp;amp;= \bar{\mathbf{x}}_i^- + \mathcal{K}_i(\mathbf{y}_i - \bar{\mathbf{x}}_i^-)\\
\Sigma_i^+ &amp;amp;= \Sigma_i^- - \mathcal{K}_i \Sigma_i^-,
\end{align*}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\mathcal{K}_i = \Sigma_i^-(\Sigma_i^- + \Sigma_{m_i})^{-1}.
\]&lt;/p&gt;
&lt;p&gt;\(\mathcal{K}_i\) is called the &lt;strong&gt;Kalman gain&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Let&amp;rsquo;s combine the prediction and correction steps&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We have a state distribution with mean and variance&lt;/p&gt;
&lt;p&gt;\begin{align*}
\bar{\mathbf{x}}_k^- &amp;amp;= \mathcal{D}_k\bar{\mathbf{x}}_{k-1}^+ + B_k \mathbf{u}_k\\
\Sigma_k^- &amp;amp;= \mathcal{D}_k \Sigma_{k-1}^+\mathcal{D}_k^T + \xi_k
\end{align*}&lt;/p&gt;
&lt;p&gt;as well as an observation distribution with mean and variance \(\mathbf{y}_k\) and \(\Sigma_{m_k}\).&lt;/p&gt;
&lt;p&gt;Plugging these into the update equations yield&lt;/p&gt;
&lt;p&gt;\begin{align*}
\bar{\mathbf{x}}_i^+ &amp;amp;= \mathcal{D}_k\bar{\mathbf{x}}_{k-1}^+ + \mathcal{K}_i(\mathbf{y}_i - \mathcal{D}_k\bar{\mathbf{x}}_{k-1}^+)\\
\Sigma_i^+ &amp;amp;= \mathcal{D}_k \Sigma_{k-1}^+\mathcal{D}_k^T - \mathcal{K}_i \mathcal{D}_k \Sigma_{k-1}^+\mathcal{D}_k^T,
\end{align*}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\mathcal{K}_i = \mathcal{D}_k \Sigma_{k-1}^+\mathcal{D}_k^T(\mathcal{D}_k \Sigma_{k-1}^+\mathcal{D}_k^T + \Sigma_{m_i})^{-1}.
\]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optical Flow</title>
      <link>https://ajdillhoff.github.io/notes/optical_flow/</link>
      <pubDate>Sun, 06 Mar 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/optical_flow/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#review-of-szeliski&#34;&gt;Review of Szeliski&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#motion-features&#34;&gt;Motion Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#computing-optical-flow&#34;&gt;Computing Optical Flow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#assumptions-of-small-motion&#34;&gt;Assumptions of Small Motion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;review-of-szeliski&#34;&gt;Review of Szeliski&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Estimating motion&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Error metric&lt;/li&gt;
&lt;li&gt;Search function&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Optical flow refers to the apparent motion in a 2D image. Optical flow methods estimate a &lt;strong&gt;motion field&lt;/strong&gt;, which refers to the true motion of objects in 3D. If a fixed camera records a video of someone walking from the left side of the screen to the right, a difference of two consecutive frames reveals much about the apparent motion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TODO&lt;/strong&gt; Frame differencing figure&lt;/p&gt;
&lt;p&gt;Consider a sphere with Lambertian reflectance.&lt;/p&gt;
&lt;p&gt;If the sphere is rotated:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What does the motion field look like?&lt;/li&gt;
&lt;li&gt;What does the optical flow look like?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If, instead, a point light is rotated around the sphere:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What does the motion field look like?&lt;/li&gt;
&lt;li&gt;What does the optical flow look like?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;motion-features&#34;&gt;Motion Features&lt;/h2&gt;
&lt;p&gt;Motion features are used a wide variety of tasks from compression, image segmentation, tracking, detection, video de-noising, and more.
Consider a fixed camera with a sphere centered in the middle of the frame.
As the frame moves towards the camera, its apparent size will become larger.
If we were to analyze the optical flow of such a sequence, we would see that the flow is radial as the sphere, projected as a circle, grows.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-06_17-16-16_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;From &amp;#34;Computer Vision - A Modern Approach&amp;#34; by Forsyth and Ponce&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;From &amp;ldquo;Computer Vision - A Modern Approach&amp;rdquo; by Forsyth and Ponce
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;At \(t=1\), the radius of the circle is given as \(R\).
At \(t=2\), the radius is \(r = f\frac{R}{Z}\), where \(f\) is some function of the motion and \(Z\) is the distance between the sphere and the camera.
From this, we can also compute the speed at which the sphere is travelling towards the camera as \(V=\frac{dZ}{dt}\).
The apparent rate of growth as observed by the camera is \(\frac{dr}{dt} = -f\frac{RV}{Z^2}\).
We can also determine the time to contact with the camera as \(-\frac{Z}{V}\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-06_18-06-27_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Optical flow on different parts of the image as observed from a moving camera whose direction of focus is perpendicular to the white plane. Source: &amp;#34;Computer Vision - A Modern Approach&amp;#34; by Forsyth and Ponce&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Optical flow on different parts of the image as observed from a moving camera whose direction of focus is perpendicular to the white plane. Source: &amp;ldquo;Computer Vision - A Modern Approach&amp;rdquo; by Forsyth and Ponce
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the figure above, the observer is moving at a constant rate to the left.
The points in the image appear to translate towards the right edge of the frame.
Points that are close to the camera appear to move faster than those that are farther away.
This can be used to estimate the apparent depth between objects in a scene.&lt;/p&gt;
&lt;h2 id=&#34;computing-optical-flow&#34;&gt;Computing Optical Flow&lt;/h2&gt;
&lt;p&gt;A popular assumption for optical flow, as discussed in (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Horn and Schunck 1980&lt;/a&gt;), is that of brightness constancy.
That is, a local feature has the same image intensity in one frame as it does in the subsequent frame.&lt;/p&gt;
&lt;p&gt;\[
I(x + u, y + v, t + 1) = I(x, y, t)
\]&lt;/p&gt;
&lt;p&gt;To formulate the problem as a differentiable function, an additional spatial smoothness assumption is made.
It is assumed that a pixel&amp;rsquo;s neighbors will also have the same optical flow as the pixel in question.
This is a reasonable assumption that asserts that patches are locally rigid and will exhibit uniform motion.&lt;/p&gt;
&lt;p&gt;Given these two constraints, we can formulate an objective function.
First, the objective function assuming brightness constancy is given by&lt;/p&gt;
&lt;p&gt;\[
E_D(\mathbf{u}, \mathbf{v}) = \sum_{s}(I(x_s + u_s, y_s + v_s, t + 1) - I(x, y, t))^2.
\]&lt;/p&gt;
&lt;p&gt;Adding in the assumption of uniform motion in a local region yields the term&lt;/p&gt;
&lt;p&gt;\[
E_S(\mathbf{u}, \mathbf{v}) = \sum_{n\in G(s)}(u_s - u_n)^2 + \sum_{n \in G(s)}(v_s - v_n)^2.
\]&lt;/p&gt;
&lt;p&gt;Putting these together, with a weighting term, yields&lt;/p&gt;
&lt;p&gt;\[
E(\mathbf{u}, \mathbf{v}) = E_D + \lambda E_S.
\]&lt;/p&gt;
&lt;p&gt;This energy function \(E_D\) is minimized by differentiating and setting the equation to 0:&lt;/p&gt;
&lt;p&gt;\[
I_x u + I_y v + I_t = 0.
\]&lt;/p&gt;
&lt;p&gt;The entire energy to be minimized is then&lt;/p&gt;
&lt;p&gt;\[
E_D(\mathbf{u}, \mathbf{v}) = \sum_{s}(I_{x,s}u_s + I_{y,s}v_s + I_{t,s})^2 + \lambda \sum_{n\in G(s)}(u_s - u_n)^2 + \sum_{n \in G(s)}(v_s - v_n)^2.
\]&lt;/p&gt;
&lt;p&gt;Differentiating this and setting to 0 yields two equations in \(u\) and \(v\):&lt;/p&gt;
&lt;p&gt;\begin{align*}
\sum_s{(I_{x,s}^2 u_s + I_{x,s}I_{y,s}v_s + I_{x,s}I_{t,s}) + \lambda \sum_{n \in G(s)}(u_s - u_n)} &amp;amp;= 0\\
\sum_s{(I_{x,s}I_{y,s} u_s + I_{y,s}^2v_s + I_{y,s}I_{t,s}) + \lambda \sum_{n \in G(s)}(v_s - v_n)} &amp;amp;= 0\\
\end{align*}&lt;/p&gt;
&lt;p&gt;Note that this is computed for every pixel in the image.
This system is no longer underspecified because of the assumption that neighbors will exhibit the same flow.
We now have 5 equations per pixel.
In more recent works, larger neighborhood grids (\(5 \times 5\)) are used.
Then, we have 25 equations per pixel.
Since this is a system of linear equations, it could be computed directly using the normal equations.&lt;/p&gt;
&lt;p&gt;However, Horn and Schunck did not have very fast computers in 1981.
So, they introduced an iterative solution (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Horn and Schunck 1980&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;assumptions-of-small-motion&#34;&gt;Assumptions of Small Motion&lt;/h2&gt;
&lt;p&gt;One of the core assumptions in early formulations of optical flow is that motion is very small (&amp;lt;1 pixel).
In reality, some objects may move over 100 pixels within a single frame.
A simple solution to this problem was proposed by Bergen et al. in 1992 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Bergen et al., n.d.&lt;/a&gt;).
By creating an image pyramid over several resolutions, the assumption of small motion at each scale is still reasonable.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-06_19-37-48_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Hierarchical motion estimation (Bergen et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Hierarchical motion estimation (Bergen et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;At one scale, the warping parameters are estimated.
Next, they are used to warp the image to match the one at \(t-1\).
The warped image and true image at \(t-1\) are compared to refine the parameters.
The refined parameters are then sent to the next scale layer.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Bergen, James R, P Anandan, Keith J Hanna, and Rajesh Hingorani. n.d. “Hierarchical Model-Based Motion Estimation,” 16.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Horn, Berthold K. P., and Brian G. Schunck. 1980. “Determining Optical Flow.” &lt;a href=&#34;https://dspace.mit.edu/bitstream/handle/1721.1/6337/%EE%80%80AIM%EE%80%81-572.pdf?sequence=2&#34;&gt;https://dspace.mit.edu/bitstream/handle/1721.1/6337/%EE%80%80AIM%EE%80%81-572.pdf?sequence=2&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Segmentation via Clustering</title>
      <link>https://ajdillhoff.github.io/notes/segmentation_via_clustering-2/</link>
      <pubDate>Thu, 24 Feb 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/segmentation_via_clustering-2/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#agglomerative-clustering&#34;&gt;Agglomerative Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-means-clustering&#34;&gt;K-Means Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#simple-linear-iterative-clustering--slic&#34;&gt;Simple Linear Iterative Clustering (SLIC)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#superpixels-in-recent-work&#34;&gt;Superpixels in Recent Work&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The goal of segmentation is fairly broad: group visual elements together.
For any given task, the question is &lt;em&gt;how are elements grouped?&lt;/em&gt;
At the smallest level of an image, pixels can be grouped by color, intensity, or spatial proximity.
Without a model of higher level objects, the pixel-based approach will break down at a large enough scale.&lt;/p&gt;
&lt;p&gt;Segmentation by thresholding works in cases where the boundaries between features are clearly defined.
However, thresholding is not very robust to complex images with noise.
Consider a simple image and its intensity histogram as noise is added.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-01_10-27-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;From left to right, a noiseless image with increasing amounts of Gaussian noise added. Source: Pearson Education, Inc.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;From left to right, a noiseless image with increasing amounts of Gaussian noise added. Source: Pearson Education, Inc.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Even with some noise added, as seen in the middle image, thresholding is still relatively straightforward.
Once enough noise is added, thresholding via pixel intensities will not work.
A more sophisticated approach is needed in this case.&lt;/p&gt;
&lt;p&gt;Clustering is a fairly intuitive way to think about segmentation.
Instead of a fine-grained representation of an image as a collection of pixels, it is represented as groups or clusters that share some common features.
The general process of clustering is simple.
The image is represented as a collection of feature vectors (intensity, pixel color, etc.).
Feature vectors are assigned to a single cluster. These clusters represent some segment of the image.&lt;/p&gt;
&lt;p&gt;When it comes to clustering methods, there are two main approaches: agglomerative and divisive.
Simply, one is a bottom-up approach. The other is a top-down approach.
After briefly introductin agglomerative clustering, we will explore specific implementations of segmentation using k-means clustering as well as segmentation using superpixels &amp;lt;&amp;amp;achantaSLICSuperpixelsCompared2012&amp;gt;.&lt;/p&gt;
&lt;h2 id=&#34;agglomerative-clustering&#34;&gt;Agglomerative Clustering&lt;/h2&gt;
&lt;p&gt;Agglomerative clustering methods start by assuming every element is a separate cluster.
Elements are formed based on some local similarities.
As these methods iterate, the number of clusters decreases.
Deciding which elements to merge depends on &lt;strong&gt;inter-cluster distance&lt;/strong&gt;.
The exact choice of distance is dependent on the task. Some examples include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Single-link clustering&lt;/strong&gt;: The distance between the closest elements.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Complete-link clustering&lt;/strong&gt;: The maximum distance between an element of the first cluster and one of the second.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Group average clustering&lt;/strong&gt;: Average distance of elements in a cluster.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;How many clusters are or should be in a single image?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a difficult question to answer for many reasons. The answer will be largely dependent on the task at hand.
It is a problem of learning the underlying generative process of the visual elements in the image.
By defining the specific goal of segmentation (segment by color, shape, etc.), we are introducing a prior about the underlying generative processes which formed the image.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;figure--fig1&#34;&gt;&lt;/a&gt;&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-24_16-24-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;3D-PointCapsNet learns point segmentations on only 1% of the training data (Zhao et al.).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;3D-PointCapsNet learns point segmentations on only 1% of the training data (Zhao et al.).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;There are approaches which attempt to segment objects in semi-supervised settings.
As seen in &lt;a href=&#34;#figure--fig1&#34;&gt;Figure 1&lt;/a&gt;, Zhao et al. propose a part segmentation model for 3D objects which only utilizes 1-5% of the training part labels &amp;lt;&amp;amp;zhao3DPointCapsule2019&amp;gt;.&lt;/p&gt;
&lt;p&gt;For example, if we divised an algorithm that would segment an image by color values, it might be able to segment the hand wearing a solid color glove relatively easily.
If we wanted to segment the hand into its individual joints, we would have to introduce a visual prior such as asking the subject to wear a multicolored glove.
We could also add prior information about the hand shape and joint configuration into the model itself.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;figure--joint-pc&#34;&gt;&lt;/a&gt;&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-01_22-08-18_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;An image-based joint regression model predicts joint locations (left) along with a point cloud generated from the joint estimates (right).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;An image-based joint regression model predicts joint locations (left) along with a point cloud generated from the joint estimates (right).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the &lt;a href=&#34;#figure--joint-pc&#34;&gt;figure above&lt;/a&gt;, the kinematic hand model could be used to segment the hand by assigning points in the point cloud to the nearest joint as estimated by the model.&lt;/p&gt;
&lt;p&gt;One way to visualize the cluster relationships is a &lt;em&gt;dendrogram&lt;/em&gt;.
Initially, each element is its own cluster. As the process evolves and clusters are merged based on some similarity,
the hierarchy is updated to show how the connections are formed.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-24_18-16-31_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Example output from scikit-image.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Example output from scikit-image.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;k-means-clustering&#34;&gt;K-Means Clustering&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/SLU-CSCI5750-SP2022/homework03_DigitClassificationKNN&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;K-Means Variant KNN Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html#sphx-glr-auto-examples-cluster-plot-color-quantization-py&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html#sphx-glr-auto-examples-cluster-plot-color-quantization-py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;K-Means clustering is a popular machine learning method used in both supervised and unsupervised settings.
It works by iteratively updating a set of &lt;em&gt;centroids&lt;/em&gt; or means until some stopping criteria is achieved.&lt;/p&gt;
&lt;p&gt;To use this with image segmentation, we start by treating our image features as vectors.
In the RGB case, each pixel is a vector of 3 values.
It starts out by initializing \(k\) clusters randomly with means \(\mathbf{m}_i\).
The next step is to compute the distance between the clusters and each point in the image.
Points are assigned to the cluster that is closest.&lt;/p&gt;
&lt;p&gt;\[
\text{arg}\min_{C} \sum_{i=1}^k \sum_{\mathbf{z}\in C_i}\|\mathbf{z} - \mathbf{m}_i\|^2,
\]&lt;/p&gt;
&lt;p&gt;where \(C = \{C_1, \dots, C_k\}\) is the cluster set.&lt;/p&gt;
&lt;p&gt;K-Means uses Expectation Maximization to update its parameters.
That is, it first computes the expected values given its current cluster centers before updating the cluster centers based on the new assignments.
The standard algorithm is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Initialize clusters&lt;/strong&gt; - Randomly select \(k\) points as cluster centers \(\mathbf{m}_i\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assign samples to clusters&lt;/strong&gt; - Assign each sample to the closest cluster center based on some distance metric.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Update the means&lt;/strong&gt; - Compute a new value for the cluster centers based on the assignments in the previous step.
\[
\mathbf{m}_i = \frac{1}{|C_i|}\sum_{\mathbf{z} \in C_i}\mathbf{z}, \quad i = 1, \dots, k
\]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Test for convergence&lt;/strong&gt; - Compute the distances between the means at time \(t\) and time \(t - 1\) as \(E\). Stop if the difference is less than some threshold: \(E \leq T\).&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-01_21-41-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Image segmented using k-means with k=3. Source: Pearson Education, Inc.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Image segmented using k-means with k=3. Source: Pearson Education, Inc.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;simple-linear-iterative-clustering--slic&#34;&gt;Simple Linear Iterative Clustering (SLIC)&lt;/h2&gt;
&lt;p&gt;Simple Linear Iterative Clustering (SLIC) is widely used algorithm based on K-Means clustering for image segmentation &amp;lt;&amp;amp;achantaSLICSuperpixelsCompared2012&amp;gt;.&lt;/p&gt;
&lt;p&gt;As discussed in the original paper, the authors state that SLIC h   as two main advantages over traditional K-Means:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The search space for assigning points is reduced, leading to an increase in performance.&lt;/li&gt;
&lt;li&gt;By weighting the distance measure, color and spatial proximity are both considered when forming clusters.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The algorithm itself is simple to understand and implement, as seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-01_23-39-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;SLIC Algorithm (Achanta et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;SLIC Algorithm (Achanta et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;initialization&#34;&gt;Initialization&lt;/h3&gt;
&lt;p&gt;To keep the search space smaller, the individual search regions are spaced \(S = \sqrt{N/k}\) pixels apart, where \(N\) is the number of pixels and \(k\) is the number of cluster centers.&lt;/p&gt;
&lt;p&gt;The image itself is represented in &lt;a href=&#34;https://en.wikipedia.org/wiki/CIELAB_color_space&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;CIELAB color space&lt;/a&gt;.
This color space was chosen because it is &lt;em&gt;perceputally uniform&lt;/em&gt;.
That is, it is useful for detecting small differences in color.&lt;/p&gt;
&lt;p&gt;Each of the \(k\) pixel clusters is then defined as a superpixel consisting of the CIELAB color and position:&lt;/p&gt;
&lt;p&gt;\[
C_i = [l_i\ a_i\ b_i\ x_i\ y_i]^T.
\]&lt;/p&gt;
&lt;p&gt;For stability, the seed locations are moved to the lowest gradient position in a \(3 \times 3\) neighborhood.
If the superpixels are building locally distinct regions, it is better to avoid placing them on an edge (boundary) pixel.&lt;/p&gt;
&lt;h3 id=&#34;search-space-and-distance&#34;&gt;Search Space and Distance&lt;/h3&gt;
&lt;p&gt;The search space for a cluster center is a region \(2S \times 2S\) around the cluster.
Each pixel in this region is compared to the cluster center \(C_k\) using a distance measure \(D\).&lt;/p&gt;
&lt;p&gt;The distance measure should consider both the spatial and color distances:&lt;/p&gt;
&lt;p&gt;\begin{align*}
d_c &amp;amp;= \sqrt{(l_j - l_i)^2 + (a_j - a_i)^2 + (b_j - b_i)^2}\\
d_s &amp;amp;= \sqrt{(x_j - x_i)^2 + (y_j - y_i)^2}\\
D&amp;rsquo; &amp;amp;= \sqrt{\Big(\frac{d_c}{N_c}\Big)^2 + \Big(\frac{d_s}{N_s}\Big)^2}
\end{align*}&lt;/p&gt;
&lt;p&gt;The individual distances should be normalized by their respective maximums since the range of CIELAB values is different from the variable maximum of \(N_s\), which is based on the image size.
Here, \(N_s\) corresponds to the sampling size \(\sqrt{N/k}\).&lt;/p&gt;
&lt;p&gt;The authors found that normalizing this way was inconsistent since the color distances vary greatly from cluster to cluster.
They turn this normalization into a hyperparameter constant \(m\) so that the user can control the importance between spatial and color proximity.&lt;/p&gt;
&lt;p&gt;\[
D = \sqrt{d_c^2 + \Big(\frac{d_s}{S}\Big)^2 m^2}
\]&lt;/p&gt;
&lt;p&gt;A smaller \(m\) results in superpixels that adhere more to image boundaries, where a larger value promotes compact superpixels.&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_20-24-07_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Comparison of SLIC against other superpixel methods (Achanta et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Comparison of SLIC against other superpixel methods (Achanta et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_20-26-00_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Images segmented using a varying number of clusters (Achanta et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Images segmented using a varying number of clusters (Achanta et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;superpixels-in-recent-work&#34;&gt;Superpixels in Recent Work&lt;/h2&gt;
&lt;p&gt;Superpixels are useful for reducing the dimensionality of the feature space.
Their applications include tracking, segmentation, and object detection.
Methods that extract superpixels do not work out of the box with deep learning methods
due to their non-differentiable formulation.
Deep learning methods rely on gradient descent to optimize their parameters.
This requires that the functions used in a deep network be differentiable.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_20-47-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Superpixels optimized for semantic segmentation (Jampani et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Superpixels optimized for semantic segmentation (Jampani et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Superpixel Sampling Networks, proposed by Jampani et al., introduce the first attempt at integrating superpixel extraction methods with deep learning models &amp;lt;&amp;amp;jampaniSuperpixelSamplingNetworks2018&amp;gt;.
In this work, they adapt SLIC as a differentiable layer in a deep network which result in superpixels that are fine-tuned for specific tasks.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_21-45-09_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;Model diagram for SSN (Jampani et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;Model diagram for SSN (Jampani et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The train their model on a semantic segmentation task which fine tunes the learned superpixels such that they adhere more closely to segmentation boundaries.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_21-51-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 11: &amp;lt;/span&amp;gt;Results on semantic segmentation (Jampani et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;Results on semantic segmentation (Jampani et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In a more recent work, Yang et al. propose a deep network that directly produces the superpixels as opposed to using a soft K-Means layer &amp;lt;&amp;amp;yangSuperpixelSegmentationFully2020&amp;gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_22-05-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 12: &amp;lt;/span&amp;gt;Model comparison between Jampani et al. and Yang et al. (Yang et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 12: &lt;/span&gt;Model comparison between Jampani et al. and Yang et al. (Yang et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Similar to SSN, they experiment on the Berkeley Image Segmentation Dataset.
Their results are competitive with other deep learning-based approaches.
The authors note that their method generalizes better in segmentation tasks by being robust to fine details and noise.
Additionally, their model runs at 50 fps using 4 NVIDIA Titan Xp GPUs.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_22-14-22_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 13: &amp;lt;/span&amp;gt;Comparison of results on competing methods (Yang et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 13: &lt;/span&gt;Comparison of results on competing methods (Yang et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;#print_bibliography: t&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Segmentation via Clustering</title>
      <link>https://ajdillhoff.github.io/notes/segmentation_via_clustering/</link>
      <pubDate>Thu, 24 Feb 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/segmentation_via_clustering/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#agglomerative-clustering&#34;&gt;Agglomerative Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-means-clustering&#34;&gt;K-Means Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#simple-linear-iterative-clustering--slic&#34;&gt;Simple Linear Iterative Clustering (SLIC)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#superpixels-in-recent-work&#34;&gt;Superpixels in Recent Work&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The goal of segmentation is fairly broad: group visual elements together.
For any given task, the question is &lt;em&gt;how are elements grouped?&lt;/em&gt;
At the smallest level of an image, pixels can be grouped by color, intensity, or spatial proximity.
Without a model of higher level objects, the pixel-based approach will break down at a large enough scale.&lt;/p&gt;
&lt;p&gt;Segmentation by thresholding works in cases where the boundaries between features are clearly defined.
However, thresholding is not very robust to complex images with noise.
Consider a simple image and its intensity histogram as noise is added.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-01_10-27-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;From left to right, a noiseless image with increasing amounts of Gaussian noise added. Source: Pearson Education, Inc.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;From left to right, a noiseless image with increasing amounts of Gaussian noise added. Source: Pearson Education, Inc.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Even with some noise added, as seen in the middle image, thresholding is still relatively straightforward.
Once enough noise is added, thresholding via pixel intensities will not work.
A more sophisticated approach is needed in this case.&lt;/p&gt;
&lt;p&gt;Clustering is a fairly intuitive way to think about segmentation.
Instead of a fine-grained representation of an image as a collection of pixels, it is represented as groups or clusters that share some common features.
The general process of clustering is simple.
The image is represented as a collection of feature vectors (intensity, pixel color, etc.).
Feature vectors are assigned to a single cluster. These clusters represent some segment of the image.&lt;/p&gt;
&lt;p&gt;When it comes to clustering methods, there are two main approaches: agglomerative and divisive.
Simply, one is a bottom-up approach. The other is a top-down approach.
After briefly introductin agglomerative clustering, we will explore specific implementations of segmentation using k-means clustering as well as segmentation using superpixels (Achanta et al. 2012).&lt;/p&gt;
&lt;h2 id=&#34;agglomerative-clustering&#34;&gt;Agglomerative Clustering&lt;/h2&gt;
&lt;p&gt;Agglomerative clustering methods start by assuming every element is a separate cluster.
Elements are formed based on some local similarities.
As these methods iterate, the number of clusters decreases.
Deciding which elements to merge depends on &lt;strong&gt;inter-cluster distance&lt;/strong&gt;.
The exact choice of distance is dependent on the task. Some examples include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Single-link clustering&lt;/strong&gt;: The distance between the closest elements.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Complete-link clustering&lt;/strong&gt;: The maximum distance between an element of the first cluster and one of the second.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Group average clustering&lt;/strong&gt;: Average distance of elements in a cluster.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;How many clusters are or should be in a single image?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a difficult question to answer for many reasons. The answer will be largely dependent on the task at hand.
It is a problem of learning the underlying generative process of the visual elements in the image.
By defining the specific goal of segmentation (segment by color, shape, etc.), we are introducing a prior about the underlying generative processes which formed the image.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;figure--fig1&#34;&gt;&lt;/a&gt;&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-24_16-24-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;3D-PointCapsNet learns point segmentations on only 1% of the training data (Zhao et al.).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;3D-PointCapsNet learns point segmentations on only 1% of the training data (Zhao et al.).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;There are approaches which attempt to segment objects in semi-supervised settings.
As seen in &lt;a href=&#34;#figure--fig1&#34;&gt;Figure 1&lt;/a&gt;, Zhao et al. propose a part segmentation model for 3D objects which only utilizes 1-5% of the training part labels (Zhao et al. 2019).&lt;/p&gt;
&lt;p&gt;For example, if we divised an algorithm that would segment an image by color values, it might be able to segment the hand wearing a solid color glove relatively easily.
If we wanted to segment the hand into its individual joints, we would have to introduce a visual prior such as asking the subject to wear a multicolored glove.
We could also add prior information about the hand shape and joint configuration into the model itself.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;figure--joint-pc&#34;&gt;&lt;/a&gt;&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-01_22-08-18_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;An image-based joint regression model predicts joint locations (left) along with a point cloud generated from the joint estimates (right).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;An image-based joint regression model predicts joint locations (left) along with a point cloud generated from the joint estimates (right).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the &lt;a href=&#34;#figure--joint-pc&#34;&gt;figure above&lt;/a&gt;, the kinematic hand model could be used to segment the hand by assigning points in the point cloud to the nearest joint as estimated by the model.&lt;/p&gt;
&lt;p&gt;One way to visualize the cluster relationships is a &lt;em&gt;dendrogram&lt;/em&gt;.
Initially, each element is its own cluster. As the process evolves and clusters are merged based on some similarity,
the hierarchy is updated to show how the connections are formed.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-24_18-16-31_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Example output from scikit-image.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Example output from scikit-image.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;k-means-clustering&#34;&gt;K-Means Clustering&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/SLU-CSCI5750-SP2022/homework03_DigitClassificationKNN&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;K-Means Variant KNN Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html#sphx-glr-auto-examples-cluster-plot-color-quantization-py&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html#sphx-glr-auto-examples-cluster-plot-color-quantization-py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;K-Means clustering is a popular machine learning method used in both supervised and unsupervised settings.
It works by iteratively updating a set of &lt;em&gt;centroids&lt;/em&gt; or means until some stopping criteria is achieved.&lt;/p&gt;
&lt;p&gt;To use this with image segmentation, we start by treating our image features as vectors.
In the RGB case, each pixel is a vector of 3 values.
It starts out by initializing \(k\) clusters randomly with means \(\mathbf{m}_i\).
The next step is to compute the distance between the clusters and each point in the image.
Points are assigned to the cluster that is closest.&lt;/p&gt;
&lt;p&gt;\[
\text{arg}\min_{C} \sum_{i=1}^k \sum_{\mathbf{z}\in C_i}\|\mathbf{z} - \mathbf{m}_i\|^2,
\]&lt;/p&gt;
&lt;p&gt;where \(C = \{C_1, \dots, C_k\}\) is the cluster set.&lt;/p&gt;
&lt;p&gt;K-Means uses Expectation Maximization to update its parameters.
That is, it first computes the expected values given its current cluster centers before updating the cluster centers based on the new assignments.
The standard algorithm is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Initialize clusters&lt;/strong&gt; - Randomly select \(k\) points as cluster centers \(\mathbf{m}_i\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assign samples to clusters&lt;/strong&gt; - Assign each sample to the closest cluster center based on some distance metric.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Update the means&lt;/strong&gt; - Compute a new value for the cluster centers based on the assignments in the previous step.
\[
\mathbf{m}_i = \frac{1}{|C_i|}\sum_{\mathbf{z} \in C_i}\mathbf{z}, \quad i = 1, \dots, k
\]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Test for convergence&lt;/strong&gt; - Compute the distances between the means at time \(t\) and time \(t - 1\) as \(E\). Stop if the difference is less than some threshold: \(E \leq T\).&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-01_21-41-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Image segmented using k-means with k=3. Source: Pearson Education, Inc.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Image segmented using k-means with k=3. Source: Pearson Education, Inc.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;simple-linear-iterative-clustering--slic&#34;&gt;Simple Linear Iterative Clustering (SLIC)&lt;/h2&gt;
&lt;p&gt;Simple Linear Iterative Clustering (SLIC) is widely used algorithm based on K-Means clustering for image segmentation (Achanta et al. 2012).&lt;/p&gt;
&lt;p&gt;As discussed in the original paper, the authors state that SLIC h   as two main advantages over traditional K-Means:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The search space for assigning points is reduced, leading to an increase in performance.&lt;/li&gt;
&lt;li&gt;By weighting the distance measure, color and spatial proximity are both considered when forming clusters.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The algorithm itself is simple to understand and implement, as seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-01_23-39-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;SLIC Algorithm (Achanta et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;SLIC Algorithm (Achanta et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;initialization&#34;&gt;Initialization&lt;/h3&gt;
&lt;p&gt;To keep the search space smaller, the individual search regions are spaced \(S = \sqrt{N/k}\) pixels apart, where \(N\) is the number of pixels and \(k\) is the number of cluster centers.&lt;/p&gt;
&lt;p&gt;The image itself is represented in &lt;a href=&#34;https://en.wikipedia.org/wiki/CIELAB_color_space&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;CIELAB color space&lt;/a&gt;.
This color space was chosen because it is &lt;em&gt;perceputally uniform&lt;/em&gt;.
That is, it is useful for detecting small differences in color.&lt;/p&gt;
&lt;p&gt;Each of the \(k\) pixel clusters is then defined as a superpixel consisting of the CIELAB color and position:&lt;/p&gt;
&lt;p&gt;\[
C_i = [l_i\ a_i\ b_i\ x_i\ y_i]^T.
\]&lt;/p&gt;
&lt;p&gt;For stability, the seed locations are moved to the lowest gradient position in a \(3 \times 3\) neighborhood.
If the superpixels are building locally distinct regions, it is better to avoid placing them on an edge (boundary) pixel.&lt;/p&gt;
&lt;h3 id=&#34;search-space-and-distance&#34;&gt;Search Space and Distance&lt;/h3&gt;
&lt;p&gt;The search space for a cluster center is a region \(2S \times 2S\) around the cluster.
Each pixel in this region is compared to the cluster center \(C_k\) using a distance measure \(D\).&lt;/p&gt;
&lt;p&gt;The distance measure should consider both the spatial and color distances:&lt;/p&gt;
&lt;p&gt;\begin{align*}
d_c &amp;amp;= \sqrt{(l_j - l_i)^2 + (a_j - a_i)^2 + (b_j - b_i)^2}\\
d_s &amp;amp;= \sqrt{(x_j - x_i)^2 + (y_j - y_i)^2}\\
D&amp;rsquo; &amp;amp;= \sqrt{\Big(\frac{d_c}{N_c}\Big)^2 + \Big(\frac{d_s}{N_s}\Big)^2}
\end{align*}&lt;/p&gt;
&lt;p&gt;The individual distances should be normalized by their respective maximums since the range of CIELAB values is different from the variable maximum of \(N_s\), which is based on the image size.
Here, \(N_s\) corresponds to the sampling size \(\sqrt{N/k}\).&lt;/p&gt;
&lt;p&gt;The authors found that normalizing this way was inconsistent since the color distances vary greatly from cluster to cluster.
They turn this normalization into a hyperparameter constant \(m\) so that the user can control the importance between spatial and color proximity.&lt;/p&gt;
&lt;p&gt;\[
D = \sqrt{d_c^2 + \Big(\frac{d_s}{S}\Big)^2 m^2}
\]&lt;/p&gt;
&lt;p&gt;A smaller \(m\) results in superpixels that adhere more to image boundaries, where a larger value promotes compact superpixels.&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_20-24-07_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Comparison of SLIC against other superpixel methods (Achanta et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Comparison of SLIC against other superpixel methods (Achanta et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_20-26-00_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Images segmented using a varying number of clusters (Achanta et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Images segmented using a varying number of clusters (Achanta et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;superpixels-in-recent-work&#34;&gt;Superpixels in Recent Work&lt;/h2&gt;
&lt;p&gt;Superpixels are useful for reducing the dimensionality of the feature space.
Their applications include tracking, segmentation, and object detection.
Methods that extract superpixels do not work out of the box with deep learning methods
due to their non-differentiable formulation.
Deep learning methods rely on gradient descent to optimize their parameters.
This requires that the functions used in a deep network be differentiable.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_20-47-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Superpixels optimized for semantic segmentation (Jampani et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Superpixels optimized for semantic segmentation (Jampani et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Superpixel Sampling Networks, proposed by Jampani et al., introduce the first attempt at integrating superpixel extraction methods with deep learning models (Jampani et al. 2018).
In this work, they adapt SLIC as a differentiable layer in a deep network which result in superpixels that are fine-tuned for specific tasks.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_21-45-09_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;Model diagram for SSN (Jampani et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;Model diagram for SSN (Jampani et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The train their model on a semantic segmentation task which fine tunes the learned superpixels such that they adhere more closely to segmentation boundaries.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_21-51-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 11: &amp;lt;/span&amp;gt;Results on semantic segmentation (Jampani et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;Results on semantic segmentation (Jampani et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In a more recent work, Yang et al. propose a deep network that directly produces the superpixels as opposed to using a soft K-Means layer (Yang et al. 2020).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_22-05-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 12: &amp;lt;/span&amp;gt;Model comparison between Jampani et al. and Yang et al. (Yang et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 12: &lt;/span&gt;Model comparison between Jampani et al. and Yang et al. (Yang et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Similar to SSN, they experiment on the Berkeley Image Segmentation Dataset.
Their results are competitive with other deep learning-based approaches.
The authors note that their method generalizes better in segmentation tasks by being robust to fine details and noise.
Additionally, their model runs at 50 fps using 4 NVIDIA Titan Xp GPUs.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_22-14-22_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 13: &amp;lt;/span&amp;gt;Comparison of results on competing methods (Yang et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 13: &lt;/span&gt;Comparison of results on competing methods (Yang et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;#print_bibliography: t&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Active Contours</title>
      <link>https://ajdillhoff.github.io/notes/active_contours/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/active_contours/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parametric-representation&#34;&gt;Parametric Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#motivation-of-the-fundamental-snake-equation&#34;&gt;Motivation of the Fundamental Snake Equation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#external-force&#34;&gt;External Force&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#energy-minimization&#34;&gt;Energy Minimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#iterative-solution&#34;&gt;Iterative Solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#applications&#34;&gt;Applications&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.cs.ait.ac.th/~mdailey/cvreadings/Kass-Snakes.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;http://www.cs.ait.ac.th/~mdailey/cvreadings/Kass-Snakes.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.spiedigitallibrary.org/conference-proceedings-of-spie/4322/0000/Statistical-models-of-appearance-for-medical-image-analysis-and-computer/10.1117/12.431093.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.spiedigitallibrary.org/conference-proceedings-of-spie/4322/0000/Statistical-models-of-appearance-for-medical-image-analysis-and-computer/10.1117/12.431093.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://web.mat.upc.edu/toni.susin/files/SnakesAivru86c.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://web.mat.upc.edu/toni.susin/files/SnakesAivru86c.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Snakes&lt;/strong&gt;, as named by Kass et al., is a spline curve that is minimized such that it moves towards distinct image features such as edges.
The closed curve, or snake, can be thought of as a rubber band.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;Introduction/2022-02-23_08-37-57_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Example of snake snapping to object. (Copyright 2018, 2008 Pearson Education, Inc.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Example of snake snapping to object. (Copyright 2018, 2008 Pearson Education, Inc.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;When stretched out, the band has an internal potential energy that forces the band to close in around some rigid object which exerts force against the band&amp;rsquo;s internal energy.
This method does not tout itself it be a fully autonomous way to segment interesting features.
Instead, it is useful in semi-supervised settings where the user knows a general region of interest.
The minimization of the snake will segment the desired object under reasonable settings.&lt;/p&gt;
&lt;h2 id=&#34;parametric-representation&#34;&gt;Parametric Representation&lt;/h2&gt;
&lt;p&gt;The contour is represented as a curve using a parametric representation:&lt;/p&gt;
&lt;p&gt;\[
(x, y) = (g(s), h(s)).
\]&lt;/p&gt;
&lt;p&gt;The argument \(s\) can be thought of as the trajectory of the curve.&lt;/p&gt;
&lt;p&gt;Parametric representations are a natural choice for representing curves in computing due to their compact representation.
For example, a circle is defined by \(x^2 + y^2 = r^2\). The individual values \(x\) and \(y\) are&lt;/p&gt;
&lt;p&gt;\begin{align*}
x &amp;amp;= r\cos(s)\\
y &amp;amp;= r\sin(s)
\end{align*}&lt;/p&gt;
&lt;p&gt;It can then be shown that&lt;/p&gt;
&lt;p&gt;\begin{align*}
x^2 + y^2 &amp;amp;= r^2\cos^2(s) + r^2\sin^2(s)\\
&amp;amp;= r^2\big(\cos^2(s) + \sin^2(s)\big)\\
&amp;amp;= r^2.
\end{align*}&lt;/p&gt;
&lt;p&gt;As a vector, we can represent \((x, y)\) as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{c}=
\begin{bmatrix}
x\\
y
\end{bmatrix}=
\begin{bmatrix}
x\\
\pm \sqrt{r^2 - x^2}
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Using the more efficient parametric representation, this vector is defined as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{c}=
\begin{bmatrix}
r\cos(s)\\
r\sin(s)
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Another example using parametric representation is that of spline curves.&lt;/p&gt;
&lt;p&gt;\begin{align*}
x(t) &amp;amp;= a_xt^3 + b_xt^2 + c_xt + dx\\
y(t) &amp;amp;= a_yt^3 + b_yt^2 + c_yt + dy
\end{align*}&lt;/p&gt;
&lt;p&gt;Then, \(\mathbf{f}(t) = (x(t), y(t))\).&lt;/p&gt;
&lt;h2 id=&#34;motivation-of-the-fundamental-snake-equation&#34;&gt;Motivation of the Fundamental Snake Equation&lt;/h2&gt;
&lt;p&gt;Given a vector \(\mathbf{c}(s) = \big(x(s), y(s)\big)\) normalized such that \(0 \leq s \leq 1\), an energy function is defined based on internal and external forces:&lt;/p&gt;
&lt;p&gt;\[
E(\mathbf{c}) = E_{int} + E_{ext}.
\]&lt;/p&gt;
&lt;p&gt;As the snake is updated iteratively, its final position should be one such that the energy \(E\) is minimized.&lt;/p&gt;
&lt;p&gt;The internal energy function is given as&lt;/p&gt;
&lt;p&gt;\[
E_{int} = \frac{\alpha}{2}\|\mathbf{c}&amp;rsquo;(s)\|^2 + \frac{\beta}{2}\|\mathbf{c}&amp;rsquo;&amp;rsquo;(s)\|^2,
\]&lt;/p&gt;
&lt;p&gt;where the first-order term is controlled by \(\alpha\) and the second-order term controlled by \(\beta\).
The first-order term gives the snake an elastic quality that shrinks towards a rigid object.
The second-order term controls the siffness of the contour.&lt;/p&gt;
&lt;h2 id=&#34;external-force&#34;&gt;External Force&lt;/h2&gt;
&lt;p&gt;The external force \(E_{ext}\) is based on the magnitude of the image gradient:&lt;/p&gt;
&lt;p&gt;\[
E_{img}(x, y) = \|\nabla f(x, y)\|^2.
\]&lt;/p&gt;
&lt;p&gt;Additionally, the gradient vectors are recorded. The combination of the two serve to represent a force field of the edge map.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-27_22-35-31_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Force field using the edge map using normalized gradients. Source: Pearson Education, Inc.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Force field using the edge map using normalized gradients. Source: Pearson Education, Inc.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;energy-minimization&#34;&gt;Energy Minimization&lt;/h2&gt;
&lt;p&gt;The total energy of the snake is then&lt;/p&gt;
&lt;p&gt;\[
E(\mathbf{c}(s)) = \int_0^1 \frac{\alpha}{2}\|\mathbf{c}&amp;rsquo;(s)\|^2 ds + \int_0^1 \frac{\beta}{2}\|\mathbf{c}&amp;rsquo;&amp;rsquo;(s)\|^2 ds + \int_0^1 E_{img}(\mathbf{c}(s))ds.
\]&lt;/p&gt;
&lt;p&gt;To find the minimum energy, we write the above equation as a function \(F\) and take its derivative with respect to s.
The minimum energy must satisfy&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial}{\partial s}\Big(\frac{\partial F}{\partial\mathbf{c}&amp;rsquo;}\Big) - \frac{\partial^2}{\partial s^2} \Big(\frac{\partial F}{\partial \mathbf{c}&amp;rsquo;&amp;rsquo;}\Big) - \frac{\partial F}{\partial \mathbf{c}} = 0.
\]&lt;/p&gt;
&lt;p&gt;Solving for the partials above yields&lt;/p&gt;
&lt;p&gt;\[
\alpha \mathbf{c}&amp;rsquo;&amp;rsquo; - \beta \mathbf{c}&amp;rsquo;&amp;rsquo;&amp;rsquo;&amp;rsquo; - \nabla E_{img} = 0.
\]&lt;/p&gt;
&lt;p&gt;Since the derivative of energy is a force, and the external force of the object is against the internal force of the snake, we can write&lt;/p&gt;
&lt;p&gt;\[
\nabla E_{img} = -\mathbf{F}.
\]&lt;/p&gt;
&lt;p&gt;Under this perspective, the minimum energy is found when&lt;/p&gt;
&lt;p&gt;\[
\alpha \mathbf{c}&amp;rsquo;&amp;rsquo;(s) - \beta \mathbf{c}&amp;rsquo;&amp;rsquo;&amp;rsquo;&amp;rsquo;(s) + \mathbf{F}(\mathbf{c}(s)) = 0.
\]&lt;/p&gt;
&lt;h2 id=&#34;iterative-solution&#34;&gt;Iterative Solution&lt;/h2&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-27_22-28-54_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;From top left to bottom right: initial snake, 10 steps, 50, 100, 150, 200. Source: Pearson Education, Inc.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;From top left to bottom right: initial snake, 10 steps, 50, 100, 150, 200. Source: Pearson Education, Inc.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To solve this as an iterative process over time \(t\), we write the force vector \(\mathbf{F}\) in terms of its 2D components dependent on \(t\):&lt;/p&gt;
&lt;p&gt;\[
\mathbf{F}(\mathbf{c}(s, t)) = \mathbf{F}(x(s, t), y(s, t)) = \begin{bmatrix}
F_x (x(s, t), y(s, t))\\
F_y (x(s, t), y(s, t))
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;For the internal energy components, we take the partial derivative of \(\mathbf{c}\) with respect to time:&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathbf{c}(s, t)}{\partial t} = \begin{bmatrix}
\frac{\partial x(s, t)}{\partial t}\\
\frac{\partial y(s, t)}{\partial t}\\
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;These derivatives rely on second and fourth order derivatives. For \(x(s, t)\), this is&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial x(s, t)}{\partial t} = \alpha \frac{\partial^2 x(s, t)}{\partial s^2} - \beta \frac{\partial^4 x(s, t)}{\partial s^4} + F_x(x(s, t), y(s, t)).
\]&lt;/p&gt;
&lt;p&gt;The partial for \(y\) follows a similar formulation. These derivatives are approximated using finite differences.
The second order derivative is approximated as&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial^2 x(s, t)}{\partial s^2} = x&amp;rsquo;&amp;rsquo;(k, t) = x(k + 1, t) - 2x(k, t) + x(k-1, t),
\]&lt;/p&gt;
&lt;p&gt;and the fourth order derivative is approximated as&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial^4 x(s, t)}{\partial s^4} = x&amp;rsquo;&amp;rsquo;&amp;rsquo;&amp;rsquo;(k, t) = x(k + 2, t) - 4x(k + 1, t) + 6x(k, t) - 4x(k-1, t) + x(k-2, t).
\]&lt;/p&gt;
&lt;p&gt;The finite differences can be written in matrix form as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Pentadiagonal_matrix&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;pentadiagonal banded matrix&lt;/a&gt;.
Adding back the external force yields a much simpler equation:&lt;/p&gt;
&lt;p&gt;\[
Dx + F_x(\mathbf{x}(t), \mathbf{y}(t)) = 0,
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
D = \alpha D_2 - \beta D_4,
\]&lt;/p&gt;
&lt;p&gt;the matrix of finite differences.&lt;/p&gt;
&lt;p&gt;Solving the above equation involves taking a finite step in time multiplied by the product of the negative time derivatives.
To simplify this process further, an assumption is made that the external force remains constant over time.&lt;/p&gt;
&lt;p&gt;\[
D\mathbf{x}(t) + \mathbf{F}_x(\mathbf{x}(t-1, \mathbf{y}(y-1)) = -\gamma(\mathbf{x}(t) - \mathbf{x}(t-1))
\]&lt;/p&gt;
&lt;p&gt;This is solved using matrix inversion yielding the final update for time \(t\) as&lt;/p&gt;
&lt;p&gt;\[
\mathbf{x}(t) = A[\mathbf{x}(t-1) + \gamma \mathbf{F}_x(\mathbf{x}(t-1), \mathbf{y}(t-1))],
\]&lt;/p&gt;
&lt;p&gt;where \(A = [I - D]^{-1}\).&lt;/p&gt;
&lt;p&gt;The step for \(\mathbf{y}(t)\) follows a similar formulation.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-27_22-24-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Snake transition between time steps. Source: Pearson Education, Inc.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Snake transition between time steps. Source: Pearson Education, Inc.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;applications&#34;&gt;Applications&lt;/h2&gt;
&lt;p&gt;In the original paper, the authors show an application where an initial frame of video is initialize with a snak by hand to track the contours of a mouth.
From that point, the shape automatically matches the subsequent frames.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-27_22-32-03_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Snakes for motion tracking (Kass et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Snakes for motion tracking (Kass et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Hidden Markov Models</title>
      <link>https://ajdillhoff.github.io/notes/hidden_markov_models/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/hidden_markov_models/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-markov-assumption&#34;&gt;The Markov Assumption&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-viterbi-algorithm&#34;&gt;The Viterbi Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#estimating-parameters&#34;&gt;Estimating Parameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#expectation-maximization&#34;&gt;Expectation Maximization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This article is essentially a grok of a tutorial on HMMs by (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;RABINER 1989&lt;/a&gt;). It will be useful for the reader to reference the &lt;a href=&#34;https://courses.physics.illinois.edu/ece417/fa2017/rabiner89.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;original paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Up to this point, we have only explored &amp;ldquo;atomic&amp;rdquo; data points.
That is, all of the information about a particular sample is encapsulated into one vector.
Sequential data is easily represented by graphical models.
This article introduces Hidden Markov Models, a powerful probabilistic graphical model used in many applications from gesture recognition to natural language processing.&lt;/p&gt;
&lt;p&gt;There are many tasks for which we do not know the underlying process.
However, we can observe samples that are produced from such processes.
Music, gesture recognition, speech, text, etc.
All of these have some underlying process which forms their outputs together into a hopefully coherent sequence.
If we wish to make predictions about future samples given these sequences, we will need to make some guess
about the underlying processes defining their output.&lt;/p&gt;
&lt;h2 id=&#34;the-markov-assumption&#34;&gt;The Markov Assumption&lt;/h2&gt;
&lt;p&gt;Markov models make a convenient assumption about sequential data.
That is, all relevant information required for predicting future samples is captured in the current time step \(t\).
Given a joint distribution over an input of \(T\) frames, \(p(\mathbf{x}_{1:T})\), the Markov assumption allows us to represent it as&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{x}_{1:T}) = p(\mathbf{x}_1)\prod_{t=2}^T p(\mathbf{x}_t|\mathbf{x}_{t-1})
\]&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;A more complicated case is when we are attempting to model some unknown process that is responsible for the observations.
In this case, an ordinary Markov chain is not sufficient.
A &lt;strong&gt;hidden Markov model (HMM)&lt;/strong&gt; is defined by a set \(z_t \in \{1, \dots, K\}\) of discrete hidden states and an &lt;strong&gt;observation&lt;/strong&gt; model \(p(\mathbf{x}_i|z_t)\).
The joint probability distribution of this model is given by&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{z}, \mathbf{x}) = p(\mathbf{z})p(\mathbf{x}|\mathbf{z}) = \Big(p(z_1)\prod_{t=2}^Tp(z_t|z_{t-1})\Big)\Big(\prod_{t=1}^Tp(\mathbf{x}_t|z_t)\Big).
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-24_20-41-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;The observations y are generated by the latent states x. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;The observations y are generated by the latent states x. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Although the states themselves are discrete, the observations may be continuous: \(p(\mathbf{x}|z_t, \mathbf{\theta})\).
If they are discrete, they can be modeled by an observation matrix \(B\).
Continuous observations are typically modeled using a conditional Gaussian:&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{x}_t|z_t=k, \theta) = \mathcal{N}(\mathbf{x}_t|\mathbf{\mu}_k,\mathbf{\Sigma}_k).
\]&lt;/p&gt;
&lt;p&gt;Following &lt;a href=&#34;https://courses.physics.illinois.edu/ece417/fa2017/rabiner89.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Rabiner&lt;/a&gt;, an HMM can be characterized by&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The number of states in the model \(N\).&lt;/li&gt;
&lt;li&gt;The number of distinct observation symbols per state \(M\).&lt;/li&gt;
&lt;li&gt;The state probability distribution \(A = \{a_{ij}\}\), \(a_{ij} = p(z_t=j | z_{t-1} = i)\).&lt;/li&gt;
&lt;li&gt;The observation symbol probability distribution \(B = \{b_j(k)\} = p(\mathbf{x}_t = k|z_t = j)\).&lt;/li&gt;
&lt;li&gt;An initial state distribution \(\mathbf{\pi}_i = p(z_t = i)\).&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-24_20-42-34_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;HMM with observation probabilities and state transition probabilities. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;HMM with observation probabilities and state transition probabilities. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The observation probability distribution is commonly modeled as a Gaussian, Mixture of Gaussians, or Multinomial distribution. Thus, the parameter estimates for those distributions follow the likelihood estimates for each respective distribution.&lt;/p&gt;
&lt;p&gt;In his famous tutorial on HMMs, Rabiner addressed the three fundamental problems of HMMs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Given an observation sequence and model parameters, how do we compute the probability of the observation sequence given the parameters (likelihood)?&lt;/li&gt;
&lt;li&gt;Given an observation sequence and model parameters, how do we choose a state sequence which is optimal (decoding)?&lt;/li&gt;
&lt;li&gt;How do we adjust the model parameters (learning)?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;HMMs are able to solve several different inference problems.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Filtering&lt;/strong&gt; computes \(p(z_t | \mathbf{x}_{1:t})\). That is, we are computing this probability as new samples come in up to time \(t\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smoothing&lt;/strong&gt; is accomplished when we have all the data in the sequence.
This is expressed as \(p(z_t|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fixed lag smoothing&lt;/strong&gt; allows for a trade off between accuracy and delay. It is useful in cases where we might not have the full sequence, but we wish to compute \(p(z_{t-l}|\mathbf{x}_{1:t})\) for some \(l &amp;gt; 0\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Predictions&lt;/strong&gt; are represented as \(p(z_{t+h}|\mathbf{x}_{1:t})\), where \(h &amp;gt; 0\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MAP estimation&lt;/strong&gt; yields the most probably state sequence \(\text{arg}\max_{\mathbf{z}_{1:T}}p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;We can sample the &lt;strong&gt;posterior&lt;/strong&gt; \(p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;We can also compute \(p(\mathbf{x}_{1:T})\) by summing up over all hidden paths. This is useful for classification tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;We start by solving the first problem posited by (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;RABINER 1989&lt;/a&gt;).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Given an observation sequence and model parameters, how do we compute the probability of the observation sequence given the parameters?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That is, given some model parameters \(\lambda = (A, B, \pi)\), compute \(p(z_t|\mathbf{x}_{1:t})\).&lt;/p&gt;
&lt;h3 id=&#34;forwards-pass&#34;&gt;Forwards Pass&lt;/h3&gt;
&lt;p&gt;The forwards algorithm solves two problems of interest.
First, we want to know how well our current parameters explain the observation sequence.
That is, \(p(\mathbf{x}_{1:T}|\lambda)\).&lt;/p&gt;
&lt;p&gt;Second, we want to compute \(p(z_t | \mathbf{x}_{1:t})\).
To compute these in an efficient way, a recursive strategy is adopted.
Let the forward variable \(\alpha_t(i)\) be defined as&lt;/p&gt;
&lt;p&gt;\[
\alpha_t(i) = p(\mathbf{x}_{1:t}, z_t = i | \lambda).
\]&lt;/p&gt;
&lt;p&gt;The forwards algorithm is defined as 3 steps.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Initialization:&lt;/p&gt;
&lt;p&gt;\[
\alpha_1(i) = \pi_i b_i(\mathbf{x}_1),\quad 1 \leq i \leq N.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recursion:&lt;/p&gt;
&lt;p&gt;\[
\alpha_{t+1}(j) = \Big(\sum_{i=1}^N \alpha_t(i)a_{ij}\Big)b_j(\mathbf{x}_{t+1}),\quad 1 \leq t \leq T - 1,\quad 1 \leq j \leq N
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Termination:&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{x}_{1:T})  = \sum_{i=1}^N \alpha_T(i).
\]&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The recursive step is visualized as a lattice structure &lt;a href=&#34;#figure--lattice&#34;&gt;as seen below.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;figure--lattice&#34;&gt;&lt;/a&gt;&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-24_20-12-13_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;From Rabiner 1989.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;From Rabiner 1989.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;With this step, we have a solution for the first problem.
We can now calculate more efficiently the probability of our observations given the current model parameters.
This along with the following backwards pass will be essential for updating our model parameters.&lt;/p&gt;
&lt;p&gt;The forwards algorithm is also used to solve the &lt;strong&gt;filtering&lt;/strong&gt; problem.
To see how, consider \(p(z_t | \mathbf{x}_{1:t-1})\) right before time \(t\).&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(z_t=j|\mathbf{x}_{1:t-1}) = \sum_i p(z_t=j|z_{t-1}=i)p(z_{t-1}=i|\mathbf{x}_{1:t-1})
\end{equation*}&lt;/p&gt;
&lt;p&gt;When we update for time \(t\), we have that&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(z_t=j|\mathbf{x}_{1:t}) &amp;amp;= p(z_t=j|\mathbf{x}_t, \mathbf{x}_{1:t})\\
&amp;amp;=\frac{p(\mathbf{x}_t|z_t=j, \mathbf{x}_{1:t-1})p(z_t=j|\mathbf{x}_{1:t-1})}{p(\mathbf{x}_t|\mathbf{x}_{t-1})}
\end{align*}&lt;/p&gt;
&lt;p&gt;However, \(\mathbf{x}_{1:t-1}\) is conditionally independent given \(z_t\), so it becomes&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(z_t=j|\mathbf{x}_{1:t})=\frac{p(\mathbf{x}_t|z_t=j)p(z_t=j|\mathbf{x}_{1:t-1})}{p(\mathbf{x}_t|\mathbf{x}_{t-1})}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Writing out \(p(z_t=j|\mathbf{x}_{1:t-1})\) fully yields&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(z_t=j|\mathbf{x}_{1:t}) \propto p(\mathbf{x}_t|z_t=j)\sum_i p(z_t=j|z_{t-1}=i)p(z_{t-1}=i|\mathbf{x}_{1:t-1}).
\end{equation*}&lt;/p&gt;
&lt;p&gt;This is the recursion step from above!&lt;/p&gt;
&lt;p&gt;This can also be represented in terms of the \(\alpha\) variables from above. To compute \(p(z_t=i|\mathbf{x}_{1:t})\), we can use the definition of a conditional probability distribution:&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(z_t=i|\mathbf{x}_{1:t}) &amp;amp;= \frac{p(z_t=i, \mathbf{x}_{1:t})}{p(\mathbf{x}_{1:t})}\\
&amp;amp;= \frac{\alpha_t(i)}{\sum_{j=1}^N \alpha_t(j)}
\end{align*}&lt;/p&gt;
&lt;p&gt;Compared to the complexity of the explicit representation, the forwards pass needs only \(N^2T\) calculations.
As pointed out in (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;RABINER 1989&lt;/a&gt;), with 5 hidden states and an observation sequence of length 100, the forwards pass only needs around 3000 computations.
A direct calculation would require \(10^{72}\).&lt;/p&gt;
&lt;h3 id=&#34;backwards-pass&#34;&gt;Backwards Pass&lt;/h3&gt;
&lt;p&gt;When updating the parameters of our model, we will need to consider the entire observation sequence.
The forward pass did not require the entire sequence.
Instead, we can compute the probability of the observation up to some time \(t\).
The backwards pass begins by defining the variable&lt;/p&gt;
&lt;p&gt;\[
\beta_t(i) = p(\mathbf{x}_{t+1:T} | z_t = i).
\]&lt;/p&gt;
&lt;p&gt;We can utilize a recursive process similar to the forwards algorithm with the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Initialization:&lt;/p&gt;
&lt;p&gt;\[
\beta_T(i) = 1,\quad 1 \leq i \leq N
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recursion:&lt;/p&gt;
&lt;p&gt;\[
\beta_t(i) = \sum_{j=1}^N a_{ij}b_j(\mathbf{x}_{t+1})\beta_{t+1}(j),\quad t = T-1,\dots,1,\quad 1 \leq i \leq N.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Termination:&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{x}_{1:T}) = \sum_{j=1}^N \pi_j b_j(x_1) \beta_1(j)
\]&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The complexity of the backwards algorithm is similar to that of the forwards: \(N^2T\).&lt;/p&gt;
&lt;p&gt;With both the forward and backwards passes defined, we can compute the &lt;strong&gt;smoothing&lt;/strong&gt; problem:&lt;/p&gt;
&lt;p&gt;\[
p(z_t=i|\mathbf{x}_{1:T}) = \frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N \alpha_t(j)\beta_t(j)}
\]&lt;/p&gt;
&lt;h2 id=&#34;the-viterbi-algorithm&#34;&gt;The Viterbi Algorithm&lt;/h2&gt;
&lt;p&gt;With problem 1 out of the way, we turn our attention to problem 2.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Given an observation sequence and model parameters, how do we choose a state sequence which is optimal?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
\mathbf{z}^* = \text{arg}\max_{\mathbf{z}_{1:T}}p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})
\]&lt;/p&gt;
&lt;p&gt;With respect to the &lt;a href=&#34;#figure--lattice&#34;&gt;lattice diagram&lt;/a&gt;, this is equivalent to computing the shortest path.
This is accomplished via the &lt;strong&gt;Viterbi&lt;/strong&gt; algorithm, sometimes referred to as the max-sum algorithm.
As with the forwards-backwards algorithm, the Viterbi algorithm takes on a recursive approach.
It starts by defining an intermediate variable&lt;/p&gt;
&lt;p&gt;\[
\gamma_t(i) = p(z_t=i|\mathbf{x}_{1:T}).
\]&lt;/p&gt;
&lt;p&gt;Using the variables defined in the forwards-backwards algorithm, this can be expressed as&lt;/p&gt;
&lt;p&gt;\[
\gamma_t(i) = \frac{\alpha_t(i) \beta_t(i)}{\sum_{i=1}^N \alpha_t(i) \beta_t(i)}.
\]&lt;/p&gt;
&lt;p&gt;This \(\gamma_t(i)\), we can compute the most likely state at time \(t\):&lt;/p&gt;
&lt;p&gt;\[
z_t^* = \text{arg}\max_{1\leq i \leq N} \gamma_t(i), \quad 1 \leq t \leq T.
\]&lt;/p&gt;
&lt;p&gt;One problem with this approach alone is that the most likely state at a particular time \(t\) may not lead us to the most probable sequence of states.
As stated above, we need to maximize \(p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).
In order to tackle this efficiently, Viterbi employs a dynamic programming approach.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Initialization&lt;/p&gt;
&lt;p&gt;Start with the best initial state out of all states given the observation at \(t=1\).
Additionally, we want to record the index of each state through time so that the best path can be retraced.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\delta_1(i) &amp;amp;= \pi_i b_i(\mathbf{x}_1),\quad 1 \leq i \leq N\\
\psi_1(i) &amp;amp;= 0
\end{align*}&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recursion:&lt;/p&gt;
&lt;p&gt;The quantity \(\delta_t(i)\) represents the joint probability of state sequences and observations up to time \(t\) ending with state \(z_t=i\).
Thus, the recursive step is to maximize the probability of the intermediate output for \(t-1\):&lt;/p&gt;
&lt;p&gt;\[
\delta_t(j) = \max_{1 \leq i \leq N} (\delta_{t-1}(i) a_{ij})b_j(\mathbf{x}_t), \quad 2 \leq t \leq T,\quad 1 \leq j \leq N.
\]&lt;/p&gt;
&lt;p&gt;The corresponding index for this step is recorded in the path matrix:&lt;/p&gt;
&lt;p&gt;\[
\psi_t(j) = \text{arg}\max_{1 \leq i \leq N} \delta_{t-1}(i)a_{ij},\quad 2 \leq t \leq T,\quad 1 \leq j \leq N.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Termination&lt;/p&gt;
&lt;p&gt;The last step of the Viterbi algorithm completes the calcuation of the joint probability of state sequences and observations.&lt;/p&gt;
&lt;p&gt;\[
p^* = \max_{1 \leq i \leq N} \delta_T(i)
\]&lt;/p&gt;
&lt;p&gt;\[
\mathbf{z}_T^* = \text{arg}\max_{1 \leq i \leq N} \delta_T(i)
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Path Backtrace&lt;/p&gt;
&lt;p&gt;With the state sequence matrix recorded along the way, we can retrace it to get the most probable sequence:&lt;/p&gt;
&lt;p&gt;\[
z_t^* = \psi_{t+1}(z_{t+1}^*),\quad t = T-1, \cdots, 1.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;estimating-parameters&#34;&gt;Estimating Parameters&lt;/h2&gt;
&lt;p&gt;If the hidden states were fully observable, then updating our model parameters would be as straightforward as computing the maximum likelihood estimates for the model parameters \(\lambda = (A, B, \pi)\).
For \(A\) and \(\pi\), we first tally up the following counts:&lt;/p&gt;
&lt;p&gt;\[
\hat{a}_{ij} = \frac{N_{ij}}{\sum_j N_{ij}},
\]&lt;/p&gt;
&lt;p&gt;the number of times we expect to transition from \(i\) to \(j\) divided by the number of times we transition from \(i\) to any other state.&lt;/p&gt;
&lt;p&gt;For \(\pi\), we have&lt;/p&gt;
&lt;p&gt;\[
\hat{\pi_i} = \frac{N_i}{\sum_i N_i},
\]&lt;/p&gt;
&lt;p&gt;The number of times we expect to start in state \(i\) divided by the number of times we start in any other state.&lt;/p&gt;
&lt;p&gt;Estimating the parameters for \(B\) depends on which distribution we are using for our observation probabilities.
For a multinomial distribution, we would compute the number of times we are in state \(j\) and observe a symbol \(k\) divided by the number of times we are in state \(j\):&lt;/p&gt;
&lt;p&gt;\[
\hat{B}_{jk} = \frac{N_{jk}}{N_k},
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
N_{jk} = \sum_{i=1}^N \sum_{t=1}^T \mathbb{1} (z_{i, t}=j, x_{i, t}=k).
\]&lt;/p&gt;
&lt;p&gt;If the observation probability follows a Gaussian distribution, the MLEs for \(\mu\) and \(\mathbf{\Sigma}\) are&lt;/p&gt;
&lt;p&gt;\[
\hat{\mathbf{\mu}}_k = \frac{\bar{\mathbf{x}}_k}{N_k},\quad \hat{\mathbf{\Sigma}}_k = \frac{(\bar{\mathbf{x}}\bar{\mathbf{x}})_k^T - N_k \hat{\mathbf{\mu}}_k\hat{\mathbf{\mu}}_k^T}{N_k},
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\bar{\mathbf{x}}_k = \sum_{i=1}^N \sum_{t=1}^T \mathbb{1}(z_{i, t}=k)\mathbf{x}_{i, t}
\]&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;\[
(\bar{\mathbf{x}}\bar{\mathbf{x}})_k^T) = \sum_{i=1}^N \sum_{t=1}^T \mathbb{1} (z_{i, t}=k)\mathbf{x}_{i,k}\mathbf{x}_{i,k}^T.
\]&lt;/p&gt;
&lt;h2 id=&#34;expectation-maximization&#34;&gt;Expectation Maximization&lt;/h2&gt;
&lt;p&gt;Of course, HMMs have hidden states which are not fully observable.
Thus, we need to come up with another strategy for updating our parameters based on the observable data.
The intuition behind this approach is as follows.
We first start out by using our current parameters to estimate the missing data, making it complete.
Initially, we may randomize our estimates if we have no good heuristic or guess as to what they should be.&lt;/p&gt;
&lt;p&gt;With the completed data, we can update our current parameters.
In other words, the expected values of the sufficient statistics can be derived now that the data has been filled in.
A new set of parameters is found such that it maximizes the likelihood function with respect to the estimated data.&lt;/p&gt;
&lt;h3 id=&#34;e-step&#34;&gt;E Step&lt;/h3&gt;
&lt;p&gt;Following (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;RABINER 1989&lt;/a&gt;), we start with the joint probability of being in state \(i\) at time \(t\) and state \(j\) at time \(t+1\):&lt;/p&gt;
&lt;p&gt;\[
\xi_t(i, j) = p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T}).
\]&lt;/p&gt;
&lt;p&gt;This can be computed using the forwards-backwards algorithm:&lt;/p&gt;
&lt;p&gt;\[
\xi_t(i, j) = \frac{\alpha_t(i)a_{ij}b_j(\mathbf{x}_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^N \sum_{j=1}^N \alpha_t(i)a_{ij}b_j(\mathbf{x}_{t+1})\beta_{t+1}(j)}.
\]&lt;/p&gt;
&lt;p&gt;This can be related back to \(\gamma_t(i)\) by summing over over \(j\):&lt;/p&gt;
&lt;p&gt;\[
\gamma_t(i) = \sum_{j=1}^N \xi_t(i, j).
\]&lt;/p&gt;
&lt;p&gt;Here, \(\gamma_t(i)\) is the expected number of times we transition from \(z = i\).
Summing over all \(t\) yields the expected transitions from \(z_i\) over all time steps:&lt;/p&gt;
&lt;p&gt;\[
\sum_{t=1}^{T-1} \gamma_t(i).
\]&lt;/p&gt;
&lt;p&gt;Since \(\xi_t(i, j)\) is the expected transition from \(i\) to \(j\) at time \(t\), we can compute the total number of transitions from \(i\) to \(j\) via&lt;/p&gt;
&lt;p&gt;\[
\sum_{t=1}^{T-1} \xi_t(i, j).
\]&lt;/p&gt;
&lt;h3 id=&#34;m-step&#34;&gt;M Step&lt;/h3&gt;
&lt;p&gt;The previous &lt;strong&gt;E Step&lt;/strong&gt; computed the expected values given the current parameter estimates.
Now that the data is complete, we can update our parameter estimates.
Starting with the transition probabilities, we must add the expected number of transitions from \(i\) to \(j\) and divide by the expected number of times we transition from \(i\).
Using the parameters from the E Step, this can be written&lt;/p&gt;
&lt;p&gt;\[
\hat{a}_{ij} = \frac{\sum_{t=1}^{T-1}\xi_t(i, j)}{\sum_{t=1}^{T-1}\gamma_t(i)}.
\]&lt;/p&gt;
&lt;p&gt;The initial state probability at \(t=1\) is the number of times we expect to be in state \(z=i\) at \(t=1\):&lt;/p&gt;
&lt;p&gt;\[
\gamma_1(i).
\]&lt;/p&gt;
&lt;p&gt;Finally, the observation probability parameters are updated by considering the number of times we are in state \(z=j\) and observing \(x=k\) divided by the number of times we are in state \(z=j\). Note that this is for a multinomial probabiliy distribution:&lt;/p&gt;
&lt;p&gt;\[
\hat{b}_j(k) = \frac{\sum_{t=1, x_t = k}^T \gamma_t(j)}{\sum_{t=1}^T \gamma_t(j)}.
\]&lt;/p&gt;
&lt;p&gt;These formulas are derived from maximizing Baum&amp;rsquo;s auxiliary function&lt;/p&gt;
&lt;p&gt;\[
Q(\lambda, \hat{\lambda}) = \sum_{Q} p(\mathbf{z}|\mathbf{x}, \lambda) \log p(\mathbf{x}, \mathbf{z}|\hat{\lambda})
\]&lt;/p&gt;
&lt;p&gt;over \(\hat{\lambda}\). It has further been shown that maximizing this function leads to increased likelihood:&lt;/p&gt;
&lt;p&gt;\[
\max_{\hat{\lambda}} Q(\lambda, \hat{\lambda}) \implies p(\mathbf{x}|\hat{\lambda}) \geq p(\mathbf{x}|\lambda).
\]&lt;/p&gt;
&lt;p&gt;If we have a Gaussian observation model, the values for \(\hat{b}_j(k)\) are computed to accommodate the parameters of the distribution.
These parameter estimates assume a Gaussian mixture model.
Starting with \(\hat{\mu}_{jk}\), it can be estimated by dividing the expected value of observations belonging to Gaussian density \(k\) by the expected number of times we are in state \(j\) using the \(k^{\text{th}}\) mixture component:&lt;/p&gt;
&lt;p&gt;\[
\hat{\mathbf{\mu}}_{jk} = \frac{\sum_{t=1}^T \gamma_t(j, k)\mathbf{x}_t}{\sum_{t=1}^T \gamma_t(j, k)}.
\]&lt;/p&gt;
&lt;p&gt;Here, \(\gamma_t(j, k)\) is the probability of being in state \(j\) at time \(t\) with the \(k^{\text{th}}\) mixture component accounting for \(\mathbf{x}_t\):&lt;/p&gt;
&lt;p&gt;\[
\gamma_t(j, k) = \frac{\alpha_t(j)\beta_t(j)}{\sum_{j=1}^N \alpha_t(j) \beta_t(j)} \frac{c_{jk}\mathcal{N}(\mathbf{x}_t, \mu_{jk}, \mathbf{\Sigma}_{jk})}{\sum_{m=1}^M c_{jm}\mathcal{N}(\mathbf{x}_t, \mu_{jm}, \mathbf{\Sigma}_{jm})}.
\]&lt;/p&gt;
&lt;p&gt;This method is proven to improve the parameters.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Each iteration is guaranteed to improve the log-likelihood function.&lt;/li&gt;
&lt;li&gt;The process is guaranteed to converge.&lt;/li&gt;
&lt;li&gt;The convergence point is a fixed point of the likelihood function.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These guarantees are similar to gradient ascent.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;RABINER, LAWRENCE R. 1989. “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.” &lt;i&gt;Proceedings of the Ieee&lt;/i&gt; 77 (2): 30.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Image Segmentation</title>
      <link>https://ajdillhoff.github.io/notes/image_segmentation/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/image_segmentation/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#defining-segmentations&#34;&gt;Defining Segmentations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#grouping&#34;&gt;Grouping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#segmentation-methods&#34;&gt;Segmentation Methods&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/&lt;/a&gt; (Berkeley Segmentation Database)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.15203v2&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/abs/2105.15203v2&lt;/a&gt; (SegFormer)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1703.06870&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://arxiv.org/abs/1703.06870&lt;/a&gt; (Mask R-CNN)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/sithu31296/semantic-segmentation&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://github.com/sithu31296/semantic-segmentation&lt;/a&gt; (Collection of SOTA models)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Feature extraction methods such as &lt;a href=&#34;https://ajdillhoff.github.io/notes/scale_invariant_feature_transforms/&#34;&gt;SIFT&lt;/a&gt; provide us with many distinct, low-level features that are useful for providing local descriptions images.
We now &amp;ldquo;zoom out&amp;rdquo; and take a slightly higher level look at the next stage of image summarization.
Our goal here is to take these low-level features and group, or fit, them together such that they represent a higher level feature.
For example, from small patches representing color changes or edges, we may wish to build higher-level feature representing an eye, mouth, and nose.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;Introduction/2022-02-23_08-12-56_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Capsule networks learn template components that make up handwritten images (Kosiorek et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Capsule networks learn template components that make up handwritten images (Kosiorek et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The goal of &lt;strong&gt;image segmentation&lt;/strong&gt; is to obtain a compact representation of distinct features in an image.
We will see that this task is loosely defined and will vary from application to application.
For example, in one task we may wish to segment individual people detected in an image.
In another task, we may wish to segment the clothes they are wearing to identify certain fashion trends or make
predictions about the time of year based on an image.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;Introduction/2022-02-22_09-22-07_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Mask R-CNN results on COCO dataset (He et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Mask R-CNN results on COCO dataset (He et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Popular methods for segmenting images use clustering or graph techniques.
Clustering methods will group local pixels together into regions based on similarity.
There are also a class of segmentation based on locating boundary curves called &lt;a href=&#34;https://ajdillhoff.github.io/notes/active_contours/&#34;&gt;Active Contours&lt;/a&gt;.
These segment distinct boundaries based on image features such as edges and are motivated by physical or mathematical properties.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;Introduction/2022-02-22_09-32-59_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Comparison results from SegFormer paper (Xie et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Comparison results from SegFormer paper (Xie et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;defining-segmentations&#34;&gt;Defining Segmentations&lt;/h2&gt;
&lt;p&gt;The task of image segmentation comes with ambiguity.
People will segment images in different ways depending on the task.&lt;/p&gt;
&lt;h2 id=&#34;grouping&#34;&gt;Grouping&lt;/h2&gt;
&lt;p&gt;At the pixel-level, features such as edges will look similar regardless of the context.
An edge segment from a car will look similar to that of a building at a small enough scale.
As soon as those features are grouped together, their representation completely changes.
A collection of edges becomes a square or a corner.
The higher-level of grouping we have, the more that the collection of features begin to diverge from each other.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.katnoria.com/gradvizv1/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;ConvNet Gradient Visualization&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Consider the famouse Muller-Lyer illusion seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-22_20-41-36_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;From Forsyth and Ponce &amp;#34;Computer Vision - A Modern Approach&amp;#34;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;From Forsyth and Ponce &amp;ldquo;Computer Vision - A Modern Approach&amp;rdquo;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Looking at the details individually, it is easier to see that the lines are the same length.
When considering the grouped features around them, they appear different.&lt;/p&gt;
&lt;p&gt;Another view of segmentation considers what is the figure versus what is the ground, or background.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-22_20-49-46_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;From Forsyth and Ponce &amp;#34;Computer Vision - A Modern Approach&amp;#34;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;From Forsyth and Ponce &amp;ldquo;Computer Vision - A Modern Approach&amp;rdquo;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Gestalt_psychology&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Gestalt school of psychology&lt;/a&gt; posited that grouping was the key to visual understanding.
Towards establishing a theory of segmentation, a set of factors was proposed:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Proximity&lt;/strong&gt; - Group elements that are close together.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Similarity&lt;/strong&gt; - Elements that share some sort of measurable similarity are grouped.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Common fate&lt;/strong&gt; - Often tied to temporal features, elements with a similar trajectory are grouped.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Common region&lt;/strong&gt; - Elements enclosed in a region are part of the same group. This region could be arbitrary.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallelism&lt;/strong&gt; - Parallel elements are grouped.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Closure&lt;/strong&gt; - Lined or curves that are closed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Symmetry&lt;/strong&gt; - Elements exhibiting some sort of symmetry. For example, a mirrored shaped.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Continuity&lt;/strong&gt; - Continuous curves are grouped.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Familiar configuration&lt;/strong&gt; - Lower level elements, when grouped together, form a higher level object.&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-22_21-03-09_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;From Forsyth and Ponce &amp;#34;Computer Vision - A Modern Approach&amp;#34;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;From Forsyth and Ponce &amp;ldquo;Computer Vision - A Modern Approach&amp;rdquo;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-22_21-04-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;From Forsyth and Ponce &amp;#34;Computer Vision - A Modern Approach&amp;#34;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;From Forsyth and Ponce &amp;ldquo;Computer Vision - A Modern Approach&amp;rdquo;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-24_18-23-16_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Sky and Water by M.C. Escher&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Sky and Water by M.C. Escher
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Intuiting applications for some of these rules is easier than others.
For example, familiar configuration suggests that some familiar object can be identified by the sum of its parts.
This is especially helpful for problems where the whole object is occluded.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-22_21-09-15_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Parts of the hand are occluded either by the hand itself or some object (Mueller et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Parts of the hand are occluded either by the hand itself or some object (Mueller et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Common fate is a useful rule when considering tracking an object or group of objects over a series of frames.
Even something a simple as frame differencing is an efficient preprocessing step to removing unrelated information.&lt;/p&gt;
&lt;h2 id=&#34;segmentation-methods&#34;&gt;Segmentation Methods&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ajdillhoff.github.io/notes/active_contours/&#34;&gt;Active Contours&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ajdillhoff.github.io/notes/segmentation_via_clustering-2/&#34;&gt;Segmentation via Clustering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Hough Transform</title>
      <link>https://ajdillhoff.github.io/notes/hough_transform/</link>
      <pubDate>Thu, 17 Feb 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/hough_transform/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#algorithm&#34;&gt;Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rectangle-detection-based-on-a-windowed-hough-transform&#34;&gt;Rectangle Detection based on a Windowed Hough Transform&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Fitting a model to a set of data by consensus, as in &lt;a href=&#34;https://ajdillhoff.github.io/notes/random_sample_consensus/&#34;&gt;RANdom SAmple Consensus&lt;/a&gt;, produces a parameter estimate that is robust to outliers. A similar technique for detecting shapes in images is the &lt;strong&gt;Hough Transform&lt;/strong&gt;.
Originally it was designed for detecting simple lines, but it can be extended to detect &lt;a href=&#34;https://en.wikipedia.org/wiki/Generalised_Hough_transform&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;arbitrary shapes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The transform is computed given an edge image. Each edge pixel in the image casts a vote for a line given a set of parameters.
This vote is added to an accumulator array which tallies the votes over all pixels for all parameter choices.&lt;/p&gt;
&lt;h2 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h2&gt;
&lt;p&gt;An accumulator array holds the votes for each edge point.
The indices to this array represent the line parameters \((\rho, \theta)\), where&lt;/p&gt;
&lt;p&gt;\[
\rho = x \cos \theta + y \sin \theta.
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-17_16-47-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Parameterization of a line in a Hough transform. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Parameterization of a line in a Hough transform. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In this parameterization, \((\rho, \theta)\) represents a vector that is normal to the line.
For each edge pixel in the original image, a range of \(\theta\) values are tested.
The resolution of \(\theta\) values used is set as a hyperparameter to the algorithm.
A vote is cast for every single line within the resolution of \(\theta\) by incrementing the accumulator array at entry \((\rho, \theta)\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-17_16-54-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Visualization of voting procedure. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Visualization of voting procedure. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;After all edge pixels are evaluated, the accumulator array must be post processed to select the lines with the most agreement.
Since the accumulator array is 2D, it can be visualized as seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-17_16-58-06_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Visualization of accumulator array. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Visualization of accumulator array. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;rectangle-detection-based-on-a-windowed-hough-transform&#34;&gt;Rectangle Detection based on a Windowed Hough Transform&lt;/h2&gt;
&lt;p&gt;As mentioned in the introduction, Hough transforms can be extended to detect arbitrary shapes.
In their &lt;a href=&#34;http://sibgrapi.sid.inpe.br/col/sid.inpe.br/banon/2004/08.03.17.14/doc/1.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;2004 publication&lt;/a&gt;, Jung and Schramm approach the problem of detecting rectangles in an image via Hough transforms.
By analyzing the spatial relationship of peaks in a standard Hough transform, rectangles can reliably be detected.&lt;/p&gt;
&lt;h3 id=&#34;detecting-rectangles-via-hough-peaks&#34;&gt;Detecting Rectangles via Hough peaks&lt;/h3&gt;
&lt;p&gt;If we consider pixel located at the center of a rectangle, a few key symmetries can be observed.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The detected peaks from the Hough window will be in pairs, equidistant from the center.&lt;/li&gt;
&lt;li&gt;Two pairs will be separated by \(90^{\circ}\) with respect to the \(\theta\) axis.&lt;/li&gt;
&lt;li&gt;The distance between the peaks of a pair are the sides of the rectangle.&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-19_16-33-06_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Rectangle centered at the origin.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Rectangle centered at the origin.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-19_16-33-56_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;The resulting Hough transform of the previous figure.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;The resulting Hough transform of the previous figure.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;the-algorithm&#34;&gt;The Algorithm&lt;/h3&gt;
&lt;p&gt;Each pixel in the image is evaluated using a sliding window approach.
Consider a pixel centered on \((x_c, y_c)\).
The size of the window determines the maximum size of the rectangle that can be detected in a given window.
The authors use a circular threshold with a minimum and maximum diameter \(D_{min}\) and \(D_{max}\).
That is, the search region is a circle such that the smallest detectable rectangle has a side length of no less than \(D_{min}\) and a diagonal length of no more than \(D_{max}\). The figure below visualizes this region.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-19_18-56-46_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Search region based on a circle. Source: Jung and Schramm 2004.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Search region based on a circle. Source: Jung and Schramm 2004.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;With the search region defined, a hough transform for that region is computed.
The paper mentions an optimization step when selecting the discretization steps \(d_{\theta}\) and \(d_{\rho}\).
If the input image is large, the resulting Hough transform will also be large.
They recommend picking \(d_{\theta} = \frac{3 \pi}{4 D_{max}}\) and \(d_{\rho} = \frac{3}{4}\).&lt;/p&gt;
&lt;p&gt;The Hough image is further processed in order to extra local extrema.
These peaks should correspond to the lines of the rectangle.
First, an enhanced image is created following&lt;/p&gt;
&lt;p&gt;\[
C_{\text{enh}}(\rho, \theta) = h w \frac{C(\rho, \theta)^2}{\int_{-h/2}^{h/2}\int_{-w/2}^{h/2} C(\rho + y, \theta + x)dx dy}.
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;How is such an integral computed?&lt;/strong&gt;&lt;/strong&gt;
The integral is the &amp;ldquo;area under the curve&amp;rdquo; of a given signal. In this case, the accumulator image is a 2D signal for which a discrete approximation of the integral can be computed.
This can be implemented via convolution.
The local maxima of this enhanced image are those such that \(C(\rho, \theta) \geq T_C\), where \(T_C = 0.5D_{min}\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-19_19-46-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Original Hough image (left) and enhanced (right) (Jung and Schramm 2004).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Original Hough image (left) and enhanced (right) (Jung and Schramm 2004).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;With the detected peaks from the enhanced image, four peaks are selected with satisfy the symmetries of rectangles as listed above. This is shown as equation 3 in the paper:&lt;/p&gt;
&lt;p&gt;\begin{align*}
&amp;amp;\Delta \theta = |\theta_i - \theta_j| &amp;lt; T_{\theta},\\
&amp;amp;\Delta \rho = |\rho_i + \rho_j| &amp;lt; T_{\rho},\\
&amp;amp;|C(\rho_{i}, \theta_i) - C(\rho_j, \theta_j)| &amp;lt; T_{L}\frac{C(\rho_i, \theta_i) + C(\rho_j, \theta_j)}{2}.
\end{align*}&lt;/p&gt;
&lt;p&gt;\(T_{\theta}\) is used to threshold peaks corresponding to parallel lines (\(\theta_i \approx \theta_j\)).
\(T_{\rho}\) is a threshold for symmetry (equal distance between lines and center).
\(T_L\) ensures that line segments have approximately the same length.&lt;/p&gt;
&lt;p&gt;For peaks that satisfy the equations above, an extended peak \(P_k = (\pm \xi, \alpha_k)\) is formed, where&lt;/p&gt;
&lt;p&gt;\[
\alpha_k = \frac{1}{2}(\theta_i + \theta_j) \text{ and } \xi_k = \frac{1}{2}|\rho_i - \rho_j|.
\]&lt;/p&gt;
&lt;p&gt;Finally, a rectangle is detected if the pairs of lines are orthogonal. That is, if&lt;/p&gt;
&lt;p&gt;\[
\Delta \alpha = ||\alpha_k - \alpha_l| - 90^{\circ}| &amp;lt; T_{\alpha}.
\]&lt;/p&gt;
&lt;p&gt;The rectangle parameters are encoded by \(\alpha_k\) being its orientation and \(2\xi_k\) and \(2\xi_l\) being its sides.&lt;/p&gt;
&lt;p&gt;The final step in the paper is to remove duplicates since, depending on the threshold choices, multiple candidates for a rectangle may be detected.
The intuition behind this step is to create an error measure that summarizes how well the symmetries defined by the conditions required for a rectangle are respected.&lt;/p&gt;
&lt;p&gt;\[
E(P_k, P_l) = \sqrt{a(\Delta \theta_k^2 + \Delta \theta_l^2 + \Delta \alpha^2) + b(\Delta \rho_k^2 + \Delta \rho_l^2)}
\]&lt;/p&gt;
&lt;p&gt;In this error measure, \(a\) and \(b\) are used to weight the angular and distance errors differently since a change in 1 pixel would be more significant than a change of 1 degree.&lt;/p&gt;
&lt;p&gt;If the difference in orientation for each line detected in the sides is greater, the orientation error increases.
Likewise, the more that the pairs of lines stray from orthogonality, the greater the error becomes.
For the distance measure, the more offset the lines are with respect to the center, the greater the error becomes.
Thus, the rectangle that meets these criteria best will have the lowest error.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RANdom SAmple Consensus</title>
      <link>https://ajdillhoff.github.io/notes/random_sample_consensus/</link>
      <pubDate>Tue, 15 Feb 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/random_sample_consensus/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#finding-the-best-fit-model&#34;&gt;Finding the Best Fit Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Unless our data is perfect, we will not be able to find parameters that fit the data in the presence of outliers.
Consider fitting the data in the figure below using a least squares method.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-20_19-46-07_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Points sample along a line with many outliers around it. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Points sample along a line with many outliers around it. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;If we were to fit a naive least squares model, the outliers would surely produce parameters for a line that does not fit the most amount of data possible.&lt;/p&gt;
&lt;p&gt;Consider the figures below. In the first one, a least squares model is fit to points generated from a line.
With the addition of just a single outlier, the model no longer fits the line.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-20_19-46-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Least squares can easily fit a line with great accuracy.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Least squares can easily fit a line with great accuracy.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-20_19-46-52_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;A single outlier leads to a bad fit for linear regression.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;A single outlier leads to a bad fit for linear regression.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Ideally, we want a model that is robust to outliers.
That is, the model should be fit such that it matches the largest number of samples, or &lt;strong&gt;inliers&lt;/strong&gt;.
One such approach to this problem is &lt;strong&gt;RANdom SAmple Consensus (RANSAC)&lt;/strong&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-20_19-47-10_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;RANSAC fit to most inliers while ignoring the outliers. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;RANSAC fit to most inliers while ignoring the outliers. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The general process is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Randomly select source samples and their matching targets.&lt;/li&gt;
&lt;li&gt;Fit a model to the data such that transforming the input by the model parameters yields a close approximation to the targets.&lt;/li&gt;
&lt;li&gt;Measure the error of how well ALL data fits and select the number of inliers with error less than \(t\).&lt;/li&gt;
&lt;li&gt;If the error is lower than the previous best error, fit a new model to these inliers.&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-20_19-47-30_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;RANSAC fitting random samples and counting the number of inliers. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;RANSAC fitting random samples and counting the number of inliers. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The algorithm can be found on the &lt;a href=&#34;https://en.wikipedia.org/wiki/Random_sample_consensus&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Wikipedia page&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;finding-the-best-fit-model&#34;&gt;Finding the Best Fit Model&lt;/h2&gt;
&lt;p&gt;When it comes to finding the parameters of a transformation matrix that converts points in one image to another, how do we solve for that matrix? We are looking for some \(A\) such that&lt;/p&gt;
&lt;p&gt;\begin{equation*}
A\mathbf{x} = \mathbf{\hat{x}}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;In a perfect world, \(\mathbf{\hat{x}}\) will match the target point \(\mathbf{y}\). In other words,&lt;/p&gt;
&lt;p&gt;\(\|\mathbf{\hat{x}} - \mathbf{y}\|_{2} = 0\).&lt;/p&gt;
&lt;p&gt;For an affine transformation, we would have some transformation matrix&lt;/p&gt;
&lt;p&gt;\begin{equation*}
A =
\begin{bmatrix}
a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\
a_{21} &amp;amp; a_{22} &amp;amp; a_{23}
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Then we compute each component of \(A\mathbf{x}\) as&lt;/p&gt;
&lt;p&gt;\begin{align*}
\hat{x}_1 &amp;amp;= a_{11} * x_1 + a_{12} * x_2 + a_{13} * 1\\
\hat{x}_2 &amp;amp;= a_{21} * x_1 + a_{22} * x_2 + a_{23} * 1\\
\end{align*}&lt;/p&gt;
&lt;p&gt;We can fit this using a least squares approach by the following construction.&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\begin{bmatrix}
x_1^{(1)} &amp;amp; x_2^{(1)} &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; x_1^{(1)} &amp;amp; x_2^{(1)} &amp;amp; 1\\
&amp;amp;&amp;amp; \vdots\\
x_1^{(n)} &amp;amp; x_2^{(n)} &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; x_1^{(n)} &amp;amp; x_2^{(n)} &amp;amp; 1\\
\end{bmatrix}
\begin{bmatrix}
a_{11}\\
a_{12}\\
a_{13}\\
a_{21}\\
a_{22}\\
a_{23}\\
\end{bmatrix}=
\begin{bmatrix}
\hat{x}_1^{(1)}\\
\hat{x}_2^{(1)}\\
\vdots\\
\hat{x}_1^{(n)}\\
\hat{x}_2^{(n)}\\
\end{bmatrix}
\end{equation*}&lt;/p&gt;
&lt;p&gt;We can solve this analytically! Recall the &lt;strong&gt;normal equations&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;\[
A^T A \mathbf{x} = A^T \mathbf{b}.
\]&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s test this on a couple of images&amp;hellip;&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-15_19-20-07_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Two images taken with matching features shared between them.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Two images taken with matching features shared between them.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;First, we use some feature detector such as &lt;a href=&#34;https://ajdillhoff.github.io/notes/scale_invariant_feature_transforms/&#34;&gt;SIFT&lt;/a&gt; to find keypoints in each image.
Then, we can take a brute force approach to determine which keypoints match between them.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-15_19-24-24_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;We&amp;#39;ve got a lot of potential matches here.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;We&amp;rsquo;ve got a lot of potential matches here.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;After running RANSAC, we end up with a model that fits the following inlier points.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-15_19-25-31_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Many of the outliers were removed and we are left with the following matches.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Many of the outliers were removed and we are left with the following matches.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;We can use the found transformation matrix to warp our source image to fit our destination image as seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-15_19-31-23_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Images stitched together... not perfect!&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Images stitched together&amp;hellip; not perfect!
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;finding-a-better-transformation&#34;&gt;Finding a better transformation&lt;/h3&gt;
&lt;p&gt;The transformation matrix was an affine transformation matrix.
What we really want is a projective transformation!
We can extend our approach to finding an affine matrix from earlier by remembering that projective transformations are completed following a perspective divide (usually denoted by \(w\)).&lt;/p&gt;
&lt;p&gt;Instead of a constant 1 in the third position of the affine vector, we have a value \(w\):&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\begin{bmatrix}
a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\\
a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\\
a_{31} &amp;amp; a_{32} &amp;amp; a_{33}\\
\end{bmatrix}
\begin{bmatrix}
x\\
y\\
w
\end{bmatrix}=
\begin{bmatrix}
\hat{x}\\
\hat{y}\\
\hat{w}
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Dividing by \(\hat{w}\) completes the perspective projection:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\begin{bmatrix}
\frac{\hat{x}}{\hat{w}}\\
\frac{\hat{y}}{\hat{w}}\\
1
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Again, we can write out the individual equation for each component as&lt;/p&gt;
&lt;p&gt;\begin{align*}
\hat{x} &amp;amp;= (a_{11} * x + a_{12} * y + a_{13} * w) \div (a_{31} * x + a_{32} * y + a_{33} * w)\\
\hat{y} &amp;amp;= (a_{21} * x + a_{22} * y + a_{23} * w) \div (a_{31} * x + a_{32} * y + a_{33} * w)\\
\end{align*}&lt;/p&gt;
&lt;p&gt;We may assume that \(w = 1\) for the original points (before transformation).
Additionally, \(a_{33}\) is typically set to 1 when constructing a transformation matrix.
These are safe enough assumptions to make considering that we will make many attempts at finding the best fitting parameters.&lt;/p&gt;
&lt;p&gt;Solving for \(\hat{x}\) and \(\hat{y}\) in terms of a linear combination of elements yields&lt;/p&gt;
&lt;p&gt;\begin{align*}
\hat{x} &amp;amp;= a_{11} * x + a_{12} * y + a_{13} - \hat{x} * a_{31} * x - \hat{x} * a_{32} * y\\
\hat{y} &amp;amp;= a_{21} * x + a_{22} * y + a_{23} - \hat{y} * a_{31} * x - \hat{y} * a_{32} * y\\
\end{align*}&lt;/p&gt;
&lt;p&gt;We can fit this using a least squares approach by the following construction.&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\begin{bmatrix}
x_1^{(1)} &amp;amp; x_2^{(1)} &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -x_1^{(1)}\hat{x}_1^{(1)} &amp;amp; -x_2^{(1)}\hat{x}_1^{(1)}\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; x_1^{(1)} &amp;amp; x_2^{(1)} &amp;amp; 1 &amp;amp; -x_1^{(1)}\hat{x}_2^{(1)} &amp;amp; -x_2^{(1)}\hat{x}_2^{(1)}\\
&amp;amp;&amp;amp; \vdots\\
x_1^{(n)} &amp;amp; x_2^{(n)} &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -x_1^{(n)}\hat{x}_1^{(n)} &amp;amp; -x_2^{(n)}\hat{x}_1^{(n)}\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; x_1^{(n)} &amp;amp; x_2^{(n)} &amp;amp; 1 &amp;amp; -x_1^{(n)}\hat{x}_2^{(n)} &amp;amp; -x_2^{(n)}\hat{x}_2^{(n)}
\end{bmatrix}
\begin{bmatrix}
a_{11}\\
a_{12}\\
a_{13}\\
a_{21}\\
a_{22}\\
a_{23}\\
a_{31}\\
a_{32}
\end{bmatrix}=
\begin{bmatrix}
\hat{x}_1^{(1)}\\
\hat{x}_2^{(1)}\\
\vdots\\
\hat{x}_1^{(n)}\\
\hat{x}_2^{(n)}
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;We can use the normal equations as before to solve for this system.&lt;/p&gt;
&lt;p&gt;The figure below shows the final result of image stitching using a perspective projection instead of an affine matrix.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-16_17-59-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;Stitching using a perspective projection.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;Stitching using a perspective projection.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Lagrangian Multipliers</title>
      <link>https://ajdillhoff.github.io/notes/lagrangian_multipliers/</link>
      <pubDate>Sat, 05 Feb 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/lagrangian_multipliers/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s take a simple constrained problem (from Nocedal and Wright).&lt;/p&gt;
&lt;p&gt;\begin{align*}
\min \quad &amp;amp; x_1 + x_2\\
\textrm{s.t.} \quad &amp;amp; x_1^2 + x_2^2 - 2 = 0
\end{align*}&lt;/p&gt;
&lt;p&gt;The set of possible solutions to this problem lie on the boundary of the circle defined by the constraint:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2021-12-01_18-06-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Source: Nocedal and Wright&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Source: Nocedal and Wright
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;If we let \(g(\mathbf{x}) = x_1^2 + x_2^2 - 2\), then the gradient vector is \((2x_1, 2x_2)\)&lt;/p&gt;
&lt;p&gt;Our original function \(f(\mathbf{x}) = x_1 + x_2\) has a gradient vector of \((1, 1)\).&lt;/p&gt;
&lt;p&gt;The figure above visualizes these vectors at different points on the constraint boundary.&lt;/p&gt;
&lt;p&gt;Notice that the optimal solution \(\mathbf{x}^* = (-1, -1)\) is at a point where \(\nabla g(\mathbf{x}^*)\) is parallel to \(\nabla f(\mathbf{x}^*)\). However, the gradients of the vectors are not equal. So there must be some scalar \(\lambda\) such that \(\nabla f(\mathbf{x}^*) = \lambda \nabla g(\mathbf{x}^*)\).&lt;/p&gt;
&lt;p&gt;This scalar \(\lambda\) is called a Lagrangian multiplier. We use this and introduce the Lagrangian function:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathcal{L}(\mathbf{x}, \lambda) = f(\mathbf{x}) - \lambda g(\mathbf{x})
\end{equation*}&lt;/p&gt;
&lt;p&gt;This yields a form for which we can analytically calculate the stationary points. That is,&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\nabla_{\mathbf{x}} \mathcal{L}(\mathbf{x}^*, \lambda^*) = 0.
\end{equation*}&lt;/p&gt;
&lt;h2 id=&#34;lagrangian-duality&#34;&gt;Lagrangian Duality&lt;/h2&gt;
&lt;p&gt;In general, the primal optimization problem is formulated as&lt;/p&gt;
&lt;p&gt;\begin{align*}
\min_{w} \quad &amp;amp; f(w)\\
\textrm{s.t.} \quad &amp;amp; g_i(w) \leq 0, \quad i = 1, \dots, k\\
&amp;amp; h_i(w) = 0, \quad i = 1, \dots, l.
\end{align*}&lt;/p&gt;
&lt;p&gt;The Lagrangian function is then&lt;/p&gt;
&lt;p&gt;\[
L(w, \alpha, \beta) = f(w) + \sum_{i=1}^k\alpha_i g_i(w) + \sum_{i=1}^l \beta_i h_i(w).
\]&lt;/p&gt;
&lt;h2 id=&#34;additional-resources&#34;&gt;Additional Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cs229.stanford.edu/notes2021fall/cs229-notes3.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://cs229.stanford.edu/notes2021fall/cs229-notes3.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Edge Detection</title>
      <link>https://ajdillhoff.github.io/notes/edge_detection/</link>
      <pubDate>Tue, 01 Feb 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/edge_detection/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#computing-gradient-norms&#34;&gt;Computing Gradient Norms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nonmaxima-suppression&#34;&gt;Nonmaxima Suppression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#thresholding&#34;&gt;Thresholding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#connectivity-analysis&#34;&gt;Connectivity Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_22-45-59_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Vertical derivative filter (left) and horizontal derivative filter (right).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Vertical derivative filter (left) and horizontal derivative filter (right).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;When &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_filters/&#34;&gt;image gradient&lt;/a&gt; filters are applied to an image, we can observe that the sample responses are very sensitive to noise and detail. For example, look at the surface at the back of ship near the drive cone. To resolve this, the image should be smoothed before differentiating it. Recall that the Gaussian filter smooths the area so that neighboring pixels are more similar than distant pixels.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_22-49-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;dx kernel applied to image blurred with Gaussian (sigma=1).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;dx kernel applied to image blurred with Gaussian (sigma=1).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This is closer to what we want, but the end goal is to create an image that shows distinct edges. We need to be clear about what an edge is. For now, we consider the images produced by convolving the \(dx\) or \(dy\) kernels as edge score images. They are only intermediate; we still need to make a final decision.&lt;/p&gt;
&lt;p&gt;In this section, we will learn about the Canny Edge Detector. The general algorithm is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Smooth the image using Gaussian blurring.&lt;/li&gt;
&lt;li&gt;Compute the gradient image via filtering. Most commonly, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Sobel_operator&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Sobel operator&lt;/a&gt; is used.&lt;/li&gt;
&lt;li&gt;Filter out weaker edge score by selecting local pixels with the largest gradient change.&lt;/li&gt;
&lt;li&gt;Use double thresholding to separate strong, or definite, edge pixels from weak ones.&lt;/li&gt;
&lt;li&gt;Remove all weak pixels not connected to a strong pixel.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Canny edge detection follows 3 objective criteria:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Edges should be detected with a low error rate. The goal is to extract as many &lt;em&gt;actual&lt;/em&gt; edges as possible.&lt;/li&gt;
&lt;li&gt;A detected edge should correspond to the center pixel of the edge in the original image.&lt;/li&gt;
&lt;li&gt;It should be robust to noise and only mark an edge pixel once.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Smoothing the image can be done by applying a Gaussian blur. Next, we need to compute the gradient image.&lt;/p&gt;
&lt;h2 id=&#34;computing-gradient-norms&#34;&gt;Computing Gradient Norms&lt;/h2&gt;
&lt;p&gt;As we saw before, the derivative filters compute the direction of greatest change in the calculated direction. When combining the result of \(dx\) and \(dy\), we get the gradient of the pixel.&lt;/p&gt;
&lt;p&gt;\[
\nabla f(x, y) = \Bigg[\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\Bigg].
\]&lt;/p&gt;
&lt;p&gt;Canny edge detection works by selecting local pixels with the largest gradient change. In order to do this, we need to compute the &lt;strong&gt;norm&lt;/strong&gt; of the gradient. If we consider every pixel in the gradient image to be a vector indicating the direction of greatest change, the norm can be computed as&lt;/p&gt;
&lt;p&gt;\[
\|\nabla f(x, y)\| = \sqrt{\Big(\frac{\partial f}{\partial x}\Big)^2 + \Big(\frac{\partial f}{\partial y}\Big)^2}.
\]&lt;/p&gt;
&lt;p&gt;Additionally, we want the angle of direction of the gradient. This can be computed for each pixel as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\theta = \text{atan2}(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}).
\end{equation*}&lt;/p&gt;
&lt;p&gt;In practice, this can be computed at the same time. There are also efficient implementations of &lt;code&gt;atan2&lt;/code&gt; which can generate an array of the same size of the original image containing the computed angles for each pixel.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-06_12-47-47_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Gradient norm of image.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Gradient norm of image.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The figure above shows the result of computing the gradient norms for each pixel. This representation is intuitive to interpret. The largest values are on the edges of the violin. The image produced by this step is still too fuzzy. These do not represent the final edges.&lt;/p&gt;
&lt;h2 id=&#34;nonmaxima-suppression&#34;&gt;Nonmaxima Suppression&lt;/h2&gt;
&lt;p&gt;The gradient norm image is helpful in showing all edge scores, but the egdes are still too thick and there are many disconnected edge scores detected. We can thin the edges by evaluating neighboring pixels. We will select only the local pixels which have the highest absolute gradient and suppress the others.
This process is called &lt;strong&gt;nonmaxima suppression&lt;/strong&gt;.
There are two approaches to this problem. The first is approximates the closest gradient normal. The second uses interpolation to compute a more accurate value.&lt;/p&gt;
&lt;p&gt;In the first approach, the number of discrete orientations for the edge normal are split into horizontal, vertical, \(45^{\circ}\), and \(-45^{\circ}\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-06_13-13-55_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Discretizing angles into 4 regions.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Discretizing angles into 4 regions.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;For a given pixel, the gradient direction is discretized into one of the above four regions by selection the angle closest to the original angle given.
Next, the gradient norm of the given pixel is compared to that of the pixels on either side of it following the same discretized direction. If one of the neighboring pixels has a higher gradient norm, the current pixel&amp;rsquo;s value is set to 0.
The intuition here is that if it &lt;em&gt;were&lt;/em&gt; an edge pixel, it would have the largest gradient norm along its given direction.&lt;/p&gt;
&lt;p&gt;The result of applying this process on our gradient image is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-06_14-44-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Gradient norm image after nonmaxima suppression is applied.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Gradient norm image after nonmaxima suppression is applied.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;interpolation&#34;&gt;Interpolation&lt;/h3&gt;
&lt;p&gt;An alternative approach is to interpolate the gradient norm using the actual angle.
Instead of discretizing it into one of four regions above, the original angle is used to compute the neighboring pixels in continuous space.
This will, of course, produce invalid pixel locations. The gradient norm for the neighboring pixels follows the approach discussed in &lt;a href=&#34;https://ajdillhoff.github.io/notes/sampling/&#34;&gt;Sampling and Aliasing&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For example, if we are at pixel \((5, 5)\) with a gradient direction of \(55^{\circ}\), then the neighboring pixels along that angle can be computed by first finding the vector following that direction. That is&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{p}_{\text{offset}} &amp;amp;=
\begin{bmatrix}
\cos (55^{\circ} \cdot \frac{\pi}{180^{\circ}})\\
\sin (55^{\circ} \cdot \frac{\pi}{180^{\circ}})
\end{bmatrix}\\
&amp;amp;=
\begin{bmatrix}
.5736\\
.8192
\end{bmatrix}
\end{align*}&lt;/p&gt;
&lt;p&gt;Then the two neighboring pixels along this direction are \(f(5 - .5373, 5 - .8192)\) and \(f(5 + .5373, 5 + .8192)\). These are clearly not valid pixel locations.
To compute the interpolated value, a weighted contribution from the closest 4 pixels are used for each of the two neighbors.
For \(f(4.4627, 4.1808)\), these pixels are \(\{(4, 4), (5, 4), (4, 5), (5, 5)\}\).
The interpolation weights for this pixel are computed as&lt;/p&gt;
&lt;p&gt;\begin{align*}
w_x &amp;amp;= 4.4627 - 4 = .4627\\
w_y &amp;amp;= 4.1808 - 4 = .1808
\end{align*}&lt;/p&gt;
&lt;p&gt;Then the resulting pixel value is computed via bilinear interpolation:&lt;/p&gt;
&lt;p&gt;\begin{align*}
f(4.4627, 4.1808) &amp;amp;=
(1 - w_x) \cdot (1 - w_y) \cdot f(4, 4)\\
&amp;amp;+ w_x \cdot (1 - w_y) \cdot f(5, 4)\\
&amp;amp;+ (1 - w_x) \cdot w_y \cdot f(4, 5)\\
&amp;amp;+ w_x \cdot w_y \cdot f(5, 5).
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;thresholding&#34;&gt;Thresholding&lt;/h2&gt;
&lt;p&gt;We now have an image of edge scores, but have not yet made a final determination on which pixels are actually edges. One approach to selecting the edge pixel is to use thresholding. That is, we suppress any pixel value that is lower than some parameter \(T\):&lt;/p&gt;
&lt;p&gt;\[
f_{T}(x, y) = f(x, y) \geq T.
\]&lt;/p&gt;
&lt;p&gt;However, this approach will still leave many false positives as well as edge segments that may be connected to strong edges.
This issue is partly resolved via &lt;strong&gt;hysteresis thresholding&lt;/strong&gt;.
For this, we choose 2 threshold values: one for weak edges and another for strong edge scores.
Using these scores, we can generate two images:&lt;/p&gt;
&lt;p&gt;\begin{align*}
f_{T_H}(x, y) &amp;amp;= f(x, y) \geq T_{H}\\
f_{T_L}(x, y) &amp;amp;= f(x, y) \geq T_{L}
\end{align*}&lt;/p&gt;
&lt;p&gt;We can then eliminate the duplicate pixels in \(f_{T_L}\) by subtracting \(f_{T_H}\):&lt;/p&gt;
&lt;p&gt;\[
f_{T_L} = f_{T_L} - f_{T_H}.
\]&lt;/p&gt;
&lt;p&gt;Using the image processed via nonmaxima suppression from before, this generates the following images:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-06_14-43-46_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Low threshold image (left) and high threshold image (right).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Low threshold image (left) and high threshold image (right).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;connectivity-analysis&#34;&gt;Connectivity Analysis&lt;/h2&gt;
&lt;p&gt;There must be a reason why we computed a lower threshold. There are weak edge pixels that may have been apart of a segment connected to strong pixels. In this case, we want to keep every weak pixel that is 8-connected to a strong pixel.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-06_15-00-57_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;The pixels surrounding the black pixel are 8-connected to it. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;The pixels surrounding the black pixel are 8-connected to it. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This can be accomplished with the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Locate an edge pixel in the high threshold image.&lt;/li&gt;
&lt;li&gt;Mark all pixels in the weak image that are 8-connected to the current strong pixel as strong pixels.&lt;/li&gt;
&lt;li&gt;Repeat steps 1 and 2 for all strong pixels in the original high threshold image.&lt;/li&gt;
&lt;li&gt;Set all pixels in the weak image that were not marked to 0.&lt;/li&gt;
&lt;li&gt;Add the marked weak pixels to the strong image.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Applying the procedure above given the weak and strong images from before yields the following result.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-06_15-04-45_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Final edge image after connectivity analysis.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Final edge image after connectivity analysis.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Sampling and Aliasing</title>
      <link>https://ajdillhoff.github.io/notes/sampling/</link>
      <pubDate>Sun, 30 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/sampling/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#resizing&#34;&gt;Resizing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sampling&#34;&gt;Sampling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;resizing&#34;&gt;Resizing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Aliasing arises through resampling an image&lt;/li&gt;
&lt;li&gt;How to resize - algorithm&lt;/li&gt;
&lt;li&gt;How to resolve aliasing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Resizing an image, whether increase or decreasing the size, is a common image operation. In Linear Algebra, &lt;strong&gt;scaling&lt;/strong&gt; is one of the transformations usually discussed, along with rotation and skew. Scaling is performed by creating a transformation matrix&lt;/p&gt;
&lt;p&gt;\begin{equation*}
M =
\begin{bmatrix}
s &amp;amp; 0\\
0 &amp;amp; s
\end{bmatrix},
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(s\) is the scaling factor. This matrix can then be used to transform the location of each point via matrix multiplication.&lt;/p&gt;
&lt;p&gt;Is is that simple for digital images? Can we simply transform each pixel location of the image using \(M\)? There are a couple of steps missing when it comes to scaling digital images. First, \(M\) simply creates a mapping between the location in the original image and the corresponding output location in the scaled image. If we were to implement this in code, we would need to take the pixel&amp;rsquo;s value from the original image.&lt;/p&gt;
&lt;h3 id=&#34;a-simple-example&#34;&gt;A Simple example&lt;/h3&gt;
&lt;p&gt;Take a \(2 \times 2\) image whose pixel values are all the same color.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-29_21-24-11_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;2 by 2 image whose values are the same.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;2 by 2 image whose values are the same.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;If we transform each pixel location of the image and copy that pixel&amp;rsquo;s value to the mapped location in the larger image, we would get something as seen in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-29_21-29-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;The resulting scaled image.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;The resulting scaled image.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This image is exactly what we would expect. The resulting image is two times as large as the first. What pixel values should the new ones take on? This is a question of &lt;strong&gt;sampling&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;sampling&#34;&gt;Sampling&lt;/h2&gt;
&lt;p&gt;Given \(s = 2\), the scaling matrix maps the original pixel locations to their new values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\((0, 0) \mapsto (0, 0)\)&lt;/li&gt;
&lt;li&gt;\((0, 1) \mapsto (0, 2)\)&lt;/li&gt;
&lt;li&gt;\((1,0) \mapsto (2, 0)\)&lt;/li&gt;
&lt;li&gt;\((1,1) \mapsto (2, 2)\)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What values should be given to the unmapped values of the new image? There are several sampling strategies used in practice. Two of the most common approaches are &lt;strong&gt;nearest neighbor&lt;/strong&gt; and &lt;strong&gt;bilinear&lt;/strong&gt; sampling. Let&amp;rsquo;s start with the nearest neighbor approach.&lt;/p&gt;
&lt;h3 id=&#34;nearest-neighbor&#34;&gt;Nearest Neighbor&lt;/h3&gt;
&lt;p&gt;First, we let the pixel location in an image be the center of that pixel, as depicted below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-29_22-50-00_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;A 2-by-2 image with pixel locations depicted as dots in the center.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;A 2-by-2 image with pixel locations depicted as dots in the center.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To establish a map between the pixel locations in the scaled image and that of the original image, we shrink the grid on the larger image and superimpose it over the smaller image.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-29_22-51-11_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Pixel grid of larger image superimposed on original image.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Pixel grid of larger image superimposed on original image.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;With nearest neighbor interpolation, the pixel value in the resized image corresponds to that of the nearest pixel in the original image. In the above figure, we can see that pixels \((0, 0), (0, 1), (1, 0), \text{ and } (1, 1)\) in the resized image are closest to pixel \((0, 0)\) in the original image. Thus, they will take on that pixel&amp;rsquo;s value.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s compare both of these approaches on a real image. The first figure below shows the original image \((16 \times 16\)). The following figure shows the image resized to \((32 \times 32)\) with no interpolation and nearest neighbor interpolation, respectively.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-29_23-59-17_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Original image.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Original image.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-29_23-59-52_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Image resized with no interpolation (left) and nearest neighbor (right).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Image resized with no interpolation (left) and nearest neighbor (right).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;bilinear-interpolation&#34;&gt;Bilinear Interpolation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Bilinear interpolation&lt;/strong&gt; is a slightly more sophisticated way of sampling which takes into account all neighboring pixels in the original image. The value of the pixel in the sampled image is a linear combination of the values of the neighbors of the corresponding pixel it is mapped to.&lt;/p&gt;
&lt;p&gt;Consider a \(3 \times 3\) image upsampled to a \(8 \times 8\) image. The figure below shows the original image with the coordinates of the upsampled image superimposed on it.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_11-36-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;3-by-3 grid with 8-by-8 coordinates overlaid.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;3-by-3 grid with 8-by-8 coordinates overlaid.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;How do we determine the coordinate map between the original and upscaled image?&lt;/strong&gt;
Solve a linear system.&lt;/p&gt;
&lt;p&gt;Note the extreme values of the image. That is, the smallest and largest coordinates. Since we stated previously that \((0, 0)\) refers to the coordinate in the middle of the pixel, the top-left of the image boundary for any image is \((-0.5, -0.5)\). The bottom-right corner for the smaller image is \((2.5, 2.5)\). The bottom-right corner for the resized image is \((7.5, 7.5)\). The equation that maps the top-left coordinates between the images is given by&lt;/p&gt;
&lt;p&gt;\[
-\frac{1}{2} = -\frac{1}{2}a + b.
\]&lt;/p&gt;
&lt;p&gt;The equation that maps the bottom-right coordinates between the images is given by&lt;/p&gt;
&lt;p&gt;\[
\frac{5}{2} = \frac{15}{2}a + b.
\]&lt;/p&gt;
&lt;p&gt;Thus, we have 2 equations 2 unknowns. Solving this yields \(a = \frac{3}{8}\) and \(b = -\frac{5}{16}\).&lt;/p&gt;
&lt;p&gt;With the mapping solved, let&amp;rsquo;s compute the color value for pixel \((3, 3)\) in the upsampled image. Here, \((3, 3) \mapsto (\frac{13}{16}, \frac{13}{16})\) in the original image. Our problem for this particular pixel is reduced to the following figure.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_18-08-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Determining the pixel value for the mapped pixel using bilinear interpolation.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Determining the pixel value for the mapped pixel using bilinear interpolation.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;We first interpolate between two pairs of pixels in the original image. That is, we find \(p_1\) and \(p_2\) in the following figure.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_18-15-27_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Step 1: Interpolate the pixel values between two pixels for (p_1) and (p_2).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Step 1: Interpolate the pixel values between two pixels for (p_1) and (p_2).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Here, \(p_2 = \frac{3}{16}(255, 255, 255) + \frac{13}{16}(128, 128, 128) \approx (152, 152, 152)\) and \(p_1 = \frac{3}{16}(255, 0, 0) + \frac{13}{16}(255, 255, 255) \approx (255, 207, 207)\). Note that the contribution of the pixel depends on the weight on the other side the intermediate value \(p_i\). For example, if you think of \(p_1\) as a slider from the red pixel to the white pixel. The value to the left of the slider reflects the contribution of the pixel to the right, and vice versa.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_18-23-43_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;Computed values of (p_1) and (p_2).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;Computed values of (p_1) and (p_2).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Finally, the value of the new pixel is a linear combination of \(p_1\) and \(p_2\). That is \(p = \frac{13}{16}(152, 152, 152) + \frac{3}{16}(255, 207, 207) \approx (171, 162, 162)\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_18-28-03_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 11: &amp;lt;/span&amp;gt;The final pixel value computed from (p_1) and (p_2).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;The final pixel value computed from (p_1) and (p_2).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The following figure compares the original image with both types of interpolation discussed in this section.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_18-34-13_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 12: &amp;lt;/span&amp;gt;Original image (left), upscaled 2x with NN interpolation (middle), upscaled with bilinear interpolation (right).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 12: &lt;/span&gt;Original image (left), upscaled 2x with NN interpolation (middle), upscaled with bilinear interpolation (right).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;aliasing&#34;&gt;Aliasing&lt;/h3&gt;
&lt;p&gt;Nearest neighbor interpolation often leads to images with &lt;strong&gt;aliasing&lt;/strong&gt;. In general, aliasing occurs when two signals are sampled at such a frequency that they become indistinguishable from each other. Usually, images are smoothed prior to upsampling or downsampling in an effort to alleviate the effects of aliasing. The figure below shows a downsampled image using nearest neighbor interpolation.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_10-31-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 13: &amp;lt;/span&amp;gt;Image downsampled by 4x. Notice the &amp;#34;jaggies&amp;#34;, especially along straight lines.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 13: &lt;/span&gt;Image downsampled by 4x. Notice the &amp;ldquo;jaggies&amp;rdquo;, especially along straight lines.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;By blurring the image and using bilinear interpolation, the same image looks much smoother when downsized.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-01-25_19-30-16_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 14: &amp;lt;/span&amp;gt;Image downsampled by 4x using bilinear interpolation.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 14: &lt;/span&gt;Image downsampled by 4x using bilinear interpolation.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Color</title>
      <link>https://ajdillhoff.github.io/notes/color/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/color/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#topics&#34;&gt;Topics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-human-eye&#34;&gt;The Human Eye&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#color-matching&#34;&gt;Color Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#color-physics&#34;&gt;Color Physics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#color-spaces&#34;&gt;Color Spaces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hsv-color-space&#34;&gt;HSV Color Space&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;What is color?&lt;/li&gt;
&lt;li&gt;How do we process color?&lt;/li&gt;
&lt;li&gt;What information does color contain?&lt;/li&gt;
&lt;li&gt;What can we infer from color?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-human-eye&#34;&gt;The Human Eye&lt;/h2&gt;
&lt;p&gt;The eye acts as a camera, including a lens which focuses light onto a receptive surface. The &lt;strong&gt;cornea&lt;/strong&gt; covers the &lt;strong&gt;lens&lt;/strong&gt; which combine to make a compound lens. The lens itself is flexible to allow the eye to focus on objects of variable distance. The lens is attached to &lt;strong&gt;ciliary muscles&lt;/strong&gt; which contract or expand to change the shape of the lens. This allows us to focus on near or far objects. As we age, the lens itself becomes hardened and does not transform back to a spherical shape when the ciliary muscles contract, resulting in farsightedness.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;pupil&lt;/strong&gt; is a diaphragm that adjusts the amount of light that enters the eye in response to varying intensities. However, this isn&amp;rsquo;t the only mechanism available for this task. Our perception of the amount of light, &lt;em&gt;luminance adaptation&lt;/em&gt;, occurs also in the retina and brain over a longer time period (usually several minutes).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-02_20-23-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The following image from Wikipedia shows the evolution of the eye from a simple region of photoreceptors to the current model we have today.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-04_10-00-18_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Source: &amp;lt;https://en.wikipedia.org/wiki/Eye&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Source: &lt;a href=&#34;https://en.wikipedia.org/wiki/Eye&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://en.wikipedia.org/wiki/Eye&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;retina&#34;&gt;Retina&lt;/h3&gt;
&lt;p&gt;The primary purpose of the optics of the eye is to focus light onto the &lt;strong&gt;retina&lt;/strong&gt;. This is a thin layer of nerve tissue that covers the inner surface of the eye. It consists of approximately 200 million layered cells. Half are &lt;strong&gt;photoreceptor&lt;/strong&gt; cells and the other half recode photoreceptor outputs before passing them on towards the brain.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fun fact:&lt;/strong&gt; The retina is woven with blood vessels whose purpose is to nourish the retinal tissue. These blood vessels produce the &lt;em&gt;red eye&lt;/em&gt; effect that is seen in flash photography.&lt;/p&gt;
&lt;h3 id=&#34;photoreceptor-cells&#34;&gt;Photoreceptor Cells&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-23_21-19-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Distribution of rods and cones. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Distribution of rods and cones. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Photorceptor cells can be classified into two types: rods and cones. There are about 100 million rods and 6 million cones. &lt;strong&gt;Rods&lt;/strong&gt; dominate our low-light vision. They do not have the variety of photopigments necessary for color vision. &lt;strong&gt;Cones&lt;/strong&gt; dominate our color vision. They can be separated into one of three classes of light receptors. These three classes contain a specific type of photopigment that is sensitive to different wavelengths of light.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2021-12-31_13-24-45_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Source: &amp;lt;https://handprint.com/HP/WCL/color1.html#3cones&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Source: &lt;a href=&#34;https://handprint.com/HP/WCL/color1.html#3cones&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://handprint.com/HP/WCL/color1.html#3cones&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Rods can pool input together before sending it to the brain. Low-light perception is better through peripheral vision.&lt;/p&gt;
&lt;p&gt;Most of the cones are in the &lt;strong&gt;fovea&lt;/strong&gt;. The high density of cones in this region means that the resolution is highest. Microsaccades - The eye is making tiny movements so that the eye is never fixated on a single point. This allows light to hit a greater number of photoreceptors.&lt;/p&gt;
&lt;p&gt;The other segment of a photoreceptor cell lies the photopigment molecules. These act as transducers which convert light energy into a biological response. These molecules are made up of a light sensitive molecule called &lt;strong&gt;chromophore&lt;/strong&gt; and a protein called &lt;strong&gt;opsin&lt;/strong&gt;. Together, they are usually referred to as &lt;strong&gt;rhodopsin&lt;/strong&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-02_20-47-55_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Photopigment Molecules. Source: &amp;lt;https://handprint.com/HP/WCL/color1.html&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Photopigment Molecules. Source: &lt;a href=&#34;https://handprint.com/HP/WCL/color1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://handprint.com/HP/WCL/color1.html&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;types-of-photopigments&#34;&gt;Types of Photopigments&lt;/h4&gt;
&lt;p&gt;Each photoreceptor can respond to light differently depending on the specific type of rhodopsin it is made up of.&lt;/p&gt;
&lt;p&gt;The response of each type of cone has been measured and can is separated into short wavelength (S), medium wavelength (M), and long wavelength (L). Some resources incorrectly label these as B, G, and R receptors. This is not entirely accurate as there is much overlap between the medium and long wavelength receptors.&lt;/p&gt;
&lt;h4 id=&#34;mathematical-model&#34;&gt;Mathematical Model&lt;/h4&gt;
&lt;p&gt;How would we model a photoreceptor, mathematically? The response is dependent on how sensitive the receptor is to a specific wavelength along with the light arriving at the receptor.&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p_k = \int_{\Lambda} \sigma_k(\lambda)E(\lambda)d\lambda
\end{equation*}&lt;/p&gt;
&lt;h2 id=&#34;color-matching&#34;&gt;Color Matching&lt;/h2&gt;
&lt;p&gt;The theory that the visible colors are visible from the three primaries is called &lt;strong&gt;trichomatic theory&lt;/strong&gt;. Trichromacy has been measured and observed.&lt;/p&gt;
&lt;p&gt;How is it known that these 3 (or 4) types of photoreceptors absorb a specific wavelength? In other words, how do we know that color vision break down? There has not been a way to measure cone responses in living humans, but there is a way to measure them indirectly.&lt;/p&gt;
&lt;p&gt;James Clerk Maxwell&amp;rsquo;s color matching experiments aim to do exactly that. In this experiment, participants are given a test color and a matching color. The goal is to add some amount of primary colors until the matching color is the same as the test color. There are some saturated test colors that cannot be matched in an additive manner (combining primaries). In these cases, the subjects are allowed to subtract some amount of a primary color from the test color until it matches the matching color.&lt;/p&gt;
&lt;p&gt;Over many different experiments involving many subjects, it was found that most subjects will match the test color with the same amount of primary weights. This provided confirmation that human vision is trichromatic. There were also carefully screen &lt;strong&gt;dichromats&lt;/strong&gt; who lacked one class of photoreceptor.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is color matching reliable?&lt;/strong&gt;
&lt;strong&gt;Do we see all wavelengths the same? That is, do we have an equal number of photoreceptors for each range?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The fact that many independent observers came up with similar distributions of primary colors to match test colors gave rise to Grassman&amp;rsquo;s laws.&lt;/p&gt;
&lt;p&gt;The result of these experiments reveals a function of cone sensitivities. Cones are unequally distributed across the retina. Visual angle and whether the light is detected in the central vs. peripheral region matters. The CIE 1931 RGB color matching functions show how much of each primary wavelength is required to match a particular target wavelength. The negative values mean that the primary was added to the test color in order to match.&lt;/p&gt;
&lt;p&gt;
  &lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-03_12-36-25_screenshot.png&#34; alt=&#34;&#34;&gt;

Source: &lt;a href=&#34;https://en.wikipedia.org/wiki/CIE_1931_color_space&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://en.wikipedia.org/wiki/CIE_1931_color_space&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Actual cone sensitivy depends on light sensitivy. One of the most popular graphs of cone response is from Stockman &amp;amp; Sharpe (2000):&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-03_13-27-23_screenshot.png&#34; &gt;


&lt;/figure&gt;

&lt;p&gt;This graph is normalized to peak response and is not representative of the distribution of cones in the retina. If we weight the responses by the proportion of each class of cone in the retina, the graph looks like:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-03_13-28-55_screenshot.png&#34; &gt;


&lt;/figure&gt;

&lt;p&gt;The fact that the 3 shapes are colored as blue, green, and red are misleading. There are far more photoreceptors that perceive green light than there are blue or red light. You can see this when comparing green text versus blue text. Reading the blue text may strain your eyes and appear blurry. This is because there are simply fewer receptors available for these wavelengths. Fewer such receptors also implies that the resolution is smaller.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-03_13-34-15_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Fewer blue cones results in blue light appearing more blurry.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Fewer blue cones results in blue light appearing more blurry.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;color-physics&#34;&gt;Color Physics&lt;/h2&gt;
&lt;p&gt;Light sources themselves can produce different colors at different wavelengths. The sky varies in color depending on the relative location of the sun. A surface is dependent on the color of the surface itself as well as the incident light shining towards it. A white surface with a green light shining on it will reflect green light. A green surface with white light will also reflect green. Further complexities such as atmosphere and dust can further complicate this.&lt;/p&gt;
&lt;p&gt;In our world, both the sky and the sun are important light sources. They are often referred to as skylight and sunlight, respectively. Imagine a clear day with the sun at the highest point in the sky. The sun is seen as a yellow object with the surrounding sky being a rich blue. Compared to the zenith of the sky, the horizon is typically brighter. This is because air scatters the incident light. This can be modeled by assuming the sky emits a constant amount of exitant light per volume.&lt;/p&gt;
&lt;p&gt;Why does the sun appear yellow and the sky appear blue? Longer wavelengths can travel farther before being scattered. This means that a ray of light travelling from the sun to the earth will scatter blue light before the other rays. Taking the blue light out of a ray of white light will leave a yellowish color. When the scattered blue light eventually hits the atmosphere of Earth, it is scattered once again and see as incident light to an observer. This gives the apperance of a blue sky.&lt;/p&gt;
&lt;p&gt;The perceived color of an object can be computed by multiplying the incident illumination with the reflectance of the object.&lt;/p&gt;
&lt;h3 id=&#34;color-temperature&#34;&gt;Color Temperature&lt;/h3&gt;
&lt;p&gt;One quantity that is commonly used to describe a light source is temperature (measured in Kelvins). This is derived from the concept of the &lt;strong&gt;black body&lt;/strong&gt;. That is, a body that does not reflect light. A heated black body emits radiation and the spectral power distribution of this radiation depends only on the temperature. The color temperature is the surface temperature of an ideal black body.&lt;/p&gt;
&lt;p&gt;At lower temperature, the color is a red. As it increases, the color becomes whiter until reaching a light blue. See the table below:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_11-49-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Colors corresponding to different temperatures. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Colors corresponding to different temperatures. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;color-spaces&#34;&gt;Color Spaces&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;How is color represented based on color matching?&lt;/strong&gt;
&lt;strong&gt;Can we accurately reproduce any color digitally?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.wnycstudios.org/podcasts/radiolab/episodes/211119-colors&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.wnycstudios.org/podcasts/radiolab/episodes/211119-colors&lt;/a&gt;
Interesting podcast on Color.&lt;/p&gt;
&lt;p&gt;There are several color spaces (or color models) available. The most common is RGB. In this section, we will explore the most common color spaces.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-03_17-51-15_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Comparison of several common color spaces. Source: &amp;lt;https://en.wikipedia.org/wiki/ProPhoto_RGB_color_space&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Comparison of several common color spaces. Source: &lt;a href=&#34;https://en.wikipedia.org/wiki/ProPhoto_RGB_color_space&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://en.wikipedia.org/wiki/ProPhoto_RGB_color_space&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;cie-xy-color-space&#34;&gt;CIE xy Color Space&lt;/h3&gt;
&lt;p&gt;Verifying trichomatic theory meant that one should attempt to reproduce monochromatic (single wavelength) colors as a weighted mixture of primary colors. The Commision Internationale d&amp;rsquo;Eclairage did just that in the 1930s. They performed their own color matching experiments using red, blue, and green wavelengths as primary colors. Another benefit of this process is to set a standard in which colors can be reproduced.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-03_17-30-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;CIE xy color space.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;CIE xy color space.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This shows the CIE xy space which was taken from CIE XYZ space. This 2D space is an intersection of the original XYZ space by the plane \(X + Y + Z = 1\). The coordinates are then&lt;/p&gt;
&lt;p&gt;\begin{equation*}
(x, y) = \Big(\frac{x}{x + y + z}, \frac{y}{x + y + z}\Big).
\end{equation*}&lt;/p&gt;
&lt;p&gt;The border of this shape represents the wavevelength of the pure color. The colors in the middle represent some linear combination of those wavelengths.&lt;/p&gt;
&lt;h3 id=&#34;rgb-color-spaces&#34;&gt;RGB Color Spaces&lt;/h3&gt;
&lt;p&gt;RGB color spaces are the most commonly used in computer graphics. CIE also has their own RGB color space. It is derived from color matching experiments using 3 monochromatic primary colors.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-24_20-49-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;CIE RGB gamut on the CIE xy color space.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;CIE RGB gamut on the CIE xy color space.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;RGB models are commonly represented as a cube, as seen in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-24_20-54-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 11: &amp;lt;/span&amp;gt;Cube representation of an RGB color space.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;Cube representation of an RGB color space.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;hsv-color-space&#34;&gt;HSV Color Space&lt;/h2&gt;
&lt;p&gt;Hue, Saturation, Value (HSV) provides an alternative representation of RGB. &lt;strong&gt;Hue&lt;/strong&gt; represents the color. Given a fixed value for saturation and value, colors in HSV should appear as if they are receiving the same level of light.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-24_21-05-11_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 12: &amp;lt;/span&amp;gt;Hue values when saturation and value are fixed. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 12: &lt;/span&gt;Hue values when saturation and value are fixed. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The CIE defines &lt;strong&gt;saturation&lt;/strong&gt; as &amp;ldquo;the colourfulness of an area judged in proportion to its brightness.&amp;rdquo; The &lt;strong&gt;value&lt;/strong&gt; of a pixel represents how bright the color is compared to black.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-24_21-10-29_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 13: &amp;lt;/span&amp;gt;HSV cylinder exemplifies the concepts of hue, saturation, and value. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 13: &lt;/span&gt;HSV cylinder exemplifies the concepts of hue, saturation, and value. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;conversion-from-rgb&#34;&gt;Conversion from RGB&lt;/h4&gt;
&lt;p&gt;We can convert images to HSV from RGB and vice versa. This article will only go through the steps of actually transforming it. To gain a better understanding of the conversion between HSV to RGB, check out the &lt;a href=&#34;https://en.wikipedia.org/wiki/HSL_and_HSV#General_approach&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Wikipedia page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We start by making sure that are input image is normalized such that the RGB values are in the range \([0, 1]\). If that&amp;rsquo;s the case, we can calculate the HSV value \(V\) as&lt;/p&gt;
&lt;p&gt;\[
V = \max(R, G, B).
\]&lt;/p&gt;
&lt;p&gt;We can do this in one command in Python using numpy:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;V &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max(img, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Saturation is computed based on another quantity called &lt;a href=&#34;https://en.wikipedia.org/wiki/Colorfulness&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Chroma&lt;/a&gt;. It is simply computed as&lt;/p&gt;
&lt;p&gt;\[
C = V - \min(R,G,B).
\]&lt;/p&gt;
&lt;p&gt;In Python, this is simply&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;C &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; V &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;min(img, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Saturation is then computed as \(S = \frac{C}{V}\). Note that this will be undefined if \(V = 0\). In practice, we can set \(S = 0\) if \(V\) is also 0.&lt;/p&gt;
&lt;p&gt;Hue is commonly measured in degrees between \([0, 360]\). As seen in the figure below, it is the angle past the previous edge of the hexagon.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_10-29-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 14: &amp;lt;/span&amp;gt;Hue is the angle of the projected point with respect to the hexagon. 0 degrees is marked by the red edge. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 14: &lt;/span&gt;Hue is the angle of the projected point with respect to the hexagon. 0 degrees is marked by the red edge. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The function for Hue can be written as a piecewise function, altered slightly to account for undefined values in practice:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
H&amp;rsquo; =
\begin{cases}
0, &amp;amp; C = 0\\
\frac{G - B}{C} \text{ mod } 6, &amp;amp; \text{if } V = R\\
\frac{B - R}{C} + 2, &amp;amp; \text{if } V = G\\
\frac{R - G}{C} + 4, &amp;amp; \text{if } V = B\\
\end{cases}
\end{equation*}&lt;/p&gt;
&lt;p&gt;Then, \(H = 60^{\circ} \times H&amp;rsquo;\).&lt;/p&gt;
&lt;h4 id=&#34;conversion-to-rgb&#34;&gt;Conversion to RGB&lt;/h4&gt;
&lt;p&gt;To go back to RGB, we take an HSV image with \(H \in [0^{\circ}, 360^{\circ}]\), and \(S, V \in [0, 1]\). The Chrome value is calcuated as&lt;/p&gt;
&lt;p&gt;\[
C = V \times S.
\]&lt;/p&gt;
&lt;p&gt;We then divide up the Hue into one of 6 values:&lt;/p&gt;
&lt;p&gt;\[
H&amp;rsquo; = \frac{H}{60^{\circ}}.
\]&lt;/p&gt;
&lt;p&gt;With these intermediate value, we can calculate the corresponding point within the RGB cube that has the same hue and chroma as the current pixel value, with \(X\) being the second largest component of the color:&lt;/p&gt;
&lt;p&gt;\[
X = C \times (1 - |H&amp;rsquo; \text{ mod } 2 - 1|)
\]&lt;/p&gt;
&lt;p&gt;and then&lt;/p&gt;
&lt;p&gt;\begin{equation*}
(R&amp;rsquo;, G&amp;rsquo;, B&amp;rsquo;) =
\begin{cases}
(C, X, 0) &amp;amp; \text{if } 0 \leq H&amp;rsquo; &amp;lt; 1\\
(X, C, 0) &amp;amp; \text{if } 1 \leq H&amp;rsquo; &amp;lt; 2\\
(0, C, X) &amp;amp; \text{if } 2 \leq H&amp;rsquo; &amp;lt; 3\\
(0, X, C) &amp;amp; \text{if } 3 \leq H&amp;rsquo; &amp;lt; 4\\
(X, 0, C) &amp;amp; \text{if } 4 \leq H&amp;rsquo; &amp;lt; 5\\
(C, 0, X) &amp;amp; \text{if } 5 \leq H&amp;rsquo; &amp;lt; 6\\
\end{cases}
\end{equation*}&lt;/p&gt;
&lt;p&gt;The final RGB value can be calculated by adding the difference between the value and chroma to each pixel:&lt;/p&gt;
&lt;p&gt;\[
m = V - C
\]&lt;/p&gt;
&lt;p&gt;\[
(R, G, B) = (R&amp;rsquo; + m, G&amp;rsquo; + m, B&amp;rsquo; + m).
\]&lt;/p&gt;
&lt;h4 id=&#34;some-examples&#34;&gt;Some Examples&lt;/h4&gt;
&lt;p&gt;Given an original image (below), we&amp;rsquo;ll view the output of changing the Hue, Saturation, and Value.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_10-53-17_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 15: &amp;lt;/span&amp;gt;Original image. Credit: The Expanse&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 15: &lt;/span&gt;Original image. Credit: The Expanse
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Reducing the Hue by 20 produces the following image:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_10-55-52_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 16: &amp;lt;/span&amp;gt;Image with Hue subtracted by 20 degrees. Credit: The Expanse&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 16: &lt;/span&gt;Image with Hue subtracted by 20 degrees. Credit: The Expanse
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Given our working knowledge of how hue is computed, this makes sense. The previous angle clearly pointed to lighter blue colors, reducing that by \(20^{\circ}\) moves us towards the green edge.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s take the original image and increase saturation by 0.3:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_10-59-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 17: &amp;lt;/span&amp;gt;Image with Saturation increased by 0.3. Credit: The Expanse&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 17: &lt;/span&gt;Image with Saturation increased by 0.3. Credit: The Expanse
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;All colors across the board look &amp;ldquo;richer&amp;rdquo; and &amp;ldquo;deeper&amp;rdquo;. This corresponds with the definition of the HSV cylinder.&lt;/p&gt;
&lt;p&gt;Finally, we&amp;rsquo;ll view an image in which the value was modified. Let&amp;rsquo;s increase the values by 0.2:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_11-03-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 18: &amp;lt;/span&amp;gt;Image with Values increased by 0.2. Credit: The Expanse&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 18: &lt;/span&gt;Image with Values increased by 0.2. Credit: The Expanse
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This looks washed out. All of the pixels with 0 values were increased uniformly. Perhaps we could clamp that by setting all pixels with the given value change back to their original values.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Histogram of Oriented Gradients</title>
      <link>https://ajdillhoff.github.io/notes/histogram_of_oriented_gradients/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/histogram_of_oriented_gradients/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#orientation-histograms&#34;&gt;Orientation Histograms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#histogram-of-oriented-gradients&#34;&gt;Histogram of Oriented Gradients&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Key Questions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why are these necessary?&lt;/li&gt;
&lt;li&gt;What limitations do they address that corner interest points cannot?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although the original &lt;a href=&#34;http://vision.stanford.edu/teaching/cs231b_spring1213/papers/CVPR05_DalalTriggs.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;HOG Paper&lt;/a&gt; came out after &lt;a href=&#34;https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;SIFT&lt;/a&gt;, it is much simpler to describe the process (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dalal and Triggs 2005&lt;/a&gt;).
Histograms of Oriented Gradients are feature vectors that are generated by evaluating gradients within a local neighborhood of interest points.&lt;/p&gt;
&lt;h2 id=&#34;orientation-histograms&#34;&gt;Orientation Histograms&lt;/h2&gt;
&lt;p&gt;This approach depends on building &lt;a href=&#34;https://john.cs.olemiss.edu/heroes/papers/hand_gesture.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;orientation histograms&lt;/a&gt;.
For each pixel in the original image, construct a histogram of gradient orientations of all pixels within a square window.
The gradient orientations of each pixel are easily calculated following the approach used for &lt;a href=&#34;https://ajdillhoff.github.io/notes/edge_detection/&#34;&gt;Edge Detection&lt;/a&gt;.
This transformation can be flattened into a single vector that is used to compare images via L2 distance or some other metric.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-12_19-48-07_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Orientation histograms of hand images.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Orientation histograms of hand images.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The pseudocode to generate orientation histograms is shown below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-nil&#34; data-lang=&#34;nil&#34;&gt;let w be the window_size
let h be half the window_size
let norms be the gradient norms of the input image for each pixel
let angles be the computed orientations of the gradient vectors for each pixel
for each pixel (i, j):
    create a histogram of orientations with b bins
    weight the orientations of the bins based on the gradient norm
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The histograms of each local feature are translation invariant.
A histogram of gradient orientations for a feature in one image should be the same as one generated to a similar feature in another image.&lt;/p&gt;
&lt;p&gt;This approach is not scale invariant.&lt;/p&gt;
&lt;h2 id=&#34;histogram-of-oriented-gradients&#34;&gt;Histogram of Oriented Gradients&lt;/h2&gt;
&lt;p&gt;Dalal and Triggs propose a feature extraction method based on orientation histograms (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dalal and Triggs 2005&lt;/a&gt;). In their work published at CVPR, they evaluate their features by training a SVM for pedestrian detection on a standard (at that time) benchmark. They evaluate on a person detector using the metric &lt;strong&gt;False Positives Per Window (FPPW)&lt;/strong&gt;. This can be calculated as &lt;code&gt;num_fp / num_windows&lt;/code&gt;. This represents a tradeoff between the number of false positives and the number false negatives. Intuitively, lowering the threshold for detection will generate more false positives, but will also reduce the number of false negatives.&lt;/p&gt;
&lt;h3 id=&#34;computing-hog&#34;&gt;Computing HoG&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Normalize for color and gamma values.&lt;/li&gt;
&lt;li&gt;Compute the gradient image.&lt;/li&gt;
&lt;li&gt;Extract a window of some size.&lt;/li&gt;
&lt;li&gt;Divide window into sub-grid&lt;/li&gt;
&lt;li&gt;Compute orientation histogram of each cell.&lt;/li&gt;
&lt;li&gt;Concatentate the four histograms.&lt;/li&gt;
&lt;li&gt;Normalize the feature vector.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;During the binning step, each pixel provides a weighted vote for the histogram based on the orientation of the gradient element it centered on. This vote is weighted based on a function of the gradient magnitude.&lt;/p&gt;
&lt;p&gt;In the paper, they experiment with a wide range of different parameters. They show that optimal performance coincides with choosing 4 cells per window with 9 orientation bins.&lt;/p&gt;
&lt;h3 id=&#34;normalization-schemes&#34;&gt;Normalization Schemes&lt;/h3&gt;
&lt;p&gt;There were several normalizion schemes addressed in the paper.
The normalization scheme picked based on lowest FPPW is &lt;code&gt;L2-Hys&lt;/code&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Normalize the concatenated vector.&lt;/li&gt;
&lt;li&gt;Clip values to 0.2&lt;/li&gt;
&lt;li&gt;Normalize again.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;They also evaulated the features with L2, L1, and L1-sqrt.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-12_15-48-55_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Evaluation of normalization approaches (Dalal and Triggs, 2005).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Evaluation of normalization approaches (Dalal and Triggs, 2005).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Dalal, N., and B. Triggs. 2005. “Histograms of Oriented Gradients for Human Detection.” In &lt;i&gt;2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)&lt;/i&gt;, 1:886–93 vol. 1. &lt;a href=&#34;https://doi.org/10.1109/CVPR.2005.177&#34;&gt;https://doi.org/10.1109/CVPR.2005.177&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Image Features</title>
      <link>https://ajdillhoff.github.io/notes/image_features/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/image_features/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#detecting-corners&#34;&gt;Detecting Corners&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#describing-image-patches&#34;&gt;Describing Image Patches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scale-invariance&#34;&gt;Scale Invariance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Why do we care about image features? One of the main goals of computer vision is understanding of some environment through visual perception. In order to summarize a visual object, we need some description of it.
These descriptions can come in many forms, so we need to articulate some goals as to what we are ultimately looking for when describing an image.&lt;/p&gt;
&lt;p&gt;What makes an interesting feature in an image?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Something distinct&lt;/li&gt;
&lt;li&gt;Invariance properties (translation, rotation, scaling)&lt;/li&gt;
&lt;li&gt;Easy to compute&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Image features are the building blocks of many higher level applications such as&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Stereo correspondence&lt;/li&gt;
&lt;li&gt;Image stitching&lt;/li&gt;
&lt;li&gt;Object recognition and detection&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;Introduction/2022-02-08_09-02-34_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Patches taken from two images from different perspectives. Some patches are more descriptive than others. Source: Szeliski&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Patches taken from two images from different perspectives. Some patches are more descriptive than others. Source: Szeliski
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;Introduction/2022-02-08_09-06-10_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Objects detected using YOLOv3. Source: Wikipedia.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Objects detected using YOLOv3. Source: Wikipedia.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;Introduction/2022-02-08_09-07-29_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Image stitching result. The red lines show the seams at which the images are joined. Source: Wikipedia.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Image stitching result. The red lines show the seams at which the images are joined. Source: Wikipedia.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Topics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Corners&lt;/li&gt;
&lt;li&gt;HOG&lt;/li&gt;
&lt;li&gt;SIFT&lt;/li&gt;
&lt;li&gt;Correlation with template&lt;/li&gt;
&lt;li&gt;PCA&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We have talked about &lt;a href=&#34;https://ajdillhoff.github.io/notes/edge_detection/&#34;&gt;Edge Detection&lt;/a&gt;, which produces an image of edge pixels given some raw input. Edges are certainly useful features, but are they distinct enough to produce consistent image features?&lt;/p&gt;
&lt;p&gt;Consider an image patch detected from three different primitives:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Points&lt;/li&gt;
&lt;li&gt;Edges&lt;/li&gt;
&lt;li&gt;Corners&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;Introduction/2022-02-08_09-33-27_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Aperture problem for patches detected from different primitives. Source: Szeliski.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Aperture problem for patches detected from different primitives. Source: Szeliski.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The above figure illustrates the aperture problem.
Consider a flat surface without texture. If we generate a patch around any arbitrary point, it will have many correspondences with other patches.
It may be obvious that picking any arbitrary point on a flat, single-colored surface would not be descriptive enough to match with anything useful.&lt;/p&gt;
&lt;p&gt;What about an edge? An edge is distinct based on its orientation. The middle image in the figure above shows that, while some ambiguity has been resolved, there are still a wide range of possible locations that it could be matched to. There could be many such edges found between images.&lt;/p&gt;
&lt;p&gt;This brings us to a corner. A corner has two distinct gradient changes which make it a perfect candidate for interest point detection.&lt;/p&gt;
&lt;h2 id=&#34;detecting-corners&#34;&gt;Detecting Corners&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.434.4816&amp;amp;rep=rep1&amp;amp;type=pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Corners are a great choice for a feature. They are small, rotation and translation invariant, and can be computed simply from the gradient images we have computed before.&lt;/p&gt;
&lt;p&gt;There are a few distinct interest points of a violin. Maybe we can use a corner detector to come up with patches which can be reproduced across different images.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_09-51-14_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;A replica violin.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;A replica violin.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In a local window, a corner exhibits a large change in orientation.
A flat surface has no orientation response at all.
An edge only has an orientation in one direction.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How can we detect such changes in an image?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;sum of square differences (SSD)&lt;/strong&gt;. If we take some window and move it over some image, taking a the SSD at each point, we can produce an image of responses.
SSD is defined as&lt;/p&gt;
&lt;p&gt;\[
f(x, y) = \sum_{(u, v) \in W}\big(I(x + u, y + v) - I(u, v)\big)^2.
\]&lt;/p&gt;
&lt;p&gt;This difference was previously used to evaluate discrete steps. Harris et al. note this as a limitation and instead aim to evaluate all possible &lt;em&gt;small&lt;/em&gt; shifts about the origin of the shift. This is accomplished through analytic expansion of the term \(I(x + u, y + v)\).&lt;/p&gt;
&lt;p&gt;Through Taylor expansion, this can be approximated as&lt;/p&gt;
&lt;p&gt;\begin{align*}
I(x + u, y + v) &amp;amp;= I(u, v) + x \frac{\partial}{\partial x}I(x, y) + y \frac{\partial}{\partial y}I(x, y) + O(x^2, y^2) \\
&amp;amp;\approx I(u, v) + xI_x + yI_y
\end{align*}&lt;/p&gt;
&lt;p&gt;In the above approximation, \(O(x^2, y^2)\) describes the upper bound of the behavior of the function. Through Taylor expansion, we could write the higher order terms. However, we only care about small shifts about the shift origin, so only a first order, or linear, approximation is sufficient.&lt;/p&gt;
&lt;p&gt;Using this first order approximation, SSD can be written&lt;/p&gt;
&lt;p&gt;\begin{align*}
f(x, y) &amp;amp;\approx \sum_{(u, v) \in W} w(u, v) \big(I(u, v) + xI_x + yI_y - I(u,v)\big)^2\\
&amp;amp;= \sum_{(u, v) \in W} w(u, v) \big(xI_x + yI_y\big)^2\\
&amp;amp;= \sum_{(u, v) \in W} w(u, v) \big(x^2I_x^2 + 2xyI_xI_y + y^2I_y^2\big)
\end{align*}&lt;/p&gt;
&lt;p&gt;The term \(x^2I_x^2 + 2xyI_xI_y + y^2I_y^2\) is a linear combination and can be efficiently computed via matrix multiplication.&lt;/p&gt;
&lt;p&gt;\begin{equation*}
x^2I_x^2 + 2xyI_xI_y + y^2I_y^2 =
\begin{bmatrix}
x &amp;amp; y
\end{bmatrix}
\begin{bmatrix}
I_x^2 &amp;amp; I_x I_y \\
I_x I_y &amp;amp; I_y^2\\
\end{bmatrix}
\begin{bmatrix}
x\\
y
\end{bmatrix}
\end{equation*}&lt;/p&gt;
&lt;p&gt;We can now rewrite the original SSD as follows.&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f(x, y) \approx
\begin{bmatrix}
x &amp;amp; y
\end{bmatrix}
M
\begin{bmatrix}
x\\
y
\end{bmatrix},
\end{equation*}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{equation*}
M = \sum_{(u, v) \in W} w(u, v) H.
\end{equation*}&lt;/p&gt;
&lt;p&gt;\begin{equation*}
H =
\begin{bmatrix}
I_x^2 &amp;amp; I_x I_y\\
I_x I_y &amp;amp; I_y^2 \\
\end{bmatrix}
\end{equation*}&lt;/p&gt;
&lt;p&gt;\(M\) is then an autocorrelation matrix. The benefit of this formulation is that \(M\) is a symmetric matrix. If we remember our studies from linear algebra, we remember that there are some very important properties and characteristics of symmetric matrices.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_16-54-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Gradient image (I_x^2).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Gradient image (I_x^2).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_16-55-10_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Gradient image (I_x I_y).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Gradient image (I_x I_y).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_16-55-38_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Gradient image (I_y^2).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Gradient image (I_y^2).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_10-07-09_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Gradient change in both (x) and (y). Credit: David Jacobs&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Gradient change in both (x) and (y). Credit: David Jacobs
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;First, lets consider a simple case of detecting the following corner. At this orientation, the changes in gradient are only in the vertical and horizontal directions. If we consider the matrix \(M\) from above, we would get the following result&lt;/p&gt;
&lt;p&gt;\begin{equation*}
M =
\begin{bmatrix}
\sum I_x^2 &amp;amp; \sum I_x I_y\\
\sum I_x I_y &amp;amp; \sum I_y^2
\end{bmatrix} =
\begin{bmatrix}
\lambda_1 &amp;amp; 0\\
0 &amp;amp; \lambda_2
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;The off-diagonal entries will be 0, by definition of the dot product and orthogonal vectors.
The entries on the main diagonal will represent the eigenvalues of \(M\).
If both entries on the main diagonal are large, this would indicate a large change in orientation within the window.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if the corner is not as ideal?&lt;/strong&gt;&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_10-13-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;Eigenvalue analysis of autocorrelation matrix. Source: Szeliski.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;Eigenvalue analysis of autocorrelation matrix. Source: Szeliski.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Relationship to Eigenvalues&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Recall the Spectral Theorem for Symmetric Matrices, which states:
A symmetric matrix \(A \in \mathbb{R}^{n\times n}\) has the following properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A has \(n\) real eigenvalues, counting multiplicities.&lt;/li&gt;
&lt;li&gt;The dimension of the eigenspace for each eigenvalue \(\lambda\) equals the multiplicity of \(\lambda\) as a root of the characteristic equation.&lt;/li&gt;
&lt;li&gt;The eigenspaces are mutually orthogonal, in the sense that eigenvectors corresponding to different eigenvalues are orthogonal.&lt;/li&gt;
&lt;li&gt;\(A\) is orthogonally diagonalizable.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Symmetric matrices are orthogonally diagonalizable. Thus, a symmetric matrix \(A\) can be written as \(A = PDP^{-1}\), where the columns of \(P\) are the eigenvectors and \(D\) are the corresponding eigenvalues.
Another perspective of this is that \(A\) is an ellipse with axis lengths determined by the eigenvalues (diagonal entries of \(D\)) rotated by \(P\).&lt;/p&gt;
&lt;p&gt;The eigenvalues of \(M\) can be classified into different regions depending on if they are indicative of a flat region, edge, or corner.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_10-15-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 11: &amp;lt;/span&amp;gt;Classification of responses. Source: Harris (1988).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;Classification of responses. Source: Harris (1988).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Performing eigendecomposition seems cumbersome in this case. There must be a simpler way we could compute these responses.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We can then approximate this response!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\begin{align*}
R &amp;amp;= \det H - \alpha \cdot \textrm{tr}(H)^2\\
&amp;amp;= I_x^2 \cdot I_y^2 - (I_x I_y)^2 - \alpha\big(I_x^2 + I_y^2\big)^2
\end{align*}&lt;/p&gt;
&lt;p&gt;If there is a corner, the gradient values will depict orthogonality. That is, the middle term in the equation above will be smaller. This results in a larger response.&lt;/p&gt;
&lt;p&gt;The larger the value from the middle term, the less orthogonality is present. This results in a smaller response. In practice, we will see a negative response.&lt;/p&gt;
&lt;p&gt;In practice, \(\alpha \in [0.04, 0.06]\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_16-59-04_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 12: &amp;lt;/span&amp;gt;Response image (R).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 12: &lt;/span&gt;Response image (R).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;window-selection&#34;&gt;Window Selection&lt;/h3&gt;
&lt;p&gt;What is the best window to choose when computing responses across an image? Harris et al. considered this in their original formulation when comparing to Moravec&amp;rsquo;s corner detection function.
Using a flat window with uniform values produces a binary response when over interest points and 0 everywhere else. This can be written as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
M = \sum_{u, v} w(u, v)
\begin{bmatrix}
I_x^2 &amp;amp; I_x I_y\\
I_x I_y &amp;amp; I_y^2
\end{bmatrix}
\end{equation*}&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_10-23-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 13: &amp;lt;/span&amp;gt;Uniform response window. Results in 1 for interest points inside the window, 0 otherwise. Credit: Fei-Fei Li&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 13: &lt;/span&gt;Uniform response window. Results in 1 for interest points inside the window, 0 otherwise. Credit: Fei-Fei Li
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Instead, Harris et al. propose using a circular Gaussian window which can be computed as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
M = g(\sigma) *
\begin{bmatrix}
I_x^2 &amp;amp; I_x I_y\\
I_x I_y &amp;amp; I_y^2
\end{bmatrix}
\end{equation*}&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_10-25-42_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 14: &amp;lt;/span&amp;gt;Gaussian window response. Credit: Fei-Fei Li&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 14: &lt;/span&gt;Gaussian window response. Credit: Fei-Fei Li
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The Guassian window is easy enough to compute and has the added bonus of making the responses rotation invariant!&lt;/p&gt;
&lt;h3 id=&#34;nonmaxima-suppression--again&#34;&gt;Nonmaxima Suppression (Again)&lt;/h3&gt;
&lt;p&gt;We now have a response image in which each pixel gives an indication as to whether a corner has been detected.
To thin out these hypotheses, we will need to suppress neighborhood values that are not maximal.
Just like with &lt;a href=&#34;https://ajdillhoff.github.io/notes/edge_detection/&#34;&gt;Edge Detection&lt;/a&gt;, we will need to employ nonmaxima suppression.&lt;/p&gt;
&lt;p&gt;Before applying that, we may choose to threshold the image to filter out points that are obviously not candidates.&lt;/p&gt;
&lt;p&gt;The approach is quite simple here:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Slide a \(3 \times 3\) window across the image.&lt;/li&gt;
&lt;li&gt;If the center pixel is not the maximum value in the \(3 \times 3\) window, set it to 0.&lt;/li&gt;
&lt;li&gt;Continue until all pixels are evaluated.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Our final result is shown below. The detected corners are marked.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_17-05-13_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 15: &amp;lt;/span&amp;gt;Final output of corner detection.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 15: &lt;/span&gt;Final output of corner detection.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;describing-image-patches&#34;&gt;Describing Image Patches&lt;/h2&gt;
&lt;p&gt;Given a list of interest points, we can start to build a collection of regions or patches surrounding the point which are useful for feature matching.
The simple choice here is to take a fixed size patch surrounding an interest point and use it as a template.
We can then compare that to other interest points in different images to see how well they score.
There are many limitations to this naive approach that prevent it from working well in general.
Even if the perspective of the object is the same in multiple images, slight changes is brightness could affect the matching scores greatly.&lt;/p&gt;
&lt;p&gt;Another consideration is scale. If we have an interest point at one scale, will it be detected when that image is scaled by some factor \(k\)?&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-10_09-33-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 16: &amp;lt;/span&amp;gt;Patch surrounding a similar interest point at different scales. Credit: Kristen Grauman, B. Liebe.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 16: &lt;/span&gt;Patch surrounding a similar interest point at different scales. Credit: Kristen Grauman, B. Liebe.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;However we choose to represent the information surrounding an interest point need to be robust to translation, scale, and orientation changes.
It is important to note that, although the Harris corner detector is invariant to orientation, a naive image patch surrounding the interest point may not be.&lt;/p&gt;
&lt;h2 id=&#34;scale-invariance&#34;&gt;Scale Invariance&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Is the Harris corner detector scale-invariant?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;No! Consider the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-09_19-10-13_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 17: &amp;lt;/span&amp;gt;Harris corner detector is not scale invariant. Credit: Kristen Grauman&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 17: &lt;/span&gt;Harris corner detector is not scale invariant. Credit: Kristen Grauman
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The output from the detector will be different depending on scale.
One solution would be to resize the image over several different scales and consolidate the detections.
Doing this would produce many features and take much longer to compute.&lt;/p&gt;
&lt;p&gt;Given some feature template that is centered on an interest point, we would expect that size of the patch scales with the scale change of the image itself.
This property will drive the development of a scale-invariant method.
We select the size of the patch by placing some dark blob with a light background (or vice versa) over the interest point and then selecting the size which provides the greatest response.&lt;/p&gt;
&lt;h3 id=&#34;laplacian-filter&#34;&gt;Laplacian Filter&lt;/h3&gt;
&lt;p&gt;A good choice for this is the Laplacian filter. The Laplacian of a 2D function is&lt;/p&gt;
&lt;p&gt;\begin{equation*}
(\nabla^2 f)(x, y) = \frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;The Laplacian filter is created following the derivation in the two figures below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-10_09-55-08_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 18: &amp;lt;/span&amp;gt;Deriving second partial derivative filters for x and y. Source: &amp;lt;https://theailearner.com/2019/05/25/laplacian-of-gaussian-log/&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 18: &lt;/span&gt;Deriving second partial derivative filters for x and y. Source: &lt;a href=&#34;https://theailearner.com/2019/05/25/laplacian-of-gaussian-log/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://theailearner.com/2019/05/25/laplacian-of-gaussian-log/&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-10_09-55-53_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 19: &amp;lt;/span&amp;gt;Combining the x and y filters. Source: &amp;lt;https://theailearner.com/2019/05/25/laplacian-of-gaussian-log/&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 19: &lt;/span&gt;Combining the x and y filters. Source: &lt;a href=&#34;https://theailearner.com/2019/05/25/laplacian-of-gaussian-log/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://theailearner.com/2019/05/25/laplacian-of-gaussian-log/&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;It is also common to smooth the operator before use.
This can be done by convolving with a Gaussian kernel:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
K_{\nabla^2} * G_{\sigma}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;This is referred as the &lt;strong&gt;Laplacian of Gaussian&lt;/strong&gt; filter.&lt;/p&gt;
&lt;h3 id=&#34;scale-space&#34;&gt;Scale Space&lt;/h3&gt;
&lt;p&gt;We can use the Laplacian of Gaussian to find the appropriate size of image patch for a given scale.
This is achieved by computing a &lt;strong&gt;scale-space&lt;/strong&gt; representation of an image.
When we resize an image to make it smaller, there is a loss of information.
Similarly, blurring the image causes a loss of information.
The larger the \(\sigma\) value for the Gaussian, the more information that is lost.
Thus, we can quickly compute different scale-image representations by applying Gaussian blurring with a range of \(\sigma\) values.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-09_19-27-45_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 20: &amp;lt;/span&amp;gt;Scale space representations. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 20: &lt;/span&gt;Scale space representations. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;As it turns out, blurring and resizing correspond with each other.
This is calculated by applying a Gaussian blur to the image following:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
L_f(\sigma) = f(G_\sigma * I).
\end{equation*}&lt;/p&gt;
&lt;p&gt;This is the response of function \(f\) in scale-space \(\sigma\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-09_20-01-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 21: &amp;lt;/span&amp;gt;Selecting features at different scales. Credit: Kristen Grauman&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 21: &lt;/span&gt;Selecting features at different scales. Credit: Kristen Grauman
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The size of the patch can be found by iterating through different values of \(\sigma\), applying the Laplacian at each scale, and selecting the value of \(\sigma\) which produced the greatest result.&lt;/p&gt;
&lt;p&gt;Consider a simple rounded square as depicted below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-10_10-15-11_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 22: &amp;lt;/span&amp;gt;Simple rounded square.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 22: &lt;/span&gt;Simple rounded square.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;If we apply the LoG filter to select the scale which gives the greatest response for the region centered on the top left corner at the original scale, we produce the following graph.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-10_10-16-22_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 23: &amp;lt;/span&amp;gt;Responses of LoG at the top left corner from (sigma = 0) to (sigma = 8).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 23: &lt;/span&gt;Responses of LoG at the top left corner from (sigma = 0) to (sigma = 8).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;If we scale the original image by 2 and apply the same analysis again, we get the following graph.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-10_10-20-17_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 24: &amp;lt;/span&amp;gt;Responses of LoG at original image size at the top left corner from (sigma = 0) to (sigma = 8).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 24: &lt;/span&gt;Responses of LoG at original image size at the top left corner from (sigma = 0) to (sigma = 8).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To summarize, using this method will allow us to select the appropriate scale at which our interest point provides the strongest response.
However, the cost of this search is high. As we increase the size of the filters, the more work required for each convolution.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kernels</title>
      <link>https://ajdillhoff.github.io/notes/kernels/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/kernels/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dual-representation&#34;&gt;Dual Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#relating-back-to-the-original-formulation&#34;&gt;Relating Back to the Original Formulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#types-of-kernels&#34;&gt;Types of Kernels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#constructing-kernels&#34;&gt;Constructing Kernels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rbf-maps-to-infinite-dimensional-space&#34;&gt;RBF maps to infinite-dimensional space&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Slides for these notes can be found &lt;a href=&#34;https://ajdillhoff.github.io/teaching/cse6363/lectures/kernels.pdf&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Notebook link: &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/blob/main/svm/kernels.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://github.com/ajdillhoff/CSE6363/blob/main/svm/kernels.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Parametric models use training data to estimate a set of parameters that can then be used to perform inference on new data.
An alternative approach uses &lt;strong&gt;nonparametric methods&lt;/strong&gt;, meaning the function is estimated directly from the data instead of optimizing a set of parameters.&lt;/p&gt;
&lt;p&gt;One possible downside to such an approach is that it becomes less efficient as the amount of training data increases.
Additionally, the transformation into a feature space such that the data becomes linearly separable may be intractable.
Consider sequential data such as text or audio.
If each sample has a variable number of features, how do we account for this using standard linear models with a fixed number of parameters?&lt;/p&gt;
&lt;p&gt;The situations described above can be overcome through the use of the &lt;strong&gt;kernel trick&lt;/strong&gt;.
We will see that, by computing a measure of similarity between samples in the feature space, we do not need to directly transform each individual sample to that space.&lt;/p&gt;
&lt;p&gt;A kernel function is defined as&lt;/p&gt;
&lt;p&gt;\[
k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = \phi(\mathbf{x})^T \phi(\mathbf{x}&amp;rsquo;),
\]&lt;/p&gt;
&lt;p&gt;where \(\phi\) is some function which transforms the input to a feature space.&lt;/p&gt;
&lt;p&gt;Methods that require part or all of the training data to make prediction will benefit from using kernel representations, especially when using high dimensional data. Instead of transforming the data into a high dimensional space which may be computationally intractable, a measure of similarity via the &lt;em&gt;inner product&lt;/em&gt; is used. The inner product is not the projection into some space. Instead, it represents the outcome of that projection.&lt;/p&gt;
&lt;p&gt;If the input vector takes on the form of scalar products, it can be represented as a kernel function.&lt;/p&gt;
&lt;h2 id=&#34;dual-representation&#34;&gt;Dual Representation&lt;/h2&gt;
&lt;p&gt;The key to taking advantage of the kernel trick relies on reformulating our linear model into a dual representation.
In this form, we will establish a dependence on the kernel function.&lt;/p&gt;
&lt;p&gt;The following derivation of the dual representation for linear regression follows (Bishop). Consider the least squares loss with \(L2\) regularization, as we discussed with &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_regression/&#34;&gt;Linear Regression&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;\[
J(\mathbf{w}) = \frac{1}{2}\sum_{i=1}^n(\mathbf{w}^T\phi(\mathbf{x}_i) - y_i)^2 + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}
\]&lt;/p&gt;
&lt;p&gt;Here, \(\phi\) is a basis function that transforms the input. This could also be a simple identity function in which \(\phi(\mathbf{x}) = \mathbf{x}\). To solve for \(\mathbf{w}\), we take the gradient of \(J(\mathbf{w})\) with respect to \(\mathbf{w}\) and set it to 0.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\nabla_{\mathbf{w}}J(\mathbf{w}) &amp;amp;= \sum_{i=1}^n(\mathbf{w}^T\phi(\mathbf{x}_i) - y_i)\phi(\mathbf{x}_i) + \lambda \mathbf{w}\\
\implies \mathbf{w} &amp;amp;= -\frac{1}{\lambda}\sum_{i=1}^n(\mathbf{w}^T\phi(\mathbf{x}_i) - y_i)\phi(\mathbf{x}_i)
\end{align*}&lt;/p&gt;
&lt;p&gt;We can formulate this as a matrix-vector product by letting&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{\Phi} =
\begin{bmatrix}
\phi(\mathbf{x}_1)^T\\
\vdots \\
\phi(\mathbf{x}_n)^T\\
\end{bmatrix}
\text{ and }
a_{i} = -\frac{1}{\lambda}(\mathbf{w}^T\phi(\mathbf{x}_i) - y_i).
\end{equation*}&lt;/p&gt;
&lt;p&gt;Then, \(\mathbf{w} = \mathbf{\Phi}^T\mathbf{a}\), where \(\mathbf{a} = [a_1, \dots, a_n]^T\).&lt;/p&gt;
&lt;p&gt;The dual representation is derived by reformulating \(J(\mathbf{w})\) in terms of \(\mathbf{a}\).&lt;/p&gt;
&lt;p&gt;\begin{equation*}
J(\mathbf{a}) = \frac{1}{2}\mathbf{a}^T\mathbf{\Phi}\mathbf{\Phi}^T\mathbf{\Phi}\mathbf{\Phi}^T\mathbf{a} - \mathbf{a}^T\mathbf{\Phi}\mathbf{\Phi}^T\mathbf{y} + \frac{1}{2}\mathbf{y}^T\mathbf{y} + \frac{\lambda}{2} \mathbf{a}^T\mathbf{\Phi}\mathbf{\Phi}^T\mathbf{a},
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(\mathbf{y} = [y_1, \dots, y_n]\).&lt;/p&gt;
&lt;p&gt;Looking at the products \(\mathbf{\Phi}\mathbf{\Phi}^T\), we see that these relate to our original kernel form: \(\phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j)\). This product defines a &lt;a href=&#34;https://en.wikipedia.org/wiki/Gram_matrix&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Gram matrix&lt;/a&gt; \(\mathbf{K} = \mathbf{\Phi}\mathbf{\Phi}^T\) whose elements are \(k(\mathbf{x}_i, \mathbf{x}_j)\). Thus, we can rewrite \(J(\mathbf{a})\) as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
J(\mathbf{a}) = \frac{1}{2}\mathbf{a}^T\mathbf{K}\mathbf{K}\mathbf{a} - \mathbf{a}^T\mathbf{K}\mathbf{y} + \frac{1}{2}\mathbf{y}^T\mathbf{y} + \frac{\lambda}{2}\mathbf{a}^T\mathbf{K}\mathbf{a}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Solving for \(\mathbf{a}\) can be done by computing the gradient of \(J(\mathbf{a})\) with respect to \(\mathbf{a}\) and setting the result to 0.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\nabla_\mathbf{a}J(\mathbf{a}) = \mathbf{K}\mathbf{K}\mathbf{a} - \mathbf{K}\mathbf{y} + \lambda \mathbf{K}\mathbf{a} &amp;amp;= 0\\
\mathbf{K}\mathbf{a} + \lambda I\mathbf{a} - \mathbf{y} &amp;amp;= 0\\
(\mathbf{K} + \lambda I)\mathbf{a} &amp;amp;= \mathbf{y}\\
\mathbf{a} &amp;amp;= (\mathbf{K} + \lambda I)^{-1} \mathbf{y}.
\end{align*}&lt;/p&gt;
&lt;p&gt;With \(\mathbf{a}\) solved, we can complete the dual representation of our original linear regression model. Recall that&lt;/p&gt;
&lt;p&gt;\begin{equation*}
h(\mathbf{x}; \mathbf{w}) = \mathbf{w}^T\phi(\mathbf{x}).
\end{equation*}&lt;/p&gt;
&lt;p&gt;If we substitute \(\mathbf{w} = \mathbf{\Phi}^T\mathbf{a}\), we get&lt;/p&gt;
&lt;p&gt;\begin{align*}
f(\mathbf{x};\mathbf{a}) &amp;amp;= \mathbf{a}^T\mathbf{\Phi}\phi(\mathbf{x})\\
&amp;amp;= \Big[(\mathbf{K} + \lambda I)^{-1}\mathbf{y})\Big]^T\mathbf{\Phi}\phi(\mathbf{x}).
\end{align*}&lt;/p&gt;
&lt;p&gt;Again, the kernel form is apparent in the product \(\mathbf{\Phi}\phi(\mathbf{x})\). If we let \(k_i(\mathbf{x}) = k(\mathbf{x}_i,\mathbf{x})\) and&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{k}(\mathbf{x}) =
\begin{bmatrix}
k_1(\mathbf{x})\\
\vdots \\
k_n(\mathbf{x})
\end{bmatrix},
\end{equation*}&lt;/p&gt;
&lt;p&gt;we can write the dual representation of our linear regression model as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f(\mathbf{x}) = \mathbf{k}(\mathbf{x})^T(\mathbf{K} + \lambda \mathbf{I})^{-1}\mathbf{y}.
\end{equation*}&lt;/p&gt;
&lt;h2 id=&#34;relating-back-to-the-original-formulation&#34;&gt;Relating Back to the Original Formulation&lt;/h2&gt;
&lt;p&gt;In this dual formulation, the solution for \(\mathbf{a}\) can be expressed as a linear combination of elements \(\phi(\mathbf{x})\).
From above, we see that&lt;/p&gt;
&lt;p&gt;\[
a_i = -\frac{1}{\lambda}\big(\mathbf{w}^T\phi(\mathbf{x}_i) - y_i\big).
\]&lt;/p&gt;
&lt;p&gt;Expanding this into individual coefficients yields&lt;/p&gt;
&lt;p&gt;\begin{align*}
a_i &amp;amp;= -\frac{1}{\lambda}\big(w_1\phi_1(\mathbf{x}_i) + \cdots + w_m \phi_m(\mathbf{x}_i) - y_i\big)\\
&amp;amp;= -\frac{w_1}{\lambda}\phi_1(\mathbf{x}_i) - \cdots - \frac{w_m}{\lambda} \phi_m(\mathbf{x}_i) + \frac{y_i}{\lambda}.
\end{align*}&lt;/p&gt;
&lt;p&gt;We are close, but we still need to do something about the term \(\frac{y_i}{\lambda}\). For this, we can multiply both sides of our equation by a convenient 1. That is, we multiply by&lt;/p&gt;
&lt;p&gt;\[
\frac{\phi_1(\mathbf{x}_i) + \cdots + \phi_m(\mathbf{x}_i)}{\phi_1(\mathbf{x}_i) + \cdots + \phi_m(\mathbf{x}_i)}.
\]&lt;/p&gt;
&lt;p&gt;By doing this and grouping the \(\phi_j\) terms, we get&lt;/p&gt;
&lt;p&gt;\begin{align*}
&amp;amp;\Big(\frac{y_i}{\lambda}\cdot \frac{1}{\phi_1(\mathbf{x}_i) + \cdots + \phi_m(\mathbf{x}_i)} - \frac{w_1}{\lambda}\Big)\phi_1(\mathbf{x}_i) + \cdots\\
&amp;amp;+ \Big(\frac{y_i}{\lambda}\cdot \frac{1}{\phi_1(\mathbf{x}_i) + \cdots + \phi_m(\mathbf{x}_i)} - \frac{w_m}{\lambda}\Big)\phi_m(\mathbf{x}_i).
\end{align*}&lt;/p&gt;
&lt;p&gt;We can simplify this by introducing a term&lt;/p&gt;
&lt;p&gt;\[
c_i = \frac{y_i}{\lambda}\cdot \frac{1}{\phi_1(\mathbf{x}_i) + \cdots + \phi_m(\mathbf{x}_i)}.
\]&lt;/p&gt;
&lt;p&gt;Then the solution can be rewritten as&lt;/p&gt;
&lt;p&gt;\[
\Big(c_i - \frac{w_1}{\lambda}\Big)\phi_1(\mathbf{x}_i) + \cdots + \Big(c_i - \frac{w_m}{\lambda}\Big)\phi_m(\mathbf{x}_i).
\]&lt;/p&gt;
&lt;p&gt;With this, we can step backwards using intermediate results in the previous section to get back to the original formulation of our linear regression model.&lt;/p&gt;
&lt;h2 id=&#34;types-of-kernels&#34;&gt;Types of Kernels&lt;/h2&gt;
&lt;p&gt;There are several types of kernels that can be used to transform the input data depending on the problem. The simplest kernel is the &lt;strong&gt;identity kernel:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\[
k(\mathbf{x}, \mathbf{x&amp;rsquo;}) = \mathbf{x}^T \mathbf{x&amp;rsquo;}.
\]&lt;/p&gt;
&lt;h3 id=&#34;polynomial-kernel&#34;&gt;Polynomial Kernel&lt;/h3&gt;
&lt;p&gt;A polynomial kernel is defined as&lt;/p&gt;
&lt;p&gt;\[
k(\mathbf{x}, \mathbf{x&amp;rsquo;}) = (\mathbf{x}^T\mathbf{x&amp;rsquo;}+c)^d.
\]&lt;/p&gt;
&lt;p&gt;This is a common choice for solving problems akin to polynomial regression.
We can use this kernel to present a visual explanation of kernel functions.
Consider the following dataset.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-19_22-06-55_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Binary classification dataset that is not linearly separable.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Binary classification dataset that is not linearly separable.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;It is easy enough to see that this dataset could not be separated using a hyperplane in 2D.
We could separate the two using some nonlinear decision boundary like a circle.
If we could transform this into 3D space, we could come up with some features such that it is linearly separable in 3D.
For example, let \(\phi(\mathbf{x}) = (x_1^2, x_2^2, \sqrt{2}x_1x_2)\).&lt;/p&gt;
&lt;p&gt;Transforming all points and visualizing yields the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-19_22-11-36_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Binary classification dataset transformed into a 3D feature space.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Binary classification dataset transformed into a 3D feature space.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;From this perspective, we can clearly see that the data is linearly separable.
The question remains: if we only have the original 2D features, how do we compare points in this 3D features space without explicitly transforming each point?
The kernel function corresponding to the feature transform above is&lt;/p&gt;
&lt;p&gt;\begin{align*}
k(\mathbf{x}, \mathbf{x}&amp;rsquo;) &amp;amp;= (\mathbf{x}^T\mathbf{x}&amp;rsquo;)^2\\
&amp;amp;= (x_1x&amp;rsquo;_1 + x_2x&amp;rsquo;_2)^2\\
&amp;amp;= 2x_1x&amp;rsquo;_1x_2x&amp;rsquo;_2 + (x_1x&amp;rsquo;_1)^2 + (x_2x&amp;rsquo;_2)^2\\
&amp;amp;= \phi(\mathbf{x})^T \phi(\mathbf{x}&amp;rsquo;)
\end{align*}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\phi(\mathbf{x}) =
\begin{bmatrix}
\sqrt{2}x_1x_2\\
x_1^2\\
x_2^2
\end{bmatrix}.
\]&lt;/p&gt;
&lt;h3 id=&#34;radial-basis-function-kernel&#34;&gt;Radial Basis Function Kernel&lt;/h3&gt;
&lt;p&gt;This kernel follows a Gaussian term and is commonly used with &lt;a href=&#34;https://ajdillhoff.github.io/notes/support_vector_machine/&#34;&gt;SVMs&lt;/a&gt;. It is defined as&lt;/p&gt;
&lt;p&gt;\[
k(\mathbf{x}, \mathbf{x&amp;rsquo;}) = \exp\Big(-\frac{\|\mathbf{x}-\mathbf{x&amp;rsquo;}\|^2}{2\sigma^2}\Big).
\]&lt;/p&gt;
&lt;h3 id=&#34;cosine-similarity&#34;&gt;Cosine Similarity&lt;/h3&gt;
&lt;p&gt;Consider the problem of comparing text sequences for a document classification task.
One approach is to compare the number of occurrences of each word.
The idea is that documents that are similar will have a similar number of words that occur.&lt;/p&gt;
&lt;p&gt;\[
k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = \frac{\mathbf{x}^T \mathbf{x}&amp;rsquo;}{\|\mathbf{x}\|_2 \|\mathbf{x}&amp;rsquo;\|_2}
\]&lt;/p&gt;
&lt;p&gt;Documents that are &lt;strong&gt;orthogonal&lt;/strong&gt;, in the sense that the resulting cosine similarity is 0, are dissimilar.
The similarity increases as the score approaches 1.
There are several issues with this approach which are addressed by using the term frequence-inverse document frequency (TF-IDF) score.&lt;/p&gt;
&lt;h2 id=&#34;constructing-kernels&#34;&gt;Constructing Kernels&lt;/h2&gt;
&lt;p&gt;A valid kernel function must satisfy the following conditions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Symmetry: \(k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = k(\mathbf{x}&amp;rsquo;, \mathbf{x})\)&lt;/li&gt;
&lt;li&gt;Positive semi-definite: \(k(\mathbf{x}, \mathbf{x}&amp;rsquo;) \geq 0\)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the feature space can be represented as a dot product, then it will satisfy the first condition by definition. The second condition can be shown by constructing a Gram matrix \(\mathbf{K}\) and showing that it is positive semi-definite. A matrix \(\mathbf{K}\) is positive semi-definite if and only if \(\mathbf{v}^T\mathbf{K}\mathbf{v} \geq 0\) for all \(\mathbf{v} \in \mathbb{R}^n\).&lt;/p&gt;
&lt;h3 id=&#34;direct-construction-of-a-kernel&#34;&gt;Direct Construction of a Kernel&lt;/h3&gt;
&lt;p&gt;In this approach, we define a feature space \(\phi(\mathbf{x})\) and then compute the kernel function as&lt;/p&gt;
&lt;p&gt;\[
k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = \phi(\mathbf{x})^T \phi(\mathbf{x}&amp;rsquo;).
\]&lt;/p&gt;
&lt;p&gt;This is the approach used in the example from above. In that example, we used the kernel function \(k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = (\mathbf{x}^T\mathbf{x}&amp;rsquo;)^2\). For our 2D input, the feature space is \(\phi(\mathbf{x}) = (x_1^2, x_2^2, \sqrt{2}x_1x_2)\). It is easy to see that the kernel function is the dot product of the feature space, and its kernel matrix is positive semi-definite.&lt;/p&gt;
&lt;h3 id=&#34;construction-from-other-valid-kernels&#34;&gt;Construction from other valid kernels&lt;/h3&gt;
&lt;p&gt;As a more convenient approach, it is possible to construct complex kernels from known kernels. Given valid kernels \(k_1(\mathbf{x}, \mathbf{x}&amp;rsquo;)\) and \(k_2(\mathbf{x}, \mathbf{x}&amp;rsquo;)\), we can construct a new kernel \(k(\mathbf{x}, \mathbf{x}&amp;rsquo;)\) using the following operations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;\(k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = ck_1(\mathbf{x}, \mathbf{x}&amp;rsquo;)\) for \(c &amp;gt; 0\)&lt;/li&gt;
&lt;li&gt;\(k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = f(\mathbf{x})k_1(\mathbf{x}, \mathbf{x}&amp;rsquo;)f(\mathbf{x}&amp;rsquo;)\) for \(f(\mathbf{x})\)&lt;/li&gt;
&lt;li&gt;\(k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = k_1(\mathbf{x}, \mathbf{x}&amp;rsquo;) + k_2(\mathbf{x}, \mathbf{x}&amp;rsquo;)\)&lt;/li&gt;
&lt;li&gt;\(k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = k_1(\mathbf{x}, \mathbf{x}&amp;rsquo;)k_2(\mathbf{x}, \mathbf{x}&amp;rsquo;)\)&lt;/li&gt;
&lt;li&gt;\(k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = \exp(k_1(\mathbf{x}, \mathbf{x}&amp;rsquo;))\)&lt;/li&gt;
&lt;li&gt;\(k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = \tanh(k_1(\mathbf{x}, \mathbf{x}&amp;rsquo;))\)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;rbf-maps-to-infinite-dimensional-space&#34;&gt;RBF maps to infinite-dimensional space&lt;/h2&gt;
&lt;p&gt;It can be shown that the RBF kernel maps the input to an infinite-dimensional space. This is a result of the Taylor series expansion of the exponential function. The RBF kernel is defined as&lt;/p&gt;
&lt;p&gt;\[
k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = \exp\Big(-\frac{\|\mathbf{x}-\mathbf{x&amp;rsquo;}\|^2}{2\sigma^2}\Big).
\]&lt;/p&gt;
&lt;p&gt;The Taylor series expansion of the exponential function is&lt;/p&gt;
&lt;p&gt;\[
\exp(x) = \sum_{n=0}^\infty \frac{x^n}{n!}.
\]&lt;/p&gt;
&lt;p&gt;Substituting the RBF kernel into the Taylor series expansion yields&lt;/p&gt;
&lt;p&gt;\[
\exp\Big(-\frac{\|\mathbf{x}-\mathbf{x&amp;rsquo;}\|^2}{2\sigma^2}\Big) = \sum_{n=0}^\infty \frac{\Big(-\frac{\|\mathbf{x}-\mathbf{x&amp;rsquo;}\|^2}{2\sigma^2}\Big)^n}{n!}.
\]&lt;/p&gt;
&lt;p&gt;This expansion can be viewed as an infnite sum of polynomial terms. A more formal proof of this result can be found &lt;a href=&#34;https://pages.cs.wisc.edu/~matthewb/pages/notes/pdf/svms/RBFKernel.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The benefit of this result is that it allows us to work in a high-dimensional space without explicitly transforming the input. This is especially useful when the input space is infinite-dimensional, such as with text data. It is also used to compare the similarity of documents without explicitly transforming the input into a high-dimensional space.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linear Discriminant Analysis</title>
      <link>https://ajdillhoff.github.io/notes/linear_discriminant_analysis/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/linear_discriminant_analysis/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gaussian-class-conditional-densities&#34;&gt;Gaussian Class Conditional Densities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#decision-boundaries&#34;&gt;Decision Boundaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quadratic-descriminant-analysis&#34;&gt;Quadratic Descriminant Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example&#34;&gt;Example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Slides for these notes can be found &lt;a href=&#34;https://ajdillhoff.github.io/teaching/cse6363/lectures/lda.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This section covers classification from a probabilistic perspective.
The &lt;a href=&#34;https://ajdillhoff.github.io/notes/discriminant_functions/&#34;&gt;discriminative approach&lt;/a&gt; involves a parameterized function which assigns each input vector \(\mathbf{x}\) to a specific class.
We will see that modeling the conditional probability distribution \(p(C_k|\mathbf{x})\) grants us additional benefits while still fulfilling our original classification task.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s begin with a 2 class problem. To classify this with a generative model, we use the class-conditional densities \(p(\mathbf{x}|C_i)\) and class priors \(p(C_i)\).
The posterior probability for \(C_1\) can be written in the form of a sigmoid function:&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(C_1|\mathbf{x}) &amp;amp;= \frac{p(\mathbf{x}|C_1)p(C_1)}{p(\mathbf{x}|C_1)p(C_1) + p(\mathbf{x}|C_2)p(C_2)}
\end{align*}&lt;/p&gt;
&lt;p&gt;Then multiply the numerator and denominator by&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{(p(\mathbf{x}|C_1))^{-1}}{(p(\mathbf{x}|C_1))^{-1}},
\end{equation*}&lt;/p&gt;
&lt;p&gt;which yields&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{1}{1 + \frac{p(\mathbf{x}|C_2)p(C_2)}{p(\mathbf{x}|C_1)p(C_1)}}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Noting that \(a = \exp(\ln(a))\), we can rewrite further&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{1}{1 + \exp(-a)},
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(a = \ln \frac{p(\mathbf{x}|C_1)p(C_1)}{p(\mathbf{x}|C_2)p(C_2)}\).&lt;/p&gt;
&lt;p&gt;Writing this distribution in the form of the sigmoid function is convenient as it is a natural choice for many other classification models. It also has a very simple derivative which is convenient for models optimized using gradient descent.&lt;/p&gt;
&lt;p&gt;Given certain choices for the class conditional densities, the posterior probabilty distribution will be a linear function of the input features:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\ln p(C_k|\mathbf{x};\theta) = \mathbf{w}^T \mathbf{x} + c,
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(\mathbf{w}\) is a parameter vector based on the parameters of the chosen probability distribution, and \(c\) is a constant term that is not dependent on the parameters. As we will see, the resulting model will take an equivalent form to the discriminative approach.&lt;/p&gt;
&lt;h2 id=&#34;gaussian-class-conditional-densities&#34;&gt;Gaussian Class Conditional Densities&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s assume that our class conditional densities \(p(\mathbf{x}|C_k)\) are Gaussian. We will additionally assume that the covariance matrices between classes are shared. This will result in linear decision boundaries. Since the conditional densities are chosen to be Gaussian, the posterior is given by&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(C_k|\mathbf{x};\theta) \propto \pi_k\mathcal{N}(\mathbf{x}|\mathbf{\mu}_c,\Sigma),
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(\pi_k\) is the prior probability of class \(k\). We choose to ignore the normalizing constant since it is not dependent on the class.&lt;/p&gt;
&lt;p&gt;The class conditional density function for class \(k\) is given by&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(\mathbf{x}|C_k;\theta) = \frac{1}{2\pi^{D/2}}\frac{1}{|\Sigma|^{1/2}}\exp\Big(-\frac{1}{2}(\mathbf{x} - \mathbf{\mu}_k)^T \Sigma^{-1} (\mathbf{x} - \mathbf{\mu}_k)\Big).
\end{equation*}&lt;/p&gt;
&lt;p&gt;Now that we have a concrete function to work with, let&amp;rsquo;s go back to the simple case of two classes and define \(a = \ln \frac{p(\mathbf{x}|C_1)p(C_1)}{p(\mathbf{x}|C_2)p(C_2)}\). First, we rewrite \(a\):&lt;/p&gt;
&lt;p&gt;\begin{equation*}
a = \ln p(\mathbf{x}|C_1) - \ln p(\mathbf{x}|C_2) + \ln \frac{p(C_1)}{p(C_2)}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;The log of the class conditional density for a Gaussian is&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\ln p(\mathbf{x}|C_k;\mathbf{\mu}_k,\Sigma) =
-\frac{D}{2}\ln(2\pi) - \frac{1}{2}\ln|\Sigma|-\frac{1}{2}(\mathbf{x}-\mathbf{\mu}_k)^T \Sigma^{-1} (\mathbf{x}-\mathbf{\mu}_k).
\end{equation*}&lt;/p&gt;
&lt;p&gt;To simplify the above result, we will group the terms that are not dependent on the class parameters since they are consant:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\ln p(\mathbf{x}|C_k;\mathbf{\mu}_k,\Sigma) =
-\frac{1}{2}(\mathbf{x}-\mathbf{\mu}_k)^T \Sigma^{-1} (\mathbf{x}-\mathbf{\mu}_k) + c.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Observing that this quantity takes on a quadratic form, we can rewrite the above as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\ln p(\mathbf{x}|C_k;\mathbf{\mu}_k,\Sigma) =
-\frac{1}{2}\mathbf{\mu}_k\Sigma^{-1}\mathbf{\mu}_k + \mathbf{x}^T \Sigma^{-1} \mathbf{\mu}_k
-\frac{1}{2}\mathbf{x}^T \Sigma^{-1}\mathbf{x} + c.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Using this, we complete the definition of \(a\):&lt;/p&gt;
&lt;p&gt;\begin{align*}
a &amp;amp;= \ln p(\mathbf{x}|C_1) - \ln p(\mathbf{x}|C_2) + \ln \frac{p(C_1)}{p(C_2)}\\
&amp;amp;= -\frac{1}{2}\mathbf{\mu}_1\Sigma^{-1}\mathbf{\mu}_1 + \mathbf{x}^T \Sigma^{-1} \mathbf{\mu}_1 + \frac{1}{2}\mathbf{\mu}_2\Sigma^{-1}\mathbf{\mu}_2 - \mathbf{x}^T \Sigma^{-1} \mathbf{\mu}_2 + \ln \frac{p(C_1)}{p(C_2)}\\
&amp;amp;= \mathbf{x}^T(\Sigma^{-1}(\mathbf{\mu}_1 - \mathbf{\mu}_2)) - \frac{1}{2}\mathbf{\mu}_1\Sigma^{-1}\mathbf{\mu}_1 + \frac{1}{2}\mathbf{\mu}_2\Sigma^{-1}\mathbf{\mu}_2 + \ln \frac{p(C_1)}{p(C_2)}\\
&amp;amp;= (\Sigma^{-1}(\mathbf{\mu}_1 - \mathbf{\mu}_2))^T \mathbf{x} - \frac{1}{2}\mathbf{\mu}_1\Sigma^{-1}\mathbf{\mu}_1 + \frac{1}{2}\mathbf{\mu}_2\Sigma^{-1}\mathbf{\mu}_2 + \ln \frac{p(C_1)}{p(C_2)}.
\end{align*}&lt;/p&gt;
&lt;p&gt;Finally, we define&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{w} = \Sigma^{-1}(\mathbf{\mu}_1 - \mathbf{\mu}_2)
\end{equation*}&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;\begin{equation*}
w_0 = - \frac{1}{2}\mathbf{\mu}_1\Sigma^{-1}\mathbf{\mu}_1 - \frac{1}{2}\mathbf{\mu}_2\Sigma^{-1}\mathbf{\mu}_2 + \ln \frac{p(C_1)}{p(C_2)}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Thus, our posterior takes on the form&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(C_1|\mathbf{x};\theta) = \sigma(\mathbf{w}^T \mathbf{x} + w_0).
\end{equation*}&lt;/p&gt;
&lt;h3 id=&#34;multiple-classes&#34;&gt;Multiple Classes&lt;/h3&gt;
&lt;p&gt;What if we have more than 2 classes?
Recall that a &lt;strong&gt;generative classifier&lt;/strong&gt; is modeled as&lt;/p&gt;
&lt;p&gt;\[
p(C_k|\mathbf{x};\mathbf{\theta}) = \frac{p(C_k|\mathbf{\theta})p(\mathbf{x}|C_k, \mathbf{\theta})}{\sum_{k&amp;rsquo;}p(C_{k&amp;rsquo;}|\mathbf{\theta})p(\mathbf{x}|C_{k&amp;rsquo;}, \mathbf{\theta})}.
\]&lt;/p&gt;
&lt;p&gt;As stated above, \(\mathbf{\pi}_k = p(C_k|\mathbf{\theta})\) and \(p(\mathbf{x}|C_k,\mathbf{\theta}) = \mathcal{N}(\mathbf{x}|\mathbf{\mu}_c,\Sigma)\).&lt;/p&gt;
&lt;p&gt;For LDA, the covariance matrices are shared across all classes.
This permits a simplification of the class posterior distribution \(p(C_k|\mathbf{x};\mathbf{\theta})\):&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(C_k|\mathbf{x};\mathbf{\theta}) &amp;amp;\propto \mathbf{\pi}_k \exp\big(\mathbf{\mu}_k^T \mathbf{\Sigma}^{-1}\mathbf{x} - \frac{1}{2}\mathbf{x}^T\mathbf{\Sigma}^{-1}\mathbf{x} - \frac{1}{2}\mathbf{\mu}_k\mathbf{\Sigma}^{-1}\mathbf{\mu}_k\big)\\
&amp;amp;= \exp\big(\mathbf{\mu}_k^T \mathbf{\Sigma}^{-1}\mathbf{x}  - \frac{1}{2}\mathbf{\mu}_k\mathbf{\Sigma}^{-1}\mathbf{\mu}_k + \log \mathbf{\pi}_k \big) \exp\big(- \frac{1}{2}\mathbf{x}^T\mathbf{\Sigma}^{-1}\mathbf{x}\big).
\end{align*}&lt;/p&gt;
&lt;p&gt;The term \(\exp\big(- \frac{1}{2}\mathbf{x}^T\mathbf{\Sigma}^{-1}\mathbf{x}\big)\) is placed aside since it is not dependent on the class \(k\).
When divided by the sum per the definition of \(p(C_k|\mathbf{x};\mathbf{\theta})\), it will equal to 1.&lt;/p&gt;
&lt;p&gt;Under this formulation, we let&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{w}_k &amp;amp;= \mathbf{\Sigma}^{-1}\mathbf{\mu}_k\\
\mathbf{b}_k &amp;amp;= -\frac{1}{2}\mathbf{\mu}_k^T \mathbf{\Sigma}^{-1}\mathbf{\mu}_k + \log \mathbf{\pi}_k.
\end{align*}&lt;/p&gt;
&lt;p&gt;This lets us express \(p(C_k|\mathbf{x};\mathbf{\theta})\) as the &lt;strong&gt;softmax&lt;/strong&gt; function:&lt;/p&gt;
&lt;p&gt;\(p(C_k|\mathbf{x};\mathbf{\theta}) = \frac{\exp(\mathbf{w}_k^T\mathbf{x}+\mathbf{b}_k)}{\sum_{k&amp;rsquo;}\exp(\mathbf{w}_{k&amp;rsquo;}^T\mathbf{x}+\mathbf{b}_{k&amp;rsquo;})}\).&lt;/p&gt;
&lt;h2 id=&#34;decision-boundaries&#34;&gt;Decision Boundaries&lt;/h2&gt;
&lt;p&gt;When using LDA, classifications can be made by choosing the class with the highest posterior probability. Geometrically, this decision boundary has a direct connection to logistic regression. The decision boundary is the set of points where the posterior probability of two classes is equal. This is the set of points where the linear discriminant function is equal to 0. This connection follows the derivation given by Kevin P. Murphy in his book &lt;em&gt;Probabilistic Machine Learning: An Introduction&lt;/em&gt; (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Murphy 2022&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In the previous section, the derivation for the posterior probability of class \(C_k\) was written in the form of the softmax function&lt;/p&gt;
&lt;p&gt;\[
p(C_k|\mathbf{x};\mathbf{\theta}) = \frac{\exp(\mathbf{w}_k^T\mathbf{x}+\mathbf{b}_k)}{\sum_{k&amp;rsquo;}\exp(\mathbf{w}_{k&amp;rsquo;}^T\mathbf{x}+\mathbf{b}_{k&amp;rsquo;})}.
\]&lt;/p&gt;
&lt;p&gt;In the binary case, the posterior for class 1 is given by&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(C_1|\mathbf{x};\mathbf{\theta}) &amp;amp;= \frac{\exp(\mathbf{w}_1^T\mathbf{x}+\mathbf{b}_1)}{\exp(\mathbf{w}_1^T\mathbf{x}+\mathbf{b}_1) + \exp(\mathbf{w}_2^T\mathbf{x}+\mathbf{b}_2)}\\
&amp;amp;= \frac{1}{1 + \exp((\mathbf{w}_1 - \mathbf{w}_2)^T\mathbf{x}+(\mathbf{b}_1 - \mathbf{b}_2))}\\
&amp;amp;= \sigma((\mathbf{w}_1 - \mathbf{w}_2)^T\mathbf{x}+(\mathbf{b}_1 - \mathbf{b}_2)).
\end{align*}&lt;/p&gt;
&lt;p&gt;Using the previous definition of \(\mathbf{b}_k\), we can rewrite \(\mathbf{b}_1 - \mathbf{b}_2\) as&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{b}_1 - \mathbf{b}_2 &amp;amp;= -\frac{1}{2}\mathbf{\mu}_1^T \mathbf{\Sigma}^{-1}\mathbf{\mu}_1 + \log \mathbf{\pi}_1 + \frac{1}{2}\mathbf{\mu}_2^T \mathbf{\Sigma}^{-1}\mathbf{\mu}_2 - \log \mathbf{\pi}_2\\
&amp;amp;= -\frac{1}{2}(\mathbf{\mu}_1 - \mathbf{\mu}_2)^T \mathbf{\Sigma}^{-1} (\mathbf{\mu}_1 + \mathbf{\mu}_2) + \log \frac{\mathbf{\pi}_1}{\mathbf{\pi}_2}\\
\end{align*}&lt;/p&gt;
&lt;p&gt;This can be used to define a new weight vector \(\mathbf{w}&amp;rsquo;\) and a point directly between the two class means \(\mathbf{x}_0\):&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{w}&amp;rsquo; &amp;amp;= \mathbf{\Sigma}^{-1}(\mathbf{\mu}_1 - \mathbf{\mu}_2)\\
\mathbf{x}_0 &amp;amp;= \frac{1}{2}(\mathbf{\mu}_1 + \mathbf{\mu}_2) - (\mathbf{\mu}_1 - \mathbf{\mu}_2)\frac{\log \frac{\mathbf{\pi}_1}{\mathbf{\pi}_2}}{(\mathbf{\mu}_1 - \mathbf{\mu}_2)^T \mathbf{\Sigma}^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2)}.
\end{align*}&lt;/p&gt;
&lt;p&gt;With these new terms defined, we have that \(\mathbf{w}&amp;rsquo;^T\mathbf{x}_0 = -(\mathbf{b}_1 - \mathbf{b}_2)\) and the posterior probability for class 1 can be written in the form of binary logistic regression:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(C_1|\mathbf{x};\mathbf{\theta}) = \sigma(\mathbf{w}&amp;rsquo;^T(\mathbf{x} - \mathbf{x}_0)).
\end{equation*}&lt;/p&gt;
&lt;p&gt;The middle point between the two class means \(\mathbf{x}_0\) is the point where the posterior probability of class 1 is 0.5. This is the decision boundary between the two classes. That is, if \(\mathbf{w}&amp;rsquo;^T\mathbf{x} &amp;gt; \mathbf{w}&amp;rsquo;^T\mathbf{x}_0\), then the posterior probability of class 1 is greater than \(0.5\) and the input vector \(\mathbf{x}\) is classified as class 1.&lt;/p&gt;
&lt;p&gt;The split between the class priors controls the location of the decision boundary. If the class priors are equal, then the decision boundary is the point directly between the two class means. If the class priors are not equal, then the decision boundary is shifted towards the class with the higher prior. The figure below visualizes this.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-30_11-23-18_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Decision boundary between two classes (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Murphy 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Decision boundary between two classes (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Murphy 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/h2&gt;
&lt;p&gt;Given our formulation in the previous section, we can estimate the parameters of the model via &lt;strong&gt;maximum likelihood estimation&lt;/strong&gt;. Assuming \(K\) classes with Gaussian class conditional densities, the likelihood function is&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(\mathbf{X}|\mathbf{\theta}) = \prod_{i=1}^n \mathcal{M}(y_i|\mathbf{\pi})\prod_{k=1}^K \mathcal{N}(\mathbf{x}_i|\mathbf{\mu}_k, \mathbf{\Sigma}_k)^{\mathbb{1}(y_i=k)}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Taking the log of this function yields&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\ln p(\mathbf{X}|\mathbf{\theta}) = \Big[\sum_{i=1}^n \sum_{k=1}^K \mathbb{1}(y_i=k)\ln \pi_k\Big] + \sum_{k=1}^K\Big[\sum_{i:y_i=c} \ln \mathcal{N}(\mathbf{x}_n|\mathbf{\mu}_k, \mathbf{\Sigma}_k)\Big].
\end{equation*}&lt;/p&gt;
&lt;p&gt;Given that this is a sum of two different components, we can optimize the multinomial parameter \(\mathbf{\pi}\) and the class Gaussian parameters \((\mathbf{\mu}_k, \mathbf{\Sigma}_k)\) separately.&lt;/p&gt;
&lt;h3 id=&#34;class-prior&#34;&gt;Class Prior&lt;/h3&gt;
&lt;p&gt;For multinomial distributions, the class prior parameter estimation \(\hat{\pi}_k\) is easily calculated by counting the number of samples belonging to class \(k\) and dividing it by the total number of samples.&lt;/p&gt;
&lt;p&gt;\[
\hat{\pi}_k = \frac{n_k}{n}
\]&lt;/p&gt;
&lt;h3 id=&#34;class-gaussians&#34;&gt;Class Gaussians&lt;/h3&gt;
&lt;p&gt;The Gaussian parameters can be calculated as discussed during the probability review. The parameter estimates are&lt;/p&gt;
&lt;p&gt;\begin{align*}
\hat{\mathbf{u}}_k &amp;amp;= \frac{1}{n_k}\sum_{i:y_i=k}\mathbf{x}_i\\
\hat{\Sigma}_k &amp;amp;= \frac{1}{n_k}\sum_{i:y_i=k}(\mathbf{x}_i - \hat{\mathbf{\mu}}_k)(\mathbf{x}_i - \hat{\mathbf{\mu}}_k)^T
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;the-decision-boundary&#34;&gt;The Decision Boundary&lt;/h3&gt;
&lt;p&gt;The decision boundary between two classes can be visualized at the point when \(p(C_k|\mathbf{x};\theta) = 0.5\).&lt;/p&gt;
&lt;h2 id=&#34;quadratic-descriminant-analysis&#34;&gt;Quadratic Descriminant Analysis&lt;/h2&gt;
&lt;p&gt;Linear Discriminant Analysis is a special case of Quadratic Discriminant Analysis (QDA) where the covariance matrices are shared across all classes. Assuming each class conditional density is Gaussian, the posterior probability is given by&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(C_k|\mathbf{x};\theta) \propto \pi_k\mathcal{N}(\mathbf{x}|\mathbf{\mu}_k,\Sigma_k).
\end{equation*}&lt;/p&gt;
&lt;p&gt;Taking the log of this function yields&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\ln p(C_k|\mathbf{x};\theta) = \ln \pi_k - \frac{1}{2}\ln |\Sigma_k| - \frac{1}{2}(\mathbf{x} - \mathbf{\mu}_k)^T \Sigma_k^{-1}(\mathbf{x} - \mathbf{\mu}_k) + c.
\end{equation*}&lt;/p&gt;
&lt;p&gt;With LDA, the term \(\frac{1}{2}\ln |\Sigma_k|\) is constant across all classes, so we treat it as another constant. Since QDA considers a different covariance matrix for each class, we must keep this term in the equation.&lt;/p&gt;
&lt;h3 id=&#34;quadratic-decision-boundary&#34;&gt;Quadratic Decision Boundary&lt;/h3&gt;
&lt;p&gt;In the more general case of QDA, the decision boundary is quadratic, leading to a quadratic discriminant function. As shown above, the posterior probability function for LDA is linear in \(\mathbf{x}\), which leads to a linear discriminant function.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;See &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/blob/main/logistic_regression/lda.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt; for an example using scikit-learn.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Murphy, Kevin P. 2022. &lt;i&gt;Probabilistic Machine Learning: An Introduction&lt;/i&gt;. MIT Press.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Filters</title>
      <link>https://ajdillhoff.github.io/notes/linear_filters/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/linear_filters/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#smoothing&#34;&gt;Smoothing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#convolution&#34;&gt;Convolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gaussian-filters&#34;&gt;Gaussian Filters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#image-derivatives&#34;&gt;Image Derivatives&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;How do we detect specific patterns in images (eyes, nose, spots, etc.)?&lt;/li&gt;
&lt;li&gt;Weighted sums of pixel values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;smoothing&#34;&gt;Smoothing&lt;/h2&gt;
&lt;p&gt;When discussing resizing and interpolation, we saw how the choice of scale factor and rotation can produce aliasing in images. Typically, this effect is hidden using some sort of smoothing.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s first look at the case of downsampling an image to 10\% of its original size. If we use nearest neighbor interpolation, the result is very blocky, as seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_20-49-14_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Rocinante cropped and scaled to 10% of the original size. Source: The Expanse&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Rocinante cropped and scaled to 10% of the original size. Source: The Expanse
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Effectively, an entire block of pixels in the original image is being mapped to the nearest neighbor. We are losing a lot of information from the original image. The figure below shows an overlay of the low resolution grid over a patch of the high resolution image.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_21-48-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;The center dots show the selected pixel value for the downsampled grid.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;The center dots show the selected pixel value for the downsampled grid.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;computing-the-average&#34;&gt;Computing the Average&lt;/h3&gt;
&lt;p&gt;Instead of naively selecting one pixel to represent an entire block, we could compute the average pixel value of all pixels within that block. This is a simple as taking an equal contribution from each pixel in the block and dividing by the total number of pixels in the block. An example of such a block is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_21-55-06_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;A (3 times 3) averaging filter.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;A (3 times 3) averaging filter.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;By varying the size of this filter, we can effectively change the factor for which we smooth the aliasing.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_21-32-36_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Original image smoothed with (9 times 9) average filter before downsampling to 10% of its original size.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Original image smoothed with (9 times 9) average filter before downsampling to 10% of its original size.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;convolution&#34;&gt;Convolution&lt;/h2&gt;
&lt;p&gt;How can we apply the averaging filter to an image effectively? We slide the filter across the image starting at the first pixel in the first row and move row by row until all pixels have been computed.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_22-20-56_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;The takes input from all pixels under it and computes the average.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;The takes input from all pixels under it and computes the average.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_23-13-19_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Final portion of image after kernel is applied.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Final portion of image after kernel is applied.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This is performed by the &lt;strong&gt;convolution&lt;/strong&gt; operation. It is defined as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
g(x, y) = \omega * f(x, y) = \sum_{dx = -a}^a \sum_{dy=-b}^b \omega(dw, dy)f(x + dx, y + dy).
\end{equation*}&lt;/p&gt;
&lt;p&gt;The range \((-a, a)\) represents the rows of the kernel and \((-b, b)\) the range of the columns of the kernel. The center of the kernel is at \((dx = 0, dy = 0)\).&lt;/p&gt;
&lt;p&gt;This operation is one of, if not the most, important operations in computer vision. It is used to apply filters, but we will later see it as one of the guiding operators for feature extraction and deep learning methods.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_15-40-19_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;An image (f) convolved with kernel (h). Source: Szeliski&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;An image (f) convolved with kernel (h). Source: Szeliski
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;properties&#34;&gt;Properties&lt;/h3&gt;
&lt;h4 id=&#34;commutativity&#34;&gt;Commutativity&lt;/h4&gt;
&lt;p&gt;\(f * g = g * f\)&lt;/p&gt;
&lt;h4 id=&#34;associativity&#34;&gt;Associativity&lt;/h4&gt;
&lt;p&gt;\(f * (g * h) = (f * g) * h\)&lt;/p&gt;
&lt;h4 id=&#34;distributivity&#34;&gt;Distributivity&lt;/h4&gt;
&lt;p&gt;\(f * (g + h) = (f * g) + (f * h)\)&lt;/p&gt;
&lt;h3 id=&#34;shift-invariant-linear-systems&#34;&gt;Shift Invariant Linear Systems&lt;/h3&gt;
&lt;p&gt;Convolution is a &lt;em&gt;linear shift-invariant&lt;/em&gt; operator. That is, it obeys the following properties.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Superposition:&lt;/strong&gt; The response of the sum of the input is the sum of the individual responses.&lt;/p&gt;
&lt;p&gt;\[
R(f + g) = R(f) + R(g)
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Scaling:&lt;/strong&gt; The response to a scaled input is equal to the scaled response of the same input.&lt;/p&gt;
&lt;p&gt;\[
R(kf) = kR(f)
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Shift Invariance:&lt;/strong&gt; The response to a translated input is equal to the translation of the response to the input.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;system&lt;/strong&gt; is linear if it satisfies both the superposition and scaling properties. Further, it is a shift-invariant linear system if it is linear and satisfies the shift-invariance property.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is the average box filter linear?&lt;/strong&gt; Yes, it is applied with convolution which behaves the same everywhere.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is thresholding a linear system?&lt;/strong&gt; No, it can be shown that \(f(n, m) + g(n, m) &amp;gt; T\), but \(f(n, m) &amp;lt; T\) and \(g(n, m) &amp;lt; T\).&lt;/p&gt;
&lt;h2 id=&#34;gaussian-filters&#34;&gt;Gaussian Filters&lt;/h2&gt;
&lt;p&gt;Blurring an image using a box filter does not simulate realistic blurring as well as Gaussian filters do. The following figure exemplifies the difference between the two.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_16-57-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;The left image is blurred using a uniform average box filter. The right image is blurred with a Gaussian filter.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;The left image is blurred using a uniform average box filter. The right image is blurred with a Gaussian filter.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The shortcomings of the average box filter can be seen when viewing the artifacting visible at edges, especially at corners.&lt;/p&gt;
&lt;p&gt;A Guassian kernel is defined as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
G(x, y;\sigma) = \frac{1}{2\pi \sigma^2}\exp\Big(-\frac{(x^2 + y^2)}{2 \sigma^2}\Big).
\end{equation*}&lt;/p&gt;
&lt;p&gt;In effect, it enforces a greater contribution from neighbors near the pixel and a smaller contribution from distant pixels.&lt;/p&gt;
&lt;h2 id=&#34;image-derivatives&#34;&gt;Image Derivatives&lt;/h2&gt;
&lt;p&gt;We can use convolution to approximate the partial derivative (or finite difference) of an image. Recall that&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{\partial f}{\partial x} = \lim\limits_{\epsilon \rightarrow 0} \frac{f(x + \epsilon, y) - f(x, y)}{\epsilon}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;We can estimate this as a finite difference&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{\partial h}{\partial x} \approx h_{i+1, j} - h_{i-1, j}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Which can be applied via convolution given a kernal&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathcal{H} =
\begin{bmatrix}
0 &amp;amp; 0 &amp;amp; 0\\
-1 &amp;amp; 0 &amp;amp; 1\\
0 &amp;amp; 0 &amp;amp; 0\\
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Applying both the horizontal and vertical derivative kernels to an image to a simple square shows the detection of horizontal and vertical edges.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_17-36-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Horizontal (right) and vertical (middle) derivative kernels applied to the original image (left).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Horizontal (right) and vertical (middle) derivative kernels applied to the original image (left).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The results of applying the derivative kernels are referred to as vertical edge and horizontal edge &lt;strong&gt;scores&lt;/strong&gt;. Consider the middle image in the figure above. The left edge has pixel values of 255; the right edge has -255. In either case, a high absolute score reveals that there is an edge. These scores report the direction of greatest change. The 255 on the left edge indicates the direction is to the right, while the -255 score indicates the direction is to the left. All other instances of the image return a rate of change of 0.
Let&amp;rsquo;s see how these filters perform on a more interesting image.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_22-45-59_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;Vertical derivative filter (left) and horizontal derivative filter (right).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;Vertical derivative filter (left) and horizontal derivative filter (right).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>https://ajdillhoff.github.io/notes/logistic_regression/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/logistic_regression/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#picking-a-model&#34;&gt;Picking a Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#binary-classification&#34;&gt;Binary Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiple-classes&#34;&gt;Multiple Classes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Slides for these notes are available &lt;a href=&#34;https://ajdillhoff.github.io/teaching/cse6363/lectures/logistic_regression.pdf&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;With &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_regression/&#34;&gt;Linear Regression&lt;/a&gt; we were able to fit a model to our data in order to make inferences on unseen data points. In the examples, both the input features and observation were continuous. With logistic regression, we will use similar models to classify the data points based on their input features. We start out with the simplest approach: we assume that the data is linearly separable and can be assigned one of \(K\) discrete classes.&lt;/p&gt;
&lt;p&gt;In the binary case, the target variable will takes on either a 0 or 1. For \(K &amp;gt; 2\), we will use a \(K\) dimensional vector that has a 1 corresponding to the class encoding for that input and a 0 for all other positions. For example, if our possible target classes were \(\{\text{car, truck, person}\}\), then a target vector for \(\text{person}\) would be \(\mathbf{y} = [0, 0, 1]^T\).&lt;/p&gt;
&lt;p&gt;This article will stick to a discriminative approach to logistic regression. That is, we define a discriminant function which assigns each data input \(\mathbf{x}\) to a class. For a probabilistic perspective, see &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_discriminant_analysis/&#34;&gt;Linear Discriminant Analysis&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;picking-a-model&#34;&gt;Picking a Model&lt;/h2&gt;
&lt;p&gt;We will again start with a linear model \(y = f(\mathbf{x}; \mathbf{w})\). Unlike the model used with &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_regression/&#34;&gt;Linear Regression&lt;/a&gt;, ours will need to predict a discrete class label. The logistic model is often approached by introducing the &lt;strong&gt;odds&lt;/strong&gt; of an event occurring:&lt;/p&gt;
&lt;p&gt;\[
\frac{p}{1-p},
\]&lt;/p&gt;
&lt;p&gt;where \(p\) is the probability of the event happening.
As \(p\) increases, the odds of it happening increase exponentially.&lt;/p&gt;
&lt;p&gt;Our input \(p\) represents the probability in range \((0, 1)\) which we want to map to the real number space.
To approximate this, we apply the natural logarithm to the odds.&lt;/p&gt;
&lt;p&gt;The logistic model assumes a linear relationship between the linear model \(\mathbf{w}^T\mathbf{x}\) and the logit function&lt;/p&gt;
&lt;p&gt;\[
\text{logit}(p) = \ln \frac{p}{1-p}.
\]&lt;/p&gt;
&lt;p&gt;This function maps a value in range \((0, 1)\) to the space of real numbers.
Under this assumption, we can write&lt;/p&gt;
&lt;p&gt;\[
\text{logit}(p) = \mathbf{w}^T\mathbf{x}.
\]&lt;/p&gt;
&lt;p&gt;This assumption is reasonable because we ultimately want to predict the &lt;strong&gt;probability&lt;/strong&gt; that an event occurs.
The output should then be in the range of \((0, 1)\).
If the logit function produces output in the range of real numbers, as does our linear model \(\mathbf{w}^T\mathbf{x}\), then we ultimately want a function that maps &lt;strong&gt;from&lt;/strong&gt; the range of real numbers to &lt;strong&gt;to&lt;/strong&gt; \((0, 1)\).&lt;/p&gt;
&lt;p&gt;We can achieve this using the &lt;strong&gt;inverse&lt;/strong&gt; of the logit function, the logistic sigmoid function.
It is defined as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\sigma(z) = \frac{1}{1 + \exp(-z)},
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(z = \mathbf{w}^T\mathbf{x}\).&lt;/p&gt;
&lt;p&gt;The reason for this choice becomes more clear when plotting the function, as seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-23_17-43-13_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;The logistic sigmoid function. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;The logistic sigmoid function. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The inputs on the \(x\) axis are clamped to values between 0 and 1. It is also called a squashing function because of this property. This form is also convenient and arises naturally in many probabilistic settings. With this nonlinear activation function, the form of our model becomes&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f(\mathbf{x};\mathbf{w}) = h(\mathbf{w}^T\mathbf{x}),
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(h\) is our choice of activation function.&lt;/p&gt;
&lt;p&gt;The logistic sigmoid function also has a convenient derivative, which is useful when solving for the model parameters via gradient descent.&lt;/p&gt;
&lt;p&gt;\[
\frac{d}{dx} = \sigma(x)(1 - \sigma(x))
\]&lt;/p&gt;
&lt;h2 id=&#34;binary-classification&#34;&gt;Binary Classification&lt;/h2&gt;
&lt;p&gt;Consider a simple dataset with 2 features per data sample. Our goal is to classify the data as being one of two possible classes. For now, we&amp;rsquo;ll drop the activation function so that our model represents a line that separates both groups of data.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-23_18-10-03_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Two groups of data that are very clearly linearly separable.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Two groups of data that are very clearly linearly separable.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the binary case, we are approximating \(p(C_1|\mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{x})\).
Then \(p(C_2|\mathbf{x}) = 1 - p(C_1| \mathbf{x})\).&lt;/p&gt;
&lt;p&gt;The parameter vector \(\mathbf{w}\) is orthogonal to the decision boundary that separates the two classes. The model output is such that \(f(\mathbf{x};\mathbf{w}) = 0\) when \(\mathbf{x}\) lies on the decision boundary. If \(f(\mathbf{x};\mathbf{w}) \geq 0\) then \(\mathbf{x}\) is assigned to class 1. It is assigned to class 2 otherwise. Since we originally stated that the model should predict either a 0 or 1, we can use the model result as input to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Heaviside_step_function&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Heaviside step function&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;fitting-the-model-via-maximum-likelihood&#34;&gt;Fitting the Model via Maximum Likelihood&lt;/h3&gt;
&lt;p&gt;Let \(y_i \in \{0, 1\}\) be the target for binary classification and \(\hat{y}_i \in (0, 1)\) be the output of a logistic regression model.
The likelihood function is&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{y}|\mathbf{w}) = \prod_{i=1}^n \hat{y}_i^{y_i}(1 - \hat{y}_i)^{1 - y_i}.
\]&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s briefly take a look at \(\hat{y}_i^{y_i}(1 - \hat{y}_i)^{1 - y_i}\) to understand the output when the model correctly predicts the \(i^{\text{th}}\) sample or not.
Since the output is restricted within the range \((0, 1)\), the model will never produce 0 or 1.&lt;/p&gt;
&lt;p&gt;If the target \(y_i = 0\), then we can evaluate the subexpression \(1 - \hat{y}_i\).
In this case, the likelihood increases as \(\hat{y}_i\) decreases.&lt;/p&gt;
&lt;p&gt;If the target \(y_i = 1\), then we evaluate the subexpression \(\hat{y}_i\).&lt;/p&gt;
&lt;p&gt;When fitting this model, we want to define an error measure based on the above function.
This is done by taking the negative logarithm of \(p(\mathbf{y}|\mathbf{w})\).&lt;/p&gt;
&lt;p&gt;\[
E(\mathbf{w}) = -\ln p(\mathbf{y}|\mathbf{w}) = -\sum_{i=1}^n y_i \ln \hat{y}_i + (1 - y_i) \ln (1 - \hat{y}_i)
\]&lt;/p&gt;
&lt;p&gt;This function is commonly referred to as the &lt;strong&gt;cross-entropy&lt;/strong&gt; function.&lt;/p&gt;
&lt;p&gt;If we use this as an objective function for gradient descent with the understanding that \(\hat{y}_i = \sigma(\mathbf{w}^T \mathbf{x})\), then the gradient of the error function is&lt;/p&gt;
&lt;p&gt;\[
\nabla E(\mathbf{w}) = \sum_{i=1}^n (\hat{y}_i - y_i)\mathbf{x}_i.
\]&lt;/p&gt;
&lt;p&gt;This results in a similar update rule as linear regression, even though the problem itself is different.&lt;/p&gt;
&lt;h3 id=&#34;measuring-classifier-performance&#34;&gt;Measuring Classifier Performance&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;How do we determine how well our model is performing?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will use L1 loss because it works well with discrete outputs. L1 loss is defined as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
L_1 = \sum_{i}|\hat{y}_i - y_i|,
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(\hat{y}_i\) is the ground truth corresponding to \(\mathbf{x}_i\) and \(y_i\) is the output of our model. We can further normalize this loss to bound it between 0 and 1. Either way, a loss of 0 will indicate 100% classification accuracy.&lt;/p&gt;
&lt;h2 id=&#34;multiple-classes&#34;&gt;Multiple Classes&lt;/h2&gt;
&lt;p&gt;In multiclass logistic regression, we are dealing with target values that can take on one of \(k\) values \(y \in \{1, 2, \dots, k\}\).
If our goal is to model the distribution over \(K\) classes, a multinomial distribution is the obvious choice.
Let \(p(y|\mathbf{x};\theta)\) be a distribution over \(K\) numbers \(w_1, \dots, w_K\) that sum to 1.
Our parameterized model cannot be represented exactly by a multinomial distribution, so we will derive it so that it satisfies the same constraints.&lt;/p&gt;
&lt;p&gt;We can start by introducing \(K\) parameter vectors \(\mathbf{w}_1, \dots, \mathbf{w}_K \in \mathbb{R}^{d}\), where \(d\) is the number of input features.
Then each vector \(\mathbf{w}_k^T \mathbf{x}\) represents \(p(C_k | \mathbf{x};\mathbf{w}_k)\).
We need to &lt;em&gt;squash&lt;/em&gt; each \(\mathbf{w}_k^T \mathbf{x}\) so that the output sums to 1.&lt;/p&gt;
&lt;p&gt;This is accomplished via the &lt;strong&gt;softmax function&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;\[
p(C_k|\mathbf{x}) = \frac{\exp(\mathbf{w}_k^T \mathbf{x})}{\sum_{j} \exp(\mathbf{w}_j^T \mathbf{x})}.
\]&lt;/p&gt;
&lt;h3 id=&#34;maximum-likelihood&#34;&gt;Maximum Likelihood&lt;/h3&gt;
&lt;p&gt;The target vector for each sample is \(\mathbf{y}_i \in \mathbb{R}^{k}\).
Likewise, the output vector \(\hat{\mathbf{y}}_i\) also has \(k\) elements.&lt;/p&gt;
&lt;p&gt;The maximum likelihood function for the multiclass setting is given by&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{Y}|\mathbf{W}) = \prod_{i=1}^n \prod_{k=1}^K p(C_k|\mathbf{x}_i)^{y_{ik}} = \prod_{i=1}^n \prod_{k=1}^K \hat{y}_{ik}^{y_{ik}}.
\]&lt;/p&gt;
&lt;p&gt;\(\mathbf{Y} \in \mathbb{R}^{n \times K}\) is a matrix of all target vectors in the data set.
As with the binary case, we can take the negative logarithm of this function to produce an error function.&lt;/p&gt;
&lt;p&gt;\[
E(\mathbf{W}) = -\ln p(\mathbf{Y}|\mathbf{W}) = -\sum_{i=1}^n \sum_{k=1}^K y_{ik} \ln \hat{y}_{ik}
\]&lt;/p&gt;
&lt;p&gt;This is the &lt;strong&gt;cross-entropy&lt;/strong&gt; function for multiclass classification.&lt;/p&gt;
&lt;p&gt;The gradient of this function is given as&lt;/p&gt;
&lt;p&gt;\[
\nabla_{\mathbf{w}_j}E(\mathbf{W}) = \sum_{i=1}^n (\hat{y}_{ij} - y_{ij}) \mathbf{x}_i.
\]&lt;/p&gt;
&lt;p&gt;Part of your first assignment will be to work through the derivation of this function.
It is standard practice at this point, but it is highly valuable to understand how the result was produced.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Naive Bayes</title>
      <link>https://ajdillhoff.github.io/notes/naive_bayes/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/naive_bayes/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#making-a-decision&#34;&gt;Making a Decision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#relation-to-multinomial-logistic-regression&#34;&gt;Relation to Multinomial Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mnist-example&#34;&gt;MNIST Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gaussian-formulation&#34;&gt;Gaussian Formulation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Slides for these notes can be found &lt;a href=&#34;https://ajdillhoff.github.io/teaching/cse6363/lectures/naive_bayes.pdf&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;To motivate naive Bayes classifiers, let&amp;rsquo;s look at slightly more complex data. The MNIST dataset was one of the standard benchmarks for computer vision classification algorithms for a long time. It remains useful for educational purposes. The dataset consists of 60,000 training images and 10,000 testing images of size \(28 \times 28\). These images depict handwritten digits. For the purposes of this section, we will work with binary version of the images. This implies that each data sample has 784 binary features.&lt;/p&gt;
&lt;p&gt;We will use the naive Bayes classifier to make an image classification model which predicts the class of digit given a new image. Each image will be represented by a vector \(\mathbf{x} \in \mathbb{R}^{784}\). Modeling \(p(\mathbf{x}|C_k)\) with a multinomial distribution would require \(10^{784} - 1\) parameters since there are 10 classes and 784 features.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-01_18-47-49_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Samples of the MNIST training dataset.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Samples of the MNIST training dataset.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;With the naive assumption that the features are independent conditioned on the class, the number model parameters becomes \(10 \times 784\).&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;A naive Bayes classifier makes the assumption that the features of the data are independent. That is,
\[
p(\mathbf{x}|C_k, \mathbf{\theta}) = \prod_{d=1}^D p(x_i|C_k, \theta_{dk}),
\]
where \(\mathbf{\theta}_{dk}\) are the parameters for the class conditional density for class \(k\) and feature \(d\). Using the MNIST dataset, \(\mathbf{\theta}_{dk} \in \mathbb{R}^{784}\). The posterior distribution is then&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(C_k|\mathbf{x},\mathbf{\theta}) = \frac{p(C_k|\mathbf{\pi})\prod_{i=1}^Dp(x_i|C_k, \mathbf{\theta}_{dk})}{\sum_{k&amp;rsquo;}p(C_{k&amp;rsquo;}|\mathbf{\pi})\prod_{i=1}^Dp(x_i|C_{k&amp;rsquo;},\mathbf{\theta}_{dk&amp;rsquo;})}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;If we convert the input images to binary, the class conditional density \(p(\mathbf{x}|C_k, \mathbf{\theta})\) takes on the Bernoulli pdf. That is,&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(\mathbf{x}|C_k, \mathbf{\theta}) = \prod_{i=1}^D\text{Ber}(x_i|\mathbf{\theta}_{dk}).
\end{equation*}&lt;/p&gt;
&lt;p&gt;The parameter \(\theta_{dk}\) is the probability that the feature \(x_i=1\) given class \(C_k\).&lt;/p&gt;
&lt;h2 id=&#34;maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/h2&gt;
&lt;p&gt;Fitting a naive Bayes classifier is relatively simple using MLE. The likelihood is given by&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(\mathbf{X}, \mathbf{y}|\mathbf{\theta}) = \prod_{n=1}^N \mathcal{M}(y_n|\mathbf{\pi})\prod_{d=1}^D\prod_{k=1}^{K}p(x_{nd}|\mathbf{\theta}_{dk})^{\mathbb{1}(y_n=k)}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;To derive the estimators, we first take the log of the likelihood:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\ln p(\mathbf{X}, \mathbf{y}|\mathbf{\theta}) = \Bigg[\sum_{n=1}^N\sum_{k=1}^K \mathbb{1}(y_n = k)\ln \pi_k\Bigg] + \sum_{k=1}^K\sum_{d=1}^D\Bigg[\sum_{n:y_n=k}\ln p(x_{nd}|\theta_{dk})\Bigg].
\end{equation*}&lt;/p&gt;
&lt;p&gt;Thus, we have a term for the the multinomial and terms for the class-feature parameters. As with previous models that use a multinomial form, the parameter estimate for the first term is computed as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\hat{\pi}_k = \frac{N_k}{N}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;The features used in our data are binary, so the parameter estimate for each \(\hat{\theta}_{dk}\) follows the Bernoulli distribution:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\hat{\theta}_{dk} = \frac{N_{dk}}{N_{k}}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;That is, the number of times that feature \(d\) is in an example of class \(k\) divided by the total number of samples for class \(k\).&lt;/p&gt;
&lt;h2 id=&#34;making-a-decision&#34;&gt;Making a Decision&lt;/h2&gt;
&lt;p&gt;Given parameters \(\mathbf{\theta}\), how can we classify a given data sample?&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\text{arg}\max_{k}p(y=k)\prod_{i}p(x_i|y=k)
\end{equation*}&lt;/p&gt;
&lt;h2 id=&#34;relation-to-multinomial-logistic-regression&#34;&gt;Relation to Multinomial Logistic Regression&lt;/h2&gt;
&lt;p&gt;Consider some data with discrete features having one of \(K\) states, then \(x_{dk} = \mathbb{1}(x_d=k)\). The class conditional density, in this case, follows a multinomial distribution:&lt;/p&gt;
&lt;p&gt;\[
p(y=c|\mathbf{x}, \mathbf{\theta}) = \prod_{d=1}^D \prod_{k=1}^K \theta_{dck}^{x_{dk}}.
\]&lt;/p&gt;
&lt;p&gt;We can see a connection between naive Bayes and logistic regression when we evaluate the posterior over classes:&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(y=c|\mathbf{x}, \mathbf{\theta}) &amp;amp;= \frac{p(y)p(\mathbf{x}|y, \mathbf{\theta})}{p(\mathbf{x})}\\
&amp;amp;= \frac{\pi_c \prod_{d} \prod_{k} \theta_{dck}^{x_{dk}}}{\sum_{c&amp;rsquo;}\pi_{c&amp;rsquo;}\prod_{d}\prod_{k}\theta_{dc&amp;rsquo;k}^{x_{dk}}} \\
&amp;amp;= \frac{\exp[\log \pi_c + \sum_d \sum_k x_{dk}\log \theta_{dck}]}{\sum_{c&amp;rsquo;} \exp[\log \pi_{c&amp;rsquo;} + \sum_d \sum_k x_{dk} \log \theta_{dc&amp;rsquo;k}]}.
\end{align*}&lt;/p&gt;
&lt;p&gt;This has the same form as the softmax function:&lt;/p&gt;
&lt;p&gt;\[
p(y=c|\mathbf{x}, \mathbf{\theta}) = \frac{e^{\beta^{T}_c \mathbf{x} + \gamma_c}}{\sum_{c&amp;rsquo;=1}^C e^{\beta^{T}_{c&amp;rsquo;}\mathbf{x} + \gamma_{c&amp;rsquo;}}}
\]&lt;/p&gt;
&lt;h2 id=&#34;mnist-example&#34;&gt;MNIST Example&lt;/h2&gt;
&lt;p&gt;With the model definition and parameter estimates defined, we can fit and evaluate the model. Using &lt;code&gt;scikit-learn&lt;/code&gt;, we fit a Bernoulli naive Bayes classifier on the MNIST training set: &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/blob/main/logistic_regression/naive_bayes_mnist.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Naive Bayes&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;gaussian-formulation&#34;&gt;Gaussian Formulation&lt;/h2&gt;
&lt;p&gt;If our features are continuous, we would model them with univariate Gaussians.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Neural Networks</title>
      <link>https://ajdillhoff.github.io/notes/neural_networks/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/neural_networks/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#forward-pass&#34;&gt;Forward Pass&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#activation-functions&#34;&gt;Activation Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multi-class-classification&#34;&gt;Multi-Class Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backpropagation&#34;&gt;Backpropagation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#non-convex-optimization&#34;&gt;Non-Convex Optimization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://playground.tensorflow.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://playground.tensorflow.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Previously, we studied the &lt;a href=&#34;https://ajdillhoff.github.io/notes/perceptron/&#34;&gt;Perceptron&lt;/a&gt; and saw that while it made for a simple linear classifier, it is severely limited to problems that are already linearly separable.
This limitation was resolved by introduding a hidden layer with multiple perceptron units, aptly named Multi-Layer Perceptrons.&lt;/p&gt;
&lt;p&gt;In this series, we will explore the more general method of neural networks.
We will see that even a network of only two layers can approximate any continuous functional mapping to arbitrary accuracy.
Through a discussion about network architectures, activation functions, and backpropagation, we will understand and use neural networks to resolve a large number of both classification and regression tasks.&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;We will take an abstract view of neural networks in which any formulation of a neural network defines a nonlinear mapping from an input space to some output space.
This implies that our choice of activation function &lt;strong&gt;must&lt;/strong&gt; be nonlinear.
The function we create will be parameterized by some weight matrix \(W\).
Thus, any neural network can be simply formulated as&lt;/p&gt;
&lt;p&gt;\[
f(\mathbf{x};W).
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-12_18-08-25_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;General neural network diagram.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;General neural network diagram.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;A neural network is in part defined by its &lt;strong&gt;layers&lt;/strong&gt;, the number of &lt;strong&gt;nodes&lt;/strong&gt; in each layer, the choice of &lt;strong&gt;activation function&lt;/strong&gt;, and the choice of &lt;strong&gt;loss function&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Each layer has a number of weights equal to the number of input nodes times the number of output nodes.
This is commonly represented as a weight matrix \(W\).&lt;/p&gt;
&lt;p&gt;The network produces output through the &lt;strong&gt;forward pass&lt;/strong&gt; and computes the gradients with respect to that output in the &lt;strong&gt;backwards pass&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h2&gt;
&lt;p&gt;Computing the output is done in what is called the &lt;strong&gt;forward pass&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Our neural network function takes in an input \(\mathbf{x} \in \mathbb{R}^D\), where \(D\) is the number of features in our input space.
Each output node \(a_j\) in a hidden layer \(h_l\) has a corresponding weight vector \(\mathbf{w}_j^{(l)}\).
The intermediate output of a hidden layer \(h_l\) is a linear combination of the weights and the input followed by some nonlinear function. Node \(a_j\) of a hidden layer is computed as&lt;/p&gt;
&lt;p&gt;\[
a_j = \sum_{i=1}^d w_{ji}^{(l)} x_{i} + w_{j0}^{(l)}.
\]&lt;/p&gt;
&lt;p&gt;As with &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_regression/&#34;&gt;Linear Regression&lt;/a&gt;, we will prepend a constant 1 to our input so that the computation is simply&lt;/p&gt;
&lt;p&gt;\[
a_{j} = \sum_{i=0}^d w_{ji}^{(i)} x_i = \mathbf{w}_j^T \mathbf{x}.
\]&lt;/p&gt;
&lt;p&gt;The final output of the hidden layer is \(a_j\) transformed by a nonlinear function \(g\) such that&lt;/p&gt;
&lt;p&gt;\[
z_j = g(a_j).
\]&lt;/p&gt;
&lt;p&gt;We can combine all weight vectors for each hidden layer node into a weight matrix \(W \in \mathbb{R}^{n \times d}\), where \(n\) is the number of nodes in the layer and \(d\) is the number of input features such that&lt;/p&gt;
&lt;p&gt;\begin{equation*}
W =
\begin{bmatrix}
\mathbf{w}_1^T\\
\vdots\\
\mathbf{w}_n^T\\
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Then the output of the hidden layer can be computed as&lt;/p&gt;
&lt;p&gt;\[
\mathbf{a} = W\mathbf{x}.
\]&lt;/p&gt;
&lt;p&gt;If you instead wanted to separate the bias term, this would be&lt;/p&gt;
&lt;p&gt;\[
\mathbf{a} = W\mathbf{x} + \mathbf{b}.
\]&lt;/p&gt;
&lt;p&gt;Using the notation to specify the individual layer, we can write the output of a full network.
Let \(W^{(l)} \in \mathbb{R}^{n_{l} \times n_{l-1}}\) be the weights for layer \(l\) which have \(n_{l-1}\) input connections and \(n_{l}\) output nodes.
The activation function for layer \(l\) is given by \(g^{(l)}\).&lt;/p&gt;
&lt;p&gt;The complete forward pass of the network is computed by repeating the following step for all layers:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{z}^{(l)} = g^{(l)}(\mathbf{a}^{(l-1)}),
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\mathbf{a}^{(l-1)} = W^{(l-1)}\mathbf{z}^{(l-1)} + \mathbf{b}^{(l-1)}.
\]&lt;/p&gt;
&lt;p&gt;Once all layers have been computed, then the output of the last layer, \(\hat{\mathbf{y}}^{(L)}\) is used as the final output of the model.
For training, this is compared with some ground truth label \(\mathbf{y}\) using a loss function \(\mathcal{L}\):&lt;/p&gt;
&lt;p&gt;\[
\mathcal{L}(\hat{\mathbf{y}}, \mathbf{y}).
\]&lt;/p&gt;
&lt;h3 id=&#34;xor-example&#34;&gt;XOR Example&lt;/h3&gt;
&lt;p&gt;Consider the XOR problem. A single &lt;a href=&#34;https://ajdillhoff.github.io/notes/perceptron/&#34;&gt;Perceptron&lt;/a&gt; was unable to solve that problem.
However, adding a hidden layer and forming a multi-layer perceptron network allowed for a more complex decision boundary.
Consider the network below and produce the output given all combinations of binary input:
\(\{(0, 0), (0, 1), (1, 0), (1, 1)\}\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-13_22-36-49_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A network with 1 hidden layer that computes XOR. Source: &amp;lt;https://athitsos.utasites.cloud/courses/cse4309_fall2021/lectures/09a_neural_networks.pdf&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A network with 1 hidden layer that computes XOR. Source: &lt;a href=&#34;https://athitsos.utasites.cloud/courses/cse4309_fall2021/lectures/09a_neural_networks.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://athitsos.utasites.cloud/courses/cse4309_fall2021/lectures/09a_neural_networks.pdf&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;activation-functions&#34;&gt;Activation Functions&lt;/h2&gt;
&lt;h3 id=&#34;sigmoid-function&#34;&gt;Sigmoid Function&lt;/h3&gt;
&lt;p&gt;\[
g(x) = \frac{1}{1 + e^{-x}}
\]&lt;/p&gt;
&lt;p&gt;The logistic sigmoid function serves two purposes.
First, it allows the output of the neuron to be interpreted as a posterior probability.
Note that this is not actually a probability.
Second, it is a continuous function for which the derivative can be computed:&lt;/p&gt;
&lt;p&gt;\[
g&amp;rsquo;(x) = g(x)(1 - g(x)).
\]&lt;/p&gt;
&lt;h3 id=&#34;hyperbolic-tangent-function&#34;&gt;Hyperbolic Tangent Function&lt;/h3&gt;
&lt;p&gt;\[
\tanh x = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
\]&lt;/p&gt;
&lt;p&gt;The hyperbolic tangent function maps input to a range of \((-1, 1)\).&lt;/p&gt;
&lt;p&gt;The derivative is calculated as&lt;/p&gt;
&lt;p&gt;\[
\frac{d}{dx} \tanh x = 1 - \tanh^2 x.
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-13_23-00-27_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Hyperbolic Tangent Function. Source: Wolfram&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Hyperbolic Tangent Function. Source: Wolfram
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Key Terms&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;bias&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;activation function&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Neurons fire after input reaches some threshold.&lt;/li&gt;
&lt;li&gt;Differential activation functions necessary for backpropagation.&lt;/li&gt;
&lt;li&gt;Multi-class learning&lt;/li&gt;
&lt;li&gt;How long to train?&lt;/li&gt;
&lt;li&gt;Weight decay&lt;/li&gt;
&lt;li&gt;How many layers versus how many nodes per layer?&lt;/li&gt;
&lt;li&gt;Training&lt;/li&gt;
&lt;li&gt;Data split (train/test/val)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;multi-class-classification&#34;&gt;Multi-Class Classification&lt;/h2&gt;
&lt;p&gt;Consider an output layer of a network with \(k\) nodes.
Each of these nodes represents a decision node for a one-versus-all classifier.
For a classification task, we have to think about whether or not the sum of squares loss function works.&lt;/p&gt;
&lt;p&gt;As far as activation functions go, the logistic sigmoid function is a good way to produce some interpretation of probability.
If we treat every output node as its own one versus all classifier, then a logistic sigmoid at the end of each one would
indicate the &amp;ldquo;probability&amp;rdquo; that node \(k\) assigns class \(k\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do we formulate this in a neural network?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The number of nodes in the output layer will be \(K\), the number of classes.
Since the output of each node produces a value in range \((0, 1)\), we want to construct a target value that works with this.
Instead of assigning an integer to each class label (e.g. 1 for class 2, 2 for class 3, etc.), we will encode the target label as a \(K\) dimensional vector.
For example, if our class label is for the class 1, then the corresponding target vector will be&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{t} =
\begin{bmatrix}
1\\
0\\
\vdots\\
0
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Since the output of our final layer is also a \(K\) dimensional vector, we can compare the two using some loss function.&lt;/p&gt;
&lt;h2 id=&#34;backpropagation&#34;&gt;Backpropagation&lt;/h2&gt;
&lt;p&gt;Given a series of linear layers with nonlinear activation functions,
how can we update the weights across the entire network?&lt;/p&gt;
&lt;p&gt;The short answer is through the chain rule of differentiation.
Let&amp;rsquo;s explore this through an example.&lt;/p&gt;
&lt;p&gt;After constructing some series of hidden layers with an arbitrary number of nodes,
we will pick an error function that provides a metric of how our network performs
on a given regression or classification task.
This loss is given by \(\mathcal{L}\).&lt;/p&gt;
&lt;p&gt;Neural networks are traditionally trained using &lt;strong&gt;gradient descent&lt;/strong&gt;.
The goal is to optimize the weights such that they result in the lowest loss, or error.
This is also why our choice of loss function is important.&lt;/p&gt;
&lt;p&gt;\[
\mathbf{W}^* = \text{argmin}\frac{1}{n}\sum_{i=1}^n \mathcal{L}(f(\mathbf{x}^{(i)}; \mathbf{W}), \mathbf{y}^{(i)})
\]&lt;/p&gt;
&lt;p&gt;We first compute the gradients of the network with respect to the weights and biases.
Then, we use those gradients to update our previous values for the weights and biases.&lt;/p&gt;
&lt;h3 id=&#34;a-simple-example&#34;&gt;A Simple Example&lt;/h3&gt;
&lt;p&gt;We will first look at computing these gradients on a smaller network for binary classification with 1 hidden layer and 1 output layer.
The loss function is defined using the binary cross-entropy function:&lt;/p&gt;
&lt;p&gt;\[
\mathcal{L}(\hat{\mathbf{y}}, \mathbf{y}) = -\mathbf{y}\log \hat{\mathbf{y}} - (1 - \mathbf{y}) \log (1 - \hat{\mathbf{y}})
\]&lt;/p&gt;
&lt;p&gt;The network&amp;rsquo;s output is computed in sequence following&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{a}^{(1)} &amp;amp;= W^{(1)}\mathbf{x} + \mathbf{b}^{(1)}\\
\mathbf{z}^{(1)} &amp;amp;= g^{(1)}(\mathbf{a}^{(1)})\\
\mathbf{a}^{(2)} &amp;amp;= W^{(2)}\mathbf{z}^{(1)} + \mathbf{b}^{(2)}\\
\mathbf{z}^{(2)} &amp;amp;= g^{(2)}(\mathbf{a}^{(2)})\\
\end{align*}&lt;/p&gt;
&lt;p&gt;The goal is to compute the gradients for all weights and biases:&lt;/p&gt;
&lt;p&gt;\[
\frac{d\mathcal{L}}{dW^{(1)}},\quad \frac{d\mathcal{L}}{d\mathbf{b}^{(1)}},\quad \frac{d\mathcal{L}}{dW^{(2)}},\quad \frac{d\mathcal{L}}{d\mathbf{b}^{(2)}}.
\]&lt;/p&gt;
&lt;p&gt;Starting with the weights of the output layer:&lt;/p&gt;
&lt;p&gt;\[
\frac{d\mathcal{L}}{dW^{(2)}} = \frac{d\mathcal{L}}{d\mathbf{z}^{(2)}} \frac{d\mathbf{z}^{(2)}}{d\mathbf{a}^{(2)}} \frac{d\mathbf{a}^{(2)}}{dW^{(2)}}.
\]&lt;/p&gt;
&lt;p&gt;The first step is to compute the partial gradient of the loss function with respect to its input \(\hat{\mathbf{y}} = \mathbf{z}^{(2)}\):&lt;/p&gt;
&lt;p&gt;\[
\frac{d\mathcal{L}}{d\mathbf{z}^{(2)}} = \frac{\mathbf{z}^{(2)} - \mathbf{y}}{\mathbf{z}^{(2)}(1 - \mathbf{z}^{(2)})}.
\]&lt;/p&gt;
&lt;p&gt;Next, compute the gradient of the last layer&amp;rsquo;s activation function with respect to its input \(\mathbf{a}^{(2)}\):&lt;/p&gt;
&lt;p&gt;\[
\frac{d\mathbf{z}^{(2)}}{d\mathbf{a}^{(2)}} = \mathbf{z}^{(2)}(1 - \mathbf{z}^{(2)}).
\]&lt;/p&gt;
&lt;p&gt;Finally, we compute \(\frac{d\mathbf{a}^{(2)}}{dW^{(2)}}\):
\[
\frac{d\mathbf{a}^{(2)}}{dW^{(2)}} = \mathbf{z}^{(1)}.
\]&lt;/p&gt;
&lt;p&gt;Putting all of this together yields&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{d\mathcal{L}}{dW^{(2)}} &amp;amp;= \frac{\mathbf{z}^{(2)} - \mathbf{y}}{\mathbf{z}^{(2)}(1 - \mathbf{z}^{(2)})} * \mathbf{z}^{(2)}(1 - \mathbf{z}^{(2)}) * \mathbf{z}^{(1)}\\
&amp;amp;= \mathbf{z}^{(1)} (\mathbf{z}^{(2)} - \mathbf{y}).
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;non-convex-optimization&#34;&gt;Non-Convex Optimization&lt;/h2&gt;
&lt;p&gt;Optimizing networks with non-linearities produces a non-convex landscape.
Depending on our choice of optimization algorithm and initial starting point, the algorithm will most likely get &amp;ldquo;stuck&amp;rdquo; in some local minimum.
Consider the figure below produced by (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Li et al. 2017&lt;/a&gt;).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-31_09-48-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Loss surface of ResNet-56 (Li et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Loss surface of ResNet-56 (Li et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Li, Hao, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2017. “Visualizing the Loss Landscape of Neural Nets,” 11.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Perceptron</title>
      <link>https://ajdillhoff.github.io/notes/perceptron/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/perceptron/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-perceptron-learning-algorithm&#34;&gt;The Perceptron Learning Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#limitations-of-single-layer-perceptrons&#34;&gt;Limitations of Single-Layer Perceptrons&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;A popular example of a &lt;a href=&#34;https://ajdillhoff.github.io/notes/logistic_regression/&#34;&gt;Logistic Regression&lt;/a&gt; model is the &lt;strong&gt;perceptron&lt;/strong&gt;. Proposed by Frank Rosenblatt in 1962, the perceptron is defined as a generalized linear model:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f(\mathbf{w}^T\mathbf{\phi}(\mathbf{x})),
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(\phi\) is a basis function and \(f\) is a stepwise function with the form&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f(a) =
\begin{cases}
1, a \geq 0\\
-1, a &amp;lt; 0
\end{cases}
\end{equation*}&lt;/p&gt;
&lt;p&gt;To match this, the targets will take on a value of either 1 or -1.&lt;/p&gt;
&lt;h2 id=&#34;the-perceptron-learning-algorithm&#34;&gt;The Perceptron Learning Algorithm&lt;/h2&gt;
&lt;p&gt;Based on the stepwise function, the parameters \(\mathbf{w}\) should lead to outputs above 0 for one class and outputs below 0 for the other.
There is 0 error with a correct classification.&lt;/p&gt;
&lt;p&gt;The original formulation does not work well with gradient based optimization methods due to the fact that the derivative of the stepwise function is 0 almost everyone. To get around this, the perceptron criterion is used:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
E(\mathbf{w}) = -\sum_i \mathbf{w}^T\phi(\mathbf{x}_i)\hat{y}_i,
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(\hat{y}_i\) is the target class (either 1 or -1).&lt;/p&gt;
&lt;p&gt;An incorrect classification will minimize \(\mathbf{w}^T\phi_i y_i\). We can consider this loss only for misclassified patterns.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For each input, evaluate \(f(\mathbf{w}^T\phi(\mathbf{x}_i))\).&lt;/li&gt;
&lt;li&gt;For incorrect classifications
&lt;ul&gt;
&lt;li&gt;Add \(\phi(\mathbf{x}_i)\) to \(\mathbf{w}\) estimate for class 1&lt;/li&gt;
&lt;li&gt;Subtract \(\phi(\mathbf{x}_i)\) from \(\mathbf{w}\) for class 2.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Does not necessarily get better each step, but guaranteed to converge.&lt;/p&gt;
&lt;h2 id=&#34;limitations-of-single-layer-perceptrons&#34;&gt;Limitations of Single-Layer Perceptrons&lt;/h2&gt;
&lt;p&gt;Single layer perceptrons are limited to solving linearly separable patterns. As we have seen with a few datasets now, expecting our data to be linearly separable is wishful thinking. Minsky and Papert exposed this limitation in their book &lt;a href=&#34;https://en.wikipedia.org/wiki/Perceptrons_%28book%29&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Perceptrons: an introduction to computational geometry&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Consider the example XOR problem. It is a binary classification problem consisting of 4 data points. It is &lt;strong&gt;not&lt;/strong&gt; linearly separable as seen in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-06-27_21-22-04_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;XOR cannot be solved with a linear classifier.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;XOR cannot be solved with a linear classifier.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This is the result of using only a single Perceptron. What if we added another perceptron? A single perceptron computes \(\mathbf{w}^T + b\). It is important to transform the first perceptron&amp;rsquo;s output using a non-linear activation function, otherwise the output would be similar to that of a logistic regression model. The updated &amp;ldquo;network&amp;rdquo; is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-06-27_21-54-23_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A 2 layer perceptron for which each layer has a single node.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A 2 layer perceptron for which each layer has a single node.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The result is the same! The original input in 2D is transformed to a single dimensional output. This is then used as input to the second perceptron. The result is a linear decision boundary followed by another linear decision boundary. What if we used 2 perceptrons in the first layer? The idea is that using two linear decision boundaries in a single space would allow our model to create a more complex boundary. The updated network is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-06-27_21-58-30_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;A 2 layer perceptron for which the first layer has 2 nodes.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;A 2 layer perceptron for which the first layer has 2 nodes.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This effectively solves the XOR problem! Since each node computes a linear combination of the input, we can visualize two decision boundaries with respect to the input space.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-06-27_22-04-07_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Visualization of input space.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Visualization of input space.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Similarly, we can visualize how the data points are transformed by visualizing the space of the output layer.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-06-27_22-05-05_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Output space&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Output space
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Principal Component Analysis</title>
      <link>https://ajdillhoff.github.io/notes/principal_component_analysis/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/principal_component_analysis/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maximum-variance-formulation&#34;&gt;Maximum Variance Formulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#motivating-example&#34;&gt;Motivating Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#noise-and-redundancy&#34;&gt;Noise and Redundancy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#covariance-matrix&#34;&gt;Covariance Matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;If we have some measurements of data, but do not know the underlying dynamics, PCA can resolve this by producing a change of basis such that the dynamics are reflected upon the eigenvectors.&lt;/p&gt;
&lt;h2 id=&#34;maximum-variance-formulation&#34;&gt;Maximum Variance Formulation&lt;/h2&gt;
&lt;p&gt;Although there are several derivations of PCA. I really like the approach of projecting the data onto a lower dimensional space in order to maximize the variance of the projected data.&lt;/p&gt;
&lt;p&gt;Let \(\mathbf{X}\) be a dataset of \(N\) samples, each with \(D\) features. The goal of PCA is to project this data onto an $M$-dimensional space such that \(M &amp;lt; D\).&lt;/p&gt;
&lt;p&gt;Remember that the goal here is to maximize the variance of the &lt;strong&gt;projected data&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do we project the data?&lt;/strong&gt;
Let&amp;rsquo;s say that we want to go from $D$-dimensional space to $M$-dimensional space where \(M = 1\). Let the vector \(\mathbf{u}\) define this 1D space. If \(\mathbf{u}\) is a unit vector, then the scalar projection of a data point \(\mathbf{x}\) onto \(\mathbf{u}\) is simply \(\mathbf{u} \cdot \mathbf{x}\).&lt;/p&gt;
&lt;p&gt;Since we are maximizing variance, we need to subtract the mean sample from our data&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{\bar{x}} = \frac{1}{N}\sum_{n=1}^{N}\mathbf{x}_n
\end{equation*}&lt;/p&gt;
&lt;p&gt;Then, the mean of the projected data is \(\mathbf{u} \cdot \mathbf{\bar{x}}\).&lt;/p&gt;
&lt;p&gt;With the mean of the projected data, we can calculate the variance:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{1}{N}\sum_{n=1}^{N}\{\mathbf{u}^T\mathbf{x}_n - \mathbf{u}^T\mathbf{\bar{x}}\}^2 = \mathbf{u}^T\mathbf{S}\mathbf{u}
\end{equation*}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{S} = \frac{1}{N}\sum_{n=1}^{N}(\mathbf{x}_n - \mathbf{\bar{x}})(\mathbf{x}_n - \mathbf{\bar{x}})^T
\end{equation*}&lt;/p&gt;
&lt;p&gt;Thus, if we are maximizing the variance of the projected data, then we are maximizing \(\mathbf{u}^T\mathbf{S}\mathbf{u}\)!&lt;/p&gt;
&lt;p&gt;So this is an optimization problem, but there is one minor issue to deal with: if \(\mathbf{u}\) is not constrained, then we scale it to infinity while maximizing the function.&lt;/p&gt;
&lt;p&gt;Before, we stated that \(\mathbf{u}\) is a unit vector. Thus, the constraint is that \(\mathbf{u} \cdot \mathbf{u} = 1\).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;After reviewing Lagrangian multipliers&amp;hellip;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To enforce this constraint, we can use a lagrangian multiplier:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathcal{L}(\mathbf{u}, \lambda) = \mathbf{u}^T\mathbf{S}\mathbf{u} + \lambda(1 - \mathbf{u}^T\mathbf{u}).
\end{equation*}&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see what happens when we compute the stationary points (critical points) of the given Lagrangian function.&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\nabla_{\mathbf{u}}\mathcal{L}(\mathbf{u}, \lambda) = \mathbf{S}\mathbf{u} - \lambda \mathbf{u} = 0
\end{equation*}&lt;/p&gt;
&lt;p&gt;This implies that&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{S}\mathbf{u} = \lambda \mathbf{u}
\end{equation*}&lt;/p&gt;
&lt;p&gt;That particular equation means that \(\mathbf{u}\) is an eigenvector of \(\mathbf{S}\) with \(\lambda\) being the corresponding eigenvalue. Since \(\mathbf{u}\) is a unit vector, we can conveniently left-multiply both sides of that equation by \(\mathbf{u}^T\), resulting in:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{u}^T\mathbf{S}\mathbf{u} = \lambda
\end{equation*}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What does this mean?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;That means that the variance is maximized when \(\mathbf{u}\) is the eigenvector corresponding to the largest eigenvalue \(\lambda\).&lt;/p&gt;
&lt;p&gt;We can repeat this process to find the direction (eigenvector) corresponding to the second largest variance by considering eigenvectors that are orthogonal to the first one. This is where an orthonormal eigenbasis comes in handy.&lt;/p&gt;
&lt;h2 id=&#34;motivating-example&#34;&gt;Motivating Example&lt;/h2&gt;
&lt;p&gt;Consider a frictionless, massless spring that produces dynamics in a single direction.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2021-11-24_12-23-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Toy model of spring with ball observed from 3 perspectives.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Toy model of spring with ball observed from 3 perspectives.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;We can clearly understand that the spring will only move in a single direction. That movement reflects the underlying dynamics of this data. To understand how PCA can be useful in this situation, let&amp;rsquo;s pretend that we do not know the underlying dynamics. Instead, we observe that the data seems to go back and forth along a single axis. We observe the data over time from 3 different perspectives given by the cameras in the above figure.&lt;/p&gt;
&lt;p&gt;From the perspective of the observer, we are recording some observations in an effort to understand which dimensions are the most salient at representing the underlying mechanics. From the above figure, we know that the most important dimension in this system is that of the labeled x-axis.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How would we figure this out if we did not already know that?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each camera has its own coordinate system (basis). If each camera gives us a 2D location of the ball relative to that camera&amp;rsquo;s basis, then each sample in time gives us a 6D vector of locations.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Equivalently, every time sample is a vector that lies in an $m$-dimensional vector space spanned by an orthonormal basis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Is it possible to find another basis that best expresses the data?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Mathemtically, is there some matrix \(P\) that changes our original data \(X\) into a new representation \(Y\)?&lt;/p&gt;
&lt;p&gt;\(PX = Y\)&lt;/p&gt;
&lt;h2 id=&#34;noise-and-redundancy&#34;&gt;Noise and Redundancy&lt;/h2&gt;
&lt;p&gt;When observing real data, we will have to account for noisy measurements. Noise can come from a wide variety of sources. Being able to reduce it or filter it out is vital to understanding the underlying system.&lt;/p&gt;
&lt;p&gt;Noise is an arbitrary measurement and means nothing without some measurement of a signal. Thus, we typically measure the amount of noise in our system using a Signal-to-Noise Ratio (SNR). This assumes we have some idea of what our signal is. This is usually given based on the nature of whatever problem we are investigating. &lt;strong&gt;In the toy example, we know that the spring largely moves in a single dimension. That is the signal we expect to observe.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For arguments sake, imagine we that the recordings over time from a single camera plot the following data:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2021-11-24_14-56-35_screenshot.png&#34; &gt;


&lt;/figure&gt;

&lt;p&gt;From our advantageous position of knowing the true nature of the problem, we understand there really should be no noise. However, let&amp;rsquo;s say that our camera has some noise in interpreting the precise location of the ball at any given time. In this case, our SNR is quite high, which is good! Ideally, it would be a straight line.&lt;/p&gt;
&lt;p&gt;There is a second factor to consider: the fact that we are taking measurements from multiple sensors means that there may be some redundancy among the data collected from them. If we were to discover features that have high redundancy, we could be confident in concluding that they are highly correlated.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2021-11-24_15-01-05_screenshot.png&#34; &gt;


&lt;/figure&gt;

&lt;h2 id=&#34;covariance-matrix&#34;&gt;Covariance Matrix&lt;/h2&gt;
&lt;p&gt;Let \(X\) be a an \(m \times n\) matrix of \(n\) observations with \(m\) features per observation.&lt;/p&gt;
&lt;p&gt;We can produce a covariance matrix of the features via \(S_{X} = \frac{1}{n-1}XX^{T}\).&lt;/p&gt;
&lt;p&gt;This gives us a measurement of the correlations between all pairs of measurements.&lt;/p&gt;
&lt;p&gt;If we want to reduce redunancy between separate measurements (those in the off-diagonal of the matrix), we would then want to diagonalize this matrix. In terms of the equation \(PX=Y\), this has the effective of finding a new covariance matrix \(S_{Y}\) that is diagonal. This means that each value in the off-diagonal of \(S_{Y}\) is 0.&lt;/p&gt;
&lt;p&gt;PCA has a convenient assumption: the change of basis matrix \(P\) is orthonormal.
&lt;strong&gt;Why is this convenient?&lt;/strong&gt;
PCA can then select the normalized direction in the feature space for which the variance in the data is maximized. This is called the first &lt;em&gt;principal component&lt;/em&gt;. Because we assume \(P\) is orthonormal, the subsequent principal components must be orthogonal to the previously discovered components.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;One more thing&lt;/strong&gt;
If \(P\) is not orthonormal, then we can simply scale our eigenvectors to maximize variance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Probability Theory</title>
      <link>https://ajdillhoff.github.io/notes/probability_theory/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/probability_theory/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-simple-example&#34;&gt;A Simple Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#probability-distributions&#34;&gt;Probability Distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conditional-probability&#34;&gt;Conditional Probability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rules-of-probability&#34;&gt;Rules of Probability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-variables&#34;&gt;Random Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#continuous-variables&#34;&gt;Continuous Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#moments-of-a-distribution&#34;&gt;Moments of a Distribution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Slides for these notes are available &lt;a href=&#34;https://ajdillhoff.github.io/teaching/cse6363/lectures/probability.pdf&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Probability theory provides a consistent framework for the quantification and manipulation of uncertainty.
It allows us to make the best decisions given the limited information we may have.
Many tasks, models, and evaluation metrics that we will explore in this course are either based on, or are inspired by, probability theory.&lt;/p&gt;
&lt;h2 id=&#34;a-simple-example&#34;&gt;A Simple Example&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Scenario:&lt;/strong&gt; There are two cookie jars, a blue one for cookies with oatmeal raisin cookies and a red one for chocolate chip cookies. The jar with oatmeal raisin cookies has 8 cookies in it. The chocolate chip jar has 10 cookies. Some monster took 2 of the chocolate chip cookies and placed them in the oatmeal raisin jar and placed 1 of the oatmeal raisin cookies in the chocolate chip jar. Thus, the oatmeal raisin jar has 2 chocolate chip and 7 oatmeal raisin. The chocolate chip jar has 8 chocolate chip and 1 oatmeal raisin.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say that we pick the chocolate chip jar 80% of the time and the oatmeal raisin jar 20% of the time. For a given jar, the cookies inside are all equally likely to be picked. We can assign this probability to &lt;strong&gt;random variables&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(J\) - The type of jar, either blue \(b\) or red \(r\).&lt;/li&gt;
&lt;li&gt;\(C\) - The type of cookie, either oatmeal \(o\) or chocolate chip \(c\).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can define the probability of picking a particular jar:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(p(J=b) = 0.2\)&lt;/li&gt;
&lt;li&gt;\(p(J = r) = 0.8\)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notice that their sum is 1.0. These probabilities can be estimated empirically given an observer recording the events. We may also define the probabilities of picking a particular type of cookie.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(p(C = o)\)&lt;/li&gt;
&lt;li&gt;\(p(C = c)\)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For each jar, the probabilities of picking the cookies must sum to 1. The tables below show the individual probabilities of picking each type of cookie from each jar. Since we can observe the actual quantities, we can define the probabilities empirically.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Chocolate Chip&lt;/th&gt;
&lt;th&gt;Oatmeal Raisin&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Blue Jar&lt;/td&gt;
&lt;td&gt;2 / 9 = 0.222&lt;/td&gt;
&lt;td&gt;7 / 9 = 0.778&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Chocolate Chip&lt;/th&gt;
&lt;th&gt;Oatmeal Raisin&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Red Jar&lt;/td&gt;
&lt;td&gt;8 / 9 = 0.889&lt;/td&gt;
&lt;td&gt;1 / 9 = 0.111&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Given these quantities, we can ask slightly more complicated questions such as &amp;ldquo;what is the probability that I will select the red jar AND take a chocolate chip cookie?&amp;rdquo; This is expressed as a &lt;strong&gt;joint probability distribution&lt;/strong&gt;, written as \(p(J = r, C = c)\). It is defined based on two events:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the prior probability of picking the red jar,&lt;/li&gt;
&lt;li&gt;the conditional probability of picking a chocolate chip cookie conditioned on the event that the red jar was picked.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\begin{equation*}
p(J=r, C=c) = p(C=c | J=r) p(J = r)
\end{equation*}&lt;/p&gt;
&lt;p&gt;This is also referred to as the &lt;strong&gt;product rule&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We already know \(p(J=r) = 0.8\). From the table above, we can see that \(p(C=c|J=r) = 0.889\). Thus, \(p(C=c,J=r) = 0.8 * 0.889 = 0.711\).&lt;/p&gt;
&lt;p&gt;If we knew nothing about the contents of the jar or the prior probabilities of selecting a jar, we could measure the joint probability empirically. This would simply be the number of times we select the red jar AND a chocolate chip cookie divided by total number of trials. For best results, perform an infinite number of trials.&lt;/p&gt;
&lt;p&gt;If instead we wanted to measure the &lt;strong&gt;conditional probability&lt;/strong&gt; \(p(C=c|J=r)\), we would simply take the number of times a chocolate chip cookie is taken from the red jar and divide by the total number of times the red jar was selected.&lt;/p&gt;
&lt;p&gt;We can construct a joint probability table given the joint probabilities of all the events listed.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Chocolate Chip&lt;/th&gt;
&lt;th&gt;Oatmeal Raisin&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Red Jar&lt;/td&gt;
&lt;td&gt;0.711&lt;/td&gt;
&lt;td&gt;0.089&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Blue Jar&lt;/td&gt;
&lt;td&gt;0.044&lt;/td&gt;
&lt;td&gt;0.156&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If you summed each row and further took the sum of the sum of rows, you would get 1. Likewise, the sum of the sum of columns would equal 1.&lt;/p&gt;
&lt;p&gt;Summing the columns for each row yields the prior probability of selecting each type of jar. Similarly, summing the rows for each column gives the prior probability of selecting that type of cookie. This is referred to as the &lt;strong&gt;marginal probability&lt;/strong&gt; or &lt;strong&gt;sum rule&lt;/strong&gt;, which is computed by summing out the other variables in the joint distribution. For example,&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(x_i) = \sum_j p(x_i, y_j)
\end{equation*}&lt;/p&gt;
&lt;p&gt;Empirically, this is computed as the number of times event \(x_i\) occurs out of ALL trials.&lt;/p&gt;
&lt;p&gt;Although the joint probabilities \(p(X, Y)\) and \(p(Y, X)\) would be written slightly differently, they are equal. With this in mind, we can set them equal to each other to derive &lt;strong&gt;Bayes&amp;rsquo; rule&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(X, Y) &amp;amp;= p(Y, X)\\
p(X|Y)p(Y) &amp;amp;= p(Y|X)p(X)\\
p(X|Y) &amp;amp;= \frac{p(Y|X)p(X)}{p(Y)}
\end{align*}&lt;/p&gt;
&lt;p&gt;In this context, \(p(X|Y)\) is referred to as the &lt;strong&gt;posterior probability&lt;/strong&gt; of event \(X\) conditioned on the fact that we know event \(Y\) has occurred. On the right, \(p(X)\) is the &lt;strong&gt;prior probability&lt;/strong&gt; of event \(X\) in the absence of any additional evidence.&lt;/p&gt;
&lt;p&gt;Two variables are &lt;strong&gt;independent&lt;/strong&gt;, then&lt;/p&gt;
&lt;p&gt;\[
p(X, Y) = p(X)p(Y)
\]&lt;/p&gt;
&lt;p&gt;If two variables are conditionally independent given a third event, then&lt;/p&gt;
&lt;p&gt;\[
p(X, Y|Z) = P(X|Z)P(Y|Z)
\]&lt;/p&gt;
&lt;h2 id=&#34;probability-distributions&#34;&gt;Probability Distributions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Events&lt;/strong&gt; come from a &lt;strong&gt;space&lt;/strong&gt; of possible outcomes.&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\Omega = {1, 2, 3, 4, 5, 6}
\end{equation*}&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;measureable event&lt;/strong&gt; is one for which we can assign a probability.&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;event space&lt;/strong&gt; must satisfy the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It contains the empty event \(\emptyset\) and trivial event \(\Omega\)&lt;/li&gt;
&lt;li&gt;It is closed under union&lt;/li&gt;
&lt;li&gt;It is closed under complementation: if \(\alpha \in S\), so is \(\Omega - \alpha\)&lt;/li&gt;
&lt;li&gt;Statement 2 implies difference and intersection&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A &lt;strong&gt;probability distribution&lt;/strong&gt; \(P\) over \((\Omega, S)\) maps events \(S\) to real values and satisfies:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;\(P(\alpha) \geq 0\) for all \(\alpha \in S\)&lt;/li&gt;
&lt;li&gt;\(P(\Omega) = 1\)&lt;/li&gt;
&lt;li&gt;If \(\alpha,\beta \in S\) and \(\alpha \cap \beta = \emptyset\), then \(P(\alpha \cup \beta) = P(\alpha)+P(\beta)\)&lt;/li&gt;
&lt;li&gt;\(P(\emptyset) = 0\)&lt;/li&gt;
&lt;li&gt;\(P(\alpha \cup \beta) = P(\alpha) + P(\beta) - P(\alpha \cap \beta)\)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conditional-probability&#34;&gt;Conditional Probability&lt;/h2&gt;
&lt;p&gt;Defined as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
P(\beta | \alpha) = \frac{P(\alpha \cap \beta)}{P(\alpha)}
\end{equation*}&lt;/p&gt;
&lt;p&gt;The more that \(\alpha\) and \(\beta\) relate, the higher the probability.&lt;/p&gt;
&lt;h3 id=&#34;the-chain-rule-of-probability&#34;&gt;The Chain Rule of Probability&lt;/h3&gt;
&lt;p&gt;\begin{equation*}
P(\alpha \cap \beta) = P(\alpha) P(\beta | \alpha)
\end{equation*}&lt;/p&gt;
&lt;p&gt;Generally&amp;hellip;&lt;/p&gt;
&lt;p&gt;\begin{equation*}
P(\alpha_1 \cap \dotsb \cap \alpha_k) = P(\alpha_1)P(\alpha_2 | \alpha_1) \dotsm P(\alpha_k | \alpha_1 \cap \dotsb \cap \alpha_{k-1})
\end{equation*}&lt;/p&gt;
&lt;h3 id=&#34;bayes-rule&#34;&gt;Bayes&amp;rsquo; Rule&lt;/h3&gt;
&lt;p&gt;\begin{equation*}
P(\alpha | \beta) = \frac{P(\beta | \alpha)P(\alpha)}{P(\beta)}
\end{equation*}&lt;/p&gt;
&lt;p&gt;Computes the inverse conditional probability.&lt;/p&gt;
&lt;p&gt;A general conditional version of Baye&amp;rsquo;s rule:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
P(\alpha | \beta \cap \gamma) = \frac{P(\beta | \alpha \cap \gamma)P(\alpha | \gamma)}{P(\beta | \gamma)}
\end{equation*}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example: TB Tests&lt;/strong&gt;
A common example for introduction Bayes&amp;rsquo; rule is that of the test that gives 95% accuracy.
The naive assumption here is that if you receive a positive result with no prior information, then
there is a 95% chance you have the infection. This is wrong because that value is conditioned on
&lt;strong&gt;already being infected&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;rules-of-probability&#34;&gt;Rules of Probability&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Sum Rule:&lt;/strong&gt;&lt;/strong&gt; \(p(X) = \sum_{Y}p(X, Y)\)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Product Rule:&lt;/strong&gt;&lt;/strong&gt; \(p(X, Y) = p(Y|X)p(X)\)&lt;/p&gt;
&lt;h2 id=&#34;random-variables&#34;&gt;Random Variables&lt;/h2&gt;
&lt;p&gt;Allows for compact notation when talking about an event. It can also be represented as a function:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f_{\text{Grade}}
\end{equation*}&lt;/p&gt;
&lt;p&gt;maps each person in \(\Omega\) to a grade value.&lt;/p&gt;
&lt;p&gt;Random variables are commonly either &lt;strong&gt;categorical&lt;/strong&gt; or &lt;strong&gt;real numbers&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;multinoulli distribution&lt;/strong&gt; is one over \(k &amp;gt; 2\) categorical random variables.
If \(k = 2\), the distribution is called the &lt;strong&gt;Bernoulli&lt;/strong&gt; or &lt;strong&gt;binomial&lt;/strong&gt; distribution.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;marginal distribution&lt;/strong&gt; is one over a single random variable \(X\).&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;joint distribution&lt;/strong&gt; is one over a set of random variables.&lt;/p&gt;
&lt;p&gt;The marginal can be computed from a joint distribution.&lt;/p&gt;
&lt;p&gt;\begin{equation*}
P(x) = \sum_{y}P(x, y)
\end{equation*}&lt;/p&gt;
&lt;h2 id=&#34;continuous-variables&#34;&gt;Continuous Variables&lt;/h2&gt;
&lt;p&gt;The introductory example looked at events that take on discrete values. That is, we either selected a cookie or did not. Most of the problems we will deal with in this course involve continuous values. In this case, we are concerned with intervals that the values may take on. If we consider a small differential of our random variable \(x\) as \(\delta x\), we can compute the probability density \(p(x)\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-23_13-11-32_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;PDF (p(x)) and CDF (P(x)). Source: Bishop&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;PDF (p(x)) and CDF (P(x)). Source: Bishop
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;With this differential \(\delta x\), we can compute the probability that \(x\) lies on some interval \((a, b)\):&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(a \leq x \leq b) = \int_{a}^{b} p(x) dx
\end{equation*}&lt;/p&gt;
&lt;p&gt;As with discrete probability distributions, the probability density must sum to 1 and cannot take a negative value. That is&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(x) &amp;amp;\geq 0\\
\int_{-\infty}^{\infty}p(x)dx &amp;amp;= 1
\end{align*}&lt;/p&gt;
&lt;p&gt;In the plot above, \(p(x)\) is the probability density function (pdf) and \(P(x)\) is the cumulative distribution function (cdf). It is possible for a pdf to have a value greater than 1, as long as integrals over any interval are less than or equal to 1.&lt;/p&gt;
&lt;p&gt;The cumulative distribution function \(P(x)\) is the probability that \(x\) lies in the interval \((-\infty, z)\), given by&lt;/p&gt;
&lt;p&gt;\begin{equation*}
P(z) = \int_{\infty}^{z} p(x)dx.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Note that the derivative of the cdf is equal to the pdf.&lt;/p&gt;
&lt;p&gt;The product rule for continuous probability distributions takes on the same form as that of discrete distributions. The sum rule is written in terms of integration:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(x) = \int p(x, y)dy.
\end{equation*}&lt;/p&gt;
&lt;h2 id=&#34;moments-of-a-distribution&#34;&gt;Moments of a Distribution&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;moment&lt;/strong&gt; of a function describes a quantitative measurement related to its graph. With respect to probability densities, the $k$th moment of \(p(x)\) is defined as \(\mathbb{E}[x^k]\). The first moment is the &lt;strong&gt;mean&lt;/strong&gt; of the distribution, the second moment is the &lt;strong&gt;variance&lt;/strong&gt;, and the third moment is the &lt;strong&gt;skewness&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Three extremely important statistics for any probability distribution are the average, variance, and covariance.&lt;/p&gt;
&lt;h3 id=&#34;expectation&#34;&gt;Expectation&lt;/h3&gt;
&lt;p&gt;The average of a function \(f(x)\) under a probability distribution \(p(x)\) is referred to as the &lt;strong&gt;expectation&lt;/strong&gt; of \(f(x)\), written as \(\mathbb{E}[f]\). The expectation for discrete and continuous distributions are&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbb{E}[f] &amp;amp;= \sum_x p(x)f(x) \text{ and}\\
\mathbb{E}[f] &amp;amp;= \int p(x)f(x)dx,
\end{align*}&lt;/p&gt;
&lt;p&gt;respectively.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_17-56-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Expectation of rolling a d6 over ~1800 trials converges to 3.5. Source: [Seeing Theory](https://seeing-theory.brown.edu/)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Expectation of rolling a d6 over ~1800 trials converges to 3.5. Source: &lt;a href=&#34;https://seeing-theory.brown.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Seeing Theory&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The &lt;strong&gt;mean value&lt;/strong&gt; for a discrete and continuous probability distribution is define as&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbb{E}[f] &amp;amp;= \sum_x p(x)x \text{ and}\\
\mathbb{E}[f] &amp;amp;= \int_{-\infty}^{\infty} p(x)xdx,
\end{align*}&lt;/p&gt;
&lt;p&gt;respectively.&lt;/p&gt;
&lt;p&gt;Empirically, we can approximate this quantity given \(N\) samples as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbb{E}[f] \approx \frac{1}{N}\sum_{i=1}^{N}f(x_i).
\end{equation*}&lt;/p&gt;
&lt;h3 id=&#34;variance&#34;&gt;Variance&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;variance&lt;/strong&gt; of a function \(f(x)\) under a probability distribution \(p(x)\) measures how much variability is in \(f(x)\) around the expected value \(\mathbb{E}[f(x)]\) and is defined by&lt;/p&gt;
&lt;p&gt;\begin{align*}
\text{var}[f] &amp;amp;= \mathbb{E}[(f(x) - \mathbb{E}[f(x)])^2]\\
&amp;amp;= \mathbb{E}[f(x)^2] - \mathbb{E}[f(x)]^2.
\end{align*}&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_18-02-03_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Variance of drawing cars with values 1-10 100 trials converges to 8.79. True variance is 8.25. Source: [Seeing Theory](https://seeing-theory.brown.edu/)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Variance of drawing cars with values 1-10 100 trials converges to 8.79. True variance is 8.25. Source: &lt;a href=&#34;https://seeing-theory.brown.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Seeing Theory&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;covariance&#34;&gt;Covariance&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;covariance&lt;/strong&gt; of two random variables \(x\) and \(y\) provides a measure of dependence between the two variables. This implies that the covariance between two independent variables is 0.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\text{cov}[\mathbf{x},\mathbf{y}] &amp;amp;= \mathbf{E}_{\mathbf{x},\mathbf{y}}[\{\mathbf{x} - \mathbb{E}[\mathbf{x}]\}\{\mathbf{y}^T - \mathbb{E}[\mathbf{y}^T]\}]\\
&amp;amp;= \mathbb{E}_{\mathbf{x},\mathbf{y}}[\mathbf{x}\mathbf{y}^T] - \mathbb{E}[\mathbf{x}]\mathbb{E}[\mathbf{y}^T].
\end{align*}&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_18-13-52_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Plot of 2D data with negative covariance. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Plot of 2D data with negative covariance. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_18-14-22_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Plot of 2D data with approximately 0 covariance. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Plot of 2D data with approximately 0 covariance. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_18-14-45_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Plot of data with positive covariance. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Plot of data with positive covariance. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;correlation&#34;&gt;Correlation&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;correlation&lt;/strong&gt; between two random variables \(x\) and \(y\) relates to their covariance, but it is normalized to lie between -1 and 1.&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\text{corr}[\mathbf{x},\mathbf{y}] = \frac{\text{cov}[\mathbf{x},\mathbf{y}]}{\sqrt{\text{var}[\mathbf{x}]\text{var}[\mathbf{y}]}}
\end{equation*}&lt;/p&gt;
&lt;p&gt;The correlation between two variables will equal 1 if there is a linear relationship between them. We can then view the correlation as providing a measurement of linearity.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-29_23-01-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Sets of points with their correlation coefficients. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Sets of points with their correlation coefficients. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;limitations-of-moments&#34;&gt;Limitations of Moments&lt;/h3&gt;
&lt;p&gt;Summary statistics can be useful but do not tell the whole story of your data. When possible, it is always better to visualize the data. An example of this is the &lt;strong&gt;Anscombosaurus&lt;/strong&gt;, derived from the Anscombe&amp;rsquo;s quartet. The quartet consists of four datasets that have nearly identical summary statistics but are visually distinct. A modern version, called the Datasaurus Dozen, consists of 12 datasets that have the same summary statistics but are visually distinct.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-08-29_21-15-04_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Datasaurus Dozen (source: [Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing](https://www.autodeskresearch.com/publications/samestats))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Datasaurus Dozen (source: &lt;a href=&#34;https://www.autodeskresearch.com/publications/samestats&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Regularization</title>
      <link>https://ajdillhoff.github.io/notes/regularization/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/regularization/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#overfitting&#34;&gt;Overfitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#penalizing-weights&#34;&gt;Penalizing Weights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dataset-augmentation&#34;&gt;Dataset Augmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#early-stopping&#34;&gt;Early Stopping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dropout&#34;&gt;Dropout&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Slides for these notes are available &lt;a href=&#34;https://ajdillhoff.github.io/teaching/cse6363/lectures/regularization.pdf&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error. - Goodfellow et al.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;Regularization comes in many forms.
Some techniques may add an additional penalty to the loss function.
Others, such as data augmentation, add artificial variation to the data.
In all cases, regularization aims to improve the generalization performance by preventing the model from overfitting.&lt;/p&gt;
&lt;h2 id=&#34;overfitting&#34;&gt;Overfitting&lt;/h2&gt;
&lt;p&gt;What happens when the complexity of our chosen model fits the data &lt;em&gt;too&lt;/em&gt; well? Take a look at the following plot of data. The red curve is the true underlying function that generated the data. The blue line represents a polynomial of degree 9 fit via linear regression. It is first necessary to understand what is happening.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-01_10-18-30_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;A polynomial of degree 11 (blue) fit to data generated following the red line.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;A polynomial of degree 11 (blue) fit to data generated following the red line.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The model with more parameters is able to fit some the noisy data slightly better.
&lt;strong&gt;Does this necessarily mean it will perform better on new samples?&lt;/strong&gt;
No, it will usually perform worse. This is referred to as &lt;strong&gt;overfitting.&lt;/strong&gt;
Overfitting can be identified as the model trains. When the testing loss continues to decrease while the validation loss increases, the model is probably overfitting. It is also evident from looking at the weights.&lt;/p&gt;
&lt;h3 id=&#34;identifying-the-cause&#34;&gt;Identifying the Cause&lt;/h3&gt;
&lt;p&gt;The goal of training is to modify the weights such that they minimize the loss function. Models with more parameters have the capacity to fit more of their training data. Given the presence of noise, this is not a good thing. A very low loss on the training set may not translate to good performance on the validation set.&lt;/p&gt;
&lt;p&gt;Looking at weights of the trained model is a good way of detecting overfitting. From the model above, the mean of the absolute value of the weights is \(11.1\). Left unchecked, the weights will take on whatever values necessary to meet the objective function.&lt;/p&gt;
&lt;h2 id=&#34;penalizing-weights&#34;&gt;Penalizing Weights&lt;/h2&gt;
&lt;p&gt;The most common form of regularization is to penalize the weights from taking on a high value. That is, we define a penalty term \(E(\mathbf{w})\) that is added to the loss. The higher the weight values, the higher the total loss. Thus, optimization will also include minimizing the absolute values of the weights. A simple choice for \(E(\mathbf{w})\), especially in the context of least squares, is \(L2\) regularzation:&lt;/p&gt;
&lt;p&gt;\[
E(\mathbf{w}) = \frac{\lambda}{2}||\mathbf{w}||^2 = \frac{\lambda}{2}\mathbf{w}^T \mathbf{w}.
\]&lt;/p&gt;
&lt;p&gt;Added to the sum-of-squares error for least squares, the final loss becomes&lt;/p&gt;
&lt;p&gt;\[
J(\mathbf{w}) = \frac{1}{2}\sum_{i=1}^n(h(\mathbf{x}_i;\mathbf{w}) - \mathbf{y}_i)^2 + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}.
\]&lt;/p&gt;
&lt;p&gt;This choice of regularization also has the benefit of being in a form that can be minimized in closed form via the normal equations. Taking the gradient of \(J(\mathbf{w})\) above with respect to 0 and solving for \(\mathbf{w}\) yields&lt;/p&gt;
&lt;p&gt;\[
\mathbf{w} = (\lambda I + \mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y},
\]&lt;/p&gt;
&lt;p&gt;where \(\lambda\) is a regularization hyperparameter.&lt;/p&gt;
&lt;p&gt;Applying this regularization term to the model above with \(\lambda=1\) yields the model shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-01_10-45-57_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Least squares model fit with (L2) regularization ((lambda = 1)).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Least squares model fit with (L2) regularization ((lambda = 1)).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Inspecting the weights as before, we can see that the mean of the absolute values of \(\mathbf{w}\) is \(0.0938\).&lt;/p&gt;
&lt;h3 id=&#34;evaluating-on-the-testing-data&#34;&gt;Evaluating on the Testing Data&lt;/h3&gt;
&lt;p&gt;To see which model generalizes better, we set aside some samples from the original dataset to use as testing.&lt;/p&gt;
&lt;p&gt;With regularization, the model error on the test set is \(1.8\). Without regularization, the model error on the test set is \(2.2\).&lt;/p&gt;
&lt;h2 id=&#34;dataset-augmentation&#34;&gt;Dataset Augmentation&lt;/h2&gt;
&lt;p&gt;The same data augmentation techniques should be applied on both methods being compared.
Getting a better result on a benchmark because of data augmentation does not mean the method was better suited for the task.
By controlling these factors, a fair comparison can be made.&lt;/p&gt;
&lt;p&gt;There are many forms of augmentation available for image tasks in particular.
Rotating, translating, and scaling images are the most common.
Additionally applying random crops can further augment the dataset.&lt;/p&gt;
&lt;p&gt;The original dataset may only include samples of a class that have similar lighting.
Color jitter is an effective way of including a broader range of hue or brightness and usually leads to a model that is robust to such changes.&lt;/p&gt;
&lt;p&gt;It is important to make sure that the crops still contain enough information to properly classify it.
Common forms of data augmentation are available through APIs like &lt;a href=&#34;https://pytorch.org/vision/stable/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;torchvision&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;early-stopping&#34;&gt;Early Stopping&lt;/h2&gt;
&lt;p&gt;If the validation loss begins to increase while the training loss continues to decrease, this is a clear indication that the model is beginning to overfit the training data.
Stopping the model in this case is the best way to prevent this.
Frameworks like &lt;a href=&#34;https://www.pytorchlightning.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;PyTorch Lightning&lt;/a&gt; include features to checkpoing the models based on best validation loss and stop the model whenever the validation loss begins to diverge.&lt;/p&gt;
&lt;h2 id=&#34;dropout&#34;&gt;Dropout&lt;/h2&gt;
&lt;p&gt;Dropout is a regularization method introduced by &amp;lt;&amp;amp;srivastavaDropoutSimpleWay2014&amp;gt; which is motivated by ensemble methods.
Ensembles of models are regularized by the fact that many different models are trained on random permutations of the dataset with varying parameters and initializations.
Using an ensemble of networks is a powerful way of increasing generalization performance.
However, it requires much more compute due to the fact that several models must be trained.&lt;/p&gt;
&lt;p&gt;Training a single network with dropout approximates training several models in an ensemble.
It works by randomly removing a node from the network during a forward/backward pass.
The node is not truly removed. Instead, its output during the forward and backward passes is ignored via a binary mask.&lt;/p&gt;
&lt;p&gt;When training a network with dropout, it will generally take longer for the model to converge to a solution.
Intuitively, this is because a different subnetwork is being used for each pass.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scale Invariant Feature Transforms</title>
      <link>https://ajdillhoff.github.io/notes/scale_invariant_feature_transforms/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/scale_invariant_feature_transforms/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#difference-of-gaussians&#34;&gt;Difference of Gaussians&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#keypoint-localization&#34;&gt;Keypoint Localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#orientation-assignment&#34;&gt;Orientation Assignment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#descriptor-formation&#34;&gt;Descriptor Formation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;General approach to computing SIFT features:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Scale-space Extrema Detection&lt;/li&gt;
&lt;li&gt;Keypoint localization&lt;/li&gt;
&lt;li&gt;Orientation Assignment&lt;/li&gt;
&lt;li&gt;Generate keypoint descriptors&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;difference-of-gaussians&#34;&gt;Difference of Gaussians&lt;/h2&gt;
&lt;p&gt;This same technique for detecting interesting points in a scale-invariant way can be approximated by taking the &lt;strong&gt;Difference of Gaussians&lt;/strong&gt;. Consider the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-10_10-31-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Comparison of DoG and Laplacian. Credit: Fei-Fei Li.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Comparison of DoG and Laplacian. Credit: Fei-Fei Li.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;By taking the difference of images smoothed by a Gaussian with different values of \(\sigma\), the resulting pixel values correspond to areas with high gradient norms in the less blurry version.&lt;/p&gt;
&lt;p&gt;Let \(I_{\sigma_1}\) be the image blurred with a smaller value of \(\sigma\) and \(I_{\sigma_2}\) be the image blurred with a larger value.
Then \(D(I_{\sigma_1}, I_{\sigma_2}) = I_{\sigma_2} - I_{\sigma_1}\).
If a region in \(I_{\sigma_1}\) is locally flat, it will also be in flat in \(I_{\sigma_2}\).
The difference will be relatively small for that region.
If there are abrupt changes in a local region within \(I_{\sigma_1}\), they will be smoothed in \(I_{\sigma_2}\).
Therefore, the difference \(D(I_{\sigma_1}, I_{\sigma_2})\) will be higher for that region.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-13_20-29-08_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;The Royal Concertgebouw in Amsterdam.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;The Royal Concertgebouw in Amsterdam.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-13_20-41-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Difference of Gaussian between the original image blurred with (sigma = 0.5) and (sigma=1.5).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Difference of Gaussian between the original image blurred with (sigma = 0.5) and (sigma=1.5).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;When building SIFT features, the extremum are selected by comparing 3 DoG images.
These are selected by evaluating each pixel to 26 of its neighbors in the current scale space and neighboring DoG spaces as visualized below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-13_18-50-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Finding extrema of pixel (i, j) in a neighborhood of 26 values (&amp;lt;a href=&amp;#34;#citeproc_bib_item_2&amp;#34;&amp;gt;Lowe 2004&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Finding extrema of pixel (i, j) in a neighborhood of 26 values (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Lowe 2004&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To build the DoG pyramid, the authors propose that images are separated by a constant factor \(k\) in scale space.
Each octave of scale space is divided such that the scalespace doubles every \(s\) samples.&lt;/p&gt;
&lt;p&gt;Starting with \(\sigma = 0.5\), if we choose \(s=3\) then the fourth sample will be at \(\sigma = 1\), the seventh at \(\sigma=2\), and so on.
To make sure the DoG images cover the full range of an octave, \(s + 3\) images need to be created per octave.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why \(s + 3\)?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each octave should evaluate local extrema for \(s\) scales.
To evaluate this for scale \(\sigma_s\), we need the DoG for scales \(\sigma_{s-1}\) and \(\sigma_{s+1}\).
This would require 4 Gaussians images to compute.
The figure below represents the stack for \(s=2\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-10_17-35-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;DOG figure (&amp;lt;a href=&amp;#34;#citeproc_bib_item_2&amp;#34;&amp;gt;Lowe 2004&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;DOG figure (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Lowe 2004&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;How is the value of \(s\) determined?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the paper, the authors perform a repeatability test to determine if the keypoints would be localized even with random augmentations. The process is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Randomly augment an input image with noise, color jitter, scale, rotation, etc.&lt;/li&gt;
&lt;li&gt;Compute keypoints using using the extrema detection.&lt;/li&gt;
&lt;li&gt;Compare detected keypoints with known keypoints from original samples.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The authors found that using \(s = 3\) provided the highest percentage of repeatability in their experiments.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-13_19-25-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Measuring repeatability of keypoint detections versus # of scales sampled per octave (&amp;lt;a href=&amp;#34;#citeproc_bib_item_2&amp;#34;&amp;gt;Lowe 2004&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Measuring repeatability of keypoint detections versus # of scales sampled per octave (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Lowe 2004&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;keypoint-localization&#34;&gt;Keypoint Localization&lt;/h2&gt;
&lt;p&gt;Given the candidate keypoints selected by picking out local extrema, they pool of responses can further be refined
by removing points that are sensitive to noise or located along an edge. They borrow the same approach used in the Harris corner detector to select more robust interest points in corners.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-15_20-36-13_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Refinement of candidate keypoints by filtering those sensitive to noise (c) and those representing ambiguity along edges (d) (&amp;lt;a href=&amp;#34;#citeproc_bib_item_2&amp;#34;&amp;gt;Lowe 2004&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Refinement of candidate keypoints by filtering those sensitive to noise (c) and those representing ambiguity along edges (d) (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Lowe 2004&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;orientation-assignment&#34;&gt;Orientation Assignment&lt;/h2&gt;
&lt;p&gt;Given a keypoint, an orientation histogram is generated. The authors use 36 bins to cover a 360 degree range for orientations.
Similar to &lt;a href=&#34;https://ajdillhoff.github.io/notes/histogram_of_oriented_gradients/&#34;&gt;Histogram of Oriented Gradients&lt;/a&gt;, the orientations are weighted by their gradient magnitudes (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dalal and Triggs 2005&lt;/a&gt;).
Additionally, a Gaussian-weighted circular patch is applied, centered on the keypoint, to further weight the responses.
This means that points farther away from the center contribute less to the overall feature vector.&lt;/p&gt;
&lt;p&gt;In order to make the keypoint rotation invariant, the dominant orientation is determined.
If there are orientations that are within 80% of the highest orientation peak, multiple keypoints will be created using those orientations as well.&lt;/p&gt;
&lt;p&gt;Orientations in this window are rotated by the dominant gradient so that all directions are with respect to the dominant orientation.
This is a more efficient alternative to rotating the entire image by that orientation.&lt;/p&gt;
&lt;h2 id=&#34;descriptor-formation&#34;&gt;Descriptor Formation&lt;/h2&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-15_20-22-39_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Keypoint descriptor generation (&amp;lt;a href=&amp;#34;#citeproc_bib_item_2&amp;#34;&amp;gt;Lowe 2004&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Keypoint descriptor generation (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Lowe 2004&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the paper, the authors generate keypoints using a \(16 \times 16\) window from which \(4 \times 4\) descriptors are generated following the descriptions above.
Through experimentation, each \(4 \times 4\) descriptor uses 8 orientations, resulting in a feature vector \(\mathbf{x} \in \mathbb{R}^{128}\).&lt;/p&gt;
&lt;p&gt;Different levels of contrast will product edges with higher gradient magnitudes.
To account for this, the final feature vector is normalized using the \(L2\) hysteresis approach used in Harris corner detection.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Dalal, N., and B. Triggs. 2005. “Histograms of Oriented Gradients for Human Detection.” In &lt;i&gt;2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)&lt;/i&gt;, 1:886–93 vol. 1. &lt;a href=&#34;https://doi.org/10.1109/CVPR.2005.177&#34;&gt;https://doi.org/10.1109/CVPR.2005.177&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Lowe, David G. 2004. “Distinctive Image Features from Scale-Invariant Keypoints.” &lt;i&gt;International Journal of Computer Vision&lt;/i&gt; 60 (2): 91–110. &lt;a href=&#34;https://doi.org/10.1023/B:VISI.0000029664.99615.94&#34;&gt;https://doi.org/10.1023/B:VISI.0000029664.99615.94&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Support Vector Machine</title>
      <link>https://ajdillhoff.github.io/notes/support_vector_machine/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/support_vector_machine/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maximum-margin-classifier&#34;&gt;Maximum Margin Classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#formulation&#34;&gt;Formulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#overlapping-class-distributions&#34;&gt;Overlapping Class Distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiclass-svm&#34;&gt;Multiclass SVM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#additional-resources&#34;&gt;Additional Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Support Vector Machines are a class of supervised learning methods primarily used for classification. Although they can be formulated for regression and outlier detection as well. Instead of optimizing a set of parameters which compress or summarize the training set, they use a small subset of the training data to compute the decision function.&lt;/p&gt;
&lt;p&gt;They rely on the data being linearly separable, so feature transformations are critical for problems in which the original representation of the data is not linearly separable.&lt;/p&gt;
&lt;h2 id=&#34;maximum-margin-classifier&#34;&gt;Maximum Margin Classifier&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with a simple classification model as we studied with &lt;a href=&#34;https://ajdillhoff.github.io/notes/logistic_regression/&#34;&gt;Logistic Regression&lt;/a&gt;. That is, we have&lt;/p&gt;
&lt;p&gt;\[
f(\mathbf{x}) = \mathbf{w}^T\phi(\mathbf{x}),
\]&lt;/p&gt;
&lt;p&gt;where \(\phi(\mathbf{x})\) is a function which transforms our original input into some new feature space. The transformed input is assumed to be linearly separable so that a decision boundary can be computed. In the original logistic regression problem, a decision boundary was found through optimization. For linearly separable data, there are an infinite number of decision boundaries that satisfy the problem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What about the quality of the decision boundary?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Is one decision boundary better than the other?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TODO:&lt;/strong&gt; Add a few plots comparing decision boundaries&lt;/p&gt;
&lt;h2 id=&#34;formulation&#34;&gt;Formulation&lt;/h2&gt;
&lt;p&gt;Given a training set \(\{\mathbf{x}_1, \dots, \mathbf{x}_n\}\) with labels \(\{y_1, \dots, y_n\}\), where \(y_i \in \{-1, 1\}\), we construct a linear model which classifies an input sample depending on the sign of the output.&lt;/p&gt;
&lt;p&gt;Our decision rule for classification, given some input \(\mathbf{x}\), is&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f(\mathbf{x}) =
\begin{cases}
1\text{ if }\mathbf{w}^T\mathbf{x} + b \geq 0\\
-1\text{ if }\mathbf{w}^T\mathbf{x} + b &amp;lt; 0
\end{cases}
\end{equation*}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How large should the margin be?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the original formulation of &lt;a href=&#34;https://ajdillhoff.github.io/notes/logistic_regression/&#34;&gt;Logistic Regression&lt;/a&gt;, we saw that the parameter vector \(\mathbf{w}\) described the &lt;strong&gt;normal&lt;/strong&gt; to the decision boundary. The distance between a given point \(\mathbf{x}\) and the decision boundary is given by&lt;/p&gt;
&lt;p&gt;\[
\frac{y_if(\mathbf{x})}{||\mathbf{w}||}.
\]&lt;/p&gt;
&lt;p&gt;We can frame this as an optimization problem: come up with a value for \(\mathbf{w}\) that maximizes the margin.&lt;/p&gt;
&lt;p&gt;\[
\text{arg max}_{\mathbf{w}, b} \frac{1}{\|\mathbf{w}\|}\min_{i} y_i (\mathbf{w}^T\phi(\mathbf{x}_i) + b)
\]&lt;/p&gt;
&lt;p&gt;We can arbitrarily scale the parameters, so we add an additional constraint that any point that lies on the boundary of the margin satisfies&lt;/p&gt;
&lt;p&gt;\[
y_i(\mathbf{w}^T\mathbf{x} + b) = 1.
\]&lt;/p&gt;
&lt;p&gt;Under this constraint, we have that all samples satisfy&lt;/p&gt;
&lt;p&gt;\[
y_i(\mathbf{w}^T\mathbf{x} + b) \geq 1.
\]&lt;/p&gt;
&lt;p&gt;That is, all positive samples with target \(1\) will produce at least a \(1\), yielding a value greater than or equal to 1. All negative samples with target \(-1\) will produce at most a \(-1\), yielding a value greater than or equal to 1.&lt;/p&gt;
&lt;p&gt;Another way of writing this is&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f(\mathbf{x}) =
\begin{cases}
1\text{ if }\mathbf{w}^T\mathbf{x}_{+} + b \geq 1\\
-1\text{ if }\mathbf{w}^T\mathbf{x}_{-} + b \leq -1,
\end{cases}
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(\mathbf{x}_+\) is a positive sample and \(\mathbf{x}_-\) is a negative sample. The decision rule can then be written as&lt;/p&gt;
&lt;p&gt;\[
y_i(\mathbf{w}^T\mathbf{x} + b) - 1 \geq 0.
\]&lt;/p&gt;
&lt;p&gt;This implies that the only samples that would yield an output of 0 are those that lie directly on the margins of the decision boundary.&lt;/p&gt;
&lt;p&gt;Given this constraint of \(y_i(\mathbf{w}^T\mathbf{x} + b) - 1 = 0\), we can derive our optimization objective.&lt;/p&gt;
&lt;p&gt;The margin can be computed via the training data. To do this, consider two data points which lie on their respective boundaries, one positive and one negative, and compute the distance between them: \(\mathbf{x}_+ - \mathbf{x}_-\). This distance with respect to our decision boundary, defined by \(\mathbf{w}\), is given by&lt;/p&gt;
&lt;p&gt;\[
(\mathbf{x}_+ - \mathbf{x}_-) \cdot \frac{\mathbf{w}}{||\mathbf{w}||}.
\]&lt;/p&gt;
&lt;p&gt;For clarity, we can rewrite this as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{1}{||\mathbf{w}||}(\mathbf{x}_{+} \cdot \mathbf{w} - \mathbf{x}_{-} \cdot \mathbf{w}).
\end{equation*}&lt;/p&gt;
&lt;p&gt;If we substitute the sample values into the equality constraint above, we can simplify this form. For the positive sample, we have \(\mathbf{w}^T\mathbf{x} = 1 - b\). For the negative sample, we get \(\mathbf{w}^T\mathbf{x} = -1 - b\). The equation above then becomes&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{1}{||\mathbf{w}||}(1 - b - (-1 - b)) = \frac{2}{||\mathbf{w}||}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Thus, our objective is to maximize \(\frac{2}{||\mathbf{w}||}\) which is equivalent to minimizing \(\frac{1}{2}||\mathbf{w}||^2\) subject to the constraints \(y_i(\mathbf{w}^T\mathbf{x}+b)\geq 1\). This is a constrainted optimization problem. As discussed previously, we can simplify such problems by introducing &lt;a href=&#34;https://ajdillhoff.github.io/notes/lagrangian_multipliers/&#34;&gt;Lagrangian Multipliers&lt;/a&gt;. Doing this produces the dual representation of our optimization objection:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
L = \frac{1}{2}||\mathbf{w}||^2 - \sum_{i=1}^n \alpha_i \big(y_i(\mathbf{w}^T\mathbf{x}_i + b) - 1\big).
\end{equation*}&lt;/p&gt;
&lt;p&gt;To solve for \(\mathbf{w}\) we compute \(\frac{\partial}{\partial \mathbf{w}}L\).&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{\partial}{\partial \mathbf{w}}L = \mathbf{w} - \sum_{i=1}^n \alpha_i y_i \mathbf{x}_i.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Setting this to 0 yields&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{w} = \sum_{i=1}^n \alpha_i y_i \mathbf{x}_i.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Doing the same for the other parameter \(b\) yields&lt;/p&gt;
&lt;p&gt;\[
0 = \sum_{i=1}^n \alpha_i y_i.
\]&lt;/p&gt;
&lt;p&gt;We can now simplify our objective function by substituting these results into it:&lt;/p&gt;
&lt;p&gt;\begin{align*}
L &amp;amp;= \frac{1}{2}\Big(\sum_{i=1}^n \alpha_i y_i \mathbf{x}_i\Big)^2 - \sum_{i=1}^n \alpha_i\Big(y_i\big((\sum_{i=1}^n\alpha_i y_i \mathbf{x}_i)^T\mathbf{x}_i + b \big) - 1 \Big)\\
&amp;amp;= \frac{1}{2}\Big(\sum_{i=1}^n \alpha_i y_i \mathbf{x}_i\Big)^2 - \Big(\sum_{i=1}^n \alpha_i y_i \mathbf{x}_i \Big)^2 - \sum_{i=1}^n \alpha_i y_i b + \sum_{i=1}^n \alpha_i\\
&amp;amp;= -\frac{1}{2} \Big(\sum_{i=1}^n \alpha_i y_i \mathbf{x}_i \Big)^2 + \sum_{i=1}^n \alpha_i\\
&amp;amp;= \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^m \alpha_i \alpha_j y_i y_j \mathbf{x}_i \cdot \mathbf{x}_j
\end{align*}&lt;/p&gt;
&lt;p&gt;Thus, the objective is dependent on the inner product of samples \(\mathbf{x}_i\) and \(\mathbf{x}_j\). If these were representations in some complex feature space, our problem would remain computationally inefficient. However, we can take advantage of &lt;a href=&#34;https://ajdillhoff.github.io/notes/kernels/&#34;&gt;Kernels&lt;/a&gt; for this.&lt;/p&gt;
&lt;p&gt;Note that, in most cases, \(\alpha_i\) will be 0 since we only consider &lt;strong&gt;support vectors&lt;/strong&gt;. That is, the points that lie on the margins of the decision boundary.&lt;/p&gt;
&lt;h2 id=&#34;overlapping-class-distributions&#34;&gt;Overlapping Class Distributions&lt;/h2&gt;
&lt;p&gt;The above formulation is fine and works with datasets that have no overlap in feature space.
That is, they are completely linearly separable.
However, it is not always the case that they will be.&lt;/p&gt;
&lt;p&gt;To account for misclassifications while still maximizing a the margin between datasets, we introduce a penalty value for points that are misclassified.
As long as there aren&amp;rsquo;t too many misclassifications, this penalty will stay relatively low while still allowing us to come up with an optimal solution.&lt;/p&gt;
&lt;p&gt;This penalty comes in the form of a &lt;strong&gt;slack variable&lt;/strong&gt; \(\xi_i \geq 0\) for each sample that is \(0\) for points that are on or inside the correct margin and \(\xi_i = |y_i - f(\mathbf{x})|\) for others.
If the point is misclassified, its slack variable will be \(\xi_i &amp;gt; 1\).&lt;/p&gt;
&lt;h2 id=&#34;multiclass-svm&#34;&gt;Multiclass SVM&lt;/h2&gt;
&lt;p&gt;Similar to our simple &lt;a href=&#34;https://ajdillhoff.github.io/notes/logistic_regression/&#34;&gt;Logistic Regression&lt;/a&gt; method, SVMs are binary classifiers by default. We can take a similar approach to extending them to multiple classes, but there are downsides to each approach.&lt;/p&gt;
&lt;p&gt;The &amp;ldquo;one-vs-all&amp;rdquo; approach entails building \(|K|\) classifiers and choose the classifier which predicts the input with the greatest margin.&lt;/p&gt;
&lt;p&gt;The &amp;ldquo;one-vs-one&amp;rdquo; approach involves building \(|K|\cdot\frac{|K| - 1}{2}\) classifiers. In this case, training each classifer will be more tractable since the amount of data required for each one is less. For example, you would have a model for class 1 vs 2, class 1 vs 3, &amp;hellip;, class 1 vs \(K\). Then repeat for class 2: 2 vs 3, 2 vs 4, &amp;hellip;, 2 vs \(|K|\), and so on.&lt;/p&gt;
&lt;p&gt;A third approach is to construct several models using a feature vector dependent on both the data and class label. When given a new input, the model computes&lt;/p&gt;
&lt;p&gt;\[
y = \text{arg}\max_{y&amp;rsquo;}\mathbf{w}^T\phi(\mathbf{x},y&amp;rsquo;).
\]&lt;/p&gt;
&lt;p&gt;The margin for this classifier is the distance between the correct class and the closest data point of any other class.&lt;/p&gt;
&lt;h2 id=&#34;additional-resources&#34;&gt;Additional Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://web.mit.edu/6.034/wwwbob/svm.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://web.mit.edu/6.034/wwwbob/svm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://ajdillhoff.github.io/notes/linear_regression/</link>
      <pubDate>Wed, 12 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/linear_regression/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#probabilistic-interpretation&#34;&gt;Probabilistic Interpretation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solving-with-normal-equations&#34;&gt;Solving with Normal Equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#another-approach-to-normal-equations&#34;&gt;Another Approach to Normal Equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fitting-polynomials&#34;&gt;Fitting Polynomials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear-basis-functions&#34;&gt;Linear Basis Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Slides for these notes are available &lt;a href=&#34;https://ajdillhoff.github.io/teaching/cse6363/lectures/linear_regression.pdf&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Given a dataset of observations \(\mathbf{X} \in \mathbb{R}^{n \times d}\), where \(n\) is the number of samples and \(d\) represents the number of features per sample, and corresponding target values \(\mathbf{Y} \in \mathbb{R}^n\), create a simple prediction model which predicts the target value \(\mathbf{y}\) given a new observation \(\mathbf{x}\). The classic example in this case is a linear model, a function that is a linear combination of the input features and some weights \(\mathbf{w}\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-15_13-35-19_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Plot of univariate data where the (x) values are features and (y) are observations.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Plot of univariate data where the (x) values are features and (y) are observations.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The generated data is plotted above along with the underlying true function that was used to generate it. If we already know what the true function is, our job is done. Suppose that we only have the data points (in blue). How do we go about modelling it? It is reasonable to first visualize the data and observe that it does follow a linear pattern. Thus, a linear model would be a decent model to choose.&lt;/p&gt;
&lt;p&gt;If the data followed a curve, we may decide to fit a polynomial. We will look at an example of that later on. For now, let&amp;rsquo;s formalize all of the information that we have.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\((\mathbf{x}, \mathbf{y})\) - Data points from the original dataset. Generally, \(\mathbf{x}\) is a vector of features and \(\mathbf{y}\) is the target vector. In our simple dataset above, these are both scalar values.&lt;/li&gt;
&lt;li&gt;\(\mathbf{w} = (w_0, w_1)\) - Our model parameters. Comparing to the equation \(y = mx + b\), \(w_0\) is our bias term and \(w_1\) is our slope parameter.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;making-predictions&#34;&gt;Making Predictions&lt;/h3&gt;
&lt;p&gt;Given \(\mathbf{w}\), we can make a prediction for a new data sample \(\mathbf{x} = x_1\).&lt;/p&gt;
&lt;p&gt;\[
h(\mathbf{x}; \mathbf{w}) = w_0 + w_1 x_1
\]&lt;/p&gt;
&lt;p&gt;Note that the bias term is always added to the result. We can simplify this into a more general form by appending a constant 1 (s.t. \(x_0 = 1\)) to each of our samples such that \(\mathbf{x} = (1, x_1, &amp;hellip;, x_d)\). Then, the general linear model becomes&lt;/p&gt;
&lt;p&gt;\[
h(\mathbf{x}; \mathbf{w}) = \sum_{i=0}^{d} w_i x_i = \mathbf{w}^T \mathbf{x}.
\]&lt;/p&gt;
&lt;p&gt;If our data happened to have more than 1 feature, it would be easy enough to model it appropriately using this notation.&lt;/p&gt;
&lt;h3 id=&#34;determining-fitness&#34;&gt;Determining Fitness&lt;/h3&gt;
&lt;p&gt;If we really wanted to, we could fit our model by plotting it and manually adjusting the weights until our model looked acceptable by some qualitative standard. Fortunately we won&amp;rsquo;t be doing that. Instead, we will use a quantitative measurement that provides a metric of how well our current parameters fit the data.&lt;/p&gt;
&lt;p&gt;For this, we use a &lt;strong&gt;&lt;strong&gt;cost function&lt;/strong&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;strong&gt;loss function&lt;/strong&gt;&lt;/strong&gt;. The most common one to use for this type of model is the least-squares function:&lt;/p&gt;
&lt;p&gt;\[
J(\mathbf{w}) = \frac{1}{2}\sum_{i=1}^{n}(h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i})^2.
\]&lt;/p&gt;
&lt;h3 id=&#34;stochastic-gradient-descent&#34;&gt;Stochastic Gradient Descent&lt;/h3&gt;
&lt;p&gt;Depending on the random initialization of parameters, our error varies greatly. We can observe that no matter what the chose parameters are, there is no possible way we can achieve an error of 0. The best we can do is minimize this error:&lt;/p&gt;
&lt;p&gt;\[
\min_{\mathbf{w}} J(\mathbf{w}).
\]&lt;/p&gt;
&lt;p&gt;For this, we rely on stochastic gradient descent. The basic idea is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Begin with an initial guess for \(\mathbf{w}\).&lt;/li&gt;
&lt;li&gt;Compare the prediction for sample \(\mathbf{x}^{(i)}\) with its target \(\mathbf{y}^{(i)}\).&lt;/li&gt;
&lt;li&gt;Update \(\mathbf{w}\) based on the comparison in part 2.&lt;/li&gt;
&lt;li&gt;Repeat steps 2 and 3 on the dataset until the loss has converged.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Steps 1, 3, and 4 are easy enough. What about step 2? How can we possibly know how to modify \(\mathbf{w}\) such that \(J(\mathbf{w})\) will decrease? By computing the gradient \(\frac{d}{d\mathbf{w}}J(\mathbf{w})\)! How will we know when we have arrived at a minima? When \(\nabla J(\mathbf{w}) = 0\).&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{d}{d\mathbf{w}}J(\mathbf{w}) &amp;amp;= \frac{d}{d\mathbf{w}}\frac{1}{2}(h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i})^2\\
&amp;amp;= 2 \cdot \frac{1}{2}(h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i}) \cdot \frac{d}{d\mathbf{w}} (h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i})\\
&amp;amp;= (h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i}) \cdot \frac{d}{d\mathbf{w}} (\mathbf{w}^T \mathbf{x}_{i} - \mathbf{y}_{i})\\
&amp;amp;= (h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i}) \mathbf{x}_{i}
\end{align*}&lt;/p&gt;
&lt;p&gt;The gradient represents the direction of greatest change for a function evaluated With this gradient, we can use an update rule to modify the previous parameter vector \(\mathbf{w}\):&lt;/p&gt;
&lt;p&gt;\[
\mathbf{w}_{t+1} = \mathbf{w}_{t} - \alpha \sum_{i=1}^{n} (h(\mathbf{x}_{i};\mathbf{w}_{t}) - \mathbf{y}_{i}) \mathbf{x}_{i}.
\]&lt;/p&gt;
&lt;p&gt;Here, \(\alpha\) is an update hyperparameter that allows us to control how big or small of a step our weights can take with each update. In general, a smaller value will be more likely to get stuck in local minima. However, too large of a value may never converge to any minima.&lt;/p&gt;
&lt;p&gt;Another convenience of this approach is that it is possible to update the weights based on a single sample, batch of samples, or the entire dataset. This sequential process makes optimization using very large dataset feasible.&lt;/p&gt;
&lt;h2 id=&#34;probabilistic-interpretation&#34;&gt;Probabilistic Interpretation&lt;/h2&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Probability theory is nothing but common sense reduced to calculation.&amp;rdquo; - Pierre-Simon Laplace&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;Recall Bayes&amp;rsquo; theorem:&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{w}|\mathbf{X}) = \frac{p(\mathbf{X}|\mathbf{w})p(\mathbf{w})}{p(\mathbf{X})}.
\]&lt;/p&gt;
&lt;p&gt;That is, the &lt;em&gt;posterior&lt;/em&gt; probability of the weights conditioned on the observered data \(\mathbf{X}\) is equal to the &lt;em&gt;likelihood&lt;/em&gt; of the observed data given the times the &lt;em&gt;prior&lt;/em&gt; distribution. This base notation doesn&amp;rsquo;t line up well with our problem. For our problem, we have observations \(\mathbf{Y}\) which are dependent on the input features \(\mathbf{X}\):&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{w}|\mathbf{X}, \mathbf{Y}) = \frac{p(\mathbf{Y}|\mathbf{X}, \mathbf{w}) p(\mathbf{w}|\mathbf{X})}{p(\mathbf{Y}|\mathbf{X})},
\]&lt;/p&gt;
&lt;p&gt;where \(\mathbf{X} \in \mathbb{R}^{n \times d}\) and \(\mathbf{Y} \in \mathbb{R}^n\).&lt;/p&gt;
&lt;p&gt;The choice of least squares also has statistical motivations. As discussed previously, we are making a reasonable assumption that there is some relationship between the features of the data and the observed output. This is typically modeled assume&lt;/p&gt;
&lt;p&gt;\[
\hat{\mathbf{Y}} = f(\mathbf{X}) + \epsilon.
\]&lt;/p&gt;
&lt;p&gt;Here, \(\epsilon\) is a random error term that is independent of \(\mathbf{X}\) and has 0 mean. This term represents any random noise that occurs either naturally or from sampling. It also includes any effects that are not properly captured by \(f\). Rearranging the terms of this equation to solve for \(\epsilon\) allows us to define the discrepencies in the model:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{\epsilon}_i = h(\mathbf{x}_{i}; \mathbf{w}) - \mathbf{y}_{i}.
\]&lt;/p&gt;
&lt;p&gt;If we assume that these discrepancies are independent and identically distributed with variance \(\sigma^2\) and Gaussian PDF \(f\), the likelihood of observations \(\mathbf{y}^{(i)}\) given parameters \(\mathbf{w}\) is&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) = \prod_{i=1}^{n} f(\epsilon_i; \sigma),
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
f(\epsilon_i; \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{\epsilon^2}{2\sigma^2}\Big).
\]&lt;/p&gt;
&lt;p&gt;This new parameter changes our original distribution function to&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{w}|\mathbf{X}, \mathbf{Y}, \sigma) = \frac{p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) p(\mathbf{w}|\mathbf{X}, \sigma)}{p(\mathbf{Y}|\mathbf{X}, \sigma)}.
\]&lt;/p&gt;
&lt;p&gt;Two things to note before moving on. First, the prior \(p(\mathbf{Y}|\mathbf{X}, \sigma)\) is a normalizing constant to ensure that the posterior is a valid probability distribution. Second, if we assume that all value for \(\mathbf{w}\) are equally likely, then \(p(\mathbf{w}|\mathbf{x}, \sigma)\) also becomes constant. This is a convenient assumption which implies that maximizing the posterior is equivalent to maximizing the likelihood function.&lt;/p&gt;
&lt;p&gt;With that out of the way, we can focus solely on the likelihood function. Expanding out the gaussian PDF \(f\) yields&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) = -\frac{n}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i})^2\Big).
\]&lt;/p&gt;
&lt;p&gt;We can see that maximizing \(p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma)\) is the same as minimizing the sum of squares. In practice, we use the negative log of the likelihood function since the negative logarithm is monotonically decreasing.&lt;/p&gt;
&lt;h2 id=&#34;solving-with-normal-equations&#34;&gt;Solving with Normal Equations&lt;/h2&gt;
&lt;p&gt;You may have studied the normal equations when you took Linear Algebra. The normal equations are motivated by finding approximate solutions to \(A\mathbf{x} = \mathbf{b}\). Most of the earlier part of linear algebra courses focus on finding exact solutions by solving systems of equations using Gaussian elimination (row reduction). Approximate solutions can be found by projecting the observed data points \(\mathbf{b}\) onto the column space of \(A\) and solving \(A \mathbf{x} = \hat{\mathbf{b}}\), where \(\hat{\mathbf{b}} = \text{proj}_{\text{Col} A}\mathbf{b}\). Then, \(\mathbf{b} - \hat{\mathbf{b}}\) represents a vector orthogonal to \(\text{Col}A\).&lt;/p&gt;
&lt;p&gt;It is helpful to keep in mind what \(A\), \(\mathbf{x}\), and \(\mathbf{b}\) represent. \(A\) and \(\mathbf{b}\) are the input and output values. If we were trying to predict home prices based on size, each row of \(A\) would represent the size of a different house. In \(\mathbf{b}\) we would record the corresponding prices. We are trying to solve for \(\mathbf{x}\), which is a vector that relates the input to the output.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-15_20-40-39_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;The plane represents every linear combination of the columns of A.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;The plane represents every linear combination of the columns of A.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Since each column vector of \(A\) is orthogonal to \(\mathbf{b} - \hat{\mathbf{b}}\), the dot product between them should be 0. Rewriting this, we get&lt;/p&gt;
&lt;p&gt;\begin{aligned}
A^T(\mathbf{b} - A\mathbf{x}) &amp;amp;= \mathbf{0}\\
A^T \mathbf{b} - A^T A \mathbf{x} &amp;amp;= \mathbf{0}.
\end{aligned}&lt;/p&gt;
&lt;p&gt;This means that each least-squares solution of \(A\mathbf{x} = \mathbf{b}\) satisfies&lt;/p&gt;
&lt;p&gt;\[
A^T A \mathbf{x} = A^T \mathbf{b}.
\]&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s take our univariate problem of \((\mathbf{x}, \mathbf{y})\) pairs. To use the normal equations to solve the least squares problem, we first change the notation just a bit as not confuse our data points and our parameters:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{X}^T \mathbf{X} \beta = \mathbf{X}^T \mathbf{y}
\]&lt;/p&gt;
&lt;p&gt;Create the design matrix \(\mathbf{X}\) where each row represents the the \(\mathbf{x}\) values. Recall that even though we only have 1 feature for \(\mathbf{x}\), we append the bias constant as \(x_0 = 1\) to account for the bias parameter. \(\mathbf{X}\) is then&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{X} =
\begin{bmatrix}
x_0^{(0)} &amp;amp; x_1^{(0)}\\
x_0^{(1)} &amp;amp; x_1^{(1)}\\
\vdots &amp;amp; \vdots \\
x_0^{(n)} &amp;amp; x_1^{(n)}
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;The parameter vector is&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\beta =
\begin{bmatrix}
\beta_0\\
\beta_1
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;The observed values are packed into \(\mathbf{y}\). We can then solve for \(\beta\) using any standard solver:&lt;/p&gt;
&lt;p&gt;\[
\beta = (\mathbf{X}^T \mathbf{X})^{-1}X^T \mathbf{y}.
\]&lt;/p&gt;
&lt;h3 id=&#34;rank-deficient-matrices&#34;&gt;Rank-Deficient matrices&lt;/h3&gt;
&lt;p&gt;In the event that the matrix \(\mathbf{X}^T \mathbf{X}\) is singular, then its inverse cannot be computed.
This implies that one or more of the features is a linear combination of the others.&lt;/p&gt;
&lt;p&gt;This can be detected by checking the rank of \(\mathbf{X}^T \mathbf{X}\) before attempting to compute the inverse.
You can also determine which features are redundant via Gaussian elimination.
The columns in the reduced matrix that do not have a pivot entry are redundant.&lt;/p&gt;
&lt;h2 id=&#34;another-approach-to-normal-equations&#34;&gt;Another Approach to Normal Equations&lt;/h2&gt;
&lt;p&gt;We can arrive at the normal equations by starting at the probabilistic perspective. Recall the likelihood function&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) = -\frac{n}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i})^2\Big).
\]&lt;/p&gt;
&lt;p&gt;Taking the natural log of this function yields&lt;/p&gt;
&lt;p&gt;\[
\ln p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) = - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(h(\mathbf{x}_{i}; \mathbf{w}) - \mathbf{y}_{i})^2 - \frac{n}{2}\ln(\sigma^2) - \frac{n}{2}\ln(2\pi).
\]&lt;/p&gt;
&lt;p&gt;As mentioned before, maximizing the likelihood function is equivalent to minimizing the sum-of-squares function. Thus, we must find the critical point of the likelihood function by computing the gradient (w.r.t. \(\mathbf{w}\)) and solving for 0:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\nabla \ln p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) &amp;amp;= \sum_{i=1}^{n}(\mathbf{w}^T\mathbf{x}_{i} - \mathbf{y}_{i})\mathbf{x}_{i}^{T}\\
&amp;amp;= \mathbf{w}^T \sum_{i=1}^{n}\mathbf{x}_i\mathbf{x}_i^T - \sum_{i=1}^{n}\mathbf{y}_{i}\mathbf{x}_{i}^{T}\\
\end{align*}&lt;/p&gt;
&lt;p&gt;Noting that \(\sum_{i=1}^{n}\mathbf{x}_i \mathbf{x}_i^T\) is simply matrix multiplication, we can use&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{X} =
\begin{bmatrix}
\mathbf{x}_1^T\\
\vdots\\
\mathbf{x}_n^T\\
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Then, \(\sum_{i=1}^{n}\mathbf{x}_i \mathbf{x}_i^T = \mathbf{X}^T \mathbf{X}\), \(\sum_{i=1}^{n}\mathbf{y}_i \mathbf{x}_i^T = \mathbf{Y}^T \mathbf{X}\), and&lt;/p&gt;
&lt;p&gt;\[
\nabla \ln p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) = \mathbf{w}^T \mathbf{X}^T \mathbf{X} - \mathbf{Y}^T \mathbf{X}.
\]&lt;/p&gt;
&lt;p&gt;Since we are finding the maximum likelihood, we set \(\nabla \ln p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) = 0\) and solve for \(\mathbf{w}\):&lt;/p&gt;
&lt;p&gt;\[
\mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}.
\]&lt;/p&gt;
&lt;p&gt;Thus, we arrive again at the normal equations and can solve this using a linear solver.&lt;/p&gt;
&lt;h2 id=&#34;fitting-polynomials&#34;&gt;Fitting Polynomials&lt;/h2&gt;
&lt;p&gt;Not every dataset can be modeled using a simple line.
Data can be exponential or logarithmic in nature.
We may also look to use &lt;a href=&#34;https://en.wikipedia.org/wiki/Spline_%28mathematics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;splines&lt;/a&gt; to model more complex data.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-01_17-08-27_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Data generated from a nonlinear function with added noise.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Data generated from a nonlinear function with added noise.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The dataset above was generated from the function as seen in red.
Using a simple linear model (blue) does not fit the data well.
For cases such as this, we can fit a polynomial to the data by changing our input data.&lt;/p&gt;
&lt;p&gt;The simple dataset above has 100 paired samples \((x_i, y_i)\).
There is only a single feature \(x_i\) for each sample.
It is trivial to determine that the shape of the data follows a cubic function.
One solution would be to raise each input to the power of 3.
This results in the function (blue) below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-01_17-30-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Solution from raising each input to the power of 3.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Solution from raising each input to the power of 3.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To fit this data, we need to add more features to our input.
Along with the original \(x_i\) features, we will also add \(x_i^2\) and \(x_i^3\).
Our data is then 3 dimensional.
The figure below shows the least squares fit using the modified data (blue).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-01_17-38-57_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Least squares fit using a polynomial model (blue).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Least squares fit using a polynomial model (blue).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;A demo of this can be found &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/blob/main/linear_regression/Linear%20Regression.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;linear-basis-functions&#34;&gt;Linear Basis Functions&lt;/h2&gt;
&lt;p&gt;Linear models are linear in their inputs.
This formulation is simple, producing models with limited representation.
Linear models can be extended as a linear combination of fixed nonlinear functions of the original features.
In the previous section, was saw that they could easily be extended to fit polynomial functions.&lt;/p&gt;
&lt;p&gt;We now consider creating a model that transforms the original input using one or more nonlinear functions.
This type of model is called a &lt;strong&gt;&lt;strong&gt;linear basis function model&lt;/strong&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;\[
h(\mathbf{x};\mathbf{w}) = \sum_{j=1}^{m} w_j\phi_j(\mathbf{x})
\]&lt;/p&gt;
&lt;p&gt;Common basis functions are the sigmoid, Gaussian, or exponential function.
If we choose the \(\sin\) function as a basis function, we can more closely fit our dataset using the least squares approach.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-01_18-46-08_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;A linear basis function model using the sin function as the choice of basis.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;A linear basis function model using the sin function as the choice of basis.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
  </channel>
</rss>
