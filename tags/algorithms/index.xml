<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Algorithms on Alex Dillhoff</title>
    <link>https://ajdillhoff.github.io/tags/algorithms/</link>
    <description>Recent content in Algorithms on Alex Dillhoff</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; {year}</copyright>
    <lastBuildDate>Thu, 25 Apr 2024 11:03:00 -0500</lastBuildDate>
    
	    <atom:link href="https://ajdillhoff.github.io/tags/algorithms/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>NP-Completeness</title>
      <link>https://ajdillhoff.github.io/notes/np_completeness/</link>
      <pubDate>Thu, 25 Apr 2024 11:03:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/np_completeness/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reductions&#34;&gt;Reductions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#clique-problem&#34;&gt;Clique Problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vertex-cover-problem&#34;&gt;Vertex Cover Problem&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Most of the algorithms discussed in a typical algorithms course run in polynomial time. This focus is reasonable since algorithms that run worse than polynomial time have little practical use. To simplify this notion: a problem for which a polynomial-time algorithm exists is &amp;ldquo;easy&amp;rdquo; and a problem for which no polynomial-time algorithm exists is &amp;ldquo;hard&amp;rdquo;. Knowing how to determine whether a problem is easy or hard is extremely useful. If one can identify a hard problem, then an approximate solution may be the best that can be achieved.&lt;/p&gt;
&lt;p&gt;One of the most fundamental problems in computer science is the classification of problems into these two categories. These notes provide an introduction to this classification.&lt;/p&gt;
&lt;h3 id=&#34;p-np-and-np-complete&#34;&gt;P, NP, and NP-Complete&lt;/h3&gt;
&lt;p&gt;We start with three classes of algorithms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Polynomial-time&lt;/li&gt;
&lt;li&gt;NP (nondeterministic polynomial time)&lt;/li&gt;
&lt;li&gt;NP-complete&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Problems in P are those solvable in polynomial time. This means &lt;em&gt;any&lt;/em&gt; constant \(k\) such that the running time is \(O(n^k)\).&lt;/p&gt;
&lt;p&gt;The class NP is a superset of P. These are problems that can be &lt;strong&gt;verified&lt;/strong&gt; in polynomial time. This means that if someone gives you a solution to the problem, you can verify that it is correct in polynomial time. This is different from solving the problem in polynomial time. Problems in NP can be solved in &lt;strong&gt;nondeterministic&lt;/strong&gt; polynomial time. However, such a model of computation does not exist in the real world.&lt;/p&gt;
&lt;p&gt;There is also a class of problems labeled &lt;strong&gt;NP-Hard&lt;/strong&gt;. These are problems that cannot be solved in polynomial time, nor can they be verified in polynomial times. Basically, they are not decision problems. This class includes certain optimization problems and the classic &lt;a href=&#34;https://en.wikipedia.org/wiki/Halting_problem&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Halting problem&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;NP-Complete problems are problems in NP that are as &lt;em&gt;hard&lt;/em&gt; as any other problem in NP. This means that if you can solve an NP-Complete problem in polynomial time, you can solve any problem in NP in polynomial time. This is why NP-Complete problems are so important.&lt;/p&gt;
&lt;h3 id=&#34;verifying-a-solution&#34;&gt;Verifying a Solution&lt;/h3&gt;
&lt;p&gt;As long as we can come up with a verification algorithm for a problem in polynomial time, we can say that the problem is in \(NP\). This is true even if we later find a polynomial-time algorithm for the problem.&lt;/p&gt;
&lt;h3 id=&#34;proving-that-a-problem-is-np-complete&#34;&gt;Proving that a problem is NP-Complete&lt;/h3&gt;
&lt;p&gt;Proving that a problem belongs to either NP or NPC is difficult the first time you do it. Luckily, now that problems have been proven to be NP-Complete, you can use these problems to prove that other problems are NP-Complete. Informally, a problem \(X\) is NP-Hard if it is at least as hard as any problem in NP. If we can reduce every problem \(Y \in NP\) to \(X\) in polynomial time, then \(X\) is NP-Hard. If \(X\) is also in NP, then \(X\) is NP-Complete.&lt;/p&gt;
&lt;h3 id=&#34;optimization-versus-decision-problems&#34;&gt;Optimization versus decision problems&lt;/h3&gt;
&lt;p&gt;Many problems are framed as optimization problems. Given some criteria, the goal is to find the best solution according to that criteria. For the shortest path problem, the algorithm finds a path between two vertices in the fewest number of edges. One can intuit that this is a slightly harder problem than that of determining if a path exists using only \(k\) edges. This latter problem is a decision problem.&lt;/p&gt;
&lt;p&gt;The reason this is worth talking about is that decision problems are often easier to come up with than optimization problems. If one can provide that a decision problem is hard, then its optimization problem is also hard.&lt;/p&gt;
&lt;h3 id=&#34;reducing-one-problem-to-another&#34;&gt;Reducing one problem to another&lt;/h3&gt;
&lt;p&gt;A common strategy for relating two problems is to reduce one to the other. For example, if problem \(B\) runs in polynomial time, and we can reduce problem \(A\) to problem \(B\) in polynomial time, then problem \(A\) is also in P. This is because we can solve \(A\) by reducing it to \(B\) and then solving \(B\) in polynomial time.&lt;/p&gt;
&lt;h2 id=&#34;reductions&#34;&gt;Reductions&lt;/h2&gt;
&lt;p&gt;The main idea behind reductions is to first show that a problem is NP-Complete. This first proof was done by Cook in 1971. With a problem proven to be NP-Complete, one can then show that other problems are NP-Complete by reducing them to the first problem. This method is far simpler than the original proof and provides a convenient process for proving that a problem is NP-Complete. The process is based on the following lemma.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 34.8&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If \(L\) is a language such that \(L&amp;rsquo; \leq_p L\) for some \(L&amp;rsquo; \in \text{NPC}\), then \(L\) is NP-hard. If, in addition, we have \(L \in \text{NP}\), then \(L\) is NP-Complete.&lt;/p&gt;
&lt;h3 id=&#34;circuit-satisfiability&#34;&gt;Circuit Satisfiability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What is this problem?&lt;/li&gt;
&lt;li&gt;Why is it important?&lt;/li&gt;
&lt;li&gt;How is it related to NP-Completeness?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given a boolean combinatorial circuit, is it satisfiable. That is, is there an assignment of values to the inputs that makes the output true?&lt;/p&gt;
&lt;p&gt;This problem is in the class NP. To prove this, we only need to show that given an assignment of values to the inputs, we can verify that the output is true in polynomial time. For a given circuit, we can verify that the output is true by following the circuit from the inputs to the outputs.&lt;/p&gt;
&lt;p&gt;Proving that it is NP-Complete is much more difficult, so a brief overview is provided here. Let \(A\) be any problem in \(NP\). Since any problem in \(NP\) has a polynomial-time verification algorithm, we can construct a boolean circuit that simulates this algorithm. This circuit will have a single output that is true if and only if the input is a valid solution to the problem. This circuit is satisfiable if and only if the input is a valid solution to the problem. Therefore, the circuit satisfiability problem is NP-Complete.&lt;/p&gt;
&lt;h4 id=&#34;example-exercise-34-dot-3-1&#34;&gt;Example: Exercise 34.3-1&lt;/h4&gt;
&lt;p&gt;Verify that the given circuit is unsatisfiable.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-27_14-31-53_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Figure 34.8 from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Figure 34.8 from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;For reference, here are definitions for each of the gates listed in the figure.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-27_14-35-18_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Definitions for the gates in the circuit from Figure 34.7 (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Definitions for the gates in the circuit from Figure 34.7 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;How can we prove this circuit is unsatisfiable?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The easiest way to do this is to code it up and brute force it.&lt;/p&gt;
&lt;p&gt;The satisfiability problem was the first problem proved to be NP-Complete by the Cook-Levin theorem. Cormen et al. describe a summarized proof in section 34.3 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;). Michael Sipser&amp;rsquo;s book on the theory of computation offers a more detailed sketch of the proof (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Sipser 2012&lt;/a&gt;). You can also &lt;a href=&#34;https://www.youtube.com/watch?v=6Az1gtDRaAU&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;watch his lecture&lt;/a&gt; on the Cook-Levin theorem from MIT.&lt;/p&gt;
&lt;p&gt;The basic idea of the proof is as follows. For any problem in NP, there exists some machine that can verify solutions in polynomial time. Cook&amp;rsquo;s proof involves simulating this machine with a Boolean formula. The idea is to encode the computation of the machine as a sequence of logical operations that can be expressed as a Boolean formula. Since the boolean formula is satisfiable if and only if the machine accepts the input, the satisfiability problem is NP-Complete.&lt;/p&gt;
&lt;h3 id=&#34;formula-satisfiability&#34;&gt;Formula Satisfiability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Satisfiability (SAT) is NP-Hard.&lt;/li&gt;
&lt;li&gt;Can show that CIRCUIT-SAT \(\leq_p\) SAT.&lt;/li&gt;
&lt;li&gt;Then show that SAT \(\leq_p\) 3SAT.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An instance of the &lt;strong&gt;formula satisfiability (SAT)&lt;/strong&gt; problem is a boolean formula \(\phi\) with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(n\) variables \(x_1, x_2, \ldots, x_n\)&lt;/li&gt;
&lt;li&gt;\(m\) clauses \(C_1, C_2, \ldots, C_m\)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, the formula&lt;/p&gt;
&lt;p&gt;\[
\phi = (x_1 \lor x_2) \land (\lnot x_1 \lor x_3) \land (x_2 \lor x_3)
\]&lt;/p&gt;
&lt;p&gt;has the satisfying assignment \(x_1 = 1, x_2 = 0, x_3 = 1\).&lt;/p&gt;
&lt;h4 id=&#34;sat-belongs-to-np&#34;&gt;SAT belongs to NP&lt;/h4&gt;
&lt;p&gt;Showing that SAT is in NP is straightforward. Given a boolean formula \(\phi\) and an assignment of values to the variables, one can verify that the formula is satisfied in polynomial time. This is enough to show that SAT is in NP.&lt;/p&gt;
&lt;h4 id=&#34;circuit-sat-leq-p-sat&#34;&gt;CIRCUIT-SAT \(\leq_p\) SAT&lt;/h4&gt;
&lt;p&gt;If we can reduce an instance of CIRCUIT-SAT, which is known to be NP-Complete, to SAT, then SAT is also NP-Complete. This proof is by contradiction: assume that SAT is not NP-Complete. Then, CIRCUIT-SAT is not NP-Complete. But we know that CIRCUIT-SAT is NP-Complete, so this is a contradiction.&lt;/p&gt;
&lt;p&gt;The reduction starts by introducing a variable for each wire and a clause for each gate, as seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-27_15-34-18_screenshot.png&#34; &gt;


&lt;/figure&gt;

&lt;p&gt;The reduction algorithm produces a formula for each gate in terms of an &amp;ldquo;if and only if&amp;rdquo; statement.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\phi = x_{10} &amp;amp;\land (x_4 \leftrightarrow \lnot x_3)\\
&amp;amp;\land (x_5 \leftrightarrow (x_1 \lor x_2))\\
&amp;amp;\land (x_6 \leftrightarrow \lnot x_4)\\
&amp;amp;\land (x_7 \leftrightarrow (x_1 \land x_2 \land x_4))\\
&amp;amp;\land (x_8 \leftrightarrow (x_5 \lor x_6))\\
&amp;amp;\land (x_9 \leftrightarrow (x_6 \lor x_7))\\
&amp;amp;\land (x_{10} \leftrightarrow (x_6 \land x_8 \land x_9))\\
\end{align*}&lt;/p&gt;
&lt;p&gt;A simpler explanation for this reduction is that a circuit can be represented as a boolean formula. This formula can be solved by the SAT algorithm.&lt;/p&gt;
&lt;h3 id=&#34;3sat&#34;&gt;3SAT&lt;/h3&gt;
&lt;p&gt;The 3SAT problem is a special case of SAT where each clause has exactly three literals. This problem is also NP-Complete. Many problems can be reduced to 3SAT, which is why it is so important.&lt;/p&gt;
&lt;h4 id=&#34;definition&#34;&gt;Definition&lt;/h4&gt;
&lt;p&gt;An instance of 3SAT is a boolean formula \(\phi\) with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(n\) &lt;strong&gt;literals&lt;/strong&gt; \(x_1, x_2, \ldots, x_n\)&lt;/li&gt;
&lt;li&gt;\(m\) &lt;strong&gt;clauses&lt;/strong&gt; \(C_1, C_2, \ldots, C_m\)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each clause has exactly three literals and is in &lt;strong&gt;conjunctive normal form&lt;/strong&gt; (CNF), which means it is expressed as an AND of clauses. For example, the formula&lt;/p&gt;
&lt;p&gt;\[
\phi = (x_1 \lor x_2 \lor x_3) \land (\lnot x_1 \lor x_2 \lor x_3) \land (x_1 \lor \lnot x_2 \lor x_3)
\]&lt;/p&gt;
&lt;p&gt;is a 3SAT formula.&lt;/p&gt;
&lt;h4 id=&#34;3sat-is-np-complete&#34;&gt;3SAT is NP-Complete&lt;/h4&gt;
&lt;p&gt;The 3SAT problem is NP-Complete. This can be shown by reducing SAT to 3SAT. A thorough proof is provided in the textbook (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;clique-problem&#34;&gt;Clique Problem&lt;/h2&gt;
&lt;p&gt;A clique is a &lt;strong&gt;complete subgraph&lt;/strong&gt; of an undirected graph \(G\). That is, a clique is a set of vertices such that every pair of vertices is connected by an edge. The &lt;strong&gt;clique problem&lt;/strong&gt; is to find the largest clique in a graph.&lt;/p&gt;
&lt;p&gt;\[
\text{CLIQUE} = \{ \langle G, k \rangle \mid G \text{ has a clique of size } k \}
\]&lt;/p&gt;
&lt;h3 id=&#34;clique-is-in-np&#34;&gt;Clique is in NP&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s say you have access to a clique of size \(k\). You can verify that this is a clique in polynomial time by checking that every pair of vertices is connected by an edge. That is, for each pair \(u, v \in V&amp;rsquo;\), the edge \((u, v)\) is in \(E\), where \(V&amp;rsquo;\) is the set of vertices in the clique.&lt;/p&gt;
&lt;p&gt;Thus, we have a polynomial-time verification algorithm for the clique problem, so it is in NP.&lt;/p&gt;
&lt;h3 id=&#34;clique-is-np-complete&#34;&gt;Clique is NP-Complete&lt;/h3&gt;
&lt;p&gt;Knowing that 3SAT is NP-Complete, we can reduce 3SAT to the clique problem. The reduction may not be intuitive as a boolean formula seems to have no relation to a graph.&lt;/p&gt;
&lt;p&gt;Let \(\phi = C_1 \land C_2 \land \ldots \land C_m\) be a 3SAT formula with \(n\) variables. We construct a graph \(G\) with \(3n\) vertices. Each \(C_r\) has three literals, \(l_1^r, l_2^r, l_3^r\).&lt;/p&gt;
&lt;p&gt;To construct the graph, we create a triplet of vertices \(v_1^r, v_2^r, v_3^r\) for each clause \(C_r\) such that there is no edge connecting any two vertices in the same triplet. There is an edge \((v_i^r, v_j^s) \in E\) if&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;\(v_i^r\) and \(v_j^s\) are in different triplets&lt;/li&gt;
&lt;li&gt;\(l_i^r\) and \(l_j^s\) are not negations of each other&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One such formula that we can convert is&lt;/p&gt;
&lt;p&gt;\[
\phi = (x_1 \lor \lnot x_2 \lor \lnot x_3) \land (\lnot x_1 \lor x_2 \lor x_3) \land (x_1 \lor x_2 \lor x_3).
\]&lt;/p&gt;
&lt;p&gt;The resulting graph is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-05-01_09-44-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Graph constructed from the 3SAT formula (phi) (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Graph constructed from the 3SAT formula (phi) (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;If a satisfying assignment exists for \(\phi\), then each \(C_r\) has at least one literal that is true. Consider the corresponding vertices in the graph for a satisfying assignment. Since there is at least one true literal in each clause, there is at least one edge between the corresponding vertices. Thus, a reduction from 3SAT to the clique problem is possible.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How does this show that Clique is NP-Complete?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is true that this example shows a very specialized graph. However, this is enough to show that the problem is NPC. If there were a polynomial time solution for Clique on a general graph \(G\), then surely it would work for a specialized graph as well.&lt;/p&gt;
&lt;p&gt;If it could solve this one, then the corresponding 3SAT formula would be solvable as well. This is a contradiction, so the Clique problem is NP-Complete.&lt;/p&gt;
&lt;h2 id=&#34;vertex-cover-problem&#34;&gt;Vertex Cover Problem&lt;/h2&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-05-01_09-49-06_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Vertex cover examples (Wikipedia).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Vertex cover examples (Wikipedia).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The &lt;strong&gt;vertex cover problem&lt;/strong&gt; is to find the smallest set of vertices such that every edge in the graph is incident to at least one vertex in the set. More formally, a vertex cover of a graph \(G\) is a set \(V&amp;rsquo; \subseteq V\) such that for every edge \((u, v) \in E\), either \(u \in V&amp;rsquo;\) or \(v \in V&amp;rsquo;\).&lt;/p&gt;
&lt;h3 id=&#34;vertex-cover-is-in-np&#34;&gt;Vertex Cover is in NP&lt;/h3&gt;
&lt;p&gt;Given a set of vertices \(V&amp;rsquo;\), one can verify that it is a vertex cover in polynomial time by checking that every edge is incident to at least one vertex in the set. This is a polynomial-time verification algorithm, so the vertex cover problem is in NP.&lt;/p&gt;
&lt;h3 id=&#34;vertex-cover-is-np-complete&#34;&gt;Vertex Cover is NP-Complete&lt;/h3&gt;
&lt;p&gt;We can show that the vertex cover problem is NP-Complete by reducing it to an instance of the clique problem. For this, we need to introduce the definition of a graph complement. Given a graph \(G = (V, E)\), the &lt;strong&gt;complement&lt;/strong&gt; of \(G\) is the graph \(\overline{G} = (V, E&amp;rsquo;)\) where \(E&amp;rsquo; = \{ (u, v) \mid u, v \in V \text{ and } (u, v) \notin E \}\). Basically, \(\overline{G}\) has all the edges that \(G\) does not have.&lt;/p&gt;
&lt;p&gt;Let \(G\) contain a clique \(V&amp;rsquo; \subseteq V\), where \(|V&amp;rsquo;| = k\). Then \(V - V&amp;rsquo;\) is a vertex cover of \(\overline{G}\). If \((u, v) \in \overline{E}\), but is not in \(E\), then at least one of \(u\) or \(v\) is not in \(V&amp;rsquo;\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-05-01_09-54-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Graph (G) and its complement (overline{G}) (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Graph (G) and its complement (overline{G}) (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the example above, any edge in \(\overline{G}\) has at least one vertex that is not in \(G\). On the same edge, at least one is in \(V - V&amp;rsquo;\), implying that \((u, v)\) is covered by \(V - V&amp;rsquo;\). Thus, a reduction from the clique problem to the vertex cover problem is possible.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Sipser, Michael. 2012. &lt;i&gt;Introduction to the Theory of Computation&lt;/i&gt;. 3rd. CENGAGE Learning.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>All-Pairs Shortest Paths</title>
      <link>https://ajdillhoff.github.io/notes/all_pairs_shortest_paths/</link>
      <pubDate>Sat, 20 Apr 2024 11:32:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/all_pairs_shortest_paths/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#problem-representation&#34;&gt;Problem Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-naive-solution&#34;&gt;A Naive Solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-floyd-warshall-algorithm&#34;&gt;The Floyd-Warshall Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;TychoLink is a telecommunications company looking to optimize its network for the fastest and most efficient data transfer possible. The network consists of multiple routers, each connected by various types of links that differ in latency and bandwidth. The company wants to ensure that data packets can travel from any router to any other router in the network using the path that offers the best balance between low latency and high bandwidth. There are three objectives in total:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Determine the all-pairs shortest paths across the network, taking into account both latency and bandwidth.&lt;/li&gt;
&lt;li&gt;Minimize the overall latency for data packet transmission across the network.&lt;/li&gt;
&lt;li&gt;Maximize the effective bandwidth along the chosen paths to ensure high data transfer rates.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Given the solutions discussed in &lt;a href=&#34;https://ajdillhoff.github.io/notes/single_source_shortest_paths/&#34;&gt;Single-Source Shortest Paths&lt;/a&gt;, we can simply run the Bellman-Ford algorithm for each router in the network to find the shortest paths to all other routers. This results in a time complexity of \(O(V^2 E)\). If the network is dense, then the number of edges \(E = \Theta(V^2)\), which results in a time complexity of \(O(V^4)\).&lt;/p&gt;
&lt;p&gt;These notes discuss another solution, the Floyd-Warshall algorithm, which can find the shortest paths between all pairs of routers in the network in \(O(V^3)\) time. The algorithm is particularly useful when the network is dense, as it is more efficient than running the Bellman-Ford algorithm for each router.&lt;/p&gt;
&lt;h2 id=&#34;problem-representation&#34;&gt;Problem Representation&lt;/h2&gt;
&lt;p&gt;Given a network \(G = (V, E)\) and a set of weights \(W = (w_{ij})\) for each edge \((i, j) \in E\), the goal is to find the shortest path between all pairs of vertices in \(V\). The graph and weights will be represented as an adjacency matrix with entries \(w_{ij}\) for each edge \((i, j) \in E\).&lt;/p&gt;
&lt;p&gt;\[
w_{ij} = \begin{cases}
0 &amp;amp; \text{if } i = j \\
\text{weight of edge } (i, j) &amp;amp; \text{if } i \neq j, (i, j) \in E \\
\infty &amp;amp; \text{if } i \neq j, (i, j) \notin E.
\end{cases}
\]&lt;/p&gt;
&lt;p&gt;The \((i, j)\) entry of the output matrix is \(\delta(i, j)\), the shortest-path weight from \(i\) to \(j\).&lt;/p&gt;
&lt;h2 id=&#34;a-naive-solution&#34;&gt;A Naive Solution&lt;/h2&gt;
&lt;p&gt;To construct a dynamic programming solution, we need to establish that the problem has &lt;strong&gt;optimal substructure&lt;/strong&gt;. The shortest path structure was first discussed in &lt;a href=&#34;https://ajdillhoff.github.io/notes/single_source_shortest_paths/&#34;&gt;Single-Source Shortest Paths&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;recursive-solution&#34;&gt;Recursive Solution&lt;/h3&gt;
&lt;p&gt;Step 2 is to state the recursive solution. Let \(l_{ij}^{( r)}\) be the minimum weight of any path \(i \leadsto j\) that contains at most \(r\) edges. For \(r = 0\), the cost is either 0 if \(i = j\) or \(\infty\) otherwise.&lt;/p&gt;
&lt;p&gt;For \(r = 1\), try all possible predecessors \(k\) of \(j\).&lt;/p&gt;
&lt;p&gt;\begin{align*}
l_{ij}^{( r)} &amp;amp;= \min \Big\{l_{ij}^{(r-1)}, \min \{l_{ik}^{(r-1)} + w_{kj} : 1 \leq k \leq n\}\Big\}\\
&amp;amp;= \min \{l_{ik}^{(r-1)} + w_{kj} : 1 \leq k \leq n\}.
\end{align*}&lt;/p&gt;
&lt;p&gt;Either the solution comes from a path of length \(r-1\) or from a path of length \(r-1\) with an additional edge \((k, j)\). The shortest path weights \(\delta(i, j)\) contain at most \(n-1\) edges since the shortest path cannot contain a cycle. This implies that \(\delta(i, j) = l_{ij}^{(n-1)} = l_{ij}^{(n)} = l_{ij}^{(n+1)} = \ldots\)&lt;/p&gt;
&lt;h3 id=&#34;bottom-up-approach&#34;&gt;Bottom-Up Approach&lt;/h3&gt;
&lt;p&gt;Starting with a matrix \(W = (w_{ij})\), where \(w_{ij}\) are the edge weights, the following approach computes a series of matrices \(L^{(0)}, L^{(1)}, \ldots, L^{(n-1)}\), where \(L^{( r)} = (l_{ij}^{( r)})\). The final matrix \(L^{(n-1)}\) contains the shortest path weights.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;extend_shortest_paths&lt;/span&gt;(L, W):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(L)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    L_prime &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;inf&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                L_prime[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(L_prime[i][j], L[i][k] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; W[k][j])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; L_prime
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This function has the structure of matrix multiplication and has a running time of \(O(n^3)\). Since this must be repeated \(n\) times, the total running time is \(O(n^4)\).&lt;/p&gt;
&lt;p&gt;Since they are so similar, we can actually reframe this as matrix multiplication. This will lead to a more efficient algorithm. Start with the statement being computed inside the loop:&lt;/p&gt;
&lt;p&gt;\[
l_{ij}^{( r)} = \min \{l_{ij}^{( r)}, l_{ik}^{(r-1)} + w_{kj}\}.
\]&lt;/p&gt;
&lt;p&gt;If we swap \(\min\) with \(+\) and \(\cdot\) with \(\min\), we can rewrite this as a matrix multiplication:&lt;/p&gt;
&lt;p&gt;\[
l_{ij}^{( r)} = l_{ij}^{( r)} + l_{ik}^{(r-1)} \cdot w_{kj}.
\]&lt;/p&gt;
&lt;p&gt;For this to yield the correct result, we also need to swap the identity of \(\min\), which is \(\infty\), with the identity of \(+\), which is 0.&lt;/p&gt;
&lt;h3 id=&#34;faster-apsp&#34;&gt;Faster APSP&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;What&amp;rsquo;s the point?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The purpose of this reframing is to benefit from the associative property of matrix multiplication. As it turns out, our modified matrix multiplication using \(\min\) is also associative. This allows us to compute the shortest paths in \(O(n^3 \lg n)\) time. Consider that we only really care about \(L^{(n-1)}\). If we are not using negative weights, then computing anything above \(n-1\) will yield the same result.&lt;/p&gt;
&lt;p&gt;We can get to this result in fewer steps by using &lt;strong&gt;repeated squaring&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;\begin{align*}
L^{(1)} &amp;amp;= &amp;amp;W, &amp;amp;\quad\\
L^{(2)} &amp;amp;= &amp;amp;W^2 &amp;amp;= W \cdot W,\\
L^{(4)} &amp;amp;= &amp;amp;W^4 &amp;amp;= W^2 \cdot W^2,\\
&amp;amp;   &amp;amp;\vdots &amp;amp;\\
L^{(2^{\lceil \lg (n-1) \rceil})} &amp;amp;= &amp;amp;W^{2^{\lceil \lg (n-1) \rceil}} &amp;amp;= W^{2^{\lceil \lg (n-1) \rceil - 1}} \cdot W^{2^{\lceil \lg (n-1) \rceil - 1}}.
\end{align*}&lt;/p&gt;
&lt;p&gt;In total, we compute \(O(\lg n)\) matrices, each of which takes \(O(n^3)\) time to compute. The total running time is \(O(n^3 \lg n)\).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;faster_all_pairs_shortest_paths&lt;/span&gt;(W):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(W)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    L &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; W
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; m &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        L &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; extend_shortest_paths(L, L)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        m &lt;span style=&#34;color:#f92672&#34;&gt;*=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; L
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Since we know that \(L^{( r)} = L^{(n-1)}\) for all \(r \geq n-1\), we can stop the loop when \(m \geq n-1\).&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Exercise 23.1-1:&lt;/strong&gt; Run APSP on the following graph and show the resulting matrices at each step.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-23_17-02-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Figure 23.2 from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Figure 23.2 from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;the-floyd-warshall-algorithm&#34;&gt;The Floyd-Warshall Algorithm&lt;/h2&gt;
&lt;p&gt;The next solution to the APSP problem is the &lt;strong&gt;Floyd-Warshall algorithm&lt;/strong&gt;. This algorithm can handle negative weight edges, but it will fail to produce a result if a negative weight cycle exists. It is a dynamic programming approach that reconsiders the structure of a shortest path.&lt;/p&gt;
&lt;p&gt;Given a path \(p = \langle v_1, v_2, \dots, v_l\rangle\), an &lt;strong&gt;intermediate vertex&lt;/strong&gt; is an vertex of \(p\) other than \(v_1\) and \(v_l\). Then define \(d_{ij}^{(k)}\) as the weight of the shortest path from \(i\) to \(j\) that uses only the vertices \(\{1, 2, \dots, k\}\) as intermediate vertices. Note that the vertices are arbitrarily numbered from 1 to \(n\).&lt;/p&gt;
&lt;p&gt;Consider a shortest path \(i \leadsto j\) that uses intermediate vertices in the set \(\{1, 2, \dots, k\}\):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Drop \(k\)&lt;/strong&gt;: If \(k\) is not an intermediate vertex, then the path is a shortest path from \(i\) to \(j\) that uses only the vertices \(\{1, 2, \dots, k-1\}\) as intermediate vertices.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Split \(k\)&lt;/strong&gt;: If \(k\) is an intermediate vertex, split the path \(p\) into \(i \overset{p_1}{\leadsto} k \overset{p_2}{\leadsto} j\). In this case, \(p_1\) is a shortest path from \(i\) to \(k\) that uses only the vertices \(\{1, 2, \dots, k-1\}\) as intermediate vertices, and \(p_2\) is a shortest path from \(k\) to \(j\) that uses only the vertices \(\{1, 2, \dots, k-1\}\) as intermediate vertices.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Notice that, in either case, the set of intermediate vertices is reduced.&lt;/p&gt;
&lt;h3 id=&#34;recursive-solution&#34;&gt;Recursive Solution&lt;/h3&gt;
&lt;p&gt;Let \(d_{ij}^{(k)}\) be the weight of a shortest path from \(i\) to \(j\) that uses only the vertices \(\{1, 2, \dots, k\}\) as intermediate vertices. The base case is \(d_{ij}^{(0)} = w_{ij}\).&lt;/p&gt;
&lt;p&gt;\[
d_{ij}^{(k)} = \begin{cases}
w_{ij} &amp;amp; \text{if } k = 0,\\
\min \{d_{ij}^{(k-1)}, d_{ik}^{(k-1)} + d_{kj}^{(k-1)}\} &amp;amp; \text{if } k \geq 1.
\end{cases}
\]&lt;/p&gt;
&lt;p&gt;The goal is to compute \(D^{(n)} = (d_{ij}^{(n)})\), since all intermediate vertices belong to the set \(\{1, 2, \dots, n\}\).&lt;/p&gt;
&lt;h3 id=&#34;bottom-up-approach&#34;&gt;Bottom-Up Approach&lt;/h3&gt;
&lt;p&gt;With a recurrent solution in hand, a bottom-up approach can be used to compute the shortest path. The &lt;strong&gt;Floyd-Warshall&lt;/strong&gt; algorithm takes as input a weighted adjacency matrix \(W = (w_{ij})\) and returns a matrix \(D = (d_{ij})\).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;floyd_warshall&lt;/span&gt;(W):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(W)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    D &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; W
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        D_prime &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;inf&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                D_prime[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(D[i][j], D[i][k] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; D[k][j])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        D &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; D_prime
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; D
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This provides a running time of \(\Theta(n^3)\).&lt;/p&gt;
&lt;h3 id=&#34;constructing-the-shortest-paths&#34;&gt;Constructing the Shortest Paths&lt;/h3&gt;
&lt;p&gt;The Floyd-Warshall algorithm can be modified to construct the shortest paths themselves. This can be done by maintaining a matrix \(P\) that stores the predecessor of each vertex along the shortest path. Let \(P^{(k)} = (p_{ij}^{(k)})\) for \(k = 0, 1, \dots, n\) be the matrix of predecessors. Each entry \(p_{ij}^{(k)}\) is defined recursively. The base case is&lt;/p&gt;
&lt;p&gt;\[
p_{ij}^{(0)} = \begin{cases}
i &amp;amp; \text{if } i \neq j \text{ and } w_{ij} &amp;lt; \infty,\\
\text{NIL} &amp;amp; \text{otherwise}.
\end{cases}
\]&lt;/p&gt;
&lt;p&gt;The recursive case is&lt;/p&gt;
&lt;p&gt;\[
p_{ij}^{(k)} = \begin{cases}
p_{kj}^{(k-1)} &amp;amp; \text{if } d_{ij}^{(k-1)} &amp;gt; d_{ik}^{(k-1)} + d_{kj}^{(k-1)},\\
p_{ij}^{(k-1)} &amp;amp; \text{otherwise}.
\end{cases}
\]&lt;/p&gt;
&lt;p&gt;In words, the recursive case is split into two parts. If the shortest path from \(i\) to \(j\) has \(k\) as an intermediate vertex, then it is \(i \leadsto k \leadsto j\) where \(k \neq j\). In this case, choose \(j\)&amp;rsquo;s predecessor to be the predecessor of \(j\) on a shortest path from \(k\) to \(j\) with all intermediate vertices less than \(k\): \(p_{ij}^{(k)} = p_{kj}^{(k-1)}\).&lt;/p&gt;
&lt;p&gt;The second subcase is when \(k\) is not an intermediate vertex. Keep the same predecessor as the shortest path from \(i\) to \(j\) with all intermediate vertices less than \(k\): \(p_{ij}^{(k)} = p_{ij}^{(k-1)}\).&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Walk through the Floyd-Warshall algorithm on the graph from the previous example. A Python notebook of this example is available &lt;a href=&#34;https://github.com/ajdillhoff/python-examples/blob/main/data_structures/graphs/floyd_warshall.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;in the repository.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;transitive-closure-of-a-graph&#34;&gt;Transitive Closure of a Graph&lt;/h3&gt;
&lt;p&gt;The algorithms presented above successfully solve the all-pairs shortest paths problem. &lt;strong&gt;What if we simply wanted to determine if a path exists between for all pairs of vertices?&lt;/strong&gt; The answer to this question lies in the &lt;strong&gt;transitive closure&lt;/strong&gt; of a graph.&lt;/p&gt;
&lt;p&gt;Let \(G = (V, E)\) be a directed graph with a vertex set \(V = \{1, 2, \dots, n\}\). The &lt;strong&gt;transitive closure&lt;/strong&gt; of \(G\) is a graph \(G^* = (V, E^*)\) such that \((i, j) \in E^*\) if there is a path from \(i\) to \(j\) in \(G\).&lt;/p&gt;
&lt;p&gt;One solution to this problem is to assign a weight of 1 to each edge of \(E\) and run the Floyd-Warshall algorithm. If \(d_{ij} &amp;lt; n\), then there is a path from \(i\) to \(j\) in \(G\). If \(d_{ij} = \infty\), then there is no path from \(i\) to \(j\) in \(G\). First, substitute the \(\min\) and \(+\) operations with \(\lor\) (OR) and \(\land\) (AND), respectively. This will allow us to determine if a path exists between two vertices.&lt;/p&gt;
&lt;p&gt;Just like Floyd-Warshall, we will maintain a series of matrices \(T^{(0)}, T^{(1)}, \ldots, T^{(n-1)}\), where \(T^{( r)} = (t_{ij}^{( r)})\). The final matrix \(T^{(n)}\) contains the transitive closure of the graph. The values are defined as&lt;/p&gt;
&lt;p&gt;\[
t_{ij}^{( r)} = \begin{cases}
1 &amp;amp; \text{if } r = 0 \text{ and } (i, j) \in E,\\
1 &amp;amp; \text{if } r &amp;gt; 0 \text{ and } (t_{ij}^{(r-1)} = 1 \lor (t_{ik}^{(r-1)} \land t_{kj}^{(r-1)})),\\
0 &amp;amp; \text{otherwise}.
\end{cases}
\]&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;transitive_closure&lt;/span&gt;(W):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(W)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    T &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; W
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        T_prime &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                T_prime[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; (T[i][k] &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; T[k][j])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        T &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T_prime
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; T
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This algorithm has a running time of \(\Theta(n^3)\) while using simpler operations compared to the Floyd-Warshall algorithm.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Maximum Flow</title>
      <link>https://ajdillhoff.github.io/notes/maximum_flow/</link>
      <pubDate>Fri, 12 Apr 2024 18:51:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/maximum_flow/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#objective-questions&#34;&gt;Objective Questions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maximum-flow&#34;&gt;Maximum Flow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-polynomial-time-solution&#34;&gt;A polynomial time solution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;A flow network is a directed graph in which the edges begin at a node that produces the flow and the adjacent nodes are the ones that receive it. &lt;em&gt;Flow&lt;/em&gt; in this context could take on many meanings, such as the amount of water that can flow through a pipe, the amount of data that can be sent through a network, or the amount of traffic that can be sent through a road network. The goal of a flow network is to maximize the flow from the source to the sink.&lt;/p&gt;
&lt;p&gt;The problem may have intermediate constraints. For example, a network graph may have a node with limited bandwidth, so the flow through that node must be less than or equal to the bandwidth. These notes review the formal definition of the problem followed by a solution using the Ford-Fulkerson algorithm as well as one related to bipartite matching.&lt;/p&gt;
&lt;h2 id=&#34;objective-questions&#34;&gt;Objective Questions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; What is the &lt;strong&gt;maximum flow&lt;/strong&gt; problem?&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; How can we solve it?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;maximum-flow&#34;&gt;Maximum Flow&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;flow network&lt;/strong&gt; \(G = (V, E)\) is a directed graph in which each edge \((u, v) \in E\) has a nonnegative &lt;strong&gt;capacity&lt;/strong&gt; \(c(u, v) \geq 0\). The graph does not contain reverse edges between two vertices. If an edge does not exist in the set, then its capacity is 0. Each graph has a &lt;strong&gt;source&lt;/strong&gt; and a &lt;strong&gt;sink&lt;/strong&gt;, which will be the main edges of note when analyzing the graph. The goal is to maximize the flow going from the source to the sink. This implies that the source has no incoming edges.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-13_19-02-08_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;A flow network. Each edge depicts (f(u,v)/c(u,v)), the flow and capacity (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;A flow network. Each edge depicts (f(u,v)/c(u,v)), the flow and capacity (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;A &lt;strong&gt;flow&lt;/strong&gt; in a graph \(G\) is a function \(f : V \times V \rightarrow \mathbb{R}\) that satisfies two properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Capacity constraint:&lt;/strong&gt; For all \(u, v \in V\),
\[
0 \leq f(u,v) \leq c(u,v).
\]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flow conservation:&lt;/strong&gt; For all \(u \in V - \{s, t\}\),
\[
\sum_{v \in V} f(v, u) = \sum_{v \in V} f(u, v).
\]&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are usually many different possibly paths of flow in a flow network. The &lt;strong&gt;maximum flow problem&lt;/strong&gt; asks: what is the path that yields the maximum flow?&lt;/p&gt;
&lt;h3 id=&#34;antiparallel-edges&#34;&gt;Antiparallel Edges&lt;/h3&gt;
&lt;p&gt;The restriction that no two nodes may have more than one edge seems to be unrealistic. For example, modeling the flow of a network graph with this restriction means that network traffic can only move in one direction between two datacenters. If there were two edges between adjacent nodes \(v_1\) and \(v_2\) such that \((v_1, v_2) \in E\) and \((v_1, v_2) \in E\), we would call these edges &lt;strong&gt;antiparallel&lt;/strong&gt;. In such cases, the graph is modified with a new node \(v&amp;rsquo;\) such that \((v_1, v&amp;rsquo;) \in E\) and \((v&amp;rsquo;, v_2) \in E\). An example is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-13_19-23-43_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Addressing antiparallel edges in a flow network (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Addressing antiparallel edges in a flow network (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;multiple-sources-and-sinks&#34;&gt;Multiple Sources and Sinks&lt;/h3&gt;
&lt;p&gt;Another restriction that is unrealistic in many real-world scenarios is that maximum flow graphs can only have a single source and sink. It is easy to imagine a scenario where multiple sources and sinks within a network. Again, the graph can be modified to accommodate this scenario by defining a &lt;strong&gt;supersource&lt;/strong&gt; and &lt;strong&gt;supersink&lt;/strong&gt; whose outgoing and incoming flows are infinite.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-13_19-27-09_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Assing a supersource and supersink to a graph with multiple sources and sinks (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Assing a supersource and supersink to a graph with multiple sources and sinks (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;a-polynomial-time-solution&#34;&gt;A polynomial time solution&lt;/h2&gt;
&lt;p&gt;Ford-Fulkerson relies on three foundational concepts:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Residual networks&lt;/li&gt;
&lt;li&gt;Augmenting paths&lt;/li&gt;
&lt;li&gt;Cuts&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The solution presented by Ford and Fulkerson is not a single algorithm but rather a set of general instructions.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize \(f(u, v) = 0\) for all \(u,v \in V\), giving an initial flow of 0.&lt;/li&gt;
&lt;li&gt;Increase the flow by finding an &lt;strong&gt;augmenting path&lt;/strong&gt; in a &lt;strong&gt;residual network&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;The edges of the augmented path indicate where to increase the flow.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is repeated until the residual network has no more augmenting paths.&lt;/p&gt;
&lt;h3 id=&#34;residual-networks&#34;&gt;Residual Networks&lt;/h3&gt;
&lt;p&gt;Consider a pair of vertices \(u, v \in V\), the residual capacity \(c_f(u, v)\) is amount of additional flow that can be pushed from \(u\) to \(v\).&lt;/p&gt;
&lt;p&gt;\[
c_f(u, v)= \begin{cases}
c(u,v) - f(u, v)\quad \text{if } (u,v) \in E,\\
f(v, u)\quad \text{if } (v, u) \in E,\\
0\quad \text{otherwise (i.e., } (u, v), (v, u) \notin E).
\end{cases}
\]&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;residual network&lt;/strong&gt; is \(G_f = (V, E_f)\), where&lt;/p&gt;
&lt;p&gt;\[
E_f= \{(u, v) \in V \times V : c_f(u, v) &amp;gt; 0\}.
\]&lt;/p&gt;
&lt;p&gt;The edges of \(G_f\) represent those edges in \(G\) with the capacity to change the flow. There is also no requirement for all edges in \(G\) to be present in \(G_f\). As the algorithm works out the solution, we are only considered with edges that permit more flow.&lt;/p&gt;
&lt;p&gt;An edge \((u, v) \in E\) means that the reverse edge \((v, u) \notin E\). However, the residual network can have edges that are not in \(G\). These are used to represent paths in which flow is sent in the reverse direction. This can happen if reducing flow from one edge results in a net increase across some other.&lt;/p&gt;
&lt;p&gt;In \(G_f\), the reverse edges \((v, u)\) represent the flow on \((u, v) \in G\) that could be sent back.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-19_15-29-03_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;A flow network and its residual network (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;A flow network and its residual network (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;augmentation-function&#34;&gt;Augmentation Function&lt;/h4&gt;
&lt;p&gt;Given flows \(f\) in \(G\) and \(f&amp;rsquo;\) in \(G_f\), define the &lt;strong&gt;augmentation&lt;/strong&gt; of \(f\) by \(f&amp;rsquo;\), as a function \(V \times V \rightarrow \mathbb{R}\):&lt;/p&gt;
&lt;p&gt;\[
(f \uparrow f&amp;rsquo;)(u, v) = \begin{cases}
f(u, v) + f&amp;rsquo;(u, v) - f&amp;rsquo;(u, v)\quad \text{if } (u, v) \in E,\\
0 \quad \text{otherwise}
\end{cases}
\]&lt;/p&gt;
&lt;p&gt;for all \((u, v) \in V\).&lt;/p&gt;
&lt;p&gt;This augmentation function represents an increase of flow on \((u, v)\) by \(f&amp;rsquo;(u, v)\) with a decrease by \(f&amp;rsquo;(v, u)\) since pushing flow on the reverse edge in \(G_f\) represents a decrease in \(G\). This is known as &lt;strong&gt;cancellation&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given a flow network \(G\), a flow \(f\) in \(G\), and the residual network \(G_f\), let \(f&amp;rsquo;\) be a flow in \(G_f\). Then \((f \uparrow f&amp;rsquo;)\) is a flow in \(G\) with value \(|f \uparrow f&amp;rsquo;| = |f| + |f&amp;rsquo;|\).&lt;/p&gt;
&lt;p&gt;This lemma defines the idea of &lt;strong&gt;net&lt;/strong&gt; flow. If there are 10 units of flow in one direction and 4 in the other, the edge effectively has 6 units of flow.&lt;/p&gt;
&lt;h3 id=&#34;augmenting-paths&#34;&gt;Augmenting Paths&lt;/h3&gt;
&lt;p&gt;An &lt;strong&gt;augmenting path&lt;/strong&gt; is a simple path from the source to the sink in the residual network.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-28_13-46-13_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;An augmenting path in a flow network (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;An augmenting path in a flow network (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The purpose of an augmenting path is to increase the flow from the source to the sink. The flow is increased by the minimum capacity of the edges in the path.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 24.2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let \(G = (V, E)\) be a flow network, let \(f\) be a flow in \(G\), and let \(p\) be an augmenting path in \(G_f\). Define \(f_p : V \times V \rightarrow \mathbb{R}\) by&lt;/p&gt;
&lt;p&gt;\[
f_p(u, v) = \begin{cases}
c_f(p) &amp;amp; \text{if } (u, v) \text{ is in } p,\\
0 &amp;amp; \text{otherwise}.
\end{cases}
\]&lt;/p&gt;
&lt;p&gt;Then \(f_p\) is a flow in \(G_f\) with value \(|f_p| &amp;gt; 0\).&lt;/p&gt;
&lt;p&gt;The maximum amount that an augmenting path can be increased is the minimum capacity of the edges in the path. This is known as the &lt;strong&gt;residual capacity&lt;/strong&gt; of the path.&lt;/p&gt;
&lt;p&gt;\[
c_f(p) = \min \{c_f(u, v) : (u, v) \text{ is in } p\}.
\]&lt;/p&gt;
&lt;p&gt;Put simply, if there is a path in the residual network, the flow can be increased by the minimum capacity of the edges in the path. If there is no path, the flow is at its maximum.&lt;/p&gt;
&lt;h3 id=&#34;cuts&#34;&gt;Cuts&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;cut&lt;/strong&gt; \((S, T)\) of a flow network \(G = (V, E)\) is a partition of \(V\) into two sets \(S\) and \(T = V - S\) such that \(s \in S\) and \(t \in T\). The &lt;strong&gt;capacity&lt;/strong&gt; of the cut is the sum of the capacities of the edges from \(S\) to \(T\). Any cut is a valid cut as long as the source is in \(S\) and the sink is in \(T\).&lt;/p&gt;
&lt;p&gt;If \(f\) is a flow in \(G\) and \((S, T)\) is a cut of \(G\), then the &lt;strong&gt;net flow&lt;/strong&gt; across the cut is&lt;/p&gt;
&lt;p&gt;\[
f(S, T) = \sum_{u \in S} \sum_{v \in T} f(u, v) - \sum_{u \in S} \sum_{v \in T} f(v, u).
\]&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;capacity&lt;/strong&gt; of the cut is&lt;/p&gt;
&lt;p&gt;\[
c(S, T) = \sum_{u \in S} \sum_{v \in T} c(u, v).
\]&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;minimum cut&lt;/strong&gt; is a cut whose capacity is the smallest among all cuts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For any flow \(f\) and any cut \((S, T)\) of \(G\), we have that $|f| = f(S, T).$&lt;/p&gt;
&lt;p&gt;This lemma states that the flow across a cut is equal to the value of the flow.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\begin{align*}
f(S, T) &amp;amp;= f(S, V) - f(S, S)\\
&amp;amp;= f(S, V)\\
&amp;amp;= f(s, V) + f(S - s, V)\\
&amp;amp;= f(s, V) = |f|.
\end{align*}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key concept:&lt;/strong&gt; We can determine the flow by making cuts in the graph. The minimum cut leads to the maximum flow.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Corollary&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The value of any flow \(f\) in a flow network \(G\) is bounded from above by the capacity of any cut of \(G\).&lt;/p&gt;
&lt;h4 id=&#34;max-flow-min-cut-theorem&#34;&gt;Max-flow Min-cut Theorem&lt;/h4&gt;
&lt;p&gt;The following statements are logically equivalent.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The flow \(f\) is a maximum flow in \(G\).&lt;/li&gt;
&lt;li&gt;The residual network \(G_f\) contains no augmenting paths.&lt;/li&gt;
&lt;li&gt;The value of the flow \(f\) is equal to the capacity of the cut \((S, T)\) for some cut of \(G\).&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;ford-fulkerson-algorithm&#34;&gt;Ford-Fulkerson Algorithm&lt;/h3&gt;
&lt;p&gt;The Ford-Fulkerson algorithm is a general method for solving the maximum flow problem. The algorithm is not a single algorithm but rather a set of instructions that can be implemented in different ways. The algorithm is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ford_fulkerson&lt;/span&gt;(G, s, t):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {u: {v: &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G} &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; u &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Find an augmenting path&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bfs(G, s, t, f)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; path:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        cf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(G[u][v] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; f[u][v] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; u, v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; u, v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; path:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            f[u][v] &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; cf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            f[v][u] &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; cf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; f
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;example&#34;&gt;Example&lt;/h4&gt;
&lt;p&gt;Run the Ford-Fulkerson algorithm on the following graph.&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;The running time of Ford-Fulkerson hinges on how the augmenting path is found. If implemented with a breadth-first search, the algorithm runs in \(O(VE^2)\) time.&lt;/p&gt;
&lt;h3 id=&#34;edmonds-karp-algorithm&#34;&gt;Edmonds-Karp Algorithm&lt;/h3&gt;
&lt;p&gt;The Edmonds-Karp algorithm is a specific implementation of Ford-Fulkerson that uses breadth-first search to find the augmenting path. The algorithm presented above is actually the Edmonds-Karp algorithm.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Topological Sort</title>
      <link>https://ajdillhoff.github.io/notes/topological_sort/</link>
      <pubDate>Thu, 04 Apr 2024 10:21:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/topological_sort/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#topological-sort&#34;&gt;Topological Sort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#strongly-connected-components&#34;&gt;Strongly Connected Components&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#application-recommender-graphs&#34;&gt;Application: Recommender Graphs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;topological-sort&#34;&gt;Topological Sort&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;topological sort&lt;/strong&gt; of a directed acyclic graph \(G = (V, E)\) is a linear ordering of all its vertices such that for every directed edge \((u, v) \in E\), vertex \(u\) comes before vertex \(v\) in the ordering.&lt;/p&gt;
&lt;p&gt;The process itself can be described simply:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Call \(\text{DFS}(G)\) to compute the finishing times for each vertex \(v\).&lt;/li&gt;
&lt;li&gt;As each vertex is finished, insert it onto the front of a linked list.&lt;/li&gt;
&lt;li&gt;Return the linked list of vertices.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The entire call takes \(\Theta(V + E)\) since DFS\((G)\) takes \(\Theta(V + E)\) time. Inserting each vertex onto the front of the list can be done in constant time.&lt;/p&gt;
&lt;h4 id=&#34;lemma&#34;&gt;Lemma&lt;/h4&gt;
&lt;p&gt;A directed graph \(G\) is acyclic if and only if a DFS of \(G\) yields no &lt;em&gt;back edges&lt;/em&gt; &amp;ndash; an edge \((u, v)\) such that \(v\) is an ancestor of \(u\) in the DFS forest.&lt;/p&gt;
&lt;h4 id=&#34;proof&#34;&gt;Proof&lt;/h4&gt;
&lt;p&gt;The proof is by contradiction: if a back edge exists, then there is a cycle in the graph.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-04_10-59-53_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;A back edge between edges (u) and (v) (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;A back edge between edges (u) and (v) (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Suppose there is a back edge \((u, v)\) as shown in the figure above. In this case, \(v\) is an ancestor of \(u\) in the depth-first forest. There is a path \(v \leadsto u\), so \(v \leadsto u \rightarrow v\) is a cycle.&lt;/p&gt;
&lt;p&gt;In the other direction, suppose that \(G\) contains a cycle \(c\). Let \(v\) be the first vertex discovered in \(c\), and let \((u, v)\) be the preceding edge in \(c\). At time \(v.d\), vertices of \(c\) form a white path \(v \leadsto u\). Since \(u\) is a descendant of \(v\), \((u, v)\) is a back edge. \(\blacksquare\)&lt;/p&gt;
&lt;h4 id=&#34;theorem&#34;&gt;Theorem&lt;/h4&gt;
&lt;p&gt;The topological sort algorithm produces a topological sort of a directed acyclic graph.&lt;/p&gt;
&lt;h4 id=&#34;proof&#34;&gt;Proof&lt;/h4&gt;
&lt;p&gt;Run a DFS on the graph \(G\) to determine finish times for its vertices. For any pair of vertices \((u, v)\), if \(G\) contains an edge from \(u\) to \(v\), then \(v.f &amp;lt; u.f\). (Review DFS).&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-06_13-04-12_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;DAG for topological sorting. Figure 20.8 from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;DAG for topological sorting. Figure 20.8 from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;strongly-connected-components&#34;&gt;Strongly Connected Components&lt;/h2&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-06_14-40-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Strongly connected components of a directed graph (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Strongly connected components of a directed graph (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;A &lt;strong&gt;strongly connected component&lt;/strong&gt; of a directed graph \(G\) is a maximal set of vertices such that for every pair of vertices \(u\) and \(v\) in the set, there is a path from \(u\) to \(v\) and a path from \(v\) to \(u\). The algorithm goes as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Call \(\text{DFS}(G)\) to compute the finishing times for each vertex \(v\).&lt;/li&gt;
&lt;li&gt;Compute the transpose of \(G\).&lt;/li&gt;
&lt;li&gt;Call \(\text{DFS}(G^T)\), but in the main loop of DFS, consider the vertices in order of decreasing finishing times.&lt;/li&gt;
&lt;li&gt;Output the vertices of each tree in the depth-first forest as a separate strongly connected component.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The transpose of a graph \(G^T\) is the graph \(G\) with all edges reversed.&lt;/p&gt;
&lt;p&gt;\begin{align*}
G^T &amp;amp;= (V, E^T) \\
E^T &amp;amp;= \{(v, u) \mid (u, v) \in E\}
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;component-graphs&#34;&gt;Component Graphs&lt;/h3&gt;
&lt;p&gt;The resulting &lt;strong&gt;component graph&lt;/strong&gt; is a directed graph \(G_{SCC} = (V_{SCC}, E_{SCC})\) where each vertex represents a strongly connected component of the original graph \(G\). There is an edge \((C_i, C_j)\) in \(G_{SCC}\) if there is a vertex \(u \in C_i\) and a vertex \(v \in C_j\) such that \((u, v) \in E\). The component graph of the DAG from above is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-06_14-42-39_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Component graph of the DAG from above (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Component graph of the DAG from above (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;lemma&#34;&gt;Lemma&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;The component graph \(G^{SCC}\) is a directed acyclic graph.&lt;/strong&gt; Let \(C\) and \(C&amp;rsquo;\) be distinct strongly connected components in \(G\), where \(u, v \in C\) and \(u&amp;rsquo;, v&amp;rsquo; \in C&amp;rsquo;\), and suppose there is a path \(u \leadsto u&amp;rsquo;\) in \(G\). Then there cannot also be a path \(v&amp;rsquo; \leadsto v\) in \(G\).&lt;/p&gt;
&lt;h4 id=&#34;proof&#34;&gt;Proof&lt;/h4&gt;
&lt;p&gt;Suppose there is a path \(v&amp;rsquo; \leadsto v\) in \(G\). This implies there are paths \(u \leadsto u&amp;rsquo; \leadsto v&amp;rsquo;\) and \(v&amp;rsquo; \leadsto v \leadsto u\). If this were possible, then \(u\) and \(v&amp;rsquo;\) are reachable from each other, which contradicts the assumption that \(C\) and \(C&amp;rsquo;\) are distinct strongly connected components. \(\blacksquare\)&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;h3 id=&#34;finishing-times&#34;&gt;Finishing Times&lt;/h3&gt;
&lt;p&gt;The previous and following lemmas establish, given the algorithm presented above, the rules of the finishing times of strongly connected components. These are used to prove the correctness of the algorithm.&lt;/p&gt;
&lt;h4 id=&#34;lemma&#34;&gt;Lemma&lt;/h4&gt;
&lt;p&gt;Let \(C\) and \(C&amp;rsquo;\) be strongly connected components in a directed graph \(G\). If there is an edge \((u, v) \in E\) such that \(u \in C\) and \(v \in C&amp;rsquo;\), then \(f( C) &amp;gt; f(C&amp;rsquo;)\).&lt;/p&gt;
&lt;h4 id=&#34;proof&#34;&gt;Proof&lt;/h4&gt;
&lt;p&gt;There are two cases to consider depending on which strongly connected component had the first discovered vertex during the first DFS call.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 1&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If \(d( C) &amp;lt; d(C&amp;rsquo;)\), let \(x\) be the first vertex discovered in \(C\). At time \(x.d\), the time of discovery, all vertices in \(C\) and \(C&amp;rsquo;\) are white. Thus, there exists paths of white vertices from \(x\) to all vertices in \(C\) and \(C&amp;rsquo;\).&lt;/li&gt;
&lt;li&gt;By the &lt;a href=&#34;https://ajdillhoff.github.io/notes/introduction_to_graph_theory/&#34;&gt;white-path theorem&lt;/a&gt;, all vertices in \(C\) and \(C&amp;rsquo;\) are descendants of \(x\) in the depth-first tree.&lt;/li&gt;
&lt;li&gt;By the parenthesis theorem, \(x.f = f( C) &amp;gt; f(C&amp;rsquo;)\).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Case 2&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If \(d( C) &amp;gt; d(C&amp;rsquo;)\), let \(y\) be the first vertex discovered in \(C&amp;rsquo;\). At time \(y.d\), all vertices in \(C\) and \(C&amp;rsquo;\) are white. Thus, there exists paths of white vertices from \(y\) to all vertices in \(C&amp;rsquo;\). All vertices in \(C&amp;rsquo;\) become descendants of \(y\). Again, \(y.f = f(C&amp;rsquo;)\).&lt;/li&gt;
&lt;li&gt;At time \(y.d\), all vertices in \(C\) are also white.&lt;/li&gt;
&lt;li&gt;Since there is an edge \((u, v)\), where \(u \in C\) and \(u&amp;rsquo; \in C&amp;rsquo;\), we cannot have a path from \(C&amp;rsquo;\) to \(C\).&lt;/li&gt;
&lt;li&gt;No vertex in \(C\) is reachable from \(y\).&lt;/li&gt;
&lt;li&gt;Therefore, at time \(y.f\), all vertices in \(C\) are white.&lt;/li&gt;
&lt;li&gt;Therefore, for all \(w \in C, w.f &amp;gt; y.f\), which implies that \(f( C) &amp;gt; f(C&amp;rsquo;)\). \(\blacksquare\)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;corollary&#34;&gt;Corollary&lt;/h4&gt;
&lt;p&gt;Let \(C\) and \(C&amp;rsquo;\) be distinct strongly connected components in \(G\). Suppose there is an edge \((u, v) \in E^T\), where \(u \in C\) and \(v \in C&amp;rsquo;\). Then \(f( C) &amp;lt; f(C&amp;rsquo;)\).&lt;/p&gt;
&lt;h4 id=&#34;proof&#34;&gt;Proof&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;\((u, v) \in E^T \implies (v, u) \in E\)&lt;/li&gt;
&lt;li&gt;Since strongly connected components of \(G\) and \(G^T\) are the same, \(f(C&amp;rsquo;) &amp;gt; f( C)\).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;correctness&#34;&gt;Correctness&lt;/h3&gt;
&lt;p&gt;Now we can combine the previous results to prove the correctness of the algorithm.&lt;/p&gt;
&lt;h4 id=&#34;corollary&#34;&gt;Corollary&lt;/h4&gt;
&lt;p&gt;Let \(C\) and \(C&amp;rsquo;\) be distinct strongly connected components in \(G\), and suppose that \(f( C) &amp;gt; f(C&amp;rsquo;)\). Then there cannot be an edge from \(C\) to \(C&amp;rsquo;\) in \(G^T\).&lt;/p&gt;
&lt;h4 id=&#34;proof&#34;&gt;Proof&lt;/h4&gt;
&lt;p&gt;When we perform the second DFS call, on \(G^T\), it starts with the component \(C\) such that \(f( C)\) is the maximum. This call starts from some \(x \in C\) and explores all vertices in \(C\). The corollary says that since \(f( C) &amp;gt; f(C&amp;rsquo;)\), there cannot be an edge from \(C\) to \(C&amp;rsquo;\) in \(G^T\). Therefore, DFS will visit &lt;em&gt;only&lt;/em&gt; vertices in \(C\). This means that the depth-first tree rooted at \(x\) will contain only vertices in \(C\).&lt;/p&gt;
&lt;p&gt;The next root chosen is in \(C&amp;rsquo;\) such that \(f(C&amp;rsquo;)\) is maximum over all strongly connected components &lt;strong&gt;other than&lt;/strong&gt; \(C\). DFS visits all vertices in \(C&amp;rsquo;\), but the only edges out of \(C&amp;rsquo;\) go to \(C\), &lt;strong&gt;which have already been visited.&lt;/strong&gt; Therefore, the only tree edges will be to vertices in \(C&amp;rsquo;\).&lt;/p&gt;
&lt;p&gt;As this process continues, we can observe that each root chosen for the second DFS can reach only&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;vertices in its own strongly connected component, and&lt;/li&gt;
&lt;li&gt;vertices in strongly connected components &lt;em&gt;already visited&lt;/em&gt; in the second DFS.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;application-recommender-graphs&#34;&gt;Application: Recommender Graphs&lt;/h2&gt;
&lt;p&gt;A recommender graph is a directed graph where each vertex represents an item and each edge represents a transition between the items based on the context of the data. For example, in a movie recommender graph, each vertex represents a movie, and an edge from \(u\) to \(v\) indicates that users typically transitioned from watching movie \(u\) to watching movie \(v\). The weight of such an edge could be the number of users who made the transition or the average rating improvement when moving from \(u\) to \(v\) (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Lamprecht, Strohmaier, and Helic 2017&lt;/a&gt;).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-07_16-56-29_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;A recommender graph (&amp;lt;a href=&amp;#34;#citeproc_bib_item_2&amp;#34;&amp;gt;Lamprecht, Strohmaier, and Helic 2017&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;A recommender graph (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Lamprecht, Strohmaier, and Helic 2017&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In such a graph, strongly connected components can be used to identify groups of items that are closely related. Based on this information, a recommender system can suggest items that are similar to the ones a user has already interacted with. If the edges contained information such as improvement of ratings, the recommendation system could suggest items that are likely to be enjoyed by the user.&lt;/p&gt;
&lt;p&gt;Identifying such a strongly connected component can also provide insights into the structure of the data. Given the current recommender graph, it is possible that the strongly connected component related to a particular sub-genre of movies is small, leading to a cycle of recommendations within that sub-genre. This discovery would prompt the recommender system to suggest items from other genres to provide a more diverse set of recommendations.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Lamprecht, Daniel, Markus Strohmaier, and Denis Helic. 2017. “A Method for Evaluating Discoverability and Navigability of Recommendation Algorithms.” &lt;i&gt;Computational Social Networks&lt;/i&gt; 4 (1): 9. &lt;a href=&#34;https://doi.org/10.1186/s40649-017-0045-3&#34;&gt;https://doi.org/10.1186/s40649-017-0045-3&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Recursion Tree Method</title>
      <link>https://ajdillhoff.github.io/notes/recursion_tree_method/</link>
      <pubDate>Mon, 18 Mar 2024 22:10:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/recursion_tree_method/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#example-4-dot-13-from-clrs&#34;&gt;Example 4.13 from CLRS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Visualizing the characteristics of an algorithm is a great way to build intuition about its runtime. Although it can be used to prove a recurrence, it is often a good jumping off point for the &lt;a href=&#34;https://ajdillhoff.github.io/notes/substitution_method/&#34;&gt;Substitution Method&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;example-4-dot-13-from-clrs&#34;&gt;Example 4.13 from CLRS&lt;/h2&gt;
&lt;p&gt;Consider the recurrence \(T(n) = 3T(n/4) + \Theta(n^2)\). We start by describing \(\Theta(n^2) = cn^2\), where the constant \(c &amp;gt; 0\) serves as an upper-bound constant. It reflects the amount of work done at each level of the recursion tree. The tree is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-19_09-14-00_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Example 4.13 from CLRS (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Example 4.13 from CLRS (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;As the tree expands out over a few levels, we can see a pattern in the cost at depth \(i\). Each level of increasing depth has 3 times as many nodes as the previous. With the exception of the leaves, the cost for each level is \((\frac{3}{16})^i cn^2\). The total cost of the leaves is based on the number of leaves, which is \(3^{\log_4 n}\) since each level has \(3^i\) nodes and the depth is \(\log_4 n\). Using the identity \(a^{\log_b c} = c^{\log_b a}\), we can simplify the leaves to \(n^{\log_4 3}\). The total cost of the leaves is \(\Theta(n^{\log_4 3})\).&lt;/p&gt;
&lt;p&gt;The last step is to add up the costs over all levels:&lt;/p&gt;
&lt;p&gt;\begin{align*}
T(n) &amp;amp;= \sum_{i=0}^{\log_4 n} \left( \frac{3}{16} \right)^i cn^2 + \Theta(n^{\log_4 3}) \\
&amp;amp;&amp;lt; \sum_{i=0}^{\infty} \left( \frac{3}{16} \right)^i cn^2 + \Theta(n^{\log_4 3}) \\
&amp;amp;= \frac{cn^2}{1 - \frac{3}{16}} + \Theta(n^{\log_4 3}) \\
&amp;amp;= \frac{16}{13}cn^2 + \Theta(n^{\log_4 3}) \\
&amp;amp;= \Theta(n^2).
\end{align*}&lt;/p&gt;
&lt;p&gt;The second line in the equation is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Geometric_series&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;geometric series.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;verifying-using-the-substitution-method&#34;&gt;Verifying using the Substitution Method&lt;/h3&gt;
&lt;p&gt;Even if we weren&amp;rsquo;t so particular with the maths, the recursion tree method is a great way to build intuition about the runtime. Let&amp;rsquo;s verify that this recurrence is bounded above by \(O(n^2)\) using the &lt;a href=&#34;https://ajdillhoff.github.io/notes/substitution_method/&#34;&gt;Substitution Method&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here we show that \(T(n) \leq dn^2\) for a constant \(d &amp;gt; 0\). The previous constant \(c &amp;gt; 0\) is reused to describe the cost at each level of the recursion tree.&lt;/p&gt;
&lt;p&gt;\begin{align*}
T(n) &amp;amp;\leq 3T(n/4) + cn^2 \\
&amp;amp;\leq 3(d(n/4)^2) + cn^2 \\
&amp;amp;= \frac{3}{16}dn^2 + cn^2 \\
&amp;amp;\leq dn^2 \text{ if } d \geq \frac{16}{13}c.
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;exercises&#34;&gt;Exercises&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Solve the recurrence \(T(n) = 2T(n/2) + cn\) using the recursion tree method.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Greedy Algorithms</title>
      <link>https://ajdillhoff.github.io/notes/greedy_algorithms/</link>
      <pubDate>Mon, 18 Mar 2024 14:45:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/greedy_algorithms/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#activity-selection&#34;&gt;Activity Selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#properties-of-greedy-solutions&#34;&gt;Properties of Greedy Solutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#huffman-codes&#34;&gt;Huffman Codes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Greedy algorithms are a class of algorithms that yield &lt;em&gt;locally&lt;/em&gt; optimal solutions. In cases where the local optimum is also the global optimum, greedy algorithms are ideal. Even in cases where the global solution is more elusive, a local solution may be sufficient.&lt;/p&gt;
&lt;h2 id=&#34;activity-selection&#34;&gt;Activity Selection&lt;/h2&gt;
&lt;p&gt;Given a set of activities that need to be scheduled using a common resource, the &lt;strong&gt;activity selection&lt;/strong&gt; problem is to find the maximum number of activities that can be scheduled without overlapping.&lt;/p&gt;
&lt;p&gt;Each activity has a start time \(s_i\) and finish time \(f_i\), where \(0 \leq s_i &amp;lt; f_i &amp;lt; \infty\). An activity \(a_i\) takes place over the interval \([s_i, f_i)\). Two activities \(a_i\) and \(a_j\) are mutually compatible if \(s_i \geq f_j\) or \(s_j \geq f_i\).&lt;/p&gt;
&lt;p&gt;Sort activities by their finish time. &lt;strong&gt;Objective:&lt;/strong&gt; Find the largest subset of mutually compatible activities.&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;\(i\)&lt;/th&gt;
          &lt;th&gt;1&lt;/th&gt;
          &lt;th&gt;2&lt;/th&gt;
          &lt;th&gt;3&lt;/th&gt;
          &lt;th&gt;4&lt;/th&gt;
          &lt;th&gt;5&lt;/th&gt;
          &lt;th&gt;6&lt;/th&gt;
          &lt;th&gt;7&lt;/th&gt;
          &lt;th&gt;8&lt;/th&gt;
          &lt;th&gt;9&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;\(s_i\)&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;4&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;5&lt;/td&gt;
          &lt;td&gt;8&lt;/td&gt;
          &lt;td&gt;9&lt;/td&gt;
          &lt;td&gt;11&lt;/td&gt;
          &lt;td&gt;13&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;\(f_i\)&lt;/td&gt;
          &lt;td&gt;3&lt;/td&gt;
          &lt;td&gt;5&lt;/td&gt;
          &lt;td&gt;7&lt;/td&gt;
          &lt;td&gt;8&lt;/td&gt;
          &lt;td&gt;9&lt;/td&gt;
          &lt;td&gt;10&lt;/td&gt;
          &lt;td&gt;11&lt;/td&gt;
          &lt;td&gt;14&lt;/td&gt;
          &lt;td&gt;16&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-22_17-14-45_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Visualization of activities over time (Cormen et al. 2022).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Visualization of activities over time (Cormen et al. 2022).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;How many mutually compatible sets are there?&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;\(\{a_1, a_3, a_6, a_8\}\)&lt;/li&gt;
&lt;li&gt;\(\{a_1, a_3, a_6, a_9\}\)&lt;/li&gt;
&lt;li&gt;\(\{a_1, a_3, a_7, a_9\}\)&lt;/li&gt;
&lt;li&gt;\(\{a_1, a_5, a_7, a_8\}\)&lt;/li&gt;
&lt;li&gt;\(\{a_1, a_5, a_7, a_9\}\)&lt;/li&gt;
&lt;li&gt;\(\{a_2, a_5, a_7, a_8\}\)&lt;/li&gt;
&lt;li&gt;\(\{a_2, a_5, a_7, a_8\}\)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;optimal-substructure&#34;&gt;Optimal Substructure&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;How do we verify that this problem has optimal substructure?&lt;/strong&gt; First, it is important to formalize the problem based on the definition given previously. Define \(S_{ij}\) as the set of all activities that start after \(a_i\) finishes and finish before \(a_j\) starts.&lt;/p&gt;
&lt;p&gt;\[
S_{ij} = \{a_k \in S : f_i \leq s_k &amp;lt; f_k \leq s_j\}
\]&lt;/p&gt;
&lt;p&gt;This defines a clear subset of the original set of data. That is, we have defined a subproblem of the original problem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Which activities are those in \(S_{ij}\) compatible with?&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;any \(a_i\) that finish by \(f_i\)&lt;/li&gt;
&lt;li&gt;any \(a_i\) that start no earlier than \(s_j\).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Given this subset, our subproblem is that of finding a maximum set of mutually compatible activities in \(S_{ij}\), denoted \(A_{ij}\). If \(a_k \in A_{ij}\), we are left with two subproblems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Find mutually compatible activities in \(S_{ik}\) &amp;ndash; starts after \(a_i\) finishes and finish before \(a_k\) starts.&lt;/li&gt;
&lt;li&gt;Find mutually compatible activities in \(S_{kj}\) &amp;ndash; starts after \(a_k\) finishes and finish before \(a_j\) start.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The two subsets above are defined as \(A_{ik} = A_{ij} \cap S_{ik}\) and \(A_{kj} = A_{ij} \cap S_{kj}\), respectively. Then \(A_{ij} = A_{ik} \cup \{a_k\} \cup A_{kj}\). The size of the set is given by \(|A_{ij}| = |A_{ik}| + 1 + |A_{kj}|\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Claim:&lt;/strong&gt; If our problem has optimal substructure, then the optimal solution \(A_{ij}\) must include optimal solutions for \(S_{ik}\) and \(S_{kj}\).&lt;/p&gt;
&lt;p&gt;This claim can be proven using the &lt;strong&gt;cut-and-paste&lt;/strong&gt; method used in &lt;a href=&#34;https://ajdillhoff.github.io/notes/dynamic_programming/&#34;&gt;Dynamic Programming&lt;/a&gt;. This technique works by showing that if a solution to a problem is not optimal, then there exists a way to &lt;strong&gt;cut&lt;/strong&gt; the suboptimal portion and &lt;strong&gt;paste&lt;/strong&gt; an optimal one. This will lead to a contradiction because the original was assumed to be optimal.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Suppose that \(A_{kj}\) is not optimal, and we could find a set \(A&amp;rsquo;_{kj}\) that is larger. Then we could replace \(A_{kj}\) with \(A&amp;rsquo;_{kj}\) in \(A_{ij}\) to obtain a larger set. This contradicts the assumption that \(A_{ij}\) is optimal.&lt;/p&gt;
&lt;p&gt;Simply put, if the claim is that the given solution is optimal, and the solution is constructed from optimal solutions to subproblems, then there cannot exist any other solution that is better. Another way to look at this: if we construct optimal solutions to subproblems, then the solution to the original problem must be optimal.&lt;/p&gt;
&lt;h3 id=&#34;recursive-solution&#34;&gt;Recursive Solution&lt;/h3&gt;
&lt;p&gt;Let \(c[i, j]\) be the size of the optimal solution for \(S_{ij}\). Based on the above discussion, the size is computed as&lt;/p&gt;
&lt;p&gt;\[
c[i, j] = c[i, k] + c[k, j] + 1.
\]&lt;/p&gt;
&lt;p&gt;This dynamic programming solution assumes we know the optimal solution for all subproblems. To know this, we need to examine all possibilities which include \(a_k\) in the solution.&lt;/p&gt;
&lt;p&gt;\[
c[i, j] = \begin{cases}
0 &amp;amp; \text{if } S_{ij} = \emptyset, \\
\max \{c[i, k] + c[k, j] + 1 : a_k \in S_{ij}\} &amp;amp; \text{if } S_{ij} \neq \emptyset.
\end{cases}
\]&lt;/p&gt;
&lt;h3 id=&#34;greedy-solution&#34;&gt;Greedy Solution&lt;/h3&gt;
&lt;p&gt;The greedy solution is the naive one: select an activity that leaves the resource available for as many other activities as possible, which is the activity that finishes first. If multiple activities finish at the same time, select one arbitrarily.&lt;/p&gt;
&lt;p&gt;The subproblem is that of finding a maximum size set of mutually compatible activities that start after \(a_1\) finishes. More generally, the optimal solution consists of all \(a_i\) that start after \(a_k\) finishes, where \(a_k\) is the last activity to finish:&lt;/p&gt;
&lt;p&gt;\[
S_{k} = \{a_i \in S : s_i \geq f_k\}.
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is the greedy solution optimal?&lt;/strong&gt; Suppose that \(a_m \in S_k\) is the activity that finishes first. Then it must be included in the maximum size subset of mutually compatible activities \(A_k\). Suppose we are given \(A_k\) and we look at \(a_j \in A_k\), the activity that finishes first. If \(a_j = a_m\), then the greedy solution is optimal. If \(a_j \neq a_m\), then we can replace \(a_j\) with \(a_m\) since they are both compatible with all other activities in \(A_k\). Picking the activity that finishes first does not change the size of the optimal solution.&lt;/p&gt;
&lt;p&gt;The solution is top-down:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;pick the solution that finishes first,&lt;/li&gt;
&lt;li&gt;remove all activities that are incompatible with the chosen activity,&lt;/li&gt;
&lt;li&gt;repeat until no activities remain.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;recursive-greedy-algorithm&#34;&gt;Recursive Greedy Algorithm&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;recursive_activity_selector&lt;/span&gt;(s, f, k, n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; m &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; s[m] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; f[k]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        m &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; m &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; n:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; [m] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; recursive_activity_selector(s, f, m, n)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This algorithm assumes that &lt;code&gt;f&lt;/code&gt; is sorted in increasing order. The index &lt;code&gt;k&lt;/code&gt; represents the index of the current subproblem. The number of activities is given by &lt;code&gt;n&lt;/code&gt;. The &lt;code&gt;while&lt;/code&gt; loop increments &lt;code&gt;m&lt;/code&gt; until it finds an activity that starts after activity &lt;code&gt;k&lt;/code&gt; finishes. If such an activity exists, it is added to the solution set and the algorithm is called recursively with the new subproblem.&lt;/p&gt;
&lt;h4 id=&#34;example&#34;&gt;Example&lt;/h4&gt;
&lt;p&gt;A run of this algorithm is visualized below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-24_14-42-29_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Recursive activity selector example (Cormen et al. 2022).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Recursive activity selector example (Cormen et al. 2022).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;analysis&#34;&gt;Analysis&lt;/h4&gt;
&lt;p&gt;At first glance, the algorithm appears to be \(O(n^2)\) because of the &lt;code&gt;while&lt;/code&gt; loop coupled with the recursive call. Once an activity has been selected, the recursive call only considers the activities next \(n - k\) activities. The &lt;code&gt;while&lt;/code&gt; loop picks up where the previous call left off, so the total number of iterations is \(n\). The algorithm is \(O(n)\).&lt;/p&gt;
&lt;h3 id=&#34;iterative-algorithm&#34;&gt;Iterative Algorithm&lt;/h3&gt;
&lt;p&gt;The above solution can be adapted to an iterative one.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;greedy_activity_selector&lt;/span&gt;(s, f):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(s) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; m &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, n &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; s[m] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; f[k]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            A&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(m)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; A
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the &lt;code&gt;for&lt;/code&gt; loop, the first line essentially asks if \(a_m \in S_k\). If so, then add it to the solution set and update \(k\) to \(m\).&lt;/p&gt;
&lt;h4 id=&#34;analysis&#34;&gt;Analysis&lt;/h4&gt;
&lt;p&gt;The analysis of the iterative approach is much clearer. The loop goes over all activities once, so the algorithm is \(O(n)\).&lt;/p&gt;
&lt;h2 id=&#34;properties-of-greedy-solutions&#34;&gt;Properties of Greedy Solutions&lt;/h2&gt;
&lt;p&gt;You can probably imagine a problem for which a greedy solution would not provide the optimal solution. Path planning is one such problem. If we greedily chose the shortest path at each step, we may have missed a shorter path that is not the shortest at each step. The activity selection problem just so happens to be a perfect candidate for a greedy solution, &lt;strong&gt;but what makes it so?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s review the major steps that led us to the greedy solution for activity selection.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Determine the optimal substructure.&lt;/li&gt;
&lt;li&gt;Develop a recursive solution.&lt;/li&gt;
&lt;li&gt;Show that making the greedy choice leaves only a single subproblem.&lt;/li&gt;
&lt;li&gt;Prove that making the greedy choice leads to an optimal solution.&lt;/li&gt;
&lt;li&gt;Develop a recursive algorithm.&lt;/li&gt;
&lt;li&gt;Convert it to an iterative algorithm.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first couple of steps are common to dynamic programming problems. In this case, we could have jumped straight to the greedy approach. Filtering out these extra steps leaves us with:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Cast the optimization problem as one in which we make a choice and are left with a single subproblem.&lt;/li&gt;
&lt;li&gt;Prove that the greedy choice is optimal.&lt;/li&gt;
&lt;li&gt;Demonstrate optimal substructure: if you make a greedy choice, then you are left with a subproblem such that combining an optimal solution with the greedy choice made previously, you end up with an optimal solution to the original problem.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As we will see, we need two properties to prove that a greedy solution is optimal: the &lt;strong&gt;greedy choice property&lt;/strong&gt; and the &lt;strong&gt;optimal substructure property&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;greedy-choice-property&#34;&gt;Greedy Choice Property&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;greedy choice property&lt;/strong&gt; states that the optimal solution can be found by making locally greedy choices. This approach is opposite of dynamic programming, where the choices at each step are made from the knowledge of optimal solutions to subproblems. That is, dynamic programming is a &lt;strong&gt;bottom-up&lt;/strong&gt; approach.&lt;/p&gt;
&lt;p&gt;A greedy solution also makes a choice at each step, but it is only based on local information. This is a &lt;strong&gt;top-down&lt;/strong&gt; approach. They key of this property is to show that the greedy choice is optimal at each step. For the activity selection problem, the steps were&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Examine the optimal solution.&lt;/li&gt;
&lt;li&gt;If it has the greedy choice, then the greedy choice is optimal.&lt;/li&gt;
&lt;li&gt;If it does not have the greedy choice, then replace the suboptimal choice with the greedy choice.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;optimal-substructure-property&#34;&gt;Optimal Substructure Property&lt;/h3&gt;
&lt;p&gt;A problem has optimal substructure if the optimal solution contains optimal solutions to subproblems. We demonstrated this earlier for activity selection. We can start with the assumption that we arrived to a subproblem by making greedy choices. The next step is to show that the optimal solution to the subproblem combined with the greedy choice leads to an optimal solution to the original problem.&lt;/p&gt;
&lt;h3 id=&#34;greedy-vs-dot-dynamic-programming&#34;&gt;Greedy vs. Dynamic Programming&lt;/h3&gt;
&lt;p&gt;Since there is such overlap between greedy algorithms and dynamic programming in terms of their properties, it is important to understand the differences between the two. To illustrate these difference, we will look at two variations of the same problem.&lt;/p&gt;
&lt;h4 id=&#34;0-1-knapsack-problem&#34;&gt;\(0-1\) Knapsack Problem&lt;/h4&gt;
&lt;p&gt;Consider the &lt;strong&gt;\(0-1\) knapsack problem.&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You have \(n\) items.&lt;/li&gt;
&lt;li&gt;Item \(i\) is worth \(v_i\) and weighs \(w_i\).&lt;/li&gt;
&lt;li&gt;Find the most valuable subset of items with total weight less than or equal to \(W\).&lt;/li&gt;
&lt;li&gt;Items cannot be divided.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the most valuable subset of items weighing at most \(W\) includes item \(j\), then the remaining weight must be the most valuable subset of items weighing at most \(W - w_j\) taken from \(n-1\) original items excluding item \(j\).&lt;/p&gt;
&lt;h4 id=&#34;fractional-knapsack-problem&#34;&gt;Fractional Knapsack Problem&lt;/h4&gt;
&lt;p&gt;This is similar to the \(0-1\) knapsack problem, but items can be divided. The objective is to maximize the value of the items in the knapsack.&lt;/p&gt;
&lt;p&gt;The optimal substructure of this problem varies slightly: if the most valuable subset weighing at most \(W\) includes the weight \(w\) of item \(j\), then the remaining weight must be the most valuable subset weighing at most \(W- w\) that can be taken from the \(n-1\) original items plus \(w_j - w\) of item \(j\). There is some fraction of item \(j\) left after taking \(w\) of it.&lt;/p&gt;
&lt;h4 id=&#34;showing-the-greedy-property&#34;&gt;Showing the Greedy Property&lt;/h4&gt;
&lt;p&gt;It is established that both problems have optimal substructure. However, only the fractional knapsack problem has the greedy property. Examine the following code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;fractional_knapsack&lt;/span&gt;(v, w, W):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(v)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    load &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; load &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; W &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; n:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; w[i] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; W &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; load:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            load &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; w[i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            i &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            load &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; (W &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; load) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; v[i] &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; w[i]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If we are able to sort each item by its value-to-weight ratio, then the greedy choice is to take as much as possible from the most valuable item first, the second most valuable item next, and so on. This considers \(n\) items in the worst case, and the items need to be sorted by value-to-weight ratio. The algorithm is \(O(n \log n)\).&lt;/p&gt;
&lt;p&gt;This does not work for the \(0-1\) knapsack problem. Consider the problem visualized below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-24_17-05-36_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Greedy solution to the (0-1) knapsack problem (Cormen et al. 2022).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Greedy solution to the (0-1) knapsack problem (Cormen et al. 2022).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In this problem, we have a knapsack whose total capacity is \(W = 50\). A table of the weights, values, and value-to-weight ratios is given below.&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;\(i\)&lt;/th&gt;
          &lt;th&gt;1&lt;/th&gt;
          &lt;th&gt;2&lt;/th&gt;
          &lt;th&gt;3&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;\(v_i\)&lt;/td&gt;
          &lt;td&gt;60&lt;/td&gt;
          &lt;td&gt;100&lt;/td&gt;
          &lt;td&gt;120&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;\(w_i\)&lt;/td&gt;
          &lt;td&gt;10&lt;/td&gt;
          &lt;td&gt;20&lt;/td&gt;
          &lt;td&gt;30&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;\(v_i/w_i\)&lt;/td&gt;
          &lt;td&gt;6&lt;/td&gt;
          &lt;td&gt;5&lt;/td&gt;
          &lt;td&gt;4&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The fractional algorithm would selection the first item since it has the greatest value-to-weight ratio. The \(0-1\) knapsack problem, however, would select the second and third items to maximize the value of the items in the knapsack.&lt;/p&gt;
&lt;h2 id=&#34;huffman-codes&#34;&gt;Huffman Codes&lt;/h2&gt;
&lt;p&gt;Huffman coding is a lossless data compression algorithm that assigns variable-length codes to input characters, with lengths based on the frequencies of occurrence for those characters. Originally developed by David A. Huffman in 1952 during his Ph.D. at MIT, he published the algorithm under the title &amp;ldquo;&lt;a href=&#34;https://ieeexplore.ieee.org/document/4051119&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;A Method for the Construction of Minimum-Redundancy Codes&lt;/a&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Huffman coding involves:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Building a Huffman tree from the input characters and their frequencies.&lt;/li&gt;
&lt;li&gt;Traversing the tree to assign codes to each character.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Before getting into the specifics of the algorithm, let&amp;rsquo;s look at an example. Suppose we have a document consisting of 6 unique characters, each represented by a byte (8 bits). We could represent these 6 characters using 3 bits, since that is the minimum number of bits needed to represent 6 unique values. This is known as a &lt;strong&gt;fixed-length code&lt;/strong&gt;. If we instead assigned a &lt;strong&gt;variable-length code&lt;/strong&gt; to each character based on its frequency of occurrence, we would further reduce the footprint of the file size.&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;A&lt;/th&gt;
          &lt;th&gt;B&lt;/th&gt;
          &lt;th&gt;C&lt;/th&gt;
          &lt;th&gt;D&lt;/th&gt;
          &lt;th&gt;E&lt;/th&gt;
          &lt;th&gt;F&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Frequency (in thousands)&lt;/td&gt;
          &lt;td&gt;45&lt;/td&gt;
          &lt;td&gt;13&lt;/td&gt;
          &lt;td&gt;12&lt;/td&gt;
          &lt;td&gt;16&lt;/td&gt;
          &lt;td&gt;9&lt;/td&gt;
          &lt;td&gt;5&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Fixed-length code&lt;/td&gt;
          &lt;td&gt;000&lt;/td&gt;
          &lt;td&gt;001&lt;/td&gt;
          &lt;td&gt;010&lt;/td&gt;
          &lt;td&gt;011&lt;/td&gt;
          &lt;td&gt;100&lt;/td&gt;
          &lt;td&gt;101&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Variable-length code&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;101&lt;/td&gt;
          &lt;td&gt;100&lt;/td&gt;
          &lt;td&gt;111&lt;/td&gt;
          &lt;td&gt;1101&lt;/td&gt;
          &lt;td&gt;1100&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Does this seem like the best coding?&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Why isn&amp;rsquo;t a code of 1 used?&lt;/li&gt;
&lt;li&gt;What about 2 bit encoding?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The truth is that it depends on the document being compressed. The encoding is optimal consider the overall but length of the encoded file. Based on the above &lt;strong&gt;fixed-length code&lt;/strong&gt;, the file size is 300,000 bits. The &lt;strong&gt;variable-length code&lt;/strong&gt; reduces the file size to 224,000 bits.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How many bits are needed to encode \(n \geq 2\) characters?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\[
\lceil \lg n \rceil
\]&lt;/p&gt;
&lt;h3 id=&#34;prefix-free-codes&#34;&gt;Prefix-free Codes&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;prefix-free code&lt;/strong&gt; is a code in which no codeword is also a prefix of another codeword. This property simplifies decoding since the code can be read from left to right without ambiguity. The codeword &amp;ldquo;beef&amp;rdquo; has the encoding \(101110111011100 = 101 \cdot 1101 \cdot 1101 \cdot 1100\), where \(\cdot\) denotes concatenation.&lt;/p&gt;
&lt;p&gt;This definition may start to clarify why a code of 1 is not used. If a code of 1 were used, then the code would be a prefix of all other codes. This would make decoding ambiguous.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How are the codes decoded?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One solution to this is to keep a table of all codewords and their corresponding characters. A more compact solution is to use a binary tree. Starting with the first bit in the encoded message, traverse the tree until a leaf node is reached. The character at the leaf node is the decoded character. The tree is known as a &lt;strong&gt;Huffman tree&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;full binary tree&lt;/strong&gt;, where each nonleaf node has two subnodes, is optimal for decoding. If the tree has this property then an optimal prefix-free code has \(|C|\) leaves and exactly \(|C| - 1\) internal nodes. The tree for the variable-length code is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-26_09-07-14_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Huffman tree for the variable-length code (Cormen et al. 2022).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Huffman tree for the variable-length code (Cormen et al. 2022).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Given a character \(c\) with frequency \(c.freq\), let \(d_T( c)\) denote the depth of character \(c\) in tree \(T\). The cost of the code in bits is given by&lt;/p&gt;
&lt;p&gt;\[
B(T) = \sum_{c \in C} c.freq \cdot d_T( c).
\]&lt;/p&gt;
&lt;p&gt;The depth of the character \(d_T( c)\) is used since it also denotes the length of the codeword.&lt;/p&gt;
&lt;h3 id=&#34;constructing-a-huffman-code&#34;&gt;Constructing a Huffman Code&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Node&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, freq, char&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;freq &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; freq
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;char &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; char
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;huffman&lt;/span&gt;(C):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(C)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; build_min_heap(C)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        z &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Node(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; extract_min(Q)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; extract_min(Q)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;freq &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;freq &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;freq
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        insert(Q, z)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; extract_min(Q)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The function above builds a Huffman tree from a set of characters \(C\). At each iteration, the two nodes with the smallest frequencies are extracted from the queue \(Q\) and are used to create a new node \(z\). This node represents the sum of the frequencies of the two nodes. The node is then inserted back into the queue so that it can be used in future iterations. The result is a Huffman tree.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-01_20-48-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Huffman tree for the data in the table above (Cormen et al. 2022).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Huffman tree for the data in the table above (Cormen et al. 2022).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;analysis&#34;&gt;Analysis&lt;/h4&gt;
&lt;p&gt;If the priority queue is implemented as a binary min-heap, the call to &lt;code&gt;build_min_heap&lt;/code&gt; initializes the priority queue in \(O(n)\) time. The &lt;code&gt;for&lt;/code&gt; loop runs \(n-1\) times, calling &lt;code&gt;extract_min&lt;/code&gt; twice and &lt;code&gt;insert&lt;/code&gt; once. Each call to &lt;code&gt;extract_min&lt;/code&gt; takes \(O(\lg n)\) time yielding a total of \(O(n \lg n)\) time.&lt;/p&gt;
&lt;h3 id=&#34;correctness-of-huffman-codes&#34;&gt;Correctness of Huffman Codes&lt;/h3&gt;
&lt;p&gt;The correctness of an algorithm means that it produces the expected output based on the input and its properties. To show that the Huffman algorithm is correct, we can show that it exhibits the greedy choice and optimal substructure properties.&lt;/p&gt;
&lt;h4 id=&#34;lemma-15-dot-2-optimal-prefix-free-codes-have-the-greedy-choice-property&#34;&gt;Lemma 15.2: Optimal prefix-free codes have the greedy-choice property&lt;/h4&gt;
&lt;p&gt;For alphabet \(C\), let \(x\) and \(y\) be the two characters with the lowest frequencies. Then there exists an optimal prefix-free code for \(C\) where the codewords for \(x\) and \(y\) have the same length and differ only in the last bit.&lt;/p&gt;
&lt;p&gt;This establishes the greedy choice property because the algorithm selects the two characters with the lowest frequencies at each step.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the given optimal tree \(T\), leaves \(a\) and \(b\) are two siblings with maximum depth. It&amp;rsquo;s also given that \(x\) and \(y\) are the two characters with the lowest frequencies, but they appear in arbitrary positions.&lt;/p&gt;
&lt;p&gt;Assume that \(x \neq b\). Swapping \(a\) and \(x\) produces tree \(T&amp;rsquo;\) does not increase the cost. Swapping \(b\) and \(y\) produces tree \(T&amp;rsquo;&amp;rsquo;\) that also does not increase the cost. &lt;strong&gt;This is the key argument: if swapping the lowest frequency characters with the deepest characters does not increase the cost, then the greedy choice is optimal.&lt;/strong&gt;&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-02_10-40-38_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Creating (T&amp;#39;) and (T&amp;#39;&amp;#39;) from (T) (Cormen et al. 2022).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Creating (T&amp;rsquo;) and (T&amp;rsquo;&amp;rsquo;) from (T) (Cormen et al. 2022).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Next, we need to show that exchanging \(a\) and \(x\) does not increase the cost. The cost of the tree is given by&lt;/p&gt;
&lt;p&gt;\[
B(T) = \sum_{c \in C} c.freq \cdot d_T( c).
\]&lt;/p&gt;
&lt;p&gt;\begin{align*}
B(T) - B(T&amp;rsquo;) &amp;amp;= \sum_{c \in C} c.freq \cdot d_{T}( c) - \sum_{c \in C} c.freq \cdot d_{T&amp;rsquo;}( c) \\
&amp;amp;= x.freq \cdot d_T(x) + a.freq \cdot d_T(a) - x.freq \cdot d_{T&amp;rsquo;}(x) - a.freq \cdot d_{T&amp;rsquo;}(a) \\
&amp;amp;= x.freq \cdot d_T(x) + a.freq \cdot d_T(a) - x.freq \cdot d_{T}(a) - a.freq \cdot d_{T}(x) \\
&amp;amp;= (x.freq - a.freq)(d_T(x) - d_T(a)) \\
&amp;amp;\geq 0.
\end{align*}&lt;/p&gt;
&lt;p&gt;The last line is true because \(x.freq \leq a.freq\) and \(d_T(x) \geq d_T(a)\). A similar argument can be made for \(T&amp;rsquo;&amp;rsquo;\). \(B(T&amp;rsquo;&amp;rsquo;) \leq B(T&amp;rsquo;)\) since exchanging \(y\) and \(b\) does not increase the cost. This means that \(B(T&amp;rsquo;&amp;rsquo;) \leq B(T&amp;rsquo;) \leq B(T)\). \(T\) is an optimal tree, so \(B(T) \leq B(T&amp;rsquo;&amp;rsquo;) \implies B(T) = B(T&amp;rsquo;&amp;rsquo;) \implies T\) is optimal where \(x\) and \(y\) are siblings of maximum depth.&lt;/p&gt;
&lt;h4 id=&#34;lemma-15-dot-3-optimal-substructure-property&#34;&gt;Lemma 15.3: Optimal-substructure property&lt;/h4&gt;
&lt;p&gt;Let \(x\) and \(y\) be two characters with minimum frequency in alphabet \(C\) and let \(C&amp;rsquo; = (C - \{x, y\}) \cup z\) for a new character \(z\) with \(z.freq = x.freq + y.freq\). Additionally, let \(T&amp;rsquo;\) be a tree representing an optimal prefix-free code for \(C&amp;rsquo;\), and \(T\) be \(T&amp;rsquo;\) with the leaf for \(z\) replaced by an internal node with children \(x\) and \(y\). Then \(T\) represents an optimal prefix-free code for \(C\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Simplified:&lt;/strong&gt; If the algorithm computed the optimal solution to the simplified problem, where \(z\) replaced \(x\) and \(y\), it can extend this to an optimal solution to the original problem by putting \(x\) and \(y\) back.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first part of the proof establishes the costs of the relevant trees.&lt;/p&gt;
&lt;p&gt;\(c \in C - \{x, y\} \implies d_T( c) = d_{T&amp;rsquo;}( c) \implies c.freq \cdot d_T( c) = c.freq \cdot d_{T&amp;rsquo;}( c)\)&lt;/p&gt;
&lt;p&gt;The depth of \(x\) and \(y\) are equal to the depth of \(z\) in \(T&amp;rsquo;\) + 1:&lt;/p&gt;
&lt;p&gt;\begin{align*}
d_T(x) = d_T(y) = d_{T&amp;rsquo;}(z) + 1 &amp;amp;\implies x.freq \cdot d_T(x) + y.freq \cdot d_T(y)\\
&amp;amp;= (x.freq + y.freq)(d_{T&amp;rsquo;}(z) + 1)\\
&amp;amp;= z.freq \cdot d_{T&amp;rsquo;}(z) + (x.freq + y.freq).
\end{align*}&lt;/p&gt;
&lt;p&gt;This means that \(B(T) = B(T&amp;rsquo;) + x.freq + y.freq\), which is equivalent to \(B(T&amp;rsquo;) = B(T) - x.freq - y.freq\).&lt;/p&gt;
&lt;p&gt;The second part of this proof supposes that \(T\) is not an optimal prefix code for \(C\) and ends in a contradiction, thus proving the original lemma. If \(T\) is not optimal for \(C\), then \(B(T&amp;rsquo;&amp;rsquo;) &amp;lt; B(T)\) for some optimal tree \(T&amp;rsquo;&amp;rsquo;\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Here, \(T&amp;rsquo;&amp;rsquo;\) is introduced as the supposed optimal tree for \(C\) if it turns out that \(T\) is not.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If \(T&amp;rsquo;&amp;rsquo;\) is optimal, then lemma 15.2 (greedy property) from above implies that it has \(x\) and \(y\) as siblings. After all, \(\{x, y\} \in C\) and \(T&amp;rsquo;&amp;rsquo;\) is an optimal tree for \(C\). Create a tree \(T&amp;rsquo;&amp;rsquo;&amp;rsquo;\) by replacing the parent of \(x\) and \(y\) with a leaf (implying we remove \(x\) and \(y\)) \(z\) with \(z.freq = x.freq + y.freq\). Then,&lt;/p&gt;
&lt;p&gt;\begin{align*}
B(T&amp;rsquo;&amp;rsquo;&amp;rsquo;) &amp;amp;= B(T&amp;rsquo;&amp;rsquo;) - x.freq - y.freq\\
&amp;amp;&amp;lt; B(T) - x.freq - y.freq\\
&amp;amp;= B(T&amp;rsquo;).
\end{align*}&lt;/p&gt;
&lt;p&gt;\(B(T&amp;rsquo;&amp;rsquo;&amp;rsquo;) &amp;lt; B(T&amp;rsquo;)\) is a contradiction because it was previously established that \(T&amp;rsquo;\) is an optimal tree for \(C&amp;rsquo;\). Therefore a suboptimal \(T\) is impossible if \(T&amp;rsquo;\) is optimal.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hash Tables</title>
      <link>https://ajdillhoff.github.io/notes/hash_tables/</link>
      <pubDate>Thu, 14 Mar 2024 15:16:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/hash_tables/</guid>
      <description>&lt;p&gt;See &lt;a href=&#34;https://ajdillhoff.github.io/teaching/dasc5300/lectures/hash_maps.pdf&#34;&gt;these slides&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic Programming</title>
      <link>https://ajdillhoff.github.io/notes/dynamic_programming/</link>
      <pubDate>Thu, 14 Mar 2024 10:40:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/dynamic_programming/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#rod-cutting&#34;&gt;Rod Cutting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix-chain-multiplication&#34;&gt;Matrix-chain Multiplication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#applying-dynamic-programming&#34;&gt;Applying Dynamic Programming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#longest-common-subsequence&#34;&gt;Longest Common Subsequence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exercises&#34;&gt;Exercises&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Dynamic programming is a technique for solving problems by breaking them down into simpler subproblems, very much like divide and conquer algorithms. One primary difference is that the subproblems are designed in such a way that they do not need to be recomputed.&lt;/p&gt;
&lt;p&gt;Many common problems have efficience dynamic programming solutions, and we will investigate several of them in these notes. In general, a dynamic programming solution can be applied if the problem has the following features.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Optimal substructure:&lt;/strong&gt; An optimal solution can be constructed by optimal solutions to the subproblems.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overlapping subproblems:&lt;/strong&gt; The problem can be broken down into subproblems which can be reused.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, the Fibonacci sequence has &lt;strong&gt;optimal substructure&lt;/strong&gt; because the value of the sequence at any index is the sum of the values at the two previous indices. It also has &lt;strong&gt;overlapping subproblems&lt;/strong&gt; because the value of the sequence at any index is used in the calculation of the values at the two subsequent indices. A recursive solution to the Fibonacci sequence will have exponential time complexity, but a dynamic programming solution will have linear time complexity.&lt;/p&gt;
&lt;p&gt;The two main approaches to dynamic programming are top-down (memoization) and bottom-up (tabulation). &lt;strong&gt;Memoization&lt;/strong&gt; involves writing a recursive solution that stores each sub=solution in a table so that it can be reused. &lt;strong&gt;Tabulation&lt;/strong&gt; involves solving the problem by filling in a table of subproblems from the bottom up. In either case, a dynamic programming solution can be formulated with the following steps.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Identify subproblems&lt;/strong&gt; so that the problem can be broken down.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Solve the subproblems&lt;/strong&gt; following an optimal solution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Store the solutions&lt;/strong&gt; to avoid redundant computation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Combine solutions&lt;/strong&gt; from the subproblems to solve the original problem.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;rod-cutting&#34;&gt;Rod Cutting&lt;/h2&gt;
&lt;p&gt;Given a rod of length \(n\) and table of prices \(p_i\) for \(i = 1, 2, \ldots, n\), determine the maximum revenue \(r_n\) that can be obtained by cutting up the rod and selling the pieces.&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Length&lt;/th&gt;
          &lt;th&gt;1&lt;/th&gt;
          &lt;th&gt;2&lt;/th&gt;
          &lt;th&gt;3&lt;/th&gt;
          &lt;th&gt;4&lt;/th&gt;
          &lt;th&gt;5&lt;/th&gt;
          &lt;th&gt;6&lt;/th&gt;
          &lt;th&gt;7&lt;/th&gt;
          &lt;th&gt;8&lt;/th&gt;
          &lt;th&gt;9&lt;/th&gt;
          &lt;th&gt;10&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Price&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;5&lt;/td&gt;
          &lt;td&gt;8&lt;/td&gt;
          &lt;td&gt;9&lt;/td&gt;
          &lt;td&gt;10&lt;/td&gt;
          &lt;td&gt;17&lt;/td&gt;
          &lt;td&gt;17&lt;/td&gt;
          &lt;td&gt;20&lt;/td&gt;
          &lt;td&gt;24&lt;/td&gt;
          &lt;td&gt;30&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The table of prices is shown above. For a rod of length 4, there are 8 (\(2^{n-1}\), where \(n=4\)) different ways to cut the rod.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-14_11-47-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;8 different ways to cut a rod of length 4 (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;8 different ways to cut a rod of length 4 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The maximum revenue for a rod of length \(n\) can be determined by the following optimization problem:&lt;/p&gt;
&lt;p&gt;\[
r_n = \max(p_n, r_1 + r_{n-1}, r_2 + r_{n-2}, \ldots, r_{n-1} + r_1),
\]&lt;/p&gt;
&lt;p&gt;where \(r_i\) is the maximum revenue for a rod of length \(i\). The maximum revenue for a rod of length \(n\) can be determined by solving the subproblems for rods of length \(i\) for \(i = 1, 2, \ldots, n-1\). Each of the terms \(r_i\) in the equation above implies a recursive solution to the problem. You should be able to see that solving this recursively would lead to many redundant computations. For example, \(r_1\) is computed at least twice in the equation above.&lt;/p&gt;
&lt;p&gt;This recursion is more compactly written as&lt;/p&gt;
&lt;p&gt;\[
r_n = \max_{1 \leq i \leq n}(p_i + r_{n-i}).
\]&lt;/p&gt;
&lt;p&gt;This problem has &lt;strong&gt;optimal substructure&lt;/strong&gt;. If we cut the rod into smaller subsections, we can recursively solve the subproblems and combine them. The recursive algorithm is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cut_rod&lt;/span&gt;(p, n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;inf&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(q, p[i] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; cut_rod(p, n&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;i))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; q
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If \(T(n)\) is a recurrence that represents the number of times &lt;code&gt;cur_rod&lt;/code&gt; is called recursively, then we can write the following recurrence relation:&lt;/p&gt;
&lt;p&gt;\[
T(n) = 1 + \sum_{j=0}^{n-1}T(j).
\]&lt;/p&gt;
&lt;p&gt;Using the substitution method, we can show that \(T(n) = 2^n\).&lt;/p&gt;
&lt;p&gt;\begin{align*}
T(n) &amp;amp;= 1 + \sum_{j=0}^{n-1}2^j \\
&amp;amp;= 1 + (2^n - 1) \\
&amp;amp;= 2^n.
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;memoization-solution&#34;&gt;Memoization Solution&lt;/h3&gt;
&lt;p&gt;To solve this with dynamic programming, the goal is to make sure that each subproblem is computed &lt;em&gt;only once&lt;/em&gt;. This is accomplished by saving the result of each subproblem in a table so that it can be reused. This does incur a space complexity of \(O(n)\), but it reduces the time complexity to \(O(n^2)\).&lt;/p&gt;
&lt;p&gt;The solution requires a small modification to the recursive algorithm. When the solution to a subproblem is required, the table is first checked for a stored solution. If the solution is not found, the subproblem is solved recursively and the solution is stored in the table. The code is given below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;memoized_cut_rod&lt;/span&gt;(p, n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    r &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;inf&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; memoized_cut_rod_aux(p, n, r)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;memoized_cut_rod_aux&lt;/span&gt;(p, n, r):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; r[n] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; r[n]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;inf&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(q, p[i] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; memoized_cut_rod_aux(p, n&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;i, r))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    r[n] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; q
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; q
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The algorithm starts off with a call to &lt;code&gt;memoized_cut_rod&lt;/code&gt; which initializes the table &lt;code&gt;r&lt;/code&gt; and then calls &lt;code&gt;memoized_cut_rod_aux&lt;/code&gt;. The table &lt;code&gt;r&lt;/code&gt; is initialized with \(-\infty\) so that we can check if a solution has been computed for a subproblem. Each subproblem is solved only once, leading to \(O(1)\) lookups after that. The time complexity of this solution is \(O(n^2)\).&lt;/p&gt;
&lt;h3 id=&#34;bottom-up-solution&#34;&gt;Bottom-Up Solution&lt;/h3&gt;
&lt;p&gt;The other dynamic programming solution is to first sort the subproblems by their size, solve the smaller ones first, and build up to the larger ones. This is called &lt;strong&gt;tabulation&lt;/strong&gt;. The time complexity of this solution is also \(O(n^2)\).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bottom_up_cut_rod&lt;/span&gt;(p, n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    r &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;inf&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, j&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(q, p[i] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; r[j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;i])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        r[j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; q
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; r[n]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first &lt;code&gt;for&lt;/code&gt; loop effectively sorts the problem by size. It starts with a cut of size 1 and builds up to a cut of size \(n\).&lt;/p&gt;
&lt;h3 id=&#34;subproblem-graphs&#34;&gt;Subproblem Graphs&lt;/h3&gt;
&lt;p&gt;Subproblem graphs offer a concise way to visualize the subproblems and their dependencies. The subproblem graph for the rod cutting problem with \(n=4\) is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-14_14-54-03_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Subproblem graph for the rod cutting problem with (n=4) (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Subproblem graph for the rod cutting problem with (n=4) (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Subproblem \(n=4\) is dependent on subproblems \(n=3\), \(n=2\), and \(n=1\). The bottom-up approach follows this dependency by ensuring that the subproblems are solved in the correct order.&lt;/p&gt;
&lt;p&gt;Besides serving as a helpful visualization, depicting the problem using a DAG can also help to identify the time complexity of the problem. This is the sum of the time needed to solve each subproblem. Each problem of size \(n\) requires \(n-1\) subproblems to be solved, and each subproblem of size \(n-1\) requires \(n-2\) subproblems to be solved. This leads to a time complexity of \(O(n^2)\).&lt;/p&gt;
&lt;h3 id=&#34;reconstructing-a-solution&#34;&gt;Reconstructing a Solution&lt;/h3&gt;
&lt;p&gt;The two dynamic programming solutions above return the maximum revenue that can be obtained by cutting up the rod. However, they do not return the actual cuts that should be made. This can be done by modifying the algorithms to store the cuts that are made. The code is given below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;extended_bottom_up_cut_rod&lt;/span&gt;(p, n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    r &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;inf&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, j&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; q &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; p[i] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; r[j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;i]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; p[i] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; r[j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                s[j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        r[j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; q
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; r, s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;print_cut_rod_solution&lt;/span&gt;(p, n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    r, s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; extended_bottom_up_cut_rod(p, n)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(s[n])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        n &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; s[n]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the bottom-up approach, the table &lt;code&gt;s&lt;/code&gt; is used to store the size of the first piece to cut off. The function &lt;code&gt;print_cut_rod_solution&lt;/code&gt; uses this table to print the cuts that should be made.&lt;/p&gt;
&lt;h2 id=&#34;matrix-chain-multiplication&#34;&gt;Matrix-chain Multiplication&lt;/h2&gt;
&lt;p&gt;The next problem covered by Cormen et al. is &lt;strong&gt;matrix-chain multiplication&lt;/strong&gt; (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;). Given a sequence of matrices \(A_1, A_2, \ldots, A_n\), where the dimensions of matrix \(A_i\) are \(p_{i-1} \times p_i\), determine the most efficient way to multiply the matrices. The problem is to determine the order in which the matrices should be multiplied so that the number of scalar multiplications is minimized.&lt;/p&gt;
&lt;p&gt;Understanding the solution to this problem requires understanding the problem itself. Depending on the order in which matrices are multiplied in a chain, the number of scalar multiplications can vary. Consider three matrices \(A \in \mathbb{R}^{10 \times 100}\), \(B \in \mathbb{R}^{100 \times 5}\), and \(C \in \mathbb{R}^{5 \times 50}\). The number of scalar multiplications required to compute \((AB)C\) is \(10 \times 100 \times 5 + 10 \times 5 \times 50 = 7500\), while the number of scalar multiplications required to compute \(A(BC)\) is \(10 \times 100 \times 50 + 100 \times 5 \times 50 = 75000\). The order in which the matrices are multiplied can have a significant impact on the number of scalar multiplications required.&lt;/p&gt;
&lt;p&gt;Matrix multiplication is associative, so the order in which the matrices are grouped does not matter. The key to solving this problem is to find the most efficient way to group the matrices. The first part of the solution is to determine the number of possible groupings, or parenthesizations, we can make.&lt;/p&gt;
&lt;h3 id=&#34;determining-parenthesizations&#34;&gt;Determining Parenthesizations&lt;/h3&gt;
&lt;p&gt;The number of possible parenthesizations of a chain of \(n\) matrices is given by \(P(n)\). When \(n \geq 2\), the number of possible parenthesizations is given by&lt;/p&gt;
&lt;p&gt;\[
P(n) = \sum_{k=1}^{n-1}P(k)P(n-k).
\]&lt;/p&gt;
&lt;p&gt;A brute force solution to this problem would require \(O(2^n)\) time (see Exercise 14.2-3 in (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;)).&lt;/p&gt;
&lt;h3 id=&#34;dynamic-programming-solution&#34;&gt;Dynamic Programming Solution&lt;/h3&gt;
&lt;p&gt;We now review the four step process to formulating a dynamic programming solution as put forth by Cormen et al. (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/p&gt;
&lt;h4 id=&#34;optimal-substructure&#34;&gt;Optimal Substructure&lt;/h4&gt;
&lt;p&gt;What is the optimal substructure of this problem? Consider matrix-chain sequence \(A_{i:j} = A_i A_{i+1} \cdots A_j\). If we split the sequence at \(k\), then the optimal solution to the problem is the optimal solution to the subproblems \(A_{i:k}\) and \(A_{k+1:j}\). This is because the number of scalar multiplications required to compute \(A_{i:j}\) is the sum of the number of scalar multiplications required to compute \(A_{i:k}\) and \(A_{k+1:j}\) plus the number of scalar multiplications required to compute the product of the two subproblems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How can we ensure that there is not a more optimal grouping of \(A_{h:l}\), where \(i \leq h &amp;lt; k\) and \(k &amp;lt; l \leq j\)?&lt;/strong&gt; The answer lies in evaluating &lt;strong&gt;all&lt;/strong&gt; possible splits.&lt;/p&gt;
&lt;h4 id=&#34;recursive-solution&#34;&gt;Recursive Solution&lt;/h4&gt;
&lt;p&gt;What is the cost of an optimal solution to the problem? We must first compute the minimum cost of parenthesizing \(A_{i:j}\) for \(1 \leq i \leq j \leq n\). Let \(m[i,j]\) be the minimum number of scalar multiplications needed to compute \(A_{i:j}\). Starting with the base case, \(m[i,i]\) is the cost to compute the multiplication of a single matrix, which is 0. Assuming optimal subproblems are chosen, \(m[i,j] = m[i,k] + m[k+1,j] + p_{i-1}p_kp_j\), where the last term is the cost of multiplying \(A_{i:k}A_{k+1:j}\).&lt;/p&gt;
&lt;p&gt;All possible splits must be evaluated. So, how many are there? Omitting the first and last matrices, there are \(j - i\) possible splits. We can now define the optimal solution in terms of the following recursion:&lt;/p&gt;
&lt;p&gt;\[
m[i,j] = \min \{m[i,k] + m[k+1,j] + p_{i-1}p_kp_j : i \leq k &amp;lt; j\}.
\]&lt;/p&gt;
&lt;h4 id=&#34;storing-the-solutions&#34;&gt;Storing the Solutions&lt;/h4&gt;
&lt;p&gt;The problem is taking shape and we have a recursive formula. However, this is no better than the brute-force method until we figure out how to select the optimal subproblems and store their solutions. That is, we need to optimally select \(k\). A bottom-up approach involves computing the cost of all possible combinations of the \(n\) matrices and building up from there. This requires \(O(n^2)\) memory to store both the costs \(m[i, j]\) as well as the value of \(k\) that splits them \(s[i, j]\).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;matrix_chain_order&lt;/span&gt;(p):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(p) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; l &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, n&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;l&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; l &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            m[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;inf&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(i, j):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; m[i][k] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; m[k&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][j] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; p[i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;p[k]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;p[j]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; q &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; m[i][j]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    m[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; q
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    s[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; k
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; m, s
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The function &lt;code&gt;matrix_chain_order&lt;/code&gt; computes the cost of all possible combinations of the \(n\) matrices and stores the value of \(k\) that splits them. The outer-most &lt;code&gt;for&lt;/code&gt; loop controls the length of the chain being evaluated. We start at 2 since the cost of a length 1 chain is 0. Intuition tells us that the triply-nested &lt;code&gt;for&lt;/code&gt; loop has a time complexity of \(O(n^3)\).&lt;/p&gt;
&lt;p&gt;This algorithm computes the cost in ascending order of chain length. When \(l=2\), the cost of all chains of length 2 is computed. When \(l=3\), the cost of all chains of length 3 is computed, and so on. The recursion in the inner-most nested loop will only ever access the entries in &lt;code&gt;m&lt;/code&gt; which have been previously computed.&lt;/p&gt;
&lt;h4 id=&#34;reconstructing-a-solution&#34;&gt;Reconstructing a Solution&lt;/h4&gt;
&lt;p&gt;We now have a solution which generates the optimal number of scalar multiplications needed for all possible combinations of the \(n\) matrices. However, we do not yet have a solution which tells us the order in which the matrices should be multiplied. This information is held in &lt;code&gt;s&lt;/code&gt;, which records the value of \(k\) that splits the chain. The function &lt;code&gt;print_optimal_parens&lt;/code&gt; is given below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;print_optimal_parens&lt;/span&gt;(s, i, j):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; j:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;A_&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;i&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;, end&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;(&amp;#34;&lt;/span&gt;, end&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print_optimal_parens(s, i, s[i][j])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print_optimal_parens(s, s[i][j]&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, j)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;)&amp;#34;&lt;/span&gt;, end&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Using figure 14.5 from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;) as a reference, we can test the function &lt;code&gt;print_optimal_parens&lt;/code&gt;. It is first called as &lt;code&gt;print_optimal_parens(s, 1, 6)&lt;/code&gt;. This recursively calls &lt;code&gt;print_optimal_parens(s, 1, 3)&lt;/code&gt; and &lt;code&gt;print_optimal_parens(s, 4, 6)&lt;/code&gt;. We will work from left to right, top to bottom and fill out the values.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Second call: &lt;code&gt;print_optimal_parens(s, 1, 3)&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This recursively calls &lt;code&gt;print_optimal_parens(s, 1, 1)&lt;/code&gt; and &lt;code&gt;print_optimal_parens(s, 2, 3)&lt;/code&gt;. We can see that this first call has \(i==j\), so it prints \(A_1\). The second call prints \((A_2A_3)\). This initial call already set up the first set of parenthesis, so the intermediate result is \(((A_1(A_2A_3))\cdots)\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Third call: &lt;code&gt;print_optimal_parens(s, 4, 6)&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This recursively calls &lt;code&gt;print_optimal_parens(s, 4, 5)&lt;/code&gt; and &lt;code&gt;print_optimal_parens(s, 6, 6)&lt;/code&gt;. This first call will recursively call &lt;code&gt;print_optimal_parens(s, 4, 4)&lt;/code&gt; and &lt;code&gt;print_optimal_parens(s, 5, 5)&lt;/code&gt;. This produces \((A_4A_5)\) from the first subcall and \(A_6\) from the second subcall. The intermediate result is now \((\cdots((A_4A_5)A_6))\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Putting it all together&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Combining these results yields \(((A_1(A_2A_3))((A_4A_5)A_6))\). This is the optimal parenthesization of the matrix chain \(A_1A_2A_3A_4A_5A_6\).&lt;/p&gt;
&lt;h2 id=&#34;applying-dynamic-programming&#34;&gt;Applying Dynamic Programming&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Section 14.3 of (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;) focuses on the two core components of a dynamic programming solution: &lt;strong&gt;optimal substructure&lt;/strong&gt; and &lt;strong&gt;overlapping subproblems&lt;/strong&gt;. By taking a closer look at how these two components were used in various dynamic programming solutions, you should have a greater understanding of how to apply dynamic programming to new problems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As shown in previous examples, determining the &lt;strong&gt;optimal substructure&lt;/strong&gt; is the first step in formulating a dynamic programming solution. In most cases, this comes from understanding the problem itself well. It is the result of a natural way of analysis and decomposition of the problem. When learning a new concerto, a musician must have a strong command of technique and fundamental musical concepts. Similarly, a strong understanding of the problem is required to determine the optimal substructure.&lt;/p&gt;
&lt;h3 id=&#34;determining-optimal-substructure&#34;&gt;Determining Optimal Substructure&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Show that a solution to a problem requires making a choice, like where to cut in the rod cutting problem.&lt;/li&gt;
&lt;li&gt;Assume that you are given an optimal choice.&lt;/li&gt;
&lt;li&gt;Identify the subproblems that result from this choice.&lt;/li&gt;
&lt;li&gt;Show that solutions to these subproblems are optimal.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the last step, we are typically looking for a contradiction. The assumption of step 2 means that if we end up finding a more optimal solution to a subproblem, then the original choice was not optimal. The result is that we have a better overall solution.&lt;/p&gt;
&lt;p&gt;The efficiency of a dynamic programming solution depends on the number of subproblems times the number of choices we have for each subproblem. When investigating solutions to a new problem, it is better to start with a simple case and expand outward as necessary. Using a subproblem graph is a great way to visualize the subproblems and their dependencies.&lt;/p&gt;
&lt;h3 id=&#34;counter-example-the-longest-simple-path&#34;&gt;Counter-Example: The Longest Simple Path&lt;/h3&gt;
&lt;p&gt;Consider the following problems which first appear to have optimal substructure.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Shortest path&lt;/strong&gt;: find a path \(u \leadsto v\) with the fewest edges without cycles.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Longest simple path&lt;/strong&gt;: find a path \(u \leadsto v\) with the most edges without cycles.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first problem has optimal substructure. Suppose that the shortest path \(u \leadsto v\) is given by \(p\). Given some intermediate vertex \(w\), the optimal path from \(u \leadsto w\) is given by \(p_1\) and the optimal path from \(w \leadsto v\) is given by \(p_2\). If there were a shorter path \(p&amp;rsquo;_1\) from \(u \leadsto w\) then we could replace \(p_1\) with it and get a total path with fewer edges.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Check your understanding&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Why does that argument reinforce the idea of optimal substructure? By showing that the optimal solution to a subproblem is the optimal solution to the original problem. This argument becomes clearer as we consider the longest simple path problem.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-15_13-43-21_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Subproblem graph for the longest simple path problem (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Subproblem graph for the longest simple path problem (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Consider the directed graph above. The path \(q \rightarrow r \rightarrow t\) is the longest simple path from \(q\) to \(t\). &lt;strong&gt;Keep in mind that the problem is to find a simple path with the most edges. If the substructure is optimal, then the subpaths must also exhibit maximal edges.&lt;/strong&gt; The subpath \(q \leadsto r\) in this case is simply \(q \rightarrow r\), but the longest simple path from \(q\) to \(r\) is \(q \rightarrow s \rightarrow t \rightarrow r\). Therefore, the subpath \(q \leadsto r\) is not optimal. This is a counter-example to the idea that the longest simple path problem has optimal substructure.&lt;/p&gt;
&lt;p&gt;The longest simple path problem does not have &lt;strong&gt;independent&lt;/strong&gt; subproblems. Consider a path from \(q\) to \(t\). This could be broken down into subproblem \(q \leadsto r\) and \(r \leadsto t\). For \(q \leadsto r\), we have \(q \rightarrow s \rightarrow t \rightarrow r\). This subproblem is dependent on \(s\) and \(t\), so we cannot use them in the second subproblem \(r \leadsto t\) without forming a path that is not simple. Specifically, the first subproblem includes \(t\), so the second subproblem cannot include \(t\). &lt;strong&gt;However, the second subproblem MUST include \(t\).&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;questions&#34;&gt;Questions&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;What are the independent subproblems in the rod cutting and matrix-chain multiplication problems?&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;using-overlapping-subproblems&#34;&gt;Using Overlapping Subproblems&lt;/h3&gt;
&lt;p&gt;First, do not confuse the idea of &lt;strong&gt;overlapping subproblems&lt;/strong&gt; with the need for the subproblems to be &lt;strong&gt;independent&lt;/strong&gt;. Subproblems are independent if they do not share resources, which the longest simple path problem does not have. Overlapping subproblems means that a subproblem may require the result of another independent subproblem. This is the case in the rod cutting problem, where the value of a subproblem is used in the calculation of the value of the next subproblem.&lt;/p&gt;
&lt;p&gt;A desirable trait of any recursive problem is that it have a small number of unique subproblems. The running time of such a solution is dependent on the number of subproblems, so having more of them will naturally lead to a less efficient solution. Section 14.3 reviews the bottom-up solution to matrix-chain multiplication, specifically focusing on the number of times the solution of each subproblem is required. It is recommended to review this section for further understanding.&lt;/p&gt;
&lt;h4 id=&#34;questions&#34;&gt;Questions&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;How many subproblem solutions are reused in the rod cutting problem of \(n=4\)?&lt;/li&gt;
&lt;li&gt;How many subproblem solutions are reused when computing the Fibonacci sequence of \(n\)?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;longest-common-subsequence&#34;&gt;Longest Common Subsequence&lt;/h2&gt;
&lt;p&gt;A longest common subsequence (LCS) of two input sequences \(X = \langle x_1, x_2, \ldots, x_m \rangle\) and \(Y = \langle y_1, y_2, \ldots, y_n \rangle\) is a sequence \(Z = \langle z_1, z_2, \ldots, z_k \rangle\) such that \(Z\) is a subsequence of both \(X\) and \(Y\) and \(k\) is as large as possible. For example, given \(X = \langle A, B, C, B, D, A, B \rangle\) and \(Y = \langle B, D, C, A, B, A \rangle\), the LCS is \(\langle B, C, A, B \rangle\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The subsequence is not necessarily consecutive!&lt;/strong&gt; A subsequence \(Z\) is common to a sequence \(X\) if it corresponds to a strictly increasing sequence of indices such that \(x_{i_j} = z_j\).&lt;/p&gt;
&lt;h3 id=&#34;naive-solution&#34;&gt;Naive Solution&lt;/h3&gt;
&lt;p&gt;First, how would we solve this problem using a brute-force method? We could generate all possible subsequences of \(X\) and \(Y\) and then compare them. This would require \(O(n2^m)\) time.&lt;/p&gt;
&lt;h3 id=&#34;dynamic-programming-solution&#34;&gt;Dynamic Programming Solution&lt;/h3&gt;
&lt;p&gt;Following the four step process, we can formulate a dynamic programming solution to the LCS problem. Step 1 is to determine the optimal substructure of the problem.&lt;/p&gt;
&lt;h4 id=&#34;optimal-substructure&#34;&gt;Optimal Substructure&lt;/h4&gt;
&lt;p&gt;Let \(X = \langle x_1, x_2, \ldots, x_m \rangle\) and \(Y = \langle y_1, y_2, \ldots, y_n \rangle\). Let \(Z = \langle z_1, z_2, \ldots, z_k \rangle\) be an LCS of \(X\) and \(Y\).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If \(x_m = y_n\), then \(z_k = x_m = y_n\) and \(Z_{k-1}\) is an LCS of \(X_{m-1}\) and \(Y_{n-1}\).&lt;/li&gt;
&lt;li&gt;If \(x_m \neq y_n\) and \(z_k \neq x_m\), then \(Z\) is an LCS of \(X_{m-1}\) and \(Y\).&lt;/li&gt;
&lt;li&gt;If \(x_m \neq y_n\) and \(z_k \neq y_n\), then \(Z\) is an LCS of \(X\) and \(Y_{n-1}\).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The theorem above shows that the LCS problem has optimal substructure. Let&amp;rsquo;s break this down a bit. Consider two sequences (words): rocinante and canterbury. The longest common subsequence is &amp;ldquo;cante&amp;rdquo;. Since the last characters of the two original words do not match, we can remove the last character from either word and find the LCS of the two remaining words. This implies that we could have found the LCS of the two original words by finding the LCS of a smaller subproblem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if the two words had the same last character?&lt;/strong&gt; The LCS of the shorter strings is the same as the LCS of the original strings with the last character removed.&lt;/p&gt;
&lt;h4 id=&#34;recursive-solution&#34;&gt;Recursive Solution&lt;/h4&gt;
&lt;p&gt;The next step is to write a recursive solution to the problem. Given the substrucure just presented, a bottom-up approach seems intuitive. Starting with indices \(i=0\) and \(j=0\) which indicate the length of the current strings \(X_i\) and \(Y_j\), increase the length and compute the LCS as we go.&lt;/p&gt;
&lt;p&gt;Define \(c[i, j]\) as the LCS length of \(X_i\) and \(Y_j\). The goal is to compute \(c[m,n]\), where \(m\) and \(n\) are the lengths of \(X\) and \(Y\), respectively. The recursive formula is given by&lt;/p&gt;
&lt;p&gt;\[
c[i, j] = \begin{cases}
0 &amp;amp; \text{if } i = 0 \text{ or } j = 0, \\
c[i-1, j-1] + 1 &amp;amp; \text{if } i, j &amp;gt; 0 \text{ and } x_i = y_j, \\
\max(c[i-1, j], c[i, j-1]) &amp;amp; \text{if } i, j &amp;gt; 0 \text{ and } x_i \neq y_j.
\end{cases}
\]&lt;/p&gt;
&lt;!--list-separator--&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Example: &amp;ldquo;atom&amp;rdquo; and &amp;ldquo;ant&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The LCS of &amp;ldquo;atom&amp;rdquo; and &amp;ldquo;ant&amp;rdquo; is &amp;ldquo;at&amp;rdquo;. The tree below shows the recursive calls to each subproblem. A dashed line indicates that the subproblem has already been solved.&lt;/p&gt;

    
    
    
    
    
    &lt;figure&gt;
    
    &lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-16_10-33-21_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Recursion tree for the LCS problem (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;
    
    
    
    &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
      
      &lt;p&gt;
        &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Recursion tree for the LCS problem (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
        
        
        
      &lt;/p&gt; 
    &lt;/figcaption&gt;
    
    &lt;/figure&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;storing-the-solutions&#34;&gt;Storing the Solutions&lt;/h4&gt;
&lt;p&gt;The LCS problem has \(\Theta(mn)\) distinct subproblems, so storing the solutions to these subproblems will allow us to avoid redundant computation. A dynamic programming solution goes as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Store the lengths of the LCS of the prefixes of \(X\) and \(Y\) in a table \(c\).&lt;/li&gt;
&lt;li&gt;Additionally store the solution to the subproblems in a table \(b\) so that we can reconstruct the LCS.&lt;/li&gt;
&lt;li&gt;The entries are filled in a row-major order.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The code is given below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;lcs_length&lt;/span&gt;(X, Y):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(X)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(Y)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(m&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(m&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, m&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, n&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; X[i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; Y[j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                c[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; c[i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                b[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;↖&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; c[i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][j] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; c[i][j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                c[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; c[i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][j]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                b[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;↑&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                c[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; c[i][j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                b[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;←&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; c, b
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;print_lcs&lt;/span&gt;(b, X, i, j):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; b[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;↖&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print_lcs(b, X, i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(X[i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], end&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; b[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;↑&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print_lcs(b, X, i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, j)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print_lcs(b, X, i, j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;reconstructing-a-solution&#34;&gt;Reconstructing a Solution&lt;/h4&gt;
&lt;p&gt;Printing the solution starts with the last entry in the table \(b\). If the entry is &amp;ldquo;↖&amp;rdquo;, then the last characters of \(X\) and \(Y\) are the same and we print the character. If the entry is &amp;ldquo;↑&amp;rdquo;, then we move up in the table. If the entry is &amp;ldquo;←&amp;rdquo;, then we move left in the table.&lt;/p&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Exercise 14.1-5&lt;/strong&gt; from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exercise 14.2-1&lt;/strong&gt; from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Write a recursive function to compute the Fibonacci sequence. What is the time complexity of this function? What is the time complexity of the dynamic programming solution?&lt;/li&gt;
&lt;li&gt;Write a function that prints a table similar to Figure 14.8 from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;) for the LCS problem.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Medians and Order Statistics</title>
      <link>https://ajdillhoff.github.io/notes/medians_and_order_statistics/</link>
      <pubDate>Tue, 12 Mar 2024 13:17:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/medians_and_order_statistics/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#order-statistics&#34;&gt;Order Statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#minimum-and-maximum&#34;&gt;Minimum and Maximum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#selection-in-expected-linear-time&#34;&gt;Selection in expected linear time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#problems-and-exercises&#34;&gt;Problems and Exercises&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;We briefly touched on a median finding algorithm when discussing &lt;a href=&#34;https://ajdillhoff.github.io/notes/divide_and_conquer_algorithms/&#34;&gt;Divide and Conquer Algorithms&lt;/a&gt;. This section will be a bit of a review, but the point is to touch on the topic of order statistics more generally.&lt;/p&gt;
&lt;h2 id=&#34;order-statistics&#34;&gt;Order Statistics&lt;/h2&gt;
&lt;p&gt;The \(i^{\text{th}}\) order statistic is the \(i^{\text{th}}\) smallest element in a set of \(n\) elements. The median is the \(\frac{n}{2}^{\text{th}}\) order statistic. The minimum and maximum are the \(1^{\text{st}}\) and \(n^{\text{th}}\) order statistics, respectively. When \(n\) is even, there are two medians:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the lower median \(\frac{n}{2}^{\text{th}}\) and&lt;/li&gt;
&lt;li&gt;the upper median \(\frac{n}{2} + 1^{\text{th}}\).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The goal of the algorithm of focus in these notes is to determine how select order statistic \(i\) in a set of \(n\) elements. As we saw previously, and will review in these notes, we can use a divide and conquer approach to solve this problem. Further, we will study a linear approach to this problem under the assumption that the elements are distinct.&lt;/p&gt;
&lt;h2 id=&#34;minimum-and-maximum&#34;&gt;Minimum and Maximum&lt;/h2&gt;
&lt;p&gt;One could reason very simply that the lower bound on the number of comparisons needed to find either the minimum or maximum of a set is \(n-1\). One such argument could be that if we left even 1 comparison out of the \(n-1\) comparisons, we could not guarantee that we had found the minimum or maximum. When implementing an algorithm, we would say that an optimal implementation would require \(n-1\) comparisons.&lt;/p&gt;
&lt;p&gt;As a quick aside, there are plenty of algorithms that we implement which are not optimal in terms of their theoretical lower bound. Consider a naive matrix multiplication algorithm. There are many redundant reads from memory in this algorithm. For example, if we compute \(C = AB\), we need to calculate the output values \(C_{1, 1}\) and \(C_{1, 2}\), among others. Both of these outputs require reading from the first row of \(A\).&lt;/p&gt;
&lt;p&gt;We could find both the minimum and maximum of a set in \(2n - 2\) operations by passing over the set twice. This is theoretically optimal since each pass is performing the optimal \(n-1\) comparisons. If we first compared a pair of elements with each other before comparing them to the minimum and maximum, respectively, we could find both the minimum and maximum in \(3\left\lfloor\frac{n}{2}\right\rfloor\) comparisons.&lt;/p&gt;
&lt;h2 id=&#34;selection-in-expected-linear-time&#34;&gt;Selection in expected linear time&lt;/h2&gt;
&lt;p&gt;We now turn to the problem of &lt;strong&gt;selection&lt;/strong&gt;. Given a set of \(n\) elements and an integer \(i\), we want to find the \(i^{\text{th}}\) order statistic. We will assume that all elements are distinct. We will also assume that \(i\) is between 1 and \(n\).&lt;/p&gt;
&lt;p&gt;The randomized select algorithm returns the \(i^{\text{th}}\) smallest element of an array bounded between indices \(p\) and \(r\). It relies on &lt;code&gt;randomized_partition&lt;/code&gt;, just like &lt;a href=&#34;https://ajdillhoff.github.io/notes/quicksort/&#34;&gt;Quicksort&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;randomized_select&lt;/span&gt;(A, p, r, i):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; r:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; A[p]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; randomized_partition(A, p, r)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; q &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; k:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; A[q]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; k:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; randomized_select(A, p, q&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, i)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; randomized_select(A, q&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, r, i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;k)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first conditional checks if the array had only a single element, in which case it must be the value we are looking for. If not, the array is partitioned so that each element in \(A[p:q-1]\) is less than or equal to \(A[q]\) which is less than or equal to the elements in \(A[q+1:r]\). The line \(k = q - p + 1\) calculates the number of elements less than or equal to the pivot. If the index we are looking for is equal to this number, then we have found it and can return the value immediately.&lt;/p&gt;
&lt;p&gt;If the value was not yet found and \(i &amp;lt; k\), then \(i\) must be in the subarray \(A[p:q-1]\). Therefore, the function is recursively called on that subarray. Otherwise, the subarray \(A[q+1:r]\) is checked. An example from Cormen et al. is shown below (Cormen et al. 2022).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-12_18-05-54_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Randomized select from (Cormen et al. 2022).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Randomized select from (Cormen et al. 2022).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Explanation of figure&lt;/strong&gt; \(A^{(0)}\) shows that \(A[5] = 14\) was chosen as the pivot. The next row, \(A^{(1)}\), depicts the completed partitioning. Cormen et al. note that this is &lt;em&gt;not&lt;/em&gt; a helpful partitioning since less than \(\frac{1}{4}\) of the elements are ignored. A helpful partition is one that leaves at most \(\frac{3}{4}\) of the elements after the partitioning.&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;The worst-case running time of &lt;code&gt;randomized_select&lt;/code&gt; is \(O(n^2)\) since we are partitioning \(n\) elements at \(\Theta(n)\) each. Since the pivot of &lt;code&gt;randomized_partition&lt;/code&gt; is selected at random, we can expect a &lt;em&gt;good&lt;/em&gt; split at least every 2 times it is called. The proof for this is similar to the one made when analyzing &lt;a href=&#34;https://ajdillhoff.github.io/notes/quicksort/&#34;&gt;Quicksort&lt;/a&gt;. Briefly, the expected number of times we must partition before we get a helpful split is 2, which only doubles the running time. The recurrence is still \(T(n) = T(3n/4) + \Theta(n) = \Theta(n)\).&lt;/p&gt;
&lt;p&gt;The first step to showing that the expected runtime of &lt;code&gt;randomized_select&lt;/code&gt; is \(\Theta(n)\) is to show that a partitioning is helpful with probability at least \(\frac{1}{2}\) (Cormen et al. 2022). The rest of the proof requires further examination and comprehension.&lt;/p&gt;
&lt;p&gt;The proof presented by Cormen et al. begins with the following terms:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;\(h_i\) is the event that the \(i^{\text{th}}\) partitioning is helpful.&lt;/li&gt;
&lt;li&gt;\(\{h_0, h_1, \dots, h_m\}\) is the sequence of helpful partitionings.&lt;/li&gt;
&lt;li&gt;\(n_k = |A^{(h_k)}|\) is the number of elements in the subarray \(A^{(h_k)}\) at the \(k^{\text{th}}\) partitioning.&lt;/li&gt;
&lt;li&gt;\(n_k \leq (3/4)n_{k-1}\) for \(k \geq 1\), or \(n_k \leq (3/4)^kn_0\).&lt;/li&gt;
&lt;li&gt;\(X_k = h_{k+1} - h_k\) is the number of unhelpful partitionings between the \(k^{\text{th}}\) and \((k+1)^{\text{th}}\) helpful partitionings.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are certainly partitionings that are not helpful. These are depicted as subarrays within each generation of helpful partitionings. The figure below exemplifies this.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-13_10-12-23_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;The sets within each generation of helpful partitionings are not helpful. From (Cormen et al. 2022).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;The sets within each generation of helpful partitionings are not helpful. From (Cormen et al. 2022).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Given that the probability that a partitioning is helpful is at least \(\frac{1}{2}\), we know that \(E[X_k] \leq 2\). With this, &lt;strong&gt;an upper bound on the number of comparisons of partitioning is derived.&lt;/strong&gt; The total number of comparisons made when partitioning is less than&lt;/p&gt;
&lt;p&gt;\begin{align*}
\sum_{k=0}^{m-1} \sum_{j=h_k}^{h_k + X_k - 1} |A^{(j)}| &amp;amp;\leq \sum_{k=0}^{m-1} \sum_{j=h_k}^{h_k + X_k - 1} |A^{(h_k)}| \\
&amp;amp;= \sum_{k=0}^{m-1} X_k|A^{(h_k)}| \\
&amp;amp;\leq \sum_{k=0}^{m-1} \left(\frac{3}{4}\right)^k n_0. \\
\end{align*}&lt;/p&gt;
&lt;p&gt;The first term on the first line represents the total number of comparisons across all sets. The first sum loops through the \(m\) helpful partitionings, and the inner loop sums the number of comparisons made for each unhelpful partitioning. It is bounded by the term on the right. &lt;strong&gt;This is because \(|A^{(j)}| \leq |A^{(h_k)}|\) if \(A^{(j)}\) is in the \(k^{\text{th}}\) generation of helpful partitionings (see term 4 above).&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using term 5 from above, the second line is derived. The third line leverages term 4 again. The sum is a geometric series, and the total number of comparisons is less than&lt;/p&gt;
&lt;p&gt;\begin{align*}
\text{E} \left[\sum_{k=0}^{m-1} X_k \left(\frac{3}{4}\right)^k n_0\right] &amp;amp;= n_0 \sum_{k=0}^{m-1} \left(\frac{3}{4}\right)^k \text{E}[X_k]\\
&amp;amp;\leq 2n_0 \sum_{k=0}^{m-1} \left(\frac{3}{4}\right)^k \\
&amp;amp;&amp;lt; 2n_0 \sum_{k=0}^{\infty} \left(\frac{3}{4}\right)^k \\
&amp;amp;= 8n_0.
\end{align*}&lt;/p&gt;
&lt;p&gt;The last line is the result of a geometric series. This concludes the proof that &lt;code&gt;randomized_partition&lt;/code&gt; runs in expected linear time.&lt;/p&gt;
&lt;h2 id=&#34;problems-and-exercises&#34;&gt;Problems and Exercises&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Show that the second largest of \(n\) elements can be found with \(n + \lceil\log_2 n\rceil - 2\) comparisons in the worst case.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Sorting in Linear Time</title>
      <link>https://ajdillhoff.github.io/notes/sorting_in_linear_time/</link>
      <pubDate>Mon, 11 Mar 2024 17:10:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/sorting_in_linear_time/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#establishing-a-lower-bound-on-comparison-sorts&#34;&gt;Establishing a Lower Bound on Comparison Sorts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#counting-sort&#34;&gt;Counting Sort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#radix-sort&#34;&gt;Radix Sort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bucket-sort&#34;&gt;Bucket Sort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#questions-and-exercises&#34;&gt;Questions and Exercises&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;These are my personal notes for Chapter 8 of &lt;em&gt;Introduction to Algorithms&lt;/em&gt; (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;). Readers should reference the book for more details when necessary.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;All sorting algorithms discussed up to this point are &lt;strong&gt;comparison based&lt;/strong&gt;. You may have thought, as I did, that sorting cannot be done without a comparison. If you have no way to evaluate the relative ordering of two different objects, how can you possibly arrange them in any order?&lt;/p&gt;
&lt;p&gt;The answer will become clear shortly and is investigated through &lt;strong&gt;counting sort&lt;/strong&gt;, &lt;strong&gt;radix sort&lt;/strong&gt;, and &lt;strong&gt;bucket sort&lt;/strong&gt;. First, Cormen et al. make it clear that sorting algorithms cannot reach linear time. As we will see, any comparison sort &lt;em&gt;must&lt;/em&gt; make \(\Omega (n \lg n)\) comparisons in the worst case to sort \(n\) elements. This bound is motivation enough to explore a different class of sorting algorithms.&lt;/p&gt;
&lt;h2 id=&#34;establishing-a-lower-bound-on-comparison-sorts&#34;&gt;Establishing a Lower Bound on Comparison Sorts&lt;/h2&gt;
&lt;p&gt;The basis of the proof presented in Chapter 8 it to consider that all comparison sorts can be viewed as a decision tree, where each leaf represents a unique permutation of the inputs. If there are \(n\) distinct elements in the original array, then there must be \(n!\) leaves in the decision tree. Consider the figure from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;) below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-11_17-45-30_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Decision tree for comparison sort on three elements.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Decision tree for comparison sort on three elements.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the figure above, each node compares two values as \(a:b\). If \(a \leq b\), the left path is taken. The worst case of a comparison sort can be determined by the height of the tree. A proof on the lower bound goes as follows.&lt;/p&gt;
&lt;p&gt;Consider a binary tree of height \(h\) with \(l\) reachable leaves. Each of the \(n!\) permutations occurs as one of the leaves, so \(n! \leq l\) since there may be duplicate permutations in the leaves. A binary tree with height \(h\) has no more than \(2^h\) leaves, so \(n! \leq l \leq 2^h\). Taking the logarithm of this inequality implies that \(h \geq \lg n!\). Since \(\lg n! = \Theta(n \lg n)\), and is a lower bound on the height of the tree, then any comparison sort must make \(\Omega (n \lg n)\) comparisons.&lt;/p&gt;
&lt;h2 id=&#34;counting-sort&#34;&gt;Counting Sort&lt;/h2&gt;
&lt;p&gt;Counting sort can sort an array of integers in \(O(n+k)\) time, where \(k \geq 0\) is the largest integer in the set. It works by counting the number of elements less than or equal to each element \(x\).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;counting_sort&lt;/span&gt;(A, k):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    B &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    C &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(k&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        C[A[i]] &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# C[i] contains the number of elements equal to i&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, k):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        C[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; C[i] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; C[i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# C[i] contains the number of elements less than or equal to i&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        B[C[A[i]]&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        C[A[i]] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; C[A[i]] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; B
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first two loops establish the number of elements less than or equal to \(i\) for each element \(i\). The main sticking point in understanding this algorithm is the last loop. It starts at the very end of loop, placing the last element from \(A\) into the output array \(B\) in its correct position as determined by \(C\).&lt;/p&gt;
&lt;p&gt;Consider a simple example with \(\{2, 5, 5, 3, 4\}\). After the second loop, \(C = \{0, 0, 1, 2, 3, 5\}\). On the first iteration of the last loop, \(A[4] = 4\) is used as the index into \(C\), which yields \(3\) since the value \(4\) is greater than or equal to \(3\) elements in the original array. It is then placed in the correct spot \(B[3-1] = 4\).&lt;/p&gt;
&lt;p&gt;Another example is shown in figure 8.2 from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;radix-sort&#34;&gt;Radix Sort&lt;/h2&gt;
&lt;p&gt;Dating back to 1887 by Herman Hollerith&amp;rsquo;s work on &lt;a href=&#34;https://en.wikipedia.org/wiki/Tabulating_machine&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;tabulating machines&lt;/a&gt;, this algorithm places numbers in one of \(k\) bins based on their &lt;strong&gt;radix&lt;/strong&gt;, or the number of unique digits. It was used for sorting punch cards via multi-column sorting. It works by iteratively sorting a series of inputs based on a column starting with the least-significant digit. An example is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-12_10-34-07_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Radix sort in action (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Radix sort in action (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the figure above, the items are first sorted based on the least significant digits. By the end of the process, the data is numerically sorted in ascending order. The algorithm can be written simply:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;radix_sort&lt;/span&gt;(A, d):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(d):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; counting_sort(A, len(A), &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; A
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Counting sort is the typical sorting algorithm that is used to sort the digits in each column. In fact, any &lt;em&gt;stable&lt;/em&gt; sorting algorithm can be used in place of counting sort.&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;We know that counting sort is \(\Theta(n + k)\), and that radix sort calls it \(d\) times. Therefore, the time complexity of radix sort is \(\Theta(d(n + k))\). If \(k = O(n)\), then the time complexity is \(\Theta(dn)\).&lt;/p&gt;
&lt;h3 id=&#34;complex-keys&#34;&gt;Complex Keys&lt;/h3&gt;
&lt;p&gt;What if the data is not just a single integer, but a complex key or series of keys? The keys themselves can be broken up into digits. Consider a 32-bit word. If we want to sort \(n\) of these words, and we have \(b = 32\) bits per word, we can break the words into \(r=8\) bit digits. This yields \(d = \lceil b / r \rceil = 4\) digits. The largest value for each digit is then \(k = 2^r - 1 = 255\). Plugging these values into the analysis from above yields $Θ((b/r)(n + 2^r)).$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What is the best choice of \(r\)?&lt;/strong&gt; Consider what happens for different values of \(r\). As \(r\) increases, \(2^r\) increases. As it decreases, \(\frac{b}{r}\) increases. The best choice depends on whether \(b &amp;lt; \lfloor \lg n \rfloor\). If \(b &amp;lt; \lfloor \lg n \rfloor\), then \(r \leq b\) implies \((n + 2^r) = \Theta(n)\) since \(2^{\lg n} = n\). If \(b \geq \lfloor \lg n \rfloor\), then we should choose \(r \approx \lg n\). This would yield \(\Theta((b / \lg n)(n + n)) = \Theta(bn / \lg n)\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You should spend some time to think about the choices for \(r\).&lt;/strong&gt; Specifically, what will happen to the time complexity as \(r\) increases above \(\lg n\)? What about as \(r\) decreases below \(\lg n\)?&lt;/p&gt;
&lt;h3 id=&#34;example-sorting-words&#34;&gt;Example: Sorting words&lt;/h3&gt;
&lt;p&gt;Use radix sort to sort the following list of names: &amp;ldquo;Beethoven&amp;rdquo;, &amp;ldquo;Bach&amp;rdquo;, &amp;ldquo;Mozart&amp;rdquo;, &amp;ldquo;Chopin&amp;rdquo;, &amp;ldquo;Liszt&amp;rdquo;, &amp;ldquo;Schubert&amp;rdquo;, &amp;ldquo;Haydn&amp;rdquo;, &amp;ldquo;Brahms&amp;rdquo;, &amp;ldquo;Wagner&amp;rdquo;, &amp;ldquo;Tchaikovsky&amp;rdquo;. First, we need to figure out how to encode the names as integers. If we convert the input to lowercase, we only have to deal with \(k=26\) unique characters. This only requires 5 bits. Since each name has varying length, we can use a sentinel value of 0 to pad the shorter names. That is, 0 represents a padding character and the alphabet starts at 1. The names are then encoded as follows:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Original Name&lt;/th&gt;
          &lt;th&gt;Encoded Name&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Beethoven&lt;/td&gt;
          &lt;td&gt;[2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Bach&lt;/td&gt;
          &lt;td&gt;[2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Mozart&lt;/td&gt;
          &lt;td&gt;[13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Chopin&lt;/td&gt;
          &lt;td&gt;[3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Liszt&lt;/td&gt;
          &lt;td&gt;[12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Schubert&lt;/td&gt;
          &lt;td&gt;[19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Haydn&lt;/td&gt;
          &lt;td&gt;[8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Brahms&lt;/td&gt;
          &lt;td&gt;[2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Wagner&lt;/td&gt;
          &lt;td&gt;[23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Tchaikovsky&lt;/td&gt;
          &lt;td&gt;[20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25]&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;No changes will be made for the first 2 iterations of the sort. The third iteration will yield the following:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Original Name&lt;/th&gt;
          &lt;th&gt;Encoded Name&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Bach&lt;/td&gt;
          &lt;td&gt;[2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Mozart&lt;/td&gt;
          &lt;td&gt;[13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Chopin&lt;/td&gt;
          &lt;td&gt;[3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Liszt&lt;/td&gt;
          &lt;td&gt;[12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Schubert&lt;/td&gt;
          &lt;td&gt;[19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Haydn&lt;/td&gt;
          &lt;td&gt;[8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Brahms&lt;/td&gt;
          &lt;td&gt;[2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Wagner&lt;/td&gt;
          &lt;td&gt;[23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Beethoven&lt;/td&gt;
          &lt;td&gt;[2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Tchaikovsky&lt;/td&gt;
          &lt;td&gt;[20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25]&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Iteration 3&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Original Name&lt;/th&gt;
          &lt;th&gt;Encoded Name&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Bach&lt;/td&gt;
          &lt;td&gt;[2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Mozart&lt;/td&gt;
          &lt;td&gt;[13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Chopin&lt;/td&gt;
          &lt;td&gt;[3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Liszt&lt;/td&gt;
          &lt;td&gt;[12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Haydn&lt;/td&gt;
          &lt;td&gt;[8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Brahms&lt;/td&gt;
          &lt;td&gt;[2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Wagner&lt;/td&gt;
          &lt;td&gt;[23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Schubert&lt;/td&gt;
          &lt;td&gt;[19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Beethoven&lt;/td&gt;
          &lt;td&gt;[2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Tchaikovsky&lt;/td&gt;
          &lt;td&gt;[20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25]&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Iteration 4&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Original Name&lt;/th&gt;
          &lt;th&gt;Encoded Name&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Bach&lt;/td&gt;
          &lt;td&gt;[2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Liszt&lt;/td&gt;
          &lt;td&gt;[12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Haydn&lt;/td&gt;
          &lt;td&gt;[8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Schubert&lt;/td&gt;
          &lt;td&gt;[19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Tchaikovsky&lt;/td&gt;
          &lt;td&gt;[20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Chopin&lt;/td&gt;
          &lt;td&gt;[3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Beethoven&lt;/td&gt;
          &lt;td&gt;[2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Wagner&lt;/td&gt;
          &lt;td&gt;[23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Brahms&lt;/td&gt;
          &lt;td&gt;[2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Mozart&lt;/td&gt;
          &lt;td&gt;[13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Iteration 5&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Original Name&lt;/th&gt;
          &lt;th&gt;Encoded Name&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Bach&lt;/td&gt;
          &lt;td&gt;[2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Schubert&lt;/td&gt;
          &lt;td&gt;[19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Wagner&lt;/td&gt;
          &lt;td&gt;[23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Beethoven&lt;/td&gt;
          &lt;td&gt;[2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Tchaikovsky&lt;/td&gt;
          &lt;td&gt;[20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Chopin&lt;/td&gt;
          &lt;td&gt;[3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Brahms&lt;/td&gt;
          &lt;td&gt;[2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Haydn&lt;/td&gt;
          &lt;td&gt;[8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Mozart&lt;/td&gt;
          &lt;td&gt;[13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Liszt&lt;/td&gt;
          &lt;td&gt;[12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Iteration 6&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Original Name&lt;/th&gt;
          &lt;th&gt;Encoded Name&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Tchaikovsky&lt;/td&gt;
          &lt;td&gt;[20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Mozart&lt;/td&gt;
          &lt;td&gt;[13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Haydn&lt;/td&gt;
          &lt;td&gt;[8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Bach&lt;/td&gt;
          &lt;td&gt;[2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Brahms&lt;/td&gt;
          &lt;td&gt;[2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Wagner&lt;/td&gt;
          &lt;td&gt;[23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Chopin&lt;/td&gt;
          &lt;td&gt;[3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Beethoven&lt;/td&gt;
          &lt;td&gt;[2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Schubert&lt;/td&gt;
          &lt;td&gt;[19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Liszt&lt;/td&gt;
          &lt;td&gt;[12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Iteration 7&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Original Name&lt;/th&gt;
          &lt;th&gt;Encoded Name&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Brahms&lt;/td&gt;
          &lt;td&gt;[2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Bach&lt;/td&gt;
          &lt;td&gt;[2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Beethoven&lt;/td&gt;
          &lt;td&gt;[2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Wagner&lt;/td&gt;
          &lt;td&gt;[23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Tchaikovsky&lt;/td&gt;
          &lt;td&gt;[20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Schubert&lt;/td&gt;
          &lt;td&gt;[19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Chopin&lt;/td&gt;
          &lt;td&gt;[3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Liszt&lt;/td&gt;
          &lt;td&gt;[12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Haydn&lt;/td&gt;
          &lt;td&gt;[8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Mozart&lt;/td&gt;
          &lt;td&gt;[13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Iteration 8&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Original Name&lt;/th&gt;
          &lt;th&gt;Encoded Name&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Bach&lt;/td&gt;
          &lt;td&gt;[2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Haydn&lt;/td&gt;
          &lt;td&gt;[8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Wagner&lt;/td&gt;
          &lt;td&gt;[23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Tchaikovsky&lt;/td&gt;
          &lt;td&gt;[20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Schubert&lt;/td&gt;
          &lt;td&gt;[19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Beethoven&lt;/td&gt;
          &lt;td&gt;[2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Chopin&lt;/td&gt;
          &lt;td&gt;[3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Liszt&lt;/td&gt;
          &lt;td&gt;[12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Mozart&lt;/td&gt;
          &lt;td&gt;[13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Brahms&lt;/td&gt;
          &lt;td&gt;[2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Iteration 9&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Original Name&lt;/th&gt;
          &lt;th&gt;Encoded Name&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Bach&lt;/td&gt;
          &lt;td&gt;[2, 1, 3, 8, 0, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Beethoven&lt;/td&gt;
          &lt;td&gt;[2, 5, 5, 20, 8, 15, 22, 5, 14, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Brahms&lt;/td&gt;
          &lt;td&gt;[2, 18, 1, 8, 13, 19, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Chopin&lt;/td&gt;
          &lt;td&gt;[3, 8, 15, 16, 9, 14, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Haydn&lt;/td&gt;
          &lt;td&gt;[8, 1, 25, 4, 14, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Liszt&lt;/td&gt;
          &lt;td&gt;[12, 9, 19, 26, 20, 0, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Mozart&lt;/td&gt;
          &lt;td&gt;[13, 15, 26, 1, 18, 20, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Schubert&lt;/td&gt;
          &lt;td&gt;[19, 3, 8, 21, 2, 5, 18, 20, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Tchaikovsky&lt;/td&gt;
          &lt;td&gt;[20, 3, 8, 1, 9, 11, 15, 22, 19, 11, 25]&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Wagner&lt;/td&gt;
          &lt;td&gt;[23, 1, 7, 14, 5, 18, 0, 0, 0, 0, 0]&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;bucket-sort&#34;&gt;Bucket Sort&lt;/h2&gt;
&lt;p&gt;The final sorting algorithm in Chapter 8 is bucket sort (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;). As the name suggests, bucket sort distributes the input into a number of distinct buckets based on the input value. The key here is the assumption that the data is uniformly distributed. If the data were not uniformly distributed, then more elements would be concentrated. The uniformity ensures that a relatively equal number of data points are placed in each bucket. This is also a convenient assumption to have for a parallelized implementation.&lt;/p&gt;
&lt;p&gt;The algorithm works by placing values into a bucket based on their most significant digits. Once the values are assigned, then a simple sort, like insertion sort, is used to sort the values within each bucket. Once sorted, the buckets are concatenated together to produce the final output. Under the assumption of uniformity, each bucket will contain no more than \(1/n\) of the total elements. This implies that each call to &lt;code&gt;insertion_sort&lt;/code&gt; will take \(O(1)\) time.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-12_11-54-07_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Bucket sort in action (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Bucket sort in action (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bucket_sort&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    B &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        B[int(n &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; A[i])]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(A[i])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        insertion_sort(B[i])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; B
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;Initializing the array and placing each item into a bucket takes \(\Theta(n)\) time. The call to each insertion sort is \(O(n^2)\). Therefore, the recurrence is given as&lt;/p&gt;
&lt;p&gt;\[
T(n) = \Theta(n) + \sum_{i=0}^{n-1} O(n_i^2).
\]&lt;/p&gt;
&lt;p&gt;The key is to determine the expected value \(E[n_i^2]\). We will frame the problem as a binomial distribution, where a success occurs when an element goes into bucket \(i\).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(p\) is the probability of success: \(p = \frac{1}{n}\).&lt;/li&gt;
&lt;li&gt;\(q\) is the probability of failure: \(q = 1 - \frac{1}{n}\).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Under a binomial distribution, we have that \(E[n_i] = np = n(1/n) = 1\) and \(\text{Var}[n_i] = npq = 1 - 1/n\), where \(p = 1/n\) and \(q = 1 - 1/n\). The expected value is then&lt;/p&gt;
&lt;p&gt;\[
E[n_i^2] = \text{Var}[n_i] + E[n_i]^2 = 1 - 1/n + 1 = 2 - 1/n.
\]&lt;/p&gt;
&lt;p&gt;This gives way to the fact that \(E[T(n)] = \Theta(n) + \sum_{i=0}^{n-1} O(2 - 1/n) = \Theta(n) + O(n) = \Theta(n)\).&lt;/p&gt;
&lt;h2 id=&#34;questions-and-exercises&#34;&gt;Questions and Exercises&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Come up with applications of counting sort when \(k = O(n)\).&lt;/li&gt;
&lt;li&gt;Paul wants to use radix sort to sort \(2^{16}\) 32-bit numbers. What is the best value of \(r\) to use? How many calls to a stable sort will be made?&lt;/li&gt;
&lt;li&gt;In what ways does radix sort differ from quicksort? When is one better than the other?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Substitution Method</title>
      <link>https://ajdillhoff.github.io/notes/substitution_method/</link>
      <pubDate>Tue, 27 Feb 2024 19:12:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/substitution_method/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#example-from-clrs&#34;&gt;Example from CLRS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#making-the-wrong-guess&#34;&gt;Making the Wrong Guess&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;The &lt;strong&gt;substitution method&lt;/strong&gt; is a technique for solving recurrences. It works in two steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Guess the solution&lt;/li&gt;
&lt;li&gt;Use mathematical induction to verify the guess&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This can work very well, especially if we already have some intuition about the problem. Let&amp;rsquo;s start with a simple example: The Tower of Hanoi. In this classic puzzle, we have three pegs and a number of disks of different sizes which can slide onto any peg. The puzzle starts with the disks in a neat stack in ascending order of size on one peg, with the smallest disk on top. The objective is to move the entire stack to another peg, obeying the following rules:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Only one disk can be moved at a time&lt;/li&gt;
&lt;li&gt;Each move consists of taking the top disk from one of the stacks and placing it on top of another stack&lt;/li&gt;
&lt;li&gt;No disk may be placed on top of a smaller disk&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;An algorithm to solve the puzzle goes like this:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Move \(n-1\) disks from peg 1 to peg 2 using peg 3 as a temporary holding area&lt;/li&gt;
&lt;li&gt;Move the $n$th disk from peg 1 to peg 3&lt;/li&gt;
&lt;li&gt;Move the \(n-1\) disks from peg 2 to peg 3 using peg 1 as a temporary holding area&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The number of moves required to solve the Tower of Hanoi puzzle is given by the recurrence relation \(T(n) = 2T(n-1) + 1\) with the initial condition \(T(1) = 1\). We can solve this recurrence using the substitution method.&lt;/p&gt;
&lt;p&gt;Our hypothesis might be that \(T(n) \leq c2^n - 1\) for all \(n \geq n_0\), where \(c &amp;gt; 0\) and \(n_0 &amp;gt; 0\). For the base case, we have \(T(1) = c * 2^1 - 1 = c\), so \(c = 1\). Now we need to show that \(T(n) \leq c2^n - 1\) for all \(n \geq n_0\). Then we have:&lt;/p&gt;
&lt;p&gt;\begin{align*}
T(n) &amp;amp;\leq 2T(n-1) + 1 \\
&amp;amp;\leq 2\left(2^{n-1} - 1\right) + 1 \\
&amp;amp;= 2^n - 2 + 1 \\
&amp;amp;= 2^n - 1 \\
\end{align*}&lt;/p&gt;
&lt;p&gt;What if we made a bad guess? Let&amp;rsquo;s try \(T(n) \leq cn\) for all \(n \geq n_0\). We have \(T(1) = c = 1\), so \(c = 1\). Now we need to show that \(T(n) \leq cn\) for all \(n \geq n_0\). We assume that \(T(k) \leq ck\) for all \(k &amp;lt; n\). Then we have:&lt;/p&gt;
&lt;p&gt;\begin{align*}
T(n) &amp;amp;\leq 2T(n-1) + 1 \\
&amp;amp;\leq 2c(n-1) + 1 \\
&amp;amp;= 2cn - 2c + 1 \\
\end{align*}&lt;/p&gt;
&lt;p&gt;This does not work because \(2cn - 2c + 1 &amp;gt; cn\) for all \(c &amp;gt; 1\). Therefore, our guess was wrong.&lt;/p&gt;
&lt;h2 id=&#34;example-from-clrs&#34;&gt;Example from CLRS&lt;/h2&gt;
&lt;p&gt;Determine an asymptotic upper bound for&lt;/p&gt;
&lt;p&gt;\[
T(n) = 2T(\lfloor n/2 \rfloor) + \Theta(n).
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Guess:&lt;/strong&gt; \(T(n) = O(n \lg n)\)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inductive hypothesis:&lt;/strong&gt; \(T(n) \leq cn \lg n\) for all \(n \geq n_0\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inductive step:&lt;/strong&gt; Assume \(T(n) \leq cn \lg n\) for all \(n_0 \leq k &amp;lt; n\). For \(T(\lfloor n/2 \rfloor) \leq c\lfloor n/2 \rfloor \lg \lfloor n/2 \rfloor\), it holds when \(n \geq 2\).&lt;/p&gt;
&lt;p&gt;\begin{align*}
T(n) &amp;amp;\leq 2T(\lfloor n/2 \rfloor) + \Theta(n) \\
&amp;amp;\leq 2c\lfloor n/2 \rfloor \lg \lfloor n/2 \rfloor + \Theta(n) \\
&amp;amp;= cn \lg (n / 2) + \Theta(n) \\
&amp;amp;= cn \lg n - cn\lg 2 + \Theta(n) \\
&amp;amp;= cn \lg n - cn + \Theta(n) \\
&amp;amp;\leq cn \lg n
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;making-the-wrong-guess&#34;&gt;Making the Wrong Guess&lt;/h2&gt;
&lt;p&gt;What if we took the same recurrence and guessed that \(T(n) = O(n)\)?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Guess:&lt;/strong&gt; \(T(n) = O(n)\)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inductive hypothesis:&lt;/strong&gt; \(T(n) \leq cn\) for all \(n \geq n_0\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inductive step:&lt;/strong&gt; Assume \(T(n) \leq cn\) for all \(n \geq n_0\).&lt;/p&gt;
&lt;p&gt;\begin{align*}
T(n) &amp;amp;\leq 2c\lfloor n/2 \rfloor + \Theta(n) \\
&amp;amp;\leq cn + \Theta(n) \\
\end{align*}&lt;/p&gt;
&lt;p&gt;This does not work because \(cn + \Theta(n) &amp;gt; cn\). Therefore, our guess was wrong.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quicksort</title>
      <link>https://ajdillhoff.github.io/notes/quicksort/</link>
      <pubDate>Sun, 25 Feb 2024 17:24:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/quicksort/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#basic-quicksort&#34;&gt;Basic Quicksort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#performance&#34;&gt;Performance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#randomized-quicksort&#34;&gt;Randomized Quicksort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#paranoid-quicksort&#34;&gt;Paranoid Quicksort&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Quicksort is a popular sorting algorithm implemented in many language libraries that has a worst-case running time of \(\Theta(n^2)\). &lt;strong&gt;Why would anyone choose this as the default sorting algorithm if one like mergesort has better worst-case performance?&lt;/strong&gt; As you will see, the devil is in the details. Quicksort is often faster in practice. It also has a small memory footprint and is easy to implement.&lt;/p&gt;
&lt;h2 id=&#34;basic-quicksort&#34;&gt;Basic Quicksort&lt;/h2&gt;
&lt;p&gt;Quicksort follows a divide-and-conquer approach to sorting. Given an input array of \(n\) elements, it selects a pivot element and partitions the array into two sub-arrays: one with elements less than the pivot and one with elements greater than the pivot. It then recursively sorts the sub-arrays. Since the subarrays are recursively sorted, there is no need for a merge step as in mergesort.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;quicksort&lt;/span&gt;(arr, p, r):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; partition(arr, p, r)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    quicksort(arr, p, q &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    quicksort(arr, q &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, r)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This algorithm looks deceptively simple. The complexity is hidden in the partitioning step. This procedure will rearrange the elements in the array such that the pivot element is in its final position and all elements less than the pivot are to the left of it and all elements greater than the pivot are to the right of it.&lt;/p&gt;
&lt;h3 id=&#34;partitioning&#34;&gt;Partitioning&lt;/h3&gt;
&lt;p&gt;For basic quicksort, the first or last element is chosen as the pivot. Picking it this way yields a fairly obvious recurrence of \(T(n) = T(n-1) + O(n)\), which is \(\Theta(n^2)\). As mentioned before, this algorithm is executed in-place, meaning there is no need for additional memory to store the sub-arrays. It is done through a clever use of indices.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;partition&lt;/span&gt;(arr, p, r):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; arr[r]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(p, r):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; arr[j] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; x:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            i &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            arr[i], arr[j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; arr[j], arr[i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    arr[i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], arr[r] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; arr[r], arr[i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The indices are used to define the following loop invariant.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Left:&lt;/strong&gt; if \(p \leq k \leq i\), then \(A[k] \leq x\)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Middle:&lt;/strong&gt; if \(i + 1 \leq k \leq j - 1\), then \(A[k] &amp;gt; x\)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Right:&lt;/strong&gt; if \(k = r\), then \(A[k] = x\)&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-25_19-18-19_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Operation of partition (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Operation of partition (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The figure above is from CRLS which shows an example run of the partitioning algorithm. It starts with \(i, p, j\) on the left and \(r\) on the right. Since the value at \(j\) is less than the pivot, \(i\) is incremented and the values at \(i\) and \(j\) are swapped. In the next iteration, the value at \(j\) is greater than the pivot, so nothing is done. This continues until \(j\) reaches \(r\). At this point, the pivot is swapped with the value at \(i + 1\).&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-26_16-34-22_Quicksort%20Example.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Example of Quicksort.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Example of Quicksort.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The example above starts with an unsorted array. The second row shows the array after the first call to &lt;code&gt;partition&lt;/code&gt;. The left and right subarrays are called recursively. The first subarray in row 3 has a pivot 3 and is already partitioned. The right subarray follows the same convenience.&lt;/p&gt;
&lt;p&gt;On row 4, the left subarray is modified by swapping the pivot of 2 with 1. The right subarray first swaps 5 and 7 before swapping 6 and 7. The final array is sorted as shown on the last row.&lt;/p&gt;
&lt;h2 id=&#34;performance&#34;&gt;Performance&lt;/h2&gt;
&lt;p&gt;The performance of quicksort is dependent on the pivot. If the subarrays on either side of the pivot are balanced, the running time is asymptotically similar to mergesort. In the worst case, it will run in quadratic time.&lt;/p&gt;
&lt;p&gt;The worst partitioning occurs when the pivot is the smallest or largest element in the array. This creates a subarray of size \(n - 1\) and another of size 0. The partitioning itself takes \(\Theta(n)\) time, yielding a recurrence of \(T(n) = T(n - 1) + \Theta(n)\). We can use the substitution method to solve this recurrence and find that the worst-case running time is \(\Theta(n^2)\). This can happen even if the input is already sorted before hand.&lt;/p&gt;
&lt;p&gt;Cormen et al. present a recursive analysis of the running time where, at each level, the partition produces a 9-to-1 split. This is visualized in the sketch below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-26_18-20-51_Quicksort%20Recursion%2001%20Artboard%201%20%282%29.jpg&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Recursion tree for quicksort.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Recursion tree for quicksort.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The subtree for the \(\frac{1}{10}\) split eventually bottoms out after being called \(\log_{10} n\) times. Until this happens, the cost of each level of the tree is \(n\). After the left-tree bottoms out, the right tree continues with an upper bound of \(\leq n\). The right tree completes after \(\log_{10/9} n = \Theta(\lg n)\) levels. Since each level of the tree cost no more than \(n\), the total cost is \(\Theta(n \lg n)\).&lt;/p&gt;
&lt;h3 id=&#34;best-case&#34;&gt;Best Case&lt;/h3&gt;
&lt;p&gt;In the best-case, the pivot is the median of the array and two balanced subarrays are created: one of size \(n/2\) and another of size \(\lfloor (n-1)/2 \rfloor\). The recurrence is \(T(n) = 2T(n/2) + \Theta(n)\), which is \(\Theta(n \log n)\).&lt;/p&gt;
&lt;p&gt;Using the substitution method, we can show the best-case running time. We start with the fact that the partitioning produces two subproblems with a total size of \(n-1\). This gives the following recurrence:&lt;/p&gt;
&lt;p&gt;\[
T(n) = \min_{0 \leq q \leq n-1} \{T(q) + T(n - q - 1)\} + \Theta(n).
\]&lt;/p&gt;
&lt;p&gt;The minimum function accounts for the minimum time taken to sort any partition of the array, where \(q\) represents the pivot element&amp;rsquo;s position.&lt;/p&gt;
&lt;p&gt;Our &lt;strong&gt;hypothesis&lt;/strong&gt; will be that&lt;/p&gt;
&lt;p&gt;\[
T(n) \geq cn \lg n = \Omega(n \lg n).
\]&lt;/p&gt;
&lt;p&gt;Plugging in our hypothesis, we get&lt;/p&gt;
&lt;p&gt;\begin{align*}
T(n) &amp;amp;\geq \min_{0 \leq q \leq n-1} \{cq \lg q + c(n - q - 1) \lg (n - q - 1)\} + \Theta(n) \\
&amp;amp; c \min_{0 \leq q \leq n-1} \{q \lg q + (n - q - 1) \lg (n - q - 1)\} + \Theta(n).
\end{align*}&lt;/p&gt;
&lt;p&gt;If we take the derivative of the function inside the minimum with respect to \(q\), we get&lt;/p&gt;
&lt;p&gt;\[
\frac{d}{dq} \{q \lg q + (n - q - 1) \lg (n - q - 1)\} = c\{\frac{q}{q} + \lg q - \lg (n - q - 1) - \frac{(n - q - 1)}{(n - q - 1)}\}.
\]&lt;/p&gt;
&lt;p&gt;Setting this equal to zero and solving for \(q\) yields&lt;/p&gt;
&lt;p&gt;\[
q = \frac{n - 1}{2}.
\]&lt;/p&gt;
&lt;p&gt;We can then plug this value of \(q\) into the original function to get&lt;/p&gt;
&lt;p&gt;\begin{align*}
T(n) &amp;amp;\geq c \frac{n - 1}{2} \lg \frac{n - 1}{2} + c \frac{n - 1}{2} \lg \frac{n - 1}{2} + \Theta(n) \\
&amp;amp;= cn \lg (n - 1) + c (n - 1) + \Theta(n) \\
&amp;amp;= cn \lg (n - 1) + \Theta(n) \\
&amp;amp;\geq cn \lg n \\
&amp;amp;= \Omega(n \lg n).
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;average-case&#34;&gt;Average Case&lt;/h3&gt;
&lt;p&gt;As it turns out, the average-case running time is \(\Theta(n \log n)\) (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;). Quicksort is highly dependent on the relative ordering of the input. Consider the case of a randomly ordered array. The cost of partitioning the original input is \(O(n)\). Let&amp;rsquo;s say that the pivot was the last element, yielding a split of 0 and \(n - 1\). Now, let&amp;rsquo;s say we get lucky on the next iteration and get a balanced split. Even if the rest of the algorithm splits between the median and the last element, the upper bound on the running time is \(\Theta(n \log n)\). It is highly unlikely that the split will be unbalanced on every iteration given a random initial ordering.&lt;/p&gt;
&lt;p&gt;We can make this a tad more formal by defining a lucky \(L(n) = 2U(n/2) + \Theta(n)\) and an unlucky split \(U(n) = L(n-1) + \Theta(n)\). We can solve for \(L(n)\) by plugging in the definition of \(U(n)\).&lt;/p&gt;
&lt;p&gt;\begin{align*}
L(n) &amp;amp;= 2U(n/2) + \Theta(n) \\
&amp;amp;= 2(L(n/2 - 1) + \Theta(n/2)) + \Theta(n) \\
&amp;amp;= 2L(n/2 - 1) + \Theta(n) \\
&amp;amp;= \Theta(n \log n)
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;randomized-quicksort&#34;&gt;Randomized Quicksort&lt;/h2&gt;
&lt;p&gt;The intuition of the crude analysis above is that we would have to be extremely unlucky to get a quadratic running time if the input is randomly ordered. Randomized quicksort builds on this intuition by selecting a random pivot on each iteration. This is done by swapping the pivot with a random element before partitioning.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; random
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;randomized_partition&lt;/span&gt;(arr, p, r):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(p, r)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    arr[i], arr[r] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; arr[r], arr[i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; partition(arr, p, r)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;randomized_quicksort&lt;/span&gt;(arr, p, r):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; r:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; randomized_partition(arr, p, r)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        randomized_quicksort(arr, p, q &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        randomized_quicksort(arr, q &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, r)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;The lightweight analysis above reasoned that, as long as each split puts a constant amount of elements to one side of the split, then the running time is \(\Theta(n \log n)\).&lt;/p&gt;
&lt;p&gt;We can understand this analysis simply by asking the right questions. First, our primary question: &lt;strong&gt;What is the running time of Quicksort dependent on?&lt;/strong&gt; The biggest bottleneck is the partitioning function. At most, we get really unlucky and the first pivot is picked every time. This means it is called \(n\) times yielding \(O(n)\). The variable part of this is figuring out how many element comparisons are made. The running time is then \(O(n + X)\).&lt;/p&gt;
&lt;h4 id=&#34;the-expected-value-of-x&#34;&gt;The Expected Value of \(X\)&lt;/h4&gt;
&lt;p&gt;The number of comparisons can be expressed as&lt;/p&gt;
&lt;p&gt;\[
X = \sum_{i=1}^{n-1} \sum_{j=i+1}^{n} X_{ij},
\]&lt;/p&gt;
&lt;p&gt;where \(X_{ij}\) is the indicator random variable that is 1 if \(A[i]\) and \(A[j]\) are compared and 0 otherwise. This works with our worst case analysis. If we always get a split of 0 and \(n - 1\), then the indicator random variable is 1 for every comparison, yielding \(O(n^2)\). Taking the expectation of both sides:&lt;/p&gt;
&lt;p&gt;\begin{align*}
E[X] &amp;amp;= E\left[\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} X_{ij}\right] \\
&amp;amp;= \sum_{i=1}^{n-1} \sum_{j=i+1}^{n} E[X_{ij}] \\
&amp;amp;= \sum_{i=1}^{n-1} \sum_{j=i+1}^{n} P(X_{ij} = 1).
\end{align*}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What is \(P(X_{ij} = 1)\)?&lt;/strong&gt; Let \(z_i, \dots, z_j\) be the indices of elements in a sorted version of the array. Under this assumption, \(z_i\) is compared to \(z_j\) only if \(z_i\) or \(z_j\) is the first pivot chosen from the subarray \(A[i \dots j]\). In a set of distinct elements, the probability of picking any pivot from the array from \(i\) to \(j\) is \(\frac{1}{j - i + 1}\). This means that the probability of comparing \(z_i\) and \(z_j\) is \(\frac{2}{j - i + 1}\). We can now finish the calculation.&lt;/p&gt;
&lt;p&gt;\begin{align*}
E[X] &amp;amp;= \sum_{i=1}^{n-1} \sum_{j=i+1}^{n} \frac{2}{j - i + 1} \\
&amp;amp;= \sum_{i=1}^{n-1} \sum_{k=1}^{n-i} \frac{2}{k + 1} \\
&amp;amp;&amp;lt; \sum_{i=1}^{n-1} \sum_{k=1}^{n-i} \frac{2}{k} \\
&amp;amp;= \sum_{i=1}^{n-1} O(\log n) \\
&amp;amp;= O(n \log n).
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;paranoid-quicksort&#34;&gt;Paranoid Quicksort&lt;/h2&gt;
&lt;p&gt;Repeat the following until the partitioning until the left or right subarray is less than or equal to \(\frac{3}{4}\) of the original array.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Choose a random pivot.&lt;/li&gt;
&lt;li&gt;Partition the array.&lt;/li&gt;
&lt;li&gt;Verify that the left and right subarrays are less than or equal to \(\frac{3}{4}\) of the original array.
&lt;ol&gt;
&lt;li&gt;If not, repeat the partitioning.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Recursively sort the subarrays.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;Most of the analysis of Paranoid Quicksort follows that of randomized quicksort. The focus is on the expected number of calls times &lt;code&gt;partition&lt;/code&gt; is called until no side of the split is greater than \(\frac{3}{4}\) of the input.&lt;/p&gt;
&lt;p&gt;Consider a sorted array of \(n\) &lt;em&gt;distinct&lt;/em&gt; elements. The first and last \(\frac{n}{4}\) elements would produce a bad split. That means there are \(n/2\) values that provide a good split, implying that \(p(\text{good split}) = \frac{1}{2}\). Knowing the probability of this event means we can calculated the expected number of times we should call &lt;code&gt;partition&lt;/code&gt; before getting a &lt;em&gt;good&lt;/em&gt; split, &lt;a href=&#34;https://math.stackexchange.com/questions/1196452/expected-value-of-the-number-of-flips-until-the-first-head&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;which is 2.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Continuing on with this analysis, we need to state the recurrence:&lt;/p&gt;
&lt;p&gt;\[
T(n) \leq 2cn + T(\lfloor 3n/4) \rfloor) + T(\lceil n/4 \rceil) + O(1)
\]&lt;/p&gt;
&lt;p&gt;The addition of \(2cn\) is not enough to change our analysis from above. Thus, the expected running time of Quicksort is \(O(n \log n)\).&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Priority Queues</title>
      <link>https://ajdillhoff.github.io/notes/priority_queues/</link>
      <pubDate>Sat, 24 Feb 2024 14:10:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/priority_queues/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#quick-facts&#34;&gt;Quick Facts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementation&#34;&gt;Implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exercises&#34;&gt;Exercises&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;quick-facts&#34;&gt;Quick Facts&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Time Complexity&lt;/strong&gt;&lt;/strong&gt;: \(O(\lg n)\)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Space Complexity&lt;/strong&gt;&lt;/strong&gt;: \(O(n)\)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Besides being the primary data structure for &lt;a href=&#34;https://ajdillhoff.github.io/notes/heapsort/&#34;&gt;Heapsort&lt;/a&gt;, a heap is also used to implement a priority queue. A priority queue is a key-value data structure in which the keys are used to determine the priority of each element in the queue. There are two variants, maximum and minimum, and they support the following operations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Insert: Add a new element to the queue.&lt;/li&gt;
&lt;li&gt;Extract: Remove the element with the maximum/minimum key.&lt;/li&gt;
&lt;li&gt;Maximum/Minimum: Return the element with the maximum/minimum key without removing it.&lt;/li&gt;
&lt;li&gt;Increase/Decrease Key: Increase or decrease the key of a given element.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You could probably imagine a few use cases for such a queue. For example, a priority queue could be used to schedule tasks in a multitasking operating system. The tasks with the highest priority would be executed first. Another example would be a network router that uses a priority queue to schedule packets for transmission. The packets with the highest priority would be transmitted first. In high performance computing, a priority queue could be used to schedule jobs on a supercomputer. The jobs with the highest priority would be executed first. SLURM is an example of a job scheduler that uses a priority queue.&lt;/p&gt;
&lt;p&gt;For simple applications, you could reference your application object directly inside the heap. If the objects themselves are too complex, it is optimal to simply set the &lt;em&gt;value&lt;/em&gt; of the heap as a reference to the object. A &lt;strong&gt;handle&lt;/strong&gt; is a reference that is added to both the heap and the object; it requires little overhead. This requires that your priority queue update both its own index as well as the object&amp;rsquo;s index as changes are made.&lt;/p&gt;
&lt;p&gt;An alternative approach is to establish the map using a hash table. In this case, the priority queue is the only data structure that needs to be updated.&lt;/p&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;Let us now consider the implementation and analysis of the require operations for a priority queue.&lt;/p&gt;
&lt;h3 id=&#34;extract&#34;&gt;Extract&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;keys&lt;/em&gt; in a priority queue represent the priority. The &lt;em&gt;values&lt;/em&gt; will need to be moved around with them as the priority queue is constructed. Getting the maximum or minimum value is a constant time operation and is executed by returning the first element in the array. To extract the item with the highest priority, the first element is removed and the heap is then heapified.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;max_heap_maximum&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; len(A) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ValueError&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Heap underflow&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; A[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;max_heap_extract_max&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    max_val &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max_heap_maximum(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pop()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    max_heapify(A, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; max_val
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As we saw with &lt;a href=&#34;https://ajdillhoff.github.io/notes/heapsort/&#34;&gt;Heapsort&lt;/a&gt;, &lt;code&gt;max_heapify&lt;/code&gt; runs in \(O(\lg n)\) time. The call to &lt;code&gt;max_heap_extract_max&lt;/code&gt; only adds a few constant operations on top of that, so it runs in \(O(\lg n)\) time as well.&lt;/p&gt;
&lt;h3 id=&#34;increase&#34;&gt;Increase&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;max_heap_increase_key&lt;/code&gt; function is used to increase the key of a given element. The function first checks if the new key is less than the current key. If it is, an error is raised. The function then updates the key and then traverses up the heap to ensure that the heap property is maintained.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;max_heap_increase_key&lt;/span&gt;(A, obj, key):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; key &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; obj&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ValueError&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;New key is smaller than current key&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    obj&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;index(obj) &lt;span style=&#34;color:#75715e&#34;&gt;# gets the index of the object&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; A[parent(i)]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; A[i]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A[i], A[parent(i)] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[parent(i)], A[i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; parent(i)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Moving through the height of the tree is done in \(O(\lg n)\) time. Depending on the how the index of the object is found, the complexity could be higher. In most cases, the index is found in constant time.&lt;/p&gt;
&lt;h3 id=&#34;insert&#34;&gt;Insert&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;max_heap_insert&lt;/code&gt; function is used to insert a new element into the heap. The function first appends the new element to the end of the array. It then sets the key of the new element to a very small value and then calls &lt;code&gt;max_heap_increase_key&lt;/code&gt; to update the key to the correct value.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;max_heap_insert&lt;/span&gt;(A, obj, n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; len(A) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; n:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ValueError&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Heap overflow&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-inf&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    obj&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(obj)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# map obj to the last index -- dependent on the implementation&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    max_heap_increase_key(A, obj, key)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The call to &lt;code&gt;max_heap_increase_key&lt;/code&gt; runs in \(O(\lg n)\) time, so the &lt;code&gt;max_heap_insert&lt;/code&gt; function also runs in \(O(\lg n)\) time in addition to the time it takes to map the object to its index.&lt;/p&gt;
&lt;h2 id=&#34;exercises&#34;&gt;Exercises&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Implement a minimum priority queue.&lt;/li&gt;
&lt;li&gt;Implement a decrease key function for a maximum priority queue.&lt;/li&gt;
&lt;li&gt;Simulate a job scheduler using a priority queue that considers the priority of the job and the time it was submitted.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Heapsort</title>
      <link>https://ajdillhoff.github.io/notes/heapsort/</link>
      <pubDate>Wed, 21 Feb 2024 14:58:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/heapsort/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#maintaining-the-heap-property&#34;&gt;Maintaining the Heap Property&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#building-the-heap&#34;&gt;Building the Heap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#heapsort&#34;&gt;Heapsort&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;ul&gt;
&lt;li&gt;Running time is \(O(n \lg n)\).&lt;/li&gt;
&lt;li&gt;Sorts in place, only a constant number of elements needed in addition to the input.&lt;/li&gt;
&lt;li&gt;Manages data with a &lt;strong&gt;heap&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A &lt;strong&gt;binary heap&lt;/strong&gt; can be represented as a binary tree, but is stored as an array. The root is the first element of the array. The left subnode for the element at index \(i\) is located at \(2i\) and the right subnode is located at \(2i + 1\). &lt;strong&gt;This assumes a 1-based indexing&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Using 0-based indexing, we can use \(2i + 1\) for the left and \(2i + 2\) for the right. The parent could be accessed via \(\lfloor \frac{i-1}{2} \rfloor\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-21_15-22-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;A binary tree as a heap with its array representation (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;A binary tree as a heap with its array representation (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Heaps come in two flavors: &lt;strong&gt;max-heaps&lt;/strong&gt; and &lt;strong&gt;min-heaps&lt;/strong&gt;. They can be identified by satisfying a &lt;strong&gt;heap property&lt;/strong&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;max-heap property&lt;/strong&gt;: \(A[parent(i)] \geq A[i]\)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;min-heap property&lt;/strong&gt;: \(A[parent(i)] \leq A[i]\)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These properties imply that the root is the largest element in a max-heap and the smallest element in a min-heap.&lt;/p&gt;
&lt;p&gt;When it comes to heapsort, a max-heap is used. Min-heaps are used in priority queues. These notes will cover both.&lt;/p&gt;
&lt;h2 id=&#34;maintaining-the-heap-property&#34;&gt;Maintaining the Heap Property&lt;/h2&gt;
&lt;p&gt;When using heapsort, the heap should always satisfy the &lt;strong&gt;max-heap&lt;/strong&gt; property. This relies on a procedure called &lt;code&gt;max_heapify&lt;/code&gt;. This function assumes that the root element may violate the max-heap property, but the subtrees rooted by its subnodes are valid max-heaps. The function then swaps nodes down the tree until the misplaced element is in the correct position.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;max_heapify&lt;/span&gt;(A, i, heap_size):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    l &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; left(i)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    r &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; right(i)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    largest &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; l &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; heap_size &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; A[l] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; A[i]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        largest &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; l
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; r &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; heap_size &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; A[r] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; A[largest]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        largest &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; r
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; largest &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; i:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A[i], A[largest] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[largest], A[i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        max_heapify(A, largest, heap_size)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;analysis-of-max-heapify&#34;&gt;Analysis of Max Heapify&lt;/h3&gt;
&lt;p&gt;Given that &lt;code&gt;max_heapify&lt;/code&gt; is a recursive function, we can analyze it with a recurrence. The driving function in this case would be the fix up that happens between the current node and its two subnodes, which is a constant time operation. The recurrence is based on how many elements are in the subheap rooted at the current node.&lt;/p&gt;
&lt;p&gt;In the worst case of a binary tree, the last level of the tree is half full. That means that the left subtree has height \(h + 1\) compared to the right subtree&amp;rsquo;s height of \(h\). For a tree of size \(n\), the left subtree has \(2^{h+2}-1\) nodes and the right subtree has \(2^{h+1}-1\) nodes. This is based on a geometric series.&lt;/p&gt;
&lt;p&gt;We now have that the number of nodes in the tree is equal to \(1 + (2^{h+2}-1) + (2^{h+1}-1)\).&lt;/p&gt;
&lt;p&gt;\begin{align*}
n &amp;amp;= 1 + 2^{h+2} - 1 + 2^{h+1} - 1 \\
n &amp;amp;= 2^{h+2} + 2^{h+1} - 1 \\
n &amp;amp;= 2^{h+1}(2 + 1) - 1 \\
n &amp;amp;= 3 \cdot 2^{h+1} - 1
\end{align*}&lt;/p&gt;
&lt;p&gt;This implies that \(2^{h+1} = \frac{n+1}{3}\). That means that, in the worst case, the left subtree would have \(2^{h+2} - 1 = \frac{2(n+1)}{3} - 1\) nodes which is bounded by \(\frac{2n}{3}\). Thus, the recurrence for the worst case of &lt;code&gt;max_heapify&lt;/code&gt; is \(T(n) = T(\frac{2n}{3}) + O(1)\).&lt;/p&gt;
&lt;h2 id=&#34;building-the-heap&#34;&gt;Building the Heap&lt;/h2&gt;
&lt;p&gt;Given an array of elements, &lt;strong&gt;how do we build the heap in the first place?&lt;/strong&gt; The solution is to build it using a bottom-up approach from the leaves. The elements from \(\lfloor \frac{n}{2} \rfloor + 1\) to \(n\) are all leaves. This means that they are all 1-element heaps. We can then run &lt;code&gt;max_heapify&lt;/code&gt; on the remaining elements to build the heap.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;build_max_heap&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    heap_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(len(A) &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        max_heapify(A, i, heap_size)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;why-does-this-work&#34;&gt;Why does this work?&lt;/h3&gt;
&lt;p&gt;Each node starting at \(\lfloor \frac{n}{2} \rfloor + 1\) is the root of a 1-element heap. The subnodes, which are to the right of node \(\lfloor \frac{n}{2} \rfloor\), are roots of their own max-heaps. The procedure loops down to the first node until all sub-heaps have been max-heapified.&lt;/p&gt;
&lt;p&gt;The figure below is from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;) and shows the process of building a max-heap from an array.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-23_16-37-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Building a max-heap from an array (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Building a max-heap from an array (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;analysis-of-build-max-heap&#34;&gt;Analysis of Build Max Heap&lt;/h3&gt;
&lt;p&gt;A general analysis is fairly straightforward considering that the call to &lt;code&gt;max_heapify&lt;/code&gt; is \(O(\lg n)\). The loop in &lt;code&gt;build_max_heap&lt;/code&gt; runs \(O(n)\) times. This means that the overall running time is \(O(n \lg n)\). A more careful analysis can be done by considering the height of the tree and the number of nodes at each level.&lt;/p&gt;
&lt;p&gt;A heap of \(n\) elements has height \(\lfloor \lg n \rfloor\) Each call to &lt;code&gt;max_heapify&lt;/code&gt; can also be viewed in terms of the height of the tree \(h\), so the upper bound is \(O(h)\). This bounds &lt;code&gt;build_max_heap&lt;/code&gt; at \(\sum_{h=0}^{\lfloor \lg n \rfloor} \lceil \frac{n}{2^{h+1}} \rceil ch\). When \(h = 0\), the first term \(\lceil \frac{n}{2^{h+1}} \rceil = \lceil \frac{n}{2} \rceil\). When \(h = \lfloor \lg n \rfloor\), \(\lceil \frac{n}{2^{h+1}} \rceil = 1\). Thus, \(\lceil \frac{n}{2^{h+1}} \rceil \geq \frac{1}{2}\) for \(0 \leq h \leq \lfloor \lg n \rfloor\).&lt;/p&gt;
&lt;p&gt;Let \(x = \frac{n}{2^{h+1}}\). Since \(x \geq \frac{1}{2}\), we have that \(\lceil x \rceil \leq 2x\). This means that \(\lceil \frac{n}{2^{h+1}} \rceil \leq \frac{2n}{2^{h+1}} = \frac{n}{2^h}\). An upper bound can now be derived.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\sum_{h=0}^{\lfloor \lg n \rfloor} \lceil \frac{n}{2^{h+1}} \rceil ch &amp;amp;\leq \sum_{h=0}^{\lfloor \lg n \rfloor} \frac{n}{2^h} ch \\
&amp;amp;= cn \sum_{h=0}^{\lfloor \lg n \rfloor} \frac{h}{2^h} \\
&amp;amp;\leq cn \sum_{h=0}^{\infty} \frac{h}{2^h} \\
&amp;amp;\leq cn \cdot \frac{1 / 2}{(1 - 1/2)^2}\quad \text{(See CRLS for details)} \\
&amp;amp;= O(n)
\end{align*}&lt;/p&gt;
&lt;p&gt;Thus, a heap can be constructed in linear time. This is independent on whether the original data is already sorted.&lt;/p&gt;
&lt;h2 id=&#34;heapsort&#34;&gt;Heapsort&lt;/h2&gt;
&lt;p&gt;We now have all of the components necessary to implement heapsort. The algorithm is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;heapsort&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    build_max_heap(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    heap_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(len(A) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], A[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[i], A[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        heap_size &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        max_heapify(A, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, heap_size)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It starts by building a max-heap on the input array. As seen in the previous section, this is done in linear time. From there, it&amp;rsquo;s a matter of taking the root element out of the heap and then running &lt;code&gt;max_heapify&lt;/code&gt; to maintain the max-heap property. This is done \(n-1\) times, so the overall running time is \(O(n \lg n)\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-24_13-48-17_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Heapsort in action (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Heapsort in action (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Heapsort is visualized in the figure above, starting with a constructed max-heap in (a) (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;questions&#34;&gt;Questions&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;What is the running time of heapsort given an array that is already sorted in ascending order?&lt;/li&gt;
&lt;li&gt;What is the running time of heapsort given an array that is already sorted in descending order?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Master Theorem</title>
      <link>https://ajdillhoff.github.io/notes/master_theorem/</link>
      <pubDate>Sun, 04 Feb 2024 17:49:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/master_theorem/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#example-merge-sort&#34;&gt;Example: Merge Sort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-matrix-multiplication&#34;&gt;Example: Matrix Multiplication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-median-finding&#34;&gt;Example: Median Finding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-cormen-et-al-dot-exercise-4-dot-5-2&#34;&gt;Example: Cormen et al. Exercise 4.5-2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;In the study of &lt;a href=&#34;https://ajdillhoff.github.io/notes/divide_and_conquer_algorithms/&#34;&gt;Divide and Conquer Algorithms&lt;/a&gt;, a recurrence tree can be used to determine the runtime complexity. These notes focus on the &lt;strong&gt;master theorem&lt;/strong&gt;, a blueprint for solving any recurrence of the form&lt;/p&gt;
&lt;p&gt;\[
T(n) = aT(n/b) + f(n).
\]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(n\) is the size of the problem,&lt;/li&gt;
&lt;li&gt;\(a \geq 1\) is the number of subproblems,&lt;/li&gt;
&lt;li&gt;\(b &amp;gt; 1\) is the factor by which the problem size is reduced, and&lt;/li&gt;
&lt;li&gt;\(f(n)\) is the cost of the work done outside of the recursive calls.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each recurrence is solved in \(T(n/b)\) time, and \(f(n)\) would include the cost of dividing and recombining the problem. The full theorem as described in &lt;em&gt;Introduction to Algorithms&lt;/em&gt; is restated below (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Master Theorem&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let \(a &amp;gt; 0\) and \(b &amp;gt; 1\) be constants, and let \(f(n)\) be a driving function that is defined and nonnegative on all sufficiently large reals. Define the recurrence \(T(n)\) on \(n \in \mathbb{N}\) by&lt;/p&gt;
&lt;p&gt;\[
T(n) = aT(n/b) + f(n),
\]&lt;/p&gt;
&lt;p&gt;where \(aT(n/b)\) actually means \(a&amp;rsquo;T(\lfloor n/b \rfloor) + a{&amp;rsquo;&amp;rsquo;}T(\lceil n / b \rceil)\) for some constants \(a&amp;rsquo; \geq 0\) and \(a&amp;rsquo;&amp;rsquo; \geq 0\) such that \(a = a&amp;rsquo; + a&amp;rsquo;&amp;rsquo;\). Then \(T(n)\) has the following asymptotic bounds:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If \(f(n) = O(n^{\log_b a - \epsilon})\) for some constant \(\epsilon &amp;gt; 0\), then \(T(n) = \Theta(n^{\log_b a})\).&lt;/li&gt;
&lt;li&gt;If \(f(n) = \Theta(n^{\log_b a} \log^k n)\) for some constant \(k \geq 0\), then \(T(n) = \Theta(n^{\log_b a} \log^{k+1} n)\).&lt;/li&gt;
&lt;li&gt;If \(f(n) = \Omega(n^{\log_b a + \epsilon})\) for some constant \(\epsilon &amp;gt; 0\), and if \(a f(n/b) \leq k f(n)\) for some constant \(k &amp;lt; 1\) and all sufficiently large \(n\), then \(T(n) = \Theta(f(n))\).&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;theorem-breakdown&#34;&gt;Theorem Breakdown&lt;/h3&gt;
&lt;p&gt;The common function \(n^{\log_b a}\) is called the &lt;strong&gt;watershed function&lt;/strong&gt;. The driving function \(f(n)\) is compared to it to determine which case applies. If the watershed function grows at a faster rate than \(f(n)\), case 1 applies. If they grow at the same rate, case 2 applies. If \(f(n)\) grows at a faster rate, case 3 applies.&lt;/p&gt;
&lt;p&gt;In case 1, the watershed function should grow faster than \(f(n)\) by a factor of \(n^\epsilon\) for some \(\epsilon &amp;gt; 0\). In case 2, technically the watershed function should grow at least the same rate as \(f(n)\), if not faster. That is, it grows faster by a factor of \(\Theta(\log^k n)\), where \(k \geq 0\). You can think of the extra \(\log^k n\) as an augmentation to the watershed function to ensure that they grow at the same rate. In most cases, \(k = 0\) which results in \(T(n) = \Theta(n^{\log_b a} \log n)\).&lt;/p&gt;
&lt;p&gt;Since case 2 allows for the watershed function to grow faster than \(f(n)\), case 3 requires that it grow at least &lt;strong&gt;polynomially&lt;/strong&gt; faster. \(f(n)\) should grow faster by at least a factor of \(\Theta(n^\epsilon)\) for some \(\epsilon &amp;gt; 0\). Additionally, the driving function must satisfy the regularity condition \(a f(n/b) \leq k f(n)\) for some constant \(k &amp;lt; 1\) and all sufficiently large \(n\). This condition ensures that the cost of the work done outside of the recursive calls is not too large.&lt;/p&gt;
&lt;h3 id=&#34;application-of-the-master-method&#34;&gt;Application of the Master Method&lt;/h3&gt;
&lt;p&gt;In most cases, the master method can be applied by looking at the recurrence and applying the relevant case. If the driving and watershed functions are not immediately obvious, you can use a different method as discussed in &lt;a href=&#34;https://ajdillhoff.github.io/notes/divide_and_conquer_algorithms/&#34;&gt;Divide and Conquer Algorithms&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When this can be applied, it is much simpler than the other methods. Let&amp;rsquo;s revisit some of the main problems we&amp;rsquo;ve explored before discussing applications for which the master method could not be used.&lt;/p&gt;
&lt;h2 id=&#34;example-merge-sort&#34;&gt;Example: Merge Sort&lt;/h2&gt;
&lt;p&gt;Merge Sort has a recurrence of the form \(T(n) = 2T(n/2) + \Theta(n)\). The driving function is \(f(n) = \Theta(n)\). The constants \(a\) and \(b\) are both 2, so the watershed function is \(n^{\log^2 2}\), which is \(n\). Since \(f(n)\) grows at the same rate as the watershed function, case 2 applies. Therefore, \(T(n) = \Theta(n \log n)\).&lt;/p&gt;
&lt;h2 id=&#34;example-matrix-multiplication&#34;&gt;Example: Matrix Multiplication&lt;/h2&gt;
&lt;p&gt;The recurrence of the divide and conquer version of matrix multiplication for square matrices is \(T(n) = 8T(n/2) + \Theta(1)\). Given \(a = 8\) and \(b = 2\), we can see that the complexity is inherent in the recurrence, not the driving function. The watershed function is \(n^{\log_2 8}\), which is \(n^3\). This grows at a faster rate than \(\Theta(1)\), so case 1 applies. Therefore, \(T(n) = \Theta(n^3)\).&lt;/p&gt;
&lt;h2 id=&#34;example-median-finding&#34;&gt;Example: Median Finding&lt;/h2&gt;
&lt;p&gt;Median finding has a recurrence of the form \(T(n) = T(n/5) + T(7n/10) + \Theta(n)\). Given the two recurrence factors, how do we evaluate the driving function? The form itself does not fit the master theorem, so it cannot be applied in this case. We could use the substitution method, recurrence trees, or the Akra-Bazzi theorem to solve this one.&lt;/p&gt;
&lt;h2 id=&#34;example-cormen-et-al-dot-exercise-4-dot-5-2&#34;&gt;Example: Cormen et al. Exercise 4.5-2&lt;/h2&gt;
&lt;p&gt;In this exercise from &lt;em&gt;Introduction to Algorithms&lt;/em&gt;, we are asked to find the largest integer value \(a\) such that an algorithm with the recurrence \(T(n) = aT(n/4) + \Theta(n^2)\) is asymptotically faster than \(\Theta(n^{\log_2 7})\). Since \(b = 4\), the largest integer \(a\) will be the smallest integer such that \(\log_4 a &amp;lt; \log_2 7\). Solving for the inequality shows that \(a = 48\) is the largest such integer.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Divide and Conquer Algorithms</title>
      <link>https://ajdillhoff.github.io/notes/divide_and_conquer_algorithms/</link>
      <pubDate>Tue, 23 Jan 2024 08:38:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/divide_and_conquer_algorithms/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solving-recurrences&#34;&gt;Solving Recurrences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-merge-sort&#34;&gt;Example: Merge Sort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-multiplying-square-matrices&#34;&gt;Example: Multiplying Square Matrices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-convex-hull&#34;&gt;Example: Convex Hull&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-median-search&#34;&gt;Example: Median Search&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;Divide and conquer algorithms are a class of algorithms that solve a problem by breaking it into smaller subproblems, solving the subproblems recursively, and then combining the solutions to the subproblems to form a solution to the original problem. Problems that can be solved in this manner are typically highly parallelizable. These notes investigate a few examples of classic divide and conquer algorithms and their analysis.&lt;/p&gt;
&lt;p&gt;A divide and conquer method is split into three steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Divide the problem into smaller subproblems.&lt;/li&gt;
&lt;li&gt;Conquer the subproblems by solving them recursively.&lt;/li&gt;
&lt;li&gt;Combine the solutions to the subproblems to form a solution to the original problem.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Their runtime can be characterized by the recurrence relation \(T(n)\). A recurrence \(T(n)\) is &lt;em&gt;algorithmic&lt;/em&gt; if, for every sufficiently large &lt;em&gt;threshold&lt;/em&gt; constant \(n_0 &amp;gt; 0\), the following two properties hold:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For all \(n \leq n_0\), the recurrence defines the running time of a constant-size input.&lt;/li&gt;
&lt;li&gt;For all \(n \geq n_0\), every path of recursion terminates in a defined base case within a finite number of recursive calls.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The algorithm must output a solution in finite time.
If the second property doesn&amp;rsquo;t hold, the algorithm is not correct &amp;ndash; it may end up in an infinite loop.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Whenever a recurrence is stated without an explicit base case, we assume that the recurrence is algorithmic.&amp;rdquo; (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This assumption means that the algorithm is correct and terminates in finite time, so there must be a base case. The base case is less important for analysis than the recursive case. For example, your base case might work with 100 elements, and that would still be \(\Theta(1)\) because it is a constant.&lt;/p&gt;
&lt;p&gt;It is common to break up each subproblem uniformly, but it is not always the best way to do it. For example, an application such as matrix multiplication is typically broken up uniformly since there is no spatial or temporal relationship to consider. Algorithms for image processing, on the other hand, may have input values that are locally correlated, so it may be better to break up the input in a way that preserves this correlation.&lt;/p&gt;
&lt;h2 id=&#34;solving-recurrences&#34;&gt;Solving Recurrences&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Substitution method&lt;/li&gt;
&lt;li&gt;Recursion-tree method&lt;/li&gt;
&lt;li&gt;Master method&lt;/li&gt;
&lt;li&gt;Akra-Bazzi method&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-merge-sort&#34;&gt;Example: Merge Sort&lt;/h2&gt;
&lt;p&gt;Merge sort is a classic example of a divide and conquer algorithm. It works by dividing the input array into two halves, sorting each half recursively, and then merging the two sorted halves.&lt;/p&gt;
&lt;h3 id=&#34;divide&#34;&gt;Divide&lt;/h3&gt;
&lt;p&gt;The divide step takes an input subarray \(A[p:r]\) and computes a midpoint \(q\) before partitioning it into two subarrays \(A[p:q]\) and \(A[q+1:r]\). These subarrays will be sorted recursively until the base case is reached.&lt;/p&gt;
&lt;h3 id=&#34;conquer&#34;&gt;Conquer&lt;/h3&gt;
&lt;p&gt;The conquer step recursively sorts the two subarrays \(A[p:q]\) and \(A[q+1:r]\). If the base case is such that the input array has only one element, the array is already sorted.&lt;/p&gt;
&lt;h3 id=&#34;combine&#34;&gt;Combine&lt;/h3&gt;
&lt;p&gt;The combine step merges the two sorted subarrays to produce the final sorted array.&lt;/p&gt;
&lt;h3 id=&#34;python-implementation&#34;&gt;Python Implementation&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;merge_sort&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; len(A) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Conquer -- base case&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; A
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Divide Step&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    mid &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(A) &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; merge_sort(A[:mid])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; merge_sort(A[mid:])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Combine Step&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; merge(left, right)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;merge&lt;/span&gt;(left, right):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    i, j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Merge the two subarrays&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; (i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; len(left)) &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; (j &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; len(right)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; left[i] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; right[j]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            result&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(left[i])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            i &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            result&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(right[j])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            j &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Add the remaining elements to the final array&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    result &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; left[i:]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    result &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; right[j:]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; result
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This function assumes that the subarrays &lt;code&gt;left&lt;/code&gt; and &lt;code&gt;right&lt;/code&gt; are already sorted. If the value in the left subarray is less than the value in the right subarray, the left value is added to the final array. Otherwise, the right value is added. As soon as one of the subarrays is exhausted, the remaining elements in the other subarray are added to the final array. This is done with slicing in Python.&lt;/p&gt;
&lt;p&gt;The divide step simply splits the data into &lt;code&gt;left&lt;/code&gt; and &lt;code&gt;right&lt;/code&gt; subarrays. The conquer step simplifies the sorting process by reducing it down to the base case &amp;ndash; a single element. Finally, the combine step merges or &lt;em&gt;folds&lt;/em&gt; the two sorted subarrays together.&lt;/p&gt;
&lt;p&gt;Example code can be found &lt;a href=&#34;https://github.com/ajdillhoff/python-examples/blob/main/sorting/merge_sort.py&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;When analyzing the running time of a divide and conquer algorithm, it is safe to assume that the base case runs in constant time. The focus of the analysis should be on the &lt;strong&gt;recurrence equation&lt;/strong&gt;. For merge sort, we originally have a problem of size \(n\). We then divide the problem into 2 subproblems of size \(n/2\). Therefore the recurrence is \(T(n) = 2T(n/2)\). This recurrence continues for as long as the base case is not reached.&lt;/p&gt;
&lt;p&gt;Of course we also have to factor in the time it takes for the divide and combine steps. These can be represented as \(D(n)\) and \(C(n)\), respectively. The total running time of the algorithm is then \(T(n) = 2T(n/2) + D(n) + C(n)\) when \(n &amp;gt;= n_0\), where \(n_0\) is the base case.&lt;/p&gt;
&lt;p&gt;For merge sort specifically, the base case is \(D(n) = \Theta(1)\) since all it does is compute the midpoint. As we saw above, the conquer step is the recurrence \(T(n) = 2T(n/2)\). The combine step is \(C(n) = \Theta(n)\) since it takes linear time to merge the two subarrays. Thus, the worst-case running time of merge sort is \(T(n) = 2T(n/2) + \Theta(n)\).&lt;/p&gt;
&lt;p&gt;Not every problem will have a recurrence of \(2T(n/2)\). We can generalize this to \(aT(n/b)\), where \(a\) is the number of subproblems and \(b\) is the size of the subproblems.&lt;/p&gt;
&lt;p&gt;We haven&amp;rsquo;t finished the analysis yet since it is still not clear what the asymptotic upper bound is. &lt;strong&gt;Recurrence trees&lt;/strong&gt; can be used to visualize the running time of a divide and conquer algorithms. After inspecting the result of the tree, we will be able to easily determine the complexity of merge sort in terms of big-O notation.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-04_15-09-47_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Expansion of recursion tree for merge sort (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Expansion of recursion tree for merge sort (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the figure above from &lt;em&gt;Introduction to Algorithms&lt;/em&gt;, the root of the tree represents the original problem of size \(n\) in (a). In (b), the divide step splits the problem into two problems of size \(n/2\). The cost of this step is indicated by \(c_2n\). Here, \(c_2\) represents the constant cost per element for dividing and combining. As mentioned above, the combine step is dependent on the size of the subproblems, so the cost is \(c_2n\). Subfigure (c) shows a third split, where each new subproblem has size \(n/4\). This would continue recursively until the base case is reached, as shown in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-02-04_15-14-25_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Full recursion tree for merge sort (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Full recursion tree for merge sort (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The upper bound for each level of the tree is \(c_2n\). The height of a binary tree is \(\log_b n\). The total cost of the tree is the sum of the costs at each level. In this case, the cost is \(c_2n \log n + c_1n\), where the last \(c1_n\) comes from the base case. The first term is the dominating factor in the running time, so the running time of merge sort is \(\Theta(n \log n)\).&lt;/p&gt;
&lt;h3 id=&#34;questions&#34;&gt;Questions&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Assume that the base case is \(n &amp;gt; 1\). What is the running time of the conquer step dependent on?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;example-multiplying-square-matrices&#34;&gt;Example: Multiplying Square Matrices&lt;/h2&gt;
&lt;p&gt;Matrix multiplication is defined as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;square_matrix_multiply&lt;/span&gt;(A, B):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    C &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                C[i][j] &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; A[i][k] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; B[k][j]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; C
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Initializing values takes \(\Theta(n^2)\). The full process takes \(\Theta(n^3)\).&lt;/p&gt;
&lt;h3 id=&#34;divide-and-conquer&#34;&gt;Divide and Conquer&lt;/h3&gt;
&lt;p&gt;In this approach, the matrix will be split into block matrices of size \(n/2\). Each submatrix can be multiplied with the corresponding submatrix of the other matrix. The resulting submatrices can be added together to form the final matrix. This is permissible based on the definition of matrix multiplication.&lt;/p&gt;
&lt;p&gt;Base case is \(n=1\) where only a single addition and multiplication are performed. This is \(T(1) = \Theta(1)\). For \(n &amp;gt; 1\), the recursive algorithm starts by splitting into 8 subproblems of size \(n/2\). There are 8 subproblems because there are 4 submatrices in each matrix, and each submatrix is multiplied with the corresponding submatrix in the other matrix.&lt;/p&gt;
&lt;p&gt;Each recursive call contributes \(T(n/2)\) to the running time. There are 8 recursive calls, so the total running time is \(8T(n/2) + \Theta(n^2)\). There is no need to implement a combine step since the matrix is updated in place. The final running time is \(T(n) = 8T(n/2) + \Theta(1)\) &lt;strong&gt;for the recursive portion&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This method easily adapts to parallel processing. The size of each &lt;em&gt;tile&lt;/em&gt; can be adjusted to fit the number of processors available. The algorithm can be parallelized by assigning each processor to a subproblem.&lt;/p&gt;
&lt;p&gt;We now walk through an example on a \(4 \times 4\) matrix. Assume that each \(A_{ij}\) and \(B_{ij}\) is a \(2 \times 2\) matrix.&lt;/p&gt;
&lt;p&gt;\begin{bmatrix}
A_{11} &amp;amp; A_{12} \\
A_{21} &amp;amp; A_{22}
\end{bmatrix}&lt;/p&gt;
&lt;p&gt;\begin{bmatrix}
B_{11} &amp;amp; B_{12} \\
B_{21} &amp;amp; B_{22}
\end{bmatrix}&lt;/p&gt;
&lt;p&gt;These matrices are already partitioned. They currently don&amp;rsquo;t meet the base case, so 8 recursive calls are made which compute the following products:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;\(A_{11}B_{11}\)&lt;/li&gt;
&lt;li&gt;\(A_{12}B_{21}\)&lt;/li&gt;
&lt;li&gt;\(A_{11}B_{12}\)&lt;/li&gt;
&lt;li&gt;\(A_{12}B_{22}\)&lt;/li&gt;
&lt;li&gt;\(A_{21}B_{11}\)&lt;/li&gt;
&lt;li&gt;\(A_{22}B_{21}\)&lt;/li&gt;
&lt;li&gt;\(A_{21}B_{12}\)&lt;/li&gt;
&lt;li&gt;\(A_{22}B_{22}\)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Peeking into the first recursive call, the \(2 \times 2\) matrices are partitioned into 4 \(1 \times 1\) matrices, or scalars. The base case is reached, and the product is computed. The same process is repeated for the other 7 recursive calls. The final matrix is then formed by adding the products together.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;partition&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    mid &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A11 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [row[:mid] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; A[:mid]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A12 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [row[mid:] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; A[:mid]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A21 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [row[:mid] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; A[mid:]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A22 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [row[mid:] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; A[mid:]]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; A11, A12, A21, A22
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;matrix_multiply_recursive&lt;/span&gt;(A, B, C, n):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        C[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; A[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; B[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Partition the matrices&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A11, A12, A21, A22 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; partition(A)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        B11, B12, B21, B22 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; partition(B)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        C11, C12, C21, C22 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; partition(C)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Recursively compute the products&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        matrix_multiply_recursive(A11, B11, C11, n&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        matrix_multiply_recursive(A12, B21, C11, n&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        matrix_multiply_recursive(A11, B12, C11, n&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        matrix_multiply_recursive(A12, B22, C11, n&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        matrix_multiply_recursive(A21, B11, C11, n&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        matrix_multiply_recursive(A22, B21, C11, n&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        matrix_multiply_recursive(A21, B12, C11, n&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        matrix_multiply_recursive(A22, B22, C11, n&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;Each recursive call contributes \(T(n/2)\) to the running time. Unless the base case is reached, each call contributes 8 recursive calls to the recurrence, yielding a running time of \(T(n) = 8T(n/2) + \Theta(1)\).&lt;/p&gt;
&lt;h2 id=&#34;example-convex-hull&#34;&gt;Example: Convex Hull&lt;/h2&gt;
&lt;p&gt;Given \(n\) points in plane, the convex hull is the smallest convex polygon that contains all the points.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;No two points have the same \(x\) or \(y\) coordinate.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sequence of points on boundary in clockwise order as doubly linked list.&lt;/p&gt;

    
    
    
    
    
    &lt;figure&gt;
    
    &lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-19_11-46-30_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Convex Hull (source: Wikipedia)&#34; &gt;
    
    
    
    &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
      
      &lt;p&gt;
        &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Convex Hull (source: Wikipedia)
        
        
        
      &lt;/p&gt; 
    &lt;/figcaption&gt;
    
    &lt;/figure&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;naive-solution&#34;&gt;Naive Solution&lt;/h3&gt;
&lt;p&gt;Draw lines between each pair of points. If all other points are on the same side of the line, the line is part of the convex hull. This is \(\Theta(n^3)\).&lt;/p&gt;
&lt;h3 id=&#34;divide-and-conquer&#34;&gt;Divide and Conquer&lt;/h3&gt;
&lt;p&gt;Sort the points by \(x\) coordinate. Split into two halves by \(x\). Recursively find the convex hull of each half. Merge the two convex hulls.&lt;/p&gt;
&lt;h4 id=&#34;merging&#34;&gt;Merging&lt;/h4&gt;
&lt;p&gt;Find upper tangent and lower tangent.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why not just select the highest point from each half?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The highest point in each half may not be part of the convex hull. The question assumes that the two convex hulls are relatively close to each other.&lt;/p&gt;
&lt;h4 id=&#34;two-finger-algorithm&#34;&gt;Two Finger Algorithm&lt;/h4&gt;
&lt;p&gt;Start at the rightmost point of the left convex hull and the leftmost point of the right convex hull. Move the right finger clockwise and the left finger counterclockwise until the tangent is found. The pseudocode is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;merge_convex_hulls&lt;/span&gt;(left, right):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Find the rightmost point of the left convex hull&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    left_max &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(left, key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; p: p[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Find the leftmost point of the right convex hull&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    right_min &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(right, key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; p: p[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Find the upper tangent&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Move the right finger clockwise&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        right_max &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(right, key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; p: p[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; is_upper_tangent(left_max, right_max):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Move the left finger counterclockwise&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        left_max &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(left, key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; p: p[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Find the lower tangent&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Move the right finger clockwise&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        right_min &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(right, key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; p: p[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; is_lower_tangent(left_min, right_min):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Move the left finger counterclockwise&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        left_min &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(left, key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; p: p[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This runs in \(\Theta(n)\) time.&lt;/p&gt;
&lt;p&gt;Removing the lines that are not part of the convex hull require the cut and paste operations. Starting at the upper tangent, move clockwise along the right convex hull until you reach the point in the lower tangent of the right convex hull. Make the connection to the corresponding point on the left convex hull based on the lower tangent, then move clockwise until you reach the upper tangent of the left convex hull. This is \(\Theta(n)\).&lt;/p&gt;
&lt;h4 id=&#34;orientation-test&#34;&gt;Orientation Test&lt;/h4&gt;
&lt;p&gt;The orientation test is a technique from computational geometry which determines the orientation of three points. For our purposes, it will tell us if a third point lies below or above a given line segment. The orientation test is used to determine if a point is part of the convex hull.&lt;/p&gt;
&lt;p&gt;Given three points \(p\), \(q\), and \(r\), the orientation is determined by the sign of the cross product of the vectors \(\overrightarrow{pq}\) and \(\overrightarrow{pr}\). If the cross product is positive, the orientation is clockwise. If the cross product is negative, the orientation is counterclockwise. If the cross product is zero, the points are collinear.&lt;/p&gt;
&lt;p&gt;This is expressed by a simple formula:&lt;/p&gt;
&lt;p&gt;\[
\text{orientation}(p, q, r) = (q_y - p_y)(r_x - q_x) - (q_x - p_x)(r_y - q_y)
\]&lt;/p&gt;
&lt;p&gt;Consider the visualization below. Let \(p\) be a point in the left convex hull and \(q\) be a point in the right convex hull. The orientation test will tell us if \(r\) is above or below the line segment \(\overline{pq}\). If the test is negative, \(r\) is above the line segment and is part of the convex hull, and vice versa.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-19_18-53-57_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Visualization of the orientation test.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Visualization of the orientation test.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;When checking to see if a line is an upper tangent, consider the points \(p\), \(q\), and \(r\), where \(p\) is from the left convex hull and \(q\) is from the right convex hull. Let \(r\) be the point immediately after \(q\) in a clockwise direction.&lt;/p&gt;
&lt;h2 id=&#34;example-median-search&#34;&gt;Example: Median Search&lt;/h2&gt;
&lt;p&gt;Finding the median value of a set can be performed in linear time without fully sorting the data. The recurrence is based on discarding a constant fraction of the elements at each step.&lt;/p&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Divide&lt;/strong&gt;&lt;/strong&gt;: Partition the set into groups of 5 elements. Depending on the size of the set, there may be less than 5 elements in the last set.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Conquer&lt;/strong&gt;&lt;/strong&gt;: Sort each group and find the median of each group. Since the subsets are of constant size, this is done in constant time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Combine&lt;/strong&gt;&lt;/strong&gt;: Given the median of each group from step 2, find the median of medians. This value will be used as a pivot for the next step.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Partition&lt;/strong&gt;&lt;/strong&gt;: Use the pivot to separate values smaller and larger than the pivot.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Select&lt;/strong&gt;&lt;/strong&gt;: If the given pivot is the true median based on its position in the original set, select it. If not, recursively select the median from the appropriate partition.&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-03-31_18-33-21_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Visualization of median of medians (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Visualization of median of medians (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Given a set of \(n\) numbers, define \(rank(X)\) as the number in the set that are less than or equal to \(X\).&lt;/p&gt;
&lt;p&gt;\(Select(S, i)\)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pick \(x \in S\)&lt;/li&gt;
&lt;li&gt;Compute \(k = rank(x)\)&lt;/li&gt;
&lt;li&gt;B = {y in S | y &amp;lt; x}&lt;/li&gt;
&lt;li&gt;C = {y in S | y &amp;gt; x}&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If \(i = k\), return \(x\).
else if \(k &amp;gt; i\), return \(Select(B, i)\).
else return \(Select(C, i-k)\).&lt;/p&gt;
&lt;p&gt;How do we get balanced partitions?&lt;/p&gt;
&lt;p&gt;Arrange \(S\) into columns of size 5.
Sort each column descending (linear time).
Find &amp;ldquo;median of medians&amp;rdquo; as \(X\)&lt;/p&gt;
&lt;p&gt;If the columns are sorted, it is trivial to find the median of each column.&lt;/p&gt;
&lt;p&gt;Half of the groups contribute at least 3 elements greater than \(X\), except for the last group. We have one group that contains \(x\).&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;Given a set \(A = \{a_1, a_2, \ldots, a_n\}\), the median search algorithm returns the $k$th smallest element of \(A\). We now analyze the runtime of this algorithm.&lt;/p&gt;
&lt;h4 id=&#34;divide&#34;&gt;Divide&lt;/h4&gt;
&lt;p&gt;In the &lt;strong&gt;divide&lt;/strong&gt; step, the set is partitioned into \(\lceil n/5 \rceil\) groups of 5 elements each. This is done in linear time.&lt;/p&gt;
&lt;h4 id=&#34;conquer&#34;&gt;Conquer&lt;/h4&gt;
&lt;p&gt;Each group is sorted using insertion sort or some other algorithm. Even though the search itself may have a higher complexity, the sorting of the groups is \(\Theta(1)\) since the groups are of constant size. Collectively across all groups, the sorting is \(\Theta(n)\).&lt;/p&gt;
&lt;h4 id=&#34;combine&#34;&gt;Combine&lt;/h4&gt;
&lt;p&gt;The median of each group is found, introducing a recurrence of \(T(\lceil n/5 \rceil)\). Once each median is found, the median of medians is computed.&lt;/p&gt;
&lt;h4 id=&#34;partition&#34;&gt;Partition&lt;/h4&gt;
&lt;p&gt;As reasoned above, the median of medians is used as a pivot to partition the set into two groups. At least 30% of the elements are greater than the pivot. This leaves a search space of at most \(7n/10\). Partitioning is done in linear time.&lt;/p&gt;
&lt;h4 id=&#34;select&#34;&gt;Select&lt;/h4&gt;
&lt;p&gt;If the pivot is the $k$th smallest element, the algorithm terminates. Otherwise, the algorithm recursively searches the appropriate partition. Comparing the size of the partition to the size of the original set, the recurrence is \(T(7n/10)\).&lt;/p&gt;
&lt;p&gt;Pseudocode is available &lt;a href=&#34;https://en.wikipedia.org/wiki/Median_of_medians&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Minimum Spanning Trees</title>
      <link>https://ajdillhoff.github.io/notes/minimum_spanning_trees/</link>
      <pubDate>Sat, 21 Oct 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/minimum_spanning_trees/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#finding-the-minimum-spanning-tree&#34;&gt;Finding the Minimum Spanning Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kruskal-s-algorithm&#34;&gt;Kruskal&amp;rsquo;s Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#prim-s-algorithm&#34;&gt;Prim&amp;rsquo;s Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Minimum spanning trees are undirected graphs that connect all of the vertices such that there are no redundant edges and the total weight is minimized. They are useful for finding the shortest path between two points in a graph. Useful application of MSTs include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;network design&lt;/strong&gt;: it is useful to know the least expensive path with respect to either latency or resource cost for telecommunications networks, transportation networks, or electrical grids.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;approximation algorithms&lt;/strong&gt;: MSTs can be used to approximate the solution to the traveling salesman problem.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;clustering&lt;/strong&gt;: MSTs can be used to cluster data points in a graph.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;image segmentation&lt;/strong&gt;: MSTs can be used to segment images into smaller regions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;Let \(G\) be a connected, undirected graph with edges \(E\), vertices \(V\), and edge weights \(w\). A &lt;strong&gt;minimum spanning tree&lt;/strong&gt; is a subset \(T \subseteq E\) that connects all of the vertices such that the total weight is minimized. The original graph \(G\) is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-21_18-46-30_undirected_original.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;An undirected graph with redundant edges.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;An undirected graph with redundant edges.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The minimum spanning tree of the above graph is show below. All of the redundant edges have been removed, but there is still a path between each pair of nodes.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-21_18-56-15_mst.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;The minimum spanning tree of (G).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;The minimum spanning tree of (G).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;As described in &lt;em&gt;Introduction to Algorithms&lt;/em&gt; there are two greedy algorithms for finding the minimum spanning tree of a graph (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;). These notes will review both of these, but first let&amp;rsquo;s look at a general algorithm for finding the minimum spanning tree of a graph.&lt;/p&gt;
&lt;h2 id=&#34;finding-the-minimum-spanning-tree&#34;&gt;Finding the Minimum Spanning Tree&lt;/h2&gt;
&lt;p&gt;The general algorithm for finding the minimum spanning tree of a graph grows a set of edges \(T\) from an empty set. At each step, the algorithm adds the edge with the smallest weight that does not create a cycle. The algorithm terminates when \(T\) is a complete tree.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;T = {}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;while T is not a spaning tree
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    find the edge e with the smallest weight that does not create a cycle
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    T = T union {e}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Each edge \(e\) that is added must result in a tree that is a subset of the minimum spanning tree. The challenge of this algorithm is actually finding such an edge. How would we know such an edge if we saw it? We first need to define a few properties which will shine light on this.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;cut&lt;/strong&gt; of a graph \(G\) is a partition of the vertices \(V\) into two disjoint sets \(S\) and \(V - S\). An edge \(e\) &lt;strong&gt;crosses&lt;/strong&gt; the cut if one of its endpoints is in \(S\) and the other is in \(V - S\). If no edge in a given set \(E\) crosses the cut, then that cut &lt;strong&gt;respects&lt;/strong&gt; \(E\). An edge that is the minimum weight edge that crosses a cut is called a &lt;strong&gt;light edge&lt;/strong&gt;. With these definitions, we can now formally define how to find a &lt;strong&gt;safe edge&lt;/strong&gt;, which is an edge that can be added to the current set of edges \(T\) without creating a cycle.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Theorem 21.1 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;)&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let \(G = (V, E)\) be a connected, undirected graph with a real-valued weight function \(w\) defined on \(E\). Let \(A\) be a subset of \(E\) that is included in some minimum spanning tree for \(G\), let \((S, V - S)\) be any cut of \(G\) that respects \(A\), and let \(e\) be a light edge crossing \((S, V - S)\). Then, edge \(e\) is safe for \(A\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-22_13-10-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Visual proof of Theorem 21.1 (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Visual proof of Theorem 21.1 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The two sets in the figure above represent vertices in \(S\) (orange) and vertices in \(V - S\) (tan). \(T\) is the original MST depicted in the figure. The dotted line is the new edge \((u, v)\) to consider. \(A\) is a subset of edges in \(T\) represented by the blue lines. If the safe edge \((u, v)\) is already in the original MST \(T\), then we are done.&lt;/p&gt;
&lt;p&gt;The vertices \(u\) and \(v\) lie on opposite sides of the cut. The edge \((u, v)\) would introduce a cycle since there is already a path from \(u\) to \(v\) in \(T\) that crosses the cut via \((x, y)\). Since both \((u, v)\) and \((x, y)\) are light edges that cross the cut, then it must be that \(w(u, v) \leq w(x, y)\).&lt;/p&gt;
&lt;p&gt;Let \(T&amp;rsquo;\) be the minimum spanning tree with \((x, y)\) replaced by \((u, v)\). That is \(T&amp;rsquo; = T - \{(x, y)\} \cup \{(u, v)\}\). Since \(T\) is a minimum spanning tree, then \(w(T) \leq w(T&amp;rsquo;)\). Since \(w(T) = w(T&amp;rsquo;)\), then \(T&amp;rsquo;\) is also a minimum spanning tree. Therefore, \((u, v)\) is safe for \(A\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Corollary 21.2 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;)&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We can also view this in terms of &lt;strong&gt;connected components&lt;/strong&gt;, which are subsets of vertices that are connected by a path. If \(C\) and \(C&amp;rsquo;\) are two connected components in \(T\) and \((u, v)\) is a light edge connecting \(C\) and \(C&amp;rsquo;\), then \((u, v)\) is safe for \(T\).&lt;/p&gt;
&lt;p&gt;The figure below shows a graph with two individual components. If the edge \((u, v)\) is a light edge, then it is safe to add it to the set of edges \(T\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-22_15-39-37_connected_components.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Two connected components from a graph (left). Adding a safe edge (right).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Two connected components from a graph (left). Adding a safe edge (right).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;kruskal-s-algorithm&#34;&gt;Kruskal&amp;rsquo;s Algorithm&lt;/h2&gt;
&lt;p&gt;The first solution to the minimum spanning tree that we will study is called &lt;strong&gt;Kruskal&amp;rsquo;s algorithm&lt;/strong&gt;. This algorithm grows a forest of trees from an empty set. At each step, the algorithm adds the lightest edge that does not create a cycle. The algorithm terminates when the forest is a single tree. This can be viewed as an agglomerative clustering algorithm. The algorithm starts with each vertex in its own cluster. At each step, the algorithm merges the two clusters that are closest together. The algorithm terminates when there is only one cluster.&lt;/p&gt;
&lt;p&gt;The algorithm is given below (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Kruskal&amp;rsquo;s Algorithm&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A = {}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;for each vertex v in G.V
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    MAKE-SET(v)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sort the edges of G.E into nondecreasing order by weight w
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;for each edge (u, v) in G.E, taken in nondecreasing order by weight
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    if FIND-SET(u) != FIND-SET(v)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A = A union {(u, v)}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        UNION(u, v)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;return A
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;A step-by-step example of an implementation in Python is available &lt;a href=&#34;https://github.com/ajdillhoff/python-examples/blob/main/data_structures/graphs/kruskals_algorithm.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;The running time is dependent on how the disjoint-set of vertices is implemented. In the best known case, a &lt;em&gt;disjoint-set-forest&lt;/em&gt; implementation should be used (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;). Creating a list of edges takes \(O(E)\) time. Sorting the edges takes \(O(E \log E)\) time. The &lt;code&gt;for&lt;/code&gt; loop iterates over each edge, which is \(O(E)\). All disjoin-set operations take \(O((V + E)\alpha(V))\) time. Since the graph is connected, \(E \geq V - 1\), so the total running time is \(O(E \log E + E + E \alpha(V)) = O(E \log E + E \alpha(V)) = O(E \log V)\).&lt;/p&gt;
&lt;h2 id=&#34;prim-s-algorithm&#34;&gt;Prim&amp;rsquo;s Algorithm&lt;/h2&gt;
&lt;p&gt;The second solution starts at an arbitrary vertex in a set \(A\) and adds a new vertex to \(A\) in a greedy fashion. To efficiently select a new edge to add, Prim&amp;rsquo;s algorithm uses a priority queue to keep track of the lightest edge that crosses the cut. The algorithm terminates when \(A\) is a complete tree. The full algorithm is given below. We will step through it in more detail after.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Prim&amp;rsquo;s Algorithm&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A = {}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;for each vertex v in G.V
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    key[v] = infinity
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    pi[v] = NIL
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;key[r] = 0
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Q = G.V
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;while Q is not empty
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    u = EXTRACT-MIN(Q)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A = A union {u}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    for each vertex v in G.Adj[u]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        if v in Q and w(u, v) &amp;lt; key[v]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            pi[v] = u
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            key[v] = w(u, v)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You might look at this and wonder how the MST is represented. Prim&amp;rsquo;s algorithm implicitly maintains the set \(A = \{(v, v.\pi) : v \in V - \{r\} - Q\}\). When the &lt;code&gt;while&lt;/code&gt; loop terminates, \(A = \{(v, v.\pi) : v \in V - \{r\}\}\), since the queue is empty. The critical part of this is to understand how the algorith changes the key values.&lt;/p&gt;
&lt;p&gt;A step-by-step example of an implementation in Python is available &lt;a href=&#34;https://github.com/ajdillhoff/python-examples/blob/main/data_structures/graphs/prims_algorithm.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;Prim&amp;rsquo;s algorithm uses a priority queue to keep track of the lightest edge that crosses the cut. If the priority queue is implemented as a &lt;a href=&#34;https://www.cs.cmu.edu/~tcortina/15-121sp10/Unit06B.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;min-heap&lt;/a&gt;, which has a worst-case running time of \(O(\log V)\) for both &lt;code&gt;EXTRACT-MIN&lt;/code&gt; and &lt;code&gt;DECREASE-KEY&lt;/code&gt;. The algorithm calls &lt;code&gt;EXTRACT-MIN&lt;/code&gt; once for each vertex, which is \(O(V \log V)\). The algorithm calls &lt;code&gt;DECREASE-KEY&lt;/code&gt; once for each edge, which is \(O(E \log V)\). The total running time is \(O(V \log V + E \log V) = O(E \log V)\).&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Single-Source Shortest Paths</title>
      <link>https://ajdillhoff.github.io/notes/single_source_shortest_paths/</link>
      <pubDate>Sat, 21 Oct 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/single_source_shortest_paths/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bellman-ford&#34;&gt;Bellman-Ford&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#shortest-paths-on-a-dag&#34;&gt;Shortest Paths on a DAG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dijkstra-s-algorithm&#34;&gt;Dijkstra&amp;rsquo;s Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;When you hear the term &lt;em&gt;shortest path&lt;/em&gt;, you may think of the shortest physical distance between your current location and wherever it is you&amp;rsquo;re going. Finding the most optimal route via GPS is one of the most widely used mobile applications. Physical paths are not the only types we may wish to find a shortest path for. Other examples include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Network Routing&lt;/strong&gt;: To improve network performance, it is critical to know the shortest path from one system to another in terms of latency.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Puzzle Solving&lt;/strong&gt;: For puzzles such as a Rubik&amp;rsquo;s cube, the vertices could represents states of the cube and edges could correspond to a single move.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Robotics&lt;/strong&gt;: Shortest paths in terms of robotics have a lot to do with physical distances, but it could also relate the completing a task efficiently.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These notes will cover classical single-source shortest path algorithms, but first we must formally define the problem.&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;Given a weighted, directed graph \(G = (V, E)\) with weight function \(w: E \rightarrow \mathbb{R}\), a source vertex \(s \in V\), and a destination vertex \(t \in V\), find the shortest path from \(s\) to \(t\). The weight of a path is defined as the sum of the weights of its edges:&lt;/p&gt;
&lt;p&gt;\[
w(p) = \sum_{e \in p} w(e).
\]&lt;/p&gt;
&lt;p&gt;The shortest-path weight between two vertices \(u\) and \(v\) is given by&lt;/p&gt;
&lt;p&gt;\[
\delta(u, v) = \begin{cases}
\min_{p \in P(u, v)} w(p) &amp;amp; \text{if } P(u, v) \neq \emptyset \\
\infty &amp;amp; \text{otherwise}
\end{cases}
\]&lt;/p&gt;
&lt;p&gt;where \(P(u, v)\) is the set of all paths from \(u\) to \(v\). The shortest-path weight from \(s\) to \(t\) is given by \(\delta(s, t)\).&lt;/p&gt;
&lt;p&gt;The output of a shortest-path algorithm will produce, for each vertex \(v \in V\):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(v.d\): The shortest-path estimate from \(s\) to \(v\).&lt;/li&gt;
&lt;li&gt;\(v.\pi\): The predecessor of \(v\) in the shortest path from \(s\) to \(v\).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Shortest-path algorithms rely on an optimal substructure property that is defined by Lemma 22.1 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 22.1&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Given a weighted, directed graph \(G = (V,E)\) with weight function \(w: E \rightarrow \mathbb{R}\), let \(p = \langle v_0, v_1, \dots, v_k \rangle\) be a shortest path from vertex \(v_0\) to vertex \(v_k\). For any \(i\) and \(j\) such that \(0 \leq i \leq j \leq k\), let \(p_{ij} = \langle v_i, v_{i+1}, \dots, v_j \rangle\) be the subpath of \(p\) from vertex \(v_i\) to vertex \(v_j\). Then, \(p_{ij}\) is a shortest path from \(v_i\) to \(v_j\).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is also important to note here that a shortest path should contain no cycles. Some shortest-path algorithms require that the edge weights be strictly positive. For those that do not, they may have some mechanism for detecting negative-weight cycles. In any case, a cycle of any kind cannot be included in a shortest path. This is because if a cycle were included, we could simply traverse the cycle as many times as we wanted to reduce the weight of the path. For positive-weight cycles, if a shortest path included a cycle, then surely we could remove the cycle to get a lower weight.&lt;/p&gt;
&lt;p&gt;As we build a shortest path, we need to keep track of which vertices lead us from the source to the destination. Some algorithms maintain this by keeping a &lt;strong&gt;predecessor&lt;/strong&gt; attribute for each vertex in the path. Solutions such as the Viterbi algorithm keep an array of indices that correspond to the vertices in the path. In any case, we will need to keep track of the vertices in the path as we build it.&lt;/p&gt;
&lt;h3 id=&#34;relaxation&#34;&gt;Relaxation&lt;/h3&gt;
&lt;p&gt;There is one more important property to define before discussing specific algorithms: &lt;strong&gt;relaxation&lt;/strong&gt;. Relaxing an edge \((u, v)\) is to test whether going through vertex \(u\) improves the shortest path to \(v\). If so, we update the shortest-path estimate and predecessor of \(v\) to reflect the new shortest path. Relaxation requires that we maintain the shortest-path estimate and processor for each vertex. This is initialized as follows.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;initialize_single_source&lt;/span&gt;(G, s):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;V:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;inf&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    s&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;When the values are changed, we say that the vertex has been &lt;strong&gt;relaxed&lt;/strong&gt;. Relaxing an edge \((u, v)\) is done as follows.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;relax&lt;/span&gt;(u, v, w):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; w(u, v):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; w(u, v)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; u
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;properties&#34;&gt;Properties&lt;/h4&gt;
&lt;p&gt;Relaxation has the following properties.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If the shortest-path estimate of a vertex is not \(\infty\), then it is always an upper bound on the weight of a shortest path from the source to that vertex.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The shortest-path estimate of a vertex will either stay the same or decrease as the algorithm progresses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once a vertex&amp;rsquo;s shortest-path estimate is finalized, it will never change.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The shortest-path estimate of a vertex is always greater than or equal to the actual shortest-path weight.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After \(i\) iterations of relaxing on all \((u, v)\), if the shortest path to \(v\) has \(i\) edges, then \(v.d = \delta(s, v)\).&lt;/p&gt;
&lt;p&gt;Following &lt;em&gt;Introduction to Algorithms&lt;/em&gt;, we will first discuss the Bellman-Ford algorithm, which has a higher runtime but works with graphs that have negative edge weights. Then, we will discuss Dijkstra&amp;rsquo;s algorithm, which has a lower runtime but only works with graphs that have non-negative edge weights.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;bellman-ford&#34;&gt;Bellman-Ford&lt;/h2&gt;
&lt;p&gt;The Bellman-Ford algorithm is a dynamic programming algorithm that solves the single-source shortest-paths problem in the general case in which edge weights may be negative. If a negative-weight cycle is reachable from the source, then the algorithm will report its existence. Otherwise, it will report the shortest-path weights and predecessors. It works by relaxing edges, decreasing the shortest-path estimate on the weight of a shortest path from \(s\) to each vertex \(v\) until it reaches the shortest-path weight.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bellman_ford&lt;/span&gt;(G, w, s):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    initialize_single_source(G, s)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, len(G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;V)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (u, v) &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;E:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            relax(u, v, w)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (u, v) &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;E:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; w(u, v):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;In the figure below, graph (a) shows the original graph before iterating over the edges. Graphs (b)-(e) show the result of looping over both edges originating from \(s\). Depending on the implementation, the first iteration of the vertices would result directly in graph (c). You can find a Python implementation of this example &lt;a href=&#34;https://github.com/ajdillhoff/python-examples/blob/main/data_structures/graphs/bellman_ford_algorithm.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-24_21-05-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Step-by-step execution of Bellman-Ford on a graph with negative-weight edges (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Step-by-step execution of Bellman-Ford on a graph with negative-weight edges (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;correctness&#34;&gt;Correctness&lt;/h3&gt;
&lt;p&gt;Bellman-Ford is guaranteed to converge after \(|V| - 1\) iterations, assuming no negative-weight cycles.&lt;/p&gt;
&lt;h4 id=&#34;proof&#34;&gt;Proof&lt;/h4&gt;
&lt;p&gt;The first iteration relaxes \((v_0, v_1)\). The second iteration relaxes \((v_1, v_2)\), and so on. The &lt;strong&gt;path-relaxation&lt;/strong&gt; property from before implies that \(v.d = v_k.d = \delta(s, v_k) = \delta(s, v)\). If there is a negative-weight cycle, then the shortest path to \(v_k\) is not well-defined. This is verified in the final loop over the edges.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (u, v) &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;E:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; w(u, v):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If there exists a negative-weight cycle \(c = \langle v_0, v_1, \dots, v_k \rangle\), where \(v_0 = v_k\) that can be reached from \(s\), then&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^{k} w(v_{i-1}, v_i) &amp;lt; 0.
\]&lt;/p&gt;
&lt;p&gt;To complete the proof by contradiction, assume that Bellman-Ford returns &lt;code&gt;True&lt;/code&gt;. Then we would have that \(v_i.d \leq v_{i-1}.d + w(v_{i-1}, v_i)\) for \(i = 1, 2, \dots, k\) by the &lt;strong&gt;triangle inequality&lt;/strong&gt; property. If we sum around the cycle, we get&lt;/p&gt;
&lt;p&gt;\begin{align*}
\sum_{i=1}^k v_i.d &amp;amp;\leq \sum_{i=1}^k (v_{i-1}.d + w(v_{i-1}, v_i))\\
&amp;amp;= \sum_{i=1}^k v_{i-1}.d + \sum_{i=1}^k w(v_{i-1}, v_i)\\
\end{align*}&lt;/p&gt;
&lt;p&gt;Since the vertices are in a cycle, each vertex appears only once in each summation \(\sum_{i=1}^k v_{i}.d\) and \(\sum_{i=1}^k v_{i-1}.d\). Subtracting this from both sides of the inequality, we get&lt;/p&gt;
&lt;p&gt;\[
0 \leq \sum_{i=1}^k w(v_{i-1}, v_i).
\]&lt;/p&gt;
&lt;p&gt;This contradicts the assumption that there is a negative-weight cycle. Therefore, if Bellman-Ford returns &lt;code&gt;True&lt;/code&gt;, then there are no negative-weight cycles.&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;Using an adjacency list representation, the runtime of Bellman-Ford is \(O(V^2 + VE)\). The initialization takes \(\Theta(V)\). Each of the \(|V| - 1\) iterations over the edges takes \(\Theta(V + E)\), and the final check for negative-weight cycles takes \(\Theta(V + E)\). If the number of edges and vertices is such that the number of vertices are a lower bound on the edges, then the runtime is \(O(VE)\).&lt;/p&gt;
&lt;h3 id=&#34;example-22-dot-1-1&#34;&gt;Example 22.1-1&lt;/h3&gt;
&lt;p&gt;Run Bellman-Ford on the given path using \(z\) as the source. Then change the weight of \((z, x)\) to 4 and run it again with \(s\) as the source.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-22_11-13-14_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Figure 22.4 from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Figure 22.4 from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;shortest-paths-on-a-dag&#34;&gt;Shortest Paths on a DAG&lt;/h2&gt;
&lt;p&gt;If we are given a directed acyclic graph (DAG), we can solve the single-source shortest path problem in \(O(V + E)\) time. By definition, the graph has no cycles and thus no negative-weight cycles.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dag_shortest_paths&lt;/span&gt;(G, w, s):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    initialize_single_source(G, s)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; u &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; topological_sort(G):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;adj[u]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            relax(u, v, w)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Run &lt;code&gt;dag_shortest_paths&lt;/code&gt; on the graph given below with \(s\) as the source.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-04-22_11-23-05_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Figure 22.5 from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Figure 22.5 from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;The runtime of &lt;code&gt;dag_shortest_paths&lt;/code&gt; is \(O(V + E)\), where \(V\) is the number of vertices and \(E\) is the number of edges. The topological sort takes \(O(V + E)\) time. Initializing the vertices takes \(O(V)\) time. The first &lt;code&gt;for&lt;/code&gt; loop makes on iteration per vertex, and the inner loop relaxes each edge only once.&lt;/p&gt;
&lt;h2 id=&#34;dijkstra-s-algorithm&#34;&gt;Dijkstra&amp;rsquo;s Algorithm&lt;/h2&gt;
&lt;p&gt;Dijkstra&amp;rsquo;s algorithm also solves the single-source shortest path problem on a weighted, directed graph \(G = (V,E)\) but requires nonnegative weights on all edges. It works in a breadth-first manner. A minimum priority queue is utilized to keep track of the vertices that have not been visited based on their current minimum shortest-path estimate. The algorithm works by relaxing edges, decreasing the shortest-path estimate on the weight of a shortest path from \(s\) to each vertex \(v\) until it reaches the shortest-path weight.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dijkstra&lt;/span&gt;(G, w, s):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    initialize_single_source(G, s)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    S &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;V
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; Q:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        u &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; extract_min(Q)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        S&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(u)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; G&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;adj[u]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            prev_d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            relax(u, v, w)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;d &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; prev_d:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                decrease_key(Q, v)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;A Python example of the figure below is available &lt;a href=&#34;https://github.com/ajdillhoff/python-examples/blob/main/data_structures/graphs/dijkstras_algorithm.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-25_08-21-04_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;A step-by-step execution of Dijkstra&amp;#39;s algorithm on a graph with non-negative edge weights (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Cormen et al. 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;A step-by-step execution of Dijkstra&amp;rsquo;s algorithm on a graph with non-negative edge weights (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cormen et al. 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;See Chapter 22 of &lt;em&gt;Introduction to Algorithms&lt;/em&gt; for a detailed analysis of Dijkstra&amp;rsquo;s algorithm. Inserting the nodes and then extracting them from the queue yields \(O(V \log V)\). After extracting a node, its edges are iterated with a possible update to the queue. This takes \(O(E \log V)\). The total runtime is \(O((V + E) \log V)\).&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2022. &lt;i&gt;Introduction to Algorithms&lt;/i&gt;. 4th ed. MIT Press. &lt;a href=&#34;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&#34;&gt;http://mitpress.mit.edu/9780262046305/introduction-to-algorithms/&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Binary Search Trees</title>
      <link>https://ajdillhoff.github.io/notes/binary_search_trees/</link>
      <pubDate>Tue, 10 Oct 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/binary_search_trees/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#binary-search-trees&#34;&gt;Binary Search Trees&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#operations&#34;&gt;Operations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#analysis&#34;&gt;Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;A $n$-ary tree is a graph-based data structure in which each node has up to \(n\) subnodes. It is supported by the following operations (not exclusive):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Search&lt;/li&gt;
&lt;li&gt;Insert&lt;/li&gt;
&lt;li&gt;Delete&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Tree-based data structures are defined by the following properties.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;&lt;strong&gt;size&lt;/strong&gt;&lt;/strong&gt; of a tree \(T\) is determined by the total number of nodes in \(T\).&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;&lt;strong&gt;root&lt;/strong&gt;&lt;/strong&gt; of a tree \(T\) is the starting point of \(T\).&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;&lt;strong&gt;leaf node&lt;/strong&gt;&lt;/strong&gt; of a tree \(T\) is a node that has no sub-nodes.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;&lt;strong&gt;height&lt;/strong&gt;&lt;/strong&gt; of a tree is determined by the length of the shortest path between the root of \(T\) and the lowest leaf node of \(T\).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we limit the number of subnodes each node may have to 2, the structure becomes known as a &lt;strong&gt;&lt;strong&gt;binary tree&lt;/strong&gt;&lt;/strong&gt;. Limiting the structure in this way is of interest to us because of the efficiency benefits seen in operations applied to binary trees. If we narrow the scope of these trees further, we can define a &lt;strong&gt;&lt;strong&gt;binary search tree&lt;/strong&gt;&lt;/strong&gt; whose search operation, as the name might suggest, runs in \(\Theta(lg n)\) worst-case time.&lt;/p&gt;
&lt;h2 id=&#34;binary-search-trees&#34;&gt;Binary Search Trees&lt;/h2&gt;
&lt;p&gt;A binary search tree is a regular binary tree with references to the left, right, and parent nodes, defined by the following property:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Let \(x\) be a node in a binary search tree. If \(y\) is a node in the left subtree of \(x\), then \(y.key \leq x.key\). If \(y\) is a node in the right subtree of \(x\), then \(y.key \geq x.key\).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Under this definition, operations such as search, insert, and delete can be performed in \(\Theta(lg n)\) worst-case time assuming that the tree is balanced. Later, we will explore a variant of the binary search tree that guarantees a balanced tree.&lt;/p&gt;
&lt;p&gt;A tree node implemented in Python might look like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Node&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, key):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;operations&#34;&gt;Operations&lt;/h2&gt;
&lt;h3 id=&#34;traversals&#34;&gt;Traversals&lt;/h3&gt;
&lt;p&gt;Like any other graph-based structure, a tree can be traversed using either depth-first or breadth-first search. Only an inorder depth-first search is of interest for a binary search tree, as we will see below. Consider the given tree in the figure below. Performing an inorder traversal on this tree yields the keys in sorted order from smallest to largest.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;inorder_tree_walk&lt;/span&gt;(x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        inorder_tree_walk(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        inorder_tree_walk(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Traversing the entire tree takes \(\Theta(n)\) time, as each node must be visited once. &lt;em&gt;Searching&lt;/em&gt; a tree, however, only takes \(\Theta(lg n)\) time. The search algorithm is defined recursively as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tree_search&lt;/span&gt;(x, k):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; tree_search(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left, k)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; tree_search(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right, k)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Consider the balanced tree shown in the figure below. If we search for the key 15, notice that after the first comparison with the root, the search space goes from 15 nodes to 7 nodes. After the second comparison, the search space goes from 7 nodes to 3 nodes. After the third comparison, the search space goes from 3 nodes to 1 node. This is the essence of binary search, and it is why the search operation runs in \(\Theta(lg n)\) time.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-10_20-05-13_binary_tree_full.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;A balanced binary search tree&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;A balanced binary search tree
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;minimum&#34;&gt;Minimum&lt;/h3&gt;
&lt;p&gt;In a BST, the minimum value is the leftmost node. Finding the minimum is as easy as traversing down the left branch until a leaf node is reached.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tree_minimum&lt;/span&gt;(x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;maximum&#34;&gt;Maximum&lt;/h3&gt;
&lt;p&gt;In a BST, the maximum value is the rightmost node. Finding the maximum is as easy as traversing down the right branch until a leaf node is reached.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tree_maximum&lt;/span&gt;(x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;successor&#34;&gt;Successor&lt;/h3&gt;
&lt;p&gt;The successor and predecessor operations are useful for the delete operation defined below. The successor of a node \(x\) is the node with the smallest key greater than \(x.key\). If \(x\) has a right subtree, then the successor of \(x\) is the minimum of the right subtree. If \(x\) has no right subtree, then the successor of \(x\) is the lowest ancestor of \(x\) whose left child is also an ancestor of \(x\).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tree_successor&lt;/span&gt;(x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; tree_minimum(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;predecessor&#34;&gt;Predecessor&lt;/h3&gt;
&lt;p&gt;The predecessor of a node \(x\) is the node with the largest key less than \(x.key\). If \(x\) has a left subtree, then the predecessor of \(x\) is the maximum of the left subtree. If \(x\) has no left subtree, then the predecessor of \(x\) is the lowest ancestor of \(x\) whose right child is also an ancestor of \(x\).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tree_predecessor&lt;/span&gt;(x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; tree_maximum(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;insert&#34;&gt;Insert&lt;/h3&gt;
&lt;p&gt;Inserting an item into a binary search tree follows the same logic as traversal. Starting at the root, the key is compared to see if it is greater than the root&amp;rsquo;s key. If so, recursively traverse down the right branch. If not, recursively traverse down the left branch. This process continues until a leaf node is reached, at which point the new node is inserted as a child of the leaf node.&lt;/p&gt;
&lt;p&gt;This process will not necessarily result in a balanced tree. In fact, if the keys are inserted in sorted order, the tree will be a linked list. This is the worst-case scenario for a binary search tree, as the search operation will then run in \(\Theta(n)\) time.&lt;/p&gt;
&lt;p&gt;The full algorithm is given below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tree_insert&lt;/span&gt;(T, z):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;root
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        T&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;root &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;delete&#34;&gt;Delete&lt;/h3&gt;
&lt;p&gt;Deleting a node is not a straightforward as insert. Depending on the structure of the subtree, one of three cases must be considered.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If \(z\) has no subnodes, simply remove \(z\) from the tree.&lt;/li&gt;
&lt;li&gt;If \(z\) has one subnode, replace \(z\) with its subnode.&lt;/li&gt;
&lt;li&gt;If \(z\) has two subnodes, replace \(z\) with its successor. It is a bit more complicated than this, as we explore below.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In case 3, node \(z\) has both a left and right subnode. The first step is to find the successor of \(z\), \(y\). Since \(z\) has 2 subnodes, its successor has no left subnode (&lt;strong&gt;&lt;strong&gt;convince yourself of this&lt;/strong&gt;&lt;/strong&gt;). Likewise, its predecessor has no right subnode. If \(y\) is the right subnode of \(z\), replace \(z\) by \(y\).&lt;/p&gt;
&lt;p&gt;If \(y\) is not the right subnode of \(z\), it is somewhere further down the right branch. In this case, replace \(y\) by its right subnode before replacing \(z\) by \(y\). The figure below shows the removal of node 12 from the tree in the figure above.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-10_23-03-23_binary_tree_delete.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Deleting node 12 from the tree&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Deleting node 12 from the tree
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Even though only 1 node was moved (13 to 12&amp;rsquo;s old position), the process of deleting a node actually involves &lt;strong&gt;&lt;strong&gt;transplanting&lt;/strong&gt;&lt;/strong&gt; a subtree to a new position. This is defined algorithmically as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;transplant&lt;/span&gt;(T, u, v):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        T&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;root &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; u &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; u&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the code above, &lt;code&gt;u&lt;/code&gt; is the node to be replaced, and &lt;code&gt;v&lt;/code&gt; is the node to replace it. Updating &lt;code&gt;v&lt;/code&gt;&amp;rsquo;s left and right subnodes are done in the calling function &lt;code&gt;tree_delete&lt;/code&gt;, as seen below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tree_delete&lt;/span&gt;(T, z):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:  &lt;span style=&#34;color:#75715e&#34;&gt;# Case 1 and 2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        transplant(T, z, z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;: &lt;span style=&#34;color:#75715e&#34;&gt;# Also case 1 and 2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        transplant(T, z, z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;: &lt;span style=&#34;color:#75715e&#34;&gt;# Case 3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tree_minimum(z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right) &lt;span style=&#34;color:#75715e&#34;&gt;# get successor&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            transplant(T, y, y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;right&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        transplant(T, z, y)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; z&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;left&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;analysis&#34;&gt;Analysis&lt;/h2&gt;
&lt;p&gt;Insert, delete, and search all run in \(\Theta(h)\) time, where \(h\) is the height of the tree. If the tree is balanced, \(h = \Theta(lg n)\), and all operations run in \(\Theta(lg n)\) time. If the tree is not balanced, \(h = \Theta(n)\), and all operations run in \(\Theta(n)\) time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Complexity Analysis</title>
      <link>https://ajdillhoff.github.io/notes/complexity_analysis/</link>
      <pubDate>Mon, 25 Sep 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/complexity_analysis/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-notation-of-complexity-analysis&#34;&gt;The notation of complexity analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#formal-definition-of-asymptotic-notation&#34;&gt;Formal Definition of Asymptotic Notation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#common-functions&#34;&gt;Common Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;the-notation-of-complexity-analysis&#34;&gt;The notation of complexity analysis&lt;/h2&gt;
&lt;h3 id=&#34;o-notation&#34;&gt;$O$-notation&lt;/h3&gt;
&lt;p&gt;$O$-notation, often referred to as &amp;ldquo;Big Oh&amp;rdquo; notation, describes an upper bound on the behavior of a function. It really means that the function &lt;em&gt;will not grow faster&lt;/em&gt; than the a given rate. This rate is typically the highest-order term in the function, and is often referred to as the &amp;ldquo;dominant term&amp;rdquo; or &amp;ldquo;dominant function&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;For example, the function \(f(n) = 3n^2 + 2n + 1\) has a dominant term of \(n^2\), and so we would say that \(f(n) = O(n^2)\). We could also accurately describe \(f(n)\) as \(O(n^3)\) since it technically does not grow at a faster rate than \(n^3\), but this is less common as it misleads the reader into thinking that the function is bounded at \(n^3\).&lt;/p&gt;
&lt;h3 id=&#34;and-omega-notation&#34;&gt;$Ω$-notation&lt;/h3&gt;
&lt;p&gt;$Ω$-notation is used to describe the lower bound on the asymptotic behavior of a function. Specifically, it means that the function grows &lt;em&gt;at least as fast&lt;/em&gt; as the given rate. The function \(f(n) = 3n^2 + 2n + 1\) grows at least as fast as \(n^2\), so we would say that \(f(n) = \Omega(n^2)\). It does not grow as fast as \(n^3\), however.&lt;/p&gt;
&lt;p&gt;Just like $O$-notation, we can abuse this definition and say that something that grows at least as fast as \(n^2\) also grows as fast as \(n\). This would lead the reader to believe that the function is bounded at \(n\), which is not true. For this reason, we typically use the tightest bound possible.&lt;/p&gt;
&lt;h3 id=&#34;and-theta-notation&#34;&gt;$Θ$-notation&lt;/h3&gt;
&lt;p&gt;$Θ$-notation gives a tightly bound characterization of a function&amp;rsquo;s behavior. It gives the rate of growth within a constant factor bounded above as well as constant factor bounded below.&lt;/p&gt;
&lt;p&gt;To show that a function is \(\Theta(f(n))\), we must show that it is both \(O(f(n))\) and \(\Omega(f(n))\). Taking our example from above, the function \(f(n) = 3n^2 + 2n + 1\) is \(\Theta(n^2)\).&lt;/p&gt;
&lt;h3 id=&#34;example-insertion-sort&#34;&gt;Example: Insertion Sort&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s put this notation to work and characterize the running time of insertion sort. We&amp;rsquo;ll start by writing out the pseudocode for the algorithm:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;insertion_sort&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, len(A)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; A[j] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            A[j &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[j]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A[j &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;From our &lt;a href=&#34;https://ajdillhoff.github.io/notes/introduction_to_complexity_analysis/&#34;&gt;Introduction to Algorithms&lt;/a&gt; lecture, we already know that the outer loop runs \((n-1)\) times (although the loop is checked \(n\) times). This is not dependent on the order of the \(n\) inputs either. The inner loop is dependent on the values of our input. It could run anywhere between 0 and \(i-1\) times. In the worst case, we saw that it would run \(n-1\) times as well. With this, we concluded that the running time of insertion sort is \(O(n^2)\). Since this was derived for the worst-case, it is reasonable to say that insertion sort is \(O(n^2)\) for all cases.&lt;/p&gt;
&lt;p&gt;The key to the number of operations that the inner loop takes is &lt;code&gt;A[j + 1] = A[j]&lt;/code&gt;, or the number of times a value is shifted to the right. Given an input of \(n\) elements in the worst-case scenario, we can split the input into 3 partitions where the largest \(\lfloor\frac{n}{4}\rfloor\) values are in the first partition. The second partition has size \(\lceil\frac{n}{2}\rceil\), and the last partition has size \(\lfloor\frac{n}{4}\rfloor\). By using the floor and ceiling functions, we can accommodate for odd values of \(n\).&lt;/p&gt;
&lt;p&gt;When the array is finally sorted, the largest \(\lfloor\frac{n}{4}\rfloor\) values will be in the last partition. That means that they would have passed through the middle \(\lceil\frac{n}{2}\rceil\) values one at a time. Therefore, we can state that the worst-case is proportional to&lt;/p&gt;
&lt;p&gt;\[
\left(\left\lfloor\frac{n}{4}\right\rfloor\right)\left(\left\lceil\frac{n}{2}\right\rceil\right) \leq \frac{n^2}{8}.
\]&lt;/p&gt;
&lt;p&gt;This is \(\Omega(n^2)\), so we can conclude that insertion sort is \(\Theta(n^2)\).&lt;/p&gt;
&lt;h3 id=&#34;bonus-example-selection-sort&#34;&gt;Bonus Example: Selection Sort&lt;/h3&gt;
&lt;p&gt;Use a similar analysis to show that the worst-case for selection sort is \(\Theta(n^2)\). As a reminder, selection sort is defined as&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;selection_sort&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, len(A)&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        min_j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, len(A)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; A[j] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; A[min_j]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                min_j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; j
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A[i], A[min_j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[min_j], A[i]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We have already observed that the outer loop iterates \(n-1\) times. Even in the best case, the inner loop runs proportional to \(n\) times. This is sufficient to conclude that the running time is \(O(n^2)\) for all cases.&lt;/p&gt;
&lt;p&gt;For showing that the worst case is \(\Omega(n^2)\), we could use the same argument as insertion sort. However, that isn&amp;rsquo;t necessary. In &lt;em&gt;any&lt;/em&gt; case, the inner loop will run proportional to \(n\) times. It is not dependent on any specific arrangement of the input as selection sort is. Therefore, we can conclude that the worst-case is \(\Omega(n^2)\), and so selection sort is \(\Theta(n^2)\).&lt;/p&gt;
&lt;h2 id=&#34;formal-definition-of-asymptotic-notation&#34;&gt;Formal Definition of Asymptotic Notation&lt;/h2&gt;
&lt;p&gt;Now that we have established some understanding of the notation, let&amp;rsquo;s define it formally. We typically use functions whose domains are over the set of natural or real numbers.&lt;/p&gt;
&lt;h3 id=&#34;o-notation&#34;&gt;$O$-notation&lt;/h3&gt;
&lt;p&gt;We previously established that $O$-notation described as &lt;strong&gt;asymptotic upper bound&lt;/strong&gt;. It was briefly mentioned that this bound holds within a constant factor, which we will now define more thoroughly. For a function \(g(n)\), \(O(g(n)) = \{f(n) : \exists c &amp;gt; 0, n_0 &amp;gt; 0 \text{ such that } 0 \leq f(n) \leq cg(n) \text{ for all } n \geq n_0\}\). It might make more sense to visualize this definition.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-09-26_17-43-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Visualization of $O$-notation (source: Cormen et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Visualization of $O$-notation (source: Cormen et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Notice that the function \(f(n)\) is bounded above by \(cg(n)\) for all \(n \geq n_0\) in the figure above.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s put this definition to the test with an example. Given a function \(f(n) = 3n^2 + 200n + 1000\), show that \(f(n) = O(n^2)\). The goal is to find positive constants \(c\) and \(n_0\) such that \(3n^2 + 200n + 1000 \leq cn^2\) for all \(n \geq n_0\). Dividing both sides by \(n^2\) yields&lt;/p&gt;
&lt;p&gt;\[
3 + \frac{200}{n} + \frac{1000}{n^2} \leq c.
\]&lt;/p&gt;
&lt;p&gt;This equation has many possible solutions. Let&amp;rsquo;s choose \(n_0 = 2\), then&lt;/p&gt;
&lt;p&gt;\[
3 + \frac{200}{2} + \frac{1000}{2^2} = 3 + 100 + 250 = 353 \leq c.
\]&lt;/p&gt;
&lt;p&gt;Therefore, we can conclude that \(f(n) = O(n^2)\).&lt;/p&gt;
&lt;h3 id=&#34;and-omega-notation&#34;&gt;$Ω$-notation&lt;/h3&gt;
&lt;p&gt;The notation used to describe an &lt;strong&gt;asymptotic lower bound&lt;/strong&gt; is formally defined as \(\Omega(g(n)) = \{f(n) : \exists c &amp;gt; 0, n_0 &amp;gt; 0 \text{ such that } 0 \leq cg(n) \leq f(n) \text{ for all } n \geq n_0\}\). Again, it is helpful to visualize this.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-09-26_18-17-07_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Visualization of $&amp;amp;Omega;$-notation (source: Cormen et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Visualization of $Ω$-notation (source: Cormen et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Notice that the function \(f(n)\) is bounded below by \(cg(n)\) for all \(n \geq n_0\) in the figure above.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s revisit our function from above and show that \(f(n) = \Omega(n^2)\). The goal is to find positive constants \(c\) and \(n_0\) such that \(3n^2 + 200n + 1000 \geq cn^2\) for all \(n \geq n_0\). Dividing both sides by \(n^2\) yields&lt;/p&gt;
&lt;p&gt;\[
3 + \frac{200}{n} + \frac{1000}{n^2} \geq c.
\]&lt;/p&gt;
&lt;p&gt;This holds when \(c = 3\) and \(n_0\) is any positive integer. To see this, think about what happens to this function as \(n\) approaches infinity. The first term will always be 3, and the second and third terms will approach 0. Therefore, we can conclude that \(f(n) = \Omega(n^2)\).&lt;/p&gt;
&lt;h3 id=&#34;and-theta-notation&#34;&gt;$Θ$-notation&lt;/h3&gt;
&lt;p&gt;Lastly, the notation used for an &lt;strong&gt;asymptotically tight bound&lt;/strong&gt; is \(\Theta(g(n)) = \{f(n) : \exists c_1, c_2 &amp;gt; 0, n_0 &amp;gt; 0 \text{ such that } 0 \leq c_1g(n) \leq f(n) \leq c_2g(n) \text{ for all } n \geq n_0\}\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-09-26_18-23-25_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Visualization of $&amp;amp;Theta;$-notation (source: Cormen et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Visualization of $Θ$-notation (source: Cormen et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;We had mentioned previously that if \(f(n) = \Omega(g(n))\) and \(f(n) = O(g(n))\), then \(f(n) = \Theta(g(n))\). This is formalized in the following theorem, as stated in Cormen et al.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For any two functions \(f(n)\) and \(g(n)\), we have \(f(n) = \Theta(g(n))\) if and only if \(f(n) = O(g(n))\) and \(f(n) = \Omega(g(n))\).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;o-notation&#34;&gt;$o$-notation&lt;/h3&gt;
&lt;p&gt;There are two less commonly used notations that are worth mentioning. The first is $o$-notation, which is used to describe an upper bound that is &lt;em&gt;not&lt;/em&gt; asymptotically tight. It is defined as \(o(g(n)) = \{f(n) : \forall c &amp;gt; 0, \exists n_0 &amp;gt; 0 \text{ such that } 0 \leq f(n) &amp;lt; cg(n) \text{ for all } n \geq n_0\}\).&lt;/p&gt;
&lt;p&gt;As an example, the bound on \(an^3 = O(n^3)\) is asymptotically tight, but the bound on \(an^3 = o(n^4)\) is not. Using the definition of $o$-notation, we can see that \(an^3 = o(n^4)\), but \(an^3 \neq o(n^3)\).&lt;/p&gt;
&lt;h3 id=&#34;and-omega-notation&#34;&gt;$ω$-notation&lt;/h3&gt;
&lt;p&gt;Analogous to $Ω$-notation, $ω$-notation is used to describe a lower bound that is &lt;em&gt;not&lt;/em&gt; asymptotically tight. It is defined as \(\omega(g(n)) = \{f(n) : \forall c &amp;gt; 0, \exists n_0 &amp;gt; 0 \text{ such that } 0 \leq cg(n) &amp;lt; f(n) \text{ for all } n \geq n_0\}\).&lt;/p&gt;
&lt;p&gt;It is true that \(an^3 = \Omega(n^3)\), but \(an^3 \neq \omega(n^3)\).&lt;/p&gt;
&lt;h3 id=&#34;summary-of-notation&#34;&gt;Summary of Notation&lt;/h3&gt;
&lt;p&gt;An easy way to remember each notation is to think of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$O$-notation: &amp;ldquo;less than or equal to&amp;rdquo;&lt;/li&gt;
&lt;li&gt;$Ω$-notation: &amp;ldquo;greater than or equal to&amp;rdquo;&lt;/li&gt;
&lt;li&gt;$Θ$-notation: &amp;ldquo;equal to&amp;rdquo;&lt;/li&gt;
&lt;li&gt;$o$-notation: &amp;ldquo;strictly less than&amp;rdquo;&lt;/li&gt;
&lt;li&gt;$ω$-notation: &amp;ldquo;strictly greater than&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;function-properties&#34;&gt;Function Properties&lt;/h3&gt;
&lt;p&gt;The following properties are useful when analyzing the asymptotic behavior of functions.&lt;/p&gt;
&lt;h4 id=&#34;transitivity&#34;&gt;Transitivity&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;If \(f(n) = O(g(n))\) and \(g(n) = O(h(n))\), then \(f(n) = O(h(n))\).&lt;/li&gt;
&lt;li&gt;If \(f(n) = \Omega(g(n))\) and \(g(n) = \Omega(h(n))\), then \(f(n) = \Omega(h(n))\).&lt;/li&gt;
&lt;li&gt;If \(f(n) = \Theta(g(n))\) and \(g(n) = \Theta(h(n))\), then \(f(n) = \Theta(h(n))\).&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;reflexivity&#34;&gt;Reflexivity&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;\(f(n) = O(f(n))\)&lt;/li&gt;
&lt;li&gt;\(f(n) = \Omega(f(n))\)&lt;/li&gt;
&lt;li&gt;\(f(n) = \Theta(f(n))\)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;symmetry&#34;&gt;Symmetry&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;\(f(n) = \Theta(g(n))\) if and only if \(g(n) = \Theta(f(n))\).&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;transpose-symmetry&#34;&gt;Transpose Symmetry&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;\(f(n) = O(g(n))\) if and only if \(g(n) = \Omega(f(n))\).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;common-functions&#34;&gt;Common Functions&lt;/h2&gt;
&lt;p&gt;The functions used to describe both time and space complexity are visualized below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-09-26_19-11-32_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Common functions used in complexity analysis (source: Wikipedia)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Common functions used in complexity analysis (source: Wikipedia)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Introduction to Algorithms</title>
      <link>https://ajdillhoff.github.io/notes/introduction_to_complexity_analysis/</link>
      <pubDate>Tue, 19 Sep 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/introduction_to_complexity_analysis/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-to-algorithms&#34;&gt;Introduction to Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#insertion-sort&#34;&gt;Insertion Sort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-sorting-numbers&#34;&gt;Example: Sorting Numbers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#correctness&#34;&gt;Correctness&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#worst-case-analysis&#34;&gt;Worst-Case Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#best-case-analysis&#34;&gt;Best-Case Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rate-of-growth&#34;&gt;Rate of Growth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-analysis-of-selection-sort&#34;&gt;Example: Analysis of Selection Sort&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction-to-algorithms&#34;&gt;Introduction to Algorithms&lt;/h2&gt;
&lt;p&gt;One of the major goals of computer science is to solve important problems. In order to do that, we must be able to express those solutions both mathematically and in a way that can be executed by a computer. Further, those solutions need to be aware of the resources that are available to them. It does us no good to come up with a solution that could never be run by current hardware or executed in a reasonable amount of time.&lt;/p&gt;
&lt;p&gt;There are of course other considerations besides runtime. How much memory does the solution require? Does it require a lot of data to be stored on disk? What about distributed solutions that can be run on multiple machines? Some solutions can be so complex, that we must also consider their environmental impact. For example, Meta&amp;rsquo;s Llama 2 large language models required 3,311,616 combined GPU hours to train. They report that their total carbon emissions from training were 539 tons of CO2 equivalent (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Touvron et al. 2023&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;what-is-an-algorithm&#34;&gt;What is an algorithm?&lt;/h3&gt;
&lt;p&gt;Generally speaking, an &lt;strong&gt;algorithm&lt;/strong&gt; is a sequence of steps describing some computational process. It is more abstract that a single function, and an algorithm will typically call many functions to accomplish its task. Algorithms give us a way of translating complex processes from human language into a form that can be executed by a computer.&lt;/p&gt;
&lt;p&gt;We begin our algorithmic journey by studying a simple sorting algorithm, insertion sort. First, we need to formally define the problem of sorting. Given a sequence of \(n\) objects \(A = \langle a_1, a_2, \ldots, a_n \rangle\), we want to rearrange the elements such that \(a_1&amp;rsquo; \leq a_2&amp;rsquo; \leq \ldots \leq a_n&amp;rsquo;\). We will assume that the elements are comparable, meaning that we can use the operators \(&amp;lt;\) and \(&amp;gt;\) to compare them. Some sets, such as the set of all real numbers, have a natural ordering. A useful programming language would provide the required comparison operators. For other types of elements, such as strings, this may not be the case. For example, how would you compare the strings &amp;ldquo;apple&amp;rdquo; and &amp;ldquo;banana&amp;rdquo;? In these cases, we will need to define our own comparison operators. Either way, we will assume that the comparison operators are available to us.&lt;/p&gt;
&lt;p&gt;This example follows the one given in Chapter 2 of Cormen et al. (2009).&lt;/p&gt;
&lt;h2 id=&#34;insertion-sort&#34;&gt;Insertion Sort&lt;/h2&gt;
&lt;p&gt;Insertion sort is defined as&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;insertion_sort&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, len(A)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[i]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; A[j] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            A[j &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[j]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A[j &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;example-sorting-numbers&#34;&gt;Example: Sorting Numbers&lt;/h2&gt;
&lt;p&gt;Using the algorithm above, sort the following set of numbers. Show the output of the set each time an element is swapped.&lt;/p&gt;
&lt;p&gt;\[
S = \{10, 4, 7, 0, 2\}
\]&lt;/p&gt;
&lt;p&gt;\begin{align*}
\{10, 4, 7, 0, 2\}\\
\{4, 10, 7, 0, 2\}\\
\{4, 7, 10, 0, 2\}\\
\{4, 7, 0, 10, 2\}\\
\{4, 0, 7, 10, 2\}\\
\{0, 4, 7, 10, 2\}\\
\{0, 4, 7, 2, 10\}\\
\{0, 4, 2, 7, 10\}\\
\{0, 2, 4, 7, 10\}\\
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;correctness&#34;&gt;Correctness&lt;/h2&gt;
&lt;p&gt;When coming up with novel algorithms, we may find that it works with a specific use-case, but not in general. It is important to verify that the algorithm works for all possible inputs. Proving the &lt;strong&gt;correctness&lt;/strong&gt; of an algorithm may be tricky in some cases, but we can utilize a few techniques to make it easier. The first such technique is called a &lt;strong&gt;loop invariant&lt;/strong&gt;. A loop invariant is a statement that is true before and after each iteration of a loop.&lt;/p&gt;
&lt;p&gt;The loop invariant for insertion sort is that the subarray \(A[0 : i - 1]\) is sorted. This is chosen based on the task. In some cases, the loop invariant is clear from the problem statement. In others, it may require some thought. It is also possible that more than one loop invariant exists for a given algorithm. To determine if a loop invariant is correct, we must verify three things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Initialization&lt;/strong&gt;: The loop invariant is true before the first iteration of the loop.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Maintenance&lt;/strong&gt;: If the loop invariant is true before an iteration of the loop, it remains true after the iteration.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Termination&lt;/strong&gt;: When the loop terminates, the loop invariant is true.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As long as we can show that the properties above hold, we can be confident that the algorithm is correct. Let&amp;rsquo;s verify the loop invariant for insertion sort.&lt;/p&gt;
&lt;h3 id=&#34;initialization&#34;&gt;Initialization&lt;/h3&gt;
&lt;p&gt;Before the first iteration of the loop, \(i = 1\). The subarray \(A[0 : i - 1] = A[0 : 0] = \langle A[0] \rangle\). Since a single element is always sorted, the loop invariant is true before the first iteration.&lt;/p&gt;
&lt;h3 id=&#34;maintenance&#34;&gt;Maintenance&lt;/h3&gt;
&lt;p&gt;Assume that the loop invariant is true before the $i$th iteration of the loop. That is, \(A[0 : i - 1]\) is sorted. We need to show that the loop invariant remains true after the $i + 1$th iteration. The $i + 1$th iteration of the loop will swap elements in the subarray \(A[0 : i]\). The loop invariant is maintained if the subarray \(A[0 : i]\) is sorted after the $i + 1$th iteration. This is true because the $i + 1$th iteration will swap elements in the subarray \(A[0 : i]\) until the element at index \(i\) is in the correct position. Therefore, the loop invariant is maintained.&lt;/p&gt;
&lt;h3 id=&#34;termination&#34;&gt;Termination&lt;/h3&gt;
&lt;p&gt;The loop terminates when \(i = n\). At this point, the subarray \(A[0 : n - 1]\) is sorted. Since the loop invariant is true after the last iteration, the algorithm is correct.&lt;/p&gt;
&lt;h2 id=&#34;worst-case-analysis&#34;&gt;Worst-Case Analysis&lt;/h2&gt;
&lt;p&gt;Given the definition from above, we can compute \(T(n)\), the running time of the algorithm on an input of size \(n\). To do this, we need to sum the products of the cost of each statement and the number of times each statement is executed.&lt;/p&gt;
&lt;p&gt;At first glance, the first statement &lt;code&gt;for i in range(1, len(A))&lt;/code&gt; appears to execute \(n-1\) times since it starts at 1 and only goes up to, but not including, \(n\). Remember that the &lt;code&gt;for&lt;/code&gt; statement must be checked to see if it should exit, so the test is executed one more time than the number of iterations. Therefore, the first statement is executed \(n\) times. If we say that the cost to execute each check is \(c_1\), then the total cost of the first statement is \(c_1 n\).&lt;/p&gt;
&lt;p&gt;With the exception of the &lt;code&gt;while&lt;/code&gt; loop, the statement inside the &lt;code&gt;for&lt;/code&gt; loop is executed once per iteration. The cost of executing statement \(i\) is \(c_i\). Therefore, the total cost of the second statement is \(c_2 n\). The costs are updated in the code below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;insertion_sort&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, len(A)): &lt;span style=&#34;color:#75715e&#34;&gt;# c_1 n&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[i] &lt;span style=&#34;color:#75715e&#34;&gt;# c_2 n&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# c_3 n&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; A[j] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; key:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            A[j &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[j]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A[j &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key &lt;span style=&#34;color:#75715e&#34;&gt;# c_7 n&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For the &lt;code&gt;while&lt;/code&gt; loop, we can denote the number of times it runs by \(t_i\), where \(i\) is the iteration of the &lt;code&gt;for&lt;/code&gt; loop. If the &lt;code&gt;while&lt;/code&gt; condition check costs \(c_4\) and is executed \(t_i\) times for each &lt;code&gt;for&lt;/code&gt; loop iteration, the total cost is given as \(c_4 \sum_{i=1}^{n-1} t_i\).&lt;/p&gt;
&lt;p&gt;The statements inside the &lt;code&gt;while&lt;/code&gt; loop are executed 1 fewer times than the number of times the condition check is executed. Therefore, the total cost of the statements inside the &lt;code&gt;while&lt;/code&gt; loop is \(c_5 \sum_{i=1}^{n-1} (t_i - 1) + c_5 \sum_{i=1}^{n-1} (t_i - 1)\). The cost of the &lt;code&gt;while&lt;/code&gt; loop is updated in the code below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;insertion_sort&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, len(A)): &lt;span style=&#34;color:#75715e&#34;&gt;# c_1 * n&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[i] &lt;span style=&#34;color:#75715e&#34;&gt;# c_2 * (n-1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# c_3 * (n-1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; A[j] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; key: &lt;span style=&#34;color:#75715e&#34;&gt;# c_4 * [t_i for i in range(1, len(A))]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            A[j &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[j] &lt;span style=&#34;color:#75715e&#34;&gt;# c_5 * [t_i - 1 for i in range(1, len(A))]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# c_6 * [t_i - 1 for i in range(1, len(A))]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A[j &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key &lt;span style=&#34;color:#75715e&#34;&gt;# c_7 * (n-1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To get the total running time \(T(n)\), we sum up all of the costs.&lt;/p&gt;
&lt;p&gt;\begin{align}
T(n) &amp;amp;= c_1 n + c_2 (n-1) + c_3 (n-1) + c_4 \sum_{i=1}^{n-1} t_i + c_5 \sum_{i=1}^{n-1} (t_i - 1) + c_6 \sum_{i=1}^{n-1} (t_i - 1) + c_7 (n-1) \\
\end{align}&lt;/p&gt;
&lt;p&gt;This analysis is a good start, but it doesn&amp;rsquo;t paint the whole picture. The number of actual executions will depend on the input that is given. For example, what if the input is already sorted, or given in reverse order? It is common to express the worst-case runtime for a particular algorithm. For insertion sort, that is when the input is in reverse order. In this case, each element \(A[i]\) is compared to every other element in the sorted subarray. This means that \(t_i = i\) for every iteration of the &lt;code&gt;for&lt;/code&gt; loop. Therefore, the worst-case runtime is given as&lt;/p&gt;
&lt;p&gt;\begin{align}
T(n) &amp;amp;= c_1 n + c_2 (n-1) + c_3 (n-1) + c_4 \sum_{i=1}^{n-1} i + c_5 \sum_{i=1}^{n-1} (i - 1) + c_6 \sum_{i=1}^{n-1} (i - 1) + c_7 (n-1) \\
\end{align}&lt;/p&gt;
&lt;p&gt;To express this runtime solely in terms of \(n\), we can use the fact that \(\sum_{i=1}^{n-1} i = (\sum_{i=0}^{n-1} i) - 1 =  \frac{n(n-1)}{2} - 1\) and \(\sum_{i=1}^{n-1} (i - 1) = \sum_{i=0}^{n-2} i = \frac{n(n-1)}{2}\). This gives us&lt;/p&gt;
&lt;p&gt;\begin{align}
T(n) &amp;amp;= c_1 n + c_2 (n-1) + c_3 (n-1) + c_4 \left(\frac{n(n-1)}{2} - 1\right)\\
&amp;amp;+ c_5 \left(\frac{n(n-1)}{2}\right) + c_6 \left(\frac{n(n-1)}{2}\right) + c_7 (n-1) \\
&amp;amp;= \left(\frac{c_4}{2} + \frac{c_5}{2} + \frac{c_6}{2}\right)n^2 + \left(c_1 + c_2 + c_3 + \frac{c_4}{2} - \frac{c_5}{2} - \frac{c_6}{2} + c_7\right)n - (c_2 + c_3 + c_4 + c_7) \\
\end{align}&lt;/p&gt;
&lt;p&gt;With the appropriate choice of constants, we can express this as a quadratic function \(an^2 + bn + c\).&lt;/p&gt;
&lt;h2 id=&#34;best-case-analysis&#34;&gt;Best-Case Analysis&lt;/h2&gt;
&lt;p&gt;The best-case runtime for insertion sort is when the input is already sorted. In this case, the &lt;code&gt;while&lt;/code&gt; check is executed only once per iteration of the &lt;code&gt;for&lt;/code&gt; loop. That is, \(t_i = 1\) for every iteration of the &lt;code&gt;for&lt;/code&gt; loop. Therefore, the best-case runtime is given as&lt;/p&gt;
&lt;p&gt;\begin{align}
T(n) &amp;amp;= c_1 n + c_2 (n-1) + c_3 (n-1) + c_4 (n-1) + c_7 (n-1) \\
&amp;amp;= (c_1 + c_2 + c_3 + c_4 + c_7)n - (c_2 + c_3 + c_4 + c_7) \\
\end{align}&lt;/p&gt;
&lt;p&gt;Let \(a = c_1 + c_2 + c_3 + c_4 + c_7\) and $b = -(c_2 + c_3 + c_4 + c_7)$Then the best-case runtime is given as \(an + b\), a linear function of \(n\).&lt;/p&gt;
&lt;h2 id=&#34;rate-of-growth&#34;&gt;Rate of Growth&lt;/h2&gt;
&lt;p&gt;We can simplify how we express the runtime of both these cases by considering only the highest-order term. Consider the worst-case, \(T(n) = an^2 + bn + c\). As \(n\) grows, the term \(an^2\) will dominate the runtime, rendering the others insignificant by comparison. This simplification is typically expressed using \(\Theta\) notation. For the worst-case, we say that \(T(n) = \Theta(n^2)\). It is a compact way of stating that the runtime is proportional to \(n^2\) for large values of \(n\).&lt;/p&gt;
&lt;h2 id=&#34;example-analysis-of-selection-sort&#34;&gt;Example: Analysis of Selection Sort&lt;/h2&gt;
&lt;p&gt;Based on the analysis above, let&amp;rsquo;s check our understanding and see if we can characterize the runtime of another sorting algorithm, selection sort. Selection sort is defined as&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;selection_sort&lt;/span&gt;(A):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, len(A) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        min_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, len(A)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; A[j] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; A[min_index]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                min_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; j
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        A[i], A[min_index] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[min_index], A[i]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first statement &lt;code&gt;for i in range(0, len(A) - 1)&lt;/code&gt; will be evaluated \(n\) times. With the exception of the inner &lt;code&gt;for&lt;/code&gt; loop, the rest of the statements in the scope of the first &lt;code&gt;for&lt;/code&gt; loop are executed once per iteration. Their costs are \(c_2\) and \(c_6\), respectively.&lt;/p&gt;
&lt;p&gt;The inner &lt;code&gt;for&lt;/code&gt; loop will be checked \(n-i\) times for each iteration of the outer &lt;code&gt;for&lt;/code&gt; loop. The cost of the condition check is \(c_3\). The cost of the statements inside the &lt;code&gt;for&lt;/code&gt; loop are \(c_4\) and \(c_5\). The &lt;code&gt;if&lt;/code&gt; check is evaluated for every iteration of the inner loop, but the statements inside the &lt;code&gt;if&lt;/code&gt; are only executed when the condition is true. We can denote this as \(t_i\), the number of times the &lt;code&gt;if&lt;/code&gt; condition is true for each iteration of the inner &lt;code&gt;for&lt;/code&gt; loop. The cost of the inner loop is given as&lt;/p&gt;
&lt;p&gt;\begin{align}
c_3 \sum_{i=1}^{n-1} (n-i) + c_4 \sum_{i=0}^{n-1} (n-i-1) + c_5 \sum_{i=0}^{n-1} t_i\\
\end{align}&lt;/p&gt;
&lt;p&gt;Combining this with the cost of the outer &lt;code&gt;for&lt;/code&gt; loop, we get&lt;/p&gt;
&lt;p&gt;\begin{align}
T(n) &amp;amp;= c_1 n + c_2 (n-1) + c_6 (n-1) + c_3 \sum_{i=0}^{n-1} (n-i) + c_4 \sum_{i=0}^{n-1} (n-i-1) + c_5 \sum_{i=0}^{n-1} t_i\\
\end{align}&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Touvron, Hugo, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, et al. 2023. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” arXiv. &lt;a href=&#34;https://doi.org/10.48550/arXiv.2307.09288&#34;&gt;https://doi.org/10.48550/arXiv.2307.09288&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Stereo Vision</title>
      <link>https://ajdillhoff.github.io/notes/stereo_vision/</link>
      <pubDate>Wed, 23 Mar 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/stereo_vision/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#epipolar-geometry&#34;&gt;Epipolar Geometry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#calibration-with-known-intrinsic-parameters-and-world-points&#34;&gt;Calibration with Known Intrinsic Parameters and World Points&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#estimating-depth&#34;&gt;Estimating Depth&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Binocular vision permits depth perception.
It is an important part of many tasks such as robotic vision, pose estimation, and scene understanding.
The goal of steropsis is to reconstruct a 3D representation of the world given correspondences between two or more cameras.&lt;/p&gt;
&lt;p&gt;The process of computing these correspondences assumes two or more cameras with known intrinsic and extrinsic parameters.
Methods exist to estimate the required transformation parameters using points based on matching image features.
If some set of points which a fixed coordinate system is known, such as a calibration pattern, the problem becomes even simpler.
Knowing the exact world point as it is projected in all image planes is essentially a ground truth.&lt;/p&gt;
&lt;p&gt;Hartley and Zisserman address three primary questions when dealing with two views:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Correspondence geometry:&lt;/strong&gt; How does a point in one view constraint the corresponding point in a second view?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Camera geometry:&lt;/strong&gt; How do we determine the cameras of both views given a set of corresponding image points?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scene geometry:&lt;/strong&gt; If we know the cameras and have a set of corresponding points, how can we compute the depth?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;epipolar-geometry&#34;&gt;Epipolar Geometry&lt;/h2&gt;
&lt;p&gt;Epipolar geometry is the backbone of stereopsis.
We will first define what epipolar geometry is, how it is used in the stereo vision problem, and the core constraint that limits our search space of point correspondences.
It is defined visually below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-24_20-23-56_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Overview of epipolar geometry for stereopsis (Source: Szeliski).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Overview of epipolar geometry for stereopsis (Source: Szeliski).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Consider the point \(\mathbf{p}\) whose projection is \(\mathbf{x}_0\) with respect to camera \(\mathbf{c}_0\) and \(\mathbf{x}_1\) with respect to camera \(\mathbf{c}_1\).
All 5 of these points lie on the &lt;strong&gt;epipolar plane&lt;/strong&gt;.
Additionally, points \(\mathbf{e}_0\) and \(\mathbf{e}_1\) lie on the line defined by \(\mathbf{c}_0\) and \(\mathbf{c}_1\) as it intersects the image plane of each camera, respectively.
These are called the &lt;strong&gt;epipoles&lt;/strong&gt;.
These are also the projections of the other camera centers.&lt;/p&gt;
&lt;p&gt;Fundamental to establishing the correspondences between the two cameras is the &lt;strong&gt;epipolar constraint&lt;/strong&gt;.
If \(\mathbf{x}_0\) and \(\mathbf{x}_1\) represent projections of the same point, then \(\mathbf{x}_1\) must like on the epipolar line \(l_1\) associated with \(\mathbf{x}_0\) and vice versa.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-24_21-07-55_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;The epipolar constraint restricts the search space for matching correspondences. (Source: Szeliski)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;The epipolar constraint restricts the search space for matching correspondences. (Source: Szeliski)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;As seen in the figure above, potentional matches for \(\mathbf{x}_0\) must lie on the epipolar line defined by \(\mathbf{e}_1\) and \(\mathbf{x}_1\).&lt;/p&gt;
&lt;p&gt;Our goal is to compute translation \(\mathbf{t}\) and rotation \(R\).
To find these, we start by mathematically defining the epipolar constraint.
The epipolar plane is defined by the lines \(\overrightarrow{\mathbf{c}_0 \mathbf{p}}\), \(\overrightarrow{\mathbf{c}_1 \mathbf{p}}\), and \(\overrightarrow{\mathbf{c}_0 \mathbf{c}_1}\).
This can be written as&lt;/p&gt;
&lt;p&gt;\[
\overrightarrow{\mathbf{c}_0 \mathbf{p}} \cdot [\overrightarrow{\mathbf{c}_0 \mathbf{c}_1} \times \overrightarrow{\mathbf{c}_1 \mathbf{p}}] = 0.
\]&lt;/p&gt;
&lt;p&gt;We do not know \(\mathbf{p}\), but we do have \(\mathbf{x}_0\) and \(\mathbf{x}_1\), the projections of \(\mathbf{p}\) onto the image plane of each respective camera.
Assuming that both camera are calibrated, we have a set of known intrinsic matrices \(\mathbf{K}_j\).&lt;/p&gt;
&lt;p&gt;Although we do not have the exact \(z\) value of the point, we do know the point with respect to the camera calculcated as&lt;/p&gt;
&lt;p&gt;\[
\mathbf{p}_0 = d_0 \hat{\mathbf{x}}_0,
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\hat{\mathbf{x}}_0 = \mathbf{K}_0^{-1} \mathbf{x}_0.
\]&lt;/p&gt;
&lt;p&gt;The relationship between points \(\mathbf{p}_0\) and \(\mathbf{p}_1\) is&lt;/p&gt;
&lt;p&gt;\[
d_1 \hat{\mathbf{x}}_1 = R(d_0 \hat{\mathbf{x}}_0) + \mathbf{t},
\]&lt;/p&gt;
&lt;p&gt;where \(R\) is a rotation matrix and \(\mathbf{t}\) is an offset vector.
These are the parameters that are solved from stereo calibration.&lt;/p&gt;
&lt;p&gt;Since the vectors \(\hat{\mathbf{x}}_0\), \(\hat{\mathbf{x}}_1\), and \(\mathbf{t}\) are coplanar, the plane can be represented by a normal vector.
That is, the vector that is orthogonal to all points in the plane.
Such a vector can be calculated by taking the cross product of both sides of the above equation with \(\mathbf{t}\):&lt;/p&gt;
&lt;p&gt;\[
d_1 [\mathbf{t}]_{\times} \hat{\mathbf{x}}_1 = d_0 [\mathbf{t}]_{\times} R \hat{\mathbf{x}}_0,
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
[\mathbf{t}]_{\times}=\begin{bmatrix}
0 &amp;amp; -t_z &amp;amp; t_y\\
t_z &amp;amp; 0 &amp;amp; -t_x\\
-t_y &amp;amp; t_x &amp;amp; 0
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;It is true that, since the normal vector is orthogonal to \(\hat{\mathbf{x}}_0\), \(\hat{\mathbf{x}}_1\), and \(\mathbf{t}\), taking the dot product of any of these vectors and the normal vector yields 0:&lt;/p&gt;
&lt;p&gt;\[
d_1 \hat{\mathbf{x}}_1^T [\mathbf{t}]_{\times} \hat{\mathbf{x}}_1 = d_0 \hat{\mathbf{x}}_1^T [\mathbf{t}]_{\times} R \hat{\mathbf{x}}_0 = 0.
\]&lt;/p&gt;
&lt;p&gt;We have now established the epipolar constraint in terms of the rotation matrix \(R\) and translation \(\mathbf{t}\).
These are the parameters that relate the points between both cameras.
This constraint is more compactly written as&lt;/p&gt;
&lt;p&gt;\[
\hat{\mathbf{x}}_1^T E \hat{\mathbf{x}}_0 = 0,
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
E = [\mathbf{t}_{\times}]R.
\]&lt;/p&gt;
&lt;p&gt;\(E\) is called the &lt;strong&gt;essential matrix&lt;/strong&gt; which relates the projected points between the two cameras.
Once we have the essential matrix, we can compute \(\mathbf{t}\) and \(R\).&lt;/p&gt;
&lt;h2 id=&#34;calibration-with-known-intrinsic-parameters-and-world-points&#34;&gt;Calibration with Known Intrinsic Parameters and World Points&lt;/h2&gt;
&lt;p&gt;We first look at the simplest case of stereo calibration.
The intrinsic parameters of both cameras have been computed using a standard calibration technique.
Additionally, we have a fixed calibration pattern used to establish a correspondence between
the world points and the points with respect to each camera.&lt;/p&gt;
&lt;p&gt;Given \(N\) measured correspondences \(\{(\mathbf{x}_{i0}, \mathbf{x}_{i1})\}\), we can form a linear system with equations of the form&lt;/p&gt;
&lt;p&gt;\begin{alignat*}{3}
x_{i0}x_{i1}e_{00} &amp;amp; {}+{} &amp;amp; y_{i0}x_{i1}e_{01} &amp;amp; {}+{} &amp;amp; x_{i1}e_{02} &amp;amp; {}+{} &amp;amp;\\
x_{i0}y_{i1}e_{10} &amp;amp; {}+{} &amp;amp; y_{i0}y_{i1}e_{11} &amp;amp; {}+{} &amp;amp; y_{i1}e_{12} &amp;amp; {}+{} &amp;amp;\\
x_{i0}e_{20} &amp;amp; {}+{} &amp;amp; y_{i0}e_{21} &amp;amp; {}+{} &amp;amp; e_{22} &amp;amp; {}={} &amp;amp; 0
\end{alignat*}&lt;/p&gt;
&lt;p&gt;Given at least 8 equations corresponding to the 8 unknowns of \(E\), we can use SVD to solve for \(E\).&lt;/p&gt;
&lt;p&gt;\[
E = [\mathbf{t}]_{\times}R = \mathbf{U \Sigma V^T} = \begin{bmatrix}
\mathbf{u}_0 &amp;amp; \mathbf{u}_1 &amp;amp; \mathbf{t}
\end{bmatrix}\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 1\\
\end{bmatrix}\begin{bmatrix}
\mathbf{v}_0^T\\
\mathbf{v}_1^T\\
\mathbf{v}_2^T\\
\end{bmatrix}
\]&lt;/p&gt;
&lt;h2 id=&#34;estimating-depth&#34;&gt;Estimating Depth&lt;/h2&gt;
&lt;p&gt;Given the intrinsic parameters and parameters relating both calibrated cameras, we can estimate the depth of a point that is seen by both cameras.&lt;/p&gt;
&lt;p&gt;We know that \(\mathbf{x}_0 = K_0 \hat{\mathbf{x}}_0\) and \(\mathbf{x}_1 = K_1 \hat{\mathbf{x}}_1\).
With the stereo calibration complete, we also know \(A = [R\quad \mathbf{t}]\) and&lt;/p&gt;
&lt;p&gt;\[
\hat{\mathbf{x}}_0 = A\hat{\mathbf{x}}_1.
\]&lt;/p&gt;
&lt;p&gt;Plugging into the projection equation for the first camera yields&lt;/p&gt;
&lt;p&gt;\[
\mathbf{x}_0 = K_0 A\hat{\mathbf{x}}_1.
\]&lt;/p&gt;
&lt;p&gt;Our knowns are \(\mathbf{x}_0, \mathbf{x}_1, K_0, K_1, \text{ and } A\).
The only unknown is \(\hat{\mathbf{x}}_1\).&lt;/p&gt;
&lt;p&gt;We are left with 2 equations&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{x}_0 &amp;amp;= K_0 A\hat{\mathbf{x}}_1\\
\mathbf{x}_1 &amp;amp;= K_1 \hat{\mathbf{x}}_1.
\end{align*}&lt;/p&gt;
&lt;p&gt;If we let \(P = K_0 A\), we write the \(\mathbf{x}_0 = \begin{bmatrix}u_1\\v_1\\1\end{bmatrix}\) and&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\begin{bmatrix}
u_1\\
v_1\\
1
\end{bmatrix}=\begin{bmatrix}
p_{11} &amp;amp; p_{12} &amp;amp; p_{13} &amp;amp; p_{14}\\
p_{21} &amp;amp; p_{22} &amp;amp; p_{23} &amp;amp; p_{24}\\
p_{31} &amp;amp; p_{32} &amp;amp; p_{33} &amp;amp; p_{34}\\
\end{bmatrix}\begin{bmatrix}
X\\
Y\\
Z\\
W
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;This gives two equations for \(x_1\) and \(y_1\), the measured 2D feature locations:&lt;/p&gt;
&lt;p&gt;\begin{align*}
x_1 &amp;amp;= \frac{p_{11}X + p_{12}Y + p_{13}Z + p_{14}W}{p_{31}X + p_{32}Y + p_{33}Z + p_{34}W}\\
y_1 &amp;amp;= \frac{p_{21}X + p_{22}Y + p_{23}Z + p_{24}W}{p_{31}X + p_{32}Y + p_{33}Z + p_{34}W}.
\end{align*}&lt;/p&gt;
&lt;p&gt;Multiplying both equations by the denominator yields a set of equations that can be solved via linear least squares or SVD.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
