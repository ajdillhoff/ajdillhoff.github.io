<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine learning on Alex Dillhoff</title>
    <link>https://ajdillhoff.github.io/tags/machine-learning/</link>
    <description>Recent content in machine learning on Alex Dillhoff</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; {year}</copyright>
    <lastBuildDate>Thu, 16 Nov 2023 00:00:00 -0600</lastBuildDate>
    
	    <atom:link href="https://ajdillhoff.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Pretraining Large Language Models</title>
      <link>https://ajdillhoff.github.io/notes/pretraining_large_language_models/</link>
      <pubDate>Thu, 16 Nov 2023 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/pretraining_large_language_models/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#unsupervised-pre-training&#34;&gt;Unsupervised Pre-training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#from-gpt-to-gpt2&#34;&gt;From GPT to GPT2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;These notes provide an overview of pre-training large language models like GPT and Llama.&lt;/p&gt;
&lt;h2 id=&#34;unsupervised-pre-training&#34;&gt;Unsupervised Pre-training&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start by reviewing the pre-training procedure detailed in the GPT paper (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Radford et al. 2020&lt;/a&gt;). The &lt;em&gt;Generative&lt;/em&gt; in Generative Pre-Training reveals much about how the network can be trained without direct supervision. It is analogous to how you might have studied definitions as a kid: create some flash cards with the term on the front and the definition on the back. Given the context of the word, you try and recite the definition. For a pre-training language model, it is given a series of tokens and is tasked with generating the next token in the sequence. Since we have access to the original documents, we can easily determine if it was correct.&lt;/p&gt;
&lt;p&gt;Given a sequence of tokens \(\mathcal{X} = \{x_1, x_2, \ldots, x_n\}\), the model is trained to predict the next token \(x_{n+1}\) in the sequence. The model is trained to maximize the log-likelihood of the next token:&lt;/p&gt;
&lt;p&gt;\[\mathcal{L}(\mathcal{X}) = \sum_{i=1}^{n} \log p(x_{i+1} \mid x_{i-k}, \ldots, x_i)\]&lt;/p&gt;
&lt;p&gt;where \(k\) is the size of the context window.&lt;/p&gt;
&lt;p&gt;Large language models are typically based on the &lt;a href=&#34;https://ajdillhoff.github.io/notes/transformers/&#34;&gt;Transformers&lt;/a&gt; model. The original model was trained for language translation. Depending on the task, different variants are employed. For GPT models, a decoder-only architecture is used, as see below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-11-16_15-56-14_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Decoder-only diagram from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_2&amp;#34;&amp;gt;Vaswani et al. 2017&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Decoder-only diagram from (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Vaswani et al. 2017&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The entire input pipeline for GPT can be expressed rather simply. First, the tokenized input is passed through an embedding layer \(W_{e}\). Embedding layers map the tokenized input into a lower-dimensional vector representation. A positional embedding matrix of the same size as \(\mathcal{X} W_{e}\) is added in order to preserve the order of the tokens.&lt;/p&gt;
&lt;p&gt;The embedded data \(h_0\) is then passed through \(n\) transformer blocks. The output of this is passed through the softmax function in order to produce an output distribution over target tokens.&lt;/p&gt;
&lt;h2 id=&#34;from-gpt-to-gpt2&#34;&gt;From GPT to GPT2&lt;/h2&gt;
&lt;p&gt;GPT2 is a larger version of GPT, with an increased context size of 1024 tokens and a vocabulary of 50,257 vocabulary. In this paper, they posit that a system should be able to perform many tasks on the same input. For example, we may want our models to summarize complex texts as well as provide answers to specific questions we have about the content. Instead of training multiple separate models to perform these tasks individually, the model should be able to adapt to these tasks based on the context. In short, it should model \(p(output \mid input, task)\) instead of \(p(output \mid input)\).&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2020. “Improving Language Understanding by Generative Pre-Training,” 12.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need,” 11.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Gradient Boosting</title>
      <link>https://ajdillhoff.github.io/notes/gradient_boosting/</link>
      <pubDate>Mon, 17 Jul 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/gradient_boosting/</guid>
      <description>&lt;h2 id=&#34;notes-from&#34;&gt;Notes from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Friedman 2001&lt;/a&gt;)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Many machine learning methods are parameterized functions that are optimized using some numerical optimization techniques, notably steepest-descent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Initial learner is a stump, subsequent learners are trees with depth as some power of 2 (commonly).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Numerical optimization in function space&lt;/strong&gt;
\[
g_m(\mathbf{x}) = E_y\Big[\frac{\partial L(y, F(\mathbf{x}))}{\partial F(\mathbf{x})}|\mathbf{x}\Big]_{F(\mathbf{x})=F_{m-1}(\mathbf{x})}
\]
The optimal step size found by solving&lt;/p&gt;
&lt;p&gt;\[
\rho_m = \mathop{\arg \min}_{\rho} E_{y,\mathbf{x}}L(y,F_{m-1}(\mathbf{x})-\rho g_m(\mathbf{x}))
\]
Then the function \(m\) is updated:
\[
f_m(\mathbf{x}) = -\rho_m g_m(\mathbf{x})
\]&lt;/p&gt;
&lt;p&gt;Walking through it&amp;hellip;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Make an initial guess with \(f_0(\mathbf{x})\)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Evaluate \(L(y, f_0(\mathbf{x}))\)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Improve model by boosting \(f_1(\mathbf{x}) = -\rho_1 g_1(\mathbf{x})\), where \[ g_1(\mathbf{x}) = \frac{\partial L(y, f_0(\mathbf{x}))}{\partial f_0(\mathbf{x})}. \]
This implies that \(f_1\) is predicting the gradient of the previous function.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the model is nonparametric, the expected value of the function conditioned on the input cannot be estimated accurately because we cannot sample the entire distribution of \(\mathbf{x}\). The author&amp;rsquo;s note that &amp;ldquo;&amp;hellip;even if it could, one would like to estimate \(F^*(\mathbf{x})\) at \(\mathbf{x}\) values other than the training sample points.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Smoothness is imposed by approximating the function with a parametric model. I think this means that the distribution is approximated as well.&lt;/p&gt;
&lt;p&gt;\begin{equation}
(\beta_m, \mathbf{a}_m) = \mathop{\arg \min}_{\beta, \mathbf{a}}\sum_{i=1}^N L(y_i, F_{m-1}(\mathbf{x}_i) + \beta h(\mathbf{x}_i; \mathbf{a}))
\end{equation}&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What if a solution to the above equation is difficult to obtain? Instead, view \(\beta_m h(\mathbf{x};\mathbf{a}_m)\) as the best greedy step toward \(F^*(\mathbf{x})\), under the constraint that the step direction, in this case \(h(\mathbf{x};\mathbf{a}_m)\), is a member of the class of functions \(h(\mathbf{x};\mathbf{a})\). The negative gradient can be evaluated at each data point:
\[
-g_m(\mathbf{x}_i) = -\frac{\partial L(y_i, F_{m-1}(\mathbf{x}_i))}{\partial F_{m-1}(\mathbf{x}_i)}.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This gradient is evaluated at every data point. However, we cannot generalize to new values not in our dataset. The proposed solution comes via \(\mathbf{h}_m = \{h(\mathbf{x}_i;\mathbf{a}_m)\}_{1}^N\) &amp;ldquo;most parallel to&amp;rdquo; \(-\mathbf{g}_m \in \mathbb{R}^N\).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As long as we can compute a derivative for the original loss function, our subsequent boosting problems are solved via least-squared error:
\[
\mathbf{a}_m = \mathop{\arg \min}_{\mathbf{a}, \beta} \sum_{i=1}^N \Big(-g_m(\mathbf{x}_i)-\beta h(\mathbf{x}_i;\mathbf{a})\Big)^2
\]&lt;/p&gt;

        
        
        
        
        
        &lt;figure&gt;
        
        &lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-07-18_19-43-31_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Original generic algorithm from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Friedman 2001&amp;lt;/a&amp;gt;).&#34; &gt;
        
        
        
        &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
          
          &lt;p&gt;
            &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Original generic algorithm from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Friedman 2001&lt;/a&gt;).
            
            
            
          &lt;/p&gt; 
        &lt;/figcaption&gt;
        
        &lt;/figure&gt;

&lt;p&gt;Check out a basic implementation in Python &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/blob/main/boosting/intro_to_gradient_boosting.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Friedman, Jerome H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” &lt;i&gt;The Annals of Statistics&lt;/i&gt; 29 (5): 1189–1232. &lt;a href=&#34;https://www.jstor.org/stable/2699986&#34;&gt;https://www.jstor.org/stable/2699986&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An Introduction to Hidden Markov Models for Gesture Recognition</title>
      <link>https://ajdillhoff.github.io/articles/intro_to_hmms/</link>
      <pubDate>Sat, 15 Jul 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/articles/intro_to_hmms/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Hidden Markov Models provide a way of modeling the dynamics of sequential information. They have been used for speech recognition, part-of-speech tagging, machine translation, handwriting recognition, and, as we will see in this article, gesture recognition.&lt;/p&gt;
&lt;p&gt;Consider a somewhat practical use-case: you are going to throw a party with a meticulously curated playlist. You would rather not let anyone have the remote as it might get lost, and letting anyone interrupt the playlist with their own selections may derail the entire event. However, you still want to give your guests the ability to control the volume and skip back and forth between tracks in the playlist. We will also assume that guests will use change tracks and control the volume responsibly.&lt;/p&gt;
&lt;p&gt;The solution to this problem is to implement a gesture recognition system to identify simple hand motions. In this case, we only have to model 4 separate gestures: VolumeUp, VolumeDown, PrevTrack, NextTrack. Since the motions are temporal in nature, we can model each gesture using Hidden Markov Models. First, we need to cover a bit of background on what a Hidden Markov Model actually is.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;First, introduce Markov Chains&lt;/li&gt;
&lt;li&gt;Then the Markov assumption&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the core of our problem, we want to model a distribution over a sequence of states. Consider a sequence of only 3 states \(p(x_1, x_2, x_3)\). The full computation of this can be done using the chain rule of probability:&lt;/p&gt;
&lt;p&gt;\[
p(x_1, x_2, x_3) = p(x_1) p(x_2 | x_1) p(x_3 | x_1, x_2).
\]&lt;/p&gt;
&lt;p&gt;If the random variables of our problem are not conditionally independent, the complexity of calculating this is exponential in the number of random variables.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Markov&lt;/strong&gt; in Hidden Markov Models addresses this complexity. The &lt;strong&gt;Markov Assumption&lt;/strong&gt; states that the probability of an event at time \(t\) is conditioned &lt;em&gt;only&lt;/em&gt; on the previously observed event: \(p(x_t | x_{t-1})\). This is compactly represented with a graphical model, as seen in figure &lt;strong&gt;TODO&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TODO: Figure of basic Markov Chain&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;hidden&lt;/strong&gt; qualifier comes from the fact that the data we wish to model was generated from some underlying process that is not directly observable. A classic example for HMMs uses the weather. Imagine you had a log which had the number of water bottles a person had drank per day over the entire year. To make the problem slightly more difficult, the log entries were not associated with a date. It is reasonable to say that the amount of water a person drinks is influenced by how hot or cold it is on a particular day. So, the &lt;strong&gt;hidden state&lt;/strong&gt; in this case is the weather: hot or cold. We can model this with an HMM by establishing that the amount of water (&lt;strong&gt;observed state&lt;/strong&gt;) is conditioned on the weather (&lt;strong&gt;hidden state&lt;/strong&gt;). Figure &lt;strong&gt;TODO&lt;/strong&gt; shows this HMM graphically.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-07-20_19-12-54_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;An HMM with 4 states and 2 observation symbols (y_1) or (y_2).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;An HMM with 4 states and 2 observation symbols (y_1) or (y_2).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Formally, a Hidden Markov Model is defined by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The number of hidden states \(N\).&lt;/li&gt;
&lt;li&gt;A transition probability matrix \(A \in \mathbb{R}^{N \times N}\), where \(a_{ij} = p(z_t = j | z_{t-1} = i)\).&lt;/li&gt;
&lt;li&gt;An observation symbol probability distribution \(B = \{b_j(k)\} = p(\mathbf{x}_t = k | z_t = j)\).&lt;/li&gt;
&lt;li&gt;An initial state distribution \(\pi_i = p(z_t = i)\).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The trainable parameters of our model are \(\lambda = (A, B, \pi)\).&lt;/p&gt;
&lt;h2 id=&#34;functions-of-an-hmm&#34;&gt;Functions of an HMM&lt;/h2&gt;
&lt;p&gt;Given the basic definition of what an HMM is, how can we train the parameters defined in \(\lambda\). If we somehow already knew the parameters, how can we extract useful information from the model? Depending on our task, we can use HMMs to answer many important questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Filtering&lt;/strong&gt; computes \(p(z_t | \mathbf{x}_{1:t})\). That is, we are computing this probability as new samples come in up to time \(t\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smoothing&lt;/strong&gt; is accomplished when we have all the data in the sequence.
This is expressed as \(p(z_t|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fixed lag smoothing&lt;/strong&gt; allows for a trade off between accuracy and delay. It is useful in cases where we might not have the full sequence, but we wish to compute \(p(z_{t-l}|\mathbf{x}_{1:t})\) for some \(l &amp;gt; 0\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Predictions&lt;/strong&gt; are represented as \(p(z_{t+h}|\mathbf{x}_{1:t})\), where \(h &amp;gt; 0\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MAP estimation&lt;/strong&gt; yields the most probably state sequence \(\text{arg}\max_{\mathbf{z}_{1:T}}p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;We can sample the &lt;strong&gt;posterior&lt;/strong&gt; \(p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;We can also compute \(p(\mathbf{x}_{1:T})\) by summing up over all hidden paths. This is useful for classification tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course not all of these functions make sense for every possible task, more on that later. This article is not meant to be an exhaustive resource for all HMM functions; we will only look at the tasks necessary to train and use HMMs for isolated gesture recognition &lt;strong&gt;TODO: offer additional reading suggestions&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;data-processing&#34;&gt;Data Processing&lt;/h2&gt;
&lt;p&gt;As far as the efficacy of our model goes, how we process the data is the most important. Our system will start with a camera that records our guests performing one of the four simple motions. For simplicity, let&amp;rsquo;s pretend that the camera has an onboard chip that detects the 2D centroids of the left hand for each frame. That helps a lot, but there is still the problem of isolating a group of frames based on when the user wanted to start and finish the command. Assuming we have a solution for both of these problems, we still need to take into account that users will gesture at different speeds. Since all of these problems are challenging in their own right, we will assume the computer vision fairy has taken care of this for us.&lt;/p&gt;
&lt;p&gt;Each gesture in our dataset consists of 30 \((x, y)\) locations of the center of the left hand with respect to image coordinates. Even with this simplified data, we have another problem: different users may gesture from different locations. The hand locations for one user performing the &lt;code&gt;VolumeUp&lt;/code&gt; gesture may be vastly different from another. This isn&amp;rsquo;t too bad to deal with. We could normalize or training data by subtracting the location of the hand in the first frame from the gesture. That way every input would start at \((0, 0)\). We can simplify this even further by using &lt;strong&gt;relative motion states&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;relative-motion-states&#34;&gt;Relative Motion States&lt;/h3&gt;
&lt;p&gt;Relative motion states discretize our data, thus simplifying the input space. The idea is quite simple: if the hand moved to the right relative to the previous frame, we assign \(x = 1\) for that frame. If it moved to the left, assign \(x = -1\). If it didn&amp;rsquo;t move at all, or did not move a significant amount, assign \(x = 0\). We apply similar rules for the \(y\) locations as well. The &lt;strong&gt;TODO: figure&lt;/strong&gt; below shows the relative motion grid.&lt;/p&gt;
&lt;p&gt;Besides greatly simplifying our input space, meaning we can use a simple categorical distribution to model these observations, we no longer have to worry about the discrepency between where each user performed the gesture.&lt;/p&gt;
&lt;h2 id=&#34;modeling-a-gesture&#34;&gt;Modeling a Gesture&lt;/h2&gt;
&lt;p&gt;Our system will consist of 4 HMM models to model the dynamics of each gesture. To determine which gesture was performed, we will given our input sequence to each one and have it compute \(p(\mathbf{x}_{1:T}; \lambda_i)\), the probability of the observation given the parameters of model \(i\). Whichever model gives the high probability wins.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TODO&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Describe EM at a high level, show the breakdown of probabilities that need to be known&lt;/li&gt;
&lt;li&gt;Go into forward-backwards&lt;/li&gt;
&lt;li&gt;Go back to EM and plug them in&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;training-expectation-maximization&#34;&gt;Training: Expectation-Maximization&lt;/h3&gt;
&lt;p&gt;If we cannot observe the hidden states directly, how are we supposed to update the model parameters \(\lambda = (A, B, \pi)\)? We may not have all of the information, but we do have &lt;em&gt;some&lt;/em&gt; information. We can use that to fill in the missing values with what we would expect them to be given what we already know. Then, we can update our parameters using those expected values. This is accomplished through a two-stage algorithm called &lt;strong&gt;Expectation-Maximization&lt;/strong&gt;. Those familiar with k-Nearest Neighbors should already be familiar with this process.&lt;/p&gt;
&lt;h4 id=&#34;updating-with-perfect-information&#34;&gt;Updating with Perfect Information&lt;/h4&gt;
&lt;p&gt;It is useful to know how we would update our parameters assuming we had perfect information. If the hidden states were fully observable, then updating our model parameters would be as straightforward as computing the maximum likelihood estimates.
For \(A\) and \(\pi\), we first tally up the following counts:&lt;/p&gt;
&lt;p&gt;\[
\hat{a}_{ij} = \frac{N_{ij}}{\sum_j N_{ij}},
\]&lt;/p&gt;
&lt;p&gt;the number of times we expect to transition from \(i\) to \(j\) divided by the number of times we transition from \(i\) to any other state. Put simply, this computes the expected transitions from \(i\) to \(j\) normalized by all the times we expect to start in state \(i\).&lt;/p&gt;
&lt;p&gt;For \(\pi\), we have&lt;/p&gt;
&lt;p&gt;\[
\hat{\pi_i} = \frac{N_i}{\sum_i N_i},
\]&lt;/p&gt;
&lt;p&gt;the number of times we expect to start in state \(i\) divided by the number of times we start in any other state.&lt;/p&gt;
&lt;p&gt;Estimating the parameters for \(B\) depends on which distribution we are using for our observation probabilities.
For a multinomial distribution, we would compute the number of times we are in state \(j\) and observe a symbol \(k\) divided by the number of times we are in state \(j\):&lt;/p&gt;
&lt;p&gt;\[
\hat{b}_{jk} = \frac{N_{jk}}{N_k},
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
N_{jk} = \sum_{i=1}^N \sum_{t=1}^T \mathbb{1} (z_{i, t}=j, x_{i, t}=k).
\]&lt;/p&gt;
&lt;p&gt;It is also common to model our emission probability using a Normal distribution. We can even use a parameterized model like a neural network. &lt;strong&gt;TODO: provide links to examples of these&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;updating-with-missing-information&#34;&gt;Updating with Missing Information&lt;/h4&gt;
&lt;p&gt;Now to the real problem: fill in our missing information using our observable data and the current parameter estimates. There are two important statistics that we need to compute, called the &lt;strong&gt;sufficient statistics&lt;/strong&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The expected number of transitions from \(i\) to \(j\).&lt;/li&gt;
&lt;li&gt;The expected number of times we are transitioning from \(i\) to any other state.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both of these can be computed starting with the same probability &lt;em&gt;conditioned&lt;/em&gt; on our observable data:&lt;/p&gt;
&lt;p&gt;\[
p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T}).
\]&lt;/p&gt;
&lt;h3 id=&#34;forwards-backwards-algorithm&#34;&gt;Forwards-Backwards Algorithm&lt;/h3&gt;
&lt;p&gt;Computing joint distribution can be very computationally expensive. Fortunately for us, the Markov assumption along with operations on graphs open the door to a dynamic programming approach named the Forward-Backward algorithm.&lt;/p&gt;
&lt;p&gt;The Forwards-Backwards Algorithm, also known as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Baum-Welch algorithm&lt;/a&gt;, provides an effective solution to computing the joint described above. In fact, there are many useful distributions that can be computed with this algorithm such as the &lt;strong&gt;filtering&lt;/strong&gt; and &lt;strong&gt;smoothing&lt;/strong&gt; tasks.&lt;/p&gt;
&lt;h4 id=&#34;forward-probability&#34;&gt;Forward Probability&lt;/h4&gt;
&lt;p&gt;The forward probability, often denoted as \(\alpha\), represents the probability of ending up at a particular hidden state \(i\) at time \(t\) having seen the observations up to that time:&lt;/p&gt;
&lt;p&gt;\[
\alpha_t(i) = p(z_t = i, \mathbf{x}_{1:t} | \lambda).
\]&lt;/p&gt;
&lt;p&gt;This value is computed recursively starting from \(t=1\) and going forwards to \(t=T\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Initialization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For \(t=1\), we calculate:&lt;/p&gt;
&lt;p&gt;\[
\alpha_1(i) = \pi_i b_i(x_1),\quad 1 \leq i \leq N,
\]&lt;/p&gt;
&lt;p&gt;where \(\pi_i\) is the initial probability of state \(i\) and \(b_i(x_1)\) is the emission probability of the first observation \(x_1\) given that we are in state \(i\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recursion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;After that, we calculate the remaining \(\alpha_t(i)\) as follows:&lt;/p&gt;
&lt;p&gt;\[
\alpha_{t+1}(j) = b_j(x_{t+1}) \sum_{i=1}^{N} \alpha_{t}(i)a_{ij},
\]&lt;/p&gt;
&lt;p&gt;where \(N\) is the number of hidden states, and \(a_{ij}\) is the transition probability from state \(i\) to state \(j\).&lt;/p&gt;
&lt;h4 id=&#34;backward-probability&#34;&gt;Backward Probability&lt;/h4&gt;
&lt;p&gt;The backward probability, denoted as \(\beta\), gives the probability of observing the remaining observations from time \(t+1\) to \(T\) given that we are in state \(i\) at time \(t\):&lt;/p&gt;
&lt;p&gt;\[
\beta_t(i) = p(\mathbf{x}_{t+1:T} | z_t = i, \lambda).
\]&lt;/p&gt;
&lt;p&gt;Again, this is calculated recursively but this time starting from \(t=T\) and going backwards to \(t=1\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Initialization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For \(t=T\), we initialize:&lt;/p&gt;
&lt;p&gt;\[
\beta_T(i) = 1, \forall i.
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recursion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Then we calculate the remaining \(\beta_t(i)\) as:&lt;/p&gt;
&lt;p&gt;\[
\beta_{t}(i) = \sum_{j=1}^{N} a_{ij}b_j(x_{t+1})\beta_{t+1}(j).
\]&lt;/p&gt;
&lt;h4 id=&#34;calculating-the-sufficient-statistics&#34;&gt;Calculating the Sufficient Statistics&lt;/h4&gt;
&lt;p&gt;With these two sets of probabilities, we can calculate the two required sufficient statistics as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The expected number of transitions from \(i\) to \(j\):&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
\frac{\sum_{t=1}^{T-1} \alpha_t(i) a_{ij} b_j(x_{t+1}) \beta_{t+1}(j)}{P(X|\lambda)}
\]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The expected number of times we are transitioning from \(i\) to any other state:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
\frac{\sum_{t=1}^{T-1} \alpha_t(i) \beta_t(i)}{P(X|\lambda)}
\]&lt;/p&gt;
&lt;p&gt;Where \(P(X|\lambda)\) is the total probability of the observations, calculated as:&lt;/p&gt;
&lt;p&gt;\[
P(X|\lambda) = \sum_{i=1}^{N} \alpha_T(i)
\]&lt;/p&gt;
&lt;h4 id=&#34;how-does-this-give-us-p--z-t-i-z-t-plus-1-j-mathbf-x-1-t&#34;&gt;How does this give us \(p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T})\)?&lt;/h4&gt;
&lt;p&gt;To understand how the variables of the Forwards-Backwards algorithm relate to the original probabilities, we can express the term \(p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T})\) in terms of the original probability distributions in the HMM:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(\pi_i\) - the probability of starting in state \(i\),&lt;/li&gt;
&lt;li&gt;\(a_{ij}\) - the probability of transitioning from state \(i\) to state \(j\),&lt;/li&gt;
&lt;li&gt;\(b_j(x_t)\) - the probability that state \(j\) will emit observation \(x_t\).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The joint probability \(p(z_t = i, z_{t+1} = j, \mathbf{x}_{1:T})\) would represent the probability of being in state \(i\) at time \(t\), moving to state \(j\) at time \(t+1\), and observing the sequence of emissions \(\mathbf{x}_{1:T}\). This can be factored as follows due to the Markov property:&lt;/p&gt;
&lt;p&gt;\[
p(z_t = i, z_{t+1} = j, \mathbf{x}_{1:T}) = p(\mathbf{x}_{1:t}, z_t = i)p(z_{t+1} = j| z_t = i)p(\mathbf{x}_{t+1:T} | z_{t+1} = j, \mathbf{x}_{1:t}).
\]&lt;/p&gt;
&lt;p&gt;Using our definitions of \(\alpha\) and \(\beta\), we can rewrite this in terms of our HMM quantities:&lt;/p&gt;
&lt;p&gt;\[
p(z_t = i, z_{t+1} = j, \mathbf{x}_{1:T}) = \alpha_t(i)a_{ij}b_j(x_{t+1})\beta_{t+1}(j).
\]&lt;/p&gt;
&lt;p&gt;Here, \(\alpha_t(i)\) represents \(p(\mathbf{x}_{1:t}, z_t = i)\), the joint probability of the observations until time \(t\) and being in state \(i\) at time \(t\), and \(\beta_{t+1}(j)\) represents \(p(\mathbf{x}_{t+1:T} | z_{t+1} = j)\), the probability of the observations from time \(t+1\) to \(T\) given we&amp;rsquo;re in state \(j\) at time \(t+1\).&lt;/p&gt;
&lt;p&gt;Then, to obtain \(p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T})\), we divide by \(p(\mathbf{x}_{1:T})\) to normalize the probabilities, which is the sum over all states of \(\alpha_T(i)\), or equivalently, the sum over all states of \(\beta_1(i)\pi_i b_i(x_1)\).&lt;/p&gt;
&lt;p&gt;This gives us:&lt;/p&gt;
&lt;p&gt;\[
p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T}) = \frac{\alpha_t(i)a_{ij}b_j(x_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^{N}\alpha_T(i)}.
\]&lt;/p&gt;
&lt;p&gt;This is the same expression as before, but broken down in terms of the original HMM quantities and the forward and backward variables. This can also be explained through graph properties and operations. See &lt;a href=&#34;https://cedar.buffalo.edu/~srihari/CSE574/Chap13/13.2.2-ForwardBackward.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Sargur Srihari&amp;rsquo;s excellent lecture slides&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h2 id=&#34;implementation-in-python&#34;&gt;Implementation in Python&lt;/h2&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Bias and Variance</title>
      <link>https://ajdillhoff.github.io/notes/bias_and_variance/</link>
      <pubDate>Tue, 04 Jul 2023 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/bias_and_variance/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#generalization&#34;&gt;Generalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bias&#34;&gt;Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variance&#34;&gt;Variance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bias-variance-tradeoff&#34;&gt;Bias-Variance Tradeoff&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;generalization&#34;&gt;Generalization&lt;/h2&gt;
&lt;p&gt;When fitting machine learning models to data, we want them to &lt;strong&gt;generalize&lt;/strong&gt; well to the distribution that we have sampled from. We can measure a model&amp;rsquo;s ability to generalize by evaluating it on previously unseen data that is sampled from the same distribution as the training set. However, we often do not know the true underlying distribution. So we must fit the models to empirical distributions derived from observed data.&lt;/p&gt;
&lt;p&gt;Measuring bias and variance is crucial for determining the quality of a model. &lt;strong&gt;Bias&lt;/strong&gt; refers to the difference between the average prediction of a model and the correct value we are trying to predict. A model with high bias oversimplifies the problem and leads to high error on both training and test data. &lt;strong&gt;Variance&lt;/strong&gt; refers to the sensitivity of a model to fluctuations in the training set. High variance suggests that the model&amp;rsquo;s performance changes significantly when it is fit on different samplings of the training data, which can lead to overfitting.&lt;/p&gt;
&lt;p&gt;To achieve good generalization, it is essential to find a balance between bias and variance, minimizing the total error. This can be done by selecting appropriate model complexity and using regularization techniques to prevent overfitting or underfitting. Additionally, model validation techniques, such as hold-out validation and cross-validation, can be employed to assess a model&amp;rsquo;s ability to generalize to unseen data.&lt;/p&gt;
&lt;h2 id=&#34;bias&#34;&gt;Bias&lt;/h2&gt;
&lt;p&gt;Consider fitting a simple linear model to nonlinear data. The model will not be able to generalize well, regardless of the size of the training set. In fact, it would also exhibit poor performance when evaluated on the training set as well. When a model has not learned the patterns in the training data and is likewise unable to generalize to new data, it is known as &lt;strong&gt;underfitting&lt;/strong&gt;. In this case, such a model has &lt;strong&gt;high bias&lt;/strong&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-07-04_22-51-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Regardless of the dataset sampled, a linear model exhibits high bias.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Regardless of the dataset sampled, a linear model exhibits high bias.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;variance&#34;&gt;Variance&lt;/h2&gt;
&lt;p&gt;Variance is described in terms of the model fitting procedure and the training data. In terms of data, variance measures dispersion. It could also be interpreted as a measure of diversity. Sets with low variance contain samples that are close to the mean, and sampling from such a set would produce rather consistent data points.&lt;/p&gt;
&lt;p&gt;In terms of model fitting, a model that fits the training data well but not the test data describes &lt;strong&gt;overfitting&lt;/strong&gt;. This is because the training data is only an empirical sample of the true underlying distribution. A different sampling of the distribution may yield a set that more closely resembles the test set. Due to the &lt;strong&gt;variance&lt;/strong&gt; of the underlying distribution, our model overfits the patterns that exist in the training set.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-07-04_17-54-36_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A 5th degree polynomial trained on 3 different samplings of the distribution.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A 5th degree polynomial trained on 3 different samplings of the distribution.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;bias-variance-tradeoff&#34;&gt;Bias-Variance Tradeoff&lt;/h2&gt;
&lt;p&gt;If a model is not complex enough to capture the underlying distribution, it will perform poorly on both the training and test sets. Indeed, the model has low bias. If the model is too complex, it will exhibit low bias and high variance, overfitting the training set while failing to generalize well to unseen data. The solution then is to find a tradeoff between bias and variance with respect to the model complexity.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sequential Minimal Optimization</title>
      <link>https://ajdillhoff.github.io/notes/sequential_minimal_optimization/</link>
      <pubDate>Mon, 04 Jul 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/sequential_minimal_optimization/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#box-constraints&#34;&gt;Box Constraints&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#updating-the-lagrangians&#34;&gt;Updating the Lagrangians&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-algorithm&#34;&gt;The Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementation&#34;&gt;Implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Paper link:&lt;/strong&gt; &lt;a href=&#34;https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Sequential Minimal Optimization (SMO) is an algorithm to solve the SVM Quadratic Programming (QP) problem efficiently. Developed by John Platt at Microsoft Research, SMO deals with the constraints of the SVM objective by breaking it down into a smaller optimization problem at each step.&lt;/p&gt;
&lt;p&gt;The two key components of SMO are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;an analytic method to solving for two Lagrange multipliers at a time&lt;/li&gt;
&lt;li&gt;and a heuristic for choosing which multipliers to optimize.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The original objective is to maximize the margin between the nearest positive and negative examples.
For the linear case, if the output is given as&lt;/p&gt;
&lt;p&gt;\[
u = \mathbf{w}^T \mathbf{x} - b,
\]&lt;/p&gt;
&lt;p&gt;where \(\mathbf{w}\) is the normal vector to the hyperplane separating the classes, then the margin is given as&lt;/p&gt;
&lt;p&gt;\[
m = \frac{1}{\|w\|_2}.
\]&lt;/p&gt;
&lt;p&gt;Maximizing this margin yielded the primal optimization problem&lt;/p&gt;
&lt;p&gt;\begin{align*}
\min_{\mathbf{w},b} \frac{1}{2} \|\mathbf{w}\|^2\\
\textrm{s.t.} \quad &amp;amp; y_i(\mathbf{w}^T \mathbf{x} - b) \geq 1, \forall i\\
\end{align*}&lt;/p&gt;
&lt;p&gt;The dual form of the objective function for a &lt;a href=&#34;https://ajdillhoff.github.io/notes/support_vector_machine/&#34;&gt;Support Vector Machine&lt;/a&gt; is&lt;/p&gt;
&lt;p&gt;\[
\min_{\vec\alpha} \Psi(\vec{\alpha}) = \min_{\vec{\alpha}} \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N y_i y_j K(\mathbf{x}_i, \mathbf{x}_j)\alpha_i\alpha_j - \sum_{i=1}^N \alpha_i
\]&lt;/p&gt;
&lt;p&gt;with inequality constraints&lt;/p&gt;
&lt;p&gt;\[
\alpha_i \geq 0, \forall i,
\]&lt;/p&gt;
&lt;p&gt;and a linear equality constraint&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^N y_i \alpha_i = 0.
\]&lt;/p&gt;
&lt;p&gt;For a linear SVM, the output is dependent on a weight vector \(\mathbf{w}\) and threshold \(b\):&lt;/p&gt;
&lt;p&gt;\[
\mathbf{w} = \sum_{i=1}^N y_i \alpha_i \mathbf{x}_i, \quad b = \mathbf{w}^T \mathbf{x}_k - y_k.
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;The threshold is also dependent on the weight vector?&lt;/strong&gt;&lt;/strong&gt; The weight vector \(\mathbf{w}\) is computed using the training data. The threshold is only dependent on non-zero support vectors, \(\alpha_k &amp;gt; 0\).&lt;/p&gt;
&lt;h3 id=&#34;overlapping-distributions&#34;&gt;Overlapping Distributions&lt;/h3&gt;
&lt;p&gt;Slack variables were introduced to allow misclassifications at the cost of a linear penalty.
This is useful for datasets that are not linearly separable.
In practice, this is accomplished with a slight modification of the original objective function:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\min_{\mathbf{w},b} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^N \xi_i\\
\textrm{s.t.} \quad &amp;amp; y_i(\mathbf{w}^T \mathbf{x} - b) \geq 1 - \xi_i, \forall i\\
\end{align*}&lt;/p&gt;
&lt;p&gt;The convenience of this formulation is that the parameters \(\xi_i\) do not appear in the dual formulation at all.
The only added constraint is&lt;/p&gt;
&lt;p&gt;\[
0 \leq \alpha_i \leq C, \forall i.
\]&lt;/p&gt;
&lt;p&gt;This is referred to as the box constraint for reasons we shall see shortly.&lt;/p&gt;
&lt;h2 id=&#34;box-constraints&#34;&gt;Box Constraints&lt;/h2&gt;
&lt;p&gt;The smallest optimization step that SMO solves is that of two variables.
Given the constraints above, the solution lies on a diagonal line \(\sum_{i=1}^N y_i \alpha_i = 0\) bounded within a box \(0 \leq \alpha_i \leq C, \forall i\).&lt;/p&gt;
&lt;p&gt;Isolating for two samples with alphas \(\alpha_1\) and \(\alpha_2\), the constraint \(\sum_{i=1}^n y_i \alpha_i = 0\) suggests that&lt;/p&gt;
&lt;p&gt;\[
y_1 \alpha_1 + y_2 \alpha_2 = w.
\]&lt;/p&gt;
&lt;p&gt;We first consider the case when \(y_1 \neq y_2\).
Let \(y_1 = 1\) and \(y_2 = -1\), then \(a_1 - a_2 = w\).
As \(\alpha_1\) increases, \(\alpha_2\) must also increase to satisfy the constraint.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-07-10_22-48-56_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Equality constraint for case 1 (from Platt&amp;#39;s SMO paper).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Equality constraint for case 1 (from Platt&amp;rsquo;s SMO paper).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The other case is when \(y_1 = y_2\), then \(\alpha_1 + \alpha_2 = w\).
As \(\alpha_1\) is increased, \(\alpha_2\) is decreased to satisfy the constraint.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-07-10_22-51-53_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Box constraint for samples of the same class (from Platt&amp;#39;s SMO paper).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Box constraint for samples of the same class (from Platt&amp;rsquo;s SMO paper).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;updating-the-lagrangians&#34;&gt;Updating the Lagrangians&lt;/h2&gt;
&lt;p&gt;SMO solves for only two Lagrange multipliers at a time.
Solving for only 1 at a time would be impossible under the constraint \(\sum_{i=1}^N y_i \alpha_i = 0\).
The first step is to compute \(\alpha_2\) and constrain it between the ends of the diagonal line segment from the box constraints.&lt;/p&gt;
&lt;p&gt;If \(y_1 \neq y_2\), then the following bounds are applied to \(\alpha_2\):&lt;/p&gt;
&lt;p&gt;\begin{equation*}
L = \max(0, \alpha_2 - \alpha_1), \quad H = \min(C, C + \alpha_2 - \alpha_1)
\end{equation*}&lt;/p&gt;
&lt;p&gt;otherwise, the bounds are computed as:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
L = \max(0, \alpha_2 + \alpha_1 - C), \quad H = \min(C, \alpha_2 + \alpha_1)
\end{equation*}&lt;/p&gt;
&lt;p&gt;Updating the actual parameter is done following the update rule of gradient descent:&lt;/p&gt;
&lt;p&gt;\[
\alpha_2^{\text{new}} = \alpha_2 + \frac{y_2(E_1 - E_2)}{\eta}.
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do we arrive at this update rule?&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;second-derivative-of-the-objective-function&#34;&gt;Second Derivative of the Objective Function&lt;/h3&gt;
&lt;p&gt;Here, \(\eta\) represents the step size and direction. It is computed from the second derivative of the objective function along the diagonal line. To see that this is the case, consider the original objective function&lt;/p&gt;
&lt;p&gt;\begin{align*}
\min_{\mathbf{\alpha}} \quad &amp;amp; \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N y_i y_j K(\mathbf{x}_i, \mathbf{x}_j) \mathbf{\alpha}_1 \mathbf{\alpha}_2 - \sum_{i=1}^N \alpha_i\\
\textrm{s.t.} \quad &amp;amp; 0 \leq \alpha_i \leq C, \forall i\\
&amp;amp; \sum_{i=1}^N y_i \alpha_i = 0\\
\end{align*}&lt;/p&gt;
&lt;p&gt;Since we are optimizing with respect to only 2 Lagrangian multipliers at a time, we can write the Lagrangian function as&lt;/p&gt;
&lt;p&gt;\[
\frac{1}{2} y_1^2 K_{11} \alpha_1^2 + \frac{1}{2} y_2^2 K_{22} \alpha_2^2 + y_1 \alpha_1 \sum_{j=3}^N y_j \alpha_j K_{1j} + y_2 \alpha_2 \sum_{j=3}^N y_j \alpha_j K_{2j} - \alpha_1 - \alpha_2 + \sum_{j=3}^N \alpha_j
\]&lt;/p&gt;
&lt;p&gt;We are only optimizing with respect to \(\alpha_1\) and \(\alpha_2\), the next step is to extract those terms from the sum.
This is simplified further by noting that \(\sum_{j=3}^N y_j \alpha_j K_{ij}\) looks very similar to the output of an SVM:&lt;/p&gt;
&lt;p&gt;\[
u = \sum_{j=1}^N y_j \alpha_j K(\mathbf{x}_j, \mathbf{x}) - b.
\]&lt;/p&gt;
&lt;p&gt;This allows us to introduce a variable \(v_i\) based on \(u_i\), the output of an SVM given sample \(\mathbf{x}_i\):&lt;/p&gt;
&lt;p&gt;\[
v_i = \sum_{j=3}^N y_j \alpha_j K_{ij} = u_i + b - y_1 \alpha_1 K_{1i} - y_2 \alpha_2 K_{2i}.
\]&lt;/p&gt;
&lt;p&gt;The objective function is then written as&lt;/p&gt;
&lt;p&gt;\[
\frac{1}{2} y_1^2 K_{11} \alpha_1^2 + \frac{1}{2} y_2^2 K_{22} \alpha_2^2 + y_1 \alpha_1 v_1 + y_2 \alpha_2 v_2 - \alpha_1 - \alpha_2 + \sum_{j=3}^N \alpha_j.
\]&lt;/p&gt;
&lt;p&gt;Note that the trailing sum \(\sum_{j=3}^N \alpha_j\) is treated as a constant since those values are not considered when optimizing for \(\alpha_1\) and \(\alpha_2\).&lt;/p&gt;
&lt;p&gt;Given the box constraints from above, we must update \(\alpha_1\) and \(\alpha_2\) such that&lt;/p&gt;
&lt;p&gt;\[
\alpha_1 + s \alpha_2 = \alpha_1^* + s \alpha_2^* = w.
\]&lt;/p&gt;
&lt;p&gt;This linear relationship allows us to express the objective function in terms of α_2:&lt;/p&gt;
&lt;p&gt;\[
\Psi = \frac{1}{2} y_1^2 K_{11} (w - s \alpha_2)^2 + \frac{1}{2} y_2^2 K_{22} \alpha_2^2 + y_1 (w - s \alpha_2) v_1 + y_2 \alpha_2 v_2 - \alpha_1 - \alpha_2 + \sum_{j=3}^N \alpha_j.
\]&lt;/p&gt;
&lt;p&gt;The extremum of the function is given by the first derivative with respect to \(\alpha_2\):&lt;/p&gt;
&lt;p&gt;\[
\frac{d\Psi}{d\alpha_2} = -sK_{11}(w - s\alpha_2) + K_{22}\alpha_2 - K_{12}\alpha_2 + s K_{12} (w - s \alpha_2) - y_2 v_2 + s + y_2 v_2 - 1 = 0.
\]&lt;/p&gt;
&lt;p&gt;In most cases, the second derivative will be positive.
The minimum of \(\alpha_2\) is where&lt;/p&gt;
&lt;p&gt;\begin{align*}
\alpha_2 (K_{11} + K_{22} - 2 K_{12}) &amp;amp;= s(K_{11} - K_{12})w + y_2(v_1 - v_2) + 1 - s\\
&amp;amp;= s(K_{11} - K_{12})(s\alpha_2^*+\alpha_1^*)\\
&amp;amp;+ y_2(u_1-u_2+y_1\alpha_1^*(K_{12} - K_{11}) + y_2 \alpha_2^* (K_{22} - K_{21})) + y_2^2 - s\\
&amp;amp;= \alpha_2^*(K_{11}+K_{22} - 2K_{12}) + y_2(u_1 - u_2 + y_2 - y_1).
\end{align*}&lt;/p&gt;
&lt;p&gt;If we let \(E_1 = u_1 - y_1\), \(E_2 = u_2 - y_2\), and \(\eta = K_{11} + K_{22} - 2K_{12}\), then&lt;/p&gt;
&lt;p&gt;\[
\alpha_2^{\text{new}} = \alpha_2 + \frac{y_2(E_1 - E_2)}{\eta}.
\]&lt;/p&gt;
&lt;h2 id=&#34;the-algorithm&#34;&gt;The Algorithm&lt;/h2&gt;
&lt;p&gt;Sequential Minimal Optimization (SMO) solves the SVM problem which usually requires a Quadratic Programming (QP) solution.
It does this by breaking down the larger optimization problem into a small and simple form: solving for two Lagrangians.
Solving for one would not be possible without violating KKT conditions.
There are two components to Sequential Minimal Optimization: the first is how the Lagrangians are selected and the second is the actual optimization step.&lt;/p&gt;
&lt;h3 id=&#34;choosing-the-first-lagrangian&#34;&gt;Choosing the First Lagrangian&lt;/h3&gt;
&lt;p&gt;The algorithm first determines which samples in the dataset violate the given KKT conditions.
Only those violating the conditions are eligible for optimization.
Additionally, samples that are not on the bounds are selected (those with \(\alpha_i \neq 0\) and \(\alpha_i \neq C\)).
This continues through the dataset until no sample violates the KKT constraints within \(\epsilon\).&lt;/p&gt;
&lt;p&gt;As a last step, SMO searches the entire dataset to look for any bound samples that violate KKT conditions.&lt;/p&gt;
&lt;h3 id=&#34;choosing-the-second-lagrangian&#34;&gt;Choosing the Second Lagrangian&lt;/h3&gt;
&lt;p&gt;The second Lagrangian is chosen to maximize the size of the step taken during joint optimization.
Noting that the step size is based on&lt;/p&gt;
&lt;p&gt;\[
\alpha_2^{\text{new}} = \alpha_2 + \frac{y_2(E_1 - E_2)}{\eta},
\]&lt;/p&gt;
&lt;p&gt;it is approximated by computing \(|E_1 - E_2|\).&lt;/p&gt;
&lt;p&gt;If positive progress cannot be made given the choice of Lagrangian, SMO will begin iterating through non-bound examples.
If no eligible candidates are found in the non-bound samples, the entire dataset is searched.&lt;/p&gt;
&lt;h3 id=&#34;updating-the-parameters&#34;&gt;Updating the Parameters&lt;/h3&gt;
&lt;p&gt;With the second derivative of the objective function, we can take an optimization step along the diagonal line.
To ensure that this step adheres to the box constraints defined above, the new value of \(\alpha_2\) is clipped:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\alpha_2^{\text{new,clipped}} =
\begin{cases}
H &amp;amp;\text{if} \quad \alpha_2^{\text{new}} \geq H;\\
\alpha_2^{\text{new}} &amp;amp;\text{if} \quad L &amp;lt; \alpha_2^{\text{new}} &amp;lt; H;\\
L &amp;amp;\text{if} \quad \alpha_2^{\text{new}} \geq L.\\
\end{cases}
\end{equation*}&lt;/p&gt;
&lt;p&gt;With the new value of \(\alpha_2\), \(\alpha_1\) is computed such that the original KKT condition is preserved:&lt;/p&gt;
&lt;p&gt;\[
\alpha_1^{\text{new}} = \alpha_1 + s(\alpha_2 - \alpha_2^{\text{new,clipped}}),
\]&lt;/p&gt;
&lt;p&gt;where \(s = y_1y_2\).&lt;/p&gt;
&lt;p&gt;Points that are beyond the margin are given an alpha of 0: \(\alpha_i = 0\).
Points that are on the margin satisfy \(0 &amp;lt; \alpha_i &amp;lt; C\). These are the support vectors.
Points inside the margin satisfy \(\alpha_i = C\).&lt;/p&gt;
&lt;h4 id=&#34;linear-svms&#34;&gt;Linear SVMs&lt;/h4&gt;
&lt;p&gt;In the case of linear SVMs, the parameters can be stored as a single weight vector&lt;/p&gt;
&lt;p&gt;\[
\mathbf{w}^{\text{new}} = \mathbf{w} + y_1 (\alpha_1^{\text{new}} - \alpha_1)\mathbf{x}_1 + y_2(\alpha_2^{\text{new,clipped}} - \alpha_2)\mathbf{x}_2.
\]&lt;/p&gt;
&lt;p&gt;The output of a linear SVM is computed as&lt;/p&gt;
&lt;p&gt;\[
u = \mathbf{w}^T \mathbf{x} - b.
\]&lt;/p&gt;
&lt;h4 id=&#34;nonlinear-svms&#34;&gt;Nonlinear SVMs&lt;/h4&gt;
&lt;p&gt;In the nonlinear case, the output of the model is computed as&lt;/p&gt;
&lt;p&gt;\[
u = \sum_{i=1}^N y_i \alpha_i K(\mathbf{x}_i, \mathbf{x}) - b.
\]&lt;/p&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;An implementation of SMO in Python is available at &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/blob/main/svm/smo.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://github.com/ajdillhoff/CSE6363/blob/main/svm/smo.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Boosting</title>
      <link>https://ajdillhoff.github.io/notes/boosting/</link>
      <pubDate>Wed, 23 Mar 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/boosting/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#todo&#34;&gt;TODO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adaboost&#34;&gt;AdaBoost&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;todo&#34;&gt;TODO&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Gradient Boosting&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Combining predictions from multiple sources is usually preferred to a single source.
For example, a medical diagnosis would carry much more weight if it was the result of a consensus of several experts.
This idea of prediction by consensus is a powerful way to improve classification and regression models.
In fact, good performance of a committee of models can be achieved even if each individual model is conceptually very simple.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Boosting&lt;/strong&gt; is one such way of building a committee of models for classification or regression and is popularly implemented by an algorithm called &lt;strong&gt;AdaBoost&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;adaboost&#34;&gt;AdaBoost&lt;/h2&gt;
&lt;p&gt;Given a dataset \(\{\mathbf{x}_i\}\) and target variables \(\{\mathbf{y}_i\}\), AdaBoost first initializes a set of weights corresponding to each data sample as \(w_i = \frac{1}{N}\).
At each step of the algorithm, a simple classifier, called a &lt;strong&gt;weak learner&lt;/strong&gt; is fit to the data.
The weights for each sample are adjusted based on the individual classifier&amp;rsquo;s performance.
If the sample was misclassified, the relative weight for that sample is increased.
After all classifiers have been fit, they are combined to form an ensemble model.&lt;/p&gt;
&lt;h3 id=&#34;the-algorithm&#34;&gt;The Algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Initialize data weights \({w_i}\) as \(w_i^{(1)} = \frac{1}{n}\) for \(i = 1, \dots, n\).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fit each weak learner \(j\) to the training data by minimizing the misclassification cost:&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^n w_i^{(j)} \mathbb{1}(f_j(\mathbf{x}_i) \neq \mathbf{y}_i)
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute a weighted error rate&lt;/p&gt;
&lt;p&gt;\[
\epsilon_j = \frac{\sum_{i=1}^n w_i^{(j)} \mathbb{1}(f_j(\mathbf{x}_i) \neq \mathbf{y}_i)}{\sum_{i=1}^n w_i^{(j)}}
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the weighted error rate to compute a weight for each classifier such that misclassified samples are given higher weight:&lt;/p&gt;
&lt;p&gt;\[
\alpha_j = \ln \bigg\{\frac{1 - \epsilon_j}{\epsilon_j}\bigg\}.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update the data weights for the next model in the sequence:&lt;/p&gt;
&lt;p&gt;\[
w_i^{j+1} = w_i^{j} \exp\{\alpha_j \mathbb{1}(f_j(\mathbf{x}_i \neq \mathbf{y}_i)\}.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once all weak learners are trained, the final model predictions are given by&lt;/p&gt;
&lt;p&gt;\[
Y_M(\mathbf{x}) = \text{sign} \Bigg(\sum_{j=1}^M \alpha_j f_j(\mathbf{x})\Bigg).
\]&lt;/p&gt;
&lt;h3 id=&#34;weak-learners&#34;&gt;Weak Learners&lt;/h3&gt;
&lt;p&gt;The weak learners can be any classification or regression model.
However, they are typically chosen to be very simple to account for training time.
For example, a complex deep learning model would be a poor choice for a weak learner.&lt;/p&gt;
&lt;p&gt;One example of a weak learner is a simple linear model like a &lt;a href=&#34;https://ajdillhoff.github.io/notes/perceptron/&#34;&gt;Perceptron&lt;/a&gt; or decision stump.
A standard implementation of AdaBoost uses a decision tree with depth 1, as observed in &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html?highlight=boost#sklearn.ensemble.AdaBoostClassifier&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;sklearn&amp;rsquo;s implementation.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s put this together and walk through the first few steps of training an AdaBoost model using a decision stump as the weak learner. We will use a very simple dataset to keep the values easy to compute by hand.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Initial Data&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Weak Learner 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first learner is trained on the initial data and picks \(x_1 = 2.5\) as the split threshold.
Input where \(x_1 \leq 2.5\) is assigned to class 0 and all other samples are assigned class 1.
The data with this learner&amp;rsquo;s predictions are shown below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;th&gt;prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Error and weight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The error is simple enough to compute as all samples are currently weighted equally. Since two of the samples were misclassified, the error is the sum of their weights.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Total error&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(e_1 = 0.2 + 0.2 = 0.4\).&lt;/p&gt;
&lt;p&gt;The weight of the classifier can then be computed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classifier weight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(\alpha_1 = \frac{1}{2} \ln \big(\frac{1 - e_1}{e_1}\big) = 0.2027\).&lt;/p&gt;
&lt;p&gt;The weights of our data can now be updated using this value of \(\alpha_1\).
The weight of each example is updated by multiplying each correctly classifed sample by \(\exp\{-\alpha_1\}\) and each incorrectly classified sample by \(\exp\{\alpha\}\):&lt;/p&gt;
&lt;p&gt;\[
w_i^{j+1} = w_i^{j} \exp\{\alpha_j \mathbb{1}(f_j(\mathbf{x}_i \neq \mathbf{y}_i)\}.
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; You will notice that the equation above is different from the actual update rule that was applied to the weights in this example. In the original publication &lt;strong&gt;(TODO: reference Fruend)&lt;/strong&gt;, the weights are renormalized at the end of the loop. In this example, the normalization is combined with the update. In either case, the updated weights are shown below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Weak Learner 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The algorithm now moves to the next weak learner, which classifies the data given a threshold of \(x_1 = 3.5\). Its predictions are shown below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;th&gt;prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.250&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.250&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Only a single sample is misclassified, and the error is computed as before.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Total error&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(e_2 = 0.250\)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classifier weight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(\alpha_2 = \frac{1}{2} \ln \big(\frac{1 - e_2}{e_2}\big) = 0.5493\)&lt;/p&gt;
&lt;p&gt;The weights are updated for each sample, yielding the following data:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.500&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The second sample has been misclassified twice at this point, leading to a relatively high weight. This will hopefully be addressed by the third learner.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Weak Learner 3&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The final weak learner splits the data on \(x_2 = 6.5\), yielding the following output for each sample.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;th&gt;prediction&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.500&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Unfortunately, sample 2 is too tricky for any of our weak learners. The total error is shown below. Since this is a binary classification problem, the error suggests that our weak learner performs worse than random guessing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Total error&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(e_3 = 0.667\)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classifier weight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\(\alpha_3 = \frac{1}{2} \ln \big(\frac{1 - e_3}{e_3}\big) = -0.3473\)&lt;/p&gt;
&lt;p&gt;The negative value of the classifier weight suggests that its predictions will be reversed when evaluated. The updated weights of each data sample are given below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x1&lt;/th&gt;
&lt;th&gt;x2&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.375&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.125&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.167&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Final Classifier&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The final classifier is a weighted vote of the three weak learners, with the weights being the classifier weights we calculated (0.2027, 0.5493, and -0.3473). The negative weight means that the third learner&amp;rsquo;s predictions are reversed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Decision Trees</title>
      <link>https://ajdillhoff.github.io/notes/decision_trees/</link>
      <pubDate>Fri, 18 Mar 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/decision_trees/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-iris-dataset&#34;&gt;Example: Iris Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#growing-a-tree&#34;&gt;Growing a Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#examining-the-iris-classification-tree&#34;&gt;Examining the Iris Classification Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pruning-a-tree&#34;&gt;Pruning a Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-algorithm&#34;&gt;The Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/dmilla/introduction-to-decision-trees-titanic-dataset&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.kaggle.com/dmilla/introduction-to-decision-trees-titanic-dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;decision tree&lt;/strong&gt;, or Classification and Regression Trees (CART), is a model that recursively partitions the input space based on a collection of features.
The partitions are split based on very simple binary choices.
If yes, branch to the left; if no, branch to the right.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;Introduction/2022-03-18_13-03-12_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Regression tree (left) and its piecewise constant surface (right) (Source: _Machine Learning: A Probabilistic Perspective_ by Kevin P. Murphy).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Regression tree (left) and its piecewise constant surface (right) (Source: &lt;em&gt;Machine Learning: A Probabilistic Perspective&lt;/em&gt; by Kevin P. Murphy).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To compute the response, we represent each individual decision as a function \(\phi\) and sum the responses:&lt;/p&gt;
&lt;p&gt;\[
f(\mathbf{x}) = \mathbb{E}[y | \mathbf{x}] = \sum_{m=1}^M w_m \mathbb{1} (\mathbf{x} \in R_m) = \sum_{m=1}^M w_m \phi(\mathbf{x};\mathbf{v}_m),
\]&lt;/p&gt;
&lt;p&gt;where \(R_m\) is the \(m^{\text{th}}\) region, \(w_m\) is the mean response, and \(\mathbf{v}_m\) is the choice of variable to split on along with its threshold value.&lt;/p&gt;
&lt;h2 id=&#34;example-iris-dataset&#34;&gt;Example: Iris Dataset&lt;/h2&gt;
&lt;p&gt;To see this on real data, consider the Iris flower dataset.
For example, we will look at a decision tree model that classifies each flower into either &lt;strong&gt;setosa&lt;/strong&gt;, &lt;strong&gt;versicolor&lt;/strong&gt;, or &lt;strong&gt;virginica&lt;/strong&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-20_21-04-19_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Our initial Iris classifier.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Our initial Iris classifier.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;We are given data about a new iris and want to classify it using this tree.
Our sample has a sepal length of 6.1cm, a sepal width of 2.8cm, a petal length of 4.7cm, and a petal width of 1.2cm.
The first decision considers the petal width.
Our sample has a width of 1.2, so it continues down the right branch.&lt;/p&gt;
&lt;p&gt;The second decision consideres petal width again.
Since our sample does not have a width greater than 1.75, we continue down the left branch.&lt;/p&gt;
&lt;p&gt;At this point, the model was optimized to now consider the petal length.
Our length comes in at 4.7, just shy of going down the right path.&lt;/p&gt;
&lt;p&gt;We now arrive at the last decision.
Since our petal length is not greater than 1.65, the model classifies this sample as &lt;strong&gt;versicolor&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;growing-a-tree&#34;&gt;Growing a Tree&lt;/h2&gt;
&lt;p&gt;To grow a tree, a decision needs to be made as to whether or not the current set of data can be split based on some feature.
As such, there should be a reliable way of determining if a feature provided a good split.
This is evaluated using a cost function and selecting the feature and value corresponding to the minimum cost:&lt;/p&gt;
&lt;p&gt;\[
(j^*, t^*) = \text{arg} \min_{j\in\{1, \dots, D\}} \min_{t \in \mathcal{T}_j} \text{cost}(\{\mathbf{x}_i, y_i : x_{ij} \leq t\}) + \text{cost}(\{\mathbf{x}_i, y_i : x_{ij} &amp;gt; t\}).
\]&lt;/p&gt;
&lt;p&gt;In words, this function finds a value \(t\) such that groups the data with the lowest cost.
For a regression task, the cost function is typically defined as&lt;/p&gt;
&lt;p&gt;\[
\text{cost}(\mathcal{D}) = \sum_{i \in \mathcal{D}}(y_i - \bar{y})^2,
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\bar{y} = \frac{1}{|\mathcal{D}|}\sum_{i \in \mathcal{D}} y_i.
\]&lt;/p&gt;
&lt;p&gt;Splits that result in clusters with high variance may still see a higher cost, even though they are the minimum.&lt;/p&gt;
&lt;p&gt;As their alternative name implies, decision trees can also be used for classification.
The splits are still based on features and threshold values at each branch.
When a split is considered, a class-conditional probability is estimated for that data.
Given data satisfying \(X_j &amp;lt; t\), the class-conditional probability is&lt;/p&gt;
&lt;p&gt;\[
\hat{\pi}_c = \frac{1}{|\mathcal{D}|}\sum_{i \in \mathcal{D}} \mathbb{1}(y_i = c).
\]&lt;/p&gt;
&lt;p&gt;The common error functions used for classification are &lt;strong&gt;misclassification rate&lt;/strong&gt;, &lt;strong&gt;entropy&lt;/strong&gt;, and &lt;strong&gt;Gini index&lt;/strong&gt;.
Misclassification rate is computed by summing the number of misclassifications:&lt;/p&gt;
&lt;p&gt;\[
\frac{1}{|\mathcal{D}|} \sum_{i \in \mathcal{D}} \mathbb{1}(y_i \neq \hat{y}) = 1 - \hat{\pi}_{\hat{y}}.
\]&lt;/p&gt;
&lt;p&gt;Entropy is computed as&lt;/p&gt;
&lt;p&gt;\[
\mathbb{H}(\mathbb{\hat{\pi}}) = -\sum_{c=1}^C \hat{\pi}_c \log \hat{\pi}_c.
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gini index&lt;/strong&gt; computes the expected error rate.&lt;/p&gt;
&lt;p&gt;\[
G = \sum_{c=1}^C \hat{\pi}_c (1 - \hat{\pi}_c) = 1 - \sum_{c=1}^C \hat{\pi}_c^2
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-19_17-32-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Impurity measured for binary classification (Source: _Machine Learning: A Probabilistic Perspective_ by Kevin P. Murphy)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Impurity measured for binary classification (Source: &lt;em&gt;Machine Learning: A Probabilistic Perspective&lt;/em&gt; by Kevin P. Murphy)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Like entropy, it promotes an equal number of observations across all classes in a node.
For small values of \(\hat{\pi}\), the error is smaller than that of entropy.
If the dataset is imbalanced, entropy is typically favored as it penalizes imbalanced datasets more than Gini will.
Both will favor splits that result in one node being pure.&lt;/p&gt;
&lt;h3 id=&#34;stopping-growth&#34;&gt;Stopping Growth&lt;/h3&gt;
&lt;p&gt;If left unchecked, the algorithm to grow a tree will continue until the data can no longer be split.
In the trivial case, this will be when every data point represents a leaf node.
In order to prevent overfitting, there are several criteria that are considered.&lt;/p&gt;
&lt;h4 id=&#34;does-the-split-reduce-the-cost-enough&#34;&gt;Does the split reduce the cost enough?&lt;/h4&gt;
&lt;p&gt;It may be ideal to only split the data if the cost is reduced by some acceptable value.
The reduction can be computed by&lt;/p&gt;
&lt;p&gt;\[
\Delta = \text{cost}(\mathcal{D}) - \bigg(\frac{|\mathcal{D}_L|}{|\mathcal{D}|}\text{cost}(\mathcal{D}_L) + \frac{|\mathcal{D}_R|}{|\mathcal{D}|} \text{cost}(\mathcal{D}_R)\bigg).
\]&lt;/p&gt;
&lt;h4 id=&#34;has-the-tree-reached-some-maximum-depth&#34;&gt;Has the tree reached some maximum depth?&lt;/h4&gt;
&lt;p&gt;The depth of a tree is set as a hyperparameter.
Later, when we look at an example, we will use cross validation to select the best depth parameter for our model.&lt;/p&gt;
&lt;h4 id=&#34;is-the-distribution-of-the-split-pure&#34;&gt;Is the distribution of the split &lt;strong&gt;pure&lt;/strong&gt;?&lt;/h4&gt;
&lt;p&gt;If either of the splits is fully made up of data with the same label, there is no need to split it any further.&lt;/p&gt;
&lt;h4 id=&#34;is-the-split-too-small&#34;&gt;Is the split too small?&lt;/h4&gt;
&lt;p&gt;A split that is too small may lead to overfitting.&lt;/p&gt;
&lt;h2 id=&#34;examining-the-iris-classification-tree&#34;&gt;Examining the Iris Classification Tree&lt;/h2&gt;
&lt;p&gt;How exactly does the earlier example model make its decision at each node?
The full tree is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-20_21-11-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;A detailed view of our Iris classifier.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;A detailed view of our Iris classifier.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The first split is visually very simple to intuit.
Using petal width can perfectly split all setosa samples into a single leaf node.&lt;/p&gt;
&lt;h2 id=&#34;pruning-a-tree&#34;&gt;Pruning a Tree&lt;/h2&gt;
&lt;p&gt;Depending on the data, stopping growth based on measuring the relative decrease in error may not result in a model that performs well.
Image a dataset that requires multiple features to provide a sufficient classification.
If only one of the features is considered in isolation, it may provide no decrease in error.
A practical example of this is the XOR problem.
Splitting on \(x_1\) or \(x_2\) in isolation does not provide any indication about the true output.
It is only when \(x_1 \neq x_2\) does the output equal to 1.&lt;/p&gt;
&lt;p&gt;To rectify this, a tree can be grown until it is completely full before &lt;strong&gt;pruning&lt;/strong&gt; the branches
that result in the smallest increase in error.&lt;/p&gt;
&lt;h2 id=&#34;the-algorithm&#34;&gt;The Algorithm&lt;/h2&gt;
&lt;p&gt;The general algorithm is shown in MATLAB below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-matlab&#34; data-lang=&#34;matlab&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;% node = fitTree(node, D, depth)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;% Recursive function to learn a decision tree&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;% Returns the index of the current node.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;%&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;%   node  - The node index in obj.Nodes.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;%   D     - Indices to the current data.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;%   depth - Current depth of the tree.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; node = &lt;span style=&#34;color:#a6e22e&#34;&gt;fitTree&lt;/span&gt;(obj, node, D, depth)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;% Determine best split for the data and return the split&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    [j, t, dSplit, classDist] = obj.split(D, obj.Nodes(node).features);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    obj.Nodes(node).prediction = classDist;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    disp(classDist);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;% Use heuristic to determine if node is worth splitting&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; obj.splitNode(depth, classDist) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; true
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;% set the node test, the function that determines the branch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        obj.Nodes(node .test = {j, t};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        newFeatures = obj.Nodes(node).features(obj.Nodes(node).features &lt;span style=&#34;color:#f92672&#34;&gt;~=&lt;/span&gt; j);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;% set the child nodes to the left and right splits&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        obj.Nodes(node).children = zeros(size(dSplit, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        numNewNodes = size(dSplit, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i = &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; : numNewNodes
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            obj.Nodes(&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) = struct(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;prediction&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;test&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#75715e&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;features&amp;#39;&lt;/span&gt;, newFeatures, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;children&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;parent&amp;#39;&lt;/span&gt;, node);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            obj.Nodes(node).children(i)  = obj.fitTree(length(obj.Nodes), dSplit{i}, depth &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Hidden Markov Models</title>
      <link>https://ajdillhoff.github.io/notes/hidden_markov_models/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/hidden_markov_models/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-markov-assumption&#34;&gt;The Markov Assumption&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-viterbi-algorithm&#34;&gt;The Viterbi Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#estimating-parameters&#34;&gt;Estimating Parameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#expectation-maximization&#34;&gt;Expectation Maximization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This article is essentially a grok of a tutorial on HMMs by (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;RABINER 1989&lt;/a&gt;). It will be useful for the reader to reference the &lt;a href=&#34;https://courses.physics.illinois.edu/ece417/fa2017/rabiner89.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;original paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Up to this point, we have only explored &amp;ldquo;atomic&amp;rdquo; data points.
That is, all of the information about a particular sample is encapsulated into one vector.
Sequential data is easily represented by graphical models.
This article introduces Hidden Markov Models, a powerful probabilistic graphical model used in many applications from gesture recognition to natural language processing.&lt;/p&gt;
&lt;p&gt;There are many tasks for which we do not know the underlying process.
However, we can observe samples that are produced from such processes.
Music, gesture recognition, speech, text, etc.
All of these have some underlying process which forms their outputs together into a hopefully coherent sequence.
If we wish to make predictions about future samples given these sequences, we will need to make some guess
about the underlying processes defining their output.&lt;/p&gt;
&lt;h2 id=&#34;the-markov-assumption&#34;&gt;The Markov Assumption&lt;/h2&gt;
&lt;p&gt;Markov models make a convenient assumption about sequential data.
That is, all relevant information required for predicting future samples is captured in the current time step \(t\).
Given a joint distribution over an input of \(T\) frames, \(p(\mathbf{x}_{1:T})\), the Markov assumption allows us to represent it as&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{x}_{1:T}) = p(\mathbf{x}_1)\prod_{t=2}^T p(\mathbf{x}_t|\mathbf{x}_{t-1})
\]&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;A more complicated case is when we are attempting to model some unknown process that is responsible for the observations.
In this case, an ordinary Markov chain is not sufficient.
A &lt;strong&gt;hidden Markov model (HMM)&lt;/strong&gt; is defined by a set \(z_t \in \{1, \dots, K\}\) of discrete hidden states and an &lt;strong&gt;observation&lt;/strong&gt; model \(p(\mathbf{x}_i|z_t)\).
The joint probability distribution of this model is given by&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{z}, \mathbf{x}) = p(\mathbf{z})p(\mathbf{x}|\mathbf{z}) = \Big(p(z_1)\prod_{t=2}^Tp(z_t|z_{t-1})\Big)\Big(\prod_{t=1}^Tp(\mathbf{x}_t|z_t)\Big).
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-24_20-41-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;The observations y are generated by the latent states x. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;The observations y are generated by the latent states x. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Although the states themselves are discrete, the observations may be continuous: \(p(\mathbf{x}|z_t, \mathbf{\theta})\).
If they are discrete, they can be modeled by an observation matrix \(B\).
Continuous observations are typically modeled using a conditional Gaussian:&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{x}_t|z_t=k, \theta) = \mathcal{N}(\mathbf{x}_t|\mathbf{\mu}_k,\mathbf{\Sigma}_k).
\]&lt;/p&gt;
&lt;p&gt;Following &lt;a href=&#34;https://courses.physics.illinois.edu/ece417/fa2017/rabiner89.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Rabiner&lt;/a&gt;, an HMM can be characterized by&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The number of states in the model \(N\).&lt;/li&gt;
&lt;li&gt;The number of distinct observation symbols per state \(M\).&lt;/li&gt;
&lt;li&gt;The state probability distribution \(A = \{a_{ij}\}\), \(a_{ij} = p(z_t=j | z_{t-1} = i)\).&lt;/li&gt;
&lt;li&gt;The observation symbol probability distribution \(B = \{b_j(k)\} = p(\mathbf{x}_t = k|z_t = j)\).&lt;/li&gt;
&lt;li&gt;An initial state distribution \(\mathbf{\pi}_i = p(z_t = i)\).&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-24_20-42-34_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;HMM with observation probabilities and state transition probabilities. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;HMM with observation probabilities and state transition probabilities. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The observation probability distribution is commonly modeled as a Gaussian, Mixture of Gaussians, or Multinomial distribution. Thus, the parameter estimates for those distributions follow the likelihood estimates for each respective distribution.&lt;/p&gt;
&lt;p&gt;In his famous tutorial on HMMs, Rabiner addressed the three fundamental problems of HMMs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Given an observation sequence and model parameters, how do we compute the probability of the observation sequence given the parameters (likelihood)?&lt;/li&gt;
&lt;li&gt;Given an observation sequence and model parameters, how do we choose a state sequence which is optimal (decoding)?&lt;/li&gt;
&lt;li&gt;How do we adjust the model parameters (learning)?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;HMMs are able to solve several different inference problems.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Filtering&lt;/strong&gt; computes \(p(z_t | \mathbf{x}_{1:t})\). That is, we are computing this probability as new samples come in up to time \(t\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smoothing&lt;/strong&gt; is accomplished when we have all the data in the sequence.
This is expressed as \(p(z_t|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fixed lag smoothing&lt;/strong&gt; allows for a trade off between accuracy and delay. It is useful in cases where we might not have the full sequence, but we wish to compute \(p(z_{t-l}|\mathbf{x}_{1:t})\) for some \(l &amp;gt; 0\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Predictions&lt;/strong&gt; are represented as \(p(z_{t+h}|\mathbf{x}_{1:t})\), where \(h &amp;gt; 0\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MAP estimation&lt;/strong&gt; yields the most probably state sequence \(\text{arg}\max_{\mathbf{z}_{1:T}}p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;We can sample the &lt;strong&gt;posterior&lt;/strong&gt; \(p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).&lt;/li&gt;
&lt;li&gt;We can also compute \(p(\mathbf{x}_{1:T})\) by summing up over all hidden paths. This is useful for classification tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;We start by solving the first problem posited by (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;RABINER 1989&lt;/a&gt;).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Given an observation sequence and model parameters, how do we compute the probability of the observation sequence given the parameters?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That is, given some model parameters \(\lambda = (A, B, \pi)\), compute \(p(z_t|\mathbf{x}_{1:t})\).&lt;/p&gt;
&lt;h3 id=&#34;forwards-pass&#34;&gt;Forwards Pass&lt;/h3&gt;
&lt;p&gt;The forwards algorithm solves two problems of interest.
First, we want to know how well our current parameters explain the observation sequence.
That is, \(p(\mathbf{x}_{1:T}|\lambda)\).&lt;/p&gt;
&lt;p&gt;Second, we want to compute \(p(z_t | \mathbf{x}_{1:t})\).
To compute these in an efficient way, a recursive strategy is adopted.
Let the forward variable \(\alpha_t(i)\) be defined as&lt;/p&gt;
&lt;p&gt;\[
\alpha_t(i) = p(\mathbf{x}_{1:t}, z_t = i | \lambda).
\]&lt;/p&gt;
&lt;p&gt;The forwards algorithm is defined as 3 steps.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Initialization:&lt;/p&gt;
&lt;p&gt;\[
\alpha_1(i) = \pi_i b_i(\mathbf{x}_1),\quad 1 \leq i \leq N.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recursion:&lt;/p&gt;
&lt;p&gt;\[
\alpha_{t+1}(j) = \Big(\sum_{i=1}^N \alpha_t(i)a_{ij}\Big)b_j(\mathbf{x}_{t+1}),\quad 1 \leq t \leq T - 1,\quad 1 \leq j \leq N
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Termination:&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{x}_{1:T})  = \sum_{i=1}^N \alpha_T(i).
\]&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The recursive step is visualized as a lattice structure &lt;a href=&#34;#figure--lattice&#34;&gt;as seen below.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;figure--lattice&#34;&gt;&lt;/a&gt;&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-24_20-12-13_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;From Rabiner 1989.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;From Rabiner 1989.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;With this step, we have a solution for the first problem.
We can now calculate more efficiently the probability of our observations given the current model parameters.
This along with the following backwards pass will be essential for updating our model parameters.&lt;/p&gt;
&lt;p&gt;The forwards algorithm is also used to solve the &lt;strong&gt;filtering&lt;/strong&gt; problem.
To see how, consider \(p(z_t | \mathbf{x}_{1:t-1})\) right before time \(t\).&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(z_t=j|\mathbf{x}_{1:t-1}) = \sum_i p(z_t=j|z_{t-1}=i)p(z_{t-1}=i|\mathbf{x}_{1:t-1})
\end{equation*}&lt;/p&gt;
&lt;p&gt;When we update for time \(t\), we have that&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(z_t=j|\mathbf{x}_{1:t}) &amp;amp;= p(z_t=j|\mathbf{x}_t, \mathbf{x}_{1:t})\\
&amp;amp;=\frac{p(\mathbf{x}_t|z_t=j, \mathbf{x}_{1:t-1})p(z_t=j|\mathbf{x}_{1:t-1})}{p(\mathbf{x}_t|\mathbf{x}_{t-1})}
\end{align*}&lt;/p&gt;
&lt;p&gt;However, \(\mathbf{x}_{1:t-1}\) is conditionally independent given \(z_t\), so it becomes&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(z_t=j|\mathbf{x}_{1:t})=\frac{p(\mathbf{x}_t|z_t=j)p(z_t=j|\mathbf{x}_{1:t-1})}{p(\mathbf{x}_t|\mathbf{x}_{t-1})}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Writing out \(p(z_t=j|\mathbf{x}_{1:t-1})\) fully yields&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(z_t=j|\mathbf{x}_{1:t}) \propto p(\mathbf{x}_t|z_t=j)\sum_i p(z_t=j|z_{t-1}=i)p(z_{t-1}=i|\mathbf{x}_{1:t-1}).
\end{equation*}&lt;/p&gt;
&lt;p&gt;This is the recursion step from above!&lt;/p&gt;
&lt;p&gt;This can also be represented in terms of the \(\alpha\) variables from above. To compute \(p(z_t=i|\mathbf{x}_{1:t})\), we can use the definition of a conditional probability distribution:&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(z_t=i|\mathbf{x}_{1:t}) &amp;amp;= \frac{p(z_t=i, \mathbf{x}_{1:t})}{p(\mathbf{x}_{1:t})}\\
&amp;amp;= \frac{\alpha_t(i)}{\sum_{j=1}^N \alpha_t(j)}
\end{align*}&lt;/p&gt;
&lt;p&gt;Compared to the complexity of the explicit representation, the forwards pass needs only \(N^2T\) calculations.
As pointed out in (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;RABINER 1989&lt;/a&gt;), with 5 hidden states and an observation sequence of length 100, the forwards pass only needs around 3000 computations.
A direct calculation would require \(10^{72}\).&lt;/p&gt;
&lt;h3 id=&#34;backwards-pass&#34;&gt;Backwards Pass&lt;/h3&gt;
&lt;p&gt;When updating the parameters of our model, we will need to consider the entire observation sequence.
The forward pass did not require the entire sequence.
Instead, we can compute the probability of the observation up to some time \(t\).
The backwards pass begins by defining the variable&lt;/p&gt;
&lt;p&gt;\[
\beta_t(i) = p(\mathbf{x}_{t+1:T} | z_t = i).
\]&lt;/p&gt;
&lt;p&gt;We can utilize a recursive process similar to the forwards algorithm with the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Initialization:&lt;/p&gt;
&lt;p&gt;\[
\beta_T(i) = 1,\quad 1 \leq i \leq N
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recursion:&lt;/p&gt;
&lt;p&gt;\[
\beta_t(i) = \sum_{j=1}^N a_{ij}b_j(\mathbf{x}_{t+1})\beta_{t+1}(j),\quad t = T-1,\dots,1,\quad 1 \leq i \leq N.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Termination:&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{x}_{1:T}) = \sum_{j=1}^N \pi_j b_j(x_1) \beta_1(j)
\]&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The complexity of the backwards algorithm is similar to that of the forwards: \(N^2T\).&lt;/p&gt;
&lt;p&gt;With both the forward and backwards passes defined, we can compute the &lt;strong&gt;smoothing&lt;/strong&gt; problem:&lt;/p&gt;
&lt;p&gt;\[
p(z_t=i|\mathbf{x}_{1:T}) = \frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N \alpha_t(j)\beta_t(j)}
\]&lt;/p&gt;
&lt;h2 id=&#34;the-viterbi-algorithm&#34;&gt;The Viterbi Algorithm&lt;/h2&gt;
&lt;p&gt;With problem 1 out of the way, we turn our attention to problem 2.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Given an observation sequence and model parameters, how do we choose a state sequence which is optimal?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
\mathbf{z}^* = \text{arg}\max_{\mathbf{z}_{1:T}}p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})
\]&lt;/p&gt;
&lt;p&gt;With respect to the &lt;a href=&#34;#figure--lattice&#34;&gt;lattice diagram&lt;/a&gt;, this is equivalent to computing the shortest path.
This is accomplished via the &lt;strong&gt;Viterbi&lt;/strong&gt; algorithm, sometimes referred to as the max-sum algorithm.
As with the forwards-backwards algorithm, the Viterbi algorithm takes on a recursive approach.
It starts by defining an intermediate variable&lt;/p&gt;
&lt;p&gt;\[
\gamma_t(i) = p(z_t=i|\mathbf{x}_{1:T}).
\]&lt;/p&gt;
&lt;p&gt;Using the variables defined in the forwards-backwards algorithm, this can be expressed as&lt;/p&gt;
&lt;p&gt;\[
\gamma_t(i) = \frac{\alpha_t(i) \beta_t(i)}{\sum_{i=1}^N \alpha_t(i) \beta_t(i)}.
\]&lt;/p&gt;
&lt;p&gt;This \(\gamma_t(i)\), we can compute the most likely state at time \(t\):&lt;/p&gt;
&lt;p&gt;\[
z_t^* = \text{arg}\max_{1\leq i \leq N} \gamma_t(i), \quad 1 \leq t \leq T.
\]&lt;/p&gt;
&lt;p&gt;One problem with this approach alone is that the most likely state at a particular time \(t\) may not lead us to the most probable sequence of states.
As stated above, we need to maximize \(p(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\).
In order to tackle this efficiently, Viterbi employs a dynamic programming approach.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Initialization&lt;/p&gt;
&lt;p&gt;Start with the best initial state out of all states given the observation at \(t=1\).
Additionally, we want to record the index of each state through time so that the best path can be retraced.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\delta_1(i) &amp;amp;= \pi_i b_i(\mathbf{x}_1),\quad 1 \leq i \leq N\\
\psi_1(i) &amp;amp;= 0
\end{align*}&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recursion:&lt;/p&gt;
&lt;p&gt;The quantity \(\delta_t(i)\) represents the joint probability of state sequences and observations up to time \(t\) ending with state \(z_t=i\).
Thus, the recursive step is to maximize the probability of the intermediate output for \(t-1\):&lt;/p&gt;
&lt;p&gt;\[
\delta_t(j) = \max_{1 \leq i \leq N} (\delta_{t-1}(i) a_{ij})b_j(\mathbf{x}_t), \quad 2 \leq t \leq T,\quad 1 \leq j \leq N.
\]&lt;/p&gt;
&lt;p&gt;The corresponding index for this step is recorded in the path matrix:&lt;/p&gt;
&lt;p&gt;\[
\psi_t(j) = \text{arg}\max_{1 \leq i \leq N} \delta_{t-1}(i)a_{ij},\quad 2 \leq t \leq T,\quad 1 \leq j \leq N.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Termination&lt;/p&gt;
&lt;p&gt;The last step of the Viterbi algorithm completes the calcuation of the joint probability of state sequences and observations.&lt;/p&gt;
&lt;p&gt;\[
p^* = \max_{1 \leq i \leq N} \delta_T(i)
\]&lt;/p&gt;
&lt;p&gt;\[
\mathbf{z}_T^* = \text{arg}\max_{1 \leq i \leq N} \delta_T(i)
\]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Path Backtrace&lt;/p&gt;
&lt;p&gt;With the state sequence matrix recorded along the way, we can retrace it to get the most probable sequence:&lt;/p&gt;
&lt;p&gt;\[
z_t^* = \psi_{t+1}(z_{t+1}^*),\quad t = T-1, \cdots, 1.
\]&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;estimating-parameters&#34;&gt;Estimating Parameters&lt;/h2&gt;
&lt;p&gt;If the hidden states were fully observable, then updating our model parameters would be as straightforward as computing the maximum likelihood estimates for the model parameters \(\lambda = (A, B, \pi)\).
For \(A\) and \(\pi\), we first tally up the following counts:&lt;/p&gt;
&lt;p&gt;\[
\hat{a}_{ij} = \frac{N_{ij}}{\sum_j N_{ij}},
\]&lt;/p&gt;
&lt;p&gt;the number of times we expect to transition from \(i\) to \(j\) divided by the number of times we transition from \(i\) to any other state.&lt;/p&gt;
&lt;p&gt;For \(\pi\), we have&lt;/p&gt;
&lt;p&gt;\[
\hat{\pi_i} = \frac{N_i}{\sum_i N_i},
\]&lt;/p&gt;
&lt;p&gt;The number of times we expect to start in state \(i\) divided by the number of times we start in any other state.&lt;/p&gt;
&lt;p&gt;Estimating the parameters for \(B\) depends on which distribution we are using for our observation probabilities.
For a multinomial distribution, we would compute the number of times we are in state \(j\) and observe a symbol \(k\) divided by the number of times we are in state \(j\):&lt;/p&gt;
&lt;p&gt;\[
\hat{B}_{jk} = \frac{N_{jk}}{N_k},
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
N_{jk} = \sum_{i=1}^N \sum_{t=1}^T \mathbb{1} (z_{i, t}=j, x_{i, t}=k).
\]&lt;/p&gt;
&lt;p&gt;If the observation probability follows a Gaussian distribution, the MLEs for \(\mu\) and \(\mathbf{\Sigma}\) are&lt;/p&gt;
&lt;p&gt;\[
\hat{\mathbf{\mu}}_k = \frac{\bar{\mathbf{x}}_k}{N_k},\quad \hat{\mathbf{\Sigma}}_k = \frac{(\bar{\mathbf{x}}\bar{\mathbf{x}})_k^T - N_k \hat{\mathbf{\mu}}_k\hat{\mathbf{\mu}}_k^T}{N_k},
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\bar{\mathbf{x}}_k = \sum_{i=1}^N \sum_{t=1}^T \mathbb{1}(z_{i, t}=k)\mathbf{x}_{i, t}
\]&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;\[
(\bar{\mathbf{x}}\bar{\mathbf{x}})_k^T) = \sum_{i=1}^N \sum_{t=1}^T \mathbb{1} (z_{i, t}=k)\mathbf{x}_{i,k}\mathbf{x}_{i,k}^T.
\]&lt;/p&gt;
&lt;h2 id=&#34;expectation-maximization&#34;&gt;Expectation Maximization&lt;/h2&gt;
&lt;p&gt;Of course, HMMs have hidden states which are not fully observable.
Thus, we need to come up with another strategy for updating our parameters based on the observable data.
The intuition behind this approach is as follows.
We first start out by using our current parameters to estimate the missing data, making it complete.
Initially, we may randomize our estimates if we have no good heuristic or guess as to what they should be.&lt;/p&gt;
&lt;p&gt;With the completed data, we can update our current parameters.
In other words, the expected values of the sufficient statistics can be derived now that the data has been filled in.
A new set of parameters is found such that it maximizes the likelihood function with respect to the estimated data.&lt;/p&gt;
&lt;h3 id=&#34;e-step&#34;&gt;E Step&lt;/h3&gt;
&lt;p&gt;Following (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;RABINER 1989&lt;/a&gt;), we start with the joint probability of being in state \(i\) at time \(t\) and state \(j\) at time \(t+1\):&lt;/p&gt;
&lt;p&gt;\[
\xi_t(i, j) = p(z_t = i, z_{t+1} = j|\mathbf{x}_{1:T}).
\]&lt;/p&gt;
&lt;p&gt;This can be computed using the forwards-backwards algorithm:&lt;/p&gt;
&lt;p&gt;\[
\xi_t(i, j) = \frac{\alpha_t(i)a_{ij}b_j(\mathbf{x}_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^N \sum_{j=1}^N \alpha_t(i)a_{ij}b_j(\mathbf{x}_{t+1})\beta_{t+1}(j)}.
\]&lt;/p&gt;
&lt;p&gt;This can be related back to \(\gamma_t(i)\) by summing over over \(j\):&lt;/p&gt;
&lt;p&gt;\[
\gamma_t(i) = \sum_{j=1}^N \xi_t(i, j).
\]&lt;/p&gt;
&lt;p&gt;Here, \(\gamma_t(i)\) is the expected number of times we transition from \(z = i\).
Summing over all \(t\) yields the expected transitions from \(z_i\) over all time steps:&lt;/p&gt;
&lt;p&gt;\[
\sum_{t=1}^{T-1} \gamma_t(i).
\]&lt;/p&gt;
&lt;p&gt;Since \(\xi_t(i, j)\) is the expected transition from \(i\) to \(j\) at time \(t\), we can compute the total number of transitions from \(i\) to \(j\) via&lt;/p&gt;
&lt;p&gt;\[
\sum_{t=1}^{T-1} \xi_t(i, j).
\]&lt;/p&gt;
&lt;h3 id=&#34;m-step&#34;&gt;M Step&lt;/h3&gt;
&lt;p&gt;The previous &lt;strong&gt;E Step&lt;/strong&gt; computed the expected values given the current parameter estimates.
Now that the data is complete, we can update our parameter estimates.
Starting with the transition probabilities, we must add the expected number of transitions from \(i\) to \(j\) and divide by the expected number of times we transition from \(i\).
Using the parameters from the E Step, this can be written&lt;/p&gt;
&lt;p&gt;\[
\hat{a}_{ij} = \frac{\sum_{t=1}^{T-1}\xi_t(i, j)}{\sum_{t=1}^{T-1}\gamma_t(i)}.
\]&lt;/p&gt;
&lt;p&gt;The initial state probability at \(t=1\) is the number of times we expect to be in state \(z=i\) at \(t=1\):&lt;/p&gt;
&lt;p&gt;\[
\gamma_1(i).
\]&lt;/p&gt;
&lt;p&gt;Finally, the observation probability parameters are updated by considering the number of times we are in state \(z=j\) and observing \(x=k\) divided by the number of times we are in state \(z=j\). Note that this is for a multinomial probabiliy distribution:&lt;/p&gt;
&lt;p&gt;\[
\hat{b}_j(k) = \frac{\sum_{t=1, x_t = k}^T \gamma_t(j)}{\sum_{t=1}^T \gamma_t(j)}.
\]&lt;/p&gt;
&lt;p&gt;These formulas are derived from maximizing Baum&amp;rsquo;s auxiliary function&lt;/p&gt;
&lt;p&gt;\[
Q(\lambda, \hat{\lambda}) = \sum_{Q} p(\mathbf{z}|\mathbf{x}, \lambda) \log p(\mathbf{x}, \mathbf{z}|\hat{\lambda})
\]&lt;/p&gt;
&lt;p&gt;over \(\hat{\lambda}\). It has further been shown that maximizing this function leads to increased likelihood:&lt;/p&gt;
&lt;p&gt;\[
\max_{\hat{\lambda}} Q(\lambda, \hat{\lambda}) \implies p(\mathbf{x}|\hat{\lambda}) \geq p(\mathbf{x}|\lambda).
\]&lt;/p&gt;
&lt;p&gt;If we have a Gaussian observation model, the values for \(\hat{b}_j(k)\) are computed to accommodate the parameters of the distribution.
These parameter estimates assume a Gaussian mixture model.
Starting with \(\hat{\mu}_{jk}\), it can be estimated by dividing the expected value of observations belonging to Gaussian density \(k\) by the expected number of times we are in state \(j\) using the \(k^{\text{th}}\) mixture component:&lt;/p&gt;
&lt;p&gt;\[
\hat{\mathbf{\mu}}_{jk} = \frac{\sum_{t=1}^T \gamma_t(j, k)\mathbf{x}_t}{\sum_{t=1}^T \gamma_t(j, k)}.
\]&lt;/p&gt;
&lt;p&gt;Here, \(\gamma_t(j, k)\) is the probability of being in state \(j\) at time \(t\) with the \(k^{\text{th}}\) mixture component accounting for \(\mathbf{x}_t\):&lt;/p&gt;
&lt;p&gt;\[
\gamma_t(j, k) = \frac{\alpha_t(j)\beta_t(j)}{\sum_{j=1}^N \alpha_t(j) \beta_t(j)} \frac{c_{jk}\mathcal{N}(\mathbf{x}_t, \mu_{jk}, \mathbf{\Sigma}_{jk})}{\sum_{m=1}^M c_{jm}\mathcal{N}(\mathbf{x}_t, \mu_{jm}, \mathbf{\Sigma}_{jm})}.
\]&lt;/p&gt;
&lt;p&gt;This method is proven to improve the parameters.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Each iteration is guaranteed to improve the log-likelihood function.&lt;/li&gt;
&lt;li&gt;The process is guaranteed to converge.&lt;/li&gt;
&lt;li&gt;The convergence point is a fixed point of the likelihood function.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These guarantees are similar to gradient ascent.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;RABINER, LAWRENCE R. 1989. “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.” &lt;i&gt;Proceedings of the Ieee&lt;/i&gt; 77 (2): 30.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Kernels</title>
      <link>https://ajdillhoff.github.io/notes/kernels/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/kernels/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dual-representation&#34;&gt;Dual Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#relating-back-to-the-original-formulation&#34;&gt;Relating Back to the Original Formulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#types-of-kernels&#34;&gt;Types of Kernels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#constructing-kernels&#34;&gt;Constructing Kernels&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Notebook link: &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/blob/main/svm/kernels.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://github.com/ajdillhoff/CSE6363/blob/main/svm/kernels.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Parametric models use training data to estimate a set of parameters that can then be used to perform inference on new data.
An alternative approach uses &lt;strong&gt;nonparametric methods&lt;/strong&gt;, meaning the function is estimated directly from the data instead of optimizing a set of parameters.&lt;/p&gt;
&lt;p&gt;One possible downside to such an approach is that it becomes less efficient as the amount of training data increases.
Additionally, the transformation into a feature space such that the data becomes linearly separable may be intractable.
Consider sequential data such as text or audio.
If each sample has a variable number of features, how do we account for this using standard linear models with a fixed number of parameters?&lt;/p&gt;
&lt;p&gt;The situations described above can be overcome through the use of the &lt;strong&gt;kernel trick&lt;/strong&gt;.
We will see that, by computing a measure of similarity between samples in the feature space, we do not need to directly transform each individual sample to that space.&lt;/p&gt;
&lt;p&gt;A kernel function is defined as&lt;/p&gt;
&lt;p&gt;\[
k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = \phi(\mathbf{x})^T \phi(\mathbf{x}&amp;rsquo;),
\]&lt;/p&gt;
&lt;p&gt;where \(\phi\) is some function which transforms the input to a feature space.&lt;/p&gt;
&lt;p&gt;Methods that require part or all of the training data to make prediction will benefit from using kernel representations, especially when using high dimensional data. Instead of transforming the data into a high dimensional space which may be computationally intractable, a measure of similarity via the &lt;em&gt;inner product&lt;/em&gt; is used. The inner product is not the projection into some space. Instead, it represents the outcome of that projection.&lt;/p&gt;
&lt;p&gt;If the input vector takes on the form of scalar products, it can be represented as a kernel function.&lt;/p&gt;
&lt;h2 id=&#34;dual-representation&#34;&gt;Dual Representation&lt;/h2&gt;
&lt;p&gt;The key to taking advantage of the kernel trick relies on reformulating our linear model into a dual representation.
In this form, we will establish a dependence on the kernel function.&lt;/p&gt;
&lt;p&gt;The following derivation of the dual representation for linear regression follows (Bishop). Consider the least squares loss with \(L2\) regularization, as we discussed with &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_regression/&#34;&gt;Linear Regression&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;\[
J(\mathbf{w}) = \frac{1}{2}\sum_{i=1}^n(\mathbf{w}^T\phi(\mathbf{x}_i) - y_i)^2 + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}
\]&lt;/p&gt;
&lt;p&gt;Here, \(\phi\) is a basis function that transforms the input. This could also be a simple identity function in which \(\phi(\mathbf{x}) = \mathbf{x}\). To solve for \(\mathbf{w}\), we take the gradient of \(J(\mathbf{w})\) with respect to \(\mathbf{w}\) and set it to 0.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\nabla_{\mathbf{w}}J(\mathbf{w}) &amp;amp;= \sum_{i=1}^n(\mathbf{w}^T\phi(\mathbf{x}_i) - y_i)\phi(\mathbf{x}_i) + \lambda \mathbf{w}\\
\implies \mathbf{w} &amp;amp;= -\frac{1}{\lambda}\sum_{i=1}^n(\mathbf{w}^T\phi(\mathbf{x}_i) - y_i)\phi(\mathbf{x}_i)
\end{align*}&lt;/p&gt;
&lt;p&gt;We can formulate this as a matrix-vector product by letting&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{\Phi} =
\begin{bmatrix}
\phi(\mathbf{x}_1)^T\\
\vdots \\
\phi(\mathbf{x}_n)^T\\
\end{bmatrix}
\text{ and }
a_{i} = -\frac{1}{\lambda}(\mathbf{w}^T\phi(\mathbf{x}_i) - y_i).
\end{equation*}&lt;/p&gt;
&lt;p&gt;Then, \(\mathbf{w} = \mathbf{\Phi}^T\mathbf{a}\), where \(\mathbf{a} = [a_1, \dots, a_n]^T\).&lt;/p&gt;
&lt;p&gt;The dual representation is derived by reformulating \(J(\mathbf{w})\) in terms of \(\mathbf{a}\).&lt;/p&gt;
&lt;p&gt;\begin{equation*}
J(\mathbf{a}) = \frac{1}{2}\mathbf{a}^T\mathbf{\Phi}\mathbf{\Phi}^T\mathbf{\Phi}\mathbf{\Phi}^T\mathbf{a} - \mathbf{a}^T\mathbf{\Phi}\mathbf{\Phi}^T\mathbf{y} + \frac{1}{2}\mathbf{y}^T\mathbf{y} + \frac{\lambda}{2} \mathbf{a}^T\mathbf{\Phi}\mathbf{\Phi}^T\mathbf{a},
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(\mathbf{y} = [y_1, \dots, y_n]\).&lt;/p&gt;
&lt;p&gt;Looking at the products \(\mathbf{\Phi}\mathbf{\Phi}^T\), we see that these relate to our original kernel form: \(\phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j)\). This product defines a &lt;a href=&#34;https://en.wikipedia.org/wiki/Gram_matrix&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Gram matrix&lt;/a&gt; \(\mathbf{K} = \mathbf{\Phi}\mathbf{\Phi}^T\) whose elements are \(k(\mathbf{x}_i, \mathbf{x}_j)\). Thus, we can rewrite \(J(\mathbf{a})\) as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
J(\mathbf{a}) = \frac{1}{2}\mathbf{a}^T\mathbf{K}\mathbf{K}\mathbf{a} - \mathbf{a}^T\mathbf{K}\mathbf{y} + \frac{1}{2}\mathbf{y}^T\mathbf{y} + \frac{\lambda}{2}\mathbf{a}^T\mathbf{K}\mathbf{a}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Solving for \(\mathbf{a}\) can be done by computing the gradient of \(J(\mathbf{a})\) with respect to \(\mathbf{a}\) and setting the result to 0.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\nabla_\mathbf{a}J(\mathbf{a}) = \mathbf{K}\mathbf{K}\mathbf{a} - \mathbf{K}\mathbf{y} + \lambda \mathbf{K}\mathbf{a} &amp;amp;= 0\\
\mathbf{K}\mathbf{a} + \lambda I\mathbf{a} - \mathbf{y} &amp;amp;= 0\\
(\mathbf{K} + \lambda I)\mathbf{a} &amp;amp;= \mathbf{y}\\
\mathbf{a} &amp;amp;= (\mathbf{K} + \lambda I)^{-1} \mathbf{y}.
\end{align*}&lt;/p&gt;
&lt;p&gt;With \(\mathbf{a}\) solved, we can complete the dual representation of our original linear regression model. Recall that&lt;/p&gt;
&lt;p&gt;\begin{equation*}
h(\mathbf{x}; \mathbf{w}) = \mathbf{w}^T\phi(\mathbf{x}).
\end{equation*}&lt;/p&gt;
&lt;p&gt;If we substitute \(\mathbf{w} = \mathbf{\Phi}^T\mathbf{a}\), we get&lt;/p&gt;
&lt;p&gt;\begin{align*}
f(\mathbf{x};\mathbf{a}) &amp;amp;= \mathbf{a}^T\mathbf{\Phi}\phi(\mathbf{x})\\
&amp;amp;= \Big[(\mathbf{K} + \lambda I)^{-1}\mathbf{y})\Big]^T\mathbf{\Phi}\phi(\mathbf{x}).
\end{align*}&lt;/p&gt;
&lt;p&gt;Again, the kernel form is apparent in the product \(\mathbf{\Phi}\phi(\mathbf{x})\). If we let \(k_i(\mathbf{x}) = k(\mathbf{x}_i,\mathbf{x})\) and&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{k}(\mathbf{x}) =
\begin{bmatrix}
k_1(\mathbf{x})\\
\vdots \\
k_n(\mathbf{x})
\end{bmatrix},
\end{equation*}&lt;/p&gt;
&lt;p&gt;we can write the dual representation of our linear regression model as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f(\mathbf{x}) = \mathbf{k}(\mathbf{x})^T(\mathbf{K} + \lambda \mathbf{I})^{-1}\mathbf{y}.
\end{equation*}&lt;/p&gt;
&lt;h2 id=&#34;relating-back-to-the-original-formulation&#34;&gt;Relating Back to the Original Formulation&lt;/h2&gt;
&lt;p&gt;In this dual formulation, the solution for \(\mathbf{a}\) can be expressed as a linear combination of elements \(\phi(\mathbf{x})\).
From above, we see that&lt;/p&gt;
&lt;p&gt;\[
a_i = -\frac{1}{\lambda}\big(\mathbf{w}^T\phi(\mathbf{x}_i) - y_i\big).
\]&lt;/p&gt;
&lt;p&gt;Expanding this into individual coefficients yields&lt;/p&gt;
&lt;p&gt;\begin{align*}
a_i &amp;amp;= -\frac{1}{\lambda}\big(w_1\phi_1(\mathbf{x}_i) + \cdots + w_m \phi_m(\mathbf{x}_i) - y_i\big)\\
&amp;amp;= -\frac{w_1}{\lambda}\phi_1(\mathbf{x}_i) - \cdots - \frac{w_m}{\lambda} \phi_m(\mathbf{x}_i) + \frac{y_i}{\lambda}.
\end{align*}&lt;/p&gt;
&lt;p&gt;We are close, but we still need to do something about the term \(\frac{y_i}{\lambda}\). For this, we can multiply both sides of our equation by a convenient 1. That is, we multiply by&lt;/p&gt;
&lt;p&gt;\[
\frac{\phi_1(\mathbf{x}_i) + \cdots + \phi_m(\mathbf{x}_i)}{\phi_1(\mathbf{x}_i) + \cdots + \phi_m(\mathbf{x}_i)}.
\]&lt;/p&gt;
&lt;p&gt;By doing this and grouping the \(\phi_j\) terms, we get&lt;/p&gt;
&lt;p&gt;\begin{align*}
&amp;amp;\Big(\frac{y_i}{\lambda}\cdot \frac{1}{\phi_1(\mathbf{x}_i) + \cdots + \phi_m(\mathbf{x}_i)} - \frac{w_1}{\lambda}\Big)\phi_1(\mathbf{x}_i) + \cdots\\
&amp;amp;+ \Big(\frac{y_i}{\lambda}\cdot \frac{1}{\phi_1(\mathbf{x}_i) + \cdots + \phi_m(\mathbf{x}_i)} - \frac{w_m}{\lambda}\Big)\phi_m(\mathbf{x}_i).
\end{align*}&lt;/p&gt;
&lt;p&gt;We can simplify this by introducing a term&lt;/p&gt;
&lt;p&gt;\[
c_i = \frac{y_i}{\lambda}\cdot \frac{1}{\phi_1(\mathbf{x}_i) + \cdots + \phi_m(\mathbf{x}_i)}.
\]&lt;/p&gt;
&lt;p&gt;Then the solution can be rewritten as&lt;/p&gt;
&lt;p&gt;\[
\Big(c_i - \frac{w_1}{\lambda}\Big)\phi_1(\mathbf{x}_i) + \cdots + \Big(c_i - \frac{w_m}{\lambda}\Big)\phi_m(\mathbf{x}_i).
\]&lt;/p&gt;
&lt;p&gt;With this, we can step backwards using intermediate results in the previous section to get back to the original formulation of our linear regression model.&lt;/p&gt;
&lt;h2 id=&#34;types-of-kernels&#34;&gt;Types of Kernels&lt;/h2&gt;
&lt;p&gt;There are several types of kernels that can be used to transform the input data depending on the problem. The simplest kernel is the &lt;strong&gt;identity kernel:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\[
k(\mathbf{x}, \mathbf{x&amp;rsquo;}) = \mathbf{x}^T \mathbf{x&amp;rsquo;}.
\]&lt;/p&gt;
&lt;h3 id=&#34;polynomial-kernel&#34;&gt;Polynomial Kernel&lt;/h3&gt;
&lt;p&gt;A polynomial kernel is defined as&lt;/p&gt;
&lt;p&gt;\[
k(\mathbf{x}, \mathbf{x&amp;rsquo;}) = (\mathbf{x}^T\mathbf{x&amp;rsquo;}+c)^d.
\]&lt;/p&gt;
&lt;p&gt;This is a common choice for solving problems akin to polynomial regression.
We can use this kernel to present a visual explanation of kernel functions.
Consider the following dataset.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-19_22-06-55_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Binary classification dataset that is not linearly separable.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Binary classification dataset that is not linearly separable.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;It is easy enough to see that this dataset could not be separated using a hyperplane in 2D.
We could separate the two using some nonlinear decision boundary like a circle.
If we could transform this into 3D space, we could come up with some features such that it is linearly separable in 3D.
For example, let \(\phi(\mathbf{x}) = (x_1^2, x_2^2, \sqrt{2}x_1x_2)\).&lt;/p&gt;
&lt;p&gt;Transforming all points and visualizing yields the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-19_22-11-36_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Binary classification dataset transformed into a 3D feature space.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Binary classification dataset transformed into a 3D feature space.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;From this perspective, we can clearly see that the data is linearly separable.
The question remains: if we only have the original 2D features, how do we compare points in this 3D features space without explicitly transforming each point?
The kernel function corresponding to the feature transform above is&lt;/p&gt;
&lt;p&gt;\begin{align*}
k(\mathbf{x}, \mathbf{x}&amp;rsquo;) &amp;amp;= (\mathbf{x}^T\mathbf{x}&amp;rsquo;)^2\\
&amp;amp;= (x_1x&amp;rsquo;_1 + x_2x&amp;rsquo;_2)^2\\
&amp;amp;= 2x_1x&amp;rsquo;_1x_2x&amp;rsquo;_2 + (x_1x&amp;rsquo;_1)^2 + (x_2x&amp;rsquo;_2)^2\\
&amp;amp;= \phi(\mathbf{x})^T \phi(\mathbf{x}&amp;rsquo;)
\end{align*}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\phi(\mathbf{x}) =
\begin{bmatrix}
\sqrt{2}x_1x_2\\
x_1^2\\
x_2^2
\end{bmatrix}.
\]&lt;/p&gt;
&lt;h3 id=&#34;radial-basis-function-kernel&#34;&gt;Radial Basis Function Kernel&lt;/h3&gt;
&lt;p&gt;This kernel follows a Gaussian term and is commonly used with &lt;a href=&#34;https://ajdillhoff.github.io/notes/support_vector_machine/&#34;&gt;SVMs&lt;/a&gt;. It is defined as&lt;/p&gt;
&lt;p&gt;\[
k(\mathbf{x}, \mathbf{x&amp;rsquo;}) = \exp\Big(-\frac{\|\mathbf{x}-\mathbf{x&amp;rsquo;}\|^2}{2\sigma^2}\Big).
\]&lt;/p&gt;
&lt;h3 id=&#34;cosine-similarity&#34;&gt;Cosine Similarity&lt;/h3&gt;
&lt;p&gt;Consider the problem of comparing text sequences for a document classification task.
One approach is to compare the number of occurrences of each word.
The idea is that documents that are similar will have a similar number of words that occur.&lt;/p&gt;
&lt;p&gt;\[
k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = \frac{\mathbf{x}^T \mathbf{x}&amp;rsquo;}{\|\mathbf{x}\|_2 \|\mathbf{x}&amp;rsquo;\|_2}
\]&lt;/p&gt;
&lt;p&gt;Documents that are &lt;strong&gt;orthogonal&lt;/strong&gt;, in the sense that the resulting cosine similarity is 0, are dissimilar.
The similarity increases as the score approaches 1.
There are several issues with this approach which are addressed by using the term frequence-inverse document frequency (TF-IDF) score.&lt;/p&gt;
&lt;h2 id=&#34;constructing-kernels&#34;&gt;Constructing Kernels&lt;/h2&gt;
&lt;p&gt;A valid kernel function must satisfy the following conditions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Symmetry: \(k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = k(\mathbf{x}&amp;rsquo;, \mathbf{x})\)&lt;/li&gt;
&lt;li&gt;Positive semi-definite: \(k(\mathbf{x}, \mathbf{x}&amp;rsquo;) \geq 0\)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the feature space can be represented as a dot product, then it will satisfy the first condition by definition. The second condition can be shown by constructing a Gram matrix \(\mathbf{K}\) and showing that it is positive semi-definite. A matrix \(\mathbf{K}\) is positive semi-definite if and only if \(\mathbf{v}^T\mathbf{K}\mathbf{v} \geq 0\) for all \(\mathbf{v} \in \mathbb{R}^n\).&lt;/p&gt;
&lt;h3 id=&#34;direct-construction-of-a-kernel&#34;&gt;Direct Construction of a Kernel&lt;/h3&gt;
&lt;p&gt;In this approach, we define a feature space \(\phi(\mathbf{x})\) and then compute the kernel function as&lt;/p&gt;
&lt;p&gt;\[
k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = \phi(\mathbf{x})^T \phi(\mathbf{x}&amp;rsquo;).
\]&lt;/p&gt;
&lt;p&gt;This is the approach used in the example from above. In that example, we used the kernel function \(k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = (\mathbf{x}^T\mathbf{x}&amp;rsquo;)^2\). For our 2D input, the feature space is \(\phi(\mathbf{x}) = (x_1^2, x_2^2, \sqrt{2}x_1x_2)\). It is easy to see that the kernel function is the dot product of the feature space, and its kernel matrix is positive semi-definite.&lt;/p&gt;
&lt;h3 id=&#34;construction-from-other-valid-kernels&#34;&gt;Construction from other valid kernels&lt;/h3&gt;
&lt;p&gt;As a more convenient approach, it is possible to construct complex kernels from known kernels. Given valid kernels \(k_1(\mathbf{x}, \mathbf{x}&amp;rsquo;)\) and \(k_2(\mathbf{x}, \mathbf{x}&amp;rsquo;)\), we can construct a new kernel \(k(\mathbf{x}, \mathbf{x}&amp;rsquo;)\) using the following operations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;\(k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = ck_1(\mathbf{x}, \mathbf{x}&amp;rsquo;)\) for \(c &amp;gt; 0\)&lt;/li&gt;
&lt;li&gt;\(k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = f(\mathbf{x})k_1(\mathbf{x}, \mathbf{x}&amp;rsquo;)f(\mathbf{x}&amp;rsquo;)\) for \(f(\mathbf{x})\)&lt;/li&gt;
&lt;li&gt;\(k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = k_1(\mathbf{x}, \mathbf{x}&amp;rsquo;) + k_2(\mathbf{x}, \mathbf{x}&amp;rsquo;)\)&lt;/li&gt;
&lt;li&gt;\(k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = k_1(\mathbf{x}, \mathbf{x}&amp;rsquo;)k_2(\mathbf{x}, \mathbf{x}&amp;rsquo;)\)&lt;/li&gt;
&lt;li&gt;\(k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = \exp(k_1(\mathbf{x}, \mathbf{x}&amp;rsquo;))\)&lt;/li&gt;
&lt;li&gt;\(k(\mathbf{x}, \mathbf{x}&amp;rsquo;) = \tanh(k_1(\mathbf{x}, \mathbf{x}&amp;rsquo;))\)&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Naive Bayes</title>
      <link>https://ajdillhoff.github.io/notes/naive_bayes/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/naive_bayes/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#making-a-decision&#34;&gt;Making a Decision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#relation-to-multinomial-logistic-regression&#34;&gt;Relation to Multinomial Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mnist-example&#34;&gt;MNIST Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gaussian-formulation&#34;&gt;Gaussian Formulation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;To motivate naive Bayes classifiers, let&amp;rsquo;s look at slightly more complex data. The MNIST dataset was one of the standard benchmarks for computer vision classification algorithms for a long time. It remains useful for educational purposes. The dataset consists of 60,000 training images and 10,000 testing images of size \(28 \times 28\). These images depict handwritten digits. For the purposes of this section, we will work with binary version of the images. This implies that each data sample has 784 binary features.&lt;/p&gt;
&lt;p&gt;We will use the naive Bayes classifier to make an image classification model which predicts the class of digit given a new image. Each image will be represented by a vector \(\mathbf{x} \in \mathbb{R}^{784}\). Modeling \(p(\mathbf{x}|C_k)\) with a multinomial distribution would require \(10^{784} - 1\) parameters since there are 10 classes and 784 features.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-01_18-47-49_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Samples of the MNIST training dataset.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Samples of the MNIST training dataset.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;With the naive assumption that the features are independent conditioned on the class, the number model parameters becomes \(10 \times 784\).&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;A naive Bayes classifier makes the assumption that the features of the data are independent. That is,
\[
p(\mathbf{x}|C_k, \mathbf{\theta}) = \prod_{d=1}^D p(x_i|C_k, \theta_{dk}),
\]
where \(\mathbf{\theta}_{dk}\) are the parameters for the class conditional density for class \(k\) and feature \(d\). Using the MNIST dataset, \(\mathbf{\theta}_{dk} \in \mathbb{R}^{784}\). The posterior distribution is then&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(C_k|\mathbf{x},\mathbf{\theta}) = \frac{p(C_k|\mathbf{\pi})\prod_{i=1}^Dp(x_i|C_k, \mathbf{\theta}_{dk})}{\sum_{k&amp;rsquo;}p(C_{k&amp;rsquo;}|\mathbf{\pi})\prod_{i=1}^Dp(x_i|C_{k&amp;rsquo;},\mathbf{\theta}_{dk&amp;rsquo;})}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;If we convert the input images to binary, the class conditional density \(p(\mathbf{x}|C_k, \mathbf{\theta})\) takes on the Bernoulli pdf. That is,&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(\mathbf{x}|C_k, \mathbf{\theta}) = \prod_{i=1}^D\text{Ber}(x_i|\mathbf{\theta}_{dk}).
\end{equation*}&lt;/p&gt;
&lt;p&gt;The parameter \(\theta_{dk}\) is the probability that the feature \(x_i=1\) given class \(C_k\).&lt;/p&gt;
&lt;h2 id=&#34;maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/h2&gt;
&lt;p&gt;Fitting a naive Bayes classifier is relatively simple using MLE. The likelihood is given by&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p(\mathbf{X}, \mathbf{y}|\mathbf{\theta}) = \prod_{n=1}^N \mathcal{M}(y_n|\mathbf{\pi})\prod_{d=1}^D\prod_{k=1}^{K}p(x_{nd}|\mathbf{\theta}_{dk})^{\mathbb{1}(y_n=k)}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;To derive the estimators, we first take the log of the likelihood:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\ln p(\mathbf{X}, \mathbf{y}|\mathbf{\theta}) = \Bigg[\sum_{n=1}^N\sum_{k=1}^K \mathbb{1}(y_n = k)\ln \pi_k\Bigg] + \sum_{k=1}^K\sum_{d=1}^D\Bigg[\sum_{n:y_n=k}\ln p(x_{nd}|\theta_{dk})\Bigg].
\end{equation*}&lt;/p&gt;
&lt;p&gt;Thus, we have a term for the the multinomial and terms for the class-feature parameters. As with previous models that use a multinomial form, the parameter estimate for the first term is computed as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\hat{\pi}_k = \frac{N_k}{N}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;The features used in our data are binary, so the parameter estimate for each \(\hat{\theta}_{dk}\) follows the Bernoulli distribution:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\hat{\theta}_{dk} = \frac{N_{dk}}{N_{k}}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;That is, the number of times that feature \(d\) is in an example of class \(k\) divided by the total number of samples for class \(k\).&lt;/p&gt;
&lt;h2 id=&#34;making-a-decision&#34;&gt;Making a Decision&lt;/h2&gt;
&lt;p&gt;Given parameters \(\mathbf{\theta}\), how can we classify a given data sample?&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\text{arg}\max_{k}p(y=k)\prod_{i}p(x_i|y=k)
\end{equation*}&lt;/p&gt;
&lt;h2 id=&#34;relation-to-multinomial-logistic-regression&#34;&gt;Relation to Multinomial Logistic Regression&lt;/h2&gt;
&lt;p&gt;Consider some data with discrete features having one of \(K\) states, then \(x_{dk} = \mathbb{1}(x_d=k)\). The class conditional density, in this case, follows a multinomial distribution:&lt;/p&gt;
&lt;p&gt;\[
p(y=c|\mathbf{x}, \mathbf{\theta}) = \prod_{d=1}^D \prod_{k=1}^K \theta_{dck}^{x_{dk}}.
\]&lt;/p&gt;
&lt;p&gt;We can see a connection between naive Bayes and logistic regression when we evaluate the posterior over classes:&lt;/p&gt;
&lt;p&gt;\begin{align*}
p(y=c|\mathbf{x}, \mathbf{\theta}) &amp;amp;= \frac{p(y)p(\mathbf{x}|y, \mathbf{\theta})}{p(\mathbf{x})}\\
&amp;amp;= \frac{\pi_c \prod_{d} \prod_{k} \theta_{dck}^{x_{dk}}}{\sum_{c&amp;rsquo;}\pi_{c&amp;rsquo;}\prod_{d}\prod_{k}\theta_{dc&amp;rsquo;k}^{x_{dk}}} \\
&amp;amp;= \frac{\exp[\log \pi_c + \sum_d \sum_k x_{dk}\log \theta_{dck}]}{\sum_{c&amp;rsquo;} \exp[\log \pi_{c&amp;rsquo;} + \sum_d \sum_k x_{dk} \log \theta_{dc&amp;rsquo;k}]}.
\end{align*}&lt;/p&gt;
&lt;p&gt;This has the same form as the softmax function:&lt;/p&gt;
&lt;p&gt;\[
p(y=c|\mathbf{x}, \mathbf{\theta}) = \frac{e^{\beta^{T}_c \mathbf{x} + \gamma_c}}{\sum_{c&amp;rsquo;=1}^C e^{\beta^{T}_{c&amp;rsquo;}\mathbf{x} + \gamma_{c&amp;rsquo;}}}
\]&lt;/p&gt;
&lt;h2 id=&#34;mnist-example&#34;&gt;MNIST Example&lt;/h2&gt;
&lt;p&gt;With the model definition and parameter estimates defined, we can fit and evaluate the model. Using &lt;code&gt;scikit-learn&lt;/code&gt;, we fit a Bernoulli naive Bayes classifier on the MNIST training set: &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/blob/main/logistic_regression/naive_bayes_mnist.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Naive Bayes&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;gaussian-formulation&#34;&gt;Gaussian Formulation&lt;/h2&gt;
&lt;p&gt;If our features are continuous, we would model them with univariate Gaussians.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Neural Networks</title>
      <link>https://ajdillhoff.github.io/notes/neural_networks/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/neural_networks/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#forward-pass&#34;&gt;Forward Pass&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#activation-functions&#34;&gt;Activation Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multi-class-classification&#34;&gt;Multi-Class Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backpropagation&#34;&gt;Backpropagation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#non-convex-optimization&#34;&gt;Non-Convex Optimization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://playground.tensorflow.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://playground.tensorflow.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Previously, we studied the &lt;a href=&#34;https://ajdillhoff.github.io/notes/perceptron/&#34;&gt;Perceptron&lt;/a&gt; and saw that while it made for a simple linear classifier, it is severely limited to problems that are already linearly separable.
This limitation was resolved by introduding a hidden layer with multiple perceptron units, aptly named Multi-Layer Perceptrons.&lt;/p&gt;
&lt;p&gt;In this series, we will explore the more general method of neural networks.
We will see that even a network of only two layers can approximate any continuous functional mapping to arbitrary accuracy.
Through a discussion about network architectures, activation functions, and backpropagation, we will understand and use neural networks to resolve a large number of both classification and regression tasks.&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;We will take an abstract view of neural networks in which any formulation of a neural network defines a nonlinear mapping from an input space to some output space.
This implies that our choice of activation function &lt;strong&gt;must&lt;/strong&gt; be nonlinear.
The function we create will be parameterized by some weight matrix \(W\).
Thus, any neural network can be simply formulated as&lt;/p&gt;
&lt;p&gt;\[
f(\mathbf{x};W).
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-12_18-08-25_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;General neural network diagram.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;General neural network diagram.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;A neural network is in part defined by its &lt;strong&gt;layers&lt;/strong&gt;, the number of &lt;strong&gt;nodes&lt;/strong&gt; in each layer, the choice of &lt;strong&gt;activation function&lt;/strong&gt;, and the choice of &lt;strong&gt;loss function&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Each layer has a number of weights equal to the number of input nodes times the number of output nodes.
This is commonly represented as a weight matrix \(W\).&lt;/p&gt;
&lt;p&gt;The network produces output through the &lt;strong&gt;forward pass&lt;/strong&gt; and computes the gradients with respect to that output in the &lt;strong&gt;backwards pass&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h2&gt;
&lt;p&gt;Computing the output is done in what is called the &lt;strong&gt;forward pass&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Our neural network function takes in an input \(\mathbf{x} \in \mathbb{R}^D\), where \(D\) is the number of features in our input space.
Each output node \(a_j\) in a hidden layer \(h_l\) has a corresponding weight vector \(\mathbf{w}_j^{(l)}\).
The intermediate output of a hidden layer \(h_l\) is a linear combination of the weights and the input followed by some nonlinear function. Node \(a_j\) of a hidden layer is computed as&lt;/p&gt;
&lt;p&gt;\[
a_j = \sum_{i=1}^d w_{ji}^{(l)} x_{i} + w_{j0}^{(l)}.
\]&lt;/p&gt;
&lt;p&gt;As with &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_regression/&#34;&gt;Linear Regression&lt;/a&gt;, we will prepend a constant 1 to our input so that the computation is simply&lt;/p&gt;
&lt;p&gt;\[
a_{j} = \sum_{i=0}^d w_{ji}^{(i)} x_i = \mathbf{w}_j^T \mathbf{x}.
\]&lt;/p&gt;
&lt;p&gt;The final output of the hidden layer is \(a_j\) transformed by a nonlinear function \(g\) such that&lt;/p&gt;
&lt;p&gt;\[
z_j = g(a_j).
\]&lt;/p&gt;
&lt;p&gt;We can combine all weight vectors for each hidden layer node into a weight matrix \(W \in \mathbb{R}^{n \times d}\), where \(n\) is the number of nodes in the layer and \(d\) is the number of input features such that&lt;/p&gt;
&lt;p&gt;\begin{equation*}
W =
\begin{bmatrix}
\mathbf{w}_1^T\\
\vdots\\
\mathbf{w}_n^T\\
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Then the output of the hidden layer can be computed as&lt;/p&gt;
&lt;p&gt;\[
\mathbf{a} = W\mathbf{x}.
\]&lt;/p&gt;
&lt;p&gt;If you instead wanted to separate the bias term, this would be&lt;/p&gt;
&lt;p&gt;\[
\mathbf{a} = W\mathbf{x} + \mathbf{b}.
\]&lt;/p&gt;
&lt;p&gt;Using the notation to specify the individual layer, we can write the output of a full network.
Let \(W^{(l)} \in \mathbb{R}^{n_{l} \times n_{l-1}}\) be the weights for layer \(l\) which have \(n_{l-1}\) input connections and \(n_{l}\) output nodes.
The activation function for layer \(l\) is given by \(g^{(l)}\).&lt;/p&gt;
&lt;p&gt;The complete forward pass of the network is computed by repeating the following step for all layers:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{z}^{(l)} = g^{(l)}(\mathbf{a}^{(l-1)}),
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\mathbf{a}^{(l-1)} = W^{(l-1)}\mathbf{z}^{(l-1)} + \mathbf{b}^{(l-1)}.
\]&lt;/p&gt;
&lt;p&gt;Once all layers have been computed, then the output of the last layer, \(\hat{\mathbf{y}}^{(L)}\) is used as the final output of the model.
For training, this is compared with some ground truth label \(\mathbf{y}\) using a loss function \(\mathcal{L}\):&lt;/p&gt;
&lt;p&gt;\[
\mathcal{L}(\hat{\mathbf{y}}, \mathbf{y}).
\]&lt;/p&gt;
&lt;h3 id=&#34;xor-example&#34;&gt;XOR Example&lt;/h3&gt;
&lt;p&gt;Consider the XOR problem. A single &lt;a href=&#34;https://ajdillhoff.github.io/notes/perceptron/&#34;&gt;Perceptron&lt;/a&gt; was unable to solve that problem.
However, adding a hidden layer and forming a multi-layer perceptron network allowed for a more complex decision boundary.
Consider the network below and produce the output given all combinations of binary input:
\(\{(0, 0), (0, 1), (1, 0), (1, 1)\}\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-13_22-36-49_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A network with 1 hidden layer that computes XOR. Source: &amp;lt;https://athitsos.utasites.cloud/courses/cse4309_fall2021/lectures/09a_neural_networks.pdf&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A network with 1 hidden layer that computes XOR. Source: &lt;a href=&#34;https://athitsos.utasites.cloud/courses/cse4309_fall2021/lectures/09a_neural_networks.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://athitsos.utasites.cloud/courses/cse4309_fall2021/lectures/09a_neural_networks.pdf&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;activation-functions&#34;&gt;Activation Functions&lt;/h2&gt;
&lt;h3 id=&#34;sigmoid-function&#34;&gt;Sigmoid Function&lt;/h3&gt;
&lt;p&gt;\[
g(x) = \frac{1}{1 + e^{-x}}
\]&lt;/p&gt;
&lt;p&gt;The logistic sigmoid function serves two purposes.
First, it allows the output of the neuron to be interpreted as a posterior probability.
Note that this is not actually a probability.
Second, it is a continuous function for which the derivative can be computed:&lt;/p&gt;
&lt;p&gt;\[
g&amp;rsquo;(x) = g(x)(1 - g(x)).
\]&lt;/p&gt;
&lt;h3 id=&#34;hyperbolic-tangent-function&#34;&gt;Hyperbolic Tangent Function&lt;/h3&gt;
&lt;p&gt;\[
\tanh x = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
\]&lt;/p&gt;
&lt;p&gt;The hyperbolic tangent function maps input to a range of \((-1, 1)\).&lt;/p&gt;
&lt;p&gt;The derivative is calculated as&lt;/p&gt;
&lt;p&gt;\[
\frac{d}{dx} \tanh x = 1 - \tanh^2 x.
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-13_23-00-27_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Hyperbolic Tangent Function. Source: Wolfram&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Hyperbolic Tangent Function. Source: Wolfram
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Key Terms&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;bias&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;activation function&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Neurons fire after input reaches some threshold.&lt;/li&gt;
&lt;li&gt;Differential activation functions necessary for backpropagation.&lt;/li&gt;
&lt;li&gt;Multi-class learning&lt;/li&gt;
&lt;li&gt;How long to train?&lt;/li&gt;
&lt;li&gt;Weight decay&lt;/li&gt;
&lt;li&gt;How many layers versus how many nodes per layer?&lt;/li&gt;
&lt;li&gt;Training&lt;/li&gt;
&lt;li&gt;Data split (train/test/val)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;multi-class-classification&#34;&gt;Multi-Class Classification&lt;/h2&gt;
&lt;p&gt;Consider an output layer of a network with \(k\) nodes.
Each of these nodes represents a decision node for a one-versus-all classifier.
For a classification task, we have to think about whether or not the sum of squares loss function works.&lt;/p&gt;
&lt;p&gt;As far as activation functions go, the logistic sigmoid function is a good way to produce some interpretation of probability.
If we treat every output node as its own one versus all classifier, then a logistic sigmoid at the end of each one would
indicate the &amp;ldquo;probability&amp;rdquo; that node \(k\) assigns class \(k\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do we formulate this in a neural network?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The number of nodes in the output layer will be \(K\), the number of classes.
Since the output of each node produces a value in range \((0, 1)\), we want to construct a target value that works with this.
Instead of assigning an integer to each class label (e.g. 1 for class 2, 2 for class 3, etc.), we will encode the target label as a \(K\) dimensional vector.
For example, if our class label is for the class 1, then the corresponding target vector will be&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{t} =
\begin{bmatrix}
1\\
0\\
\vdots\\
0
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Since the output of our final layer is also a \(K\) dimensional vector, we can compare the two using some loss function.&lt;/p&gt;
&lt;h2 id=&#34;backpropagation&#34;&gt;Backpropagation&lt;/h2&gt;
&lt;p&gt;Given a series of linear layers with nonlinear activation functions,
how can we update the weights across the entire network?&lt;/p&gt;
&lt;p&gt;The short answer is through the chain rule of differentiation.
Let&amp;rsquo;s explore this through an example.&lt;/p&gt;
&lt;p&gt;After constructing some series of hidden layers with an arbitrary number of nodes,
we will pick an error function that provides a metric of how our network performs
on a given regression or classification task.
This loss is given by \(\mathcal{L}\).&lt;/p&gt;
&lt;p&gt;Neural networks are traditionally trained using &lt;strong&gt;gradient descent&lt;/strong&gt;.
The goal is to optimize the weights such that they result in the lowest loss, or error.
This is also why our choice of loss function is important.&lt;/p&gt;
&lt;p&gt;\[
\mathbf{W}^* = \text{argmin}\frac{1}{n}\sum_{i=1}^n \mathcal{L}(f(\mathbf{x}^{(i)}; \mathbf{W}), \mathbf{y}^{(i)})
\]&lt;/p&gt;
&lt;p&gt;We first compute the gradients of the network with respect to the weights and biases.
Then, we use those gradients to update our previous values for the weights and biases.&lt;/p&gt;
&lt;h3 id=&#34;a-simple-example&#34;&gt;A Simple Example&lt;/h3&gt;
&lt;p&gt;We will first look at computing these gradients on a smaller network for binary classification with 1 hidden layer and 1 output layer.
The loss function is defined using the binary cross-entropy function:&lt;/p&gt;
&lt;p&gt;\[
\mathcal{L}(\hat{\mathbf{y}}, \mathbf{y}) = -\mathbf{y}\log \hat{\mathbf{y}} - (1 - \mathbf{y}) \log (1 - \hat{\mathbf{y}})
\]&lt;/p&gt;
&lt;p&gt;The network&amp;rsquo;s output is computed in sequence following&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{a}^{(1)} &amp;amp;= W^{(1)}\mathbf{x} + \mathbf{b}^{(1)}\\
\mathbf{z}^{(1)} &amp;amp;= g^{(1)}(\mathbf{a}^{(1)})\\
\mathbf{a}^{(2)} &amp;amp;= W^{(2)}\mathbf{z}^{(1)} + \mathbf{b}^{(2)}\\
\mathbf{z}^{(2)} &amp;amp;= g^{(2)}(\mathbf{a}^{(2)})\\
\end{align*}&lt;/p&gt;
&lt;p&gt;The goal is to compute the gradients for all weights and biases:&lt;/p&gt;
&lt;p&gt;\[
\frac{d\mathcal{L}}{dW^{(1)}},\quad \frac{d\mathcal{L}}{d\mathbf{b}^{(1)}},\quad \frac{d\mathcal{L}}{dW^{(2)}},\quad \frac{d\mathcal{L}}{d\mathbf{b}^{(2)}}.
\]&lt;/p&gt;
&lt;p&gt;Starting with the weights of the output layer:&lt;/p&gt;
&lt;p&gt;\[
\frac{d\mathcal{L}}{dW^{(2)}} = \frac{d\mathcal{L}}{d\mathbf{z}^{(2)}} \frac{d\mathbf{z}^{(2)}}{d\mathbf{a}^{(2)}} \frac{d\mathbf{a}^{(2)}}{dW^{(2)}}.
\]&lt;/p&gt;
&lt;p&gt;The first step is to compute the partial gradient of the loss function with respect to its input \(\hat{\mathbf{y}} = \mathbf{z}^{(2)}\):&lt;/p&gt;
&lt;p&gt;\[
\frac{d\mathcal{L}}{d\mathbf{z}^{(2)}} = \frac{\mathbf{z}^{(2)} - \mathbf{y}}{\mathbf{z}^{(2)}(1 - \mathbf{z}^{(2)})}.
\]&lt;/p&gt;
&lt;p&gt;Next, compute the gradient of the last layer&amp;rsquo;s activation function with respect to its input \(\mathbf{a}^{(2)}\):&lt;/p&gt;
&lt;p&gt;\[
\frac{d\mathbf{z}^{(2)}}{d\mathbf{a}^{(2)}} = \mathbf{z}^{(2)}(1 - \mathbf{z}^{(2)}).
\]&lt;/p&gt;
&lt;p&gt;Finally, we compute \(\frac{d\mathbf{a}^{(2)}}{dW^{(2)}}\):
\[
\frac{d\mathbf{a}^{(2)}}{dW^{(2)}} = \mathbf{z}^{(1)}.
\]&lt;/p&gt;
&lt;p&gt;Putting all of this together yields&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{d\mathcal{L}}{dW^{(2)}} &amp;amp;= \frac{\mathbf{z}^{(2)} - \mathbf{y}}{\mathbf{z}^{(2)}(1 - \mathbf{z}^{(2)})} * \mathbf{z}^{(2)}(1 - \mathbf{z}^{(2)}) * \mathbf{z}^{(1)}\\
&amp;amp;= \mathbf{z}^{(1)} (\mathbf{z}^{(2)} - \mathbf{y}).
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;non-convex-optimization&#34;&gt;Non-Convex Optimization&lt;/h2&gt;
&lt;p&gt;Optimizing networks with non-linearities produces a non-convex landscape.
Depending on our choice of optimization algorithm and initial starting point, the algorithm will most likely get &amp;ldquo;stuck&amp;rdquo; in some local minimum.
Consider the figure below produced by (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Li et al. 2017&lt;/a&gt;).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-31_09-48-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Loss surface of ResNet-56 (Li et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Loss surface of ResNet-56 (Li et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Li, Hao, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2017. “Visualizing the Loss Landscape of Neural Nets,” 11.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Perceptron</title>
      <link>https://ajdillhoff.github.io/notes/perceptron/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/perceptron/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-perceptron-learning-algorithm&#34;&gt;The Perceptron Learning Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#limitations-of-single-layer-perceptrons&#34;&gt;Limitations of Single-Layer Perceptrons&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;A popular example of a &lt;a href=&#34;https://ajdillhoff.github.io/notes/logistic_regression/&#34;&gt;Logistic Regression&lt;/a&gt; model is the &lt;strong&gt;perceptron&lt;/strong&gt;. Proposed by Frank Rosenblatt in 1962, the perceptron is defined as a generalized linear model:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f(\mathbf{w}^T\mathbf{\phi}(\mathbf{x})),
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(\phi\) is a basis function and \(f\) is a stepwise function with the form&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f(a) =
\begin{cases}
1, a \geq 0\\
-1, a &amp;lt; 0
\end{cases}
\end{equation*}&lt;/p&gt;
&lt;p&gt;To match this, the targets will take on a value of either 1 or -1.&lt;/p&gt;
&lt;h2 id=&#34;the-perceptron-learning-algorithm&#34;&gt;The Perceptron Learning Algorithm&lt;/h2&gt;
&lt;p&gt;Based on the stepwise function, the parameters \(\mathbf{w}\) should lead to outputs above 0 for one class and outputs below 0 for the other.
There is 0 error with a correct classification.&lt;/p&gt;
&lt;p&gt;The original formulation does not work well with gradient based optimization methods due to the fact that the derivative of the stepwise function is 0 almost everyone. To get around this, the perceptron criterion is used:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
E(\mathbf{w}) = -\sum_i \mathbf{w}^T\phi(\mathbf{x}_i)\hat{y}_i,
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(\hat{y}_i\) is the target class (either 1 or -1).&lt;/p&gt;
&lt;p&gt;An incorrect classification will minimize \(\mathbf{w}^T\phi_i y_i\). We can consider this loss only for misclassified patterns.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For each input, evaluate \(f(\mathbf{w}^T\phi(\mathbf{x}_i))\).&lt;/li&gt;
&lt;li&gt;For incorrect classifications
&lt;ul&gt;
&lt;li&gt;Add \(\phi(\mathbf{x}_i)\) to \(\mathbf{w}\) estimate for class 1&lt;/li&gt;
&lt;li&gt;Subtract \(\phi(\mathbf{x}_i)\) from \(\mathbf{w}\) for class 2.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Does not necessarily get better each step, but guaranteed to converge.&lt;/p&gt;
&lt;h2 id=&#34;limitations-of-single-layer-perceptrons&#34;&gt;Limitations of Single-Layer Perceptrons&lt;/h2&gt;
&lt;p&gt;Single layer perceptrons are limited to solving linearly separable patterns. As we have seen with a few datasets now, expecting our data to be linearly separable is wishful thinking. Minsky and Papert exposed this limitation in their book &lt;a href=&#34;https://en.wikipedia.org/wiki/Perceptrons_%28book%29&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Perceptrons: an introduction to computational geometry&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Consider the example XOR problem. It is a binary classification problem consisting of 4 data points. It is &lt;strong&gt;not&lt;/strong&gt; linearly separable as seen in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-06-27_21-22-04_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;XOR cannot be solved with a linear classifier.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;XOR cannot be solved with a linear classifier.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This is the result of using only a single Perceptron. What if we added another perceptron? A single perceptron computes \(\mathbf{w}^T + b\). It is important to transform the first perceptron&amp;rsquo;s output using a non-linear activation function, otherwise the output would be similar to that of a logistic regression model. The updated &amp;ldquo;network&amp;rdquo; is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-06-27_21-54-23_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A 2 layer perceptron for which each layer has a single node.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A 2 layer perceptron for which each layer has a single node.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The result is the same! The original input in 2D is transformed to a single dimensional output. This is then used as input to the second perceptron. The result is a linear decision boundary followed by another linear decision boundary. What if we used 2 perceptrons in the first layer? The idea is that using two linear decision boundaries in a single space would allow our model to create a more complex boundary. The updated network is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-06-27_21-58-30_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;A 2 layer perceptron for which the first layer has 2 nodes.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;A 2 layer perceptron for which the first layer has 2 nodes.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This effectively solves the XOR problem! Since each node computes a linear combination of the input, we can visualize two decision boundaries with respect to the input space.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-06-27_22-04-07_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Visualization of input space.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Visualization of input space.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Similarly, we can visualize how the data points are transformed by visualizing the space of the output layer.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-06-27_22-05-05_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Output space&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Output space
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Principal Component Analysis</title>
      <link>https://ajdillhoff.github.io/notes/principal_component_analysis/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/principal_component_analysis/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maximum-variance-formulation&#34;&gt;Maximum Variance Formulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#motivating-example&#34;&gt;Motivating Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#noise-and-redundancy&#34;&gt;Noise and Redundancy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#covariance-matrix&#34;&gt;Covariance Matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;If we have some measurements of data, but do not know the underlying dynamics, PCA can resolve this by producing a change of basis such that the dynamics are reflected upon the eigenvectors.&lt;/p&gt;
&lt;h2 id=&#34;maximum-variance-formulation&#34;&gt;Maximum Variance Formulation&lt;/h2&gt;
&lt;p&gt;Although there are several derivations of PCA. I really like the approach of projecting the data onto a lower dimensional space in order to maximize the variance of the projected data.&lt;/p&gt;
&lt;p&gt;Let \(\mathbf{X}\) be a dataset of \(N\) samples, each with \(D\) features. The goal of PCA is to project this data onto an $M$-dimensional space such that \(M &amp;lt; D\).&lt;/p&gt;
&lt;p&gt;Remember that the goal here is to maximize the variance of the &lt;strong&gt;projected data&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do we project the data?&lt;/strong&gt;
Let&amp;rsquo;s say that we want to go from $D$-dimensional space to $M$-dimensional space where \(M = 1\). Let the vector \(\mathbf{u}\) define this 1D space. If \(\mathbf{u}\) is a unit vector, then the scalar projection of a data point \(\mathbf{x}\) onto \(\mathbf{u}\) is simply \(\mathbf{u} \cdot \mathbf{x}\).&lt;/p&gt;
&lt;p&gt;Since we are maximizing variance, we need to subtract the mean sample from our data&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{\bar{x}} = \frac{1}{N}\sum_{n=1}^{N}\mathbf{x}_n
\end{equation*}&lt;/p&gt;
&lt;p&gt;Then, the mean of the projected data is \(\mathbf{u} \cdot \mathbf{\bar{x}}\).&lt;/p&gt;
&lt;p&gt;With the mean of the projected data, we can calculate the variance:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{1}{N}\sum_{n=1}^{N}\{\mathbf{u}^T\mathbf{x}_n - \mathbf{u}^T\mathbf{\bar{x}}\}^2 = \mathbf{u}^T\mathbf{S}\mathbf{u}
\end{equation*}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{S} = \frac{1}{N}\sum_{n=1}^{N}(\mathbf{x}_n - \mathbf{\bar{x}})(\mathbf{x}_n - \mathbf{\bar{x}})^T
\end{equation*}&lt;/p&gt;
&lt;p&gt;Thus, if we are maximizing the variance of the projected data, then we are maximizing \(\mathbf{u}^T\mathbf{S}\mathbf{u}\)!&lt;/p&gt;
&lt;p&gt;So this is an optimization problem, but there is one minor issue to deal with: if \(\mathbf{u}\) is not constrained, then we scale it to infinity while maximizing the function.&lt;/p&gt;
&lt;p&gt;Before, we stated that \(\mathbf{u}\) is a unit vector. Thus, the constraint is that \(\mathbf{u} \cdot \mathbf{u} = 1\).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;After reviewing Lagrangian multipliers&amp;hellip;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To enforce this constraint, we can use a lagrangian multiplier:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathcal{L}(\mathbf{u}, \lambda) = \mathbf{u}^T\mathbf{S}\mathbf{u} + \lambda(1 - \mathbf{u}^T\mathbf{u}).
\end{equation*}&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see what happens when we compute the stationary points (critical points) of the given Lagrangian function.&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\nabla_{\mathbf{u}}\mathcal{L}(\mathbf{u}, \lambda) = \mathbf{S}\mathbf{u} - \lambda \mathbf{u} = 0
\end{equation*}&lt;/p&gt;
&lt;p&gt;This implies that&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{S}\mathbf{u} = \lambda \mathbf{u}
\end{equation*}&lt;/p&gt;
&lt;p&gt;That particular equation means that \(\mathbf{u}\) is an eigenvector of \(\mathbf{S}\) with \(\lambda\) being the corresponding eigenvalue. Since \(\mathbf{u}\) is a unit vector, we can conveniently left-multiply both sides of that equation by \(\mathbf{u}^T\), resulting in:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{u}^T\mathbf{S}\mathbf{u} = \lambda
\end{equation*}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What does this mean?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;That means that the variance is maximized when \(\mathbf{u}\) is the eigenvector corresponding to the largest eigenvalue \(\lambda\).&lt;/p&gt;
&lt;p&gt;We can repeat this process to find the direction (eigenvector) corresponding to the second largest variance by considering eigenvectors that are orthogonal to the first one. This is where an orthonormal eigenbasis comes in handy.&lt;/p&gt;
&lt;h2 id=&#34;motivating-example&#34;&gt;Motivating Example&lt;/h2&gt;
&lt;p&gt;Consider a frictionless, massless spring that produces dynamics in a single direction.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2021-11-24_12-23-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Toy model of spring with ball observed from 3 perspectives.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Toy model of spring with ball observed from 3 perspectives.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;We can clearly understand that the spring will only move in a single direction. That movement reflects the underlying dynamics of this data. To understand how PCA can be useful in this situation, let&amp;rsquo;s pretend that we do not know the underlying dynamics. Instead, we observe that the data seems to go back and forth along a single axis. We observe the data over time from 3 different perspectives given by the cameras in the above figure.&lt;/p&gt;
&lt;p&gt;From the perspective of the observer, we are recording some observations in an effort to understand which dimensions are the most salient at representing the underlying mechanics. From the above figure, we know that the most important dimension in this system is that of the labeled x-axis.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How would we figure this out if we did not already know that?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each camera has its own coordinate system (basis). If each camera gives us a 2D location of the ball relative to that camera&amp;rsquo;s basis, then each sample in time gives us a 6D vector of locations.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Equivalently, every time sample is a vector that lies in an $m$-dimensional vector space spanned by an orthonormal basis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Is it possible to find another basis that best expresses the data?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Mathemtically, is there some matrix \(P\) that changes our original data \(X\) into a new representation \(Y\)?&lt;/p&gt;
&lt;p&gt;\(PX = Y\)&lt;/p&gt;
&lt;h2 id=&#34;noise-and-redundancy&#34;&gt;Noise and Redundancy&lt;/h2&gt;
&lt;p&gt;When observing real data, we will have to account for noisy measurements. Noise can come from a wide variety of sources. Being able to reduce it or filter it out is vital to understanding the underlying system.&lt;/p&gt;
&lt;p&gt;Noise is an arbitrary measurement and means nothing without some measurement of a signal. Thus, we typically measure the amount of noise in our system using a Signal-to-Noise Ratio (SNR). This assumes we have some idea of what our signal is. This is usually given based on the nature of whatever problem we are investigating. &lt;strong&gt;In the toy example, we know that the spring largely moves in a single dimension. That is the signal we expect to observe.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For arguments sake, imagine we that the recordings over time from a single camera plot the following data:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2021-11-24_14-56-35_screenshot.png&#34; &gt;


&lt;/figure&gt;

&lt;p&gt;From our advantageous position of knowing the true nature of the problem, we understand there really should be no noise. However, let&amp;rsquo;s say that our camera has some noise in interpreting the precise location of the ball at any given time. In this case, our SNR is quite high, which is good! Ideally, it would be a straight line.&lt;/p&gt;
&lt;p&gt;There is a second factor to consider: the fact that we are taking measurements from multiple sensors means that there may be some redundancy among the data collected from them. If we were to discover features that have high redundancy, we could be confident in concluding that they are highly correlated.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2021-11-24_15-01-05_screenshot.png&#34; &gt;


&lt;/figure&gt;

&lt;h2 id=&#34;covariance-matrix&#34;&gt;Covariance Matrix&lt;/h2&gt;
&lt;p&gt;Let \(X\) be a an \(m \times n\) matrix of \(n\) observations with \(m\) features per observation.&lt;/p&gt;
&lt;p&gt;We can produce a covariance matrix of the features via \(S_{X} = \frac{1}{n-1}XX^{T}\).&lt;/p&gt;
&lt;p&gt;This gives us a measurement of the correlations between all pairs of measurements.&lt;/p&gt;
&lt;p&gt;If we want to reduce redunancy between separate measurements (those in the off-diagonal of the matrix), we would then want to diagonalize this matrix. In terms of the equation \(PX=Y\), this has the effective of finding a new covariance matrix \(S_{Y}\) that is diagonal. This means that each value in the off-diagonal of \(S_{Y}\) is 0.&lt;/p&gt;
&lt;p&gt;PCA has a convenient assumption: the change of basis matrix \(P\) is orthonormal.
&lt;strong&gt;Why is this convenient?&lt;/strong&gt;
PCA can then select the normalized direction in the feature space for which the variance in the data is maximized. This is called the first &lt;em&gt;principal component&lt;/em&gt;. Because we assume \(P\) is orthonormal, the subsequent principal components must be orthogonal to the previously discovered components.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;One more thing&lt;/strong&gt;
If \(P\) is not orthonormal, then we can simply scale our eigenvectors to maximize variance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Regularization</title>
      <link>https://ajdillhoff.github.io/notes/regularization/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/regularization/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#overfitting&#34;&gt;Overfitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#penalizing-weights&#34;&gt;Penalizing Weights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dataset-augmentation&#34;&gt;Dataset Augmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#early-stopping&#34;&gt;Early Stopping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dropout&#34;&gt;Dropout&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error. - Goodfellow et al.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;Regularization comes in many forms.
Some techniques may add an additional penalty to the loss function.
Others, such as data augmentation, add artificial variation to the data.
In all cases, regularization aims to improve the generalization performance by preventing the model from overfitting.&lt;/p&gt;
&lt;h2 id=&#34;overfitting&#34;&gt;Overfitting&lt;/h2&gt;
&lt;p&gt;What happens when the complexity of our chosen model fits the data &lt;em&gt;too&lt;/em&gt; well? Take a look at the following plot of data. The red curve is the true underlying function that generated the data. The blue line represents a polynomial of degree 9 fit via linear regression. It is first necessary to understand what is happening.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-01_10-18-30_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;A polynomial of degree 11 (blue) fit to data generated following the red line.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;A polynomial of degree 11 (blue) fit to data generated following the red line.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The model with more parameters is able to fit some the noisy data slightly better.
&lt;strong&gt;Does this necessarily mean it will perform better on new samples?&lt;/strong&gt;
No, it will usually perform worse. This is referred to as &lt;strong&gt;overfitting.&lt;/strong&gt;
Overfitting can be identified as the model trains. When the testing loss continues to decrease while the validation loss increases, the model is probably overfitting. It is also evident from looking at the weights.&lt;/p&gt;
&lt;h3 id=&#34;identifying-the-cause&#34;&gt;Identifying the Cause&lt;/h3&gt;
&lt;p&gt;The goal of training is to modify the weights such that they minimize the loss function. Models with more parameters have the capacity to fit more of their training data. Given the presence of noise, this is not a good thing. A very low loss on the training set may not translate to good performance on the validation set.&lt;/p&gt;
&lt;p&gt;Looking at weights of the trained model is a good way of detecting overfitting. From the model above, the mean of the absolute value of the weights is \(11.1\). Left unchecked, the weights will take on whatever values necessary to meet the objective function.&lt;/p&gt;
&lt;h2 id=&#34;penalizing-weights&#34;&gt;Penalizing Weights&lt;/h2&gt;
&lt;p&gt;The most common form of regularization is to penalize the weights from taking on a high value. That is, we define a penalty term \(E(\mathbf{w})\) that is added to the loss. The higher the weight values, the higher the total loss. Thus, optimization will also include minimizing the absolute values of the weights. A simple choice for \(E(\mathbf{w})\), especially in the context of least squares, is \(L2\) regularzation:&lt;/p&gt;
&lt;p&gt;\[
E(\mathbf{w}) = \frac{\lambda}{2}||\mathbf{w}||^2 = \frac{\lambda}{2}\mathbf{w}^T \mathbf{w}.
\]&lt;/p&gt;
&lt;p&gt;Added to the sum-of-squares error for least squares, the final loss becomes&lt;/p&gt;
&lt;p&gt;\[
J(\mathbf{w}) = \frac{1}{2}\sum_{i=1}^n(h(\mathbf{x}_i;\mathbf{w}) - \mathbf{y}_i)^2 + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}.
\]&lt;/p&gt;
&lt;p&gt;This choice of regularization also has the benefit of being in a form that can be minimized in closed form via the normal equations. Taking the gradient of \(J(\mathbf{w})\) above with respect to 0 and solving for \(\mathbf{w}\) yields&lt;/p&gt;
&lt;p&gt;\[
\mathbf{w} = (\lambda I + \mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y},
\]&lt;/p&gt;
&lt;p&gt;where \(\lambda\) is a regularization hyperparameter.&lt;/p&gt;
&lt;p&gt;Applying this regularization term to the model above with \(\lambda=1\) yields the model shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-01_10-45-57_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Least squares model fit with (L2) regularization ((lambda = 1)).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Least squares model fit with (L2) regularization ((lambda = 1)).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Inspecting the weights as before, we can see that the mean of the absolute values of \(\mathbf{w}\) is \(0.0938\).&lt;/p&gt;
&lt;h3 id=&#34;evaluating-on-the-testing-data&#34;&gt;Evaluating on the Testing Data&lt;/h3&gt;
&lt;p&gt;To see which model generalizes better, we set aside some samples from the original dataset to use as testing.&lt;/p&gt;
&lt;p&gt;With regularization, the model error on the test set is \(1.8\). Without regularization, the model error on the test set is \(2.2\).&lt;/p&gt;
&lt;h2 id=&#34;dataset-augmentation&#34;&gt;Dataset Augmentation&lt;/h2&gt;
&lt;p&gt;The same data augmentation techniques should be applied on both methods being compared.
Getting a better result on a benchmark because of data augmentation does not mean the method was better suited for the task.
By controlling these factors, a fair comparison can be made.&lt;/p&gt;
&lt;p&gt;There are many forms of augmentation available for image tasks in particular.
Rotating, translating, and scaling images are the most common.
Additionally applying random crops can further augment the dataset.&lt;/p&gt;
&lt;p&gt;The original dataset may only include samples of a class that have similar lighting.
Color jitter is an effective way of including a broader range of hue or brightness and usually leads to a model that is robust to such changes.ZZ&lt;/p&gt;
&lt;p&gt;It is important to make sure that the crops still contain enough information to properly classify it.
Common forms of data augmentation are available through APIs like &lt;a href=&#34;https://pytorch.org/vision/stable/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;torchvision&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;early-stopping&#34;&gt;Early Stopping&lt;/h2&gt;
&lt;p&gt;If the validation loss begins to increase while the training loss continues to decrease, this is a clear indication that the model is beginning to overfit the training data.
Stopping the model in this case is the best way to prevent this.
Frameworks like &lt;a href=&#34;https://www.pytorchlightning.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;PyTorch Lightning&lt;/a&gt; include features to checkpoing the models based on best validation loss and stop the model whenever the validation loss begins to diverge.&lt;/p&gt;
&lt;h2 id=&#34;dropout&#34;&gt;Dropout&lt;/h2&gt;
&lt;p&gt;Dropout is a regularization method introduced by &amp;lt;&amp;amp;srivastavaDropoutSimpleWay2014&amp;gt; which is motivated by ensemble methods.
Ensembles of models are regularized by the fact that many different models are trained on random permutations of the dataset with varying parameters and initializations.
Using an ensemble of networks is a powerful way of increasing generalization performance.
However, it requires much more compute due to the fact that several models must be trained.&lt;/p&gt;
&lt;p&gt;Training a single network with dropout approximates training several models in an ensemble.
It works by randomly removing a node from the network during a forward/backward pass.
The node is not truly removed. Instead, its output during the forward and backward passes is ignored via a binary mask.&lt;/p&gt;
&lt;p&gt;When training a network with dropout, it will generally take longer for the model to converge to a solution.
Intuitively, this is because a different subnetwork is being used for each pass.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Support Vector Machine</title>
      <link>https://ajdillhoff.github.io/notes/support_vector_machine/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/support_vector_machine/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maximum-margin-classifier&#34;&gt;Maximum Margin Classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#formulation&#34;&gt;Formulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#overlapping-class-distributions&#34;&gt;Overlapping Class Distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiclass-svm&#34;&gt;Multiclass SVM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#additional-resources&#34;&gt;Additional Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Support Vector Machines are a class of supervised learning methods primarily used for classification. Although they can be formulated for regression and outlier detection as well. Instead of optimizing a set of parameters which compress or summarize the training set, they use a small subset of the training data to compute the decision function.&lt;/p&gt;
&lt;p&gt;They rely on the data being linearly separable, so feature transformations are critical for problems in which the original representation of the data is not linearly separable.&lt;/p&gt;
&lt;h2 id=&#34;maximum-margin-classifier&#34;&gt;Maximum Margin Classifier&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with a simple classification model as we studied with &lt;a href=&#34;https://ajdillhoff.github.io/notes/logistic_regression/&#34;&gt;Logistic Regression&lt;/a&gt;. That is, we have&lt;/p&gt;
&lt;p&gt;\[
f(\mathbf{x}) = \mathbf{w}^T\phi(\mathbf{x}),
\]&lt;/p&gt;
&lt;p&gt;where \(\phi(\mathbf{x})\) is a function which transforms our original input into some new feature space. The transformed input is assumed to be linearly separable so that a decision boundary can be computed. In the original logistic regression problem, a decision boundary was found through optimization. For linearly separable data, there are an infinite number of decision boundaries that satisfy the problem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What about the quality of the decision boundary?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Is one decision boundary better than the other?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TODO:&lt;/strong&gt; Add a few plots comparing decision boundaries&lt;/p&gt;
&lt;h2 id=&#34;formulation&#34;&gt;Formulation&lt;/h2&gt;
&lt;p&gt;Given a training set \(\{\mathbf{x}_1, \dots, \mathbf{x}_n\}\) with labels \(\{y_1, \dots, y_n\}\), where \(y_i \in \{-1, 1\}\), we construct a linear model which classifies an input sample depending on the sign of the output.&lt;/p&gt;
&lt;p&gt;Our decision rule for classification, given some input \(\mathbf{x}\), is&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f(\mathbf{x}) =
\begin{cases}
1\text{ if }\mathbf{w}^T\mathbf{x} + b \geq 0\\
-1\text{ if }\mathbf{w}^T\mathbf{x} + b &amp;lt; 0
\end{cases}
\end{equation*}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How large should the margin be?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the original formulation of &lt;a href=&#34;https://ajdillhoff.github.io/notes/logistic_regression/&#34;&gt;Logistic Regression&lt;/a&gt;, we saw that the parameter vector \(\mathbf{w}\) described the &lt;strong&gt;normal&lt;/strong&gt; to the decision boundary. The distance between a given point \(\mathbf{x}\) and the decision boundary is given by&lt;/p&gt;
&lt;p&gt;\[
\frac{y_if(\mathbf{x})}{||\mathbf{w}||}.
\]&lt;/p&gt;
&lt;p&gt;We can frame this as an optimization problem: come up with a value for \(\mathbf{w}\) that maximizes the margin.&lt;/p&gt;
&lt;p&gt;\[
\text{arg max}_{\mathbf{w}, b} \frac{1}{\|\mathbf{w}\|}\min_{i} y_i (\mathbf{w}^T\phi(\mathbf{x}_i) + b)
\]&lt;/p&gt;
&lt;p&gt;We can arbitrarily scale the parameters, so we add an additional constraint that any point that lies on the boundary of the margin satisfies&lt;/p&gt;
&lt;p&gt;\[
y_i(\mathbf{w}^T\mathbf{x} + b) = 1.
\]&lt;/p&gt;
&lt;p&gt;Under this constraint, we have that all samples satisfy&lt;/p&gt;
&lt;p&gt;\[
y_i(\mathbf{w}^T\mathbf{x} + b) \geq 1.
\]&lt;/p&gt;
&lt;p&gt;That is, all positive samples with target \(1\) will produce at least a \(1\), yielding a value greater than or equal to 1. All negative samples with target \(-1\) will produce at most a \(-1\), yielding a value greater than or equal to 1.&lt;/p&gt;
&lt;p&gt;Another way of writing this is&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f(\mathbf{x}) =
\begin{cases}
1\text{ if }\mathbf{w}^T\mathbf{x}_{+} + b \geq 1\\
-1\text{ if }\mathbf{w}^T\mathbf{x}_{-} + b \leq -1,
\end{cases}
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(\mathbf{x}_+\) is a positive sample and \(\mathbf{x}_-\) is a negative sample. The decision rule can then be written as&lt;/p&gt;
&lt;p&gt;\[
y_i(\mathbf{w}^T\mathbf{x} + b) - 1 \geq 0.
\]&lt;/p&gt;
&lt;p&gt;This implies that the only samples that would yield an output of 0 are those that lie directly on the margins of the decision boundary.&lt;/p&gt;
&lt;p&gt;Given this constraint of \(y_i(\mathbf{w}^T\mathbf{x} + b) - 1 = 0\), we can derive our optimization objective.&lt;/p&gt;
&lt;p&gt;The margin can be computed via the training data. To do this, consider two data points which lie on their respective boundaries, one positive and one negative, and compute the distance between them: \(\mathbf{x}_+ - \mathbf{x}_-\). This distance with respect to our decision boundary, defined by \(\mathbf{w}\), is given by&lt;/p&gt;
&lt;p&gt;\[
(\mathbf{x}_+ - \mathbf{x}_-) \cdot \frac{\mathbf{w}}{||\mathbf{w}||}.
\]&lt;/p&gt;
&lt;p&gt;For clarity, we can rewrite this as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{1}{||\mathbf{w}||}(\mathbf{x}_{+} \cdot \mathbf{w} - \mathbf{x}_{-} \cdot \mathbf{w}).
\end{equation*}&lt;/p&gt;
&lt;p&gt;If we substitute the sample values into the equality constraint above, we can simplify this form. For the positive sample, we have \(\mathbf{w}^T\mathbf{x} = 1 - b\). For the negative sample, we get \(\mathbf{w}^T\mathbf{x} = -1 - b\). The equation above then becomes&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{1}{||\mathbf{w}||}(1 - b - (-1 - b)) = \frac{2}{||\mathbf{w}||}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Thus, our objective is to maximize \(\frac{2}{||\mathbf{w}||}\) which is equivalent to minimizing \(\frac{1}{2}||\mathbf{w}||^2\) subject to the constraints \(y_i(\mathbf{w}^T\mathbf{x}+b)\geq 1\). This is a constrainted optimization problem. As discussed previously, we can simplify such problems by introducing &lt;a href=&#34;https://ajdillhoff.github.io/notes/lagrangian_multipliers/&#34;&gt;Lagrangian Multipliers&lt;/a&gt;. Doing this produces the dual representation of our optimization objection:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
L = \frac{1}{2}||\mathbf{w}||^2 - \sum_{i=1}^n \alpha_i \big(y_i(\mathbf{w}^T\mathbf{x}_i + b) - 1\big).
\end{equation*}&lt;/p&gt;
&lt;p&gt;To solve for \(\mathbf{w}\) we compute \(\frac{\partial}{\partial \mathbf{w}}L\).&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{\partial}{\partial \mathbf{w}}L = \mathbf{w} - \sum_{i=1}^n \alpha_i y_i \mathbf{x}_i.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Setting this to 0 yields&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{w} = \sum_{i=1}^n \alpha_i y_i \mathbf{x}_i.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Doing the same for the other parameter \(b\) yields&lt;/p&gt;
&lt;p&gt;\[
0 = \sum_{i=1}^n \alpha_i y_i.
\]&lt;/p&gt;
&lt;p&gt;We can now simplify our objective function by substituting these results into it:&lt;/p&gt;
&lt;p&gt;\begin{align*}
L &amp;amp;= \frac{1}{2}\Big(\sum_{i=1}^n \alpha_i y_i \mathbf{x}_i\Big)^2 - \sum_{i=1}^n \alpha_i\Big(y_i\big((\sum_{i=1}^n\alpha_i y_i \mathbf{x}_i)^T\mathbf{x}_i + b \big) - 1 \Big)\\
&amp;amp;= \frac{1}{2}\Big(\sum_{i=1}^n \alpha_i y_i \mathbf{x}_i\Big)^2 - \Big(\sum_{i=1}^n \alpha_i y_i \mathbf{x}_i \Big)^2 - \sum_{i=1}^n \alpha_i y_i b + \sum_{i=1}^n \alpha_i\\
&amp;amp;= -\frac{1}{2} \Big(\sum_{i=1}^n \alpha_i y_i \mathbf{x}_i \Big)^2 + \sum_{i=1}^n \alpha_i\\
&amp;amp;= \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^m \alpha_i \alpha_j y_i y_j \mathbf{x}_i \cdot \mathbf{x}_j
\end{align*}&lt;/p&gt;
&lt;p&gt;Thus, the objective is dependent on the inner product of samples \(\mathbf{x}_i\) and \(\mathbf{x}_j\). If these were representations in some complex feature space, our problem would remain computationally inefficient. However, we can take advantage of &lt;a href=&#34;https://ajdillhoff.github.io/notes/kernels/&#34;&gt;Kernels&lt;/a&gt; for this.&lt;/p&gt;
&lt;p&gt;Note that, in most cases, \(\alpha_i\) will be 0 since we only consider &lt;strong&gt;support vectors&lt;/strong&gt;. That is, the points that lie on the margins of the decision boundary.&lt;/p&gt;
&lt;h2 id=&#34;overlapping-class-distributions&#34;&gt;Overlapping Class Distributions&lt;/h2&gt;
&lt;p&gt;The above formulation is fine and works with datasets that have no overlap in feature space.
That is, they are completely linearly separable.
However, it is not always the case that they will be.&lt;/p&gt;
&lt;p&gt;To account for misclassifications while still maximizing a the margin between datasets, we introduce a penalty value for points that are misclassified.
As long as there aren&amp;rsquo;t too many misclassifications, this penalty will stay relatively low while still allowing us to come up with an optimal solution.&lt;/p&gt;
&lt;p&gt;This penalty comes in the form of a &lt;strong&gt;slack variable&lt;/strong&gt; \(\xi_i \geq 0\) for each sample that is \(0\) for points that are on or inside the correct margin and \(\xi_i = |y_i - f(\mathbf{x})|\) for others.
If the point is misclassified, its slack variable will be \(\xi_i &amp;gt; 1\).&lt;/p&gt;
&lt;h2 id=&#34;multiclass-svm&#34;&gt;Multiclass SVM&lt;/h2&gt;
&lt;p&gt;Similar to our simple &lt;a href=&#34;https://ajdillhoff.github.io/notes/logistic_regression/&#34;&gt;Logistic Regression&lt;/a&gt; method, SVMs are binary classifiers by default. We can take a similar approach to extending them to multiple classes, but there are downsides to each approach.&lt;/p&gt;
&lt;p&gt;The &amp;ldquo;one-vs-all&amp;rdquo; approach entails building \(|K|\) classifiers and choose the classifier which predicts the input with the greatest margin.&lt;/p&gt;
&lt;p&gt;The &amp;ldquo;one-vs-one&amp;rdquo; approach involves building \(|K|\cdot\frac{|K| - 1}{2}\) classifiers. In this case, training each classifer will be more tractable since the amount of data required for each one is less. For example, you would have a model for class 1 vs 2, class 1 vs 3, &amp;hellip;, class 1 vs \(K\). Then repeat for class 2: 2 vs 3, 2 vs 4, &amp;hellip;, 2 vs \(|K|\), and so on.&lt;/p&gt;
&lt;p&gt;A third approach is to construct several models using a feature vector dependent on both the data and class label. When given a new input, the model computes&lt;/p&gt;
&lt;p&gt;\[
y = \text{arg}\max_{y&amp;rsquo;}\mathbf{w}^T\phi(\mathbf{x},y&amp;rsquo;).
\]&lt;/p&gt;
&lt;p&gt;The margin for this classifier is the distance between the correct class and the closest data point of any other class.&lt;/p&gt;
&lt;h2 id=&#34;additional-resources&#34;&gt;Additional Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://web.mit.edu/6.034/wwwbob/svm.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://web.mit.edu/6.034/wwwbob/svm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://ajdillhoff.github.io/notes/linear_regression/</link>
      <pubDate>Wed, 12 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/linear_regression/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#probabilistic-interpretation&#34;&gt;Probabilistic Interpretation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solving-with-normal-equations&#34;&gt;Solving with Normal Equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#another-approach-to-normal-equations&#34;&gt;Another Approach to Normal Equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fitting-polynomials&#34;&gt;Fitting Polynomials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear-basis-functions&#34;&gt;Linear Basis Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Given a dataset of observations \(\mathbf{X} \in \mathbb{R}^{n \times d}\), where \(n\) is the number of samples and \(d\) represents the number of features per sample, and corresponding target values \(\mathbf{Y} \in \mathbb{R}^n\), create a simple prediction model which predicts the target value \(\mathbf{y}\) given a new observation \(\mathbf{x}\). The classic example in this case is a linear model, a function that is a linear combination of the input features and some weights \(\mathbf{w}\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-15_13-35-19_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Plot of univariate data where the (x) values are features and (y) are observations.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Plot of univariate data where the (x) values are features and (y) are observations.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The generated data is plotted above along with the underlying true function that was used to generate it. If we already know what the true function is, our job is done. Suppose that we only have the data points (in blue). How do we go about modelling it? It is reasonable to first visualize the data and observe that it does follow a linear pattern. Thus, a linear model would be a decent model to choose.&lt;/p&gt;
&lt;p&gt;If the data followed a curve, we may decide to fit a polynomial. We will look at an example of that later on. For now, let&amp;rsquo;s formalize all of the information that we have.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\((\mathbf{x}, \mathbf{y})\) - Data points from the original dataset. Generally, \(\mathbf{x}\) is a vector of features and \(\mathbf{y}\) is the target vector. In our simple dataset above, these are both scalar values.&lt;/li&gt;
&lt;li&gt;\(\mathbf{w} = (w_0, w_1)\) - Our model parameters. Comparing to the equation \(y = mx + b\), \(w_0\) is our bias term and \(w_1\) is our slope parameter.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;making-predictions&#34;&gt;Making Predictions&lt;/h3&gt;
&lt;p&gt;Given \(\mathbf{w}\), we can make a prediction for a new data sample \(\mathbf{x} = x_1\).&lt;/p&gt;
&lt;p&gt;\[
h(\mathbf{x}; \mathbf{w}) = w_0 + w_1 x_1
\]&lt;/p&gt;
&lt;p&gt;Note that the bias term is always added to the result. We can simplify this into a more general form by appending a constant 1 (s.t. \(x_0 = 1\)) to each of our samples such that \(\mathbf{x} = (1, x_1, &amp;hellip;, x_d)\). Then, the general linear model becomes&lt;/p&gt;
&lt;p&gt;\[
h(\mathbf{x}; \mathbf{w}) = \sum_{i=0}^{d} w_i x_i = \mathbf{w}^T \mathbf{x}.
\]&lt;/p&gt;
&lt;p&gt;If our data happened to have more than 1 feature, it would be easy enough to model it appropriately using this notation.&lt;/p&gt;
&lt;h3 id=&#34;determining-fitness&#34;&gt;Determining Fitness&lt;/h3&gt;
&lt;p&gt;If we really wanted to, we could fit our model by plotting it and manually adjusting the weights until our model looked acceptable by some qualitative standard. Fortunately we won&amp;rsquo;t be doing that. Instead, we will use a quantitative measurement that provides a metric of how well our current parameters fit the data.&lt;/p&gt;
&lt;p&gt;For this, we use a &lt;strong&gt;&lt;strong&gt;cost function&lt;/strong&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;strong&gt;loss function&lt;/strong&gt;&lt;/strong&gt;. The most common one to use for this type of model is the least-squares function:&lt;/p&gt;
&lt;p&gt;\[
J(\mathbf{w}) = \frac{1}{2}\sum_{i=1}^{n}(h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i})^2.
\]&lt;/p&gt;
&lt;h3 id=&#34;stochastic-gradient-descent&#34;&gt;Stochastic Gradient Descent&lt;/h3&gt;
&lt;p&gt;Depending on the random initialization of parameters, our error varies greatly. We can observe that no matter what the chose parameters are, there is no possible way we can achieve an error of 0. The best we can do is minimize this error:&lt;/p&gt;
&lt;p&gt;\[
\min_{\mathbf{w}} J(\mathbf{w}).
\]&lt;/p&gt;
&lt;p&gt;For this, we rely on stochastic gradient descent. The basic idea is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Begin with an initial guess for \(\mathbf{w}\).&lt;/li&gt;
&lt;li&gt;Compare the prediction for sample \(\mathbf{x}^{(i)}\) with its target \(\mathbf{y}^{(i)}\).&lt;/li&gt;
&lt;li&gt;Update \(\mathbf{w}\) based on the comparison in part 2.&lt;/li&gt;
&lt;li&gt;Repeat steps 2 and 3 on the dataset until the loss has converged.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Steps 1, 3, and 4 are easy enough. What about step 2? How can we possibly know how to modify \(\mathbf{w}\) such that \(J(\mathbf{w})\) will decrease? By computing the gradient \(\frac{d}{d\mathbf{w}}J(\mathbf{w})\)! How will we know when we have arrived at a minima? When \(\nabla J(\mathbf{w}) = 0\).&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{d}{d\mathbf{w}}J(\mathbf{w}) &amp;amp;= \frac{d}{d\mathbf{w}}\frac{1}{2}(h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i})^2\\
&amp;amp;= 2 \cdot \frac{1}{2}(h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i}) \cdot \frac{d}{d\mathbf{w}} (h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i})\\
&amp;amp;= (h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i}) \cdot \frac{d}{d\mathbf{w}} (\mathbf{w}^T \mathbf{x}_{i} - \mathbf{y}_{i})\\
&amp;amp;= (h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i}) \mathbf{x}_{i}
\end{align*}&lt;/p&gt;
&lt;p&gt;The gradient represents the direction of greatest change for a function evaluated With this gradient, we can use an update rule to modify the previous parameter vector \(\mathbf{w}\):&lt;/p&gt;
&lt;p&gt;\[
\mathbf{w}_{t+1} = \mathbf{w}_{t} - \alpha \sum_{i=1}^{n} (h(\mathbf{x}_{i};\mathbf{w}_{t}) - \mathbf{y}_{i}) \mathbf{x}_{i}.
\]&lt;/p&gt;
&lt;p&gt;Here, \(\alpha\) is an update hyperparameter that allows us to control how big or small of a step our weights can take with each update. In general, a smaller value will be more likely to get stuck in local minima. However, too large of a value may never converge to any minima.&lt;/p&gt;
&lt;p&gt;Another convenience of this approach is that it is possible to update the weights based on a single sample, batch of samples, or the entire dataset. This sequential process makes optimization using very large dataset feasible.&lt;/p&gt;
&lt;h2 id=&#34;probabilistic-interpretation&#34;&gt;Probabilistic Interpretation&lt;/h2&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Probability theory is nothing but common sense reduced to calculation.&amp;rdquo; - Pierre-Simon Laplace&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;Recall Bayes&amp;rsquo; theorem:&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{w}|\mathbf{X}) = \frac{p(\mathbf{X}|\mathbf{w})p(\mathbf{w})}{p(\mathbf{X})}.
\]&lt;/p&gt;
&lt;p&gt;That is, the &lt;em&gt;posterior&lt;/em&gt; probability of the weights conditioned on the observered data \(\mathbf{X}\) is equal to the &lt;em&gt;likelihood&lt;/em&gt; of the observed data given the times the &lt;em&gt;prior&lt;/em&gt; distribution. This base notation doesn&amp;rsquo;t line up well with our problem. For our problem, we have observations \(\mathbf{Y}\) which are dependent on the input features \(\mathbf{X}\):&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{w}|\mathbf{X}, \mathbf{Y}) = \frac{p(\mathbf{Y}|\mathbf{X}, \mathbf{w}) p(\mathbf{w}|\mathbf{X})}{p(\mathbf{Y}|\mathbf{X})},
\]&lt;/p&gt;
&lt;p&gt;where \(\mathbf{X} \in \mathbb{R}^{n \times d}\) and \(\mathbf{Y} \in \mathbb{R}^n\).&lt;/p&gt;
&lt;p&gt;The choice of least squares also has statistical motivations. As discussed previously, we are making a reasonable assumption that there is some relationship between the features of the data and the observed output. This is typically modeled assume&lt;/p&gt;
&lt;p&gt;\[
\hat{\mathbf{Y}} = f(\mathbf{X}) + \epsilon.
\]&lt;/p&gt;
&lt;p&gt;Here, \(\epsilon\) is a random error term that is independent of \(\mathbf{X}\) and has 0 mean. This term represents any random noise that occurs either naturally or from sampling. It also includes any effects that are not properly captured by \(f\). Rearranging the terms of this equation to solve for \(\epsilon\) allows us to define the discrepencies in the model:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{\epsilon}_i = h(\mathbf{x}_{i}; \mathbf{w}) - \mathbf{y}_{i}.
\]&lt;/p&gt;
&lt;p&gt;If we assume that these discrepancies are independent and identically distributed with variance \(\sigma^2\) and Gaussian PDF \(f\), the likelihood of observations \(\mathbf{y}^{(i)}\) given parameters \(\mathbf{w}\) is&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) = \prod_{i=1}^{n} f(\epsilon_i; \sigma),
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
f(\epsilon_i; \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{\epsilon^2}{2\sigma^2}\Big).
\]&lt;/p&gt;
&lt;p&gt;This new parameter changes our original distribution function to&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{w}|\mathbf{X}, \mathbf{Y}, \sigma) = \frac{p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) p(\mathbf{w}|\mathbf{X}, \sigma)}{p(\mathbf{Y}|\mathbf{X}, \sigma)}.
\]&lt;/p&gt;
&lt;p&gt;Two things to note before moving on. First, the prior \(p(\mathbf{Y}|\mathbf{X}, \sigma)\) is a normalizing constant to ensure that the posterior is a valid probability distribution. Second, if we assume that all value for \(\mathbf{w}\) are equally likely, then \(p(\mathbf{w}|\mathbf{x}, \sigma)\) also becomes constant. This is a convenient assumption which implies that maximizing the posterior is equivalent to maximizing the likelihood function.&lt;/p&gt;
&lt;p&gt;With that out of the way, we can focus solely on the likelihood function. Expanding out the gaussian PDF \(f\) yields&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) = -\frac{n}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i})^2\Big).
\]&lt;/p&gt;
&lt;p&gt;We can see that maximizing \(p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma)\) is the same as minimizing the sum of squares. In practice, we use the negative log of the likelihood function since the negative logarithm is monotonically decreasing.&lt;/p&gt;
&lt;h2 id=&#34;solving-with-normal-equations&#34;&gt;Solving with Normal Equations&lt;/h2&gt;
&lt;p&gt;You may have studied the normal equations when you took Linear Algebra. The normal equations are motivated by finding approximate solutions to \(A\mathbf{x} = \mathbf{b}\). Most of the earlier part of linear algebra courses focus on finding exact solutions by solving systems of equations using Gaussian elimination (row reduction). Approximate solutions can be found by projecting the observed data points \(\mathbf{b}\) onto the column space of \(A\) and solving \(A \mathbf{x} = \hat{\mathbf{b}}\), where \(\hat{\mathbf{b}} = \text{proj}_{\text{Col} A}\mathbf{b}\). Then, \(\mathbf{b} - \hat{\mathbf{b}}\) represents a vector orthogonal to \(\text{Col}A\).&lt;/p&gt;
&lt;p&gt;It is helpful to keep in mind what \(A\), \(\mathbf{x}\), and \(\mathbf{b}\) represent. \(A\) and \(\mathbf{b}\) are the input and output values. If we were trying to predict home prices based on size, each row of \(A\) would represent the size of a different house. In \(\mathbf{b}\) we would record the corresponding prices. We are trying to solve for \(\mathbf{x}\), which is a vector that relates the input to the output.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2024-01-15_20-40-39_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;The plane represents every linear combination of the columns of A.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;The plane represents every linear combination of the columns of A.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Since each column vector of \(A\) is orthogonal to \(\mathbf{b} - \hat{\mathbf{b}}\), the dot product between them should be 0. Rewriting this, we get&lt;/p&gt;
&lt;p&gt;\begin{aligned}
A^T(\mathbf{b} - A\mathbf{x}) &amp;amp;= \mathbf{0}\\
A^T \mathbf{b} - A^T A \mathbf{x} &amp;amp;= \mathbf{0}.
\end{aligned}&lt;/p&gt;
&lt;p&gt;This means that each least-squares solution of \(A\mathbf{x} = \mathbf{b}\) satisfies&lt;/p&gt;
&lt;p&gt;\[
A^T A \mathbf{x} = A^T \mathbf{b}.
\]&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s take our univariate problem of \((\mathbf{x}, \mathbf{y})\) pairs. To use the normal equations to solve the least squares problem, we first change the notation just a bit as not confuse our data points and our parameters:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{X}^T \mathbf{X} \beta = \mathbf{X}^T \mathbf{y}
\]&lt;/p&gt;
&lt;p&gt;Create the design matrix \(\mathbf{X}\) where each row represents the the \(\mathbf{x}\) values. Recall that even though we only have 1 feature for \(\mathbf{x}\), we append the bias constant as \(x_0 = 1\) to account for the bias parameter. \(\mathbf{X}\) is then&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{X} =
\begin{bmatrix}
x_0^{(0)} &amp;amp; x_1^{(0)}\\
x_0^{(1)} &amp;amp; x_1^{(1)}\\
\vdots &amp;amp; \vdots \\
x_0^{(n)} &amp;amp; x_1^{(n)}
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;The parameter vector is&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\beta =
\begin{bmatrix}
\beta_0\\
\beta_1
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;The observed values are packed into \(\mathbf{y}\). We can then solve for \(\beta\) using any standard solver:&lt;/p&gt;
&lt;p&gt;\[
\beta = (\mathbf{X}^T \mathbf{X})^{-1}X^T \mathbf{y}.
\]&lt;/p&gt;
&lt;h3 id=&#34;rank-deficient-matrices&#34;&gt;Rank-Deficient matrices&lt;/h3&gt;
&lt;p&gt;In the event that the matrix \(\mathbf{X}^T \mathbf{X}\) is singular, then its inverse cannot be computed.
This implies that one or more of the features is a linear combination of the others.&lt;/p&gt;
&lt;p&gt;This can be detected by checking the rank of \(\mathbf{X}^T \mathbf{X}\) before attempting to compute the inverse.
You can also determine which features are redundant via Gaussian elimination.
The columns in the reduced matrix that do not have a pivot entry are redundant.&lt;/p&gt;
&lt;h2 id=&#34;another-approach-to-normal-equations&#34;&gt;Another Approach to Normal Equations&lt;/h2&gt;
&lt;p&gt;We can arrive at the normal equations by starting at the probabilistic perspective. Recall the likelihood function&lt;/p&gt;
&lt;p&gt;\[
p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) = -\frac{n}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(h(\mathbf{x}_{i};\mathbf{w}) - \mathbf{y}_{i})^2\Big).
\]&lt;/p&gt;
&lt;p&gt;Taking the natural log of this function yields&lt;/p&gt;
&lt;p&gt;\[
\ln p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) = - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(h(\mathbf{x}_{i}; \mathbf{w}) - \mathbf{y}_{i})^2 - \frac{n}{2}\ln(\sigma^2) - \frac{n}{2}\ln(2\pi).
\]&lt;/p&gt;
&lt;p&gt;As mentioned before, maximizing the likelihood function is equivalent to minimizing the sum-of-squares function. Thus, we must find the critical point of the likelihood function by computing the gradient (w.r.t. \(\mathbf{w}\)) and solving for 0:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\nabla \ln p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) &amp;amp;= \sum_{i=1}^{n}(\mathbf{w}^T\mathbf{x}_{i} - \mathbf{y}_{i})\mathbf{x}_{i}^{T}\\
&amp;amp;= \mathbf{w}^T \sum_{i=1}^{n}\mathbf{x}_i\mathbf{x}_i^T - \sum_{i=1}^{n}\mathbf{y}_{i}\mathbf{x}_{i}^{T}\\
\end{align*}&lt;/p&gt;
&lt;p&gt;Noting that \(\sum_{i=1}^{n}\mathbf{x}_i \mathbf{x}_i^T\) is simply matrix multiplication, we can use&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{X} =
\begin{bmatrix}
\mathbf{x}_1^T\\
\vdots\\
\mathbf{x}_n^T\\
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Then, \(\sum_{i=1}^{n}\mathbf{x}_i \mathbf{x}_i^T = \mathbf{X}^T \mathbf{X}\), \(\sum_{i=1}^{n}\mathbf{y}_i \mathbf{x}_i^T = \mathbf{Y}^T \mathbf{X}\), and&lt;/p&gt;
&lt;p&gt;\[
\nabla \ln p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) = \mathbf{w}^T \mathbf{X}^T \mathbf{X} - \mathbf{Y}^T \mathbf{X}.
\]&lt;/p&gt;
&lt;p&gt;Since we are finding the maximum likelihood, we set \(\nabla \ln p(\mathbf{Y}|\mathbf{X}, \mathbf{w}, \sigma) = 0\) and solve for \(\mathbf{w}\):&lt;/p&gt;
&lt;p&gt;\[
\mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}.
\]&lt;/p&gt;
&lt;p&gt;Thus, we arrive again at the normal equations and can solve this using a linear solver.&lt;/p&gt;
&lt;h2 id=&#34;fitting-polynomials&#34;&gt;Fitting Polynomials&lt;/h2&gt;
&lt;p&gt;Not every dataset can be modeled using a simple line.
Data can be exponential or logarithmic in nature.
We may also look to use &lt;a href=&#34;https://en.wikipedia.org/wiki/Spline_%28mathematics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;splines&lt;/a&gt; to model more complex data.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-01_17-08-27_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Data generated from a nonlinear function with added noise.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Data generated from a nonlinear function with added noise.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The dataset above was generated from the function as seen in red.
Using a simple linear model (blue) does not fit the data well.
For cases such as this, we can fit a polynomial to the data by changing our input data.&lt;/p&gt;
&lt;p&gt;The simple dataset above has 100 paired samples \((x_i, y_i)\).
There is only a single feature \(x_i\) for each sample.
It is trivial to determine that the shape of the data follows a cubic function.
One solution would be to raise each input to the power of 3.
This results in the function (blue) below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-01_17-30-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Solution from raising each input to the power of 3.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Solution from raising each input to the power of 3.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To fit this data, we need to add more features to our input.
Along with the original \(x_i\) features, we will also add \(x_i^2\) and \(x_i^3\).
Our data is then 3 dimensional.
The figure below shows the least squares fit using the modified data (blue).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-01_17-38-57_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Least squares fit using a polynomial model (blue).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Least squares fit using a polynomial model (blue).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;A demo of this can be found &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/blob/main/linear_regression/Linear%20Regression.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;linear-basis-functions&#34;&gt;Linear Basis Functions&lt;/h2&gt;
&lt;p&gt;Linear models are linear in their inputs.
This formulation is simple, producing models with limited representation.
Linear models can be extended as a linear combination of fixed nonlinear functions of the original features.
In the previous section, was saw that they could easily be extended to fit polynomial functions.&lt;/p&gt;
&lt;p&gt;We now consider creating a model that transforms the original input using one or more nonlinear functions.
This type of model is called a &lt;strong&gt;&lt;strong&gt;linear basis function model&lt;/strong&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;\[
h(\mathbf{x};\mathbf{w}) = \sum_{j=1}^{m} w_j\phi_j(\mathbf{x})
\]&lt;/p&gt;
&lt;p&gt;Common basis functions are the sigmoid, Gaussian, or exponential function.
If we choose the \(\sin\) function as a basis function, we can more closely fit our dataset using the least squares approach.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-06-01_18-46-08_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;A linear basis function model using the sin function as the choice of basis.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;A linear basis function model using the sin function as the choice of basis.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
  </channel>
</rss>
