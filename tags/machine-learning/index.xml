<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Alex Dillhoff</title>
    <link>https://ajdillhoff.github.io/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on Alex Dillhoff</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Tue, 15 Apr 2025 00:00:00 -0400</lastBuildDate>
    <atom:link href="https://ajdillhoff.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Using RAG to Talk to Your Data</title>
      <link>https://ajdillhoff.github.io/blog/using-rag-to-talk-to-your-data/</link>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/blog/using-rag-to-talk-to-your-data/</guid>
      <description>How can LLMs provide results that are not only factual, but based on your own private data? This article accompanies a workshop given at HackUTA 6 on October 12, 2024.</description>
    </item>
    <item>
      <title>Automatic Differentiation</title>
      <link>https://ajdillhoff.github.io/notes/automatic_differentiation/</link>
      <pubDate>Mon, 23 Sep 2024 18:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/notes/automatic_differentiation/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#types-of-differentiation&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Types of Differentiation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#forward-mode-ad&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Forward Mode AD&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#reverse-mode-ad&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Reverse Mode AD&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#basic-implementation-in-python&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Basic Implementation in Python&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#matrix-implementation&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Matrix Implementation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#comparison-with-pytorch&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Comparison with PyTorch&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;These notes largely follow the survey presented by (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Baydin et al. 2018&lt;/a&gt;). I have added a few examples to clarify the matrix algebra as well as a lead in to a practical implemenation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Language of LLMs</title>
      <link>https://ajdillhoff.github.io/blog/the-language-of-llms/</link>
      <pubDate>Thu, 11 Apr 2024 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/blog/the-language-of-llms/</guid>
      <description>How do LLMs read and process the high dimensional landscape of text efficiently? Presented as a workshop at UTA&amp;rsquo;s Datathon on April 13, 2024.</description>
    </item>
    <item>
      <title>Bag of Visual Words</title>
      <link>https://ajdillhoff.github.io/notes/bag_of_visual_words/</link>
      <pubDate>Sun, 04 Feb 2024 18:54:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/bag_of_visual_words/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#bag-of-visual-words&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Bag of Visual Words&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;p&gt;&lt;strong&gt;Bag of Words&lt;/strong&gt; is a technique used in Natural Language Processing for document classification. It is a collection of word counts. To create a Bag of Words for a document, it necessary to create a dictionary first. Choosing the a dictionary is based on many factors including computational limitations. Next, the documents in a dataset are tokenized into words. The word counts are collected as part of a histogram and used as a feature vector for a machine learning model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pretraining Large Language Models</title>
      <link>https://ajdillhoff.github.io/notes/pretraining_large_language_models/</link>
      <pubDate>Thu, 16 Nov 2023 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/pretraining_large_language_models/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#unsupervised-pre-training&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Unsupervised Pre-training&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#from-gpt-to-gpt2&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;From GPT to GPT2&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#bert&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;BERT&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#bart&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;BART&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#gpt3&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;GPT3&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;p&gt;These notes provide an overview of pre-training large language models like GPT and Llama.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gradient Boosting</title>
      <link>https://ajdillhoff.github.io/notes/gradient_boosting/</link>
      <pubDate>Mon, 17 Jul 2023 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/notes/gradient_boosting/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#notes-from&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Notes from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Friedman 2001&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;notes-from&#34;&gt;Notes from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Friedman 2001&lt;/a&gt;)&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Many machine learning methods are parameterized functions that are optimized using some numerical optimization techniques, notably steepest-descent.&lt;/p&gt;</description>
    </item>
    <item>
      <title>An Introduction to Hidden Markov Models for Gesture Recognition</title>
      <link>https://ajdillhoff.github.io/blog/introduction-to-hmms/</link>
      <pubDate>Sat, 15 Jul 2023 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/blog/introduction-to-hmms/</guid>
      <description>Hidden Markov Models provide a way of modeling the dynamics of sequential information. They have been used for speech recognition, part-of-speech tagging, machine translation, handwriting recognition, and, as we will see in this article, gesture recognition.</description>
    </item>
    <item>
      <title>Bias and Variance</title>
      <link>https://ajdillhoff.github.io/notes/bias_and_variance/</link>
      <pubDate>Tue, 04 Jul 2023 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/notes/bias_and_variance/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#generalization&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Generalization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#bias&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Bias&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#variance&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Variance&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#bias-variance-tradeoff&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Bias-Variance Tradeoff&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;generalization&#34;&gt;Generalization&lt;/h2&gt;&#xA;&lt;p&gt;When fitting machine learning models to data, we want them to &lt;strong&gt;generalize&lt;/strong&gt; well to the distribution that we have sampled from. We can measure a model&amp;rsquo;s ability to generalize by evaluating it on previously unseen data that is sampled from the same distribution as the training set. However, we often do not know the true underlying distribution. So we must fit the models to empirical distributions derived from observed data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Transformers for Computer Vision</title>
      <link>https://ajdillhoff.github.io/notes/transfomers_for_computer_vision/</link>
      <pubDate>Tue, 18 Apr 2023 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/notes/transfomers_for_computer_vision/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#vision-transformer--vit&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Vision Transformer (ViT) (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Dosovitskiy et al. 2021&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#swin-transformer&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Swin Transformer (&lt;a href=&#34;#citeproc_bib_item_4&#34;&gt;Liu et al. 2021&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;vision-transformer--vit&#34;&gt;Vision Transformer (ViT) (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Dosovitskiy et al. 2021&lt;/a&gt;)&lt;/h2&gt;&#xA;&lt;p&gt;The original Vision Transformer (ViT) was published by Google Brain with a simple objective: apply the Transformer architecture to images, adding as few modifications necessary. When trained on ImageNet, as was standard practice, the performance of ViT does not match models like ResNet. However, scaling up to hundreds of millions results in a better performing model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sequential Minimal Optimization</title>
      <link>https://ajdillhoff.github.io/notes/sequential_minimal_optimization/</link>
      <pubDate>Mon, 04 Jul 2022 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/notes/sequential_minimal_optimization/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#box-constraints&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Box Constraints&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#updating-the-lagrangians&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Updating the Lagrangians&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#the-algorithm&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;The Algorithm&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#implementation&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Implementation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Paper link:&lt;/strong&gt; &lt;a href=&#34;https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Instance Segmentation</title>
      <link>https://ajdillhoff.github.io/notes/instance_segmentation/</link>
      <pubDate>Mon, 18 Apr 2022 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/notes/instance_segmentation/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#mask-r-cnn&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Mask R-CNN (&lt;a href=&#34;#citeproc_bib_item_4&#34;&gt;He et al. 2018&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#centermask&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;CenterMask (&lt;a href=&#34;#citeproc_bib_item_6&#34;&gt;Lee and Park 2020&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#cascade-r-cnn&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Cascade R-CNN (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cai and Vasconcelos 2019&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#maskformer&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;MaskFormer (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Cheng, Schwing, and Kirillov 2021&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#mask2former&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Mask2Former (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Cheng et al. 2022&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#mask-frozendetr&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Mask-FrozenDETR (&lt;a href=&#34;#citeproc_bib_item_7&#34;&gt;Liang and Yuan 2023&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#segment-anything&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Segment Anything (&lt;a href=&#34;#citeproc_bib_item_5&#34;&gt;Kirillov et al. 2023&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#segment-anything-2&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Segment Anything 2 (&lt;a href=&#34;#citeproc_bib_item_9&#34;&gt;Ravi et al. 2024&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;h2 id=&#34;mask-r-cnn&#34;&gt;Mask R-CNN (&lt;a href=&#34;#citeproc_bib_item_4&#34;&gt;He et al. 2018&lt;/a&gt;)&lt;/h2&gt;&#xA;&lt;p&gt;Mask R-CNN adapts Faster R-CNN to include a branch for instance segmentation (&lt;a href=&#34;#citeproc_bib_item_10&#34;&gt;Ren et al. 2017&lt;/a&gt;). This branch predicts a binary mask for each RoI, and the training loss is updated to include this branch.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Object Detection</title>
      <link>https://ajdillhoff.github.io/notes/object_detection/</link>
      <pubDate>Mon, 18 Apr 2022 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/notes/object_detection/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#papers&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Papers&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#evaluating-object-detection-methods&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Evaluating Object Detection Methods&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#datasets&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Datasets&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#an-incomplete-history-of-deep-learning-based-object-detection&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;An Incomplete History of Deep-Learning-based Object Detection&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;papers&#34;&gt;Papers&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://awesomeopensource.com/projects/object-detection&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;https://awesomeopensource.com/projects/object-detection&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;evaluating-object-detection-methods&#34;&gt;Evaluating Object Detection Methods&lt;/h2&gt;&#xA;&lt;p&gt;Object detection algorithms are evaluated using the mean of Average Precision (mAP) across all classes in the dataset.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Boosting</title>
      <link>https://ajdillhoff.github.io/notes/boosting/</link>
      <pubDate>Wed, 23 Mar 2022 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/notes/boosting/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#adaboost&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;AdaBoost&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Combining predictions from multiple sources is usually preferred to a single source.&#xA;For example, a medical diagnosis would carry much more weight if it was the result of a consensus of several experts.&#xA;This idea of prediction by consensus is a powerful way to improve classification and regression models.&#xA;In fact, good performance of a committee of models can be achieved even if each individual model is conceptually very simple.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Decision Trees</title>
      <link>https://ajdillhoff.github.io/notes/decision_trees/</link>
      <pubDate>Fri, 18 Mar 2022 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/notes/decision_trees/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#resources&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Resources&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#example-iris-dataset&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Example: Iris Dataset&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#growing-a-tree&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Growing a Tree&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#examining-the-iris-classification-tree&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Examining the Iris Classification Tree&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#pruning-a-tree&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Pruning a Tree&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#the-algorithm&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;The Algorithm&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/dmilla/introduction-to-decision-trees-titanic-dataset&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;https://www.kaggle.com/dmilla/introduction-to-decision-trees-titanic-dataset&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;A &lt;strong&gt;decision tree&lt;/strong&gt;, or Classification and Regression Trees (CART), is a model that recursively partitions the input space based on a collection of features.&#xA;The partitions are split based on very simple binary choices.&#xA;If yes, branch to the left; if no, branch to the right.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Camera Models</title>
      <link>https://ajdillhoff.github.io/notes/camera_models/</link>
      <pubDate>Fri, 11 Mar 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/camera_models/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#reading&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Reading&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#outline&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Outline&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#pinhole-model&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Pinhole Model&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#from-world-space-to-image-space&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;From World Space to Image Space&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#camera-parameters&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Camera Parameters&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#estimating-camera-parameters&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Estimating Camera Parameters&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#application-camera-calibration&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Application: Camera Calibration&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;reading&#34;&gt;Reading&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Chapters 1 and 7 (Forsyth and Ponce)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.scratchapixel.com/&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;https://www.scratchapixel.com/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1RMyNQR9jGdJm64FCiuvoyJiL6r3H18jeNIyocIO9sfA/edit#slide=id.p&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;https://docs.google.com/presentation/d/1RMyNQR9jGdJm64FCiuvoyJiL6r3H18jeNIyocIO9sfA/edit#slide=id.p&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://vision.stanford.edu/teaching/cs131_fall1617/lectures/lecture8_camera_models_cs131_2016.pdf&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;http://vision.stanford.edu/teaching/cs131_fall1617/lectures/lecture8_camera_models_cs131_2016.pdf&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://vlm1.uta.edu/~athitsos/courses/cse4310_spring2021/lectures/11_geometry.pdf&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;http://vlm1.uta.edu/~athitsos/courses/cse4310_spring2021/lectures/11_geometry.pdf&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;outline&#34;&gt;Outline&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Pinhole model&lt;/li&gt;&#xA;&lt;li&gt;Coordinates of a pinhole model&lt;/li&gt;&#xA;&lt;li&gt;Perspective projections&lt;/li&gt;&#xA;&lt;li&gt;Homogeneous coordinates&lt;/li&gt;&#xA;&lt;li&gt;Computer graphics perspective&lt;/li&gt;&#xA;&lt;li&gt;Lenses&lt;/li&gt;&#xA;&lt;li&gt;Intrinsic and extrensic parameters&lt;/li&gt;&#xA;&lt;li&gt;From world to camera to image space&lt;/li&gt;&#xA;&lt;li&gt;Camera calibration&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;pinhole-model&#34;&gt;Pinhole Model&lt;/h2&gt;&#xA;&lt;p&gt;Imagine piercing a small hole into a plate and placing it in front of a black screen.&#xA;The light that enters through the pinhole will show an inverted image against the back plane.&#xA;If we place a virtual screen in front of the pinhole plate, we can project the image onto it.&#xA;This is the basic idea behind a &lt;strong&gt;pinhole camera model&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hidden Markov Models</title>
      <link>https://ajdillhoff.github.io/notes/hidden_markov_models/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/hidden_markov_models/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#the-markov-assumption&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;The Markov Assumption&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#definition&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Definition&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#evaluation&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Evaluation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#the-viterbi-algorithm&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;The Viterbi Algorithm&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#estimating-parameters&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Estimating Parameters&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#expectation-maximization&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Expectation Maximization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;This article is essentially a grok of a tutorial on HMMs by (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;RABINER 1989&lt;/a&gt;). It will be useful for the reader to reference the &lt;a href=&#34;https://courses.physics.illinois.edu/ece417/fa2017/rabiner89.pdf&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;original paper&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RANdom SAmple Consensus</title>
      <link>https://ajdillhoff.github.io/notes/random_sample_consensus/</link>
      <pubDate>Tue, 15 Feb 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/random_sample_consensus/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#finding-the-best-fit-model&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Finding the Best Fit Model&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Unless our data is perfect, we will not be able to find parameters that fit the data in the presence of outliers.&#xA;Consider fitting the data in the figure below using a least squares method.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kernels</title>
      <link>https://ajdillhoff.github.io/notes/kernels/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/kernels/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#dual-representation&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Dual Representation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#relating-back-to-the-original-formulation&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Relating Back to the Original Formulation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#types-of-kernels&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Types of Kernels&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#constructing-kernels&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Constructing Kernels&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#rbf-maps-to-infinite-dimensional-space&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;RBF maps to infinite-dimensional space&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;p&gt;Slides for these notes can be found &lt;a href=&#34;https://ajdillhoff.github.io/teaching/cse6363/lectures/kernels.pdf&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;here.&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Naive Bayes</title>
      <link>https://ajdillhoff.github.io/notes/naive_bayes/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/naive_bayes/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#definition&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Definition&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#maximum-likelihood-estimation&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Maximum Likelihood Estimation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#making-a-decision&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Making a Decision&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#relation-to-multinomial-logistic-regression&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Relation to Multinomial Logistic Regression&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#mnist-example&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;MNIST Example&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#gaussian-formulation&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Gaussian Formulation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;p&gt;Slides for these notes can be found &lt;a href=&#34;https://ajdillhoff.github.io/teaching/cse6363/lectures/naive_bayes.pdf&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;here.&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Neural Networks</title>
      <link>https://ajdillhoff.github.io/notes/neural_networks/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/neural_networks/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#linear-models-as-a-template-for-machine-learning&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Linear Models as a Template for Machine Learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#definition&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Definition&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#forward-pass&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Forward Pass&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#activation-functions&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Activation Functions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#multi-class-classification&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Multi-Class Classification&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#backpropagation&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Backpropagation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#visualizing-neural-networks&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Visualizing Neural Networks&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#non-convex-optimization&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Non-Convex Optimization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;linear-models-as-a-template-for-machine-learning&#34;&gt;Linear Models as a Template for Machine Learning&lt;/h2&gt;&#xA;&lt;p&gt;Previously, we studied the &lt;a href=&#34;https://ajdillhoff.github.io/notes/perceptron/&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Perceptron&lt;/a&gt; and saw that while it made for a simple linear classifier, it is severely limited to problems that are already linearly separable.&#xA;This limitation was resolved by introduding a hidden layer with multiple perceptron units, aptly named Multi-Layer Perceptrons.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Perceptron</title>
      <link>https://ajdillhoff.github.io/notes/perceptron/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/perceptron/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#the-perceptron-learning-algorithm&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;The Perceptron Learning Algorithm&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#limitations-of-single-layer-perceptrons&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Limitations of Single-Layer Perceptrons&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;A popular example of a &lt;a href=&#34;https://ajdillhoff.github.io/notes/logistic_regression/&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Logistic Regression&lt;/a&gt; model is the &lt;strong&gt;perceptron&lt;/strong&gt;. Proposed by Frank Rosenblatt in 1962, the perceptron is defined as a generalized linear model:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Principal Component Analysis</title>
      <link>https://ajdillhoff.github.io/notes/principal_component_analysis/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/principal_component_analysis/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#summary&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Summary&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#maximum-variance-formulation&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Maximum Variance Formulation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#motivating-example&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Motivating Example&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#noise-and-redundancy&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Noise and Redundancy&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#covariance-matrix&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Covariance Matrix&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;&#xA;&lt;p&gt;If we have some measurements of data, but do not know the underlying dynamics, PCA can resolve this by producing a change of basis such that the dynamics are reflected upon the eigenvectors.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Probability Theory</title>
      <link>https://ajdillhoff.github.io/notes/probability_theory/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/probability_theory/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#a-simple-example&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;A Simple Example&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#probability-distributions&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Probability Distributions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#conditional-probability&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Conditional Probability&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#rules-of-probability&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Rules of Probability&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#random-variables&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Random Variables&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#continuous-variables&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Continuous Variables&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#moments-of-a-distribution&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Moments of a Distribution&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;p&gt;Slides for these notes are available &lt;a href=&#34;https://ajdillhoff.github.io/teaching/cse6363/lectures/probability.pdf&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;here.&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Regularization</title>
      <link>https://ajdillhoff.github.io/notes/regularization/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/regularization/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#overfitting&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Overfitting&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#penalizing-weights&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Penalizing Weights&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#dataset-augmentation&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Dataset Augmentation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#early-stopping&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Early Stopping&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#dropout&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Dropout&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;p&gt;Slides for these notes are available &lt;a href=&#34;https://ajdillhoff.github.io/teaching/cse6363/lectures/regularization.pdf&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;here.&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Support Vector Machine</title>
      <link>https://ajdillhoff.github.io/notes/support_vector_machine/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/support_vector_machine/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#maximum-margin-classifier&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Maximum Margin Classifier&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#formulation&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Formulation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#overlapping-class-distributions&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Overlapping Class Distributions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#multiclass-svm&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Multiclass SVM&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#additional-resources&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Additional Resources&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Support Vector Machines are a class of supervised learning methods primarily used for classification. Although they can be formulated for regression and outlier detection as well. Instead of optimizing a set of parameters which compress or summarize the training set, they use a small subset of the training data to compute the decision function.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Regression</title>
      <link>https://ajdillhoff.github.io/notes/linear_regression/</link>
      <pubDate>Wed, 12 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/linear_regression/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#probabilistic-interpretation&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Probabilistic Interpretation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#solving-with-normal-equations&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Solving with Normal Equations&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#another-approach-to-normal-equations&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Another Approach to Normal Equations&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#fitting-polynomials&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Fitting Polynomials&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#linear-basis-functions&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Linear Basis Functions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;p&gt;Slides for these notes are available &lt;a href=&#34;https://ajdillhoff.github.io/teaching/cse6363/lectures/linear_regression.pdf&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;here.&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
