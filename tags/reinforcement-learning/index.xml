<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement Learning on Alex Dillhoff</title>
    <link>http://localhost:1313/tags/reinforcement-learning/</link>
    <description>Recent content in Reinforcement Learning on Alex Dillhoff</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Tue, 18 Feb 2025 00:00:00 -0600</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Policy Gradient Methods</title>
      <link>http://localhost:1313/notes/policy_gradient_methods/</link>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0600</pubDate>
      <guid>http://localhost:1313/notes/policy_gradient_methods/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#policy-gradients&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Policy Gradients&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;p&gt;When we had full knowledge of the states, we could use &lt;a href=&#34;http://localhost:1313/notes/markov_decision_processes/&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Markov Decision Processes&lt;/a&gt; to find the optimal policy. When this assumption breaks down, we need to come up with our best approximation. This is not a far stretch from how we might handle new scenarios in our own lives. When we begin a new task, we are certainly not experts. We may learn from a teacher or set off to explore on our own. As we practice and churn out the seemingly endless variations of our endeavour, we begin to develop a sense of what works and what doesn&amp;rsquo;t. We may not be able to articulate the exact rules that we follow, but we can certainly tell when we are doing well or poorly.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Markov Decision Processes</title>
      <link>http://localhost:1313/notes/markov_decision_processes/</link>
      <pubDate>Mon, 24 Jul 2023 00:00:00 -0500</pubDate>
      <guid>http://localhost:1313/notes/markov_decision_processes/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#key-terms&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Key Terms&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#defining-goals&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Defining Goals&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#policies-and-values&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Policies and Values&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#bellman-equations&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Bellman Equations&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#optimality&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Optimality&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#optimizing-the-policy&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Optimizing the Policy&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;key-terms&#34;&gt;Key Terms&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Agent&lt;/strong&gt;: The learner or decision maker.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt;: The world that the agent can interact with.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;State&lt;/strong&gt;: A representation of the agent and environment.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: The agent can take an action in the environment.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Reward&lt;/strong&gt;: Given to the agent based on actions taken.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Maximize rewards earned over time.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
