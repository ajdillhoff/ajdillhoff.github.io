<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement Learning on Alex Dillhoff</title>
    <link>http://localhost:1313/tags/reinforcement-learning/</link>
    <description>Recent content in Reinforcement Learning on Alex Dillhoff</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; {year}</copyright>
    <lastBuildDate>Sun, 12 Nov 2023 00:00:00 -0600</lastBuildDate>
    
	    <atom:link href="http://localhost:1313/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Policy Gradient Methods</title>
      <link>http://localhost:1313/notes/policy_gradient_methods/</link>
      <pubDate>Sun, 12 Nov 2023 00:00:00 -0600</pubDate>
      
      <guid>http://localhost:1313/notes/policy_gradient_methods/</guid>
      <description>&lt;p&gt;When we had full knowledge of the states, we could use &lt;a href=&#34;http://localhost:1313/notes/markov_decision_processes/&#34;&gt;Markov Decision Processes&lt;/a&gt; to find the optimal policy. When this assumption breaks down, we need to come up with our best approximation. This is not a far stretch from how we might handle new scenarios in our own lives. When we begin a new task, we are certainly not experts. We may learn from a teacher or set off to explore on our own. As we practice and churn out the seemingly endless variations of our endeavour, we begin to develop a sense of what works and what doesn&amp;rsquo;t. We may not be able to articulate the exact rules that we follow, but we can certainly tell when we are doing well or poorly.&lt;/p&gt;
&lt;p&gt;In lieu of a conscious agent with human intelligence, we can approximate the policy using a gradient-based approach. We will use the gradient of the expected reward with respect to the policy parameters to update the policy. Methods that use this approach are called &lt;strong&gt;&lt;strong&gt;policy gradient methods&lt;/strong&gt;&lt;/strong&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Markov Decision Processes</title>
      <link>http://localhost:1313/notes/markov_decision_processes/</link>
      <pubDate>Mon, 24 Jul 2023 00:00:00 -0500</pubDate>
      
      <guid>http://localhost:1313/notes/markov_decision_processes/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#key-terms&#34;&gt;Key Terms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#defining-goals&#34;&gt;Defining Goals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#policies-and-values&#34;&gt;Policies and Values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bellman-equations&#34;&gt;Bellman Equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimality&#34;&gt;Optimality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimizing-the-policy&#34;&gt;Optimizing the Policy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;key-terms&#34;&gt;Key Terms&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Agent&lt;/strong&gt;: The learner or decision maker.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt;: The world that the agent can interact with.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;State&lt;/strong&gt;: A representation of the agent and environment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: The agent can take an action in the environment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reward&lt;/strong&gt;: Given to the agent based on actions taken.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Maximize rewards earned over time.&lt;/p&gt;
&lt;p&gt;At time \(t\), the agent observes the state of the environment \(S_t \in \mathcal{S}\) and can select an action \(A_t \in \mathcal{A}(s)\), where \(\mathcal{A}(s)\) suggests that the available actions are dependent on the current state.
At time \(t + 1\), the agent receives a reward \(R_{t+1} \in \mathcal{R}\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-11-15_19-01-30_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;The agent-environment in a Markov decision process (Credit: Sutton &amp;amp;amp; Barto).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;The agent-environment in a Markov decision process (Credit: Sutton &amp;amp; Barto).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To improve its knowledge about an environment or increase its performance on a task, an agent must first be able to interpret or make sense of that environment in some way.
Second, there must be a well defined goal. For an agent playing Super Mario, for example, the goal would be to complete each level while maximizing the score.
Third, the agent must be able to interact with its environment by taking actions.
If the Mario-playing agent could not move Mario around, it would never be able to improve.
If the agent makes a decision which leads to Mario&amp;rsquo;s untimely demise, it would update its knowledge of the world so that it would tend towards a more favorable action.
These three requirements: sensations, actions, and goals, are encapsulated by &lt;strong&gt;Markov Decision Processes&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;A Markov decision process is defined by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(\mathcal{S}\) - a set of states,&lt;/li&gt;
&lt;li&gt;\(\mathcal{A}\) - a set of actions,&lt;/li&gt;
&lt;li&gt;\(\mathcal{R}\) - a set of rewards,&lt;/li&gt;
&lt;li&gt;\(P\) - the transition probability function to determine transition between states,&lt;/li&gt;
&lt;li&gt;\(\gamma\) - discount factor for future rewards.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At time \(t\), an agent in state \(S_t\) selects an action \(A_t\).
At \(t+1\), it receives a reward \(R_{t+1}\) based on that action.&lt;/p&gt;
&lt;p&gt;In a finite MDP, the states, actions, and rewards have a finite number of elements.
Random variables \(R_t\) and \(S_t\) have discrete probability distributions dependent on the preceding state and action.&lt;/p&gt;
&lt;p&gt;\[
p(s&amp;rsquo;, r|s, a) = P\{S_t = s&amp;rsquo;, R_t = r|S_{t-1} = s, A_{t-1} = a\}
\]&lt;/p&gt;
&lt;p&gt;If we want the state transition probabilities, we can sum over the above distribution:&lt;/p&gt;
&lt;p&gt;\[
p(s&amp;rsquo;|s, a) = P\{S_t = s&amp;rsquo;|S_{t-1} = s, A_{t-1}=a\} = \sum_{r\in\mathcal{R}}p(s&amp;rsquo;, r|s, a).
\]&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;reward function&lt;/strong&gt; \(r\) gives the expected &lt;em&gt;next&lt;/em&gt; reward given some state and action:&lt;/p&gt;
&lt;p&gt;\[
r(s, a) = \mathbb{E}[R_{t+1}|S_t=s, A_t=a] = \sum_{r}r \sum_{s&amp;rsquo;}p(s&amp;rsquo;, r|s, a).
\]&lt;/p&gt;
&lt;h2 id=&#34;defining-goals&#34;&gt;Defining Goals&lt;/h2&gt;
&lt;p&gt;In reinforcement learning, the goal is encoded in the form of a &lt;strong&gt;&lt;strong&gt;reward signal&lt;/strong&gt;&lt;/strong&gt;. The agent sets out to &lt;em&gt;maximize&lt;/em&gt; the total amount of reward it receives over an &lt;strong&gt;&lt;strong&gt;episode&lt;/strong&gt;&lt;/strong&gt;. An &lt;strong&gt;&lt;strong&gt;episode&lt;/strong&gt;&lt;/strong&gt; is defined dependent on the problem context and ends in a &lt;strong&gt;&lt;strong&gt;terminal state&lt;/strong&gt;&lt;/strong&gt;. It could be a round of game, a single play, or the result of moving a robot. Typically, the rewards come as a single scalar value at teach time step. This implies that an agent might take an action that results in a negative reward if it is optimal in the long run.&lt;/p&gt;
&lt;p&gt;Formally, the &lt;strong&gt;expected return&lt;/strong&gt; includes a &lt;strong&gt;discount factor&lt;/strong&gt; that allows us to control the trade-off between short-term and long-term rewards:&lt;/p&gt;
&lt;p&gt;\[
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1},
\]&lt;/p&gt;
&lt;p&gt;where \(0 \leq \gamma \leq 1\). This can be written in terms of the expected return itself as well:&lt;/p&gt;
&lt;p&gt;\[
G_t = R_{t+1} + \gamma G_{t+1}.
\]&lt;/p&gt;
&lt;h2 id=&#34;policies-and-values&#34;&gt;Policies and Values&lt;/h2&gt;
&lt;p&gt;Two important concepts that help our agent make decisions are the policy and value functions. A &lt;strong&gt;policy&lt;/strong&gt;, typically denoted by \(\pi\), maps states to actions. Such a function can be deterministic, \(\pi(s) = a\), or stochastic, \(\pi(a|s)\).&lt;/p&gt;
&lt;p&gt;The value of a particular state under a policy \(\pi\) is defined as&lt;/p&gt;
&lt;p&gt;\[
v_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s] = \mathbb{E}_{\pi}\Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}\Bigg|S_t=s\Bigg].
\]&lt;/p&gt;
&lt;p&gt;We also must define the value of taking an action \(a\) in state \(s\) following policy \(\pi\):&lt;/p&gt;
&lt;p&gt;\[
q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t|S_t=s, A_t=a] = \mathbb{E}_{\pi}\Bigg[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}\Bigg|S_t=s, A_t=a\Bigg].
\]&lt;/p&gt;
&lt;p&gt;This function defines the expected return of following a particular policy and starting in state \(s\).
Both the &lt;strong&gt;state-value&lt;/strong&gt; and &lt;strong&gt;action-value&lt;/strong&gt; functions can be updated as a result of the agent&amp;rsquo;s experience. How it is updated is method-dependent. Certain methods will also dictate how the policy itself can be updated.&lt;/p&gt;
&lt;h2 id=&#34;bellman-equations&#34;&gt;Bellman Equations&lt;/h2&gt;
&lt;p&gt;The recursive relationship between the value of a state and its future states can be represented using &lt;strong&gt;&lt;strong&gt;Bellman equations&lt;/strong&gt;&lt;/strong&gt;. In RL, we are interested in the equations for both the state-value and action-value. Given the diagram of an MDP, we can see that they are related to each other. To make the following equations easier to understand, it is important to remember the flow of a Markov decision process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;take an action,&lt;/li&gt;
&lt;li&gt;arrive at a state and sense the reward,&lt;/li&gt;
&lt;li&gt;consult the policy for the next action.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With that in mind, let&amp;rsquo;s look at the state-value function first. This function considers the expected value of starting in a state \(s\) and following policy \(\pi\). In other words, we must consider all possible actions and their future rewards.&lt;/p&gt;
&lt;p&gt;\begin{align*}
v_{\pi}(s) &amp;amp;= \mathbb{E}_{\pi}[G_t | S_t = s]\\
&amp;amp;= \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]\\
&amp;amp;= \sum_{a} \pi(a|s) \sum_{s&amp;rsquo;}\sum_{r}p(s&amp;rsquo;, r|s, a)\Big[r + \gamma \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]\Big]\\
&amp;amp;= \sum_{a} \pi(a|s) \sum_{s&amp;rsquo;, r}p(s&amp;rsquo;, r|s, a)\Big[r + \gamma v_{\pi}(s&amp;rsquo;)\Big]\\
&amp;amp;= \sum_{a} \pi(a | s)\big[r(s, a) + \gamma \sum_{s&amp;rsquo;}p(s&amp;rsquo;|s, a)v_{\pi}(s&amp;rsquo;)\big]\\
\end{align*}&lt;/p&gt;
&lt;p&gt;The first sum over actions considers &lt;em&gt;all possible actions&lt;/em&gt;. This is followed by a transition to possible states \(s&amp;rsquo;\) conditioned on taking each action multiplied by the expected value of being at the new state.&lt;/p&gt;
&lt;p&gt;The action-value function follows a similar derivation:&lt;/p&gt;
&lt;p&gt;\begin{align*}
q_{\pi}(s, a) &amp;amp;= \mathbb{E}_{\pi}[G_t|S_t = s, A_t = a]\\
&amp;amp;= \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a]\\
&amp;amp;= r(s, a) + \sum_{s&amp;rsquo;}p(s&amp;rsquo;|s, a) v_{\pi}(s&amp;rsquo;)
\end{align*}&lt;/p&gt;
&lt;p&gt;There is a very similar looking set of terms in the state-value function above, and we should expect that! If we want to evaluate the current state, we need to look ahead at the possible actions and their resulting rewards. Similarly, evaluating the current action requires us to look head at the value of future states.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s expand \(q_{\pi}(s,a)\) once more so that it is written in terms of itself.&lt;/p&gt;
&lt;p&gt;\begin{align*}
q_{\pi}(s, a) &amp;amp;= r(s, a) + \gamma \sum_{s&amp;rsquo;}p(s&amp;rsquo;|s, a) v_{\pi}(s&amp;rsquo;)\\
&amp;amp;= r(s, a) + \gamma \sum_{s&amp;rsquo;}p(s&amp;rsquo;|s, a) \sum_{a&amp;rsquo;} \pi(s&amp;rsquo;, a&amp;rsquo;) \big[r(s&amp;rsquo;, a&amp;rsquo;) + \gamma \sum_{s&amp;rsquo;&amp;rsquo;} p(s&amp;rsquo;&amp;rsquo;|s&amp;rsquo;, a&amp;rsquo;)v_{\pi}(s&amp;rsquo;&amp;rsquo;)]\\
&amp;amp;= r(s, a) + \gamma \sum_{s&amp;rsquo;}p(s&amp;rsquo;|s, a) \sum_{a&amp;rsquo;} \pi(s&amp;rsquo;, a&amp;rsquo;) q_{\pi}(s&amp;rsquo;, a&amp;rsquo;)
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;optimality&#34;&gt;Optimality&lt;/h2&gt;
&lt;p&gt;To solve a reinforcement learning problem, we are interested in finding the policy \(\pi_{*}\) whose expected return is greater than all other possible policies over all states.
An &lt;strong&gt;&lt;strong&gt;optimal policy&lt;/strong&gt;&lt;/strong&gt; will use an &lt;strong&gt;*optimal state-value function&lt;/strong&gt; and &lt;strong&gt;&lt;strong&gt;optimal action-value function&lt;/strong&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;\begin{align*}
v_{*}(s) &amp;amp;= \max_{\pi}v_{\pi}(s)\\
q_{*}(s, a) &amp;amp;= \max_{\pi}q_{\pi}(s, a).
\end{align*}&lt;/p&gt;
&lt;p&gt;The optimal state-value function would select the best possible action instead of summing over all possibley actions starting in state \(s\):&lt;/p&gt;
&lt;p&gt;\begin{align*}
v_{*}(s) &amp;amp;= \max_{a} q_{\pi_*}(s, a)\\
&amp;amp;= \max_{a}\big[r(s, a) + \gamma \sum_{s&amp;rsquo;} p(s&amp;rsquo;|s, a) v_{*}(s&amp;rsquo;)\big]
\end{align*}&lt;/p&gt;
&lt;p&gt;Similarly, the optimal action-value function selects the best possible action from the next state \(s&amp;rsquo;\):&lt;/p&gt;
&lt;p&gt;\[
q_{*}(s) = r(s, a) + \gamma \sum_{s&amp;rsquo;} p(s&amp;rsquo;|s, a) \max_{a} q_{*}(s&amp;rsquo;, a&amp;rsquo;).
\]&lt;/p&gt;
&lt;h2 id=&#34;optimizing-the-policy&#34;&gt;Optimizing the Policy&lt;/h2&gt;
&lt;p&gt;For smaller problems with reasonably small state and action spaces, we can use Dynamic Programming to compute the optimal policy. These methods quickly become intractable as the complexity of our problem increases. As is common in machine learning, we would resort to approximation methods for complex spaces.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;In fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ndash; Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Imagine if you had a set policy that dictated the actions you would take from work to home. In this example, assume the policy is not an optimal policy. One day, you decide to take a left at a particular intersection rather than going forward. After that, you follow your policy as described. If this decision ultimately resulted in you arriving home sooner, you would probably update your policy to always take that left. This intuition describes a result of the &lt;strong&gt;policy improvement theorem&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let \(\pi\) and \(\pi&amp;rsquo;\) be two deterministic policies where&lt;/p&gt;
&lt;p&gt;\[
q_{\pi}(s, \pi&amp;rsquo;(s)) \geq v_{\pi}(s),\quad \forall s.
\]&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
