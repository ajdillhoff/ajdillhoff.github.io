<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine on Alex Dillhoff</title>
    <link>http://localhost:1313/tags/machine/</link>
    <description>Recent content in Machine on Alex Dillhoff</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; {year}</copyright>
    <lastBuildDate>Thu, 11 Apr 2024 00:00:00 -0500</lastBuildDate>
    
	    <atom:link href="http://localhost:1313/tags/machine/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>The Language of LLMs</title>
      <link>http://localhost:1313/articles/the-language-of-llms/</link>
      <pubDate>Thu, 11 Apr 2024 00:00:00 -0500</pubDate>
      
      <guid>http://localhost:1313/articles/the-language-of-llms/</guid>
      <description>&lt;p&gt;Large Language Models like ChatGPT have rapidly become ubiquitous tools that enhance productivity, creativity, and even decision-making processes across various domains. Their ability to generate human-like text, comprehend complex instructions, and provide informative responses has captivated the imagination of users worldwide. This paragraph was generated by an LLM (and edited by me).&lt;/p&gt;
&lt;p&gt;This workshop is for those that are curious as to how these models &lt;strong&gt;interpret&lt;/strong&gt; the input. By the end of this hour, you will hopefully be able to answer the following questions, among others:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How do Large Language Models &lt;strong&gt;read&lt;/strong&gt; and process text?&lt;/li&gt;
&lt;li&gt;Why are LLMs good at complex tasks, but seem to perform poorly on seemingly simple tasks like spelling or arithmetic?&lt;/li&gt;
&lt;li&gt;How does an LLM understand what it is processing?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;agenda&#34;&gt;Agenda&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Tokenization&lt;/li&gt;
&lt;li&gt;Unicode byte encodings&lt;/li&gt;
&lt;li&gt;Byte Pair Encoding (BPE)&lt;/li&gt;
&lt;li&gt;Embeddings&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;tokenization&#34;&gt;Tokenization&lt;/h2&gt;
&lt;p&gt;Tokenization is the process of transforming a sequence of characters into a sequence of tokens. A token is a unit of text that we treat as a single entity. For example, in English, a token could be a word, a sentence, or a paragraph. In programming languages, a token could be a variable name, a keyword, or a string.&lt;/p&gt;
&lt;p&gt;Consider the input prompt below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Why does my code crash with a segmentation fault?
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;int main() {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    int *arr = NULL;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    scanf(&amp;#34;%d&amp;#34;, arr);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    return 0;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;unicode-byte-encodings&#34;&gt;Unicode Byte Encodings&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;UTF-8 is a byte encoding which results in very long sequences for our input.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;byte-pair-encoding--bpe&#34;&gt;Byte Pair Encoding (BPE)&lt;/h2&gt;
&lt;h2 id=&#34;embeddings&#34;&gt;Embeddings&lt;/h2&gt;
</description>
    </item>
    
  </channel>
</rss>
