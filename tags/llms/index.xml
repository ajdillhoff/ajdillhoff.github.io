<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Llms on Alex Dillhoff</title>
    <link>https://ajdillhoff.github.io/tags/llms/</link>
    <description>Recent content in Llms on Alex Dillhoff</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Thu, 10 Oct 2024 00:00:00 -0500</lastBuildDate>
    <atom:link href="https://ajdillhoff.github.io/tags/llms/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Using RAG to Talk to Your Data</title>
      <link>https://ajdillhoff.github.io/blog/using-rag-to-talk-to-your-data/</link>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/blog/using-rag-to-talk-to-your-data/</guid>
      <description>How can LLMs provide results that are not only factual, but based on your own private data? This article accompanies a workshop given at HackUTA 6 on October 12, 2024.</description>
    </item>
    <item>
      <title>Low Rank Adaptation</title>
      <link>https://ajdillhoff.github.io/notes/low_rank_adaptation/</link>
      <pubDate>Sat, 08 Jun 2024 13:03:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/notes/low_rank_adaptation/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#key-concepts&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Key Concepts&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;key-concepts&#34;&gt;Key Concepts&lt;/h2&gt;&#xA;&lt;h3 id=&#34;traditional-fine-tuning&#34;&gt;Traditional Fine-Tuning&lt;/h3&gt;&#xA;&lt;p&gt;Fine-tuning a model for a specific task can be expensive if the entire weight matrix is updated. LLMs range from billions to trillions of parameters, making fine-tuning infeasible for many applications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Language of LLMs</title>
      <link>https://ajdillhoff.github.io/blog/the-language-of-llms/</link>
      <pubDate>Thu, 11 Apr 2024 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/blog/the-language-of-llms/</guid>
      <description>How do LLMs read and process the high dimensional landscape of text efficiently? Presented as a workshop at UTA&amp;rsquo;s Datathon on April 13, 2024.</description>
    </item>
    <item>
      <title>Transformers</title>
      <link>https://ajdillhoff.github.io/notes/transformers/</link>
      <pubDate>Sun, 06 Nov 2022 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/notes/transformers/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#definition&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Definition&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#attention&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Attention&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#key-value-store&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Key-value Store&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#scaled-dot-product-attention&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Scaled Dot Product Attention&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#multi-head-attention&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Multi-Head Attention&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#encoder-decoder-architecture&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Encoder-Decoder Architecture&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#encoder&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Encoder&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#decoder&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Decoder&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#usage&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Usage&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#resources&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Resources&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;The story of Transformers begins with &amp;ldquo;Attention Is All You Need&amp;rdquo; (Vaswani et al., n.d.). In this seminal work, the authors describe the current landscape of sequential models, their shortcomings, and the novel ideas that result in their successful application.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
