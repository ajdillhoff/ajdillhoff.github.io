<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Llms on Alex Dillhoff</title>
    <link>http://localhost:1313/tags/llms/</link>
    <description>Recent content in Llms on Alex Dillhoff</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; {year}</copyright>
    <lastBuildDate>Thu, 10 Oct 2024 00:00:00 -0500</lastBuildDate>
    
	    <atom:link href="http://localhost:1313/tags/llms/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Using RAG to Talk to Your Data</title>
      <link>http://localhost:1313/articles/using-rag-to-talk-to-your-data/</link>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0500</pubDate>
      
      <guid>http://localhost:1313/articles/using-rag-to-talk-to-your-data/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#foundations-of-nlp&#34;&gt;Foundations of NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#architecture-of-language-models&#34;&gt;Architecture of Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#advanced-concepts-in-llms&#34;&gt;Advanced Concepts in LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#retrieval-augmented-generation&#34;&gt;Retrieval Augmented Generation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;Large Language Models (LLMs) are a powerful tool for generating text. They are used to help us study, code, come up with new recipes, and more. However, they are prone to hallucinations; they can generate text that is not factual or relevant to the prompt. Retrieval Augmented Generation (RAG) is a technique that can address this issue by retrieving factual information from external databases. In this article, I will review the main concepts of LLMs and introduce RAG as a way to generate responses based on your own private data.&lt;/p&gt;
&lt;h2 id=&#34;foundations-of-nlp&#34;&gt;Foundations of NLP&lt;/h2&gt;
&lt;p&gt;How do machines &lt;em&gt;understand&lt;/em&gt; text? They surely do not understand language in the same way that we do, but they seem to do a pretty convincing job at generating useful output. Focusing on this question will be the theme of this brief overview of Natural Language Processing (NLP), specifically related to Large Language Models.&lt;/p&gt;
&lt;h3 id=&#34;tokenization&#34;&gt;Tokenization&lt;/h3&gt;
&lt;p&gt;Tokenization is the process of transforming input text into smaller chunks called &lt;strong&gt;tokens&lt;/strong&gt;. These tokens can be characters, words, phrases, or even whole sentences. The tokens themselves are then represented as numbers in a dictionary. This allows models to move back and forth between the actual text and the tokens they read and generate.&lt;/p&gt;
&lt;h4 id=&#34;types-of-tokens&#34;&gt;Types of Tokens&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Word Tokenization:&lt;/strong&gt; Divides text into individual words. It is commonly used for tasks like text classification or sentiment analysis.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sentence Tokenization:&lt;/strong&gt; Splits text into sentences, useful for analyzing document structure.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Subword Tokenization:&lt;/strong&gt; Breaks words into smaller units like prefixes or suffixes, which is helpful for handling out-of-vocabulary words.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Character Tokenization:&lt;/strong&gt; Segments text into individual characters, beneficial for languages without clear word boundaries.&lt;/li&gt;
&lt;/ul&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-10-10_16-07-49_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;An example of tokenization using the tokenizer used in gpt-4o.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;An example of tokenization using the tokenizer used in gpt-4o.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Tokenizers are trained on a large corpus of data. Frequent subwords or character pairs are identified and merged to create unique tokens. The result is a vocabulary that is tailored for that specific corpus of data, implying that the selection of input documents for training is crucial. For example, a tokenizer trained on legal documents may not be the most efficient choice for a model that works primarily with medical documents.&lt;/p&gt;
&lt;p&gt;The choice of tokenizer is dependent on more than just the task. There are also language considerations. For a live demonstration of tokenization, check out &lt;a href=&#34;https://tiktokenizer.vercel.app/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;tiktokenizer&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;embeddings&#34;&gt;Embeddings&lt;/h3&gt;
&lt;p&gt;With a tokenizer at hand, the next step is to analyze the tokens and learn from them. Specifically, relationships between the tokens are learned through &lt;strong&gt;embeddings&lt;/strong&gt;. Embeddings are numerical representations of tokens that capture the semantic meaning of the text. They are essential for training models to understand and generate text.&lt;/p&gt;
&lt;p&gt;Embeddings are learned during the training process of a model, where the model aims to capture the semantic relationships and contextual nuances of tokens. For those familiar with gradient descent, embeddings are optimized to minimize the loss function of the model. This loss function effectively drives the model to learn the best possible embeddings for the task at hand.&lt;/p&gt;
&lt;p&gt;Once the embeddings are learned, the embedding layer can then be used to transform our input into a high-dimensional vector space in which the model can operate. This further allows the model to make predictions based on the relationships between the tokens.&lt;/p&gt;
&lt;h2 id=&#34;architecture-of-language-models&#34;&gt;Architecture of Language Models&lt;/h2&gt;
&lt;h3 id=&#34;encoders-and-decoders&#34;&gt;Encoders and Decoders&lt;/h3&gt;
&lt;p&gt;Language models consist of an encoder, a decoder, or sometimes both. The exact architecture is dependent on the task. Understand the roles that each one plays makes it easier to determine which model is best suited for a given task.&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;encoder&lt;/strong&gt; takes a sequence of input vectors based on the tokens and transforms them into fixed-length representations. Their primary goal is to extract relevant information from the input text. This information is then used in some downstream task such as classification or translation.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-10-10_17-10-09_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A transformer-based encoder (&amp;lt;a href=&amp;#34;#citeproc_bib_item_5&amp;#34;&amp;gt;Vaswani et al. 2017&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A transformer-based encoder (&lt;a href=&#34;#citeproc_bib_item_5&#34;&gt;Vaswani et al. 2017&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;A &lt;strong&gt;decoder&lt;/strong&gt; takes the fixed length representation and generates an output sequence. Decoders are commonly used in tasks like machine translation or text generation. They take the information extracted by the encoder and use it to generate the desired output.&lt;/p&gt;
&lt;p&gt;The table below summarizes the roles of encoders and decoders in different tasks:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Task&lt;/th&gt;
          &lt;th&gt;Encoder Role&lt;/th&gt;
          &lt;th&gt;Decoder Role&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Text Classification&lt;/td&gt;
          &lt;td&gt;Extracts relevant information from the input text.&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Machine Translation&lt;/td&gt;
          &lt;td&gt;Transforms the input text into a fixed-length representation.&lt;/td&gt;
          &lt;td&gt;Generates the output sequence based on the fixed-length representation.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Text Generation&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Generates the output sequence based on the input text.&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;transformers&#34;&gt;Transformers&lt;/h3&gt;
&lt;p&gt;Transformers are a type of neural network architecture that has revolutionized NLP tasks. They are based on the concept of attention mechanisms, which allow the model to focus on different parts of the input text when making predictions. This attention mechanism is what enables transformers to capture long-range dependencies in text, making them particularly effective for tasks like machine translation and text generation (&lt;a href=&#34;#citeproc_bib_item_5&#34;&gt;Vaswani et al. 2017&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Attention mechanisms produce relationships between sequences. When we look at an image of a dog running in a field with the intent of figuring out what the dog is doing in the picture, we pay greater attention to the dog and look at contextual cues in the image that might inform us of their task. This is an automatic process which allows us to efficiently process information.&lt;/p&gt;
&lt;p&gt;Attention mechanisms follow the same concept. Consider a machine translation task in which a sentence in English is translated to French. Certain words between the input and output will have stronger correlations than others (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Bahdanau, Cho, and Bengio 2016&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;training-large-language-models&#34;&gt;Training Large Language Models&lt;/h3&gt;
&lt;p&gt;Since the main focus of this workshop is on Retrieval Augmented Generation, we will focus specifically on decoder-only LLMs like ChatGPT and Claude. These models are trained using a self-supervised learning approach, where the model learns to predict the next token in a sequence based on the previous tokens. This process is repeated over a large corpus of text data, allowing the model to learn the underlying patterns and relationships in the data (&lt;a href=&#34;#citeproc_bib_item_4&#34;&gt;Radford et al. 2018&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Training in this way allows the model to train on a virtually unlimited amount of data; the supervisory signals do not require human annotation. The more data the model is trained on, the better it can learn the underlying patterns in the data. This is why LLMs are typically trained on massive datasets like the Common Crawl or Wikipedia.&lt;/p&gt;
&lt;h2 id=&#34;advanced-concepts-in-llms&#34;&gt;Advanced Concepts in LLMs&lt;/h2&gt;
&lt;p&gt;After an LLM is trained, it can be used for many downstream tasks including chat, translation, summarization, and more. However, to make the most of these models, it is essential to understand some advanced concepts in LLMs.&lt;/p&gt;
&lt;h3 id=&#34;context-length&#34;&gt;Context Length&lt;/h3&gt;
&lt;p&gt;The context length of a model refers to the number of tokens that the model can consider when making predictions. This is an important factor in determining the performance of the model on different tasks. A model with a longer context length can capture more information about the input text, allowing it to make more accurate predictions. However, longer context lengths also require more computational resources, making them slower and more expensive to train.&lt;/p&gt;
&lt;h3 id=&#34;fine-tuning&#34;&gt;Fine-tuning&lt;/h3&gt;
&lt;p&gt;Fine-tuning is the process of taking a pre-trained model and training it on a smaller dataset for a specific task. This allows the model to adapt to the specific patterns in the data and improve its performance on the task. Fine-tuning is essential for achieving state-of-the-art performance on many NLP tasks. Consider a law firm that has a large corpus of legal documents. These documents will surely contain legal jargon and specific patterns that are not present in the general text data used to train the model. Fine-tuning the pre-trained model results in a model that is better suited for the legal domain while still benefiting from the general knowledge learned during pre-training.&lt;/p&gt;
&lt;p&gt;This is not always a direct upgrade from the original model. Depending on the quality of the dataset, the resulting model may perform worse than it did originally. There are also risks related to data privacy. If the dataset contains confidential information, the LLM may inadvertently memorize this information during fine-tuning.&lt;/p&gt;
&lt;p&gt;While fine-tuning may increase the performance on general tasks related to the new domain, it still does not solve the issue of context-specific data. This is where retrieval augmented generation comes into play.&lt;/p&gt;
&lt;h3 id=&#34;chat-models&#34;&gt;Chat Models&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-10-11_14-06-35_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;From Andrej Karpathy&amp;#39;s [State of GPT](&amp;lt;https://www.youtube.com/watch?v=bZQun8Y4L2A&amp;gt;) keynote.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;From Andrej Karpathy&amp;rsquo;s &lt;a href=&#34;https://www.youtube.com/watch?v=bZQun8Y4L2A&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;State of GPT&lt;/a&gt; keynote.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To create a chat model, a pre-trained LLM must be meticulously fine-tuned to behave as a chatbot. This involes three additional steps on top of the original model. This is summarized in the figure above.&lt;/p&gt;
&lt;h4 id=&#34;supervised-fine-tuning&#34;&gt;Supervised Fine-tuning&lt;/h4&gt;
&lt;p&gt;The first step is supervised fine-tuning. This involves created a specialized dataset that demonstrates the desired behavior of the chatbot. These samples must be vetted by human annotators to ensure quality. This process is not necessarily training the model to give the most accurate responses, but rather to give the most human-like responses.&lt;/p&gt;
&lt;h4 id=&#34;reward-modeling&#34;&gt;Reward Modeling&lt;/h4&gt;
&lt;p&gt;To optimize the quality of the model&amp;rsquo;s reponses, we need some way to tell the model what a quality response looks like. In this step, another specialized dataset is created based on a prompt and a number of different responses. A human annotator then assigns a ranking to these responses. This ranking is used as a reward signal for the model to optimize.&lt;/p&gt;
&lt;p&gt;The reward model itself is then trained to predict the ranking of the responses based on the human annotations. Once trained, the reward model is used to provide feedback to the chat model during training.&lt;/p&gt;
&lt;h4 id=&#34;reinforcement-learning&#34;&gt;Reinforcement Learning&lt;/h4&gt;
&lt;p&gt;With a trained reward model, the final stage of training is performed. Given an SFT model and the reward model, reinforcement learning is used to optimize the chat model. A prompt is given to the SFT model, which generates a response. This response is then ranked by the reward model, and the chat model is updated based on the reward signal.&lt;/p&gt;
&lt;h3 id=&#34;emergent-capabilities&#34;&gt;Emergent Capabilities&lt;/h3&gt;
&lt;p&gt;As it turns out, a model that was pre-trained on a large corpus of text data can be used for more than just generating text. It can also perform downstream tasks like question-answering without explicitly being trained on that task. This is known as &lt;strong&gt;zero-shot&lt;/strong&gt;, or &lt;strong&gt;few-shot&lt;/strong&gt;, learning (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Brown et al. 2020&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Given an input prompt, a pre-trained LLM will generate text that is relevant to the text contained within the prompt itself. If the prompt contains specialized context for a specific task, the generated text will also be relevant to that task. For example, provided an example of the task within the context before adding a novel prompt, the model will generate text that is relevant to the task.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example from (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Brown et al. 2020&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-nil&#34; data-lang=&#34;nil&#34;&gt;Alice was friends with Bob. Alice went to visit her friend ______. -&amp;gt; Bob
George bought some baseball equipment, a ball, a glove, and a _____. -&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;retrieval-augmented-generation&#34;&gt;Retrieval Augmented Generation&lt;/h2&gt;
&lt;p&gt;If you have used an LLM such as ChatGPT, Claude, or Gemini, you will have noticed that the responses are not always accurate. This is because the model is generating responses based on the input prompt alone. It does not have access to external information that could help it generate more accurate responses. Recent iterations of these popular models are now incorporating retrieval mechanisms to address this issue.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Retrieval Augmented Generation&lt;/strong&gt; (RAG) is a technique that retrieves relevent document chunks from external databases that are relevant to the original prompt. These chunks are compared to the context of the prompt primarily through semantic similarity, but other methods can be used as well. The model then generates a response based on the retrieved information. The benefit to this approach is that the model can generate responses that are not only factual, but based on your own private data without requiring any fine-tuning processes.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-10-11_14-40-07_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Overview of RAG (&amp;lt;a href=&amp;#34;#citeproc_bib_item_3&amp;#34;&amp;gt;Gao et al. 2024&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Overview of RAG (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Gao et al. 2024&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The basic process is conceptually simple:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Given a prompt, relevant document chunks are &lt;strong&gt;retrieved&lt;/strong&gt; from a database.&lt;/li&gt;
&lt;li&gt;The retrieved context is used to &lt;strong&gt;augment&lt;/strong&gt; the original prompt.&lt;/li&gt;
&lt;li&gt;The model &lt;strong&gt;generates&lt;/strong&gt; a response based on the augmented prompt.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;RAG is an open research area with many potential applications. It is particularly useful in situations where the model needs to generate responses based on specific information that is not present in the training data. For example, a legal chatbot could use RAG to retrieve relevant legal documents to generate responses to legal questions. It can certainly be tricky to implement, but the benefits are clear.&lt;/p&gt;
&lt;p&gt;The rest of this article will take a more hands-on approach. See the &lt;a href=&#34;https://github.com/ajdillhoff/langchain-llama3.2-rag&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;accompanying repository&lt;/a&gt; for a practical demonstration of how to use RAG to talk to your data.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2016. “Neural Machine Translation by Jointly Learning to Align and Translate.” &lt;i&gt;Arxiv:1409.0473 [Cs, Stat]&lt;/i&gt;, May. &lt;a href=&#34;http://arxiv.org/abs/1409.0473&#34;&gt;http://arxiv.org/abs/1409.0473&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” arXiv. &lt;a href=&#34;https://doi.org/10.48550/arXiv.2005.14165&#34;&gt;https://doi.org/10.48550/arXiv.2005.14165&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_3&#34;&gt;&lt;/a&gt;Gao, Yunfan, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. “Retrieval-Augmented Generation for Large Language Models: A Survey.” arXiv. &lt;a href=&#34;http://arxiv.org/abs/2312.10997&#34;&gt;http://arxiv.org/abs/2312.10997&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_4&#34;&gt;&lt;/a&gt;Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. “Improving Language Understanding by Generative Pre-Training,” 12.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_5&#34;&gt;&lt;/a&gt;Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need,” 11.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Low Rank Adaptation</title>
      <link>http://localhost:1313/notes/low_rank_adaptation/</link>
      <pubDate>Sat, 08 Jun 2024 13:03:00 -0500</pubDate>
      
      <guid>http://localhost:1313/notes/low_rank_adaptation/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#key-concepts&#34;&gt;Key Concepts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;key-concepts&#34;&gt;Key Concepts&lt;/h2&gt;
&lt;h3 id=&#34;traditional-fine-tuning&#34;&gt;Traditional Fine-Tuning&lt;/h3&gt;
&lt;p&gt;Fine-tuning a model for a specific task can be expensive if the entire weight matrix is updated. LLMs range from billions to trillions of parameters, making fine-tuning infeasible for many applications.&lt;/p&gt;
&lt;h3 id=&#34;low-rank-decomposition&#34;&gt;Low Rank Decomposition&lt;/h3&gt;
&lt;p&gt;Low Rank Adaptation (LoRA) is a method of decomposing the weight update matrix \(\Delta W\) into smaller matrices \(A\) and \(B\) such that \(\Delta W \approx AB\). The rank \(r\) of the decomposition is a hyperparameter that can be tuned to balance performance and computational cost (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hu et al. 2021&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;efficiency&#34;&gt;Efficiency&lt;/h3&gt;
&lt;p&gt;LoRA is more efficient than fine-tuning because the rank \(r\) is much smaller than the number of parameters in the model. This allows for faster training and inference times. If \(W \in \mathbb{R}^{d \times d}\), then \(A \in \mathbb{R}^{d \times r}\) and \(B \in \mathbb{R}^{r \times d}\), so the number of parameters in the decomposition is \(2dr\). Compare this to the \(d^2\) parameters in the original weight matrix.&lt;/p&gt;
&lt;h3 id=&#34;implementation&#34;&gt;Implementation&lt;/h3&gt;
&lt;p&gt;When fine-tuning with LoRA, the original weights are frozen. This has a significant impact on the performance of the model since the gradients do not need to be stored for the original weights. Only the gradients for the decomposition matrices \(A\) and \(B\) need to be stored.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. “LoRA: Low-Rank Adaptation of Large Language Models.” arXiv. &lt;a href=&#34;https://doi.org/10.48550/arXiv.2106.09685&#34;&gt;https://doi.org/10.48550/arXiv.2106.09685&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Language of LLMs</title>
      <link>http://localhost:1313/articles/the-language-of-llms/</link>
      <pubDate>Thu, 11 Apr 2024 00:00:00 -0500</pubDate>
      
      <guid>http://localhost:1313/articles/the-language-of-llms/</guid>
      <description>&lt;p&gt;The accompanying Colab notebook is &lt;a href=&#34;https://colab.research.google.com/drive/1Ch5wCSkYxXU6AAntrWH731Qk4-oE2c1P?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;available here.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Large Language Models like ChatGPT have rapidly become ubiquitous tools that enhance productivity, creativity, and even decision-making processes across various domains. Their ability to generate human-like text, comprehend complex instructions, and provide informative responses has captivated the imagination of users worldwide. This paragraph was generated by an LLM (and edited by me).&lt;/p&gt;
&lt;p&gt;This workshop is for those that are curious as to how these models &lt;strong&gt;interpret&lt;/strong&gt; the input. By the end of this hour, you will hopefully be able to answer the following questions, among others:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How do Large Language Models &lt;strong&gt;read&lt;/strong&gt; and process text?&lt;/li&gt;
&lt;li&gt;Why are LLMs good at complex tasks, but seem to perform poorly on seemingly simple tasks like spelling or arithmetic?&lt;/li&gt;
&lt;li&gt;How does an LLM understand what it is processing?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;agenda&#34;&gt;Agenda&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Tokenization&lt;/li&gt;
&lt;li&gt;Unicode byte encodings&lt;/li&gt;
&lt;li&gt;Byte Pair Encoding (BPE)&lt;/li&gt;
&lt;li&gt;Embeddings&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;tokenization&#34;&gt;Tokenization&lt;/h2&gt;
&lt;p&gt;Tokenization is the process of transforming a sequence of characters into a sequence of tokens. A token is a unit of text that we treat as a single entity. For example, in English, a token could be a word, a sentence, or a paragraph. In programming languages, a token could be a variable name, a keyword, or a string.&lt;/p&gt;
&lt;p&gt;Before we get started, let&amp;rsquo;s check out a &lt;a href=&#34;https://tiktokenizer.vercel.app&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;live demonstration of tokenization.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Consider the input prompt below. &lt;em&gt;It isn&amp;rsquo;t likely that you would have mixed emoji and code in a single text file, but it serves as a good example for tokenization.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Why does my code 💥 with a segmentation fault?
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;int main() {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    int *arr = NULL;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    scanf(&amp;#34;%d&amp;#34;, arr);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    return 0;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;At the most basic level, how is this text represented in a computer?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These characters are represented as encodings such as ASCII or Unicode. For the purposes of the rest of this article, we will assume the input is represented using Unicode.&lt;/p&gt;
&lt;h3 id=&#34;unicode-byte-encodings&#34;&gt;Unicode Byte Encodings&lt;/h3&gt;
&lt;p&gt;If we were to print out the unicode values of the prompt above, we would get the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[10, 87, 104, 121, 32, 100, 111, 101, 115, 32, 109, 121, 32, 99, 111, 100, 101, 32, 128165, ...]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Most of the values displayed in the previous cell are the same for ASCII. The emoji value has a very large number and can easily be spotted in the list.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Is that it? Is this how the input is fed into the model?&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This encoding is done at the character-level. What other types of encodings are there?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Character encoding&lt;/li&gt;
&lt;li&gt;Word encoding&lt;/li&gt;
&lt;li&gt;Sub-word encoding&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;What is the difference between them? Why would we pick one over another?&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;character-encoding&#34;&gt;Character Encoding&lt;/h3&gt;
&lt;p&gt;Character encoding converts each character into a unique integer. This is by far the simplest form of tokenization and has the benefit of a compact vocabulary. However, it is not able to effectively compress any common subsequences in the input. This leads to much larger sequences and longer training times.&lt;/p&gt;
&lt;p&gt;The biggest downside to this approach is that the individual characters are not very informative on a semantic level. For example, the word &amp;ldquo;cat&amp;rdquo; would be represented as three separate tokens, &amp;lsquo;c&amp;rsquo;, &amp;lsquo;a&amp;rsquo;, and &amp;rsquo;t&amp;rsquo;. If someone were to present you a single letter without context, you probably would likely not be able to understand the point of the message.&lt;/p&gt;
&lt;h3 id=&#34;word-encoding&#34;&gt;Word Encoding&lt;/h3&gt;
&lt;p&gt;Word encoding is a step up from character encoding. This encoding directly captures the semantic meaning of words and is a fine choice for text classification and sentiment analysis. The sequences formed are much shorter since every word can be converted into a unique token.&lt;/p&gt;
&lt;p&gt;The vocabulary size is very large since individual tokens cannot be broken down or recombined in new contexts. It also struggles with out-of-vocabulary words, since there are no base tokens to build upon.&lt;/p&gt;
&lt;h3 id=&#34;sub-word-encoding&#34;&gt;Sub-word Encoding&lt;/h3&gt;
&lt;p&gt;Sub-word encoding is a compromise between character and word encoding. It is able to capture the semantic meaning of words and can be broken down into smaller tokens. This allows for the model to generalize better to unseen words and phrases. Most large language models use sub-word encoding.&lt;/p&gt;
&lt;h2 id=&#34;byte-pair-encoding--bpe&#34;&gt;Byte Pair Encoding (BPE)&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Byte_pair_encoding&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Byte Pair Encoding&lt;/a&gt; is a sub-word encoding technique that was originally designed for data compression. It is a simple algorithm that iteratively merges the most frequent pair of bytes in a sequence. This process is repeated until a predefined vocabulary size is reached.&lt;/p&gt;
&lt;p&gt;The algorithm is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize the vocabulary with all the characters in the input.&lt;/li&gt;
&lt;li&gt;Count the frequency of all pairs of characters in the vocabulary.&lt;/li&gt;
&lt;li&gt;Merge the most frequent pair of characters.&lt;/li&gt;
&lt;li&gt;Update the vocabulary with the merged pair.&lt;/li&gt;
&lt;li&gt;Repeat steps 2-4 until the vocabulary size reaches a predefined limit.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;embeddings&#34;&gt;Embeddings&lt;/h2&gt;
&lt;p&gt;A word embedding is a learned representation of text in which semantically similar words are mapped to nearby points in the embedding space. Since they are represented as vectors, all vector operations can be applied to them. This allows for the model to learn relationships between words and phrases, quantify their similarities and differences, and encode higher-level context information.&lt;/p&gt;
&lt;p&gt;Embeddings can be learned independently or jointly with the model. For example, the Word2Vec model learns embeddings using an unsupervised approach. It predicts the context of a word given its surrounding words. The embeddings are then used as input to a downstream task (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Mikolov et al. 2013&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;LLMs typically train embeddings jointly with the model. This allows them to learn embeddings for sentences, paragraphs, or even whole documents.&lt;/p&gt;
&lt;h3 id=&#34;creating-an-embedding-layer&#34;&gt;Creating an embedding layer&lt;/h3&gt;
&lt;p&gt;We can use libraries such as PyTorch to create a learnable embedding layer. The code below creates an embedding layer that converts each individual token into a `1024` dimensional embedded layer.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch.nn &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; nn
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;token_embedding &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Embedding(vocab_size, &lt;span style=&#34;color:#ae81ff&#34;&gt;1024&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;prompt_embedded &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; token_embedding(torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;LongTensor(encode(prompt)))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(prompt_embedded&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;training-the-embeddings&#34;&gt;Training the embeddings&lt;/h3&gt;
&lt;p&gt;Our corpus of a single C file is far too small to learn anything meaningful. Learning an embedding space requires a lot data and compute power. We can instead look at pre-trained embeddings. Huggingface has a large collection of pre-trained models that can be used for a variety of tasks. The accompanying notebook uses embeddings from &lt;code&gt;SentenceTransformer&lt;/code&gt; to demonstrate how embeddings can be used in practice.&lt;/p&gt;
&lt;h2 id=&#34;sentence-embeddings&#34;&gt;Sentence Embeddings&lt;/h2&gt;
&lt;p&gt;To demonstrate the power of embeddings, we will close out the workshop by reviewing sentence embeddings. BERT (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Devlin et al. 2019&lt;/a&gt;) and RoBERTa (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Liu et al. 2019&lt;/a&gt;) are LLMs that perform tasks such as semantic textual similarity. They both require that whole sentences be input, resulting in a very expensive computation.&lt;/p&gt;
&lt;p&gt;Sentence-BERT proposed an architecture that would embed these into meaningful embeddings that could be easily compared with vector operations (&lt;a href=&#34;#citeproc_bib_item_4&#34;&gt;Reimers and Gurevych 2019&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In the cells below, we will use Huggingface (huggingface.co) to download and use a pre-trained sentence transformer. This particular one was trained on &lt;strong&gt;&lt;strong&gt;1,170,060,424&lt;/strong&gt;&lt;/strong&gt; sentence pairs.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv. &lt;a href=&#34;https://doi.org/10.48550/arXiv.1810.04805&#34;&gt;https://doi.org/10.48550/arXiv.1810.04805&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” arXiv. &lt;a href=&#34;https://doi.org/10.48550/arXiv.1907.11692&#34;&gt;https://doi.org/10.48550/arXiv.1907.11692&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_3&#34;&gt;&lt;/a&gt;Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” arXiv. &lt;a href=&#34;http://arxiv.org/abs/1301.3781&#34;&gt;http://arxiv.org/abs/1301.3781&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_4&#34;&gt;&lt;/a&gt;Reimers, Nils, and Iryna Gurevych. 2019. “Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks.” arXiv. &lt;a href=&#34;https://doi.org/10.48550/arXiv.1908.10084&#34;&gt;https://doi.org/10.48550/arXiv.1908.10084&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Transformers</title>
      <link>http://localhost:1313/notes/transformers/</link>
      <pubDate>Sun, 06 Nov 2022 00:00:00 -0500</pubDate>
      
      <guid>http://localhost:1313/notes/transformers/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#attention&#34;&gt;Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#key-value-store&#34;&gt;Key-value Store&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scaled-dot-product-attention&#34;&gt;Scaled Dot Product Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multi-head-attention&#34;&gt;Multi-Head Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#encoder-decoder-architecture&#34;&gt;Encoder-Decoder Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#encoder&#34;&gt;Encoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#decoder&#34;&gt;Decoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The story of Transformers begins with &amp;ldquo;Attention Is All You Need&amp;rdquo; (Vaswani et al., n.d.). In this seminal work, the authors describe the current landscape of sequential models, their shortcomings, and the novel ideas that result in their successful application.&lt;/p&gt;
&lt;p&gt;Their first point highlights a fundamental flaw in how &lt;a href=&#34;http://localhost:1313/notes/recurrent_neural_networks/&#34;&gt;Recurrent Neural Networks&lt;/a&gt; process sequential data: their output is a function of the previous time step. Given the hindsight of 2022, where large language models are crossing the &lt;a href=&#34;https://arxiv.org/pdf/2101.03961.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;trillion parameter milestone&lt;/a&gt;, a model requiring recurrent computation dependent on previous time steps without the possibility of parallelization would be virtually intractable.&lt;/p&gt;
&lt;p&gt;The second observation refers to attention mechanisms, a useful addition to sequential models that enable long-range dependencies focused on specific contextual information. When added to translation models, attention allows the model to focus on particular words (Bahdanau, Cho, and Bengio 2016).&lt;/p&gt;
&lt;p&gt;The Transformer architecture considers the entire sequence using only attention mechanisms.
There are no recurrence computations in the model, allowing for higher efficiency through parallelization.&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;The original architecture consists of an encoder and decoder, each containing one or more attention mechanisms.
Not every type of model uses both encoders and decoders. This is discussed later [TODO: discuss model types].
Before diving into the architecture itself, it is important to understand what an attention mechanism is and how it functions.&lt;/p&gt;
&lt;h2 id=&#34;attention&#34;&gt;Attention&lt;/h2&gt;
&lt;p&gt;Attention mechanisms produce relationships between sequences. When we look at an image of a dog running in a field with the intent of figuring out what the dog is doing in the picture, we pay greater attention to the dog and look at contextual cues in the image that might inform us of their task. This is an automatic process which allows us to efficiently process information.&lt;/p&gt;
&lt;p&gt;Attention mechanisms follow the same concept. Consider a machine translation task in which a sentence in English is translated to French. Certain words between the input and output will have stronger correlations than others.&lt;/p&gt;
&lt;h3 id=&#34;soft-attention&#34;&gt;Soft Attention&lt;/h3&gt;
&lt;p&gt;Use of context vector that is dependent on a sequence of annotations. These contain information about the input sequence with a focus on the parts surrounding the $i$-th word.&lt;/p&gt;
&lt;p&gt;\[
c_i = \sum_{j=1}^{T_x}\alpha_{ij}h_j
\]&lt;/p&gt;
&lt;p&gt;What is \(\alpha_{ij}\) and how is it computed? This comes from an alignment model which assigns a score reflecting how well the inputs around position \(j\) and output at position \(i\) match, given by&lt;/p&gt;
&lt;p&gt;\[
e_{ij} = a(s_{i-1}, h_j),
\]&lt;/p&gt;
&lt;p&gt;where \(a\) is a feed-forward neural network and \(h_j\) is an annotation produced by the hidden layer of a BRNN.
These scores are passed to the softmax function so that \(\alpha_{ij}\) represents the weight of annotation \(h_j\):&lt;/p&gt;
&lt;p&gt;\[
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp (e_{ik})}.
\]&lt;/p&gt;
&lt;p&gt;This weight reflects how important \(h_j\) is at deciding the next state \(s_i\) and generating \(y_i\).&lt;/p&gt;
&lt;h3 id=&#34;soft-vs-dot-hard-attention&#34;&gt;Soft vs. Hard Attention&lt;/h3&gt;
&lt;p&gt;This mechanism was also described in the context of visual attention as &amp;ldquo;soft&amp;rdquo; attention (Xu et al. 2016).
The authors also describe an alternative version they call &amp;ldquo;hard&amp;rdquo; attention.
Instead of providing a probability of where the model should look, hard attention provides a single location that is sampled from a multinoulli distribution parameterized by \(\alpha_i\).&lt;/p&gt;
&lt;p&gt;\[
p(s_{t,i} = 1 | s_{j&amp;lt;t}, \mathbf{a}) = \alpha_{t,i}
\]&lt;/p&gt;
&lt;p&gt;Here, \(s_{t,i}\) represents the location \(i\) at time \(t\), \(s_{j&amp;lt;t}\) are the location variables prior to \(t\), and \(\mathbf{a}\) is an image feature vector.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-11-10_12-07-42_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Hard attention for &amp;#34;A man and a woman playing frisbee in a field.&amp;#34; (Xu et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Hard attention for &amp;ldquo;A man and a woman playing frisbee in a field.&amp;rdquo; (Xu et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-11-10_12-08-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Soft attention for &amp;#34;A woman is throwing a frisbee in a park.&amp;#34; (Xu et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Soft attention for &amp;ldquo;A woman is throwing a frisbee in a park.&amp;rdquo; (Xu et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The two figures above show the difference between soft and hard attention.
Hard attention, while faster at inference time, is non-differentiable and requires more complex methods to train (TODO: cite Luong).&lt;/p&gt;
&lt;h3 id=&#34;self-attention&#34;&gt;Self-Attention&lt;/h3&gt;
&lt;p&gt;Self attention is particularly useful for determining the relationship between different parts of an input sequence. The figure below demonstrates self-attention given an input sentence (Cheng, Dong, and Lapata 2016).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-11-10_13-11-31_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Line thickness indicates stronger self-attention (Cheng et al.).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Line thickness indicates stronger self-attention (Cheng et al.).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;How aligned the two vectors are.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cross-attention&#34;&gt;Cross Attention&lt;/h3&gt;
&lt;p&gt;TODO&lt;/p&gt;
&lt;h2 id=&#34;key-value-store&#34;&gt;Key-value Store&lt;/h2&gt;
&lt;p&gt;Query, key, and value come from the same input (self-attention).&lt;/p&gt;
&lt;p&gt;Check query against all possible keys in the dictionary. They have the same size.
The value is the result stored there, not necessarily the same size.
Each item in the sequence will generate a query, key, and value.&lt;/p&gt;
&lt;p&gt;The attention vector is a function of they keys and the query.&lt;/p&gt;
&lt;p&gt;Hidden representation is a function of the values and the attention vector.&lt;/p&gt;
&lt;p&gt;The Transformer paper talks about queries, keys, and values. This idea comes from retrieval systems.
If you are searching for something (a video, book, song, etc.), you present a system your query. That system will compare your query against the keys in its database. If there is a key that matches your query, the value is returned.&lt;/p&gt;
&lt;p&gt;\[
att(q, \mathbf{k}, \mathbf{v}) = \sum_i v_i f(q, k_i),
\]
where \(f\) is a similarity function.&lt;/p&gt;
&lt;p&gt;This is an interesting and convenient representation of attention.
To implement this idea, we need some measure of similarity.
Why not orthogonality? Two vectors that are orthogonal produce a scalar value of 0.
The maximum value two vectors will produce as a result of the dot product occurs when the two vectors have the exact same direction.
This is convenient because the dot product is simple and efficient and we are already performing these calculations in our deep networks in the form of matrix multiplication.&lt;/p&gt;
&lt;h2 id=&#34;scaled-dot-product-attention&#34;&gt;Scaled Dot Product Attention&lt;/h2&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-11-21_18-39-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Scaled dot-product attention ((Vaswani et al., n.d.))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Scaled dot-product attention ((Vaswani et al., n.d.))
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Each &lt;strong&gt;query&lt;/strong&gt; vector is multiplied with each &lt;strong&gt;key&lt;/strong&gt; using the dot product.
This is implemented more efficiently via matrix multiplication.
A few other things are added here to control the output.
The first is &lt;strong&gt;scaling&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;multi-head-attention&#34;&gt;Multi-Head Attention&lt;/h2&gt;
&lt;p&gt;A single attention head can transform the input into a single representation. Is this analagous to using a single convolutional filter? The benefit of having multiple filters is to create multiple possible representations from the same input.&lt;/p&gt;
&lt;h2 id=&#34;encoder-decoder-architecture&#34;&gt;Encoder-Decoder Architecture&lt;/h2&gt;
&lt;p&gt;The original architecture of a transformer was defined in the context of sequence transduction tasks, where both the input and output are sequences. The most common task of this type is machine translation.&lt;/p&gt;
&lt;h2 id=&#34;encoder&#34;&gt;Encoder&lt;/h2&gt;
&lt;p&gt;The encoder layer takes an input sequence \(\{\mathbf{x}_t\}_{t=0}^T\) and transforms it into another sequence \(\{\mathbf{z}_t\}_{t=0}^T\).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;What is \(\mathbf{z}_t\)?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How is it used?
Input as key and value into second multi-head attention layer of the &lt;strong&gt;decoder&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Could you create an encoder only model?
Yes. Suitable for classification tasks &amp;ndash; classify the representation produced by the encoder.
&lt;strong&gt;How does this representation relate to understanding?&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It&amp;rsquo;s a transformation to another representation.&lt;/p&gt;
&lt;p&gt;Generated representation also considers the context of other parts of the same sequence (bi-directional).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;decoder&#34;&gt;Decoder&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Generates an output sequence.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Decoder-only models?
Suitable for text generation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What does the input represent?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What does the output represent?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What if we don&amp;rsquo;t use an encoder, what information is added in lieu of the encoder output?&lt;/p&gt;
&lt;!-- This HTML table template is generated by emacs/table.el --&gt;
&lt;table border=&#34;1&#34;&gt;
  &lt;tr&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Model&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Examples&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Tasks&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Encoder&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;ALBERT,&amp;nbsp;BERT,&amp;nbsp;DistilBERT,&lt;br /&gt;
      ELECTRA,&amp;nbsp;RoBERTa&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Sentence&amp;nbsp;classification,&amp;nbsp;named&amp;nbsp;entity&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      recognition,&amp;nbsp;extractive&amp;nbsp;question&amp;nbsp;answering&amp;nbsp;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Decoder&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;CTRL,&amp;nbsp;GPT,&amp;nbsp;GPT-2,&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      Transformer&amp;nbsp;XL&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Text&amp;nbsp;generation&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Encoder-decoder&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;BART,&amp;nbsp;T5,&amp;nbsp;Marian,&amp;nbsp;mBART&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Summarization,&amp;nbsp;translation,&amp;nbsp;generative&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      question&amp;nbsp;answering&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;p&gt;TODO&lt;/p&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/labmlai/status/1543159412940242945?s=20&amp;amp;t=EDu5FzDWl92EqnJlWvfAxA&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://twitter.com/labmlai/status/1543159412940242945?s=20&amp;t=EDu5FzDWl92EqnJlWvfAxA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Transduction_%28machine_learning%29&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://en.wikipedia.org/wiki/Transduction_(machine_learning)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.apronus.com/math/transformer-language-model-definition&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.apronus.com/math/transformer-language-model-definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2018-06-24-attention/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://lilianweng.github.io/posts/2018-06-24-attention/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://nlp.seas.harvard.edu/annotated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;http://nlp.seas.harvard.edu/annotated-transformer/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
