<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gpgpu on Alex Dillhoff</title>
    <link>http://localhost:1313/tags/gpgpu/</link>
    <description>Recent content in Gpgpu on Alex Dillhoff</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; {year}</copyright>
    <lastBuildDate>Fri, 19 Apr 2024 16:52:00 -0500</lastBuildDate>
    
	    <atom:link href="http://localhost:1313/tags/gpgpu/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Dynamic Parallelism</title>
      <link>http://localhost:1313/notes/dynamic_parallelism/</link>
      <pubDate>Fri, 19 Apr 2024 16:52:00 -0500</pubDate>
      
      <guid>http://localhost:1313/notes/dynamic_parallelism/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Dynamic Parallelism&lt;/strong&gt; is an extension to CUDA that enables kernels to directly call other kernels. Earlier versions of CUDA only allowed kernels to be launched from the host code. When we studied &lt;GPU Pattern: Parallel Scan&gt;, the segmented approach required multiple kernel calls.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using the cuDNN Library</title>
      <link>http://localhost:1313/notes/using_the_cudnn_library/</link>
      <pubDate>Mon, 15 Apr 2024 20:14:00 -0500</pubDate>
      
      <guid>http://localhost:1313/notes/using_the_cudnn_library/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-cudnn&#34;&gt;What is cuDNN?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#setting-up-cudnn&#34;&gt;Setting up cuDNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#handling-errors&#34;&gt;Handling Errors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#representing-data&#34;&gt;Representing Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dense-layers&#34;&gt;Dense Layers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#activation-functions&#34;&gt;Activation Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#loss-functions&#34;&gt;Loss Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#convolutions&#34;&gt;Convolutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pooling&#34;&gt;Pooling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;what-is-cudnn&#34;&gt;What is cuDNN?&lt;/h2&gt;
&lt;p&gt;NVIDIA cuDNN provides optimized implementations of core operations used in deep learning. It is designed to be integrated into higher-level machine learning frameworks, such as TensorFlow, PyTorch, and Caffe.&lt;/p&gt;
&lt;h2 id=&#34;setting-up-cudnn&#34;&gt;Setting up cuDNN&lt;/h2&gt;
&lt;p&gt;To use cuDNN in your applications, each program needs to establish a handle to the cuDNN library. This is done by creating a &lt;code&gt;cudnnHandle_t&lt;/code&gt; object and initializing it with &lt;code&gt;cudnnCreate&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;cudnn.h&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;() {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt; handle;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnCreate&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;handle);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Use the handle
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnDestroy&lt;/span&gt;(handle);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;handling-errors&#34;&gt;Handling Errors&lt;/h2&gt;
&lt;p&gt;cuDNN functions return a &lt;code&gt;cudnnStatus_t&lt;/code&gt; value, which indicates whether the function call was successful. As with previous CUDA code that we have reviewed, it is best to check the return value of each call. Not only does this help with debugging, but it also allows you to handle errors gracefully.&lt;/p&gt;
&lt;h2 id=&#34;representing-data&#34;&gt;Representing Data&lt;/h2&gt;
&lt;p&gt;All data in cuDNN is represented as a &lt;strong&gt;tensor&lt;/strong&gt;. A tensor is a multi-dimensional array of data. In cuDNN, tensors have the following parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;# of dimensions&lt;/li&gt;
&lt;li&gt;data type&lt;/li&gt;
&lt;li&gt;an array of integers indicating the size of each dimension&lt;/li&gt;
&lt;li&gt;an array of integers indicating the stride of each dimension&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are a few tensor descriptors for commonly used tensor types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3D Tensors (BMN): Batch, Height, Width&lt;/li&gt;
&lt;li&gt;4D Tensors (NCHW): Batch, Channel, Height, Width&lt;/li&gt;
&lt;li&gt;5D Tensors (NCDHW): Batch, Channel, Depth, Height, Width&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When creating a tensor to use with cuDNN operations, we need to create a &lt;strong&gt;tensor descriptor&lt;/strong&gt; as well as the data itself. The tensor descriptor is a struct that contains the parameters of the tensor.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// Create descriptor
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnDataType_t&lt;/span&gt; data_type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CUDNN_DATA_FLOAT;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorFormat_t&lt;/span&gt; format &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CUDNN_TENSOR_NCHW;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, h &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;, w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; tensor_desc;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnCreateTensorDescriptor&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;tensor_desc);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSetTensor4dDescriptor&lt;/span&gt;(tensor_desc, format, data_type, n, c, h, w);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The descriptor is then used to allocate memory for the tensor data.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;data;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudaMalloc&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;data, n &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; h &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;sizeof&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;retrieving-tensor-information&#34;&gt;Retrieving Tensor Information&lt;/h3&gt;
&lt;p&gt;To retrieve the properties of a tensor that already exists, we can use the &lt;code&gt;cudnnGetTensor4dDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnGetTensor4dDescriptor&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;  tensorDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnDataType_t&lt;/span&gt;         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dataType,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;n,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;c,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;nStride,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;cStride,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;hStride,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;wStride)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tensorDesc&lt;/code&gt;: the tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dataType&lt;/code&gt;: the data type of the tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n&lt;/code&gt;: the number of batches&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt;: the number of channels&lt;/li&gt;
&lt;li&gt;&lt;code&gt;h&lt;/code&gt;: the height of the tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;w&lt;/code&gt;: the width of the tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nStride&lt;/code&gt;: the stride of the batch dimension&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cStride&lt;/code&gt;: the stride of the channel dimension&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hStride&lt;/code&gt;: the stride of the height dimension&lt;/li&gt;
&lt;li&gt;&lt;code&gt;wStride&lt;/code&gt;: the stride of the width dimension&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;dense-layers&#34;&gt;Dense Layers&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;dense layer&lt;/strong&gt; refers to a fully connected layer in a neural network. Each neuron in the layer is connected to every neuron in the previous layer. The weights of the connections are stored in a matrix, and the biases are stored in a vector. Implementing the forward and backward pass of a dense layer involves matrix multiplication and addition for which cuBLAS has optimized routines.&lt;/p&gt;
&lt;h3 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h3&gt;
&lt;p&gt;The forward pass of a dense layer is computed as follows:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{a} = W \mathbf{x} + \mathbf{b}
\]&lt;/p&gt;
&lt;p&gt;where \(\mathbf{W}\) is the weight matrix, \(\mathbf{x}\) is the input tensor, \(\mathbf{b}\) is the bias vector, and \(\mathbf{a}\) is the output tensor.&lt;/p&gt;
&lt;p&gt;This can be implemented in CUDA with a matrix multiply followed by a vector addition. The first operation we will use is &lt;a href=&#34;https://docs.nvidia.com/cuda/cublas/index.html?highlight=cublasSgemm#cublas-t-gemm&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&lt;code&gt;cublasSgemm&lt;/code&gt;&lt;/a&gt;. The function declaration is&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cublasStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cublasSgemm&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;cublasHandle_t&lt;/span&gt; handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;cublasOperation_t&lt;/span&gt; transa, &lt;span style=&#34;color:#66d9ef&#34;&gt;cublasOperation_t&lt;/span&gt; transb,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; m, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; k,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;           &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;           &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;A, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; lda,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;           &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;B, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; ldb,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;           &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;           &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;C, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; ldc);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This computes&lt;/p&gt;
&lt;p&gt;\[
C = \alpha \text{op}(A) \text{op}(B) + \beta C.
\]&lt;/p&gt;
&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;handle&lt;/code&gt;: the cuBLAS handle&lt;/li&gt;
&lt;li&gt;&lt;code&gt;transa&lt;/code&gt;: the operation to perform on matrix A (transpose or not)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;transb&lt;/code&gt;: the operation to perform on matrix B (transpose or not)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;m&lt;/code&gt;: the number of rows in matrix A and C&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n&lt;/code&gt;: the number of columns in matrix B and C&lt;/li&gt;
&lt;li&gt;&lt;code&gt;k&lt;/code&gt;: the number of columns in matrix A and rows in matrix B&lt;/li&gt;
&lt;li&gt;&lt;code&gt;alpha&lt;/code&gt;: scalar multiplier for the product of A and B&lt;/li&gt;
&lt;li&gt;&lt;code&gt;A&lt;/code&gt;: matrix A&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lda&lt;/code&gt;: leading dimension of matrix A&lt;/li&gt;
&lt;li&gt;&lt;code&gt;B&lt;/code&gt;: matrix B&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ldb&lt;/code&gt;: leading dimension of matrix B&lt;/li&gt;
&lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: scalar multiplier for matrix C&lt;/li&gt;
&lt;li&gt;&lt;code&gt;C&lt;/code&gt;: matrix C&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ldc&lt;/code&gt;: leading dimension of matrix C&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This function is called twice in the forward pass of a dense layer: once for the matrix multiplication and once for the vector addition.&lt;/p&gt;
&lt;h3 id=&#34;backward-pass&#34;&gt;Backward Pass&lt;/h3&gt;
&lt;p&gt;The backward pass of a dense layer computes the gradients of the weights and biases with respect to the loss.&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathbf{a}}{\partial W} = \frac{\partial}{\partial W} (W \mathbf{x} + \mathbf{b}) = \mathbf{x}
\]&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathbf{a}}{\partial \mathbf{b}} = \frac{\partial}{\partial \mathbf{b}} (W \mathbf{x} + \mathbf{b}) = 1
\]&lt;/p&gt;
&lt;p&gt;Additionally, the layer should propagate the gradients of the loss with respect to the input tensor.&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathbf{a}}{\partial \mathbf{x}} = \frac{\partial}{\partial \mathbf{x}} (W \mathbf{x} + \mathbf{b}) = W
\]&lt;/p&gt;
&lt;p&gt;These gradients are only the local gradients of the layer. During backpropagation, the gradients are multiplied by the gradients propagated from the subsequent layer, as shown below:&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial \mathbf{a}} \frac{\partial \mathbf{a}}{\partial W}
\]&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial L}{\partial \mathbf{b}} = \frac{\partial L}{\partial \mathbf{a}} \frac{\partial \mathbf{a}}{\partial \mathbf{b}}
\]&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial L}{\partial \mathbf{x}} = \frac{\partial L}{\partial \mathbf{a}} \frac{\partial \mathbf{a}}{\partial \mathbf{x}}
\]&lt;/p&gt;
&lt;p&gt;The first two gradients are used to update the weights and biases of the current layer. The last gradient is propagated to the previous layer.&lt;/p&gt;
&lt;p&gt;These can be implemented in CUDA with matrix multiplication and vector addition, similar to the forward pass.&lt;/p&gt;
&lt;h2 id=&#34;activation-functions&#34;&gt;Activation Functions&lt;/h2&gt;
&lt;p&gt;cuDNN supports a variety of activation functions, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sigmoid&lt;/li&gt;
&lt;li&gt;Hyperbolic Tangent&lt;/li&gt;
&lt;li&gt;Rectified Linear Unit (ReLU)&lt;/li&gt;
&lt;li&gt;Clipped Rectified Linear Unit (CLReLU)&lt;/li&gt;
&lt;li&gt;Exponential Linear Unit (ELU)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To use an activation function, we need to create an activation descriptor and set the activation function type.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnActivationDescriptor_t&lt;/span&gt; activation_desc;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnCreateActivationDescriptor&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;activation_desc);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSetActivationDescriptor&lt;/span&gt;(activation_desc, CUDNN_ACTIVATION_RELU, CUDNN_NOT_PROPAGATE_NAN, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The third enum &lt;code&gt;CUDNN_NOT_PROPAGATE_NAN&lt;/code&gt; indicates that NaN values should not be propagated through the activation function. The last parameter is a coefficient value, which is used by clipped ReLU and ELU.&lt;/p&gt;
&lt;p&gt;We can also query the activation descriptor to extract the properties.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnGetActivationDescriptor&lt;/span&gt;(activation_desc, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;mode, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;reluNanOpt, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;coef);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h3&gt;
&lt;p&gt;To process the forward pass of an activation function, we use the &lt;code&gt;cudnnActivationForward&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnActivationForward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt; handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnActivationDescriptor_t&lt;/span&gt; activationDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha, &lt;span style=&#34;color:#75715e&#34;&gt;// scalar multiplier
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta, &lt;span style=&#34;color:#75715e&#34;&gt;// scalar modifier
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; zDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;z);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This computes the following operation:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{z} = \alpha \cdot g(\mathbf{x}) + \beta \cdot \mathbf{z}
\]&lt;/p&gt;
&lt;p&gt;where \(\mathbf{x}\) is the input tensor, \(\mathbf{z}\) is the output tensor, \(\alpha\) is a scalar multiplier, and \(\beta\) is a scalar modifier.&lt;/p&gt;
&lt;h3 id=&#34;backward-pass&#34;&gt;Backward Pass&lt;/h3&gt;
&lt;p&gt;Likewise, the backward pass is done with &lt;code&gt;cudnnActivationBackward&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnActivationBackward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt; handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnActivationDescriptor_t&lt;/span&gt; activationDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha, &lt;span style=&#34;color:#75715e&#34;&gt;// gradient modifier
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; zDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;z,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; dzDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dz,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; dxDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dx
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This computes the following operation:&lt;/p&gt;
&lt;p&gt;\[
d\mathbf{x} = \alpha \cdot \nabla_{\mathbf{x}} g(\mathbf{x}) d\mathbf{z} + \beta \cdot d\mathbf{x}
\]&lt;/p&gt;
&lt;p&gt;where \(d\mathbf{z}\) is the input tensor to the backward function. Under this same notation, \(\mathbf{z}\) was the output of the activation function. The input to the activation function \(d\mathbf{x}\) is the output tensor of the backward pass, since it is being propagated in the backwards direction.&lt;/p&gt;
&lt;h2 id=&#34;loss-functions&#34;&gt;Loss Functions&lt;/h2&gt;
&lt;p&gt;cuDNN also provides optimized implementations of loss functions such as cross-entropy. Since the related lab focuses on classification, we will limit our discussion to the cross-entropy loss combined with the softmax function.&lt;/p&gt;
&lt;h3 id=&#34;softmax&#34;&gt;Softmax&lt;/h3&gt;
&lt;p&gt;The softmax function is used to convert the output of a neural network into a probability distribution. It is defined as&lt;/p&gt;
&lt;p&gt;\[
\text{softmax}(\mathbf{x})_i = \frac{e^{x_i}}{\sum_j e^{x_j}}
\]&lt;/p&gt;
&lt;p&gt;where \(\mathbf{x}\) is the input tensor and \(i\) is the index of the output tensor.&lt;/p&gt;
&lt;h4 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h4&gt;
&lt;p&gt;Implementing the forward pass of the softmax function is straightforward. We use the &lt;code&gt;cudnnSoftmaxForward&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSoftmaxForward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                    handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnSoftmaxAlgorithm_t&lt;/span&gt;          algorithm,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnSoftmaxMode_t&lt;/span&gt;               mode,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    yDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;y)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Most of the parameters are similar to other cuDNN functions. The &lt;code&gt;algorithm&lt;/code&gt; parameter specifies the algorithm to use for the softmax function, and the &lt;code&gt;mode&lt;/code&gt; parameter specifies the mode of the softmax function.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;algorithm&lt;/code&gt;: &lt;code&gt;CUDNN_SOFTMAX_FAST&lt;/code&gt;, &lt;code&gt;CUDNN_SOFTMAX_ACCURATE&lt;/code&gt;, or &lt;code&gt;CUDNN_SOFTMAX_LOG&lt;/code&gt;. The most numerically stable is &lt;code&gt;CUDNN_SOFTMAX_ACCURATE&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mode&lt;/code&gt;: &lt;code&gt;CUDNN_SOFTMAX_MODE_INSTANCE&lt;/code&gt; or &lt;code&gt;CUDNN_SOFTMAX_MODE_CHANNEL&lt;/code&gt;. The former computes the softmax function for each instance in the batch, while the latter computes the softmax function for each channel in the tensor.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;backward-pass&#34;&gt;Backward Pass&lt;/h4&gt;
&lt;p&gt;The backward pass of the softmax function is implemented with &lt;code&gt;cudnnSoftmaxBackward&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSoftmaxBackward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                    handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnSoftmaxAlgorithm_t&lt;/span&gt;          algorithm,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnSoftmaxMode_t&lt;/span&gt;               mode,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    yDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;y,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    dyDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dy,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    dxDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dx)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that the softmax function is used in the forward pass of the loss function, so the gradients are propagated from the loss function to the softmax function. In practice, the two are combined into a much simpler gradient calculation. If the softmax function is followed by the cross-entropy loss, the gradients are computed as&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial L}{\partial \mathbf{x}} = \mathbf{x} - \mathbf{y}
\]&lt;/p&gt;
&lt;p&gt;where \(\mathbf{y}\) is the target tensor.&lt;/p&gt;
&lt;h2 id=&#34;convolutions&#34;&gt;Convolutions&lt;/h2&gt;
&lt;p&gt;For a background on convolutions, see &lt;a href=&#34;http://localhost:1313/notes/convolutional_neural_networks/&#34;&gt;these notes&lt;/a&gt;. The notes in this article refer to the cuDNN implementation of convolutions.&lt;/p&gt;
&lt;p&gt;When using convolution operations in cuDNN, we need to create a convolution descriptor &lt;code&gt;cudnnConvolutionDescriptor_t&lt;/code&gt; as well as a filter descriptor &lt;code&gt;cudnnFilterDescriptor_t&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;creating-a-filter&#34;&gt;Creating a filter&lt;/h3&gt;
&lt;p&gt;To create a filter descriptor, we use the &lt;code&gt;cudnnCreateFilterDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnFilterDescriptor_t&lt;/span&gt; filter_desc;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnCreateFilterDescriptor&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;filter_desc);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We then set the filter descriptor with the &lt;code&gt;cudnnSetFilter4dDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSetFilter4dDescriptor&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnFilterDescriptor_t&lt;/span&gt;    filterDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnDataType_t&lt;/span&gt;            dataType,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorFormat_t&lt;/span&gt;        format,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                        k,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                        c,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                        h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                        w)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;filterDesc&lt;/code&gt;: the filter descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dataType&lt;/code&gt;: the data type of the filter&lt;/li&gt;
&lt;li&gt;&lt;code&gt;format&lt;/code&gt;: the format of the filter (NCHW or NHWC). Use &lt;code&gt;CUDNN_TENSOR_NCHW&lt;/code&gt; for most cases.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;k&lt;/code&gt;: the number of output feature maps&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt;: the number of input feature maps&lt;/li&gt;
&lt;li&gt;&lt;code&gt;h&lt;/code&gt;: the height of the filter&lt;/li&gt;
&lt;li&gt;&lt;code&gt;w&lt;/code&gt;: the width of the filter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also query the filter descriptor to extract the properties.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnDataType_t&lt;/span&gt; data_type;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorFormat_t&lt;/span&gt; format;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; k, c, h, w;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnGetFilter4dDescriptor&lt;/span&gt;(filter_desc, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;data_type, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;format, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;k, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;c, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;h, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;w);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;creating-a-convolution&#34;&gt;Creating a convolution&lt;/h3&gt;
&lt;p&gt;To create a convolution descriptor, we use the &lt;code&gt;cudnnCreateConvolutionDescriptor&lt;/code&gt; function. Once we are done with it, we should destroy it with &lt;code&gt;cudnnDestroyConvolutionDescriptor&lt;/code&gt;. Since our convolution is 2D, we use the &lt;code&gt;cudnnSetConvolution2dDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSetConvolution2dDescriptor&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionDescriptor_t&lt;/span&gt;    convDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                             pad_h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                             pad_w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                             u,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                             v,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                             dilation_h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                             dilation_w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionMode_t&lt;/span&gt;          mode,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnDataType_t&lt;/span&gt;                 computeType)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;convDesc&lt;/code&gt;: the convolution descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pad_h&lt;/code&gt;: the height padding&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pad_w&lt;/code&gt;: the width padding&lt;/li&gt;
&lt;li&gt;&lt;code&gt;u&lt;/code&gt;: the vertical stride&lt;/li&gt;
&lt;li&gt;&lt;code&gt;v&lt;/code&gt;: the horizontal stride&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dilation_h&lt;/code&gt;: the height dilation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dilation_w&lt;/code&gt;: the width dilation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mode&lt;/code&gt;: the convolution mode (&lt;code&gt;CUDNN_CONVOLUTION&lt;/code&gt; or &lt;code&gt;CUDNN_CROSS_CORRELATION&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;computeType&lt;/code&gt;: the data type used for the convolution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although the library supports both convolution and cross-correlation, the difference is only in the order of the operands. In practice, the two are equivalent. Most deep learning frameworks use cross-correlation.&lt;/p&gt;
&lt;p&gt;To query the convolution descriptor, we can use the &lt;code&gt;cudnnGetConvolution2dDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnGetConvolution2dDescriptor&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionDescriptor_t&lt;/span&gt;    convDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;pad_h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;pad_w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;u,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;v,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dilation_h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dilation_w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionMode_t&lt;/span&gt;         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;mode,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnDataType_t&lt;/span&gt;                &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;computeType)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If we know all the parameters of the convolution, we can use the &lt;code&gt;cudnnGetConvolution2dForwardOutputDim&lt;/code&gt; function to calculate the output dimensions.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnGetConvolution2dForwardOutputDim&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionDescriptor_t&lt;/span&gt;    convDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;         inputTensorDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnFilterDescriptor_t&lt;/span&gt;         filterDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                                 &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;n,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                                 &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;c,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                                 &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                                 &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;cuDNN&lt;/code&gt; supports several methods for performing a convolution operation. An evaluation of the available algorithms can be found &lt;a href=&#34;https://core.ac.uk/download/pdf/224976536.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here.&lt;/a&gt; The algorithms provide tradeoffs in terms of speed and memory usage. Diving into these details is beyond the scope of this article, but it is important to be aware of the options.&lt;/p&gt;
&lt;p&gt;The forward pass of a convolution is implemented with the &lt;code&gt;cudnnConvolutionForward&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnConvolutionForward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                       handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;       xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnFilterDescriptor_t&lt;/span&gt;       wDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionDescriptor_t&lt;/span&gt;  convDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionFwdAlgo_t&lt;/span&gt;           algo,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                               &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;workSpace,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;size_t&lt;/span&gt;                              workSpaceSizeInBytes,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;       yDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                               &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;y)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;handle&lt;/code&gt;: the cuDNN handle&lt;/li&gt;
&lt;li&gt;&lt;code&gt;alpha&lt;/code&gt;: scalar multiplier for the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;xDesc&lt;/code&gt;: the input tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt;: the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;wDesc&lt;/code&gt;: the filter descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;w&lt;/code&gt;: the filter tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;convDesc&lt;/code&gt;: the convolution descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;algo&lt;/code&gt;: the algorithm to use for the convolution&lt;/li&gt;
&lt;li&gt;&lt;code&gt;workSpace&lt;/code&gt;: the workspace for the convolution&lt;/li&gt;
&lt;li&gt;&lt;code&gt;workSpaceSizeInBytes&lt;/code&gt;: the size of the workspace&lt;/li&gt;
&lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: scalar modifier for the output tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;yDesc&lt;/code&gt;: the output tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: the output tensor&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;backward-pass&#34;&gt;Backward Pass&lt;/h3&gt;
&lt;p&gt;There are three different backward passes for a convolutional layer: one for the weights, one for the input tensor, and one for the bias.&lt;/p&gt;
&lt;h4 id=&#34;weights&#34;&gt;Weights&lt;/h4&gt;
&lt;p&gt;The backward pass for the weights is implemented with the &lt;code&gt;cudnnConvolutionBackwardFilter&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnConvolutionBackwardFilter&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                       handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;       xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;       dyDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dy,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionDescriptor_t&lt;/span&gt;  convDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionBwdFilterAlgo_t&lt;/span&gt;     algo,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                               &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;workSpace,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;size_t&lt;/span&gt;                              workSpaceSizeInBytes,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnFilterDescriptor_t&lt;/span&gt;       dwDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                               &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dw)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;A detailed description of the parameters can be found &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-cnn-library.html#cudnnconvolutionbackwardfilter&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;bias&#34;&gt;Bias&lt;/h4&gt;
&lt;p&gt;The backward pass for the bias is implemented with the &lt;code&gt;cudnnConvolutionBackwardBias&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnConvolutionBackwardBias&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                    handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    dyDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dy,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    dbDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;db)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;A detailed description of the parameters can be found &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-cnn-library.html#cudnnconvolutionbackwardbias&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;input&#34;&gt;Input&lt;/h4&gt;
&lt;p&gt;The backward pass for the input tensor is implemented with the &lt;code&gt;cudnnConvolutionBackwardData&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnConvolutionBackwardData&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                       handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnFilterDescriptor_t&lt;/span&gt;       wDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;       dyDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dy,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionDescriptor_t&lt;/span&gt;  convDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionBwdDataAlgo_t&lt;/span&gt;       algo,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                               &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;workSpace,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;size_t&lt;/span&gt;                              workSpaceSizeInBytes,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;       dxDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                               &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dx)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;A detailed description of the parameters can be found &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-cnn-library.html#cudnnconvolutionbackwarddata&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;pooling&#34;&gt;Pooling&lt;/h2&gt;
&lt;p&gt;Pooling is a common operation in convolutional neural networks. It reduces the spatial dimensions of the input tensor, which helps to reduce the number of parameters and computation in the network. Using pooling in cuDNN requires creating a descriptor. Make sure to destroy it when you&amp;rsquo;re done.&lt;/p&gt;
&lt;h3 id=&#34;creating-a-pooling-descriptor&#34;&gt;Creating a pooling descriptor&lt;/h3&gt;
&lt;p&gt;To create a pooling descriptor, we use the &lt;code&gt;cudnnCreatePoolingDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnPoolingDescriptor_t&lt;/span&gt; pooling_desc;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnCreatePoolingDescriptor&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;pooling_desc);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We then set the pooling descriptor with the &lt;code&gt;cudnnSetPooling2dDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSetPooling2dDescriptor&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnPoolingDescriptor_t&lt;/span&gt;    poolingDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnPoolingMode_t&lt;/span&gt;          mode,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnNanPropagation_t&lt;/span&gt;       maxpoolingNanOpt,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                         windowHeight,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                         windowWidth,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                         verticalPadding,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                         horizontalPadding,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                         verticalStride,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                         horizontalStride)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;poolingDesc&lt;/code&gt;: the pooling descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mode&lt;/code&gt;: the pooling mode (&lt;code&gt;CUDNN_POOLING_MAX&lt;/code&gt; or &lt;code&gt;CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;maxpoolingNanOpt&lt;/code&gt;: the NaN propagation option for max pooling&lt;/li&gt;
&lt;li&gt;&lt;code&gt;windowHeight&lt;/code&gt;: the height of the pooling window&lt;/li&gt;
&lt;li&gt;&lt;code&gt;windowWidth&lt;/code&gt;: the width of the pooling window&lt;/li&gt;
&lt;li&gt;&lt;code&gt;verticalPadding&lt;/code&gt;: the vertical padding&lt;/li&gt;
&lt;li&gt;&lt;code&gt;horizontalPadding&lt;/code&gt;: the horizontal padding&lt;/li&gt;
&lt;li&gt;&lt;code&gt;verticalStride&lt;/code&gt;: the vertical stride&lt;/li&gt;
&lt;li&gt;&lt;code&gt;horizontalStride&lt;/code&gt;: the horizontal stride&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also query the pooling descriptor to extract the properties.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnPoolingMode_t&lt;/span&gt; mode;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnNanPropagation_t&lt;/span&gt; nan_opt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; window_h, window_w, pad_h, pad_w, stride_h, stride_w;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnGetPooling2dDescriptor&lt;/span&gt;(pooling_desc, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;mode, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;nan_opt, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;window_h, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;window_w, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;pad_h, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;pad_w, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;stride_h, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;stride_w);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h3&gt;
&lt;p&gt;The forward pass of a pooling operation is implemented with the &lt;code&gt;cudnnPoolingForward&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnPoolingForward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                    handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnPoolingDescriptor_t&lt;/span&gt;   poolingDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    yDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;y)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;handle&lt;/code&gt;: the cuDNN handle&lt;/li&gt;
&lt;li&gt;&lt;code&gt;poolingDesc&lt;/code&gt;: the pooling descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;alpha&lt;/code&gt;: scalar multiplier for the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;xDesc&lt;/code&gt;: the input tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt;: the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: scalar modifier for the output tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;yDesc&lt;/code&gt;: the output tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: the output tensor&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;backward-pass&#34;&gt;Backward Pass&lt;/h3&gt;
&lt;p&gt;The backward pass of a pooling operation is implemented with the &lt;code&gt;cudnnPoolingBackward&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnPoolingBackward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                    handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnPoolingDescriptor_t&lt;/span&gt;   poolingDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    yDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;y,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    dyDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dy,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    dxDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dx)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;handle&lt;/code&gt;: the cuDNN handle&lt;/li&gt;
&lt;li&gt;&lt;code&gt;poolingDesc&lt;/code&gt;: the pooling descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;alpha&lt;/code&gt;: scalar multiplier for the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;yDesc&lt;/code&gt;: the output tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: the output tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dyDesc&lt;/code&gt;: the input tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dy&lt;/code&gt;: the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;xDesc&lt;/code&gt;: the input tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt;: the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: scalar modifier for the output tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dxDesc&lt;/code&gt;: the output tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dx&lt;/code&gt;: the output tensor&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Parallel Graph Traversal</title>
      <link>http://localhost:1313/notes/parallel_graph_traversal/</link>
      <pubDate>Sat, 06 Apr 2024 15:26:00 -0500</pubDate>
      
      <guid>http://localhost:1313/notes/parallel_graph_traversal/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parallelization-over-vertices&#34;&gt;Parallelization over vertices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parallelization-over-edges&#34;&gt;Parallelization over edges&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#improving-work-efficiency&#34;&gt;Improving work efficiency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#privatization-to-reduce-contention&#34;&gt;Privatization to reduce contention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#additional-optimizations&#34;&gt;Additional Optimizations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;For an introduction on basic graph theory and traversal algorithms, see &lt;a href=&#34;http://localhost:1313/notes/introduction_to_graph_theory/&#34;&gt;these notes.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;parallelization-over-vertices&#34;&gt;Parallelization over vertices&lt;/h2&gt;
&lt;p&gt;When it comes to parallel algorithms, the first thing that may come to your mind is that they must operate on either the edges or the vertices. In either case, it doesn&amp;rsquo;t take much to imagine that such algorithms will require communication between threads due to dependencies in the graph or traversal algorithm.&lt;/p&gt;
&lt;p&gt;In a breadth-first search, all vertices in one level must be explored before moving into the next. The first approach we look at will require multiple calls to the kernel, one for each level.&lt;/p&gt;
&lt;h3 id=&#34;top-down-approach&#34;&gt;Top-down Approach&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void bfs_kernel(CSRGraph csrGraph, uint *level,
                           uint *newVertexVisited, uint *currLevel) {
    int vertex = blockIdx.x * blockDim.x + threadIdx.x;

    if (vertex &amp;lt; csrGraph.numVertices) {
        if (level[vertex] == currLevel - 1) {
            for (uint edge = csrGraph.srcPtrs[vertex]; edge &amp;lt; csrGraph.srcPtrs[vertex + 1]; edge++) {
                int neighbor = csrGraph.edges[edge];
                if (level[neighbor] == UINT_MAX) {  // Neighbor not visited
                    level[neighbor] = currLevel;
                    *newVertexVisited = 1;
                }
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The first kernel above labels the vertices that belong on the given level. For each vertex, it iterates over the outgoing edges. These can be accessed as the nonzero elements in the adjacency matrix for each row. The CSR format is ideal for this case. The figure below shows the result of the first kernel. Only 2 of the threads are active for the first level based on the input graph. This particular version of the algorithm is called the &lt;em&gt;push&lt;/em&gt; version.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-04-08_20-06-23_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Vertex-centric _push_ BFS traversal from level 1 to level 2 (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Vertex-centric &lt;em&gt;push&lt;/em&gt; BFS traversal from level 1 to level 2 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The boundary check ensures that the kernel only processes vertices that belong to the current level. Each thread has access to a global &lt;code&gt;newVertexVisited&lt;/code&gt; and will set this to 1 if it finds a new vertex to visit. This is used to determine if the traversal should continue to the next level.&lt;/p&gt;
&lt;h3 id=&#34;bottom-up-approach&#34;&gt;Bottom-up Approach&lt;/h3&gt;
&lt;p&gt;The second kernel is also a vertex-centric approach, except it considers incoming edges rather than outgoing ones. This is called the &lt;em&gt;pull&lt;/em&gt; version.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void bfs_kernel(CSCGraph cscGraph, uint *level,
                           uint *newVertexVisited, uint *currLevel) {
    int vertex = blockIdx.x * blockDim.x + threadIdx.x;

    if (vertex &amp;lt; cscGraph.numVertices) {
        if (level[vertex] == UINT_MAX) {
            for (uint edge = cscGraph.dstPtrs[vertex]; edge &amp;lt; cscGraph.dstPtrs[vertex + 1]; edge++) {
                int neighbor = cscGraph.edges[edge];
                if (level[neighbor] == currLevel - 1) {  // Neighbor visited
                    level[vertex] = currLevel;
                    *newVertexVisited = 1;
                    break;
                }
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note the use of the Compressed Sparse Column (CSC) format for the graph. The kernel requires that each thread be able to access the incoming edges, which would be determined by the nonzero elements of a give column of the adjacency matrix.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-04-08_20-35-48_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Vertex-centric _pull_ BFS traversal from level 1 to level 2 (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Vertex-centric &lt;em&gt;pull&lt;/em&gt; BFS traversal from level 1 to level 2 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;From each thread&amp;rsquo;s point of view, if there is an incoming edge at the previous level then it will be visited at the current level. In that case its job is done and it can break out of the loop. This approach is more efficient for graphs with a high average degree and variance.&lt;/p&gt;
&lt;p&gt;In early levels, the &lt;em&gt;push&lt;/em&gt; approach is more efficient because they have a relatively smaller number of vertices per level. As more vertices are visited, the &lt;em&gt;pull&lt;/em&gt; approach is more efficient because there is a higher chance of finding an incoming edge and exiting early. Since each level is a separate kernel call, both of these can be combined. An additional piece of overhead would be that one would require both CSR and CSC representations of the graph.&lt;/p&gt;
&lt;h2 id=&#34;parallelization-over-edges&#34;&gt;Parallelization over edges&lt;/h2&gt;
&lt;p&gt;As the name suggests, the edge-centric approach processes the edges in parallel. If the source vertex belongs to a previous level and the destination vertex is unvisited, then the destination vertex is labeled with the current level.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void bfs_kernel(COOGraph cooGraph, uint *level,
                           uint *newVertexVisited, uint *currLevel) {
    int edge = blockIdx.x * blockDim.x + threadIdx.x;

    if (edge &amp;lt; csrGraph.numEdges) {
        uint vertex = cooGraph.src[edge];
        if (level[vertex] == currLevel - 1) {
            uint neighbor = cooGraph.dst[edge];
            if (level[neighbor] == UINT_MAX) {
                level[neighbor] = currLevel;
                *newVertexVisited = 1;
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The edge-centric approach has more parallelism than the vertex-centric approaches. Graphs typically have more edges than vertices, so the largest benefit is on smaller graphs. Another advantage related to load imbalance. In the vertex-centric approach, imbalance comes from the fact that some vertices have more edges than others.&lt;/p&gt;
&lt;p&gt;The tradeoff of this that every edge is considered. There may be many edges that are not relevant for a particular level. The vertex-centric approach could skip these entirely. Since every edge needs to be indexed, the COO format is used. This requires more space than the CSR or CSC formats.&lt;/p&gt;
&lt;p&gt;Since these sparse representations are already used, we could perform these operations using sparse matrix multiplications. Libraries such as &lt;a href=&#34;https://github.com/rapidsai/cugraph/blob/branch-24.06/docs/cugraph/source/graph_support/algorithms.md&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;cugraph&lt;/a&gt; provide implementations of these algorithms.&lt;/p&gt;
&lt;h2 id=&#34;improving-work-efficiency&#34;&gt;Improving work efficiency&lt;/h2&gt;
&lt;p&gt;The previous two approaches have a common problem: it is likely that many threads will perform no useful work. Take the vertex-centric approach, for example. The threads that are launched only to find out that their vertex is not in the current level will not do anything. This is a waste of resources. Ideally, those threads would not be launched in the first place. A simple solution to this is to have each thread build a &lt;strong&gt;frontier&lt;/strong&gt; of vertices that they visit, so that only the vertices in the frontier are processed.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void bfs_kernel(CSRGraph csrGraph, uint *level,
                           uint *prevFrontier, uint *currFrontier,
                           uint numPrevFrontier, uint *numCurrFrontier,
                           uint currLevel) {
    uint i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &amp;lt; numPrevFrontier) {
        uint vertex = prevFrontier[i];
        for (uint edge = csrGraph.srcPtrs[vertex]; edge &amp;lt; csrGraph.srcPtrs[vertex + 1]; edge++) {
            uint neighbor = csrGraph.dst[edge];
            if (atomicCAS(&amp;amp;level[neighbor], UINT_MAX, currLevel) == UINT_MAX) {
                uint currFrontierIdx = atomicAdd(numCurrFrontier, 1);
                currFrontier[currFrontierIdx] = neighbor;
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When launched, only the threads corresponding to a frontier will be active. They start by loading the elements from the previous frontier. As before, they iterate over the outgoing edges. If the neighbor has not been visited, then it is labeled with the current level and added to the current frontier. The atomic operation ensures that the size of the frontier is updated correctly.&lt;/p&gt;
&lt;p&gt;The call to &lt;code&gt;atomicCAS&lt;/code&gt; prevents multiple threads from adding the same vertex to the frontier. It checks whether the current vertex is unvisited. Not every thread is going to visit the same neighbor, so the contention should be low for this call.&lt;/p&gt;
&lt;h2 id=&#34;privatization-to-reduce-contention&#34;&gt;Privatization to reduce contention&lt;/h2&gt;
&lt;p&gt;The use of atomic operations in the previous example introduces contention between threads. As we have previously studied, privatization can be applied in these cases to reduce that contention. In this case, each block will have its own private frontier. The contention is then reduced to atomic operations within a block. An added benefit is that the local frontier is in shared memory, resulting in lower latency atomic operations.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void bfs_kernel(CSRGraph csrGraph, uint *level,
                           uint *prevFrontier, uint *currFrontier,
                           uint numPrevFrontier, uint *numCurrFrontier,
                           uint currLevel) {

    // Initialize privatized frontier
    __shared__ uint currFrontier_s[LOCAL_FRONTIER_CAPACITY];
    __shared__ uint numCurrFrontier_s;

    if (threadIdx.x == 0) {
        numCurrFrontier_s = 0;
    }
    __syncthreads();

    // Perform BFS
    uint i = blockIdx.x * blockDim.x + threadIdx.x;

    if (i &amp;lt; numPrevFrontier) {
        uint vertex = prevFrontier[i];
        for (uint edge = csrGraph.srcPtrs[vertex]; edge &amp;lt; csrGraph.srcPtrs[vertex + 1]; edge++) {
            uint neighbor = csrGraph.dst[edge];
            if (atomicCAS(&amp;amp;level[neighbor], UINT_MAX, currLevel) == UINT_MAX) {
                uint currFrontierIdx_s = atomicAdd(&amp;amp;numCurrFrontier_s, 1);
                if (currFrontierIdx_s &amp;lt; LOCAL_FRONTIER_CAPACITY) {
                    currFrontier_s[currFrontierIdx_s] = neighbor;
                } else {
                    numCurrFrontier_s = LOCAL_FRONTIER_CAPACITY;
                    uint currFrontierIdx = atomicAdd(numCurrFrontier, 1);
                    currFrontier[currFrontierIdx] = neighbor;
                }
            }
        }
    }
    __syncthreads();

    // Allocate in global frontier
    __shared__ uint currFrontierStartIdx;
    if (threadIdx.x == 0) {
        currFrontierStartIdx = atomicAdd(numCurrFrontier, numCurrFrontier_s);
    }
    __syncthreads();

    // Commit to global frontier
    for (uint currFrontierIdx_s = threadIdx.x; currFrontierIdx_s &amp;lt; numCurrFrontier_s; currFrontierIdx_s += blockDim.x) {
        currFrontier[currFrontierStartIdx + currFrontierIdx_s] = currFrontier_s[currFrontierIdx_s];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The main BFS block of the kernel above will write to the local frontier as long as there is space. If the capacity is hit, all future writes will go to the global frontier.&lt;/p&gt;
&lt;p&gt;After BFS completes, a representative thread (index 0) from each block will allocate space in the global frontier, giving it a unique starting index. This allows each block to safely write to the global frontier without contention. The figure below shows the result of the privatized frontier.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-04-09_20-23-22_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Privatized frontier for BFS traversal (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Privatized frontier for BFS traversal (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;additional-optimizations&#34;&gt;Additional Optimizations&lt;/h2&gt;
&lt;h3 id=&#34;reducing-launch-overhead&#34;&gt;Reducing launch overhead&lt;/h3&gt;
&lt;p&gt;If the frontiers of a BFS are small, the overhead of launching a kernel for each level can be significant. In such cases, a kernel with a grid size of 1 can be launched to handle multiple levels. This block would synchronize after each level to ensure that all threads have completed the current level before moving on to the next.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-04-09_20-27-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Reducing launch overhead by handling multiple levels in a single kernel (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Reducing launch overhead by handling multiple levels in a single kernel (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;improving-load-balance&#34;&gt;Improving load balance&lt;/h3&gt;
&lt;p&gt;In the first vertex-centric approach we looked at, the threads were not evenly balanced due to the fact that some vertices have more edges than others. For graphs that have high variability in the number of edges per vertex, the frontier can be sorted and placed into multiple buckets. The buckets would be processed by a separate kernel.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Parallel Sorting Algorithms</title>
      <link>http://localhost:1313/notes/parallel_sorting_algorithms/</link>
      <pubDate>Sun, 31 Mar 2024 10:50:00 -0500</pubDate>
      
      <guid>http://localhost:1313/notes/parallel_sorting_algorithms/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#radix-sort&#34;&gt;Radix Sort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimizing-memory-access-efficiency&#34;&gt;Optimizing Memory Access Efficiency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#choosing-a-different-radix-value&#34;&gt;Choosing a different Radix value&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;radix-sort&#34;&gt;Radix Sort&lt;/h2&gt;
&lt;p&gt;For a background on Radix Sort, see these notes on &lt;a href=&#34;http://localhost:1313/notes/sorting_in_linear_time/&#34;&gt;Sorting in Linear Time&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Radix sort relies on counting sort for each section, and each section must be processed before moving onto the next. The parallel solution will not attempt to address this sequential dependency. Instead, we will focus on the parallelization of the counting sort step.&lt;/p&gt;
&lt;p&gt;Each thread must determine where to place its input elements. For each bit, the thread will assign it to either a 0 or 1 bucket. Since all values will either be 0 or 1, the thread needs to compute the number of 0s and 1s that come before it in the current section. Radix sort is also a stable sort, so the order of elements with the same key must be preserved. Consider the following array separated into 4 threads of 4 elements each:&lt;/p&gt;
&lt;p&gt;\begin{array}{l|cccc|cccc|cccc|cccc}
Value &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0\\
Index &amp;amp; 0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4 &amp;amp; 5 &amp;amp; 6 &amp;amp; 7 &amp;amp; 8 &amp;amp; 9 &amp;amp; 10 &amp;amp; 11 &amp;amp; 12 &amp;amp; 13 &amp;amp; 14 &amp;amp; 15\\
\end{array}&lt;/p&gt;
&lt;p&gt;The least significant bits for each thread are \([1, 0, 1, 0]\). From thread 4&amp;rsquo;s perspective, there are 2 1s and a single 0 that come before it. Its key index is 3 (using 0-based indexing), so it only needs to compute the number of 1s that come before it and subtract that from its key index: \(3 - 2 = 1\). More generally, for a 0 bit:&lt;/p&gt;
&lt;p&gt;\[
\text{output index} = \text{key index} - \text{number of 0s that come before it}
\]&lt;/p&gt;
&lt;p&gt;The calculation for the 1 bit hinges on the fact that all keys mapping to 0 must come before it.&lt;/p&gt;
&lt;p&gt;\[
\text{output index} = \text{input size} - \text{number of ones total} + \text{number of 1s that come before it}
\]&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void radix_sort_iter(unsigned int *input, unsigned int *output,
                                unsigned int *bits, unsigned int N, unsigned int iter) {
    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int key, bit;

    if (idx &amp;lt; N) {
        key = input[i];
        bit = (key &amp;gt;&amp;gt; iter) &amp;amp; 1;
        bits[i] = bit;
    }

    exclusiveScan(bits, N);

    if (idx &amp;lt; N) {
        unsigned int numOnesBefore = bits[idx];
        unsigned int numOnesTotal = bits[N];
        unsigned int dst = (bit == 0) ? idx - numOnesBefore
                                      : N - numOnesTotal + numOnesBefore;
        output[dst] = key;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Consider the fourth thread with &lt;code&gt;idx = 3&lt;/code&gt; using the array from above. This thread index is certainly less than the array size, so the &lt;code&gt;key&lt;/code&gt; is read from &lt;code&gt;input&lt;/code&gt; before extracting the least significant bit. &lt;strong&gt;Note that there is a call to thread synchronization inside &lt;code&gt;exclusiveScan&lt;/code&gt;.&lt;/strong&gt; The result of &lt;code&gt;exclusiveScan&lt;/code&gt; is an array that indicates, for each index, the number of ones that came before it. For our array, this is:&lt;/p&gt;
&lt;p&gt;\[
[0, 1, 1, 2]
\]&lt;/p&gt;
&lt;p&gt;The destination can be computed for each thread. The result is shown in the table below.&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Thread&lt;/th&gt;
          &lt;th&gt;0&lt;/th&gt;
          &lt;th&gt;1&lt;/th&gt;
          &lt;th&gt;2&lt;/th&gt;
          &lt;th&gt;3&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Bit&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;#1s Before&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;#1s Total&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Output Index&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;3&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;optimizing-memory-access-efficiency&#34;&gt;Optimizing Memory Access Efficiency&lt;/h2&gt;
&lt;p&gt;Each thread write their keys to global memory in an uncoalesced manner. This can be optimized by having each block maintain local buckets in shared memory. The keys within each block will be coalesced when written to global memory.&lt;/p&gt;
&lt;p&gt;TODO: Show visualization similar to 13.5&lt;/p&gt;
&lt;p&gt;In order to make this work, each thread needs to calculate where in the output array the values from its bucket should be placed. For 0 bits, the block&amp;rsquo;s 0 bucket will come after the 0 buckets from all previous blocks. These positions can be computed by performing an exclusive scan on the block&amp;rsquo;s local bucket sizes.&lt;/p&gt;
&lt;p&gt;TODO: Show visualization similar to 13.6&lt;/p&gt;
&lt;h2 id=&#34;choosing-a-different-radix-value&#34;&gt;Choosing a different Radix value&lt;/h2&gt;
&lt;p&gt;Picking a larger radix value will reduce the number of iterations required to sort the array.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sparse Matrix Computation</title>
      <link>http://localhost:1313/notes/sparse_matrix_computation/</link>
      <pubDate>Sat, 30 Mar 2024 10:36:00 -0500</pubDate>
      
      <guid>http://localhost:1313/notes/sparse_matrix_computation/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coordinate-list-format--coo&#34;&gt;Coordinate List Format (COO)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compressed-sparse-row-format--csr&#34;&gt;Compressed Sparse Row Format (CSR)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ell-format&#34;&gt;ELL Format&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ell-coo-format&#34;&gt;ELL-COO Format&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#jagged-diagonal-storage-format--jds&#34;&gt;Jagged Diagonal Storage Format (JDS)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Sparse matrices are matrices with mostly zero elements. They are common in scientific computing, machine learning, and other fields. It is important to study them in the context of GPU computing because they can be very large and require a lot of memory. Effeciently representing and computing with sparse matrices provides a substantial benefit to many applications.&lt;/p&gt;
&lt;p&gt;The obvious benefits of sparsity is that we can typically represent the same matrix using a smaller memory footprint. Fewer elements means fewer wasted operations as well. The challenges of GPU implementations are that the memory access patterns are not always friendly to the GPU architecture. This is especially true for sparse matrices, where the memory access patterns are often irregular.&lt;/p&gt;
&lt;p&gt;These notes will review the sparse matrix formats as presented in (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;). Each will be evaluated using the following criteria:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Compaction&lt;/strong&gt;: How well does the format compact the data?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexibility&lt;/strong&gt;: Is the format easy to modify?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accessibility:&lt;/strong&gt; How easy is it to access the data?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory access efficiency:&lt;/strong&gt; Are the accesses coalesced?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Load balance:&lt;/strong&gt; Are the operations balanced across threads?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;coordinate-list-format--coo&#34;&gt;Coordinate List Format (COO)&lt;/h2&gt;
&lt;p&gt;This format stores non-zero elements in a 1D array of values. It also requires two 1D arrays to store the row and column indices, incurrent an overhead of 2N. The values in each array are contiguous, which is good for memory access.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-04-02_19-42-49_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;COO Format (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;COO Format (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;kernel-implementation&#34;&gt;Kernel Implementation&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void spmv_coo_kernel(COOMatrix cooMatrix, float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &amp;lt; cooMatrix.numNonzeros) {
        int row = cooMatrix.rowIdx[i];
        int col = cooMatrix.colIdx[i];
        float val = cooMatrix.values[i];
        atomicAdd(&amp;amp;y[row], val * x[col]);
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Compaction&lt;/strong&gt;: Compared to representing the matrices in dense format, the COO format is very compact. However, it is not as compact as some other sparse matrix formats. It requires an additional over head of 2N elements to store the row and column indices.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; Indices and values can be easily modified in this format. This is good for applications that require frequent modifications.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accessibility:&lt;/strong&gt; It is easy to access nonzero elements. It is &lt;strong&gt;not&lt;/strong&gt; easy to access the original 0s in each row.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory access efficiency:&lt;/strong&gt; The values in this format are contiguous, resulting in coalesced memory access.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Load balance:&lt;/strong&gt; The data is uniformly distributed across threads, resulting in good load balance.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One major drawback, as seen in the code above, is the use of atomic operations.&lt;/p&gt;
&lt;h2 id=&#34;compressed-sparse-row-format--csr&#34;&gt;Compressed Sparse Row Format (CSR)&lt;/h2&gt;
&lt;p&gt;The key idea of this format is that each thread is responsible for all nonzeros in a row.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-04-02_19-50-47_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;CSR Format (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;CSR Format (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;kernel-implementation&#34;&gt;Kernel Implementation&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void spmv_csr_kernel(CSRMatrix csrMatrix, float *x, float *y) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    if (row &amp;lt; csrMatrix.numRows) {
        float sum = 0.0f;
        for (int j = csrMatrix.rowPtr[row]; j &amp;lt; csrMatrix.rowPtr[row + 1]; j++) {
            sum += csrMatrix.values[j] * x[csrMatrix.colIdx[j]];
        }
        y[i] = sum;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The rows are mapped to a single pointer index, so it only needs \(m\) entries to store them. The columns are not required to be in order. If the columns &lt;em&gt;are&lt;/em&gt; in order, the data is represented in row-major order without the zero elements.&lt;/p&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Compaction&lt;/strong&gt;: The CSR format is more compact than the COO format since it only requires \(m\) entries to store the row pointers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; The CSR format is not as flexible as the COO format. It is not easy to modify the values or indices.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accessibility:&lt;/strong&gt; There is less parallelization than COO due to the row sizes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory access efficiency:&lt;/strong&gt; The memory access pattern is poor since the data is separated over columns.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Load balance:&lt;/strong&gt; The load is not balanced across threads. Some threads will have more work than others, leading to control divergence.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;ell-format&#34;&gt;ELL Format&lt;/h2&gt;
&lt;p&gt;ELL fixes the non-coalesced memory accesses of CSR via data padding and transposition. This is visualized below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Start with CSR format&lt;/li&gt;
&lt;li&gt;Pad rows to equal size&lt;/li&gt;
&lt;li&gt;Store in column-major order&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-04-02_19-53-05_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;ELL Format (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;ELL Format (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;kernel-implementation&#34;&gt;Kernel Implementation&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void spmv_ell_kernel(ELLMatrix ellMatrix, float *x, float *y) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    if (row &amp;lt; ellMatrix.numRows) {
        float sum = 0.0f;
        for (int j = 0; j &amp;lt; ellMatrix.nnzPerRow[row]; j++) {
            int col = ellMatrix.colIdx[j * ellMatrix.numRows + row];
            sum += ellMatrix.values[j * ellMatrix.numRows + row] * x[col];
        }
        y[row] = sum;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Compaction:&lt;/strong&gt; Padding the rows means this is less space efficient than CSR.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; More flexible than CSR; adding nonzeros in CSR requires a shift of values. This format can replaced a padded element if necessary.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accessibility:&lt;/strong&gt; ELL can return the row given the index of a nonzero element as well as the nonzero of a row given that index.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory access efficiency:&lt;/strong&gt; Consecutive threads access consecutive memory locations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Load balance:&lt;/strong&gt; Shares the same control divergence issues as CSR.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;ell-coo-format&#34;&gt;ELL-COO Format&lt;/h2&gt;
&lt;p&gt;ELL-COO combines the two formats to improve space efficiency and control divergence.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-04-02_20-01-43_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;ELL-COO Format (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;ELL-COO Format (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Compaction:&lt;/strong&gt; ELL-COO has the same compaction as ELL.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; ELL-COO is more flexible than ELL thanks to inclusion of the COO format.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accessibility:&lt;/strong&gt; It is not always possible to access all nonzeros given a row index.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory access efficiency:&lt;/strong&gt; The memory access pattern is coalesced.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Load balance:&lt;/strong&gt; COO reduces the control divergence seen in ELL alone.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;jagged-diagonal-storage-format--jds&#34;&gt;Jagged Diagonal Storage Format (JDS)&lt;/h2&gt;
&lt;p&gt;The last format we will consider is the Jagged Diagonal Storage format. This format reduces divergence and improves memory coalescing without padding. The main idea is to sort the rows by length from longest to shortest.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Group nonzeros by row&lt;/li&gt;
&lt;li&gt;Sort rows by length while preserving their original row indices&lt;/li&gt;
&lt;li&gt;Store in column-major order&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-04-02_20-07-30_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;JDS Format (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;JDS Format (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Compaction:&lt;/strong&gt; Avoid paddding, so it is more space efficient than ELL.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; Less flexible than ELL since it requires sorting when adding new elements.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accessibility:&lt;/strong&gt; Cannot access a row and column given the index of a nonzero element.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory access efficiency:&lt;/strong&gt; Without padding, the starting location of memory accesses in each iteration can vary.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Load balance:&lt;/strong&gt; Since the rows are sorted, threads of the same warp are likely to iterate over rows of similar length.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GPU Pattern: Merge</title>
      <link>http://localhost:1313/notes/gpu_pattern_merge/</link>
      <pubDate>Wed, 28 Feb 2024 19:19:00 -0600</pubDate>
      
      <guid>http://localhost:1313/notes/gpu_pattern_merge/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#key-concepts-and-challenges&#34;&gt;Key Concepts and Challenges&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-merge-operation&#34;&gt;The Merge Operation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tiled-merge&#34;&gt;Tiled Merge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#circular-buffers&#34;&gt;Circular Buffers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#thread-coarsening&#34;&gt;Thread Coarsening&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;key-concepts-and-challenges&#34;&gt;Key Concepts and Challenges&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Dynamic input data identification&lt;/li&gt;
&lt;li&gt;Data locality&lt;/li&gt;
&lt;li&gt;Buffer management schemes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;merge&lt;/strong&gt; operation takes two sorted subarrays and combines them into a single sorted array. You may be familiar with this approach from studying &lt;a href=&#34;http://localhost:1313/notes/divide_and_conquer_algorithms/&#34;&gt;Divide and Conquer Algorithms&lt;/a&gt;. Parallelizing the merge operation is a non-trivial task and will require the use of a few new techniques.&lt;/p&gt;
&lt;h2 id=&#34;the-merge-operation&#34;&gt;The Merge Operation&lt;/h2&gt;
&lt;p&gt;The specific parallel merge operation studied in these notes is from &amp;ldquo;Perfectly load-balanced, optimal, stable, parallel merge&amp;rdquo; (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Siebert and Trff 2013&lt;/a&gt;). Their approach works by first computing which values are needed in each merge step, and then using a parallel kernel to compute the merge. These steps can be computed by each thread independently.&lt;/p&gt;
&lt;h3 id=&#34;co-rank-function&#34;&gt;Co-rank Function&lt;/h3&gt;
&lt;p&gt;The key to the parallel merge algorithm reviewed in these notes is the &lt;strong&gt;co-ranking function&lt;/strong&gt; (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Siebert and Trff 2013&lt;/a&gt;). This function computes the range of indices needed from the two input values to produce a given value in the output array, without actually needing to merge the two input arrays.&lt;/p&gt;
&lt;p&gt;When merging two sorted arrays, we can observe that the output index \(0 \leq k &amp;lt; m + n\) comes from either \(0 \leq i &amp;lt; m\) from input \(A\) or \(0 \leq j &amp;lt; n\) from input \(B\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-03-11_14-22-24_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Merging (A) and (B) to produce (C).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Merging (A) and (B) to produce (C).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the figure above, the element at \(k = 3\) comes from \(A[2]\), so \(i = 2\). It must be that \(k=3\) is the result of merging the first \(i=2\) elements of \(A\) with the first \(j=k - i\) elements of \(B\). This works both ways: for \(k=6\), the value is taken from \(B[3]\), so \(j=3\), and the result is the merge of the first \(i=k-j\) elements of \(A\) with the first \(j=3\) elements of \(B\).&lt;/p&gt;
&lt;p&gt;An efficient method for computing the co-rank function follows the first lemma put forth in (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Siebert and Trff 2013&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;strong&gt;Lemma 1&lt;/strong&gt;&lt;/strong&gt;. For any \(k, 0 \leq k &amp;lt; m + n\), there exists a unique \(i, 0 \leq i \leq m\), and a unique \(j, 0 \leq j \leq n\), with \(i + j = k\) such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;\(i = 0\) or \(A[i-1] \leq B[j]\) and&lt;/li&gt;
&lt;li&gt;\(j = 0\) or \(B[j-1] &amp;lt; A[i]\).&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-03-11_13-58-23_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Co-rank function visualization (&amp;lt;a href=&amp;#34;#citeproc_bib_item_2&amp;#34;&amp;gt;Siebert and Trff 2013&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Co-rank function visualization (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Siebert and Trff 2013&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;implementation&#34;&gt;Implementation&lt;/h4&gt;
&lt;p&gt;Given the rank \(k\) of an element in an output array \(C\) and two input arrays \(A\) and \(B\), the co-rank function \(f\) returns the co-rank value for the corresponding element in \(A\) and \(B\).&lt;/p&gt;
&lt;p&gt;How would the co-rank function be used in the example above? Given two threads, let thread 1 compute the co-rank for \(k=4\). This would return \(i=3\) and \(j=1\). We quickly verify that this passes the first lemma stated above.&lt;/p&gt;
&lt;p&gt;\[
A[2] = 5 \leq B[1] = 5$ and $B[0] = 3 &amp;lt; A[3] = 7.
\]&lt;/p&gt;
&lt;p&gt;Code for the co-rank function is given below. Since the input arrays are already sorted, we can use a binary search to find the co-rank values.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;co_rank&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; k, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;A, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; m, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;B, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(k, m);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; i;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i_low &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, k&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;n);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; j_low &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, k&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;m);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; delta;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;bool&lt;/span&gt; active &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; true;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; (active) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; A[i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; B[j]) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            delta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (i &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; i_low &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            j_low &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; j;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            i &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; delta;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            j &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; delta;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        } &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (j &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; m &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; B[j&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; A[i]) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            delta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (j &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; j_low &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            i_low &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            j &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; delta;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            i &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; delta;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        } &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            active &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; false;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; i;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Consider running a merge kernel across 3 threads where each thread takes 3 sequential output values. Use the co-rank function to compute the co-rank values for \(k=3\) and \(k=6\), simulating the tasks for the second and third threads. The values for \(k=3\) should be \(i=2\) and \(j=1\), for reference. All values below these indices would be used by the first thread.&lt;/p&gt;
&lt;h3 id=&#34;parallel-kernel&#34;&gt;Parallel Kernel&lt;/h3&gt;
&lt;p&gt;We can now implement a basic parallel merge kernel. Each thread is responsible for determining how many elements it will be responsible for merging. The range of input values is determined via two calls to &lt;code&gt;co_rank&lt;/code&gt;, one for the starting and ending point.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;__global__ &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;merge_basic_kernel&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;A, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; m, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;B, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;C) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; tid &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; blockIdx.x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; blockDim.x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; threadIdx.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; elementsPerThread &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ceil((m &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; n) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (blockDim.x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; gridDim.x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; k_curr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tid &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; elementsPerThread; &lt;span style=&#34;color:#75715e&#34;&gt;// start output index
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; k_next &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min((tid &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; elementsPerThread, m &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; n); &lt;span style=&#34;color:#75715e&#34;&gt;// end output index
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i_curr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; co_rank(k_curr, A, m, B, n);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i_next &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; co_rank(k_next, A, m, B, n);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; j_curr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; k_curr &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; i_curr;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; j_next &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; k_next &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; i_next;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    merge_sequential(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;A[i_curr], i_next &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; i_curr, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;B[j_curr], j_next &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; j_curr, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;C[k_curr]);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Two major issues should be clear from the code above. First, the memory being accesses is not coalesced. The binary search in &lt;code&gt;co_rank&lt;/code&gt; also means that the memory access pattern is less than ideal. Since the main issue in both cases relates to memory efficiency, we should look at tools that address memory access patterns.&lt;/p&gt;
&lt;h2 id=&#34;tiled-merge&#34;&gt;Tiled Merge&lt;/h2&gt;
&lt;p&gt;The memory access pattern is sparse and thus does not take advantage of coalescing. We can improve upon this by having the threads transfer data from global memory to shared memory in a coalesced manner. That way the higher latency operation will be coalesced. The data in shared memory may be accessed out of order, but the latency is much lower.&lt;/p&gt;
&lt;p&gt;The subarrays from \(A\) and \(B\) that are used by adjacent threads are also adjacent in memory. By considering block-level subarrays, we can ensure that the data is coalesced. This is the idea behind the tiled merge algorithm. The figure below visualizes this concept.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-03-16_13-22-25_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Design of a tiled merge kernel (recreated from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;)).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Design of a tiled merge kernel (recreated from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;)).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The shared memory blocks \(A_s\) and \(B_s\) obviously cannot store the entire range of data needed. In each iteration, the threads in a block will load a new set of data from global memory to shared memory. The light gray section of the block from \(A\) and \(B\) are loaded into \(A_s\) and \(B_s\), respectively. If they collectively load \(2n\) elements, only \(n\) elements will be used in the merge operation. This is because in the worst case, all elements going to the output array will come from one of the two input arrays. See the exercise at the end of this section for a more detailed explanation.&lt;/p&gt;
&lt;p&gt;Each block will use a portion of both \(A_s\) and \(B_s\) to compute the merge. This is shown with dotted lines going from the shared memory to the output array.&lt;/p&gt;
&lt;h3 id=&#34;part-1&#34;&gt;Part 1&lt;/h3&gt;
&lt;p&gt;The code below shows the first part of the tiled merge kernel.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void merge_tiled_kernel(int *A, int m, int n, int *C, int tile_size) {
    extern __shared__ int shareAB[];
    int *A_s = &amp;amp;shareAB[0];
    int *B_s = &amp;amp;shareAB[tile_size];
    int C_curr = blockIdx.x * ceil((m+n)/gridDim.x);
    int C_next = min((blockIdx.x+1) * ceil((m+n)/gridDim.x), m+n);

    if (threadIdx.x == 0) {
        // Block-level co-rank values will be available to all threads in the block
        A_s[0] = co_rank(C_curr, A, m, B, n);
        A_s[1] = co_rank(C_next, A, m, B, n);
    }
    __syncthreads();

    int A_curr = A_s[0];
    int A_next = A_s[1];
    int B_curr = C_curr - A_curr;
    int B_next = C_next - A_next;
    __syncthreads();
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The first part establishes the shared memory and the co-rank values for the &lt;strong&gt;block&lt;/strong&gt;. Each thread will have access to the start and end values for input matrices \(A\) and \(B\) as well. If the kernel is just getting started, we would have that &lt;code&gt;A_curr = 0&lt;/code&gt;, &lt;code&gt;B_curr = 0&lt;/code&gt;, and &lt;code&gt;C_curr = 0&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;part-2&#34;&gt;Part 2&lt;/h3&gt;
&lt;p&gt;The second part of the kernel is responsible for loading the input data into shared memory. This is done in a coalesced manner, as the threads in a block will load a contiguous section of the input arrays.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;int counter = 0;
int C_length = C_next - C_curr;
int A_length = A_next - A_curr;
int B_length = B_next - B_curr;
int total_iteration = ceil(C_length / tile_size);
int C_completed = 0;
int A_consumed = 0;
int B_consumed = 0;
while (counter &amp;lt; total_iteration) {
    for (int i = 0; i &amp;lt; tile_size; i += blockDim.x) {
        if (i + threadIdx.x &amp;lt; A_length - A_consumed) {
            A_s[i + threadIdx.x] = A[A_curr + A_consumed + i + threadIdx.x];
        }
        if (i + threadIdx.x &amp;lt; B_length - B_consumed) {
            B_s[i + threadIdx.x] = B[B_curr + B_consumed + i + threadIdx.x];
        }
    }
    __syncthreads();
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;part-3&#34;&gt;Part 3&lt;/h3&gt;
&lt;p&gt;With the input in shared memory, each thread will divide up this input and merge their respective sections in parallel. This is done by calculating the &lt;code&gt;c_curr&lt;/code&gt; and &lt;code&gt;c_next&lt;/code&gt; first, which is the output section of the thread. Using those boundaries, two calls to &lt;code&gt;co_rank&lt;/code&gt; will determine the input sections the thread.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;        int c_curr = threadIdx.x * (tile_size / blockDim.x);
        int c_next = (threadIdx.x + 1) * (tile_size / blockDim.x);
        c_curr = (c_curr &amp;lt;= C_length - C_completed) ? c_curr : C_length - C_completed;
        c_next = (c_next &amp;lt;= C_length - C_completed) ? c_next : C_length - C_completed;

        int a_curr = co_rank(c_curr,
                             A_s, min(tile_size, A_length - A_consumed),
                             B_s, min(tile_size, B_length - B_consumed));
        int b_curr = c_curr - a_curr;
        int a_next = co_rank(c_next,
                             A_s, min(tile_size, A_length - A_consumed),
                             B_s, min(tile_size, B_length - B_consumed));
        int b_next = c_next - a_next;

        merge_sequential(&amp;amp;A_s[a_curr], a_next - a_curr, &amp;amp;B_s[b_curr], b_next - b_curr, &amp;amp;C[C_urr + C_completed + c_curr]);
        counter++;
        C_completed += tile_size;
        A_consumed += co_rank(tile_size A_s, tile_size, B_s, tile_size);
        B_consumed = C_completed - A_consumed;
        __syncthreads();
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;example-walkthrough-of-kernel&#34;&gt;Example: Walkthrough of Kernel&lt;/h3&gt;
&lt;p&gt;Consider the following example. We have two input arrays \(A = [1, 3, 5, 7, 9]\) and \(B = [2, 4, 6, 8, 10]\). The output array \(C\) will have 10 elements. We will use 2 blocks and 4 threads per block. The tile size is 4. With 10 elements and 2 blocks, each block is responsible for 5 elements.&lt;/p&gt;
&lt;p&gt;The main &lt;code&gt;while&lt;/code&gt; loop will need to iterate twice to cover the entire output array. The first iteration will load the first 4 elements of \(A\) and \(B\) into shared memory. Once the data is in memory, each thread divides the input tiles by running the co-rank function on the data that is in shared memory. The computed indices are the boundaries between each thread.&lt;/p&gt;
&lt;p&gt;In each iteration, a block is responsible for 4 elements. Given that we have 4 threads per block, each thread will be responsible for 1 output element per iteration. For thread 0 we have that &lt;code&gt;c_curr = 0&lt;/code&gt; and &lt;code&gt;c_next = 2&lt;/code&gt;. This results in &lt;code&gt;a_curr = 0&lt;/code&gt;, &lt;code&gt;b_curr = 0&lt;/code&gt;, &lt;code&gt;a_next = 1&lt;/code&gt;, and &lt;code&gt;b_next = 1&lt;/code&gt;. The merge operation will then be performed on the first element of \(A\) and \(B\).&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Coalesces global memory accesses&lt;/li&gt;
&lt;li&gt;Shared memory is used to reduce latency&lt;/li&gt;
&lt;li&gt;Only half the data loaded into shared memory is used; wasted memory bandwidth&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;exercise&#34;&gt;Exercise&lt;/h3&gt;
&lt;p&gt;Hwu et al. suggest that you can first call the co-rank function to get the current and next output sections. This would increase memory bandwidth at the cost of an additional binary search.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Where would this be done with respect to the tiled solution discussed in this section?&lt;/li&gt;
&lt;li&gt;How do these co-rank values differ from the ones used to calculate &lt;code&gt;C_curr&lt;/code&gt; and &lt;code&gt;C_next&lt;/code&gt;?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Hint:&lt;/strong&gt; If we knew the co-rank value for the start of the next section, we could ensure that only the data below that index is loaded into shared memory.&lt;/p&gt;
&lt;h2 id=&#34;circular-buffers&#34;&gt;Circular Buffers&lt;/h2&gt;
&lt;p&gt;The tiled merge algorithm is a significant improvement over the basic merge kernel. One glaring issue is that only half of the data loaded into shared memory is used, leading to a waste of memory bandwidth. The circular buffer merge algorithm addresses this issue by using a circular buffer to store the input data. Instead of writing over the shared memory values on each iteration, the data to be used in the next iteration stays in shared memory. A portion of new data is loaded into shared memory based on how much was used in the previous iteration.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-03-17_14-58-19_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Circular buffer scheme (recreated from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;)).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Circular buffer scheme (recreated from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;)).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The figure above outlines the main idea behind the circular buffer merge algorithm. Part A shows the initial data layout of global and shared memory. Only a portion of the data loaded into shared memory is used in the merge operation. This is shown in part B, where the blank portion of the shared memory depicts the data that was used. The light gray regions of shared memory are the left over data that can be used in the next iteration.&lt;/p&gt;
&lt;p&gt;The next block of data is loaded into shared memory. Since some of the required data already exists from the last iteration, a smaller portion needs to be loaded. This is shown in part C, where the new data (dark gray) is loaded into shared memory. The starting indices for both arrays was already set in the previous iteration. Consecutive values are simple to calculate using the mod operator.&lt;/p&gt;
&lt;p&gt;Part D shows the state of the arrays after the end of the second iteration. The blank areas in shared memory are the data that was used in the merge operation. For array &lt;code&gt;A_S&lt;/code&gt;, the index wrapped around to the beginning of the array. It is ready to be used in the next iteration.&lt;/p&gt;
&lt;h3 id=&#34;implementation&#34;&gt;Implementation&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;A_consumed&lt;/code&gt; can be used to keep track of how many new elements need to be read into shared memory.
The &lt;code&gt;co_rank&lt;/code&gt; and &lt;code&gt;merge_sequential&lt;/code&gt; functions need to be updated to work with circular buffers. It is easier to treat the shared memory as an extended array, that way we avoid situations where the &lt;code&gt;next&lt;/code&gt; index is less than the &lt;code&gt;current&lt;/code&gt; index.&lt;/p&gt;
&lt;h4 id=&#34;co-rank-function&#34;&gt;Co-Rank Function&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;int co_rank_circular(int k, int *A, int m, int *B, int n, int A_S_start, int B_S_start, int tile_length) {
    int i = min(k, m);
    int j = k - i;
    int i_low = max(0, k-n);
    int j_low = max(0, k-m);
    int delta;
    bool active = true;
    while (active) {
        int i_cir = (A_S_start + i) % tile_length;
        int j_cir = (B_S_start + j) % tile_length;
        int i_m_1_cir = (A_S_start + i - 1) % tile_length;
        int j_m_1_cir = (B_S_start + j - 1) % tile_length;
        if (i &amp;gt; 0 &amp;amp;&amp;amp; j &amp;lt; n &amp;amp;&amp;amp; A[i_m_1_cir] &amp;gt; B[j_cir]) {
            delta = ((i - i_low + 1) &amp;gt;&amp;gt; 1);
            j_low = j;
            i -= delta;
            j += delta;
        } else if (j &amp;gt; 0 &amp;amp;&amp;amp; i &amp;lt; m &amp;amp;&amp;amp; B[j_m_1_cir] &amp;gt;= A[i_cir]) {
            delta = ((j - j_low + 1) &amp;gt;&amp;gt; 1);
            i_low = i;
            j -= delta;
            i += delta;
        } else {
            active = false;
        }
    }
    return i;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In this updated version of the co-rank function, the user only needs to provide the start indices for the shared memory arrays along with the tile size.&lt;/p&gt;
&lt;h4 id=&#34;merge-sequential&#34;&gt;Merge Sequential&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;void merge_sequential_circular(int *A, int m, int *B, int n, int *C, int A_S_start, int B_S_start, int tile_size) {
    int i = 0;
    int j = 0;
    int k = 0;
    while (i &amp;lt; m &amp;amp;&amp;amp; j &amp;lt; n) {
        int i_cir = (A_S_start + i) % tile_size;
        int j_cir = (B_S_start + j) % tile_size;
        if (A[i_cir] &amp;lt;= B[j_cir]) {
            C[k] = A[i_cir];
            i++;
        } else {
            C[k] = B[j_cir];
            j++;
        }
        k++;
    }
    if (i == m) {
        while (j &amp;lt; n) {
            int j_cir = (B_S_start + j) % tile_size;
            C[k] = B[j_cir];
            j++;
            k++;
        }
    } else {
        while (i &amp;lt; m) {
            int i_cir = (A_S_start + i) % tile_size;
            C[k] = A[i_cir];
            i++;
            k++;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Again, this revision makes it easier on the user since they only need to provide the start indices for the shared memory arrays and the tile size. These are used to compute the indices for the circular buffer.&lt;/p&gt;
&lt;h4 id=&#34;circular-buffer-kernel&#34;&gt;Circular Buffer Kernel&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;        int c_curr = threadIdx.x * (tile_size / blockDim.x);
        int c_next = (threadIdx.x + 1) * (tile_size / blockDim.x);
        c_curr = (c_curr &amp;lt;= C_length - C_completed) ? c_curr : C_length - C_completed;
        c_next = (c_next &amp;lt;= C_length - C_completed) ? c_next : C_length - C_completed;

        int a_curr = co_rank_circular(c_curr,
                                      A_s, min(tile_size, A_length - A_consumed),
                                      B_s, min(tile_size, B_length - B_consumed),
                                      A_curr, B_curr, tile_size);
        int b_curr = c_curr - a_curr;
        int a_next = co_rank_circular(c_curr,
                                      A_s, min(tile_size, A_length - A_consumed),
                                      B_s, min(tile_size, B_length - B_consumed),
                                      A_curr, B_curr, tile_size);
        int b_next = c_next - a_next;

        merge_sequential_circular(A_s, a_next - a_curr, B_s, b_next - b_curr, &amp;amp;C[C_urr + C_completed + c_curr],
                                  A_S_start + A_curr, B_S_start + B_curr, tile_size);

        // Compute the indices that were used
        counter++;
        A_S_consumed = co_rank_circular(min(tile_size, C_length - C_completed),
                                        A_s, min(tile_size, A_length - A_consumed),
                                        B_s, min(tile_size, B_length - B_consumed),
                                        A_S_start, B_S_start, tile_size);
        B_S_consumed = min(tile_size, C_length - C_completed) - A_S_consumed;
        A_consumed += A_S_consumed;
        C_completed += min(tile_size, C_length - C_completed);
        B_consumed = C_completed - A_consumed;

        // Update the start indices for the next iteration
        A_S_start = (A_S_start + A_S_consumed) % tile_size;
        B_S_start = (B_S_start + B_S_consumed) % tile_size;
        __syncthreads();
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The first section of part 3 of the original kernel remains mostly unchanged, with the exceptions that the co-rank function and merge function are now called with the circular versions. The larger change is in the second half of the kernel. &lt;code&gt;A_S_consumed&lt;/code&gt; and &lt;code&gt;B_S_consumed&lt;/code&gt; are used to keep track of how much of the shared memory arrays were used. This is then used to offset the used indices from the original arrays. Finally, the start indices for the shared memory arrays are updated for the next iteration.&lt;/p&gt;
&lt;h2 id=&#34;thread-coarsening&#34;&gt;Thread Coarsening&lt;/h2&gt;
&lt;p&gt;The kernels presented in these notes already utilize thread coarsening. Each thread is responsible for a range of output values. The simple example presented earlier demonstrates what a non-coarse approach would look like. Each thread was only responsible for a single output value.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Siebert, Christian, and Jesper Larsson Trff. 2013. Perfectly Load-Balanced, Optimal, Stable, Parallel Merge. arXiv. &lt;a href=&#34;http://arxiv.org/abs/1303.4312&#34;&gt;http://arxiv.org/abs/1303.4312&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GPU Pattern: Parallel Scan</title>
      <link>http://localhost:1313/notes/gpu_pattern_parallel_scan/</link>
      <pubDate>Wed, 14 Feb 2024 20:09:00 -0600</pubDate>
      
      <guid>http://localhost:1313/notes/gpu_pattern_parallel_scan/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-it&#34;&gt;What is it?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#naive-parallel-reduction&#34;&gt;Naive Parallel Reduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kogge-stone-algorithm&#34;&gt;Kogge-Stone Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#brent-kung-algorithm&#34;&gt;Brent-Kung Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-coarsening&#34;&gt;Adding Coarsening&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#segmented-parallel-scan&#34;&gt;Segmented Parallel Scan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimizing-memory-efficiency&#34;&gt;Optimizing Memory Efficiency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-takeaway&#34;&gt;The Takeaway&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;what-is-it&#34;&gt;What is it?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Parallelizes sequential problems.&lt;/li&gt;
&lt;li&gt;Works with computations that can be described in terms of a recursion.&lt;/li&gt;
&lt;li&gt;Used as a primitive operation for sorting, tree operations, and recurrences.&lt;/li&gt;
&lt;li&gt;Studying this will also reveal how parallelization can increase the complexity beyond that of a traditional sequential approach.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example-inclusive-scan&#34;&gt;Example: Inclusive Scan&lt;/h3&gt;
&lt;p&gt;Given an array of numbers, the inclusive scan computes the sum of all elements up to a given index. For example, given the array [1, 2, 3, 4, 5], the inclusive scan would produce [1, 3, 6, 10, 15]. You could solve this recursively, but it would be horribly inefficient. A sequential solution is achievable with dynamic programming. However, a parallel solution is much more efficient.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sequential_scan&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x, &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;y, uint N) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (uint i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; N; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        y[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y[i &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; x[i];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;naive-parallel-reduction&#34;&gt;Naive Parallel Reduction&lt;/h2&gt;
&lt;p&gt;If we have \(n\) elements, we could have \(n\) threads each compute the sum of a single element. How many operations would that take? The first thread computes the sum of 1 element, or 0 operations. The second thread computes the sum of 2 elements, 1 operation, and so on. This can be described as a sum of the first \(n\) natural numbers, which is \(n(n + 1)/2\). This parallel solution is worse than the sequential solution, coming in at \(O(n^2)\).&lt;/p&gt;
&lt;h2 id=&#34;kogge-stone-algorithm&#34;&gt;Kogge-Stone Algorithm&lt;/h2&gt;
&lt;p&gt;The first solution to this problem relies on &lt;a href=&#34;http://localhost:1313/notes/gpu_pattern_reduction/&#34;&gt;GPU Pattern: Reduction&lt;/a&gt; and is called the Kogge-Stone algorithm. The algorithm was published in 1973 by Peter M. Kogge and Harold S. Stone during their time at Stanford University.&lt;/p&gt;
&lt;h3 id=&#34;adapting-the-reduction-tree&#34;&gt;Adapting the Reduction Tree&lt;/h3&gt;
&lt;p&gt;Design reduction tree so that each thread has access to relevant inputs. The input matrix is modified to that input \(A_i\) contains the sum of up to \(2^k\) elements after \(k\) iterations. For example, after iteration 2, \(A_3\) contains the sum \(A_0 + A_1 + A_2 + A_3\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-02-19_18-08-45_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Visualization of parallel inclusive scan based on the Kogge-Stone algorithm (Source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Visualization of parallel inclusive scan based on the Kogge-Stone algorithm (Source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This is implemented in the following code:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void Kogge_Stone_scan_kernel(float *X, float *Y, unsigned int N) {
        __shared__ float A[SECTION_SIZE];
        unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;

        if (i &amp;lt; N) {
            A[threadIdx.x] = X[i];
        } else {
            A[threadIdx.x] = 0;
        }

        for (unsigned int stride = 1; stride &amp;lt; blockDim.x; stride *= 2) {
            __syncthreads();
            float temp;
            if (threadIdx.x &amp;gt;= stride) {
                temp = A[threadIdx.x] + A[threadIdx.x - stride];
            }
            __syncthreads();
            if (threadIdx.x &amp;gt;= stride) {
                A[threadIdx.x] = temp;
            }
        }

        if (i &amp;lt; N) {
            Y[i] = A[threadIdx.x];
        }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It is not possible to guarantee that a block can cover the entire input, so &lt;code&gt;SECTION_SIZE&lt;/code&gt; is used to ensure that the input is covered. This should be the same as the block size. Each thread starts off by loading its initial input into shared memory. Starting with thread 2, each thread computes a sum of the value assigned to its thread as well as the one before it by a factor of &lt;code&gt;stride&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The loop itself is moving &lt;em&gt;down&lt;/em&gt; the reduction tree, which is bounded logarithmically. The local variable &lt;code&gt;temp&lt;/code&gt; is used to store the intermediate result before barrier synchronization takes place. Otherwise there would be a possibility of a &lt;em&gt;write-after-read&lt;/em&gt; race condition.&lt;/p&gt;
&lt;h3 id=&#34;double-buffering&#34;&gt;Double Buffering&lt;/h3&gt;
&lt;p&gt;The temporary variable and second call to &lt;code&gt;__syncthreads()&lt;/code&gt; are necessary since a thread may read from a location that another thread is writing to. If the input and output arrays were represented by two different areas of shared memory, this call could be removed. This approach is called &lt;strong&gt;double-buffering&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;It works as follows: the input array is read from global memory into shared memory. At each iteration the data read from the first array is used to write new values to the second array. Since the values used in that iteration are only read from the first array, the second array can be used as the input array for the next iteration. This cycle continues back and forth until the final result is written to the output array.&lt;/p&gt;
&lt;h3 id=&#34;efficiency-analysis&#34;&gt;Efficiency Analysis&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Work efficiency&lt;/strong&gt; is a measure of how efficient a particular algorithm is compared to the minimum number of operations required. For inclusive scan, the minimum number of add operations required is \(n - 1\), yielding an efficiency of \(O(n)\). During each iteration of the Kogge-Stone algorithm, each thread iterates over a loop that is logarithmic in size. That is, it starts with a stride of 2, then 4, 8, \(\dots \frac{n}{2}\). This yields a complexity of \(O(n \log_2 n)\).&lt;/p&gt;
&lt;p&gt;Due to the parallel nature of the algorithm, it still requires fewer steps than the sequential algorithm. The details muddy the water a bit as threads that stop execution early still expend resources from the device. In general, we can say that the parallel algorithm takes \(\frac{1}{m} n \log_2 n\) steps, where \(m\) is the number of execution units. If we have as many execution units as we have elements, then we only need \(\log_2 n\) steps.&lt;/p&gt;
&lt;h2 id=&#34;brent-kung-algorithm&#34;&gt;Brent-Kung Algorithm&lt;/h2&gt;
&lt;p&gt;Sharing intermediate results
Distribute to different threads
Reduction tree
Sub-sums used to calculate some of the scan output values
Brent-Kung follows the same idea as Kogge-Stone, but with better work efficiency.&lt;/p&gt;
&lt;p&gt;Use reduction tree on the first \(n/2\) elements, then use the results to reverse the tree.&lt;/p&gt;
&lt;p&gt;At the start of the reverse direction, the elements with index \(2^i - 1\), for \(i = 1, 2, \dots, \log_2 n\), already have the correct value.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-02-18_18-31-36_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Visualization of parallel inclusive scan based on the Brent-Kung algorithm (Source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Visualization of parallel inclusive scan based on the Brent-Kung algorithm (Source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The figure above shows the state of the algorithm before the reverse direction begins. The second half is better seen as a table of values. The row labeled &lt;code&gt;Initial&lt;/code&gt; contains the state of the array after the first half of the algorithm is completed. For the values that already have their correct value, no update is needed. The two rows following &lt;code&gt;Initial&lt;/code&gt; show the state of the array after the first and second iterations of the reverse direction.&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;0&lt;/th&gt;
          &lt;th&gt;1&lt;/th&gt;
          &lt;th&gt;2&lt;/th&gt;
          &lt;th&gt;3&lt;/th&gt;
          &lt;th&gt;4&lt;/th&gt;
          &lt;th&gt;5&lt;/th&gt;
          &lt;th&gt;6&lt;/th&gt;
          &lt;th&gt;7&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Initial&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&amp;hellip;1&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;0&amp;hellip;3&lt;/td&gt;
          &lt;td&gt;4&lt;/td&gt;
          &lt;td&gt;4&amp;hellip;5&lt;/td&gt;
          &lt;td&gt;6&lt;/td&gt;
          &lt;td&gt;0&amp;hellip;7&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Iteration 1&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;0&amp;hellip;5&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Iteration 2&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;0&amp;hellip;2&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;0&amp;hellip;4&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;0&amp;hellip;6&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;implementing-the-forward-half&#34;&gt;Implementing the Forward Half&lt;/h3&gt;
&lt;p&gt;The relevant reduction tree phase of Brent-Kung is implemented in CUDA C++ below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;for (uint stride = 1; stride &amp;lt;= blockDim.x; stride *= 2) {
    __syncthreads();
    if ((threadIdx.x + 1) % (stride * 2) == 0) {
        A[threadIdx.x] += A[threadIdx.x - stride];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;From the perspective of &lt;code&gt;threadIdx.x = 7&lt;/code&gt;, the first iteration would add the value from &lt;code&gt;threadIdx.x = 6&lt;/code&gt; to its own value. On the next iteration, the stride offset would add &lt;code&gt;threadIdx.x = 5&lt;/code&gt;. Thread 5 already has the sum from 4 and 5 before being added to 7, so 7 now has the sums from indices 4 through 7. On its last iteration, the value for stride is now 4, and 7 adds the value from 3 to its own value. This is the final result for the first half of the algorithm.&lt;/p&gt;
&lt;p&gt;There is a lot of control divergence present in this code. Since fewer threads stay active as the loop goes on, it is better to organize the threads such that they are contiguous. We can do that with slightly more complicated indexing, so that contiguous threads use data from the active portions of the array.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;for (uint stride = 1; stride &amp;lt;= blockDim.x; stride *= 2) {
    __syncthreads();
    int index = (threadIdx.x + 1) * 2 * stride - 1;
    if (index &amp;lt; blockDim.x) {
        A[index] += A[index - stride];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Thread 0 maps to index, thread 1 maps to index 3, thread 2 to index 5, and so on. Only when the number of active threads drops below the warp size does control divergence become a problem.&lt;/p&gt;
&lt;h3 id=&#34;implementing-the-reverse-half&#34;&gt;Implementing the Reverse Half&lt;/h3&gt;
&lt;p&gt;The reverse half of the algorithm is implemented in CUDA C++ below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;for (uint stride = blockDim.x / 4; stride &amp;gt; 0; stride /= 2) {
    __syncthreads();
    int index = (threadIdx.x + 1) * 2 * stride - 1;
    if (index + stride &amp;lt; blockDim.x) {
        A[index + stride] += A[index];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Continuing the example from above, the first thread &lt;code&gt;threadIdx.x = 0&lt;/code&gt; maps to index 3. This will load the value from index 3 and add it to the value at index 5. At that point, the value at index 5 will be the sum of the values from indices 0 through 5.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-03-19_22-19-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Full Brent-Kung visualization (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Full Brent-Kung visualization (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;efficiency-analysis&#34;&gt;Efficiency Analysis&lt;/h3&gt;
&lt;p&gt;The reduction tree of the first half of the algorithm require \(n - 1\) operations. For \(n\) elements, the reverse half requires \((2 - 1) + (4 - 1) + \dots + (n/2 - 1)\) operations for a total of \(N - 1 - \log_2 n\). This yields a work efficiency of \(O(n)\). Even though the theoretical work efficiency is better than Kogge-Stone, doesn&amp;rsquo;t mean its performance will always be better in practice. The drop off in active threads for Brent-Kung is much more severe than Kogge-Stone. It also requires additional steps to perform the reverse half. In general, Kogge-Stone is a better choice when we have more execution units, owing to its better parallelism.&lt;/p&gt;
&lt;p&gt;The full code is given below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void Brent_Kung_scan_kernel(float *X, float *Y, uint N) {
    __shared__ float A[SECTION_SIZE];
    unsigned int i = 2 * blockIdx.x * blockDim.x + threadIdx.x;

    if (i &amp;lt; N) {
        A[threadIdx.x] = X[i];
    }
    if (i + blockDim.x &amp;lt; N) {
        A[threadIdx.x + blockDim.x] = X[i + blockDim.x];
    }

    for (uint stride = 1; stride &amp;lt;= blockDim.x; stride *= 2) {
        __syncthreads();
        int index = (threadIdx.x + 1) * 2 * stride - 1;
        if (index &amp;lt; blockDim.x) {
            A[index] += A[index - stride];
        }
    }

    for (uint stride = blockDim.x / 4; stride &amp;gt; 0; stride /= 2) {
        __syncthreads();
        int index = (threadIdx.x + 1) * 2 * stride - 1;
        if (index + stride &amp;lt; blockDim.x) {
            A[index + stride] += A[index];
        }
    }
    __syncthreads();

    if (i &amp;lt; N) {
        Y[i] = A[threadIdx.x];
    }
    if (i + blockDim.x &amp;lt; N) {
        Y[i + blockDim.x] = A[threadIdx.x + blockDim.x];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;adding-coarsening&#34;&gt;Adding Coarsening&lt;/h2&gt;
&lt;p&gt;Similar to other problems such as tiled matrix multiplication, if the hardware does not meet the capacity ti parallelize the entire problem, the price for parallelization is wasted. In such cases, we can coarsen the problem so that the available resources are fully utilized. Each thread will execute a &lt;em&gt;phase&lt;/em&gt; of sequential scan, which is more work efficiency than other of the solutions presented above.&lt;/p&gt;
&lt;h3 id=&#34;phase-1&#34;&gt;Phase 1&lt;/h3&gt;
&lt;p&gt;Such a solution starts off by performing a sequential scan. The threads can also collaborate in the beginning to load data into shared memory.&lt;/p&gt;
&lt;h3 id=&#34;phase-2&#34;&gt;Phase 2&lt;/h3&gt;
&lt;p&gt;In the next phase, the threads execute a parallel scan via Kogge-Stone or Brent-Kung. Since each thread has already performed a sequential scan. This phase starts off with the last element assigned to each thread.&lt;/p&gt;
&lt;h3 id=&#34;phase-3&#34;&gt;Phase 3&lt;/h3&gt;
&lt;p&gt;In the last phase, each thread adds its last value to the first \(n-1\) elements of the next section, where \(n\) is the number of elements assigned to each thread.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-03-19_22-25-15_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Three-phase parallel scan (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Three-phase parallel scan (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;segmented-parallel-scan&#34;&gt;Segmented Parallel Scan&lt;/h2&gt;
&lt;p&gt;Input to scan operations may be too large to fit in memory.
Hierarchical scans can be used to solve this problem.
Partition input so that each section fits into shared memory for an SM.
Another kernel launches after this is done to consolidate the results by adding the sum of the preceding scan blocks to each element of a scan block.&lt;/p&gt;
&lt;p&gt;Each scan block is treated as an individual application of one of the previous kernels. Each successive scan block initially does not contain the sums of the preceding blocks. Those will be added in a separate kernel.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-03-19_22-31-17_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Segmented scan (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Segmented scan (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;After each scan block has computed the scan for its partition, the last elements are then processed in the next step. However, these elements are all from different blocks, which means we must write them to a global space so they can all be accessed. Completing this step yields an array that has the final values of the scan corresponding to the indices from the original scan blocks (see the figure above). These values can then be used to update the preceding elements in each scan block to complete the scan.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kernel 1: Section Scan&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first kernel essentially implements one of the previously discussed parallel scan kernels. The only difference is that the accumulation array is passed so that the blocks can write their output element values.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__syncthreads();
if (threadIdx.x == blockDim.x - 1) {
    accumulation[blockIdx.x] = A[threadIdx.x];
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Kernel 2: Update Scan&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For step 2, we need to run a parallel scan kernel like Kogge-Stone or Brent-Kung on the accumulation array.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kernel 3: Update Elements&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The final kernel takes the accumulated values and updates the original array so that all the elements have their correct scan value.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;uint i = blockIdx.x * blockDim.x + threadIdx.x;
output[i] += accumulation[blockIdx.x - 1];
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;optimizing-memory-efficiency&#34;&gt;Optimizing Memory Efficiency&lt;/h2&gt;
&lt;p&gt;The previous solution provides a way of working with large inputs, but it is not memory efficient because of the need to store the accumulation array. There have been several attempts at optimizing this flow of information in the form of a &lt;em&gt;stream-based&lt;/em&gt; scan. To understand the general strategy behind this approach, consider the segmented scan from the previous section.&lt;/p&gt;
&lt;p&gt;The process starts by executing all the scan blocks in parallel. The first scan block has all the information it needs to compute the full scan for its partition. However, the second block needs the final output value from the first before it can update its own values. It will have to wait until block 0 has finished and written its value to global memory. It can then add that value to its own elements before sending its final value to the next block. This process continues until all the blocks have been processed.&lt;/p&gt;
&lt;p&gt;To be clear, the first phase runs completely in parallel, and the second phase is sequential. If the final values are passed quickly enough between each block, the overall scan will still be efficient. Once each block has that passed value, it can continue its work in parallel since it is no longer dependent on the previous block.&lt;/p&gt;
&lt;p&gt;For this to work, there needs to be a form of synchronization between blocks. The CUDA API does not provide grid-wide synchronization, so &lt;strong&gt;how can we accomplish this&lt;/strong&gt;? One solution is to use a lock to effectively halt a thread until the value is ready to be read (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Yan, Long, and Zhang 2013&lt;/a&gt;). The code below shows how this can be implemented.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__shared__ float previous_sum;
if (threadIdx.x == 0) {
    // Wait for previous flag
    while(atomicAdd(&amp;amp;flags[bid], 0) == 0);
    // Read previous partial sum
    previous_sum = scan_value[bid];
    // Propagate partial sum
    scan_value[bid + 1] = previous_sum + local_sum;
    // Memory fence
    __threadfence();
    // Update flag
    atomicAdd(&amp;amp;flags[bid + 1], 1);
}
__syncthreads();
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The section ensures that only the first element of each block will perform this update. The &lt;code&gt;flags&lt;/code&gt; array is a global array that is used to store the lock values. Once the flag is set to 1, the value from the global array &lt;code&gt;scan_value&lt;/code&gt; is read and used to update the local sum. We haven&amp;rsquo;t used &lt;code&gt;__threadfence()&lt;/code&gt; before, but it is used to ensure that the update to &lt;code&gt;scan_value[bid + 1]&lt;/code&gt; is written before the call to &lt;code&gt;atomicAdd&lt;/code&gt; is made.&lt;/p&gt;
&lt;p&gt;Your first thought might be that there are too many global memory accesses. Shouldn&amp;rsquo;t this incur a large access penalty? Remember that many of these are overlapping values that were accessed by previous blocks, so they are most likely in the cache. Of course, if there is ever any doubt on the performance of a kernel, we can always profile it to verify our assumptions.&lt;/p&gt;
&lt;p&gt;As opposed to the previous solution, this one only requires a single kernel. In the three kernel approach, there is no overlap between the values since each kernel is executed sequentially.&lt;/p&gt;
&lt;h3 id=&#34;preventing-deadlocks&#34;&gt;Preventing Deadlocks&lt;/h3&gt;
&lt;p&gt;This issue isn&amp;rsquo;t production ready just yet. Depending on how the blocks are scheduled, it is possible that a deadlock could occur. For example, the second block could be scheduled before the first block. If there aren&amp;rsquo;t enough SMs on the device then the first block will never be able to write its value to the global array. One solution to this is &lt;strong&gt;dynamic block index assignment&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In this approach, the block index assignment is not dependent on &lt;code&gt;blockIdx.x&lt;/code&gt;. Instead it is assigned dynamically as blocks are processed.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__shared__ uint bid_s;
if (threadIdx.x == 0) {
    bid_s = atomicAdd(&amp;amp;block_index, 1);
}
__syncthreads();
uint bid = bid_s;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This ensures that each block gets a unique index and that the blocks are processed in the order they are ready.&lt;/p&gt;
&lt;h2 id=&#34;the-takeaway&#34;&gt;The Takeaway&lt;/h2&gt;
&lt;p&gt;Parallel scan is a powerful tool for solving problems that can be described in terms of a recursion. The Kogge-Stone and Brent-Kung algorithms are two ways of parallelizing the scan operation. This problem presents a unique look at how tradeoffs must be made when parallelizing a problem. At the end of the day, we must work within the constraints of the hardware and framework made available to us.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Yan, Shengen, Guoping Long, and Yunquan Zhang. 2013. StreamScan: Fast Scan Algorithms for GPUs without Global Barrier Synchronization. In &lt;i&gt;Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming&lt;/i&gt;, 22938. PPoPP 13. New York, NY, USA: Association for Computing Machinery. &lt;a href=&#34;https://doi.org/10.1145/2442516.2442539&#34;&gt;https://doi.org/10.1145/2442516.2442539&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GPU Pattern: Reduction</title>
      <link>http://localhost:1313/notes/gpu_pattern_reduction/</link>
      <pubDate>Mon, 05 Feb 2024 15:47:00 -0600</pubDate>
      
      <guid>http://localhost:1313/notes/gpu_pattern_reduction/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reduction-trees&#34;&gt;Reduction Trees&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-simple-kernel&#34;&gt;A Simple Kernel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#minimizing-control-divergence&#34;&gt;Minimizing Control Divergence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#memory-divergence-of-reduction&#34;&gt;Memory Divergence of Reduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reducing-the-number-of-global-memory-requests&#34;&gt;Reducing the number of global memory requests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hierarchical-reduction&#34;&gt;Hierarchical Reduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#thread-coarsening-back-again&#34;&gt;Thread Coarsening - Back Again&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;The following notes follow Chapter 10 of &lt;em&gt;Programming Massively Parallel Processors&lt;/em&gt; (Hwu, Kirk, and El Hajj 2022).&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Given a set of values, a &lt;strong&gt;reduction&lt;/strong&gt; produces a single output. It is an important part of many parallel algorithms including &lt;a href=&#34;http://localhost:1313/notes/mapreduce/&#34;&gt;MapReduce&lt;/a&gt;. Other patterns that we have studied can also be viewed as reductions, such as &lt;a href=&#34;http://localhost:1313/notes/gpu_pattern_parallel_histogram/&#34;&gt;GPU Pattern: Parallel Histogram&lt;/a&gt;. Implementing this parallel pattern requires careful consideration of thread communication, and will be the focus of these notes.&lt;/p&gt;
&lt;p&gt;Many of the operations you rely on are examples of reductions. For example, the `sum` function is a reduction, as is the `max` function. A reduction can be viewed as a linear combination of the input values, or transformed values, and is often used to compute a summary statistic. If \(\phi(\cdot)\) is a binary operator, then a reduction computes the following:&lt;/p&gt;
&lt;p&gt;\[
v = \phi(v, x_i)\ \text{for}\ i = 1, 2, \ldots, n,
\]&lt;/p&gt;
&lt;p&gt;where \(v\) is the accumulated value and \(x_i\) are the input values. The operator \(\phi(\cdot)\) can be any associative and commutative operation, such as addition or multiplication. Each operator has a corresponding identity element, such as 0 for addition or 1 for multiplication. The identity element is used to initialize the reduction and can be represented as \(v = v_0\) in the equation above.&lt;/p&gt;
&lt;h2 id=&#34;reduction-trees&#34;&gt;Reduction Trees&lt;/h2&gt;
&lt;p&gt;Reductions of any kind are well represented using trees. The first level of reduction maximizes the amount of parallelism. As the input is gradually reduced, fewer threads are needed.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-02-13_18-11-10_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Sum reduce as a reduction tree.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Sum reduce as a reduction tree.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In order to implement a parallel reduction, the chosen operator must be associative. For example, \(a + (b + c) = (a + b) + c\). The operator must also be commutative, such that \(a + b = b + a\).&lt;/p&gt;
&lt;p&gt;Reduction trees reveal the logarithmic nature of parallel reductions. Just like divide and conquer algorithms, the number of threads is halved at each level of the tree. The number of levels in the tree is \(\log_2(n)\), where \(n\) is the number of input values. Given an input size of \(n = 1024\), the number of threads required is \(\log_2(1024) = 10\). This is a significant reduction from the original input size. The sequential version of this reduction would require 1023 operations.&lt;/p&gt;
&lt;h2 id=&#34;a-simple-kernel&#34;&gt;A Simple Kernel&lt;/h2&gt;
&lt;p&gt;As mentioned above, reduction requires communication between threads. Since only the threads within a single block can communicate, we will focus on a block-level reduction. For now, each block can work with a total of 2048 input values based on the limitation of 1024 threads per block.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void sumReduceKernel(float *input, float *output) {
    unsigned int i = 2 * threadIdx.x;

        for (unsigned int stride = 1; stride &amp;lt;= blockDim.x; stride *= 2) {
            // Only threads in even positions participate
            if (threadIdx.x % stride == 0) {
                input[i] += input[i + stride];
            }
            __syncthreads();
        }

        if (threadIdx.x == 0) {
            *output = input[0];
        }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Each thread is assigned to a single write location &lt;code&gt;2 * threadIdx.x&lt;/code&gt;. The stride is doubled after each iteration of the loop, effectively halving the number of active threads. The stride also determines the second value that is added to the first. By the last iteration, only one thread is active to perform that last reduction.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-03-03_19-24-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Execution of kernel reduction (Source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Execution of kernel reduction (Source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;You can see that the kernel is simple, but it is also inefficient. There is a great deal of control divergence that will be addressed in the next section.&lt;/p&gt;
&lt;h2 id=&#34;minimizing-control-divergence&#34;&gt;Minimizing Control Divergence&lt;/h2&gt;
&lt;p&gt;As we just saw, the key to optimizing a reduction kernel is to minimize control divergence and make sure as many threads stay active as possible. A warp of 32 threads would consume the execution resources even if half of them are inactive. As each stage of the reduction tree is completed, the amount of wasted resources increases. Depending on the input size, entire warps could be launched and then immediately become inactive.&lt;/p&gt;
&lt;p&gt;The number of execution resources consumes is proportional to the number of active warps across all iterations. We can compute the number of resources consumed as follows:&lt;/p&gt;
&lt;p&gt;\[
(\frac{5N}{64} + \frac{N}{128} + \frac{N}{256} + \cdots + 1) * 32
\]&lt;/p&gt;
&lt;p&gt;where \(N\) is the number of input values. Each thread operates on 2 values, so \(\frac{N}{2}\) are launched in total. Since every warp has 32 threads, a total of \(\frac{N}{64}\) warps are launched. For the first 5 iterations, all warps will be active. The 5th iteration only has 1 active thread in each warp. On the 6th iteration, the number of active warps is halved, and so on.&lt;/p&gt;
&lt;p&gt;For an input of size \(N = 1024\), the number of resources consumed is \((80 + 8 + 4 + 2 + 1) * 32 = 3040\). The total number of results committed by the active threads is equal to the number of operations performed, which is \(N - 1 = 1023\). The efficiency of the kernel is then \(\frac{1023}{3040} = 0.34\). Only around 34% of the resources are used to perform the reduction.&lt;/p&gt;
&lt;h3 id=&#34;rearranging-the-threads&#34;&gt;Rearranging the Threads&lt;/h3&gt;
&lt;p&gt;A simple rearrangement of where the active results are stored can improve the efficiency of the kernel by reducing control divergence. The idea is to keep the threads that own the results of the reduction close together. Instead of increasing the stride, it should be decreased. The figure below shows the rearrangement of the threads.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-03-03_21-28-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Optimized reduction kernel execution (Source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Optimized reduction kernel execution (Source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void sumReduceKernel(float *input, float *output) {
    unsigned int i = threadIdx.x;

    for (unsigned int stride = blockDim.x; stride &amp;gt;= 1; stride /= 2) {
        if (i &amp;lt; stride) {
            input[i] += input[i + stride];
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        *output = input[0];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The kernel itself is effectively the same, but the rearrangement of the threads ensures that each warp has less control divergence. Additionally, warps that drop off after each iteration are no longer consuming execution resources. For an input of 256, the first 4 warps are fully utilized (barring the last thread of the last warp). After the first iteration, the number of active warps is halved. Warps 3 and 4 are now fully inactive, leaving warps 1 and 2 to perform the reduction operation on all threads. We can compute the number of resources consumed under this new arrangement as follows:&lt;/p&gt;
&lt;p&gt;\[
(\frac{N}{64} + \frac{N}{128} + \frac{N}{256} + \cdots + 1 + 5) * 32
\]&lt;/p&gt;
&lt;p&gt;At each iteration, half of warps become inactive and no longer consume resources. The last warp will consume execution resources for all 32 threads, even though the number of active threads is less than 32. For our input of size \(N = 1024\), the number of resources consumed is \((16 + 8 + 4 + 2 + 1 + 5) * 32 = 1152\), resulting in an efficiency of \(\frac{1023}{1152} = 0.89\). This is a significant improvement over the original kernel. This will increase based on the input size.&lt;/p&gt;
&lt;h2 id=&#34;memory-divergence-of-reduction&#34;&gt;Memory Divergence of Reduction&lt;/h2&gt;
&lt;p&gt;Does this kernel take advantage of memory coalescing? Each thread reads and writes from and to its &lt;em&gt;assigned&lt;/em&gt; location. It also makes a read from a location that is a stride away. These locations are certainly not adjacent and will not be coalesced.&lt;/p&gt;
&lt;p&gt;Adjacent threads do not access adjacent locations. The warp itself is unable to coalesce the thread requests into a single global memory request. Each data element is 4 bytes. Since each of the 32 threads in a warp are accessing their assigned locations with a separation of &lt;code&gt;stride&lt;/code&gt;, the &lt;code&gt;64 * 4&lt;/code&gt; bytes will require two 128 byte memory requests to access the data. With each iteration, the assigned locations will always be separated such that two 128 byte memory requests will need to be made. Only on the last iteration, where only a single thread accesses a single assigned location, will a single memory request be made.&lt;/p&gt;
&lt;p&gt;The convergent kernel from the last section takes advantage of memory coalescing, leading to fewer memory requests.&lt;/p&gt;
&lt;h2 id=&#34;reducing-the-number-of-global-memory-requests&#34;&gt;Reducing the number of global memory requests&lt;/h2&gt;
&lt;p&gt;As we saw with tiling in &lt;a href=&#34;http://localhost:1313/notes/gpu_performance_basics/&#34;&gt;GPU Performance Basics&lt;/a&gt;, we can reduce the number of global memory requests by using shared memory. Threads write their results to global memory, which is read again in the next iteration. By keeping the intermediate results in shared memory, we can reduce the number of global memory requests. If implemented correctly, only the original input values will need to be read from global memory.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void sumReduceSharedKernel(float *input, float *output) {
    __shared__ float input_s[BLOCK_DIM];
    unsigned int i = threadIdx.x;
    input_s[i] = input[i] + input[i + BLOCK_DIM];

    for (unsigned int stride = blockDim.x / 2; stride &amp;gt;= 1; stride /= 2) {
        __syncthreads();
        if (i &amp;lt; stride) {
            input_s[i] += input_s[i + stride];
        }
    }

    if (i == 0) {
        *output = input_s[0];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;At the very top of this kernel, the necessary input is loaded from global memory, added, and written to shared memory. This is the only time global memory is accessed, with the exception of the final write to the output. The call to &lt;code&gt;syncthreads()&lt;/code&gt; moves to the top so that the shared memory is guaranteed before the next update.&lt;/p&gt;
&lt;p&gt;This approach not only requires fewer global memory requests, but the original input is left unmodified.&lt;/p&gt;
&lt;h2 id=&#34;hierarchical-reduction&#34;&gt;Hierarchical Reduction&lt;/h2&gt;
&lt;p&gt;One major assumption that has been made in each of these kernels is that they are running on a single block. Thread synchronization is critical to the success of the reduction. If we want to reduce a larger number of input across multiple blocks, the kernel should allow for independent execution. This is achieved by segmenting the input and performing a reduction on each segment. The final reduction is then performed on the results of the segment reductions.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void sumReduceHierarchicalKernel(float *input, float *output) {
    __shared__ float input_s[BLOCK_DIM];
    unsigned int segment = 2 * blockDim.x * blockIdx.x;
    unsigned int i = segment + threadIdx.x;
    unsigned int t = threadIdx.x;
    input_s[t] = input[i] + input[i + BLOCK_DIM];

    for (unsigned int stride = blockDim.x / 2; stride &amp;gt;= 1; stride /= 2) {
        __syncthreads();
        if (t &amp;lt; stride) {
            input_s[t] += input_s[t + stride];
        }
    }

    if (t == 0) {
        atomicAdd(output, input_s[0]);
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Each block has its own shared memory and can independently perform the reduction. Depending on the completion order, an atomic operation to add the local result is necessary.&lt;/p&gt;
&lt;h2 id=&#34;thread-coarsening-back-again&#34;&gt;Thread Coarsening - Back Again&lt;/h2&gt;
&lt;p&gt;Thread coarsening was first analyzed in the context of matrix multiplication in &lt;a href=&#34;http://localhost:1313/notes/gpu_performance_basics/&#34;&gt;GPU Performance Basics&lt;/a&gt;. Whenever the device does not have enough resources to execute the number of threads requested, it is forced to serialize the execution. In this case, we can serialize the work done by each thread so that no extra overhead is incurred. Another benefit to thread coarsening is improved data locality.&lt;/p&gt;
&lt;p&gt;Successive iterations increase the amount of inactive warps. For reduction, thread coarsening can be applied by increasing the number of elements that each one processes. If the time to perform the arithmetic is much faster than the time to load the data, then thread coarsening can be beneficial. We could further analyze our program to determine the optimal coarsening factor.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ coarsenedSumReductionKernel(float *input, float *output) {
    __shared__ float input_s[BLOCK_DIM];
    uint segment = COARSE_FACTOR * 2 * blockDim.x * blockIdx.x;
    uint i = segment + threadIdx.x;
    uint t = threadIdx.x;

    float sum = input[i];
    for (uint tile = 1; tile &amp;lt; COARSE_FACTOR * 2; tile++) {
        sum += input[i + tile * BLOCK_DIM];
    }

    input_s[t] = sum;

    for (uint stride = blockDim.x / 2; stride &amp;gt;= 1; stride /= 2) {
        __syncthreads();
        if (t &amp;lt; stride) {
            input_s[t] += input_s[t + stride];
        }
    }
    if (t == 0) {
        atomicAdd(output, input_s[0]);
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In the coarsened version, less thread communication is required since the first several steps are computed in a single thread.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GPU Pattern: Parallel Histogram</title>
      <link>http://localhost:1313/notes/gpu_pattern_parallel_histogram/</link>
      <pubDate>Mon, 29 Jan 2024 17:22:00 -0600</pubDate>
      
      <guid>http://localhost:1313/notes/gpu_pattern_parallel_histogram/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#histograms&#34;&gt;Histograms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#latency-of-atomic-operations&#34;&gt;Latency of Atomic Operations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#privatization&#34;&gt;Privatization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coarsening&#34;&gt;Coarsening&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aggregation&#34;&gt;Aggregation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-takeaway&#34;&gt;The Takeaway&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;These notes follow the presentation of the parallel histogram pattern in the book &lt;strong&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/strong&gt; (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;histograms&#34;&gt;Histograms&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Examples of histograms include:&lt;/li&gt;
&lt;li&gt;Frequency of words in a document&lt;/li&gt;
&lt;li&gt;Distribution of pixel intensities in an image&lt;/li&gt;
&lt;li&gt;Distribution of particle energies in a physics simulation&lt;/li&gt;
&lt;li&gt;Distribution of thread block execution times in a GPU kernel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Consider the program below which computes a histogram of the letters in a string. The input is assumed to be lower case.
Since this is executed sequentially, there is no risk of multiple threads writing to the same memory location at the same time.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;histogram_sequential&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;char&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;data, &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; length, &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;hist) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; length; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        hist[data[i] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;a&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To parallelize, we could launch a kernel which has each thread work with on character from the input. This presents a major problem when updating the histogram, as multiple threads may try and increment the same location simultaneously. This is called &lt;em&gt;output interference&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;race-conditions&#34;&gt;Race Conditions&lt;/h3&gt;
&lt;p&gt;For example, thread 1 and thread 2 may have the letter &amp;lsquo;a&amp;rsquo; as their input. They will both issue a &lt;em&gt;read-modify-write&lt;/em&gt; procedure where the current value of the histogram is read, incremented, and then written back to memory. If thread 1 reads the value of the histogram before thread 2 writes to it, the value that thread 1 writes back will be incorrect. This is a classic example of a &lt;em&gt;race condition&lt;/em&gt;. Depending on the timing, one thread could have read the updated value from the other thread, or both threads could have read the same value and incremented it, resulting in a loss of data.&lt;/p&gt;
&lt;h3 id=&#34;atomic-operations&#34;&gt;Atomic Operations&lt;/h3&gt;
&lt;p&gt;One solution to this problem is to perform atomic operations. This is a special type of operation that locks a memory location while it is being updated. This prevents other threads from reading or writing to the same location until the operation is complete. Each thread attempting to access a memory location will be forced to wait until the lock is released.&lt;/p&gt;
&lt;p&gt;The CUDA API provides several atomic operations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;atomicAdd&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;atomicSub&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;atomicExch&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;atomicMin&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;atomicMax&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;atomicInc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;atomicDec&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;atomicCAS&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are all &lt;em&gt;intrinsic functions&lt;/em&gt;, meaning they are processed in a special way by the compiler. Instead of acting like a function call that comes with the typical overhead from the stack, these are implemented as inline machine instructions. The CUDA kernel below uses &lt;code&gt;atomicAdd&lt;/code&gt; to increment the histogram.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void histogram_atomic(char *data, unsigned int length, unsigned int *hist) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &amp;lt; length) {
        atomicAdd(&amp;amp;hist[data[i] - &amp;#39;a&amp;#39;], 1);
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;latency-of-atomic-operations&#34;&gt;Latency of Atomic Operations&lt;/h2&gt;
&lt;p&gt;Atomic operations prevent the hardware from maximizing DRAM bursts since they serialize memory accesses. This can lead to a significant performance penalty. For each atomic operation, there are two delays: the delay from loading an element and the delay from storing the updated values.&lt;/p&gt;
&lt;p&gt;Not all threads will be loading and storing to the same memory location; this is dependent on the number of bins used in the histogram. If the number of bins is small, the performance penalty will be greater. This analysis is further complicated based on the distribution of the data.&lt;/p&gt;
&lt;p&gt;Atomic operations can be performed on the last level of cache. If the value is not in the cache, it will be brought into cache for future accesses. Although this does provide a performance benefit, it is not enough to offset the performance penalty of atomic operations.&lt;/p&gt;
&lt;h2 id=&#34;privatization&#34;&gt;Privatization&lt;/h2&gt;
&lt;p&gt;If much of the traffic is concentrated on a single area, the solution involves directing the traffic away in some manner. This is what &lt;strong&gt;privatization&lt;/strong&gt; does. Since the bottleneck is the data load and store, privatization gives each thread its own private store so that it can update without contention. Of course, each copy must be combined in some way at the end. The cost of merging these copies is much less than the cost of the atomic operations. In practice, privatization is done for groups of threads, not individual ones.&lt;/p&gt;
&lt;p&gt;The example above can be privatized by making a copy of the histogram for each thread block. The level of contention is much lower since only a single block will update their own private copy. All copies of the histogram are allocated as one monolithic array. Each individual block can use its local indices to offset the pointer. Private copies of values will likely still be cached in L2, so the cost of merging the copies is minimal.&lt;/p&gt;
&lt;p&gt;For example, if we have 256 threads per block and 26 bins, we can allocate a \(26 \times 256\) array of integers. Each thread block will have its own copy of the histogram. The kernel below demonstrates this.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;#define NUM_BINS 26
__global__ void histogram_privatized(char *data, unsigned int length, unsigned int *hist) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &amp;lt; length) {
        int pos = data[i] - &amp;#39;a&amp;#39;;
        atomicAdd(&amp;amp;hist[blockIdx.x * NUM_BINS + pos], 1);
    }
    __syncthreads();
    if (blockIdx.x &amp;gt; 0) {
        for (unsigned int bin = threadIdx.x; bin &amp;lt; NUM_BINS; bin += blockDim.x) {
            unsigned int binValue = hist[blockIdx.x * NUM_BINS + bin];
            if (binValue &amp;gt; 0) {
                atomicAdd(&amp;amp;hist[bin], binValue);
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Each block is free to use its section of the histogram without contention. After all blocks have finished, the histogram is merged by summing the values of each bin across all blocks. This is done by having each block add its values to the global histogram. The &lt;code&gt;__syncthreads&lt;/code&gt; function is used to ensure that all blocks have finished updating their private copies before the merge begins.&lt;/p&gt;
&lt;p&gt;Each block after index 0 will add its values to the global histogram, represented by the first block. Only a single thread per block will be accessing each bin, so the only contention is with other blocks. If the bins are small enough, shared memory can be used to store the private copies. Even though an atomic operation is still required, the latency for loading and storing is reduced by an order of magnitude. The shared kernel below demonstrates this.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;#define NUM_BINS 26
__global__ void histogram_privatized(char *data, unsigned int length, unsigned int *hist) {
    __shared__ unsigned int hist_s[NUM_BINS];
    for (unsigned int bin = threadIdx.x; bin &amp;lt; NUM_BINS; bin += blockDim.x) {
        hist_s[bin] = 0;
    }
    __syncthreads();

    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &amp;lt; length) {
        int pos = data[i] - &amp;#39;a&amp;#39;;
        atomicAdd(&amp;amp;hist_s[pos], 1);
    }
    __syncthreads();
    if (blockIdx.x &amp;gt; 0) {
        for (unsigned int bin = threadIdx.x; bin &amp;lt; NUM_BINS; bin += blockDim.x) {
            unsigned int binValue = hist_s[bin];
            if (binValue &amp;gt; 0) {
                atomicAdd(&amp;amp;hist[bin], binValue);
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;coarsening&#34;&gt;Coarsening&lt;/h2&gt;
&lt;p&gt;The bottleneck of using privatization moves from DRAM access to merging copies back to the &lt;em&gt;public&lt;/em&gt; copy of the data. This scales up based on the number of blocks used, since each thread in a block will be sharing a bin in the worst case. If the problem exceeds the capacity of the hardware, the scheduler will serialize the blocks. If they are serialized anyway, then the cost of privatization is not worth it.&lt;/p&gt;
&lt;p&gt;Coarsening will reduce the overhead of privatization by reducing the number of private copies that are committed to the public one. Each thread will process multiple elements.&lt;/p&gt;
&lt;h3 id=&#34;contiguous-partitioning&#34;&gt;Contiguous Partitioning&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-02-03_11-59-22_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Contiguous partitioning. Recreated from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Contiguous partitioning. Recreated from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Each thread is assigned a contiguous range of elements to process. The kernel is a straightforward extension of the privatized kernel. This approach works better on a CPU, where there are only a small number of threads. This is due to the caching behavior of the CPU. With so many threads on a GPU, it is less likely that the data will be in the cache since so many threads are competing.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;#define NUM_BINS 26
__global__ void histogram_privatized_cc(char *data, unsigned int length, unsigned int *hist) {
    __shared__ unsigned int hist_s[NUM_BINS];
    for (unsigned int bin = threadIdx.x; bin &amp;lt; NUM_BINS; bin += blockDim.x) {
        hist_s[bin] = 0;
    }
    __syncthreads();

    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &amp;lt; length) {
        int pos = data[i] - &amp;#39;a&amp;#39;;
        atomicAdd(&amp;amp;hist_s[pos], 1);
    }
    __syncthreads();
    if (blockIdx.x &amp;gt; 0) {
        for (unsigned int bin = threadIdx.x; bin &amp;lt; NUM_BINS; bin += blockDim.x) {
            unsigned int binValue = hist_s[bin];
            if (binValue &amp;gt; 0) {
                atomicAdd(&amp;amp;hist[bin], binValue);
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;interleaved-partitioning&#34;&gt;Interleaved Partitioning&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-02-03_12-00-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Interleaved partitioning. Recreated from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Interleaved partitioning. Recreated from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Contiguous partitioning allowed for contiguous access to values relative to each thread. However, the memory was not contiguous with respect to other threads. In terms of DRAM accesses, each individual read from memory was too far apart to take advantage of coalescing. With &lt;strong&gt;interleaved partitioning&lt;/strong&gt;, the memory can be accessed in a single DRAM access since the memory is coalesced.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;#define NUM_BINS 26
__global__ void histogram_privatized_ic(char *data, unsigned int length, unsigned int *hist) {
    __shared__ unsigned int hist_s[NUM_BINS];
    for (unsigned int bin = threadIdx.x; bin &amp;lt; NUM_BINS; bin += blockDim.x) {
        hist_s[bin] = 0;
    }
    __syncthreads();

    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    for (int i = tid; i &amp;lt; length; i += blockDim.x * gridDim.x) {
        int pos = data[i] - &amp;#39;a&amp;#39;;
        if (pos &amp;gt;= 0 &amp;amp;&amp;amp; pos &amp;lt; 26) {
            atomicAdd(&amp;amp;hist_s[pos], 1);
        }
    }
    __syncthreads();
    if (blockIdx.x &amp;gt; 0) {
        for (unsigned int bin = threadIdx.x; bin &amp;lt; NUM_BINS; bin += blockDim.x) {
            unsigned int binValue = hist_s[bin];
            if (binValue &amp;gt; 0) {
                atomicAdd(&amp;amp;hist[bin], binValue);
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In the code above, the main difference is the second &lt;code&gt;for&lt;/code&gt; loop. The index &lt;code&gt;i&lt;/code&gt; is incremented by &lt;code&gt;blockDim.x * gridDim.x&lt;/code&gt;. This ensures that the threads of each block access memory in a contiguous manner rather than each thread being contiguous. The differences are visualized in the figures.&lt;/p&gt;
&lt;h2 id=&#34;aggregation&#34;&gt;Aggregation&lt;/h2&gt;
&lt;p&gt;It is not uncommon that the input data will have a skewed distribution. There may be sections of the input that are locally dense. This will lead to a large number of atomic operations within a small area. To reduce the number of atomic operations, the input can be aggregated into a larger update before being committed to the global histogram. Consider the code below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void histogram_aggregate(char *data, unsigned int length, unsigned int *histo) {
    // Initialize shared memory
    __shared__ unsigned int hist_s[NUM_BINS];
    for (unsigned int bin = threadIdx.x; bin &amp;lt; NUM_BINS; bin += blockDim.x) {
        hist_s[bin] = 0;
    }
    __syncthreads();

    // Build histogram
    unsigned int accumulator = 0;
    int prevBinIdx = -1;
    unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;
    for (unsigned int i = tid; i a, length; i += blockDim.x * gridDim.x) {
        int binIdx = data[i] - &amp;#39;a&amp;#39;;
        if (binIdx == prevBinIdx) {
            accumulator++;
        } else {
            if (prevBinIdx &amp;gt;= 0) {
                atomicAdd(&amp;amp;hist_s[prevBinIdx], accumulator);
            }
            accumulator = 1;
            prevBinIdx = binIdx;
        }
    }
    if (accumulator &amp;gt; 0) {
        atomicAdd(&amp;amp;hist_s[prevBinIdx], accumulator);
    }
    __syncthreads();

    // Commit to global memory
    for (unsigned int bin = threadIdx.x; bin &amp;lt; NUM_BINS; bin += blockDim.x) {
        unsigned int binValue = hist_s[bin];
        if (binValue &amp;gt; 0) {
            atomicAdd(&amp;amp;histo[bin], binValue);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The difference in this kernel is the histogram loop in the middle. The previous bin index is tracked to determine if contiguous values would be aggregated. As long as the values are the same, the accumulator is increased. As soon as a new value is encountered, a batch update is performed. This reduces the number of atomic operations by a factor of the number of contiguous values.&lt;/p&gt;
&lt;p&gt;If the data is relatively uniform, the cost of aggregation exceeds the simple kernel. If you are working with images, spatially local data will usually be aggregated. This kernel would be beneficial in that case. Another downside to the aggregated kernel is that it requires more registers and has an increased chance for control divergence. As with all implementations, you should profile this against your use case.&lt;/p&gt;
&lt;h2 id=&#34;the-takeaway&#34;&gt;The Takeaway&lt;/h2&gt;
&lt;p&gt;Computing histograms is a common operation in fields such as image processing, natural language processing, and physics simulations. For example, a core preprocessing step for training a large language model is to compute the frequency of words in a corpus. This is a perfect example of a task that can be parallelized on a GPU.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GPU Pattern: Stencils</title>
      <link>http://localhost:1313/notes/gpu_pattern_stencils/</link>
      <pubDate>Mon, 22 Jan 2024 19:39:00 -0600</pubDate>
      
      <guid>http://localhost:1313/notes/gpu_pattern_stencils/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#differential-equations&#34;&gt;Differential Equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#stencils&#34;&gt;Stencils&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-basic-stencil&#34;&gt;Example: Basic Stencil&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tiled-stencil&#34;&gt;Tiled Stencil&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#thread-coarsening&#34;&gt;Thread Coarsening&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#register-tiling&#34;&gt;Register Tiling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#questions&#34;&gt;Questions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;ul&gt;
&lt;li&gt;Used in differential equations&lt;/li&gt;
&lt;li&gt;Frequently use higher precision&lt;/li&gt;
&lt;li&gt;Some similarity to convolutions&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;differential-equations&#34;&gt;Differential Equations&lt;/h2&gt;
&lt;p&gt;Any computational problem requires discretization of data or equations so that they can be solved numerically. This is fundamental in numerical analysis, where differential equations need to be approximated.&lt;/p&gt;
&lt;p&gt;Structured grids are used in finite-difference methods for solving partial differential equations (PDEs). Approximate derivatives can be computed point-wise by considering the neighbors on the grid. As we saw earlier in this course, a grid representation is a natural way to think about data parallelism.&lt;/p&gt;
&lt;p&gt;Depending on the function and level of discretization, interpolation will be more or less accurate. Consider the logistic sigmoid function sampled at 4 points. In the middle, linear interpolation would work just fine. Near the &lt;em&gt;bends&lt;/em&gt; of the function, a linear approximation would introduce error. The closer the spacing, the more accurate a linear approximation becomes. The downside is that more memory is required to store the points.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-27_13-03-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Logistic sigmoid function (Wikipedia).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Logistic sigmoid function (Wikipedia).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The precision of the data type also plays an important role. Higher precision data types like &lt;code&gt;double&lt;/code&gt; require more bandwidth to transfer and will typically require more cycles when computing arithmetic operations.&lt;/p&gt;
&lt;h2 id=&#34;stencils&#34;&gt;Stencils&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;stencil&lt;/strong&gt; is a geometric pattern of weights applied at each point of a structured grid. The points on the grid will derive their values from neighboring points using some numerical approximation. For example, this is used to solve differential equations. Consider a 1D grid of discretized values from a function \(f(x)\). The finite difference approximation can be used to find \(f&amp;rsquo;(x)\):&lt;/p&gt;
&lt;p&gt;\[ f&amp;rsquo;(x) = \frac{f(x+h) - f(x-h)}{2h} + O(h^2) \]&lt;/p&gt;
&lt;p&gt;In code, this would look like:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void finite_difference(float *f, float *df, float h) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    df[i] = (f[i+1] - f[i-1]) / (2 * h);
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;A PDE of two variables can be solved using a 2D stencil. Likewise, a PDE of three variables can be solved using a 3D stencil. The figures below show examples of common 2D and 3D stencils. Note that they typically have an odd number of points so that there is a center point.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-27_15-23-57_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;1D stencils. Recreated from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;1D stencils. Recreated from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The 1D stencils shown above are used to approximate the first-order (A), second-order (B), and third-order (C) derivatives of a function \(f(x)\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-27_15-24-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;2D and 3D stencils. Recreated from (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;2D and 3D stencils. Recreated from (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The 2D stencils shown above are used to approximate first-order (A) and second-order (B) derivatives of a function \(f(x, )\). Likewise, the 3D stencils are used to approximate first-order (C) and second-order (D) derivatives of a function \(f(x, y, z)\).&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;stencil sweep&lt;/em&gt; is the process of applying the stencil to all points on the grid, similar to how a convolution is applied. There are many similarities between the two, but the subtle differences will require us to think differently about how to optimize them.&lt;/p&gt;
&lt;h2 id=&#34;example-basic-stencil&#34;&gt;Example: Basic Stencil&lt;/h2&gt;
&lt;p&gt;The code below presents a naive kernel for a stencil pattern using a 3D seven-point stencil.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void stencil_kernel(float *in, float *out, unsigned int N) {
    unsigned int i = blockIdx.z * blockDim.z + threadIdx.z;
    unsigned int j = blockIdx.y * blockDim.y + threadIdx.y;
    unsigned int k = blockIdx.x * blockDim.x + threadIdx.x;

    if (i &amp;gt;= 1 &amp;amp;&amp;amp; i &amp;lt; N - 1 &amp;amp;&amp;amp; j &amp;gt;= 1 &amp;amp;&amp;amp; j &amp;lt; N - 1 &amp;amp;&amp;amp; k &amp;gt;= 1 &amp;amp;&amp;amp; k &amp;lt; N - 1) {
        out[i*N*N + j*N + k] = c0 * in[i*N*N + j*N + k]
                             + c1 * in[i*N*N + j*N + (k - 1)]
                             + c2 * in[i*N*N + j*N + (k + 1)]
                             + c3 * in[i*N*N + (j - 1)*N + k]
                             + c4 * in[i*N*N + (j + 1)*N + k]
                             + c5 * in[(i - 1)*N*N + j*N + k]
                             + c6 * in[(i + 1)*N*N + j*N + k];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This example assumes the input and output are 3D grids. For this particular stencil, you should try to identify the number of memory accesses and operations performed. Can you already see some opportunities for optimization?&lt;/p&gt;
&lt;h2 id=&#34;tiled-stencil&#34;&gt;Tiled Stencil&lt;/h2&gt;
&lt;p&gt;Just like with convolution, it is possible to use shared memory to improve the performance of a stencil. The code below shows a tiled stencil kernel that uses shared memory.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void stencil_kernel(float *in, float *out, unsigned int N) {
    __shared__ float tile[IN_TILE_DIM][IN_TILE_DIM][IN_TILE_DIM];
    int i = blockIdx.z * OUT_TILE_DIM + threadIdx.z - 1;
    int j = blockIdx.y * OUT_TILE_DIM + threadIdx.y - 1;
    int k = blockIdx.x * OUT_TILE_DIM + threadIdx.x - 1;

    if (i &amp;gt;= 0 &amp;amp;&amp;amp; i &amp;lt; N &amp;amp;&amp;amp; j &amp;gt;= 0 &amp;amp;&amp;amp; j &amp;lt; N &amp;amp;&amp;amp; k &amp;gt;= 0 &amp;amp;&amp;amp; k &amp;lt; N) {
        tile[threadIdx.z][threadIdx.y][threadIdx.x] = in[i*N*N + j*N + k];
    }
    __syncthreads();
    if (i &amp;gt;=1 &amp;amp;&amp;amp; i &amp;lt; N-1 &amp;amp;&amp;amp; j &amp;gt;= 1 &amp;amp;&amp;amp; j &amp;lt; N-1 &amp;amp;&amp;amp; k &amp;gt;= 1 &amp;amp;&amp;amp; k &amp;lt; N-1) {
        if (threadIdx.z &amp;gt;= 1 &amp;amp;&amp;amp; threadIdx.z &amp;lt; IN_TILE_DIM-1 &amp;amp;&amp;amp;
            threadIdx.y &amp;gt;= 1 &amp;amp;&amp;amp; threadIdx.y &amp;lt; IN_TILE_DIM-1 &amp;amp;&amp;amp;
            threadIdx.x &amp;gt;= 1 &amp;amp;&amp;amp; threadIdx.x &amp;lt; IN_TILE_DIM-1) {
            out[i*N*N + j*N + k] = c0 * tile[threadIdx.z][threadIdx.y][threadIdx.x]
                                 + c1 * tile[threadIdx.z][threadIdx.y][threadIdx.x - 1]
                                 + c2 * tile[threadIdx.z][threadIdx.y][threadIdx.x + 1]
                                 + c3 * tile[threadIdx.z][threadIdx.y - 1][threadIdx.x]
                                 + c4 * tile[threadIdx.z][threadIdx.y + 1][threadIdx.x]
                                 + c5 * tile[threadIdx.z - 1][threadIdx.y][threadIdx.x]
                                 + c6 * tile[threadIdx.z + 1][threadIdx.y][threadIdx.x];
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Just like before, the threads of a block will first collaborate to load a tile with the relevant data. Stencil patterns are more sparse than convolutional filters and require less data to be loaded into shared memory. This will be the central detail in our analysis of stencil patterns.&lt;/p&gt;
&lt;h3 id=&#34;scaling-stencil-size-vs-dot-convolution-filter-size&#34;&gt;Scaling stencil size vs. Convolution filter size&lt;/h3&gt;
&lt;p&gt;Convolutions are more efficient as the size increases since more values are accessed in shared memory. For a \(3 \times 3\) convolution, the upper bound on compute to memory ratio is 4.5 OP/B. A 5 point 2D stencil has a ratio of 2.5 OP/B due to the sparsity of the pattern. The threads in the block would load the diagonal values from global memory, but each thread would only use the 5 points defined by the kernel.&lt;/p&gt;
&lt;h3 id=&#34;tiling-analysis&#34;&gt;Tiling Analysis&lt;/h3&gt;
&lt;p&gt;Let us consider the effectiveness of shared memory tiling where each thread performs 13 floating-point ops (7 multiplies and 6 adds) with each block using \((T - 2)^3\) threads. Each block also performs \(T^3\) loads of 4 bytes each. The compute to memory ratio can be express as:&lt;/p&gt;
&lt;p&gt;\[
\frac{13(T - 2)^3}{4T^3} = \frac{13}{4} \left(1 - \frac{2}{T}\right)^3
\]&lt;/p&gt;
&lt;p&gt;Due to the low limit on threads, the size of \(T\) is typically small. This means there is a smaller amount of reuse of data in shared memory. The ratio of floating-point ops to memory accesses will be low.&lt;/p&gt;
&lt;p&gt;Each warp loads values from 4 distant locations in global memory. This means that the memory accesses are not coalesced: the memory bandwidth is low. Consider an \(8 \times 8 \times 8\) block. A warp of 32 threads will load 4 rows of 8 values each. &lt;strong&gt;The values within each row contiguous, but the rows are not contiguous.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;thread-coarsening&#34;&gt;Thread Coarsening&lt;/h2&gt;
&lt;p&gt;Stencils do not benefit from shared memory as much as convolutions due to the sparsity of the sampled points. Most applications of the stencil patterns work with a 3D grid, resulting in relatively small tile sizes per block.&lt;/p&gt;
&lt;p&gt;A solution to this is to increase the amount of work each thread performs, AKA &lt;em&gt;thread coarsening&lt;/em&gt;. The price paid for parallelism in the stencil pattern is the low frequency of memory reuse.&lt;/p&gt;
&lt;p&gt;Each thread performs more work in the \(z\) direction for a 3D seven-point stencil. All threads collaborate to load in a \(z\) layer at time from \(z-1\) to \(z+1\). There are then 3 different shared memory tiles per block. After computing values in the current output tile, the shared memory is rearranged for the next layer. This means that there are more transfers between shared memory as opposed to global memory.&lt;/p&gt;
&lt;p&gt;The threads are launched to work with a 2D tile at a time, so the size of the block is now \(T^2\). This means we can use a larger value for \(T\). The compute to memory ratio is almost doubled under this scheme. Additionally, the amount of shared memory required is \(3T^2\) rather than \(T^3\).&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void stencil_kernel(float *in, float *out, unsigned int N) {
    int iStart = blockIdx.z * OUT_TILE_DIM;
    int j = blockIdx.y * OUT_TILE_DIM + threadIdx.y - 1;
    int k = blockIdx.x * OUT_TILE_DIM + threadIdx.x - 1;
    __shared__ float inPrev_s[IN_TILE_DIM][IN_TILE_DIM];
    __shared__ float inCurr_s[IN_TILE_DIM][IN_TILE_DIM];
    __shared__ float inNext_s[IN_TILE_DIM][IN_TILE_DIM];

    if (iStart - 1 &amp;gt;= 0 &amp;amp;&amp;amp; iStart - 1 &amp;lt; N &amp;amp;&amp;amp; j &amp;gt;= 0 &amp;amp;&amp;amp; j &amp;lt; N &amp;amp;&amp;amp; k &amp;gt;= 0 &amp;amp;&amp;amp; k &amp;lt; N) {
        inPrev_s[threadIdx.y][threadIdx.x] = in[(iStart - 1)*N*N + j*N + k];
    }
    if (iStart &amp;gt;= 0 &amp;amp;&amp;amp; iStart &amp;lt; N &amp;amp;&amp;amp; j &amp;gt;= 0 &amp;amp;&amp;amp; j &amp;lt; N &amp;amp;&amp;amp; k &amp;gt;= 0 &amp;amp;&amp;amp; k &amp;lt; N) {
        inCurr_s[threadIdx.y][threadIdx.x] = in[iStart*N*N + j*N + k];
    }
    for (int i = iStart; i &amp;lt; iStart + OUT_TILE_DIM; i++) {
        if (i + 1 &amp;gt;= 0 &amp;amp;&amp;amp; i + 1 &amp;lt; N &amp;amp;&amp;amp; j &amp;gt;= 0 &amp;amp;&amp;amp; j &amp;lt; N &amp;amp;&amp;amp; k &amp;gt;= 0 &amp;amp;&amp;amp; k &amp;lt; N) {
            inNext_s[threadIdx.y][threadIdx.x] = in[(i + 1)*N*N + j*N + k];
        }
        __syncthreads();
        if (i &amp;gt;= 1 &amp;amp;&amp;amp; i &amp;lt; N - 1 &amp;amp;&amp;amp; j &amp;gt;= 1 &amp;amp;&amp;amp; j &amp;lt; N - 1 &amp;amp;&amp;amp; k &amp;gt;= 1 &amp;amp;&amp;amp; k &amp;lt; N - 1) {
            if (threadIdx.y &amp;gt;= 1 &amp;amp;&amp;amp; threadIdx.y &amp;lt; IN_TILE_DIM - 1 &amp;amp;&amp;amp;
                threadIdx.x &amp;gt;= 1 &amp;amp;&amp;amp; threadIdx.x &amp;lt; IN_TILE_DIM - 1) {
                out[i*N*N + j*N + k] = c0 * inCurr_s[threadIdx.y][threadIdx.x]
                    + c1 * inCurr_s[threadIdx.y][threadIdx.x - 1]
                    + c2 * inCurr_s[threadIdx.y][threadIdx.x + 1]
                    + c3 * inCurr_s[threadIdx.y - 1][threadIdx.x]
                    + c4 * inCurr_s[threadIdx.y + 1][threadIdx.x]
                    + c5 * inPrev_s[threadIdx.y][threadIdx.x]
                    + c6 * inNext_s[threadIdx.y][threadIdx.x];
            }
        }
        __syncthreads();
        inPrev_s[threadIdx.y][threadIdx.x] = inCurr_s[threadIdx.y][threadIdx.x];
        inCurr_s[threadIdx.y][threadIdx.x] = inNext_s[threadIdx.y][threadIdx.x];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This kernel is visualized below with a block size of \(6 \times 6\). The left and top sides of the tile have been removed for clarity. All of the blue blocks are loaded from global memory. The dark blue blocks represent the &lt;em&gt;active&lt;/em&gt; plane that is used to compute the corresponding output values. After the current plane is completed, the block synchronizes before moving the current values to the previous plane and loads the next plane&amp;rsquo;s values into the current plane &lt;code&gt;inCurr_s&lt;/code&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-28_15-28-25_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Visualization of the thread coarsening tile.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Visualization of the thread coarsening tile.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;register-tiling&#34;&gt;Register Tiling&lt;/h2&gt;
&lt;p&gt;In the coarsening solution presented above, each thread works with a single element in the previous and next shared memory tiles. There are 4 elements in that example that really need to be loaded from shared memory. For the 3 elements that are only required by the current thread, they can be loaded into registers.&lt;/p&gt;
&lt;p&gt;Since only the values in the \(x-y\) direction are required for shared memory, the amount of memory used is reduced by \(\frac{1}{3}\).&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void stencil_kernel(float *in, float *out, unsigned int N) {
    int iStart = blockIdx.z * OUT_TILE_DIM;
    int j = blockIdx.y * OUT_TILE_DIM + threadIdx.y - 1;
    int k = blockIdx.x * OUT_TILE_DIM + threadIdx.x - 1;
    float inPrev;
    float inCurr;
    float inNext;

    __shared__ float inCurr_s[IN_TILE_DIM][IN_TILE_DIM];

    if (iStart - 1 &amp;gt;= 0 &amp;amp;&amp;amp; iStart - 1 &amp;lt; N &amp;amp;&amp;amp; j &amp;gt;= 0 &amp;amp;&amp;amp; j &amp;lt; N &amp;amp;&amp;amp; k &amp;gt;= 0 &amp;amp;&amp;amp; k &amp;lt; N) {
        inPrev = in[(iStart - 1)*N*N + j*N + k];
    }
    if (iStart &amp;gt;= 0 &amp;amp;&amp;amp; iStart &amp;lt; N &amp;amp;&amp;amp; j &amp;gt;= 0 &amp;amp;&amp;amp; j &amp;lt; N &amp;amp;&amp;amp; k &amp;gt;= 0 &amp;amp;&amp;amp; k &amp;lt; N) {
        inCurr = in[iStart*N*N + j*N + k];
        inCurr_s[threadIdx.y][threadIdx.x] = inCurr;
    }
    for (int i = iStart; i &amp;lt; iStart + OUT_TILE_DIM; i++) {
        if (i + 1 &amp;gt;= 0 &amp;amp;&amp;amp; i + 1 &amp;lt; N &amp;amp;&amp;amp; j &amp;gt;= 0 &amp;amp;&amp;amp; j &amp;lt; N &amp;amp;&amp;amp; k &amp;gt;= 0 &amp;amp;&amp;amp; k &amp;lt; N) {
            inNext = in[(i + 1)*N*N + j*N + k];
        }
        __syncthreads();
        if (i &amp;gt;= 1 &amp;amp;&amp;amp; i &amp;lt; N - 1 &amp;amp;&amp;amp; j &amp;gt;= 1 &amp;amp;&amp;amp; j &amp;lt; N - 1 &amp;amp;&amp;amp; k &amp;gt;= 1 &amp;amp;&amp;amp; k &amp;lt; N - 1) {
            if (threadIdx.y &amp;gt;= 1 &amp;amp;&amp;amp; threadIdx.y &amp;lt; IN_TILE_DIM - 1 &amp;amp;&amp;amp;
                threadIdx.x &amp;gt;= 1 &amp;amp;&amp;amp; threadIdx.x &amp;lt; IN_TILE_DIM - 1) {
                out[i*N*N + j*N + k] = c0 * inCurr
                    + c1 * inCurr_s[threadIdx.y][threadIdx.x - 1]
                    + c2 * inCurr_s[threadIdx.y][threadIdx.x + 1]
                    + c3 * inCurr_s[threadIdx.y - 1][threadIdx.x]
                    + c4 * inCurr_s[threadIdx.y + 1][threadIdx.x]
                    + c5 * inPrev
                    + c6 * inNext;
            }
        }
        __syncthreads();
        inPrev = inCurr;
        inCurr = inNext;
        inCurr_s[threadIdx.y][threadIdx.x] = inNext_s[threadIdx.y][threadIdx.x];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The kernel always has the active plane in shared memory. Every thread collectively store the previous and next planes in registers.&lt;/p&gt;
&lt;p&gt;The larger the stencil size, the more registers are required per thread. In this case, a tradeoff between shared memory space and register usage could be made. This will be explored in your lab.&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Stencils are a useful pattern for solving differential equations. They have some similarities to convolutions, but present unique challenges in terms of optimization. The sparsity of the pattern means that shared memory is not as effective as it is for convolutions. Thread coarsening and register tiling are two techniques that can be used to improve performance.&lt;/p&gt;
&lt;h2 id=&#34;questions&#34;&gt;Questions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;How many registers per thread are required for a 3D seven-point stencil, 3D nine-point stencil, and 3D 27-point stencil?&lt;/li&gt;
&lt;li&gt;How do convolutions relate to stencil patterns? Could you implement a stencil pattern using a convolution filter?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GPU Pattern: Convolution</title>
      <link>http://localhost:1313/notes/gpu_pattern_convolution/</link>
      <pubDate>Mon, 15 Jan 2024 21:35:00 -0600</pubDate>
      
      <guid>http://localhost:1313/notes/gpu_pattern_convolution/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#convolution&#34;&gt;Convolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#properties-of-convolutions&#34;&gt;Properties of Convolutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementing-a-convolution-kernel&#34;&gt;Implementing a Convolution Kernel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#constant-memory-and-caching&#34;&gt;Constant Memory and Caching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tiled-convolutions&#34;&gt;Tiled Convolutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#caching-the-halo-cells&#34;&gt;Caching the Halo Cells&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;This pattern involves tiling and input data staging.&lt;/p&gt;
&lt;p&gt;Recall from Lab 1 where we implementing a kernel to blur an image. This kernel worked on each individual output pixel by computing the weighted average of each pixel from the input image, centered on the output pixel location. When we implemented this, we set the weight of every pixel to 1. Whether you were aware of it or not, you implemented a convolution between an input image and weighted kernel.&lt;/p&gt;
&lt;p&gt;The convolution is an extremely important operator that works on time-varying signals of different dimensionality. In computer vision, they are commonly used to compute responses between a known pattern and an input image. The pixels that return a greater response as a result of convolution indicate a match between the pattern and the input image.&lt;/p&gt;
&lt;p&gt;This operator can and has been efficiently implemented in a GPU. Through studying this pattern, you will learn to utilize constant memory storage and shared memory storage to efficiently implement a convolution kernel. These techniques will be useful in later applications as well.&lt;/p&gt;
&lt;h2 id=&#34;convolution&#34;&gt;Convolution&lt;/h2&gt;
&lt;p&gt;A convolution is a function that takes two functions as input and produces a third function as output. The first function is the input and the second function is the kernel. The output is called the feature map. The kernel is also sometimes called the filter.&lt;/p&gt;
&lt;p&gt;\[
(f * g)(t) = \int f(t-a)g(a)da
\]&lt;/p&gt;
&lt;p&gt;We can view them more concretely by considering the functions to be vectors. For example, let the function \(f\) be an input vector \(x\) and \(w\) be a kernel representing a filter. The convolution operator is then&lt;/p&gt;
&lt;p&gt;\[
(x * w)(t) = \int x(t-a)w(a)da.
\]&lt;/p&gt;
&lt;p&gt;The result the &lt;strong&gt;feature map&lt;/strong&gt; representing the response of the kernel at each location in the input.&lt;/p&gt;
&lt;p&gt;In the case of discrete values, it is common to use an odd-sized kernel and center it on an input value. The kernel size is given by some radius \(r\). The convolution operator is then&lt;/p&gt;
&lt;p&gt;\[
(x * w)(t) = \sum_{-r}^r x(t-r)w( r).
\]&lt;/p&gt;
&lt;p&gt;The figure below shows an example of a 1D convolution of a vector if size 8 with a kernel of size 5, centered on the \(t = 2\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-17_18-00-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;1D Convolution between a vector of size 8 and a kernel of size 5.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;1D Convolution between a vector of size 8 and a kernel of size 5.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Convolution is defined in such a way that the kernel is traversed in an inverted manner. In the example above, \(y_2\) is computed by applying the kernel to \(\mathbf{x}\) centered on \(x_2\). The calculation in terms of the locations accesses is&lt;/p&gt;
&lt;p&gt;\[
y_2 = x_4 w_{-2} + x_3 w_{-1} + x_2 w_0 + x_1 w_1 + x_0 w_2.
\]&lt;/p&gt;
&lt;p&gt;This operation is very similar to the &lt;em&gt;correlation&lt;/em&gt; operator, which is defined as&lt;/p&gt;
&lt;p&gt;\[
(x \star w)(t) = \sum_{-r}^r x(t+r)w( r).
\]&lt;/p&gt;
&lt;p&gt;We can use the correlation operator to compute the convolution by flipping the kernel. In this case, the calculation can be represented using the dot product. We can also slightly adjust the indexing so that the first index is 0.&lt;/p&gt;
&lt;p&gt;\[
y_i = \sum_{k=0}^{2r} x_{i+k-r} w_{2r-k}.
\]&lt;/p&gt;
&lt;p&gt;Note that the convolution shown above would be undefined for \(i = 0\) and \(i = 1\) since the kernel would be accessing negative indices. Based on the definition, we would ignore these values. This is called a &lt;em&gt;valid&lt;/em&gt; convolution. The output size is then \(n - 2r\). There is also a &lt;em&gt;full&lt;/em&gt; convolution where the output size is \(n\). In this case, the kernel is padded with zeros so that it can be applied to all elements of the input.&lt;/p&gt;
&lt;h3 id=&#34;2d-convolution&#34;&gt;2D Convolution&lt;/h3&gt;
&lt;p&gt;Image convolutions use 2D filters applied to 2D images. For a filter with radius \(r\), size of the filter is \((2r + 1) \times (2r + 1)\). The convolution is then&lt;/p&gt;
&lt;p&gt;\[
(x * w)(i, j) = \sum_{-r}^r \sum_{-r}^r x(i-r, j-r)w(r, s).
\]&lt;/p&gt;
&lt;h2 id=&#34;properties-of-convolutions&#34;&gt;Properties of Convolutions&lt;/h2&gt;
&lt;p&gt;Convolutional networks are commonly built on &lt;em&gt;full&lt;/em&gt; or &lt;em&gt;valid&lt;/em&gt; convolutions. Other variants have also been explored. Here, we will briefly discuss the different properties of this operator. A more detailed treatment can be found in (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;padding&#34;&gt;Padding&lt;/h3&gt;
&lt;p&gt;By definition, a convolution of an input with a filter of size \(n\times n\) will produce an output of size \((m-n+1)\times(m-n+1)\), where \(m\) is the size of the input. This means that the output will be smaller than the input. This is often referred to as a &lt;strong&gt;valid&lt;/strong&gt; convolution. The figure below shows a convolution between a \(3\times3\) kernel and a \(5\times5\) input.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2023-10-26_16-31-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A valid convolution (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A valid convolution (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The output of this convolution is a \(3\times3\) feature map. This is a problem if we want to build a deep network. Each convolution will reduce the size of the input. If we were to stack multiple convolutional layers, the output would eventually be too small to be useful. If we want our output to be same size as the input, we can add padding to the original input image before convolving it. This is often known as a &lt;strong&gt;full&lt;/strong&gt; convolution. An example is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2023-10-26_16-34-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;A full convolution (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;A full convolution (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;stride&#34;&gt;Stride&lt;/h3&gt;
&lt;p&gt;So far, we have only looked at convolutions which step by 1 unit as they shift over the image. We can control the size of this step, or &lt;strong&gt;stride&lt;/strong&gt;, to produce different outcomes. Picking a non-unit stride has a number of effects on the features that are learned in a convolutional neural network.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dimensionality reduction&lt;/strong&gt;: Skipping over pixels reduces the size of the output feature map. This provides another way of downsampling the input.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Less computation&lt;/strong&gt;: Fewer computations are required to produce the output feature map.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Increased field of view&lt;/strong&gt;: A larger stride increases the field of view of the kernel, leading to larger receptive fields in deeper layers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given an input of size \(m\times m\) and a kernel of size \(n\times n\), the output size of a convolution with stride \(s\) is given by&lt;/p&gt;
&lt;p&gt;\[
\left\lfloor\frac{m-n}{s}\right\rfloor + 1.
\]&lt;/p&gt;
&lt;p&gt;The figure below shows a convolution with stride 2 on a \(5\times5\) input.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2023-10-26_16-45-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;A convolution with stride 2 (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;A convolution with stride 2 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;kernel-size&#34;&gt;Kernel Size&lt;/h3&gt;
&lt;p&gt;The size of the kernel has a large impact on the features that are learned. A larger kernel will have a larger receptive field. This means that the kernel will be able to capture more information about the input. However, this comes at the cost of increased computation. Common kernel sizes in most CNNs are \(3\times3\), \(5\times5\), and \(7\times7\). It is also convenient to pick an odd kernel size so that the kernel has a center pixel.&lt;/p&gt;
&lt;h3 id=&#34;dilation&#34;&gt;Dilation&lt;/h3&gt;
&lt;p&gt;Around 2015, a research trend for CNNs was to find a way to increase the receptive field without adding more parameters. The result is a &lt;strong&gt;dilated&lt;/strong&gt; convolution. The output of a dilated convolution is computed by skipping over pixels in the input. The figure below shows a \(3\times3\) kernel with a dilation of 2.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2023-10-27_08-19-10_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;A dilated convolution (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;A dilated convolution (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The output size is computed as&lt;/p&gt;
&lt;p&gt;\[
\left\lfloor\frac{m + 2p - n - (n-1)(d-1)}{s}\right\rfloor + 1,
\]&lt;/p&gt;
&lt;p&gt;where \(p\) is the amount of padding and \(d\) is the dilation factor.&lt;/p&gt;
&lt;h2 id=&#34;implementing-a-convolution-kernel&#34;&gt;Implementing a Convolution Kernel&lt;/h2&gt;
&lt;p&gt;It is straightforward to write the convolution operation in CUDA C++. Each thread will compute the value for a single output pixel using the filter. We already implemented something very similar with the blurring kernel. The kernel itself should accept the following arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The input image&lt;/li&gt;
&lt;li&gt;The output image&lt;/li&gt;
&lt;li&gt;The kernel&lt;/li&gt;
&lt;li&gt;The radius of the kernel&lt;/li&gt;
&lt;li&gt;The width of the output image&lt;/li&gt;
&lt;li&gt;The height of the output image&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A more robust implementation would consider things like padding, stride, dilation, and whether or not a valid or full convolution is desired. For now, we will focus on the simplest case: a valid convolution with a stride of 1 and no padding or dilation. First, let&amp;rsquo;s review the initial naive solution from &lt;em&gt;Programming Massively Parallel Processors&lt;/em&gt; (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void conv2D(float *input, float *filter, float *output,
                       int r, int width, int height) {
    int outCol = blockIdx.x * blockDim.x + threadIdx.x;
    int outRow = blockIdx.y * blockDim.y + threadIdx.y;

    float sum = 0.0f;
    for (int row = 0; row &amp;lt; 2*r+1; row++) {
        for (int col = 0; col &amp;lt; 2*r+1; col++) {
            int inRow = outRow + row - r;
            int inCol = outCol + col - r;
            if (inRow &amp;gt;= 0 &amp;amp;&amp;amp; inRow &amp;lt; height &amp;amp;&amp;amp; inCol &amp;gt;= 0 &amp;amp;&amp;amp; inCol &amp;lt; width) {
                sum += input[inRow * width + inCol] * filter[row * (2*r+1) + col];
            }
        }
    }
    output[outRow * width + outCol] = sum;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With this kernel, the input and output sizes are assumed to be the same. There is a boundary check in the inner-most loop to account for pixels for which a convolution cannot be computed. Based on this check, we can see that this kernel is performing a &lt;em&gt;valid&lt;/em&gt; convolution. The extra \(2r\) pixels on each side of the output are skipped. This presents a computational problem in the form of control divergence. Recall that all threads in a warp must execute the same instruction. If the boundary check fails for some threads, they will still execute the instructions in the loop, but will not contribute to the output. This is a waste of resources.&lt;/p&gt;
&lt;p&gt;It is also a waste of resources in terms of memory used for the output. If we already know that we want to perform a valid convolution, we can allocate the output image to be the appropriate size before calling it. A slightly modified version is shown below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void conv2D(float *input, float *filter, float *output,
                       int r, int width, int height) {
    int outCol = blockIdx.x * blockDim.x + threadIdx.x;
    int outRow = blockIdx.y * blockDim.y + threadIdx.y;

    float sum = 0.0f;
    for (int row = 0; row &amp;lt; 2*r+1; row++) {
        for (int col = 0; col &amp;lt; 2*r+1; col++) {
            int inRow = outRow + row;
            int inCol = outCol + col;
            sum += input[inRow * width + inCol] * filter[row * (2*r+1) + col];
        }
    }
    output[outRow * width + outCol] = sum;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;constant-memory-and-caching&#34;&gt;Constant Memory and Caching&lt;/h2&gt;
&lt;p&gt;There is a much larger issue present in both versions of this kernel in terms of memory bandwidth. Similar to the matrix multiplication kernel, this kernel can benefit from tiling. However, there is a new problem that arises specifically with convolution. The same filter is accessed by every single thread. This filter does not change for the entire duration of the kernel. This means that we are wasting memory bandwidth by having every thread access the same filter.&lt;/p&gt;
&lt;p&gt;Given its relatively small size, this kernel is a perfect candidate for constant memory. This is a special type of memory that is cached on the GPU. It is read-only and has a limited size, but it is much faster than global memory. We can write to the devices constant memory from the host code.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;#define FILTER_RADIUS 1
__constant__ float kFilter_d[2*FILTER_RADIUS+1][2*FILTER_RADIUS+1];
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This informs the compiler to allocate a 2D array of floats in constant memory. The size of the array is determined by the constant `FILTER_RADIUS`. We can then copy the filter to the device using the `cudaMemcpyToSymbol` function.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;cudaMemcpyToSymbol(kFilter_d, filter_h, (2*FILTER_RADIUS+1)*(2*FILTER_RADIUS+1)*sizeof(float));
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The line above assumes there is some data on the host in the array `filter_h`. This array is copied to the device. A small note on naming convention, &lt;a href=&#34;https://google.github.io/styleguide/cppguide.html#Constant_Names&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Google&amp;rsquo;s C++&lt;/a&gt; style guide recommends naming constant variables with a &lt;code&gt;k&lt;/code&gt; prefix. I have adopted this convention here.&lt;/p&gt;
&lt;p&gt;At this point, &lt;code&gt;kFilter_d&lt;/code&gt; is accessible from the kernel as a global variable. There is no need to pass it as an argument. The kernel can be modified to use this constant memory as follows.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void conv2D(float *input, float *output,
                       int r, int width, int height) {
    int outCol = blockIdx.x * blockDim.x + threadIdx.x;
    int outRow = blockIdx.y * blockDim.y + threadIdx.y;

    float sum = 0.0f;
    for (int row = 0; row &amp;lt; 2*r+1; row++) {
        for (int col = 0; col &amp;lt; 2*r+1; col++) {
            int inRow = outRow + row;
            int inCol = outCol + col;
            sum += input[inRow * width + inCol] * F_d[row][col];
        }
    }
    output[outRow * width + outCol] = sum;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you organize your files such that the kernel is in a separate file from the host code, you will need to declare the constant variable in the kernel file as well.&lt;/p&gt;
&lt;p&gt;Constant memory variables are stored in DRAM with global memory. The CUDA runtime will cache them since it knows they will not be modified. Processors use caches to reduce the latency of memory accesses by keeping frequently used data in a small, fast memory that is often located directly on the chip. This type of &lt;em&gt;constant cache&lt;/em&gt; is preferable to one that would support high-throughput writes in terms of chip design. It would require specialized hardware to support both which would increase the cost of the chip.&lt;/p&gt;
&lt;h2 id=&#34;tiled-convolutions&#34;&gt;Tiled Convolutions&lt;/h2&gt;
&lt;p&gt;Even with caching, the convolutional kernel still makes many accesses to DRAM. Similar to matrix multiplication, we can tile the input image to reduce the number of accesses. Similar to that example, we will use a \(4 \times 4\) tile size. If the input is a \(16 \times 16\) image and we apply a kernel with radius \(r=2\), the output image under a valid convolution will be \(12 \times 12\). This is visualized in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-19_16-35-39_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Left: The input image and its tiling. Middle: the filter. Right: The output image and its tiling.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Left: The input image and its tiling. Middle: the filter. Right: The output image and its tiling.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The parallel solution to this problem will follow the tiled approach used for matrix multiplication. One key difference in this case is that the input tile size will be larger than the output tile size. This size difference would further be complicated if we left the kernel size as a parameter.&lt;/p&gt;
&lt;p&gt;Following the design presented by (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;) in Chapter 7, there are two immediate approaches to this problem based on the tile size. The first is to choose a block size that matches the size of the input tiles. The benefit to this approach is that each thread can load a single input element into shared memory. The drawback is that some of the threads will be disabled when computing the output value since the output tile is smaller. This is a form of control divergence and will result in wasted resources.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;#define FILTER_RADIUS 1
#define IN_TILE_DIM 4
#define OUT_TILE_DIM ((IN_TILE_DIM) - 2*(FILTER_RADIUS))
__constant__ float kFilter_d[2*FILTER_RADIUS+1][2*FILTER_RADIUS+1];
__global__ void conv2DTiledConstKernel(float *input, float *output,
                                       int width, int height) {
    __shared__ float inputTile[IN_TILE_DIM][IN_TILE_DIM];
    // Input tile coordinates
    int col = blockIdx.x * OUT_TILE_DIM + threadIdx.x;
    int row = blockIdx.y * OUT_TILE_DIM + threadIdx.y;
    if (row &amp;lt; height &amp;amp;&amp;amp; col &amp;lt; width) {
        inputTile[threadIdx.y][threadIdx.x] = input[row * width + col];
    } else {
        inputTile[threadIdx.y][threadIdx.x] = 0.0f;
    }
    __syncthreads();

    // Output tile coordinates
    int tileCol = threadIdx.x - FILTER_RADIUS;
    int tileRow = threadIdx.y - FILTER_RADIUS;

    // In a valid convolution, the output is smaller than the input
    row -= FILTER_RADIUS;
    col -= FILTER_RADIUS;

    if (tileCol &amp;gt;= 0 &amp;amp;&amp;amp; tileCol &amp;lt; OUT_TILE_DIM &amp;amp;&amp;amp; tileRow &amp;gt;= 0 &amp;amp;&amp;amp; tileRow &amp;lt; OUT_TILE_DIM) {
        float sum = 0.0f;
        for (int fRow = 0; fRow &amp;lt; 2*FILTER_RADIUS+1; fRow++) {
            for (int fCol = 0; fCol &amp;lt; 2*FILTER_RADIUS+1; fCol++) {
                sum += inputTile[tileRow + fRow][tileCol + fCol] * kFilter_d[fRow][fCol];
            }
        }
        output[row * (width - 2 * FILTER_RADIUS) + col] = sum;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;There are a few things to consider here. The first phase of this kernel collaboratively loads data into a shared memory space, similar to what we have seen before. This kernel assumes a convenient indexing scheme where the row and column will always be &amp;gt;= 0. We could adopt a scheme that centers the convolution on the center point of the kernel by allowing for negative indices. In this case, it would be necessary to check if the row and column are less than 0. This implementation only needs to verify that the row and column are within the given image size.&lt;/p&gt;
&lt;p&gt;When it comes to computing the output, not every thread will contribute. This is depicted by the lightly shaded areas in the figure below. You should also note which threads are active for output computation per block. In this simple example, a \(3 \times 3\) filter is used. The input tile dimension is \(4 \times 4\) which means the output tile will be \(2 \times 2\). Only the threads corresponding to the darker blue on the left contribute to the output calculation. Since this one block computes 4 output values, the next block should start 2 units to the right of this one.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-21_16-08-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;The active threads for computing the output tile.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;The active threads for computing the output tile.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;performance-analysis&#34;&gt;Performance Analysis&lt;/h3&gt;
&lt;p&gt;The purpose of this approach was to increase the ratio of arithmetic operations to global memory accesses. For the threads that compute an output tile, there is one multiplication and one addition which yields \(\mathtt{OUT\_TILE\_DIM}^2*(2*\mathtt{FILTER\_RADIUS} + 1)^2*2\) operations total. Each thread in the input tile loads a single &lt;code&gt;float&lt;/code&gt; value for a total of \(\mathtt{IN\_TILE\_DIM}^2 * 4\) bytes. For our small example above, this gives&lt;/p&gt;
&lt;p&gt;\[
\frac{2^2 * 3^2 * 2}{4^2 * 4} = 1.125\ \text{Ops/byte}.
\]&lt;/p&gt;
&lt;p&gt;In a more realistic example, we would maximize our input tile size to take advantage of the available threads on the device. Currently, the maximum number of supported threads is 1024. This allows for an input tile size of \(32 \times 32\). The resulting operations per byte under this tile size is&lt;/p&gt;
&lt;p&gt;\[
\frac{30^2 * 3^2 * 2}{32^2 * 4} = 3.955\ \text{Ops/byte}.
\]&lt;/p&gt;
&lt;p&gt;This ratio increases with the size of the filter.&lt;/p&gt;
&lt;h2 id=&#34;caching-the-halo-cells&#34;&gt;Caching the Halo Cells&lt;/h2&gt;
&lt;p&gt;In the previous example, the size of the input tile compared to the output tile means that there were some threads that did not contribute to the output computation. These are the threads managing the lightly shaded cells in the figure above. We will refer to these as &lt;em&gt;halo cells&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This implementation is going to take advantage of the caching behavior in the chip itself. &lt;strong&gt;Values that have been recently used are more likely to already be in L2 cache.&lt;/strong&gt; This is a safe assumption since the neighboring blocks will have loaded these values into shared memory. This means that the input and output tile sizes can be the same; there is no need to waste any threads in the block. The full kernel is given below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void conv2DTiledCachedConstKernel(float *input, float *output,
                                             int width, int height) {
    __shared__ float inputTile[IN_TILE_DIM][IN_TILE_DIM];
    // Input tile coordinates
    int col = blockIdx.x * IN_TILE_DIM + threadIdx.x;
    int row = blockIdx.y * IN_TILE_DIM + threadIdx.y;
    if (row &amp;lt; height &amp;amp;&amp;amp; col &amp;lt; width) {
        inputTile[threadIdx.y][threadIdx.x] = input[row * width + col];
    } else {
        inputTile[threadIdx.y][threadIdx.x] = 0.0f;
    }
    __syncthreads();

    if (row &amp;lt; FILTER_RADIUS || col &amp;lt; FILTER_RADIUS || col &amp;gt;= (width - FILTER_RADIUS) || row &amp;gt;= (height - FILTER_RADIUS)) return;

    // Output tile coordinates
    row -= FILTER_RADIUS;
    col -= FILTER_RADIUS;
    int tileCol = threadIdx.x - FILTER_RADIUS;
    int tileRow = threadIdx.y - FILTER_RADIUS;

    float sum = 0.0f;
    for (int fRow = 0; fRow &amp;lt; 2 * FILTER_RADIUS + 1; fRow++) {
        for (int fCol = 0; fCol &amp;lt; 2 * FILTER_RADIUS + 1; fCol++) {
            // If this value is in shared memory, access it there
            if (tileCol + fCol &amp;gt;= 0 &amp;amp;&amp;amp;
                tileCol + fCol &amp;lt; IN_TILE_DIM &amp;amp;&amp;amp;
                tileRow + fRow &amp;gt;= 0 &amp;amp;&amp;amp;
                tileRow + fRow &amp;lt; IN_TILE_DIM) {
                sum += inputTile[tileRow + fRow][tileCol + fCol] * kFilter_d[fRow][fCol];
            } else {
                // Otherwise, access it from global memory
                sum += input[(row + fRow) * width + (col + fCol)] * kFilter_d[fRow][fCol];
            }
        }
    }

    output[row * (width - 2 * FILTER_RADIUS) + col] = sum;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Dumoulin, Vincent, and Francesco Visin. 2018. A Guide to Convolution Arithmetic for Deep Learning. &lt;i&gt;Arxiv:1603.07285 [Cs, Stat]&lt;/i&gt;, January. &lt;a href=&#34;http://arxiv.org/abs/1603.07285&#34;&gt;http://arxiv.org/abs/1603.07285&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Profiling CUDA Applications</title>
      <link>http://localhost:1313/notes/profiling_cuda_applications/</link>
      <pubDate>Mon, 15 Jan 2024 14:48:00 -0600</pubDate>
      
      <guid>http://localhost:1313/notes/profiling_cuda_applications/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#overview-of-nsight&#34;&gt;Overview of Nsight&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getting-started-with-nsight&#34;&gt;Getting Started with Nsight&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-matrix-multiplication&#34;&gt;Case Study: Matrix Multiplication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tips-and-best-practices&#34;&gt;Tips and Best Practices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ocl-notes&#34;&gt;OCL Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;overview-of-nsight&#34;&gt;Overview of Nsight&lt;/h2&gt;
&lt;p&gt;NVIDIA NSight Compute is a profiling tool for CUDA kernels. It features an expert system that can help you identify performance bottlenecks in your code. It is essential for methodically optimizing your code. These notes will cover the basics of using Nsight Compute to profile your CUDA applications.&lt;/p&gt;
&lt;h2 id=&#34;getting-started-with-nsight&#34;&gt;Getting Started with Nsight&lt;/h2&gt;
&lt;h3 id=&#34;profiling-our-first-program&#34;&gt;Profiling our first program&lt;/h3&gt;
&lt;p&gt;In Lab 0, you implemented a vector addition kernel that is &lt;em&gt;embarrassingly parallel&lt;/em&gt;. We will now use Nsight to profile its performance. Realistically, there is not much we can do to increase the performance of this kernel, but it will still help us understand the information that Nsight gives. To profile the application, simply launch &lt;code&gt;ncu&lt;/code&gt; with your application.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ncu ./build/main&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Depending on where you are running this program, it may be necessary to launch it with &lt;code&gt;sudo&lt;/code&gt;. If everything was successful, it will output something similar to the following:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nsight Output&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vec_add(float *, float *, float *, int), 2024-Jan-16 10:42:52, Context 1, Stream 7
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Section: GPU Speed Of Light Throughput
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  ---------------------------------------------------------------------- --------------- ------------------------------
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  DRAM Frequency                                                           cycle/nsecond                           5.71
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  SM Frequency                                                             cycle/nsecond                           1.15
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Elapsed Cycles                                                                   cycle                          3,279
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Memory [%]                                                                           %                           7.54
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  DRAM Throughput                                                                      %                           7.54
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Duration                                                                       usecond                           2.85
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  L1/TEX Cache Throughput                                                              %                           4.32
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  L2 Cache Throughput                                                                  %                           4.86
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  SM Active Cycles                                                                 cycle                         623.58
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Compute (SM) [%]                                                                     %                           0.82
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  ---------------------------------------------------------------------- --------------- ------------------------------
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        waves across all SMs. Look at Launch Statistics for more details.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Section: Launch Statistics
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  ---------------------------------------------------------------------- --------------- ------------------------------
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Block Size                                                                                                        256
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Function Cache Configuration                                                                  cudaFuncCachePreferNone
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Grid Size                                                                                                          16
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Registers Per Thread                                                   register/thread                             16
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Shared Memory Configuration Size                                                 Kbyte                           8.19
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Driver Shared Memory Per Block                                             Kbyte/block                           1.02
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Dynamic Shared Memory Per Block                                             byte/block                              0
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Static Shared Memory Per Block                                              byte/block                              0
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Threads                                                                         thread                          4,096
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Waves Per SM                                                                                                     0.07
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  ---------------------------------------------------------------------- --------------- ------------------------------
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  WRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU&amp;#39;s 38
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        concurrently with other workloads, consider reducing the block size to have at least one block per
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        description for more details on launch configurations.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Section: Occupancy
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  ---------------------------------------------------------------------- --------------- ------------------------------
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Block Limit SM                                                                   block                             16
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Block Limit Registers                                                            block                             16
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Block Limit Shared Mem                                                           block                            100
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Block Limit Warps                                                                block                              6
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Theoretical Active Warps per SM                                                   warp                             48
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Theoretical Occupancy                                                                %                            100
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Achieved Occupancy                                                                   %                          15.85
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Achieved Active Warps Per SM                                                      warp                           7.61
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  ---------------------------------------------------------------------- --------------- ------------------------------
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  WRN   This kernel&amp;#39;s theoretical occupancy is not impacted by any block limit. The difference between calculated
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        theoretical (100.0%) and measured achieved occupancy (15.9%) can be the result of warp scheduling overheads
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        as well as across blocks of the same kernel. See the CUDA Best Practices Guide
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        optimizing occupancy.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;viewing-results-in-the-gui&#34;&gt;Viewing Results in the GUI&lt;/h3&gt;
&lt;p&gt;Nsight comes with both CLI and GUI clients. It is recommended to parse the information from the GUI. The GUI can launch programs both locally and remotely. It can also display the result of a previous launch. To generate a profiling report for our vector addition kernel, run the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ncu -o vec_add_profile ./build/main&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The argument after &lt;code&gt;-o&lt;/code&gt; is the name of the output file. Open Nsight Compute and load the saved file. It should look something like this.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-16_11-59-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Nsight Compute GUI output&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Nsight Compute GUI output
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This basic report only includes three sections: GPU Speed of Light, Launch Statistics, and Occupancy Analysis. We will go over each of these sections in detail.&lt;/p&gt;
&lt;h3 id=&#34;gpu-speed-of-light&#34;&gt;GPU Speed of Light&lt;/h3&gt;
&lt;p&gt;This section displays high level aspects of your kernel. The main metrics report what your application is doing relative to peak performance. Sparing the full details of the documentation, the most important metrics are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Duration: The total time spent executing the kernel. This is the most important metric for performance.&lt;/li&gt;
&lt;li&gt;SM &lt;code&gt;[%]&lt;/code&gt;: The relative throughput of the SMs as compared to the theoretical maximum.&lt;/li&gt;
&lt;li&gt;Memory &lt;code&gt;[%]&lt;/code&gt;: The relative throughput of the memory as compared to the theoretical maximum.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Do not get lost in the numbers!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Remember that this tool is simply reporting facts about your kernel. Take care not to misinterpret the data. In the run from above, the kernel throughput is only 0.85%. There are a number of reasons as to why this number is so low.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Latency Issues: The kernel may have to wait for memory operations, resulting in a low throughput.&lt;/li&gt;
&lt;li&gt;Workload Characteristics: Your particular kernel may not need to do much work, resulting in a low throughput.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;launch-statistics&#34;&gt;Launch Statistics&lt;/h3&gt;
&lt;p&gt;This section shows us the launch configuration that was used for this kernel. In our earlier programs, we may set these manually for testing. Later on, we will want our programs to adapt to changing input sizes, so these statistics will becomes more useful.&lt;/p&gt;
&lt;p&gt;More importantly, this shows you the resource usage per block.&lt;/p&gt;
&lt;p&gt;If you are profiling an application for which you are not familiar with the code, it is convenient to know the grid and block sizes that were used when launching the kernel.&lt;/p&gt;
&lt;h3 id=&#34;occupancy-analysis&#34;&gt;Occupancy Analysis&lt;/h3&gt;
&lt;h3 id=&#34;memory-workload-analysis&#34;&gt;Memory Workload Analysis&lt;/h3&gt;
&lt;h2 id=&#34;case-study-matrix-multiplication&#34;&gt;Case Study: Matrix Multiplication&lt;/h2&gt;
&lt;h2 id=&#34;tips-and-best-practices&#34;&gt;Tips and Best Practices&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Do not confuse high throughput for high performance. Throughput is a measure of how much work is being done, not how fast it is being done.&lt;/li&gt;
&lt;li&gt;Using a larger grid size is not always better. More grids introduce more overhead and can lead to lower performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ocl-notes&#34;&gt;OCL Notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Analysis Driven Optimization&lt;/li&gt;
&lt;li&gt;Understanding Performance Limiters&lt;/li&gt;
&lt;li&gt;Metrics Review&lt;/li&gt;
&lt;li&gt;Memory Bound Analysis&lt;/li&gt;
&lt;li&gt;Compute Bound Analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Goals&lt;/strong&gt;
Make efficient use of memory subsystem
Expose enough parallelism to hide latency&lt;/p&gt;
&lt;h3 id=&#34;analysis-driven-optimization&#34;&gt;Analysis Driven Optimization&lt;/h3&gt;
&lt;p&gt;Cyclical process&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Profile&lt;/li&gt;
&lt;li&gt;Determine Limiter&lt;/li&gt;
&lt;li&gt;Inspect&lt;/li&gt;
&lt;li&gt;Optimize&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Determine if memory or compute bound. If neither, analyze where the latency is coming from.&lt;/p&gt;
&lt;h3 id=&#34;metrics&#34;&gt;Metrics&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Latency&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sm efficiency&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dram utilization&lt;/li&gt;
&lt;li&gt;L2 utilization&lt;/li&gt;
&lt;li&gt;shared utilization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Compute&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DP utilization&lt;/li&gt;
&lt;li&gt;SP utilization&lt;/li&gt;
&lt;li&gt;HP utilization&lt;/li&gt;
&lt;li&gt;TC utilization&lt;/li&gt;
&lt;li&gt;Integer&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>GPU Performance Basics</title>
      <link>http://localhost:1313/notes/gpu_performance_basics/</link>
      <pubDate>Sun, 14 Jan 2024 13:31:00 -0600</pubDate>
      
      <guid>http://localhost:1313/notes/gpu_performance_basics/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#memory-coalescing&#34;&gt;Memory Coalescing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hiding-memory-latency&#34;&gt;Hiding Memory Latency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#thread-coarsening&#34;&gt;Thread Coarsening&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimization-checklist&#34;&gt;Optimization Checklist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#identifying-bottlenecks&#34;&gt;Identifying Bottlenecks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-takeaway&#34;&gt;The Takeaway&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;These notes are on &amp;ldquo;Chapter 6: Performance Considerations&amp;rdquo; from the book &lt;em&gt;Programming Massively Parallel Processors&lt;/em&gt; (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;memory-coalescing&#34;&gt;Memory Coalescing&lt;/h2&gt;
&lt;p&gt;Global memory accesses are one of the largest bottlenecks in GPU applications.
DRAM has high latency based on its design. Each cell has a transistor and a capacitor. If the capacitor is charged, it represents a 1. The process to detect the charges in these cells is on the order of 10s of nanoseconds. DRAM can read consecutive groups of cells via &lt;em&gt;bursts&lt;/em&gt;. This means that if the data we wish to access is stored consecutively, it can be accessed within the same burst. Contrast that was random access, in which the DRAM will have to make multiple bursts to read the required data. &lt;strong&gt;Memory coalescing&lt;/strong&gt; refers to optimizing our global memory accesses to take advantage of DRAM bursts.&lt;/p&gt;
&lt;p&gt;Matrices are &lt;em&gt;naturally coalesced&lt;/em&gt;, so we have already been utilizing this performance pattern in previous examples.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-15_13-00-31_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Memory accesses for a matrix in row-major ordering (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Memory accesses for a matrix in row-major ordering (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Strategies to optimize code for memory coalescing are to rearrange the threads, the data, or transfer the data first to shared memory so that accesses are faster, referred to as &lt;strong&gt;corner turning&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;example-matrix-multiplication&#34;&gt;Example: Matrix Multiplication&lt;/h3&gt;
&lt;p&gt;Consider \(C = AB\), where \(A\) is in row-major order and \(B\) is in column-major order. The naive implementation of this algorithm will have poor memory coalescing. The figure below demonstrates the memory accesses for this scenario. The values required are not consecutive in memory, so the DRAM will have to make multiple bursts to read the data.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-14_20-47-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Memory accesses for a matrix in column-major ordering (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Memory accesses for a matrix in column-major ordering (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The accesses to the elements in \(B\) will be slower since the data is not coalesced. Accessing the elements &lt;em&gt;is&lt;/em&gt; efficient if we assign \(N\) consecutive threads to load \(N\) consecutive elements from the same column of \(B\). This works in conjunction with tiling. The original loads to shared memory pull from consecutive elements in \(B\) which allows the application to take advantage of DRAM bursts. Once the data is in shared memory, the rest of the algorithm can be performed with coalesced accesses. Shared memory uses SRAM instead of DRAM, so coalescing is not an issue.&lt;/p&gt;
&lt;h2 id=&#34;hiding-memory-latency&#34;&gt;Hiding Memory Latency&lt;/h2&gt;
&lt;p&gt;DRAMS have &lt;em&gt;banks&lt;/em&gt; and &lt;em&gt;channels&lt;/em&gt;. A controller has a bus that connects banks to the processor. When the DRAM accesses data, the decoder enables the cells so that they can share the information stored with the sensing amplifier. This presents a high latency relative to the time it takes to actually transfer the data. This is why there are multiple banks per channel. The controller can initiate accesses on other banks instead of sitting and waiting for a single bank to finish.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-14_17-19-55_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Single versus Multi-bank burst timings (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Hwu, Kirk, and El Hajj 2022&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Single versus Multi-bank burst timings (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;It is possible that the controller will initiate a request to a bank that is already busy. This is called a &lt;em&gt;bank conflict&lt;/em&gt;. The controller will have to wait for the bank to finish its current request before it can service the new request. The more banks that are available, the less likely it is that a bank conflict will occur.&lt;/p&gt;
&lt;h3 id=&#34;example-matrix-multiplication&#34;&gt;Example: Matrix Multiplication&lt;/h3&gt;
&lt;p&gt;Consider DRAM with 4 channels and 2 banks per channel. The burst size of this DRAM is 8 bytes, or 2 elements. When data is written to DRAM in the first place, it is distributed in an interleaved fashion across the different channels and banks. The first figure below shows the input matrix \(M\) and output matrix \(P\). The second input matrix is omitted for brevity. The indices of \(M\) are linearized in row-major order to show how they are distributed across the DRAM banks. \(P\) is split into 4 blocks of size \(2 \times 2\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-15_13-55-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Matrix M with linearized indices and matrix P split into 4 blocks.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Matrix M with linearized indices and matrix P split into 4 blocks.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;\(M\) is loaded into DRAM in an interleaved fashion. The first 8 bytes are loaded into bank 0 of channel 0. The next 8 bytes go into bank 0 of channel 1, and so on. Each burst returns 8 bytes. While the first access is being performed on bank 0 channel 0, the controller can initiate a request to bank 0 channel 1. This is visualized in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-15_14-14-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;DRAM distribution for matrix M.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;DRAM distribution for matrix M.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Given the distribution visualized above, we can see that the data accesses for the blocks of \(P\) will be coalesced. Output tile 1 in matrix \(P\) requires $M_0, M_1, M_4,$ and \(M_5\). The first two are available in a single burst from channel 0 bank 0, and the second two are available in a single burst from channel 2 bank 0.&lt;/p&gt;
&lt;h2 id=&#34;thread-coarsening&#34;&gt;Thread Coarsening&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;price of parallelism&lt;/em&gt; may refer to the cost of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;launching threads&lt;/li&gt;
&lt;li&gt;synchronization&lt;/li&gt;
&lt;li&gt;redundant work&lt;/li&gt;
&lt;li&gt;redundant memory accesses&lt;/li&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It there are enough hardware resources available, parallelism at the finest level is ideal. If there are not enough resources, there is a price to pay for this parallelism. The hardware will need to serialize the work into blocks of threads that can be executed in parallel.&lt;/p&gt;
&lt;p&gt;If this is the case for a particular application, it may be beneficial to apply some form of &lt;strong&gt;thread coarsening&lt;/strong&gt;. If the hardware would serialize the work due to inefficient resources, the price of parallelism was paid for nothing. As the programmer, you have the ability to coarsen the threads to alleviate the price of parallelism.&lt;/p&gt;
&lt;h3 id=&#34;example-coarsening-tiled-matrix-multiplication&#34;&gt;Example: Coarsening Tiled Matrix Multiplication&lt;/h3&gt;
&lt;p&gt;In tiled matrix multiplication, it is possible that two separate blocks work with the same tile of data from an input matrix. We pay a price for this redundancy, but the benefit is that we can parallelize the work. If the hardware does not have sufficient resource, it will serialize these two blocks. This results in paying the price of data redundancy without the benefit of parallelism.&lt;/p&gt;
&lt;p&gt;Let \(A, B \in \mathbb{R}^{6 \times 6}\), then \(C = AB \in \mathbb{R}^{6 \times 6}\). If we use a \(2 \times 2\) tile size, then we have 9 blocks of work that can be executed concurrently. For argument&amp;rsquo;s sake, let&amp;rsquo;s say that the hardware can only execute 3 blocks of work at a time. We can use thread coarsening to reduce the number of blocks to 3. Each block will be responsible for a single row of the tiled output matrix. That is, if the output matrix is \(6 \times 6\), then each block will be responsible for a \(2 \times 6\) tile of the output matrix. This is visualized in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-14_21-42-55_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Thread coarsening for tiled matrix multiplication.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Thread coarsening for tiled matrix multiplication.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The block itself will perform a similar function as the implementation of tiled matrix multiplication we saw previously. We will need to modify the kernel so that it processes values to fill for 3 blocks of work, spanning each row. In the figure above, this is represented by the three gray, numbered blocks. Although each block uses a different column from matrix \(N\), they all use the same row from matrix \(M\). Our solution will take advantage of this reuse of data.&lt;/p&gt;
&lt;p&gt;Consider the thread that computes the value for the top left entry of block 1. This thread will compute the output value as normal before looping to compute the corresponding relative position in blocks 2 and 3. That is, if the first entry computed is \((0, 0)\) of block 1, then the next entry computed will be \((0, 0)\) of block 2, and so on. This is visualized by the three solid black cells in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-14_21-45-47_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;A single thread loops through three blocks as a result of thread coarsening.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;A single thread loops through three blocks as a result of thread coarsening.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The kernel code is given below. The additional loop controls the switch between the three consecutive tiles. The values from matrix &lt;code&gt;M&lt;/code&gt; are loaded inside the outer-most loop and are reused across the coarse tiles.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;#define TILE_WIDTH 2
#define COARSE_FACTOR 3
__global__ void matMulCoarse(float *M, float *N, float *P, int width) {
    __shared__ float Mds[TILE_WIDTH][TILE_WIDTH];
    __shared__ float Nds[TILE_WIDTH][TILE_WIDTH];

    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    // Identify the row and column of the element to work on
    int row = by * TILE_WIDTH + ty;
    int colStart = bx * TILE_WIDTH * COARSE_FACTOR;

    // Initialize Pvalue
    float Pvalue[COARSE_FACTOR];
    for (int i = 0; i &amp;lt; COARSE_FACTOR; i++) {
        Pvalue[i] = 0.0f;
    }

    // Loop over the tiles required to compute the current output value
    for (int ph = 0; ph &amp;lt; width / TILE_WIDTH; ph++) {
        Mds[ty][tx] = M[row * width + ph * TILE_WIDTH + tx];

        for (int c = 0; c &amp;lt; COARSE_FACTOR; c++) {
            int col = colStart + c * TILE_WIDTH;

            Nds[ty][tx] = N[(ph * TILE_WIDTH + ty) * width + col];
            __syncthreads();

            for (int k = 0; k &amp;lt; TILE_WIDTH; k++) {
                Pvalue[c] += Mds[ty][k] * Nds[k][tx];
            }
            __syncthreads();
        }
    }

    for (int c = 0; c &amp;lt; COARSE_FACTOR; c++) {
        int col = colStart + c * TILE_WIDTH;
        P[row * width + col] = Pvalue[c];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;how-to-use-coarsening-in-your-applications&#34;&gt;How to use coarsening in your applications&lt;/h3&gt;
&lt;p&gt;Thread coarsening is yet another technique that can be applied to optimize your parallel programs. The previous section demonstrated &lt;em&gt;how&lt;/em&gt; it can be applied, but you are probably wondering &lt;em&gt;when&lt;/em&gt; it should be applied. Deciding on whether to apply this technique is largely determined by careful analysis of your current application. This analysis should include benchmarking and profiling. There is work that provides an automatic solution (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Stawinoga and Field 2018&lt;/a&gt;), but we will rely on determining that for ourselves.&lt;/p&gt;
&lt;p&gt;For now, we can at least discuss when &lt;em&gt;not&lt;/em&gt; to apply coarsening. The most obvious instance is when coarsening is completely unnecessary. Consider the vector addition kernel. Each thread performs an independent computation that has no overlapping data with other thread. There is no need to apply coarsening in this case.&lt;/p&gt;
&lt;p&gt;Another bad case for implementation would be when the coarsening factor causes hardware to be underutilized. Parallelization in hardware is scalable. If we take away the opporunity for scale, there may be unused compute. This is typically something we can determine via benchmarking.&lt;/p&gt;
&lt;p&gt;In the coarsened version of matrix multiplication above, we had to create additional private variables to store the coarsened values. These use additional registers per thread. If our application required more than the 32 registers available on our H100, for example, this would have a direct effect on occupancy. Keep that in mind when developing your thread coarsened solution.&lt;/p&gt;
&lt;h2 id=&#34;optimization-checklist&#34;&gt;Optimization Checklist&lt;/h2&gt;
&lt;p&gt;Section 6.4 of &lt;em&gt;Programming Massively Parallel Processors&lt;/em&gt; (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;) provides a checklist of items to consider when optimizing your GPU applications. These are summarized below.&lt;/p&gt;
&lt;h3 id=&#34;maximizing-occupancy&#34;&gt;Maximizing Occupancy&lt;/h3&gt;
&lt;p&gt;Having more threads than physical cores available is beneficial, as it hides the latency required for other operations such as data fetching. Instead of waiting on some operation to return, the hardware can switch to another thread to perform work. This is implemented by adjusting the launch configurations or optimizing the number of registers used per thread, for example. We also discussed solutions for hiding memory-based latency.&lt;/p&gt;
&lt;h3 id=&#34;coalesced-global-memory-accesses&#34;&gt;Coalesced Global Memory Accesses&lt;/h3&gt;
&lt;p&gt;Random accesses to memory are less efficient than consecutive ones. This is a theme that is repeated through many themes of computer science, such as sorting. Understanding of how the underlying hardware works brought to light new ways to optimize our applications. We can rearrange our data to take advantage of DRAM bursts, or we can use shared memory to reduce the latency of memory accesses.&lt;/p&gt;
&lt;h3 id=&#34;minimizing-control-divergence&#34;&gt;Minimizing Control Divergence&lt;/h3&gt;
&lt;p&gt;Although we have not used any applications that exhibit control divergence, we have studied the concept. During SIMD execution, the hardware executes the same instructions on multiple data elements. If a thread or group of threads would diverge from the others, the hardware would have to make multiple passes to cover all of the possible paths.&lt;/p&gt;
&lt;h3 id=&#34;tiling&#34;&gt;Tiling&lt;/h3&gt;
&lt;p&gt;Global memory accesses exhibit higher latency due to the nature of DRAM. We can reduce the number of global memory accesses by using shared memory. This was exemplified in the tiled matrix multiplication examples, where there are many redundant data accesses. Moving these data to shared memory reduces the number of global memory accesses.&lt;/p&gt;
&lt;h3 id=&#34;thread-coarsening&#34;&gt;Thread Coarsening&lt;/h3&gt;
&lt;p&gt;In cases where the hardware would serialize execution of a kernel, thread coarsening can eliminate redundant work. In the tiled matrix multiplication example, we saw that the hardware would serialize execution of the kernel if there were not enough resources available. In this case, the same redundant loads to shared memory would be performed. To reduce this overhead, we coarsened the thread by having a single kernel perform the work of multiple blocks.&lt;/p&gt;
&lt;h2 id=&#34;identifying-bottlenecks&#34;&gt;Identifying Bottlenecks&lt;/h2&gt;
&lt;p&gt;Knowing when to apply each of these optimization techniques comes down to understanding your application. &lt;strong&gt;The single most important step in optimizing your application is to identify the bottleneck&lt;/strong&gt;. What resource is limiting the performance of your solution? Benchmarking and profiling are two techniques that can be used to identify these bottlenecks. We will begin learning these tools in the next lecture.&lt;/p&gt;
&lt;h2 id=&#34;the-takeaway&#34;&gt;The Takeaway&lt;/h2&gt;
&lt;p&gt;At this point, you have learned the basics of GPU programming with CUDA. You should be familiar with writing kernels, setting launch configurations, and compiling them. You should be familiar with a few optimization techniques that can be applied to your applications, but you are probably not confident in your ability to identify when they should be used.&lt;/p&gt;
&lt;p&gt;The next module of this course will focus on problems for which a straightforward solution is not obvious. These are problems that come from other domains of computer science, such as graph theory and linear algebra. We will learn how to apply the techniques we have learned to these problems, and we will learn new techniques that are specific to these problems. Even though the applications themselves may be specific, the techniques used to optimize them are not.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Stawinoga, Nicolai, and Tony Field. 2018. Predictable Thread Coarsening. &lt;i&gt;Acm Transactions on Architecture and Code Optimization&lt;/i&gt; 15 (2): 23:123:26. &lt;a href=&#34;https://doi.org/10.1145/3194242&#34;&gt;https://doi.org/10.1145/3194242&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>CUDA Memory Architecture</title>
      <link>http://localhost:1313/notes/cuda_memory_architecture/</link>
      <pubDate>Thu, 11 Jan 2024 15:07:00 -0600</pubDate>
      
      <guid>http://localhost:1313/notes/cuda_memory_architecture/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#memory-access&#34;&gt;Memory Access&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#memory-types&#34;&gt;Memory Types&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tiling&#34;&gt;Tiling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-tiled-matrix-multiplication&#34;&gt;Example: Tiled Matrix Multiplication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#boundary-checking&#34;&gt;Boundary Checking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#memory-use-and-occupancy&#34;&gt;Memory Use and Occupancy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dynamically-changing-the-block-size&#34;&gt;Dynamically Changing the Block Size&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-takeaway&#34;&gt;The Takeaway&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;So far, the kernels we have used assume everything is on global memory. Even though there are thousands of cores that can effectively hide the latency of transferring data to and from global memory, we will see this delay will become a bottleneck in many applications. These notes explore the different types of memory available on the GPU and how to use them effectively.&lt;/p&gt;
&lt;h2 id=&#34;memory-access&#34;&gt;Memory Access&lt;/h2&gt;
&lt;p&gt;Transferring memory is one of the biggest bottlenecks in GPU programming. Companies like NVIDIA devote a lot of resources to improving the bandwidth and latency of memory transfers. When training a deep learning model, the datasets used are far too large to fit on the GPU. This means that the data must be transferred to the GPU before the actual training code can execute on the device. Training large models can take days or weeks, so the time spent transferring data can be significant.&lt;/p&gt;
&lt;p&gt;The example provided in Chapter 5 of &amp;ldquo;Programming Massively Parallel Processors&amp;rdquo; is a great introduction to understanding memory access efficiency (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;). In matrix multiplication, the data accesses are limited to a single line of code in the inner-most loop. This means that the memory access pattern is very regular and predictable. The example code is shown below:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;for (int k = 0; i k &amp;lt; numCols; k++) {
    Cvalue += A[row * numCols + k] * B[k * numCols + col];
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This line consists of a floating-point multiplication, floating-point addition, and two memory accesses. Note that we are not storing the result yet, so there is no access to the C matrix. The operation effiency can be described in terms of floating-point operations per second (FLOP/s) and the accesses can be measured in the number of bytes transferred. In this case, we have 2 FLOPs and 8 bytes transferred. This means that the ratio of FLOPs to bytes transferred is 0.25 FLOP/B. This is described as &lt;em&gt;computational intensity&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;With this definition, we get a clearer picture on how to improve the performance of our code. If our kernel relies on too many memory accesses, then the computational intensity will be low. This means that the GPU will be spending more time waiting for data to be transferred than actually performing computations. The goal is to increase the computational intensity as much as possible.&lt;/p&gt;
&lt;p&gt;To put this in perspective, the H100 SXM5 has 3TB/s of memory bandwidth. This global memory bandwidth limits the kernel to 3000 * 0.25 = 750 GFLOP/s. The peak performance of the H100 is 66.9 TFLOPS. If the specialized Tensor cores are utilized, the peak performance is 494.7 TFLOPS. That means that are kernel is only using 0.15% of the peak performance of the GPU. This program is certainly &lt;strong&gt;memory bound&lt;/strong&gt;. Our theoretical limit to computational intensity is the peak performance of the GPU. Programs that achieve this peak are called &lt;strong&gt;compute bound&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Based on the tools we have discussed so far, it is not clear how we can optimize this kernel. The only way to improve the computational intensity is to reduce the number of memory accesses. Modern GPUs have more than just global memory. The next section will explore the different types of memory available on the GPU.&lt;/p&gt;
&lt;h2 id=&#34;memory-types&#34;&gt;Memory Types&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Global Memory&lt;/li&gt;
&lt;li&gt;Local Memory
Resides on global memory, but is not shared between threads. This includes local variables and function arguments.&lt;/li&gt;
&lt;li&gt;Shared Memory
Resides on the chip. Allocated to thread blocks. Shared between threads in the same block.&lt;/li&gt;
&lt;li&gt;Constant Memory&lt;/li&gt;
&lt;li&gt;Registers
Resides on the chip. Each thread has its own registers. Very fast memory.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Data in CPU registers are swapped depending on the context of the program. GPU registers are consistent even when other threads are launched to hide latency. This results in a larger register file on the GPU.&lt;/p&gt;
&lt;p&gt;Following the von Neumann architecture, memory that is closer to the chip is faster but more expensive. Data residing on registers is the most ideal for performance since the processor can work directly with the register values. This benefit comes in the form of energy consumption as well. Transferring data from global memory to the chip requires additional cycles resulting in more energy used.&lt;/p&gt;
&lt;p&gt;When a private variable is declared in a kernel, every single thread will have its own copy of that variable.&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Variable declaration&lt;/th&gt;
          &lt;th&gt;Memory&lt;/th&gt;
          &lt;th&gt;Scope&lt;/th&gt;
          &lt;th&gt;Lifetime&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Automatic variables (not arrays)&lt;/td&gt;
          &lt;td&gt;Register&lt;/td&gt;
          &lt;td&gt;Thread&lt;/td&gt;
          &lt;td&gt;Grid&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Automatic array variables&lt;/td&gt;
          &lt;td&gt;Local&lt;/td&gt;
          &lt;td&gt;Thread&lt;/td&gt;
          &lt;td&gt;Grid&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;__device__ __shared__ int SharedVar;&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Shared&lt;/td&gt;
          &lt;td&gt;Block&lt;/td&gt;
          &lt;td&gt;Grid&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;__device__ int GlobalVar;&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Global&lt;/td&gt;
          &lt;td&gt;Grid&lt;/td&gt;
          &lt;td&gt;Application&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;__device__ __constant__ int ConstVar;&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Constant&lt;/td&gt;
          &lt;td&gt;Grid&lt;/td&gt;
          &lt;td&gt;Application&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Automatic array variables should seldom be used. It may have seemed convenient to use a static array for computing channel-specific values in an image processing kernel, but it is more efficient to use three separate variables. Each variable will be allocated to a register resulting in faster access times.&lt;/p&gt;
&lt;p&gt;Global variables are more commonly used to pass information to another kernel that is being launched.&lt;/p&gt;
&lt;h2 id=&#34;tiling&#34;&gt;Tiling&lt;/h2&gt;
&lt;p&gt;These memory types serve as tools that we can use to increase efficiency. The first pattern discussed is &lt;strong&gt;tiling&lt;/strong&gt;. Throughout the rest of the course, we will add many more patterns to our repertoire. Tiling is a well-described technique that has a fitting analogy. If a wall needs to be tiled, it is more efficient to use many small tiles that are lighter and easier to handle. In GPU programming, the wall represents the entire global memory space. The individual tiles are local memory that is allocated to each thread block.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-13_10-13-54_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Global memory access pattern (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Global memory access pattern (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The kernels we have seen so far have used a &lt;em&gt;global memory access pattern&lt;/em&gt;. In this pattern, all threads have access to every data point from the input. Using a &lt;em&gt;tiling pattern&lt;/em&gt;, we can optimize memory accesses by moving shared resources to local memory that is faster to access.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-13_10-16-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Tiling pattern (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Tiling pattern (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The tool itself is quite simple in concept, but the challenge will be identifying when the tool can be properly applied. Consider matrix multiplication. The naive kernel we explored previously uses each thread to compute one value of the output matrix. This kernel uses a global memory access pattern, and we can identify that many of the computations require the same input. They key to introducing tiling for matrix multiplication will be identifying which data use reused.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-13_10-28-30_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Memory accesses for matrix multiplication (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Memory accesses for matrix multiplication (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the figure above, the block size is \(2 \times 2\). Each row of the block relies on the same input row from the matrix on the left. That is, \(P_{0,0}\) and \(P_{0,1}\) will use the same data from the first row of \(M\). In our original kernel, this requires 8 global memory accesses. If we placed this row in shared memory, each output thread could access the values much quicker. We can see a similar pattern for the column values in \(N\).&lt;/p&gt;
&lt;p&gt;Since we are using tiling with a block size of \(B\), we will consider working with \(2B\) values from the input at a time. If the number of values we need to compute an output entry exceeds \(2B\), then we can synchronize the threads before moving to the next section.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-13_10-25-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Tiled matrix multiplication overview (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Tiled matrix multiplication overview (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;Verify that the potential reduction in global memory traffic in matrix multiplication is proportional to the dimension of the blocks used.&lt;/li&gt;
&lt;li&gt;Verify that the reduction is by a factor of \(N\) if the tiles are \(N \times N\).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-tiled-matrix-multiplication&#34;&gt;Example: Tiled Matrix Multiplication&lt;/h2&gt;
&lt;p&gt;The concept of tiled matrix multiplication is this: load a subset of data from \(M\) and \(N\) into shared memory before using that data to perform the dot product. We have a few limitations to think about here. First, the amount of shared memory is much smaller than global memory; we cannot fit all the data at once. Second, the block size will limit how many elements can be loaded into shared memory at once. As suggested by tiling, we are only working with a small chunk at a time.&lt;/p&gt;
&lt;p&gt;Using a \(2 \times 2\) block gives us 4 threads to work with. Overlaying that block on the input only allows us to grab 2 values from the first 2 rows in \(M\) and 2 values from the first 2 columns in \(M\). For each tile, the subset of data will be loaded in followed by adding the dot product of the subset to the current value.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-13_11-14-57_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Loading the first tile (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Loading the first tile (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-13_11-15-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Computing the dot product of the first subset (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Computing the dot product of the first subset (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In this example, the block will move to the next subset of data to finish computing the first block of the output matrix. This process can be arbitrarily scaled up to support larger matrices without necessarily increasing the block size. Although, we would want to increase the block size to take advantage of the additional threads. The figure below shows a table of the computations required for each phase.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-13_11-18-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Tiled matrix multiplication computations (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Tiled matrix multiplication computations (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Check your understanding&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;By using tiling with a block size of \(B \times B\), what is the total reduction in global memory traffic?&lt;/p&gt;
&lt;h3 id=&#34;implementation-in-cuda&#34;&gt;Implementation in CUDA&lt;/h3&gt;
&lt;p&gt;Our implementation should follow these steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Establish shared memory for the input from matrix \(M\) and matrix \(N\).&lt;/li&gt;
&lt;li&gt;Load the first subset of data from \(M\) and \(N\) into shared memory (remember to synchronize threads).&lt;/li&gt;
&lt;li&gt;Compute the dot product of the subset (remember to synchronize threads).&lt;/li&gt;
&lt;li&gt;Repeat steps 2 and 3 until all subsets have been computed.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Step 1 is obvious. We need to establish the shared memory for this solution. Steps 2 and 3 are the same as described above, but we do need to remember to synchronize the threads. Without synchronization, the computation may continue before all the data is properly loaded. Step 4 implies that each thread will loop through the subsets until all values have been computed. The kernel is shown below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void MatMulKernel(float* M, float* N, float* P, int Width) {
    // Block index
    int bx = blockIdx.x;
    int by = blockIdx.y;

    // Thread index
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    __shared__ float Mds[TILE_WIDTH][TILE_WIDTH];
    __shared__ float Nds[TILE_WIDTH][TILE_WIDTH];

    // Identify the row and column of the P element to work on
    int Row = by * TILE_WIDTH + ty;
    int Col = bx * TILE_WIDTH + tx;

    float Pvalue = 0;
    for (int ph = 0; ph &amp;lt; Width / TILE_WIDTH; ++ph) {
        // Collaborative loading of M and N tiles into shared memory
        Mds[ty][tx] = M[Row * Width + ph * TILE_WIDTH + tx];
        Nds[ty][tx] = N[(ph * TILE_WIDTH + ty) * Width + Col];
        __syncthreads();

        for (int k = 0; k &amp;lt; TILE_WIDTH; ++k) {
            Pvalue += Mds[ty][k] * Nds[k][tx];
        }
        __syncthreads();
    }

    P[Row * Width + Col] = Pvalue;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Let&amp;rsquo;s break this down with a small example. Consider multiplying two \(4 \times 4\) matrices. We will use a block size of \(2 \times 2\), as seen in the figure below. Our block will compute the top left submatrix of the output, \(P_{0,0}\), \(P_{0,1}\), \(P_{1,0}\), and \(P_{1,1}\). We will view the computation from the perspective of the thread for \(P_{0,0}\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-13_14-15-32_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Setup of tiled matrix multiplication example.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Setup of tiled matrix multiplication example.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The row and column of the output computed by the current thread is calculated using the block and thread indices. Of course, this is simply \((0, 0)\) for the first thread. It gets slightly more complicated when computing the input subset in the loop. The input needs to be transferred to shared memory. The loop will skip over a tile at a time. At this point, we already know which row of \(M\) and column of \(N\) we need to access. We need to compute the column index for \(M\) and the row index for \(N\).&lt;/p&gt;
&lt;p&gt;For \(M\), we start with &lt;code&gt;Row * Width&lt;/code&gt;. This needs to be offset by the tile offset index &lt;code&gt;ph&lt;/code&gt; of the main loop, yielding &lt;code&gt;Row * Width + ph * TILE_WIDTH&lt;/code&gt;. Finally, we need to add the thread index &lt;code&gt;tx&lt;/code&gt; to get the final index &lt;code&gt;Row * Width + ph * TILE_WIDTH + tx&lt;/code&gt;. The same process is applied to \(N\). &lt;strong&gt;Note that this only transfers a single value from each matrix to shared memory, but our computation relies on 2 values from each matrix.&lt;/strong&gt; Each thread in the block is collaboratively loading the data into shared memory. This is why the call to &lt;code&gt;__syncthreads()&lt;/code&gt; is necessary.&lt;/p&gt;
&lt;p&gt;Specifically, the thread for \(P_{0, 0}\) copies \(M_{0, 0}\) and \(N_{0, 0}\) to shared memory. The thread for \(P_{0, 1}\) copies \(M_{0, 1}\) and \(N_{1, 0}\) to shared memory. The thread for \(P_{1, 0}\) copies \(M_{1, 0}\) and \(N_{0, 1}\) to shared memory. Finally, the thread for \(P_{1, 1}\) copies \(M_{1, 1}\) and \(N_{1, 1}\) to shared memory.&lt;/p&gt;
&lt;p&gt;The next step is to compute the dot product of the subset. Again, we see a call to &lt;code&gt;__syncthreads()&lt;/code&gt;. Without this synchronization, the loop may be allowed to continue and overwrite the data in shared memory before a thread has finished. Once the final value is computed, each thread can freely write it back to global memory. Since each thread is computing a different value, there is no need to synchronize the threads before writing to global memory.&lt;/p&gt;
&lt;p&gt;\begin{align*}
P_{0, 0} &amp;amp;+= 2 \times 2 + 1 \times 1 \\
P_{0, 1} &amp;amp;+= 2 \times 1 + 1 \times 0 \\
P_{1, 0} &amp;amp;+= 1 \times 2 + 0 \times 1 \\
P_{1, 1} &amp;amp;+= 1 \times 1 + 0 \times 0
\end{align*}&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-13_14-21-27_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Updated values for the first subset.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Updated values for the first subset.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The next iteration of the loop will grab the next subset of the data and repeat the process. The result after this step is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-13_14-26-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;Updated values for the second subset.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;Updated values for the second subset.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To summarize, &lt;code&gt;ph&lt;/code&gt; is the tile offset index, &lt;code&gt;Row&lt;/code&gt; and &lt;code&gt;Col&lt;/code&gt; are the row and column of the output computed by the current thread, and &lt;code&gt;tx&lt;/code&gt; and &lt;code&gt;ty&lt;/code&gt; will give the offset with respect to the current tile.&lt;/p&gt;
&lt;p&gt;The kernel above has an outer loop that calls another loop managed by thread synchronization, breaking the computation up into several distinct phases. This is called &lt;strong&gt;strip-mining&lt;/strong&gt; and is an important part of tiling. This existed even before GPUs were used (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;performance-analysis&#34;&gt;Performance Analysis&lt;/h3&gt;
&lt;p&gt;In the naive implementation, we had a computational intensity of 0.25 FLOP/B. With a \(16 \times 16\) tile, the number of global memory accesses is reduced by a factor of 16. This gives us a computational intensity of 4 FLOP/B. We previously stated that the H100 has a global memory bandwidth of 3TB/s. This means that the theoretical limit for the performance of this kernel is 3000 * 4 = 12000 GFLOP/s which is much better than the 750 GFLOP/s we had before.&lt;/p&gt;
&lt;p&gt;This is not the most optimal way to implement matrix multiplication, and you should always refer to the cuBLAS library for matrix operations. The purpose of this example is to demonstrate the use of tiling.&lt;/p&gt;
&lt;h2 id=&#34;boundary-checking&#34;&gt;Boundary Checking&lt;/h2&gt;
&lt;p&gt;The previous implementation assumed that the width of the matrices was a multiple of the tile width and that the input would always be square matrices. Consider changing our \(2 \times 2\) block to a \(3 \times 3\) block using the same input sizes.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-13_12-56-49_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 11: &amp;lt;/span&amp;gt;Using a 3x3 block (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;Using a 3x3 block (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Our implementation would follow the same process for the first subset of pattern. An issue arises when computing the second tile offset since the block exceeds the boundaries of our input and output. One solution would be to check the boundary condition on both the input, when transferring the data to shared memory, and the output, when reading the data from shared memory. This would require a conditional statement in the inner loop. This is not ideal since the conditional statement would be executed for every thread in the block.&lt;/p&gt;
&lt;p&gt;Another solution is to pad the input with zeros. If the index is outside our boundary, adding a 0 will not affect the result of the dot product. This allows for a simpler implementation while still being flexible enough to handle matrices of any size. The relevant portion of the kernel is shown below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;float Pvalue = 0;
for (int ph = 0; ph &amp;lt; ceil(Width/(float)TILE_WIDTH); ph++) {
    // Collaborative loading of M and N tiles into shared memory
    if (Row &amp;lt; Width &amp;amp;&amp;amp; ph * TILE_WIDTH + tx &amp;lt; Width) {
        Mds[ty][tx] = M[Row * Width + ph * TILE_WIDTH + tx];
    } else {
        Mds[ty][tx] = 0.0;
    }
    if (ph * TILE_WIDTH + ty &amp;lt; Width &amp;amp;&amp;amp; Col &amp;lt; Width) {
        Nds[ty][tx] = N[(ph * TILE_WIDTH + ty) * Width + Col];
    } else {
        Nds[ty][tx] = 0.0;
    }
    __syncthreads();
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The rest of the kernel remains the same. In Lab 2, you will implement this and adapt it to work with non square matrices as well.&lt;/p&gt;
&lt;h2 id=&#34;memory-use-and-occupancy&#34;&gt;Memory Use and Occupancy&lt;/h2&gt;
&lt;p&gt;Just like exceeding the number of registers per thread can negatively affect occupancy, so can over allocating shared memory. The H100 can have up to 228 KB per SM. If we are maximizing the 2048 threads available per SM, each block cannot exceed 228 KB / 2048 threads = 112 B/thread.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How much shared memory is used by each block?&lt;/strong&gt; Each block has 2 arrays of size \(TILE\_WIDTH \times TILE\_WIDTH\) of type &lt;code&gt;float&lt;/code&gt;. This gives us a total of \(2 \times TILE\_WIDTH \times TILE\_WIDTH \times 4 = 8(TILE\_WIDTH)^2\) B. Each block uses \(TILE\_WIDTH^2\) threads, resulting in 8 B/thread. This is well below the limit of 112 B/thread.&lt;/p&gt;
&lt;h2 id=&#34;dynamically-changing-the-block-size&#34;&gt;Dynamically Changing the Block Size&lt;/h2&gt;
&lt;p&gt;The solution presented above uses a constant to determine the tile size. What if this tile size was not optimal for a given hardware configuration? We would surely want to adjust this dynamically to maximize performance. In CUDA, we can support this by using the &lt;code&gt;extern&lt;/code&gt; keyword. First, we need to define our shared memory as one array: &lt;code&gt;extern __shared__ float Mds_Nds[];&lt;/code&gt;. This is a 1D array that represents the shared memory for both input matrices.&lt;/p&gt;
&lt;p&gt;When launching this kernel, we need some way to inform it of the tile size. First, we would query the device properties and determine the optimal tile size based on the hardware. This size can be used as a third launch configuration input, as shown below. Additionally, the size of the shared memory for each input matrix is provided as two additional arguments to the kernel.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;size_t size = compute_optimal_size(); // Determine optimal tile size
MatMulKernel&amp;lt;&amp;lt;&amp;lt;dimGrid, dimBlock, size&amp;gt;&amp;gt;&amp;gt;(M_d, N_d, P_d, Width, size/2, size/2);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The kernel will need to be modified to use the new shared memory array. The first step is to determine the offset for each matrix. This is done by multiplying the tile size by the thread index. The second step is to use the offset to access the correct value in the shared memory array. The kernel is shown below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__ void MatMulKernel(float* M, float* N, float* P, int Width, int Mds_offset, int Nds_offset) {
    extern __shared__ float Mds_Nds[];

    float *Mds = (float *)Mds_Nds;
    float *Nds = (float *)Mds_Nds + Mds_offset;

    // Rest of the kernel
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Completing this modification would require us to use linear indexing for &lt;code&gt;Mds&lt;/code&gt; and &lt;code&gt;Nds&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;the-takeaway&#34;&gt;The Takeaway&lt;/h2&gt;
&lt;p&gt;Tiling is a powerful tool that can be used to improve the performance of a kernel. It is important to understand the memory access pattern of your kernel and identify which data is reused. This will allow you to move that data to shared memory and reduce the number of global memory accesses. Tiling is the first of many &lt;em&gt;patterns&lt;/em&gt; that we will explore. Just like not every tool is useful for every job, not every pattern will be useful for each problem we face. Increasing the number of tools, or patterns, that we have available will allow us to solve a wider range of problems efficiently.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>CUDA Architecture</title>
      <link>http://localhost:1313/notes/cuda_architecture/</link>
      <pubDate>Mon, 08 Jan 2024 20:49:00 -0600</pubDate>
      
      <guid>http://localhost:1313/notes/cuda_architecture/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#architecture&#34;&gt;Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#block-scheduling&#34;&gt;Block Scheduling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#synchronization&#34;&gt;Synchronization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#warps&#34;&gt;Warps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#control-divergence&#34;&gt;Control Divergence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#warp-scheduling&#34;&gt;Warp Scheduling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#resource-partitioning&#34;&gt;Resource Partitioning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dynamic-launch-configurations&#34;&gt;Dynamic Launch Configurations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-takeaway&#34;&gt;The Takeaway&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;A GPU consists of chip that is composed of several &lt;strong&gt;streaming multiprocessors&lt;/strong&gt; (SMs). Each SM has a number of cores that execute instructions in parallel. The H100, seen below, has 144 SMs (you can actually count them by eye). Each SM has 128 FP32 cores for a total of 18,432 cores. Historically, CUDA has used DDR memory, but newer architectures use high-bandwidth memory (HBM). This is closely integrated with the GPU for faster data transfer.&lt;/p&gt;
&lt;p&gt;In the image below, you can see 6 HBM3 memory modules surrounding the GPU, 3 on either side of the die. HBM3 is capable of 3 TB/s of bandwidth. The platform shown only uses 5 of these modules. The full version will utilize all 6.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-11_14-29-08_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;NVIDIA H100 GPU with 144 SMs ([NVIDIA](https://resources.nvidia.com/en-us-tensor-core)).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;NVIDIA H100 GPU with 144 SMs (&lt;a href=&#34;https://resources.nvidia.com/en-us-tensor-core&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;NVIDIA&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;block-scheduling&#34;&gt;Block Scheduling&lt;/h2&gt;
&lt;p&gt;When a kernel is launched, the blocks that we configure in our code are assigned to SMs. All threads in each block will be assigned to each SM. Depending on the platform, the number of blocks that can be assigned to an SM will vary. This is discussed in more detail below. Since all threads in a block are on the same SM, they can share data and communicate with each other.&lt;/p&gt;
&lt;h2 id=&#34;synchronization&#34;&gt;Synchronization&lt;/h2&gt;
&lt;p&gt;Threads that run on the same block can be synchronized using &lt;code&gt;__syncthreads()&lt;/code&gt;. This is a pretty straightforward concept, but it is important to understand the caveats. When a kernel reaches this call, the execution of the threads will stop until all of them have reached that point. This construct is typically used when threads need to share data or are dependent on the results of other threads.&lt;/p&gt;
&lt;p&gt;Be careful on using this call. An example of incorrect usage is shown below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__
void kernel(int *a, int *b, int *c) {
    if (threadIdx.x % 2 == 0) {
        // Perform some work
        __syncthreads();
    else {
        // Perform some other work
        __syncthreads();
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Unlike a general-purpose processor, a GPU does not have control hardware for each individual core. This means that all threads must execute the same instructions using shared resources. In the example above, it is possible for some threads to branch off into a different part of the program. However, only one of the paths can be executed based on this limitation. This is called &lt;strong&gt;control divergence&lt;/strong&gt; and is discussed in more detail below.&lt;/p&gt;
&lt;p&gt;Even though the call looks the same, each &lt;code&gt;__syncthreads()&lt;/code&gt; is different. The first call will only synchronize the threads that executed the first path. The second call will only synchronize the threads that executed the second path. The result is either undefined output or a deadlock, in which the threads will never reach the second call.&lt;/p&gt;
&lt;p&gt;Since threads in separate blocks cannot be synchronized, the blocks can be executed in any arbitrary order. You might immediately ask yourself how a complex problem that requires synchronization between all parts of the data can get around this limitation. We will explore more complex patterns and their solutions in later sections.&lt;/p&gt;
&lt;h2 id=&#34;warps&#34;&gt;Warps&lt;/h2&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-28_21-03-23_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Warps across several blocks (credit: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Warps across several blocks (credit: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Streaming Multiprocessors in a CUDA chip execute threads in a group of 32 called &lt;strong&gt;warps&lt;/strong&gt;. Since Compute Capability 1.0, the warp size has not changed. When a block is assigned to an SM, it is divided into warps. Given this size, you can easily determine the number of warps assigned to an SM. For example, if you have a block of 256 threads, the SM has 256 / 32 = 8 warps. If the block size is not evenly divisible by the number of warps per SM, the last warp will be padded with inactive threads.&lt;/p&gt;
&lt;p&gt;When multi-dimensional thread blocks are assigned to an SM, the threads are linearly mapped in a &lt;strong&gt;row-major&lt;/strong&gt; order before being partitioned into warps. For example, a 2D block of \(16 \times 16\) threads will be mapped to a 1D array of 256 threads. The first 32 threads will be assigned to the first warp, the next 32 to the second warp, and so on.&lt;/p&gt;
&lt;p&gt;Warps are executed following the Single-Instruction, Multiple-Data (SIMD) model. There is a single program that runs the same instruction on all threads in the same order. If a thread would have executed a different path based on its input data, it would not be executed with the others. This is called &lt;strong&gt;control divergence&lt;/strong&gt; and is explained in the next section.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-28_21-08-27_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;SM layout (source: NVIDIA DLI)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;SM layout (source: NVIDIA DLI)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The advantage of this model is that more physical space can be dedicated to ALUs instead of control logic. In a traditional CPU, each processing core would have its own control logic. The tradeoff is that different cores can execute their own programs at varying points in time.&lt;/p&gt;
&lt;h2 id=&#34;control-divergence&#34;&gt;Control Divergence&lt;/h2&gt;
&lt;p&gt;Since a traditional CPU has separate control logic for each core, it can execute different programs at the same time. If the program has a conditional statement, it does not need to worry about synchronizing instructions with another core. This is not the case with a GPU. Since every thread in a warp executes the same instruction, only threads that would execute the same path can be processed at the same time. If a thread would execute a different path, it is not executed with the others. This is called &lt;strong&gt;control divergence&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;What exactly happens then if a warp has 32 threads of which only 16 would execute the same path? Simply, multiple passes are made until all possible paths of execution are considered based on the divergence of the threads. The SM would process the first 16 threads that all follow the same path before processing the second 16 threads.&lt;/p&gt;
&lt;p&gt;This also applies to other control flow statements such as loops. Consider a CUDA program that processes the elements of a vector. Depending on the loop and data used, the threads may execute a different number of iterations. As threads finished their iterations, they would be disabled while the remaining threads continue.&lt;/p&gt;
&lt;p&gt;There are some cases in which it is apparent that your program will exhibit control divergence. For example, if you have a conditional statement based on the thread index, you can be sure that the threads will execute different paths.&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Consider a \(200 \times 150\) image that is processed by a CUDA program. The kernel is launched with \(16 \times 16\) blocks which means there are \(200 / 16 = 13\) blocks in the x-direction and \(150 / 16 = 10\) blocks in the y-direction. The total number of blocks is \(13 \times 10 = 130\). Each block has 256 threads, or 8 warps. That means that the total number of warps is \(130 \times 8 = 1040\).&lt;/p&gt;
&lt;h2 id=&#34;warp-scheduling&#34;&gt;Warp Scheduling&lt;/h2&gt;
&lt;p&gt;An SM can only execute instructions for a small number of warps. The architecture allows for more warps than the SM can execute since warps will often be waiting for some result or data transfer. Warps are selected based on a priority mechanism. This is called &lt;strong&gt;latency tolerance&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Zero-overhead thread scheduling allows for selecting warps without any overhead. A CPU has more space on the chip for caching and branch prediction so that latency is as low as possible. GPUs have more floating point units and can switch between warps, effectively hiding latency.&lt;/p&gt;
&lt;p&gt;The execution states for all assigned warps are stored in the hardware registers, eliminating the need to save and restore registers when switching between warps.&lt;/p&gt;
&lt;p&gt;Under this model, it is ideal for an SM to be assigned more threads than it can execute at once. This increases the odds that the SM will have a warp ready to execute when another warp is waiting for data.&lt;/p&gt;
&lt;h2 id=&#34;resource-partitioning&#34;&gt;Resource Partitioning&lt;/h2&gt;
&lt;p&gt;There is a limit on the number of warps that an SM can support. In general, we want to maximize the throughput of an SM by assigning as many warps as possible. The ratio of warps assigned to the number of warps an SM supports is called &lt;strong&gt;occupancy&lt;/strong&gt;. If we understand how the architecture partitions the resources, we can optimize our programs for peak performance. Consider the NVIDIA GH100 GPU, pictured below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-11_11-44-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;GH100 Full GPU with 144 SMs ([NVIDIA](https://resources.nvidia.com/en-us-tensor-core)).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;GH100 Full GPU with 144 SMs (&lt;a href=&#34;https://resources.nvidia.com/en-us-tensor-core&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;NVIDIA&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The H100 architecture shares the same limitations in compute capability as the A100, so this example will follow the book closely (Hwu, Kirk, and El Hajj 2022). The H100 supports 32 threads per warp, 64 warps per SM, 32 blocks per SM, and 2048 threads per SM. Depending on the block size chosen, the number of blocks per SM will differ. For example, a block size of 256 threads means that there are 2048 / 256 = 8 blocks per SM. This block size would maximize occupancy since the architecture supports more than 8 blocks per SM. Also, the number of threads per block is less than the limit of 1024.&lt;/p&gt;
&lt;p&gt;What if we chose 32 threads per block? Then there would be 2048 / 32 = 64 blocks per SM. However, the device only supports 32 blocks per SM. With only 32 blocks allocated with 32 threads per block, a total of 1024 threads would be utilized. The occupancy in this case is 1024 / 2048 = 50%.&lt;/p&gt;
&lt;p&gt;Historically, NVIDIA provided an excel spreadsheet to compute occupancy. It has since been deprecated in favor of Nsight Compute, a tool that provides more information about the performance of your program. We will cover this tool in a later section.&lt;/p&gt;
&lt;h3 id=&#34;including-registers&#34;&gt;Including Registers&lt;/h3&gt;
&lt;p&gt;Another factor for occupancy is the number of registers used per thread. The H100 has 65,536 registers available for use. As long as your program does not use more than this, you can follow the simpler occupancy calculation from above. With 2048 threads, that leaves 65,536 / 2048 = 32 registers per thread. If we run a program with 256 threads/block, there would be 2048 / 256 = 8 blocks per SM. This means that there are 8 * 256 = 2048 threads per SM. With 31 registers per thread, the total number of registers used per SM is 2048 * 31 = 63,488. In this case we still maximize occupancy since 63,488 &amp;lt; 65,536.&lt;/p&gt;
&lt;p&gt;What if each thread required 33 registers? In that case, the total number of registers used per SM would be 2048 * 33 = 67,584. How would these resources be partitioned? Only 7 blocks could be assigned since 7 * 256 * 33 = 59,136 &amp;lt; 65,536. This means that only 7 * 256 = 1792 threads would be used, reducing the occupancy to 1792 / 2048 = 87.5%.&lt;/p&gt;
&lt;h2 id=&#34;dynamic-launch-configurations&#34;&gt;Dynamic Launch Configurations&lt;/h2&gt;
&lt;p&gt;Depending on our application requirements, we may need to support a range of devices across several compute capabalities. The CUDA API makes this simple by providing several different functions for querying device properties. These can be called from the host before configuring and launching a kernel. This is not an exhaustive list, but it covers the most important properties. When we first launch a program that utilizes CUDA, we will want to know how many devices are available. Later in this course, we will develop programs that utilize multiple GPUs, but we would also want our code to adapt dynamically to a single GPU.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; deviceCount;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cudaGetDeviceCount(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;deviceCount);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once the device count is known, the properties of each device can be acquired with &lt;code&gt;cudaGetDeviceProperties&lt;/code&gt;. This function takes a pointer to a &lt;code&gt;cudaDeviceProp&lt;/code&gt; struct. The struct contains several properties that can be used to configure the kernel launch. The most important properties are listed below. A full list can be found &lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;in the CUDA documentation.&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Property&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;name&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Name of the device&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;totalGlobalMem&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Total amount of global memory available on the device in bytes&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;sharedMemPerBlock&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Shared memory available per block in bytes&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;regsPerBlock&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;32-bit registers available per block&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;warpSize&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Warp size in threads&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;maxThreadsPerBlock&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Maximum number of threads per block&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;maxThreadsDim&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Maximum size of each dimension of a block&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;maxGridSize&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Maximum size of each dimension of a grid&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;multiProcessorCount&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Number of SMs on the device&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;maxThreadsPerMultiProcessor&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Maximum number of threads per SM&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The following example iterates through all devices and queries their properties.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; deviceCount; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    cudaDeviceProp prop;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    cudaGetDeviceProperties(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;prop, i);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Use properties to configure kernel launch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;the-takeaway&#34;&gt;The Takeaway&lt;/h2&gt;
&lt;p&gt;The CUDA architecture is designed to maximize the number of threads that can be executed in parallel. This is achieved by partitioning the resources of the GPU into SMs. Each SM can execute a small number of warps at a time. The number of warps that can be assigned to an SM is called &lt;strong&gt;occupancy&lt;/strong&gt;. The occupancy is determined by the number of threads per block, the number of blocks per SM, and the number of registers used per thread. The CUDA API provides functions for querying device properties so that the kernel launch can be configured dynamically.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multidimensional Grids and Data</title>
      <link>http://localhost:1313/notes/multidimensional_grids_and_data/</link>
      <pubDate>Fri, 05 Jan 2024 11:56:00 -0600</pubDate>
      
      <guid>http://localhost:1313/notes/multidimensional_grids_and_data/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multidimensional-grid-organization&#34;&gt;Multidimensional Grid Organization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-color-to-grayscale&#34;&gt;Example: Color to Grayscale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#no-longer-embarrassing-overlapping-data&#34;&gt;No longer embarrassing: overlapping data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix-multiplication&#34;&gt;Matrix Multiplication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-s-next&#34;&gt;What&amp;rsquo;s Next?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;The CUDA Programming model allows us to organize our data in a multidimensional grid. The purpose of this is primarily for our own convenience, but it also allows us to take advantage of the GPU&amp;rsquo;s memory hierarchy. In Lab 0, we only required a single dimension for our grid as well as each block since the input was a vector. When performing computations on multidimensional data like matrices, we can match the dimensions of our launch configuration to the dimensions of our data.&lt;/p&gt;
&lt;h2 id=&#34;multidimensional-grid-organization&#34;&gt;Multidimensional Grid Organization&lt;/h2&gt;
&lt;p&gt;All threads share a block index, &lt;code&gt;blockIdx&lt;/code&gt;, and a thread index, &lt;code&gt;threadIdx&lt;/code&gt;. These indices are three-dimensional vectors of type &lt;code&gt;dim3&lt;/code&gt;. The &lt;code&gt;dim3&lt;/code&gt; type is defined as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dim3&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; x, y, z;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Each grid is a 3D array of blocks, and every block a 3D array of threads. Consider the kernel execution for &lt;code&gt;vecAdd&lt;/code&gt; from Lab 0:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dim3 &lt;span style=&#34;color:#a6e22e&#34;&gt;blocksPerGrid&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dim3 &lt;span style=&#34;color:#a6e22e&#34;&gt;threadsPerBlock&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vecAdd&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;blocksPerGrid, threadsPerBlock&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;(a_d, b_d, c_d, n);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will execute with \(32 \times 128 = 4096\) threads.&lt;/p&gt;
&lt;p&gt;If our input is a matrix, we should organize our launch dimensions to match its 2D structure. We seemingly have two options: either the grid size or the block size. Consider the figure below, there are 4 blocks in the grid, each with 16 threads organized as a \(4 \times 2 \times 2\) volume.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-05_14-20-10_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;A 2D grid of blocks, each with 16 threads arranged in a 3D configuration (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;A 2D grid of blocks, each with 16 threads arranged in a 3D configuration (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Under such a configuration, we would make use of &lt;code&gt;gridDim.x&lt;/code&gt;, &lt;code&gt;gridDim.y&lt;/code&gt;, and &lt;code&gt;gridDim.z&lt;/code&gt; to access the dimensions of the grid. The dimensions of the block would be accessed with &lt;code&gt;blockDim.x&lt;/code&gt;, &lt;code&gt;blockDim.y&lt;/code&gt;, and &lt;code&gt;blockDim.z&lt;/code&gt;. The thread indices would be accessed with &lt;code&gt;threadIdx.x&lt;/code&gt;, &lt;code&gt;threadIdx.y&lt;/code&gt;, and &lt;code&gt;threadIdx.z&lt;/code&gt;. Would this be the best way to organize our launch configuration? &lt;strong&gt;Not exactly.&lt;/strong&gt; We have no use for the 3D structure if we are only working with matrices.&lt;/p&gt;
&lt;p&gt;Consider an \(n \times m\) matrix. If the matrix is small enough, we could launch a single block with a 2D arrangement of threads to perform the necessary computation. For larger matrices, we would optimally split the work into multiple blocks. This would allow us to perform more work in parallel. Let \(n=62\) and \(m=76\). If we chose a \(16 \times 16\) block size, we would need \(4 \times 5 = 20\) blocks to cover the entire matrix, as shown in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-05_15-04-59_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A 2D grid of blocks, each with 16 threads arranged in 2D (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A 2D grid of blocks, each with 16 threads arranged in 2D (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;notes-on-compute-capability&#34;&gt;Notes on Compute Capability&lt;/h3&gt;
&lt;p&gt;It is more important to dynamically adjust the grid size so that your program can adapt to varying input sizes. As of CC 9.0, the maximum number of threads a block can have is 1024, this means that a \(32 \times 32\) block is the largest we can do for matrix data.&lt;/p&gt;
&lt;p&gt;If the input matrix is smaller than \(32 \times 32\), then only a single block is needed. The additional threads allocated to that block will be inactive for indices outside the range of our input.&lt;/p&gt;
&lt;p&gt;If the input matrix is larger than \(32 \times 32\), additional blocks should be added to the grid to accommodate the increased size. It is safe to keep the block size fixed, but the grid size &lt;strong&gt;must&lt;/strong&gt; be dynamic.&lt;/p&gt;
&lt;h3 id=&#34;optimal-launch-parameters&#34;&gt;Optimal Launch Parameters&lt;/h3&gt;
&lt;p&gt;Is it better to have fewer blocks that maximize the amount of threads per block? Or is it better to have more blocks with fewer threads per block? The current maximum number of threads per block is 1024. In practice, a maximum block dimension size of 128 or 256 is ideal. This has more to do with the specific problem and the amount of shared memory required. You will explore this question in Lab 1.&lt;/p&gt;
&lt;h2 id=&#34;example-color-to-grayscale&#34;&gt;Example: Color to Grayscale&lt;/h2&gt;
&lt;p&gt;Given the layout just described, we will write a kernel that converts a color image to grayscale. This is an &lt;em&gt;embarrassingly parallel&lt;/em&gt; problem since each pixel can be converted independently of the others. We will use the following formula to convert each pixel:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;gray &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.299f&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; red &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.587f&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; green &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.114f&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; blue
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;A CPU implementation would require a &lt;code&gt;for&lt;/code&gt; loop over the exact number of pixels. The CUDA kernel for this is straightforward since it only depends on the current pixel. The only real challenge is to compute the correct indices for each thread.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__
void colorToGrayscale(unsigned char *rgbImage,
                      unsigned char *grayImage,
                      int numRows, int numCols)
{
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;
    if (x &amp;gt;= numCols || y &amp;gt;= numRows) return;

    int index = y * numCols + x;
    int rgbOffset = index * 3;
    unsigned char r = rgbImage[rgbOffset];
    unsigned char g = rgbImage[rgbOffset + 1];
    unsigned char b = rgbImage[rgbOffset + 2];
    float channelSum = 0.299f * r + 0.587f * g + 0.114f * b;
    grayImage[index] = channelSum;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In this example, we assume an RGB image where each pixel is represented by three unsigned characters. It is standard convention in C to pass a pointer to the first element of the array. This implies that we cannot use the &lt;code&gt;[]&lt;/code&gt; operator to access the elements in a multidimensional way. Instead, we must compute the index ourselves. If you are not currently familiar with flat indexing, you certainly will be by the end of this course.&lt;/p&gt;
&lt;p&gt;In C, multi-dimensional arrays are stored in row-major order. To compute the index of row &lt;code&gt;j&lt;/code&gt; and column &lt;code&gt;i&lt;/code&gt; in a 2D array, we need to skip over &lt;code&gt;j&lt;/code&gt; rows and &lt;code&gt;i&lt;/code&gt; columns. The total number of columns is the width of the array. The total number of rows is the height of the array. The index is computed as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; width &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; i;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This is represented in the following figure.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-05_16-56-55_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;A 2D array stored in row-major order (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;A 2D array stored in row-major order (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Since the image is now represented as a flat 1D array, we can use the index computed above to access the correct pixel. The image is typically stored in the same row-major format, although this is not always the case. You should always check the documentation for the image format you are using.&lt;/p&gt;
&lt;h3 id=&#34;launch-configuration&#34;&gt;Launch Configuration&lt;/h3&gt;
&lt;p&gt;As stated above, we are going to launch 20 blocks in a \(4 \times 5\) grid. Each block will have 256 threads arranged in a \(16 \times 16\) 2D configuration. This totals to \(20 \times 256 = 5120\) threads. The example figure above shows this configuration overlaid on a \(76 \times 62\) image. That means we have 4712 pixels that need to be converted. The remaining 408 threads will be idle.&lt;/p&gt;
&lt;p&gt;You might be wondering if all 5120 threads launch at the same time. What if the number of pixels exceeded the number of threads available on the GPU? The short answer is that the GPU will launch as many threads as possible, but the long answer is slightly more complicated and will be discussed in a later lesson.&lt;/p&gt;
&lt;p&gt;In any case, our kernel can be launched using the following code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dim3 &lt;span style=&#34;color:#a6e22e&#34;&gt;blockSize&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dim3 &lt;span style=&#34;color:#a6e22e&#34;&gt;gridSize&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;colorToGrayscale&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;gridSize, blockSize&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;(rgbImage, grayImage, numRows, numCols);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;no-longer-embarrassing-overlapping-data&#34;&gt;No longer embarrassing: overlapping data&lt;/h2&gt;
&lt;p&gt;At this point, you should have a basic understanding of how to solve problems that are embarrassingly parallel. Now comes the next step in shaping your parallel thinking skills. What if the thread relies on multiple data points that may be used by other threads. This is further complicated with problems that require some computation to complete before a thread can begin its work. Let&amp;rsquo;s take a step into deeper waters by looking at image blurring. This is a common technique used in image processing to reduce noise and detail. The basic idea is to replace each pixel with a weighted average of its neighboring pixels. The size of the neighborhood is called the &lt;strong&gt;kernel size&lt;/strong&gt;. The kernel size is typically an odd number so that the pixel of interest is in the center of the neighborhood.&lt;/p&gt;
&lt;p&gt;The core operation behind blurring is called a &lt;strong&gt;convolution&lt;/strong&gt;. We will explore this operation in depth as it serves as a more advanced pattern for parallelism. For now, we will focus on the basic idea. Given a kernel size of \(5 \times 5\) centered on a pixel, we will compute the weighted average of the 25 pixels in the neighborhood. To keep it simple, the weights will be uniform.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-06_15-50-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;A blurring kernel (red) centered on a pixel (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;A blurring kernel (red) centered on a pixel (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Given a pixel location \((x, y)\), we can compute the index of the pixel in the neighborhood as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (y &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; ky) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; numCols &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; kx);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Where \(ky\) and \(kx\) are the row and column indices of the kernel. The kernel is centered on the pixel of interest, so \(ky\) and \(kx\) range from \(-2\) to \(2\). The total number of pixels in the neighborhood is \(5 \times 5 = 25\). The weighted average is computed as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; sum &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0f&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; numPixels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; ky &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;; ky &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;; ky&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; kx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;; kx &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;; kx&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; kx &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; kx &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; numCols) &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (y &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; ky &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; ky &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; numRows) &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (y &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; ky) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; numCols &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; kx);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        sum &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; image[index];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        numPixels&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;image[y &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; numRows &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; x] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sum &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; numPixels;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Some extra care will be needed to account for pixels outside the boundaries. There are several strategies to handle out-of-bounds pixels. The simplest is to ignore them. We will explore other strategies when discussing convolutions. In Lab 1, you will implement a blur kernel that can support a varying kernel size.&lt;/p&gt;
&lt;h2 id=&#34;matrix-multiplication&#34;&gt;Matrix Multiplication&lt;/h2&gt;
&lt;p&gt;Matrix multiplication is one of the most important operations in linear algebra. Many high performance computing applications rely on it. It is one of the most widely called operations in deep learning, for example. Parallelizing this and other linear algebra operations has resulted in an explosion of research and applications ranging from computer vision to computational fluid dynamics. Exploring the parallelism of matrix multiplication will give us a deeper understanding of the CUDA programming model. It will also serve as a jumping off point for more advanced topics like shared memory and convolutional neural networks.&lt;/p&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;Let \(A = \mathbb{R}^{m \times n}\) and \(B = \mathbb{R}^{n \times p}\) be two matrices. The product \(C = AB\) is defined as follows:&lt;/p&gt;
&lt;p&gt;\[
C_{ij} = \sum_{k=1}^n A_{ik} B_{kj}\quad \text{for } i = 1, \ldots, m \text{ and } j = 1, \ldots, p
\]&lt;/p&gt;
&lt;p&gt;This operation is only defined on compatible matrices. That is, the number of columns in \(A\) must equal the number of rows in \(B\). The resulting matrix \(C\) will have \(m\) rows and \(p\) columns.&lt;/p&gt;
&lt;h3 id=&#34;cpu-implementation&#34;&gt;CPU Implementation&lt;/h3&gt;
&lt;p&gt;The CPU implementation of matrix multiplication is straightforward. There is a double &lt;code&gt;for&lt;/code&gt; loop to iterate through each element in the &lt;em&gt;output&lt;/em&gt; matrix. The inner loop computes the dot product of the $i$th row of \(A\) and the $j$th column of \(B\). The dot product is computed by summing the element-wise product of the two vectors.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;matrixMultiplyCPU&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;A, &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;B, &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;C, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; m, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; p) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; m; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; j &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; p; j&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; sum &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0f&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; k &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; n; k&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                sum &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; A[i &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; k] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; B[k &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; j];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            C[i &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sum;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;gpu-implementation&#34;&gt;GPU Implementation&lt;/h3&gt;
&lt;p&gt;For a parallel implementation, we can reason that each thread should compute a single element of the output matrix. To compute element \(C_{ij}\), the thread needs access to row \(i\) from \(A\) and column \(j\) from \(B\). Each thread is simply computing the dot product between these two vectors. The figure below visualizes this process.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-08_12-56-34_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Matrix multiplication (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Matrix multiplication (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The output matrix is separated into blocks based on our block size. When writing the kernel, it is necessary to make sure that the index is not out of bounds.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__
void matrixMultiplyGPU(float *A, float *B, float *C, int m, int n, int p) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    if (row &amp;gt;= m || col &amp;gt;= p) return;

    float sum = 0.0f;
    for (int k = 0; k &amp;lt; n; k++) {
        sum += A[row * n + k] * B[k * p + col];
    }
    C[row * p + col] = sum;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;launch-configuration&#34;&gt;Launch Configuration&lt;/h3&gt;
&lt;p&gt;The launch configuration is similar to the previous examples. We will launch a 2D grid of blocks, each with a 2D arrangement of threads. The block size will be \(16 \times 16\) and the grid size will be \(m / 16 \times p / 16\). The kernel is launched as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dim3 &lt;span style=&#34;color:#a6e22e&#34;&gt;blockSize&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dim3 &lt;span style=&#34;color:#a6e22e&#34;&gt;gridSize&lt;/span&gt;((p &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; blockSize.x &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; blockSize.x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              (m &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; blockSize.y &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; blockSize.y, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;matrixMultiplyGPU&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;gridSize, blockSize&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;(A_d, B_d, C_d, m, n, p);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What happens when the output matrix size exceeds the number of blocks per grid and threads per block? Either multiple kernels will be launched, each working with a submatrix of the original input, or each thread will be responsible for multiple elements.&lt;/p&gt;
&lt;h2 id=&#34;what-s-next&#34;&gt;What&amp;rsquo;s Next?&lt;/h2&gt;
&lt;p&gt;The complexity was slightly increased by considering multidimensional data. Matrices are a prime example of this. The algorithms explored required us to consider multiple input values to compute a single output value. However, the computation did not rely on any thread synchronization, so the task was still simple enough.&lt;/p&gt;
&lt;p&gt;Before diving into more complex operations like thread synchronization, was need a better understanding of the GPU&amp;rsquo;s architecture and memory hierarchy. With this knowledge at our disposal, we can begin to optimize our kernels for maximum performance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Heterogeneous Data Parallel Computing</title>
      <link>http://localhost:1313/notes/heterogeneous_data_parallel_computing/</link>
      <pubDate>Sat, 30 Dec 2023 14:41:00 -0600</pubDate>
      
      <guid>http://localhost:1313/notes/heterogeneous_data_parallel_computing/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#key-concepts&#34;&gt;Key Concepts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cuda-c-programs&#34;&gt;CUDA C Programs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-vector-addition&#34;&gt;Example: Vector Addition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#error-checking&#34;&gt;Error Checking&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;key-concepts&#34;&gt;Key Concepts&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Task Parallelism vs. Data Parallelism&lt;/li&gt;
&lt;li&gt;kernels&lt;/li&gt;
&lt;li&gt;threads&lt;/li&gt;
&lt;li&gt;grids&lt;/li&gt;
&lt;li&gt;blocks&lt;/li&gt;
&lt;li&gt;global memory&lt;/li&gt;
&lt;li&gt;data transfer&lt;/li&gt;
&lt;li&gt;error checking&lt;/li&gt;
&lt;li&gt;compilation of CUDA programs&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;This topic introduces the basics of data parallelism and CUDA programming. The most important concept is that data parallelism is achieved through independent computations on each sample or groups of samples. The basic structure of a CUDA C program consists of writing a &lt;strong&gt;kernel&lt;/strong&gt; that is executed independently on many threads. Memory must be allocated on the GPU device before transferring the data from the host machine (CPU). Upon completion of the kernel, the results need to be transferred back to the &lt;strong&gt;host&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;cuda-c-programs&#34;&gt;CUDA C Programs&lt;/h2&gt;
&lt;p&gt;A basic CUDA program consists of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;kernel&lt;/strong&gt; function defining the work to be performed on each thread.&lt;/li&gt;
&lt;li&gt;Data that is accessible on the &lt;strong&gt;device&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Device memory allocation.&lt;/li&gt;
&lt;li&gt;Memory transfer from the &lt;strong&gt;host&lt;/strong&gt; to the &lt;strong&gt;device&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Execution of the &lt;strong&gt;kernel&lt;/strong&gt; from the &lt;strong&gt;host&lt;/strong&gt; machine.&lt;/li&gt;
&lt;li&gt;Data transfer from the &lt;strong&gt;device&lt;/strong&gt; back to the &lt;strong&gt;host&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Memory cleanup.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At first glance, the execution flow of a CUDA program appears sequential; you launch the threads on the GPU and wait for it to complete. A more realistic program would launch the threads and continue local execution, if necessary.&lt;/p&gt;
&lt;h2 id=&#34;example-vector-addition&#34;&gt;Example: Vector Addition&lt;/h2&gt;
&lt;p&gt;Hwu et al. refer to vector addition as the &amp;ldquo;Hello World&amp;rdquo; of GPU programming (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Hwu, Kirk, and El Hajj 2022&lt;/a&gt;). It is a simple problem that can be described as &lt;em&gt;embarrassingly parallel&lt;/em&gt;. Vector addition is a simple operation. Given two vectors of the same length, \(\mathbf{x}\) and \(\mathbf{y}\), the vector addition operation is defined as:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{z}_i = \mathbf{x}_i + \mathbf{y}_i \quad \forall i \in \{1, \ldots, n\}
\]&lt;/p&gt;
&lt;p&gt;The vector addition operation is commutative and associative. The operation can be performed in parallel on each element of the vectors. This can be implemented simply in C.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;vecAdd&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x_h, &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;y_h, &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;z_h, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; n; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        z_h[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x_h[i] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; y_h[i];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;One small note about the variable names: it is common to use the suffix `_h` to denote a variable that is allocated on the host (CPU) and `_d` to denote a variable that is allocated on the device (GPU). In this case, the vector addition operation is performed on the host machine.&lt;/p&gt;
&lt;p&gt;An equivalent implementation in CUDA C is shown below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;__global__
void vecAdd(float *x_d, float *y_d, float *z_d, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &amp;lt; n) {
        z_d[i] = x_d[i] + y_d[i];
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This kernel executes on a single thread. The thread index is computed using built-in variables `blockIdx.x`, `blockDim.x`, and `threadIdx.x`. The details of how these variables are defined are not important right now. The main point is that each kernel is executed on a single thread. For a GPU with thousands of individual threads, this kernel will be executed thousands of times in parallel.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;__global__&lt;/code&gt; keyword placed before the function definition indicates that the function can be called from both the host and the device, but it is only executed on the device. The table below shows the different keywords used to define functions in CUDA C.&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Keyword&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;__global__&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Executed on the device, callable from the host and device&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;__device__&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Executed on the device, callable from the device only&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;__host__&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Executed on the host, callable from the host only&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Unless otherwise specified, functions that you define will be executed on the host. That is, it is not necessary to specify the &lt;code&gt;__host__&lt;/code&gt; keyword. If you want the compiler to generate both host and device code, you can use the &lt;code&gt;__host__ __device__&lt;/code&gt; keyword combination.&lt;/p&gt;
&lt;p&gt;The kernel is executed on the host machine using the following code.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;int main() {
    // Allocate memory on the host
    float *x_h, *y_h, *z_h;
    int n = 1024;

    x_h = malloc(n * sizeof(float));
    y_h = malloc(n * sizeof(float));
    z_h = malloc(n * sizeof(float));

    // Allocate memory on the device
    float *x_d, *y_d, *z_d;
    cudaMalloc(&amp;amp;x_d, n * sizeof(float));
    cudaMalloc(&amp;amp;y_d, n * sizeof(float));
    cudaMalloc(&amp;amp;z_d, n * sizeof(float));

    // Transfer data from host to device
    cudaMemcpy(x_d, x_h, n * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(y_d, y_h, n * sizeof(float), cudaMemcpyHostToDevice);

    // Execute kernel
    vecAdd&amp;lt;&amp;lt;&amp;lt;ceil(n / 256.0), 256&amp;gt;&amp;gt;&amp;gt;(x_d, y_d, z_d, n);

    // Transfer data from device to host
    cudaMemcpy(z_h, z_d, n * sizeof(float), cudaMemcpyDeviceToHost);

    // Free memory on host and device
    free(x_h);
    free(y_h);
    free(z_h);
    cudaFree(x_d);
    cudaFree(y_d);
    cudaFree(z_d);
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;There is a lot to unpack here, so we&amp;rsquo;ll start from the top.&lt;/p&gt;
&lt;h3 id=&#34;memory-allocation&#34;&gt;Memory Allocation&lt;/h3&gt;
&lt;p&gt;It doesn&amp;rsquo;t really matter where the host data comes from or how it is allocated, but the above example allocates memory using &lt;code&gt;malloc&lt;/code&gt; anyway. Before transferring data to the device, we must allocate memory on it. This is done via &lt;code&gt;cudaMalloc&lt;/code&gt;. The first argument is a pointer to address of the variable. Remember that taking the address of a pointer will result in a double pointer. This is necessary because the function will need to dereference the pointer to store the address to the allocated data. Once the memory is allocated on the device, it cannot be accessed from the host until it is transferred back.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-05_11-54-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Overview of memory layout (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Overview of memory layout (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The memory that is allocated on the device is called &lt;strong&gt;global memory&lt;/strong&gt;. It is accessible by all threads on the device. There is also a small amount of &lt;strong&gt;shared memory&lt;/strong&gt; that is accessible by threads within a single block along with a &lt;strong&gt;unified memory&lt;/strong&gt; model.&lt;/p&gt;
&lt;h3 id=&#34;memory-transfer&#34;&gt;Memory Transfer&lt;/h3&gt;
&lt;p&gt;Now that the memory has been allocated, the data can be safely transferred from the host to the device. This is accomplished using &lt;code&gt;cudaMemcpy&lt;/code&gt;. The arguments are the &lt;strong&gt;destination pointer&lt;/strong&gt;, &lt;strong&gt;source pointer&lt;/strong&gt;, &lt;strong&gt;size&lt;/strong&gt;, and &lt;strong&gt;direction&lt;/strong&gt;. The direction is an enumerated type that can be one of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cudaMemcpyHostToDevice&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cudaMemcpyDeviceToHost&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cudaMemcpyDeviceToDevice&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will only focus on the first two for now.&lt;/p&gt;
&lt;h3 id=&#34;grids-blocks-and-threads&#34;&gt;Grids, Blocks, and Threads&lt;/h3&gt;
&lt;p&gt;The CUDA programming model is based on a hierarchy of &lt;strong&gt;grids&lt;/strong&gt;, &lt;strong&gt;blocks&lt;/strong&gt;, and &lt;strong&gt;threads&lt;/strong&gt;. A &lt;strong&gt;grid&lt;/strong&gt; is a collection of &lt;strong&gt;blocks&lt;/strong&gt;. A &lt;strong&gt;block&lt;/strong&gt; is a collection of &lt;strong&gt;threads&lt;/strong&gt;. The number of &lt;strong&gt;blocks&lt;/strong&gt; and &lt;strong&gt;threads&lt;/strong&gt; is defined by the programmer. The number of &lt;strong&gt;blocks&lt;/strong&gt; and &lt;strong&gt;threads&lt;/strong&gt; that can be executed in parallel is limited by the hardware. The number of &lt;strong&gt;blocks&lt;/strong&gt; and &lt;strong&gt;threads&lt;/strong&gt; that can be executed in parallel is called the &lt;strong&gt;grid size&lt;/strong&gt; and &lt;strong&gt;block size&lt;/strong&gt;, respectively.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-01-05_11-22-39_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A single block of 256 threads (source: NVIDIA DLI).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A single block of 256 threads (source: NVIDIA DLI).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The figure above shows a single block of 256 threads. This could be one of many blocks in a grid. The threads within each block are executed in parallel and do not interact with threads in other blocks. For threads within a single block, there is a small amount of shared memory as well as other tools for communication. We will explore these in more depth as we dive into the details of the CUDA architecture.&lt;/p&gt;
&lt;h3 id=&#34;kernel-execution&#34;&gt;Kernel Execution&lt;/h3&gt;
&lt;p&gt;Calling the kernel function almost looks like any ordinary function call. The main difference is the inclusion of the &lt;code&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/code&gt; and &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; syntax. These are used to specify the size of the grid and blocks, respectively. In this example, we specified that each block has 256 threads. We can use that specification to dynamically determine the number of blocks based on the input size. The number of blocks is computed as the ceiling of the input size divided by the number of threads per block. This ensures that there are enough blocks to cover the entire input size.&lt;/p&gt;
&lt;p&gt;Returning to the kernel function, the thread index is computed using built-in variables &lt;code&gt;blockIdx.x&lt;/code&gt;, &lt;code&gt;blockDim.x&lt;/code&gt;, and &lt;code&gt;threadIdx.x&lt;/code&gt;. These are defined as &lt;code&gt;struct&lt;/code&gt; variables. Modern GPUs have a 3-dimensional grid, but we only need to worry about the first dimension for now. The thread index is computed as the product of the block index and the number of threads per block plus the thread index within the block. This is a common pattern for computing the thread index.&lt;/p&gt;
&lt;p&gt;You may have noticed that it is possible to have more threads than there are blocks. As much as possible, you should try and work with powers of 2. This will ensure that the hardware is used as efficiently as possible. You can always request more threads than there are data points and ignore the threads that are not needed. In this example, we check to see if the thread index is less than the input size. If it is, the vector addition operation is performed. Otherwise, the function exits.&lt;/p&gt;
&lt;p&gt;There are limits to the number of blocks and threads that can be executed in parallel. These limits are based on the compute capability of the device, referenced &lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;compiling&#34;&gt;Compiling&lt;/h3&gt;
&lt;p&gt;CUDA code is compiled using the NVCC compiler driver. It works by compiling host code using the host&amp;rsquo;s native C/C++ compiler and device code to PTX, the CUDA instruction set architecture. Each snippet of code is separated based on the CUDA keyword used to define it. For example, the &lt;code&gt;__global__&lt;/code&gt; keyword used to define the kernel function informs &lt;code&gt;nvcc&lt;/code&gt; that it should be compiled to a PTX file.&lt;/p&gt;
&lt;h2 id=&#34;error-checking&#34;&gt;Error Checking&lt;/h2&gt;
&lt;p&gt;The functions we use in the CUDA API return an error code. We can use this to create robust code that checks for errors and either corrects them or exits gracefully. The following example shows a simple way to check the result of `cudaMalloc`:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;cudaError_t err = cudaMalloc(&amp;amp;x_d, n * sizeof(float));
if (err != cudaSuccess) {
    fprintf(stderr, &amp;#34;Error: %s\n&amp;#34;, cudaGetErrorString(err));
    exit(EXIT_FAILURE);
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;A common pattern is to define a macro that checks the result of a CUDA function and exits if there is an error. This is shown below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-cuda&#34; data-lang=&#34;cuda&#34;&gt;#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true) {
   if (code != cudaSuccess) {
      fprintf(stderr,&amp;#34;GPUassert: %s %s %d\n&amp;#34;, cudaGetErrorString(code), file, line);
      if (abort) exit(code);
   }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;A small note on the above macro, it is technically C++ code. As of writing this, CUDA does not support all features of C++, but much of the code you will see is written as a mix of C and C++. CUDA was originally developed for C, but C++ features have slowly been introduced over time. If you view the &lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;official documentation&lt;/a&gt;, you can see that the link is defined as `cuda-c-programming-guide`, but the actual document has been renamed to `CUDA C++ Programming Guide`.&lt;/p&gt;
&lt;p&gt;Don&amp;rsquo;t overthink the C/C++ distinction. The main point is that you can use C++ features in your CUDA code, but you should be aware that not all features are supported.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Hwu, Wen-mei W., David B. Kirk, and Izzat El Hajj. 2022. &lt;i&gt;Programming Massively Parallel Processors: A Hands-on Approach&lt;/i&gt;. Fourth. Morgan Kaufmann.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to GPGPU Programming</title>
      <link>http://localhost:1313/notes/introduction_to_gpgpu_programming/</link>
      <pubDate>Wed, 20 Dec 2023 00:00:00 -0600</pubDate>
      
      <guid>http://localhost:1313/notes/introduction_to_gpgpu_programming/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#structure-of-the-course&#34;&gt;Structure of the Course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#heterogeneous-parallel-computing&#34;&gt;Heterogeneous Parallel Computing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#measuring-speedup&#34;&gt;Measuring Speedup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gpu-programming-history&#34;&gt;GPU Programming History&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#applications&#34;&gt;Applications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-to-expect-from-this-course&#34;&gt;What to expect from this course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;structure-of-the-course&#34;&gt;Structure of the Course&lt;/h2&gt;
&lt;p&gt;The primary of this goal is of course to learn how to program GPUs. A key skill that will be developed is the ability to think in parallel. We will start with simple problems that are &lt;em&gt;embarrassingly parallel&lt;/em&gt; and then move on to more complex problems that require synchronization. One of the biggest challenges will be in converting processes that are simple to reason about in serial to parallel processes.&lt;/p&gt;
&lt;p&gt;The course is divided into three parts. The first part will cover the fundamentals of heterogeneous parallel computing and the CUDA programming model. We will focus on problems that are mostly embarrassingly parallel, but will also step into more complicated problems.&lt;/p&gt;
&lt;p&gt;The second part will cover primitive parallel patterns. These are patterns from well-known algorithms that can be used to solve a wide variety of problems. Think of these as useful blueprints for solving problems in parallel. During the second part, we will also dive into more advanced usages of CUDA.&lt;/p&gt;
&lt;p&gt;Part three will cover advanced patterns from more specific applications, such as iterative MRI reconstruction. The course will conclude with expert practices.&lt;/p&gt;
&lt;p&gt;There will be regular assignments that focus on the concepts learned throughout the course. These will typically be accompanied by a series of questions to reinforce and verify that you are successful in each step. Quizzes will be given after each assignment to serve as a checkpoint.&lt;/p&gt;
&lt;h2 id=&#34;heterogeneous-parallel-computing&#34;&gt;Heterogeneous Parallel Computing&lt;/h2&gt;
&lt;p&gt;Heterogeneous computing refers to systems that use more than one kind of processor or core. One common theme in the course will be to focus on a perfect union between the CPU and GPU. Not every task can be fully parallelized. Many tasks are well suited for sequential processing and others are better suited for parallel processing. Parallelism can be further broken down into data parallelism and task parallelism. The majority of our time will be focused on data parallelism, but it is important to keep in mind that not everything fits into this category. Over time, you will develop a sense for what fits this paradigm and what does not.&lt;/p&gt;
&lt;p&gt;The idea of parallelism is certainly not new, but it has become ubiquitous in the computing space. Consider 30 years ago, when most consumer computers had a single core. The race between chip designers resulted in increasing single-core performance year after year in the form of increased clock speeds. This was a great way to increase performance, but it came at the cost of increased power consumption and heat. Scaling down transistors has also be a tried and true way of decreasing processor size and increasing performance. However, we are quickly reaching a physical limit on the size of a transistor.&lt;/p&gt;
&lt;p&gt;The solution to these problems is the same solution seen in scaling up large systems: horizontal scaling. The intuition is straightforward: many things can do the work faster than a single thing. For large-scale systems, the answer is distributed systems in which no single unit needs to be overly powerful or complicated. For consumer processors, this comes in the form of additional cores on a chip.&lt;/p&gt;
&lt;p&gt;In the context of CPUs, adding multiple cores means that we have a multi-core homogeneous system. These are general-purpose processors that can complete any computational task. The cores are identical and can be used interchangeably. The cores are also tightly coupled, meaning that they share memory and can communicate with each other. A similar statement can be made for GPUs. Let&amp;rsquo;s take a look at the differences between them.&lt;/p&gt;
&lt;h3 id=&#34;latency-vs-dot-throughput&#34;&gt;Latency vs. Throughput&lt;/h3&gt;
&lt;p&gt;CPUs follow a latency-first design. The space on the chip itself is not fully dedicated to the processing units. Instead, space is reserved for things like cache, branch prediction, and other features that reduce latency. All computational tasks can be completed on a CPU, but the throughput may be lower than a GPU.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2023-12-21_15-33-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;CPU Architecture from CUDA C&amp;#43;&amp;#43; Programming Guide.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;CPU Architecture from CUDA C++ Programming Guide.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;GPUs follow a throughput-first design. The space on the chip is dedicated to processing units such as ALUs. The cores themselves are not as sophisticated as those found on a CPU. Communication between cores takes more time and is more difficult, but having more of them means that the raw throughput of the chip is higher.&lt;/p&gt;
&lt;p&gt;The development of GPUs was driven by the gaming industry, specifically with rendering, where many vertices and pixels need to be processed in parallel. As we explore GPU solutions to different problems, we will see that data delivery is a key bottleneck. There are techniques available to get around this, which we will need to study closely.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2023-12-21_15-34-18_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;GPU Architecture from CUDA C&amp;#43;&amp;#43; Programming Guide.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;GPU Architecture from CUDA C++ Programming Guide.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;gpus-and-supercomputing&#34;&gt;GPUs and Supercomputing&lt;/h3&gt;
&lt;p&gt;GPUs are featured in many of the top 500 supercomputers. This goes to show that they are a powerful and cost-efficient tool for solving problems. The table below shows the top 5 supercomputers as of November 2023. 4 of them utilize some form of GPU acceleration.&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Name&lt;/th&gt;
          &lt;th&gt;CPUs&lt;/th&gt;
          &lt;th&gt;GPUs&lt;/th&gt;
          &lt;th&gt;Peak PFlop/s&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Frontier (Oak Ridge NL)&lt;/td&gt;
          &lt;td&gt;606,208 cores&lt;/td&gt;
          &lt;td&gt;37,888 AMD MI250X&lt;/td&gt;
          &lt;td&gt;1,679.72&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Aurora (Argonne NL)&lt;/td&gt;
          &lt;td&gt;1,100,000 cores (est.)&lt;/td&gt;
          &lt;td&gt;63,744 Intel GPU Max&lt;/td&gt;
          &lt;td&gt;1,059.33&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Eagle (Microsoft Azure)&lt;/td&gt;
          &lt;td&gt;1,123,200 cores (combined)&lt;/td&gt;
          &lt;td&gt;Unknown Split (NVIDIA H100)&lt;/td&gt;
          &lt;td&gt;846.74&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Fugaku&lt;/td&gt;
          &lt;td&gt;7,630,848 cores&lt;/td&gt;
          &lt;td&gt;None&lt;/td&gt;
          &lt;td&gt;537.21&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;LUMI&lt;/td&gt;
          &lt;td&gt;362,496 cores&lt;/td&gt;
          &lt;td&gt;11,712 AMD MI250X&lt;/td&gt;
          &lt;td&gt;531.51&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The results are clear: heterogeneous parallel computing is a powerful tool for solving problems. Learning how to use these tools will be a valuable skill for the future.&lt;/p&gt;
&lt;h2 id=&#34;measuring-speedup&#34;&gt;Measuring Speedup&lt;/h2&gt;
&lt;p&gt;In general, if system A takes \(T_A\) time to complete a task and system B takes \(T_B\) time to complete the same task, then the speedup of system B over system A is given by \(S = \frac{T_A}{T_B}\).&lt;/p&gt;
&lt;p&gt;Amdahl&amp;rsquo;s law is defined as follows:&lt;/p&gt;
&lt;p&gt;\[S(s) = \frac{1}{(1 - p) + \frac{p}{s}}\]&lt;/p&gt;
&lt;p&gt;where \(p\) is the fraction of the task that can be parallelized and \(s\) is the speedup of the part of the task that can be parallelized.&lt;/p&gt;
&lt;p&gt;It is not common that 100% of a task can be parallelized. Amdah&amp;rsquo;s law takes this into account. Suppose that 40% of a given task can benefit from parallelization. If that part of the task can be sped up by a factor of 10, then the overall speedup is given by:&lt;/p&gt;
&lt;p&gt;\[S = \frac{1}{(1 - 0.4) + \frac{0.4}{10}} = 1.56\]&lt;/p&gt;
&lt;p&gt;In virtually every lab that you will do in this course, you will be asked to measure the speedup of your solution. This is a good way to verify that your solution is correct and that it is actually faster than the serial version. This will also be a critical part of your project, where you will first need to create a serial version of your solution and then parallelize it.&lt;/p&gt;
&lt;h2 id=&#34;gpu-programming-history&#34;&gt;GPU Programming History&lt;/h2&gt;
&lt;p&gt;Early GPU programming was done using OpenGL and DirectX. These were graphics APIs, so everything had to be done in terms of pixel shaders. Researchers found ways to use these APIs to do general purpose computing, but it was very difficult since one could not easily debug the code. Essentially, the input had to be encoded as a texture or color. The GPU would then process the texture and output the result as a texture. The output would then have to be decoded to get the result.&lt;/p&gt;
&lt;p&gt;In 2006, NVIDIA unveiled the GeForce 8800 GTX, which was the first DirectX 10 GPU. More importantly, it was the first GPU built using the CUDA architecture. CUDA also refers to the programming model that NVIDIA developed to facilitate general purpose GPU programming. A key piece of the CUDA architecture is the unified shader pipepline, which allows each ALU to be utilized for general purpose computations.&lt;/p&gt;
&lt;p&gt;The different ALUs have access to a global memory space as well as a shared memory space managed by software. We will explore the specifics of this architecture in part 1 of this course. Since that time, many major changes have been made to the CUDA architecture. Additionally, many other standards have been developed to facilitate GPU programming and parallel computing in general.&lt;/p&gt;
&lt;p&gt;One of the most important standards, which we also study in this course, is OpenCL. OpenCL is an open standard that allows for heterogeneous parallel computing. It is supported by many different vendors, including NVIDIA, AMD, and Intel. OpenCL is a C-like language that allows for the creation of kernels that can be executed on a variety of devices. The OpenCL standard is maintained by the Khronos Group, which also maintains the OpenGL standard.&lt;/p&gt;
&lt;h2 id=&#34;applications&#34;&gt;Applications&lt;/h2&gt;
&lt;p&gt;We are currently in the midst of a data explosion. Vertical scaling, the idea of improving a single system, cannot meet the demands of modern challenges. Horizontal scaling is the most sure solution for now. Distributed systems utilize cheap, commodity servers in lieu of complex supercomputers to distribute applications to mass markets. Parallel computation has applications in just about every field imaginable. We will try to cover a wide variety of applications, as many of them feature parallel solutions that are helpful in other domains.&lt;/p&gt;
&lt;h3 id=&#34;linear-algebra-libraries&#34;&gt;Linear Algebra Libraries&lt;/h3&gt;
&lt;p&gt;One of the most widely utilized applications of data parallelism is in linear algebra libraries. Common matrix operations such as matrix multiplication and matrix inversion are highly parallelizable. The &lt;a href=&#34;https://developer.nvidia.com/cublas&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;cuBLAS&lt;/a&gt; library is a highly optimized implementation of these operations.&lt;/p&gt;
&lt;p&gt;For a great overview of the evolution of linear algebra libraries and the impact of GPUs, see Jack Dongarra&amp;rsquo;s keynote speech at the &lt;a href=&#34;https://youtu.be/8TyyCWuquI0?si=DkPEDPWp7_n8GnVe&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;50 Years of Computing at UTA&lt;/a&gt; event.&lt;/p&gt;
&lt;h3 id=&#34;machine-learning&#34;&gt;Machine Learning&lt;/h3&gt;
&lt;p&gt;Model training and optimization in machine learning is a perfect candidate for data parallelism. Large models such as Llama2 require a massive amount of data to train (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Touvron et al. 2023&lt;/a&gt;). Deep learning models such as this are trained on many GPUs that can execute functions on independent data points in parallel.&lt;/p&gt;
&lt;p&gt;NVIDIA has developed a useful library, which we will study in this course, called &lt;a href=&#34;https://developer.nvidia.com/cudnn&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;cuDNN&lt;/a&gt; that implements highly optimized implementations of common functions used in a deep learning pipeline. High level frameworks build off of this library to provide easier development interfaces for machine learning practitioners. Popular examples include &lt;a href=&#34;https://pytorch.org&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;PyTorch&lt;/a&gt;, &lt;a href=&#34;https://www.tensorflow.org&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;TensorFlow&lt;/a&gt;, and &lt;a href=&#34;https://github.com/google/jax&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;JAX&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;computer-vision&#34;&gt;Computer Vision&lt;/h3&gt;
&lt;p&gt;Most of the current state-of-the-art computer vision methods are driven by deep learning, so they also benefit greatly from data parallelism. &lt;a href=&#34;http://localhost:1313/notes/convolutional_neural_networks/&#34;&gt;Convolutional Neural Networks&lt;/a&gt; (CNN) have been the driving force behind machine-learning based computer vision methods. They are parameter efficient and take advantage of data parallelism. We will study the core operation behind this model, the convolutional opreator.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2023-12-21_15-02-18_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;2D Convolution on a 4x4 grid using a 3x3 filter with unit stride (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;2D Convolution on a 4x4 grid using a 3x3 filter with unit stride (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;computational-chemistry&#34;&gt;Computational Chemistry&lt;/h3&gt;
&lt;p&gt;CUDA has been utilized for computing heat transfer calculations efficiently (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Sosutha and Mohana 2015&lt;/a&gt;). The authors found that the computations could be computed independently, which is perfect for a parallel architecture like a GPU, where throughput is preferred to latency.&lt;/p&gt;
&lt;h3 id=&#34;other-applications&#34;&gt;Other Applications&lt;/h3&gt;
&lt;p&gt;There are many other applications of data parallelism, some of which we will explore and learn from in this course. Examples include the following.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Financial Analysis&lt;/li&gt;
&lt;li&gt;Scientific Simulation&lt;/li&gt;
&lt;li&gt;Engineering Simulation&lt;/li&gt;
&lt;li&gt;Data Intensive Analytics&lt;/li&gt;
&lt;li&gt;Medical Imaging&lt;/li&gt;
&lt;li&gt;Digital Audio Processing&lt;/li&gt;
&lt;li&gt;Digital Video Processing&lt;/li&gt;
&lt;li&gt;Biomedical Informatics&lt;/li&gt;
&lt;li&gt;Electronic Design Automation&lt;/li&gt;
&lt;li&gt;Statistical Modeling&lt;/li&gt;
&lt;li&gt;Numerical Methods&lt;/li&gt;
&lt;li&gt;Ray Tracing Rendering&lt;/li&gt;
&lt;li&gt;Interactive Physics&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-to-expect-from-this-course&#34;&gt;What to expect from this course&lt;/h2&gt;
&lt;p&gt;This course is extremely hands-on. Almost every topic we cover will have an associated programming exercise. Some of these exercises will be integrated into assignments, other will be presented as in-class demonstrations. The fact that there are so many applications means you will need to be able to adapt to new domains quickly. By the end of this course, you should have acquired the following skills:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Advanced familiarity with the CUDA programming model&lt;/li&gt;
&lt;li&gt;Ability to think in parallel&lt;/li&gt;
&lt;li&gt;Identify sections of code that can be parallelized&lt;/li&gt;
&lt;li&gt;Implementation of parallel solutions&lt;/li&gt;
&lt;li&gt;Debugging parallel code&lt;/li&gt;
&lt;li&gt;Measuring performance increase from parallelization&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Dumoulin, Vincent, and Francesco Visin. 2018. A Guide to Convolution Arithmetic for Deep Learning. &lt;i&gt;Arxiv:1603.07285 [Cs, Stat]&lt;/i&gt;, January. &lt;a href=&#34;http://arxiv.org/abs/1603.07285&#34;&gt;http://arxiv.org/abs/1603.07285&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Sosutha, S., and D. Mohana. 2015. Heterogeneous Parallel Computing Using Cuda for Chemical Process. &lt;i&gt;Procedia Computer Science&lt;/i&gt;, Graph Algorithms, High Performance Implementations and Its Applications ( ICGHIA 2014 ), 47 (January): 23746. &lt;a href=&#34;https://doi.org/10.1016/j.procs.2015.03.203&#34;&gt;https://doi.org/10.1016/j.procs.2015.03.203&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_3&#34;&gt;&lt;/a&gt;Touvron, Hugo, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, et al. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv. &lt;a href=&#34;https://doi.org/10.48550/arXiv.2307.09288&#34;&gt;https://doi.org/10.48550/arXiv.2307.09288&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Segmentation via Clustering</title>
      <link>http://localhost:1313/notes/segmentation_via_clustering-2/</link>
      <pubDate>Thu, 24 Feb 2022 00:00:00 -0600</pubDate>
      
      <guid>http://localhost:1313/notes/segmentation_via_clustering-2/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#agglomerative-clustering&#34;&gt;Agglomerative Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-means-clustering&#34;&gt;K-Means Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#simple-linear-iterative-clustering--slic&#34;&gt;Simple Linear Iterative Clustering (SLIC)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#superpixels-in-recent-work&#34;&gt;Superpixels in Recent Work&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The goal of segmentation is fairly broad: group visual elements together.
For any given task, the question is &lt;em&gt;how are elements grouped?&lt;/em&gt;
At the smallest level of an image, pixels can be grouped by color, intensity, or spatial proximity.
Without a model of higher level objects, the pixel-based approach will break down at a large enough scale.&lt;/p&gt;
&lt;p&gt;Segmentation by thresholding works in cases where the boundaries between features are clearly defined.
However, thresholding is not very robust to complex images with noise.
Consider a simple image and its intensity histogram as noise is added.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-03-01_10-27-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;From left to right, a noiseless image with increasing amounts of Gaussian noise added. Source: Pearson Education, Inc.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;From left to right, a noiseless image with increasing amounts of Gaussian noise added. Source: Pearson Education, Inc.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Even with some noise added, as seen in the middle image, thresholding is still relatively straightforward.
Once enough noise is added, thresholding via pixel intensities will not work.
A more sophisticated approach is needed in this case.&lt;/p&gt;
&lt;p&gt;Clustering is a fairly intuitive way to think about segmentation.
Instead of a fine-grained representation of an image as a collection of pixels, it is represented as groups or clusters that share some common features.
The general process of clustering is simple.
The image is represented as a collection of feature vectors (intensity, pixel color, etc.).
Feature vectors are assigned to a single cluster. These clusters represent some segment of the image.&lt;/p&gt;
&lt;p&gt;When it comes to clustering methods, there are two main approaches: agglomerative and divisive.
Simply, one is a bottom-up approach. The other is a top-down approach.
After briefly introductin agglomerative clustering, we will explore specific implementations of segmentation using k-means clustering as well as segmentation using superpixels &amp;lt;&amp;amp;achantaSLICSuperpixelsCompared2012&amp;gt;.&lt;/p&gt;
&lt;h2 id=&#34;agglomerative-clustering&#34;&gt;Agglomerative Clustering&lt;/h2&gt;
&lt;p&gt;Agglomerative clustering methods start by assuming every element is a separate cluster.
Elements are formed based on some local similarities.
As these methods iterate, the number of clusters decreases.
Deciding which elements to merge depends on &lt;strong&gt;inter-cluster distance&lt;/strong&gt;.
The exact choice of distance is dependent on the task. Some examples include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Single-link clustering&lt;/strong&gt;: The distance between the closest elements.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Complete-link clustering&lt;/strong&gt;: The maximum distance between an element of the first cluster and one of the second.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Group average clustering&lt;/strong&gt;: Average distance of elements in a cluster.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;How many clusters are or should be in a single image?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a difficult question to answer for many reasons. The answer will be largely dependent on the task at hand.
It is a problem of learning the underlying generative process of the visual elements in the image.
By defining the specific goal of segmentation (segment by color, shape, etc.), we are introducing a prior about the underlying generative processes which formed the image.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;figure--fig1&#34;&gt;&lt;/a&gt;&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-02-24_16-24-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;3D-PointCapsNet learns point segmentations on only 1% of the training data (Zhao et al.).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;3D-PointCapsNet learns point segmentations on only 1% of the training data (Zhao et al.).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;There are approaches which attempt to segment objects in semi-supervised settings.
As seen in &lt;a href=&#34;#figure--fig1&#34;&gt;Figure 1&lt;/a&gt;, Zhao et al. propose a part segmentation model for 3D objects which only utilizes 1-5% of the training part labels &amp;lt;&amp;amp;zhao3DPointCapsule2019&amp;gt;.&lt;/p&gt;
&lt;p&gt;For example, if we divised an algorithm that would segment an image by color values, it might be able to segment the hand wearing a solid color glove relatively easily.
If we wanted to segment the hand into its individual joints, we would have to introduce a visual prior such as asking the subject to wear a multicolored glove.
We could also add prior information about the hand shape and joint configuration into the model itself.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;figure--joint-pc&#34;&gt;&lt;/a&gt;&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-03-01_22-08-18_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;An image-based joint regression model predicts joint locations (left) along with a point cloud generated from the joint estimates (right).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;An image-based joint regression model predicts joint locations (left) along with a point cloud generated from the joint estimates (right).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the &lt;a href=&#34;#figure--joint-pc&#34;&gt;figure above&lt;/a&gt;, the kinematic hand model could be used to segment the hand by assigning points in the point cloud to the nearest joint as estimated by the model.&lt;/p&gt;
&lt;p&gt;One way to visualize the cluster relationships is a &lt;em&gt;dendrogram&lt;/em&gt;.
Initially, each element is its own cluster. As the process evolves and clusters are merged based on some similarity,
the hierarchy is updated to show how the connections are formed.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-02-24_18-16-31_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Example output from scikit-image.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Example output from scikit-image.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;k-means-clustering&#34;&gt;K-Means Clustering&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/SLU-CSCI5750-SP2022/homework03_DigitClassificationKNN&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;K-Means Variant KNN Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html#sphx-glr-auto-examples-cluster-plot-color-quantization-py&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html#sphx-glr-auto-examples-cluster-plot-color-quantization-py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;K-Means clustering is a popular machine learning method used in both supervised and unsupervised settings.
It works by iteratively updating a set of &lt;em&gt;centroids&lt;/em&gt; or means until some stopping criteria is achieved.&lt;/p&gt;
&lt;p&gt;To use this with image segmentation, we start by treating our image features as vectors.
In the RGB case, each pixel is a vector of 3 values.
It starts out by initializing \(k\) clusters randomly with means \(\mathbf{m}_i\).
The next step is to compute the distance between the clusters and each point in the image.
Points are assigned to the cluster that is closest.&lt;/p&gt;
&lt;p&gt;\[
\text{arg}\min_{C} \sum_{i=1}^k \sum_{\mathbf{z}\in C_i}\|\mathbf{z} - \mathbf{m}_i\|^2,
\]&lt;/p&gt;
&lt;p&gt;where \(C = \{C_1, \dots, C_k\}\) is the cluster set.&lt;/p&gt;
&lt;p&gt;K-Means uses Expectation Maximization to update its parameters.
That is, it first computes the expected values given its current cluster centers before updating the cluster centers based on the new assignments.
The standard algorithm is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Initialize clusters&lt;/strong&gt; - Randomly select \(k\) points as cluster centers \(\mathbf{m}_i\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assign samples to clusters&lt;/strong&gt; - Assign each sample to the closest cluster center based on some distance metric.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Update the means&lt;/strong&gt; - Compute a new value for the cluster centers based on the assignments in the previous step.
\[
\mathbf{m}_i = \frac{1}{|C_i|}\sum_{\mathbf{z} \in C_i}\mathbf{z}, \quad i = 1, \dots, k
\]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Test for convergence&lt;/strong&gt; - Compute the distances between the means at time \(t\) and time \(t - 1\) as \(E\). Stop if the difference is less than some threshold: \(E \leq T\).&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-03-01_21-41-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Image segmented using k-means with k=3. Source: Pearson Education, Inc.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Image segmented using k-means with k=3. Source: Pearson Education, Inc.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;simple-linear-iterative-clustering--slic&#34;&gt;Simple Linear Iterative Clustering (SLIC)&lt;/h2&gt;
&lt;p&gt;Simple Linear Iterative Clustering (SLIC) is widely used algorithm based on K-Means clustering for image segmentation &amp;lt;&amp;amp;achantaSLICSuperpixelsCompared2012&amp;gt;.&lt;/p&gt;
&lt;p&gt;As discussed in the original paper, the authors state that SLIC h   as two main advantages over traditional K-Means:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The search space for assigning points is reduced, leading to an increase in performance.&lt;/li&gt;
&lt;li&gt;By weighting the distance measure, color and spatial proximity are both considered when forming clusters.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The algorithm itself is simple to understand and implement, as seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-03-01_23-39-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;SLIC Algorithm (Achanta et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;SLIC Algorithm (Achanta et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;initialization&#34;&gt;Initialization&lt;/h3&gt;
&lt;p&gt;To keep the search space smaller, the individual search regions are spaced \(S = \sqrt{N/k}\) pixels apart, where \(N\) is the number of pixels and \(k\) is the number of cluster centers.&lt;/p&gt;
&lt;p&gt;The image itself is represented in &lt;a href=&#34;https://en.wikipedia.org/wiki/CIELAB_color_space&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;CIELAB color space&lt;/a&gt;.
This color space was chosen because it is &lt;em&gt;perceputally uniform&lt;/em&gt;.
That is, it is useful for detecting small differences in color.&lt;/p&gt;
&lt;p&gt;Each of the \(k\) pixel clusters is then defined as a superpixel consisting of the CIELAB color and position:&lt;/p&gt;
&lt;p&gt;\[
C_i = [l_i\ a_i\ b_i\ x_i\ y_i]^T.
\]&lt;/p&gt;
&lt;p&gt;For stability, the seed locations are moved to the lowest gradient position in a \(3 \times 3\) neighborhood.
If the superpixels are building locally distinct regions, it is better to avoid placing them on an edge (boundary) pixel.&lt;/p&gt;
&lt;h3 id=&#34;search-space-and-distance&#34;&gt;Search Space and Distance&lt;/h3&gt;
&lt;p&gt;The search space for a cluster center is a region \(2S \times 2S\) around the cluster.
Each pixel in this region is compared to the cluster center \(C_k\) using a distance measure \(D\).&lt;/p&gt;
&lt;p&gt;The distance measure should consider both the spatial and color distances:&lt;/p&gt;
&lt;p&gt;\begin{align*}
d_c &amp;amp;= \sqrt{(l_j - l_i)^2 + (a_j - a_i)^2 + (b_j - b_i)^2}\\
d_s &amp;amp;= \sqrt{(x_j - x_i)^2 + (y_j - y_i)^2}\\
D&amp;rsquo; &amp;amp;= \sqrt{\Big(\frac{d_c}{N_c}\Big)^2 + \Big(\frac{d_s}{N_s}\Big)^2}
\end{align*}&lt;/p&gt;
&lt;p&gt;The individual distances should be normalized by their respective maximums since the range of CIELAB values is different from the variable maximum of \(N_s\), which is based on the image size.
Here, \(N_s\) corresponds to the sampling size \(\sqrt{N/k}\).&lt;/p&gt;
&lt;p&gt;The authors found that normalizing this way was inconsistent since the color distances vary greatly from cluster to cluster.
They turn this normalization into a hyperparameter constant \(m\) so that the user can control the importance between spatial and color proximity.&lt;/p&gt;
&lt;p&gt;\[
D = \sqrt{d_c^2 + \Big(\frac{d_s}{S}\Big)^2 m^2}
\]&lt;/p&gt;
&lt;p&gt;A smaller \(m\) results in superpixels that adhere more to image boundaries, where a larger value promotes compact superpixels.&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-03-03_20-24-07_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Comparison of SLIC against other superpixel methods (Achanta et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Comparison of SLIC against other superpixel methods (Achanta et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-03-03_20-26-00_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Images segmented using a varying number of clusters (Achanta et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Images segmented using a varying number of clusters (Achanta et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;superpixels-in-recent-work&#34;&gt;Superpixels in Recent Work&lt;/h2&gt;
&lt;p&gt;Superpixels are useful for reducing the dimensionality of the feature space.
Their applications include tracking, segmentation, and object detection.
Methods that extract superpixels do not work out of the box with deep learning methods
due to their non-differentiable formulation.
Deep learning methods rely on gradient descent to optimize their parameters.
This requires that the functions used in a deep network be differentiable.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-03-03_20-47-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Superpixels optimized for semantic segmentation (Jampani et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Superpixels optimized for semantic segmentation (Jampani et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Superpixel Sampling Networks, proposed by Jampani et al., introduce the first attempt at integrating superpixel extraction methods with deep learning models &amp;lt;&amp;amp;jampaniSuperpixelSamplingNetworks2018&amp;gt;.
In this work, they adapt SLIC as a differentiable layer in a deep network which result in superpixels that are fine-tuned for specific tasks.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-03-03_21-45-09_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;Model diagram for SSN (Jampani et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;Model diagram for SSN (Jampani et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The train their model on a semantic segmentation task which fine tunes the learned superpixels such that they adhere more closely to segmentation boundaries.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-03-03_21-51-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 11: &amp;lt;/span&amp;gt;Results on semantic segmentation (Jampani et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;Results on semantic segmentation (Jampani et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In a more recent work, Yang et al. propose a deep network that directly produces the superpixels as opposed to using a soft K-Means layer &amp;lt;&amp;amp;yangSuperpixelSegmentationFully2020&amp;gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-03-03_22-05-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 12: &amp;lt;/span&amp;gt;Model comparison between Jampani et al. and Yang et al. (Yang et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 12: &lt;/span&gt;Model comparison between Jampani et al. and Yang et al. (Yang et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Similar to SSN, they experiment on the Berkeley Image Segmentation Dataset.
Their results are competitive with other deep learning-based approaches.
The authors note that their method generalizes better in segmentation tasks by being robust to fine details and noise.
Additionally, their model runs at 50 fps using 4 NVIDIA Titan Xp GPUs.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-03-03_22-14-22_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 13: &amp;lt;/span&amp;gt;Comparison of results on competing methods (Yang et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 13: &lt;/span&gt;Comparison of results on competing methods (Yang et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;#print_bibliography: t&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Active Contours</title>
      <link>http://localhost:1313/notes/active_contours/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 -0600</pubDate>
      
      <guid>http://localhost:1313/notes/active_contours/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parametric-representation&#34;&gt;Parametric Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#motivation-of-the-fundamental-snake-equation&#34;&gt;Motivation of the Fundamental Snake Equation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#external-force&#34;&gt;External Force&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#energy-minimization&#34;&gt;Energy Minimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#iterative-solution&#34;&gt;Iterative Solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#applications&#34;&gt;Applications&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.cs.ait.ac.th/~mdailey/cvreadings/Kass-Snakes.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;http://www.cs.ait.ac.th/~mdailey/cvreadings/Kass-Snakes.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.spiedigitallibrary.org/conference-proceedings-of-spie/4322/0000/Statistical-models-of-appearance-for-medical-image-analysis-and-computer/10.1117/12.431093.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.spiedigitallibrary.org/conference-proceedings-of-spie/4322/0000/Statistical-models-of-appearance-for-medical-image-analysis-and-computer/10.1117/12.431093.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://web.mat.upc.edu/toni.susin/files/SnakesAivru86c.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://web.mat.upc.edu/toni.susin/files/SnakesAivru86c.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Snakes&lt;/strong&gt;, as named by Kass et al., is a spline curve that is minimized such that it moves towards distinct image features such as edges.
The closed curve, or snake, can be thought of as a rubber band.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;Introduction/2022-02-23_08-37-57_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Example of snake snapping to object. (Copyright 2018, 2008 Pearson Education, Inc.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Example of snake snapping to object. (Copyright 2018, 2008 Pearson Education, Inc.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;When stretched out, the band has an internal potential energy that forces the band to close in around some rigid object which exerts force against the band&amp;rsquo;s internal energy.
This method does not tout itself it be a fully autonomous way to segment interesting features.
Instead, it is useful in semi-supervised settings where the user knows a general region of interest.
The minimization of the snake will segment the desired object under reasonable settings.&lt;/p&gt;
&lt;h2 id=&#34;parametric-representation&#34;&gt;Parametric Representation&lt;/h2&gt;
&lt;p&gt;The contour is represented as a curve using a parametric representation:&lt;/p&gt;
&lt;p&gt;\[
(x, y) = (g(s), h(s)).
\]&lt;/p&gt;
&lt;p&gt;The argument \(s\) can be thought of as the trajectory of the curve.&lt;/p&gt;
&lt;p&gt;Parametric representations are a natural choice for representing curves in computing due to their compact representation.
For example, a circle is defined by \(x^2 + y^2 = r^2\). The individual values \(x\) and \(y\) are&lt;/p&gt;
&lt;p&gt;\begin{align*}
x &amp;amp;= r\cos(s)\\
y &amp;amp;= r\sin(s)
\end{align*}&lt;/p&gt;
&lt;p&gt;It can then be shown that&lt;/p&gt;
&lt;p&gt;\begin{align*}
x^2 + y^2 &amp;amp;= r^2\cos^2(s) + r^2\sin^2(s)\\
&amp;amp;= r^2\big(\cos^2(s) + \sin^2(s)\big)\\
&amp;amp;= r^2.
\end{align*}&lt;/p&gt;
&lt;p&gt;As a vector, we can represent \((x, y)\) as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{c}=
\begin{bmatrix}
x\\
y
\end{bmatrix}=
\begin{bmatrix}
x\\
\pm \sqrt{r^2 - x^2}
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Using the more efficient parametric representation, this vector is defined as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathbf{c}=
\begin{bmatrix}
r\cos(s)\\
r\sin(s)
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Another example using parametric representation is that of spline curves.&lt;/p&gt;
&lt;p&gt;\begin{align*}
x(t) &amp;amp;= a_xt^3 + b_xt^2 + c_xt + dx\\
y(t) &amp;amp;= a_yt^3 + b_yt^2 + c_yt + dy
\end{align*}&lt;/p&gt;
&lt;p&gt;Then, \(\mathbf{f}(t) = (x(t), y(t))\).&lt;/p&gt;
&lt;h2 id=&#34;motivation-of-the-fundamental-snake-equation&#34;&gt;Motivation of the Fundamental Snake Equation&lt;/h2&gt;
&lt;p&gt;Given a vector \(\mathbf{c}(s) = \big(x(s), y(s)\big)\) normalized such that \(0 \leq s \leq 1\), an energy function is defined based on internal and external forces:&lt;/p&gt;
&lt;p&gt;\[
E(\mathbf{c}) = E_{int} + E_{ext}.
\]&lt;/p&gt;
&lt;p&gt;As the snake is updated iteratively, its final position should be one such that the energy \(E\) is minimized.&lt;/p&gt;
&lt;p&gt;The internal energy function is given as&lt;/p&gt;
&lt;p&gt;\[
E_{int} = \frac{\alpha}{2}\|\mathbf{c}&amp;rsquo;(s)\|^2 + \frac{\beta}{2}\|\mathbf{c}&amp;rsquo;&amp;rsquo;(s)\|^2,
\]&lt;/p&gt;
&lt;p&gt;where the first-order term is controlled by \(\alpha\) and the second-order term controlled by \(\beta\).
The first-order term gives the snake an elastic quality that shrinks towards a rigid object.
The second-order term controls the siffness of the contour.&lt;/p&gt;
&lt;h2 id=&#34;external-force&#34;&gt;External Force&lt;/h2&gt;
&lt;p&gt;The external force \(E_{ext}\) is based on the magnitude of the image gradient:&lt;/p&gt;
&lt;p&gt;\[
E_{img}(x, y) = \|\nabla f(x, y)\|^2.
\]&lt;/p&gt;
&lt;p&gt;Additionally, the gradient vectors are recorded. The combination of the two serve to represent a force field of the edge map.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-02-27_22-35-31_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Force field using the edge map using normalized gradients. Source: Pearson Education, Inc.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Force field using the edge map using normalized gradients. Source: Pearson Education, Inc.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;energy-minimization&#34;&gt;Energy Minimization&lt;/h2&gt;
&lt;p&gt;The total energy of the snake is then&lt;/p&gt;
&lt;p&gt;\[
E(\mathbf{c}(s)) = \int_0^1 \frac{\alpha}{2}\|\mathbf{c}&amp;rsquo;(s)\|^2 ds + \int_0^1 \frac{\beta}{2}\|\mathbf{c}&amp;rsquo;&amp;rsquo;(s)\|^2 ds + \int_0^1 E_{img}(\mathbf{c}(s))ds.
\]&lt;/p&gt;
&lt;p&gt;To find the minimum energy, we write the above equation as a function \(F\) and take its derivative with respect to s.
The minimum energy must satisfy&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial}{\partial s}\Big(\frac{\partial F}{\partial\mathbf{c}&amp;rsquo;}\Big) - \frac{\partial^2}{\partial s^2} \Big(\frac{\partial F}{\partial \mathbf{c}&amp;rsquo;&amp;rsquo;}\Big) - \frac{\partial F}{\partial \mathbf{c}} = 0.
\]&lt;/p&gt;
&lt;p&gt;Solving for the partials above yields&lt;/p&gt;
&lt;p&gt;\[
\alpha \mathbf{c}&amp;rsquo;&amp;rsquo; - \beta \mathbf{c}&amp;rsquo;&amp;rsquo;&amp;rsquo;&amp;rsquo; - \nabla E_{img} = 0.
\]&lt;/p&gt;
&lt;p&gt;Since the derivative of energy is a force, and the external force of the object is against the internal force of the snake, we can write&lt;/p&gt;
&lt;p&gt;\[
\nabla E_{img} = -\mathbf{F}.
\]&lt;/p&gt;
&lt;p&gt;Under this perspective, the minimum energy is found when&lt;/p&gt;
&lt;p&gt;\[
\alpha \mathbf{c}&amp;rsquo;&amp;rsquo;(s) - \beta \mathbf{c}&amp;rsquo;&amp;rsquo;&amp;rsquo;&amp;rsquo;(s) + \mathbf{F}(\mathbf{c}(s)) = 0.
\]&lt;/p&gt;
&lt;h2 id=&#34;iterative-solution&#34;&gt;Iterative Solution&lt;/h2&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-02-27_22-28-54_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;From top left to bottom right: initial snake, 10 steps, 50, 100, 150, 200. Source: Pearson Education, Inc.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;From top left to bottom right: initial snake, 10 steps, 50, 100, 150, 200. Source: Pearson Education, Inc.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To solve this as an iterative process over time \(t\), we write the force vector \(\mathbf{F}\) in terms of its 2D components dependent on \(t\):&lt;/p&gt;
&lt;p&gt;\[
\mathbf{F}(\mathbf{c}(s, t)) = \mathbf{F}(x(s, t), y(s, t)) = \begin{bmatrix}
F_x (x(s, t), y(s, t))\\
F_y (x(s, t), y(s, t))
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;For the internal energy components, we take the partial derivative of \(\mathbf{c}\) with respect to time:&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathbf{c}(s, t)}{\partial t} = \begin{bmatrix}
\frac{\partial x(s, t)}{\partial t}\\
\frac{\partial y(s, t)}{\partial t}\\
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;These derivatives rely on second and fourth order derivatives. For \(x(s, t)\), this is&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial x(s, t)}{\partial t} = \alpha \frac{\partial^2 x(s, t)}{\partial s^2} - \beta \frac{\partial^4 x(s, t)}{\partial s^4} + F_x(x(s, t), y(s, t)).
\]&lt;/p&gt;
&lt;p&gt;The partial for \(y\) follows a similar formulation. These derivatives are approximated using finite differences.
The second order derivative is approximated as&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial^2 x(s, t)}{\partial s^2} = x&amp;rsquo;&amp;rsquo;(k, t) = x(k + 1, t) - 2x(k, t) + x(k-1, t),
\]&lt;/p&gt;
&lt;p&gt;and the fourth order derivative is approximated as&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial^4 x(s, t)}{\partial s^4} = x&amp;rsquo;&amp;rsquo;&amp;rsquo;&amp;rsquo;(k, t) = x(k + 2, t) - 4x(k + 1, t) + 6x(k, t) - 4x(k-1, t) + x(k-2, t).
\]&lt;/p&gt;
&lt;p&gt;The finite differences can be written in matrix form as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Pentadiagonal_matrix&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;pentadiagonal banded matrix&lt;/a&gt;.
Adding back the external force yields a much simpler equation:&lt;/p&gt;
&lt;p&gt;\[
Dx + F_x(\mathbf{x}(t), \mathbf{y}(t)) = 0,
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
D = \alpha D_2 - \beta D_4,
\]&lt;/p&gt;
&lt;p&gt;the matrix of finite differences.&lt;/p&gt;
&lt;p&gt;Solving the above equation involves taking a finite step in time multiplied by the product of the negative time derivatives.
To simplify this process further, an assumption is made that the external force remains constant over time.&lt;/p&gt;
&lt;p&gt;\[
D\mathbf{x}(t) + \mathbf{F}_x(\mathbf{x}(t-1, \mathbf{y}(y-1)) = -\gamma(\mathbf{x}(t) - \mathbf{x}(t-1))
\]&lt;/p&gt;
&lt;p&gt;This is solved using matrix inversion yielding the final update for time \(t\) as&lt;/p&gt;
&lt;p&gt;\[
\mathbf{x}(t) = A[\mathbf{x}(t-1) + \gamma \mathbf{F}_x(\mathbf{x}(t-1), \mathbf{y}(t-1))],
\]&lt;/p&gt;
&lt;p&gt;where \(A = [I - D]^{-1}\).&lt;/p&gt;
&lt;p&gt;The step for \(\mathbf{y}(t)\) follows a similar formulation.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-02-27_22-24-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Snake transition between time steps. Source: Pearson Education, Inc.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Snake transition between time steps. Source: Pearson Education, Inc.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;applications&#34;&gt;Applications&lt;/h2&gt;
&lt;p&gt;In the original paper, the authors show an application where an initial frame of video is initialize with a snak by hand to track the contours of a mouth.
From that point, the shape automatically matches the subsequent frames.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-02-27_22-32-03_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Snakes for motion tracking (Kass et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Snakes for motion tracking (Kass et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
  </channel>
</rss>
