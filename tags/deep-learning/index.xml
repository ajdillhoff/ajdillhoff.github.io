<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning on Alex Dillhoff</title>
    <link>http://localhost:1313/tags/deep-learning/</link>
    <description>Recent content in Deep Learning on Alex Dillhoff</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; {year}</copyright>
    <lastBuildDate>Mon, 15 Apr 2024 20:14:00 -0500</lastBuildDate>
    
	    <atom:link href="http://localhost:1313/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Using the cuDNN Library</title>
      <link>http://localhost:1313/notes/using_the_cudnn_library/</link>
      <pubDate>Mon, 15 Apr 2024 20:14:00 -0500</pubDate>
      
      <guid>http://localhost:1313/notes/using_the_cudnn_library/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-cudnn&#34;&gt;What is cuDNN?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#setting-up-cudnn&#34;&gt;Setting up cuDNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#handling-errors&#34;&gt;Handling Errors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#representing-data&#34;&gt;Representing Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dense-layers&#34;&gt;Dense Layers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#activation-functions&#34;&gt;Activation Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#loss-functions&#34;&gt;Loss Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#convolutions&#34;&gt;Convolutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pooling&#34;&gt;Pooling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;what-is-cudnn&#34;&gt;What is cuDNN?&lt;/h2&gt;
&lt;p&gt;NVIDIA cuDNN provides optimized implementations of core operations used in deep learning. It is designed to be integrated into higher-level machine learning frameworks, such as TensorFlow, PyTorch, and Caffe.&lt;/p&gt;
&lt;h2 id=&#34;setting-up-cudnn&#34;&gt;Setting up cuDNN&lt;/h2&gt;
&lt;p&gt;To use cuDNN in your applications, each program needs to establish a handle to the cuDNN library. This is done by creating a &lt;code&gt;cudnnHandle_t&lt;/code&gt; object and initializing it with &lt;code&gt;cudnnCreate&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;cudnn.h&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;() {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt; handle;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnCreate&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;handle);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Use the handle
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnDestroy&lt;/span&gt;(handle);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;handling-errors&#34;&gt;Handling Errors&lt;/h2&gt;
&lt;p&gt;cuDNN functions return a &lt;code&gt;cudnnStatus_t&lt;/code&gt; value, which indicates whether the function call was successful. As with previous CUDA code that we have reviewed, it is best to check the return value of each call. Not only does this help with debugging, but it also allows you to handle errors gracefully.&lt;/p&gt;
&lt;h2 id=&#34;representing-data&#34;&gt;Representing Data&lt;/h2&gt;
&lt;p&gt;All data in cuDNN is represented as a &lt;strong&gt;tensor&lt;/strong&gt;. A tensor is a multi-dimensional array of data. In cuDNN, tensors have the following parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;# of dimensions&lt;/li&gt;
&lt;li&gt;data type&lt;/li&gt;
&lt;li&gt;an array of integers indicating the size of each dimension&lt;/li&gt;
&lt;li&gt;an array of integers indicating the stride of each dimension&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are a few tensor descriptors for commonly used tensor types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3D Tensors (BMN): Batch, Height, Width&lt;/li&gt;
&lt;li&gt;4D Tensors (NCHW): Batch, Channel, Height, Width&lt;/li&gt;
&lt;li&gt;5D Tensors (NCDHW): Batch, Channel, Depth, Height, Width&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When creating a tensor to use with cuDNN operations, we need to create a &lt;strong&gt;tensor descriptor&lt;/strong&gt; as well as the data itself. The tensor descriptor is a struct that contains the parameters of the tensor.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// Create descriptor
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnDataType_t&lt;/span&gt; data_type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CUDNN_DATA_FLOAT;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorFormat_t&lt;/span&gt; format &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CUDNN_TENSOR_NCHW;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, h &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;, w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; tensor_desc;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnCreateTensorDescriptor&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;tensor_desc);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSetTensor4dDescriptor&lt;/span&gt;(tensor_desc, format, data_type, n, c, h, w);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The descriptor is then used to allocate memory for the tensor data.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;data;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudaMalloc&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;data, n &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; h &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;sizeof&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;retrieving-tensor-information&#34;&gt;Retrieving Tensor Information&lt;/h3&gt;
&lt;p&gt;To retrieve the properties of a tensor that already exists, we can use the &lt;code&gt;cudnnGetTensor4dDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnGetTensor4dDescriptor&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;  tensorDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnDataType_t&lt;/span&gt;         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dataType,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;n,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;c,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;nStride,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;cStride,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;hStride,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                     &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;wStride)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tensorDesc&lt;/code&gt;: the tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dataType&lt;/code&gt;: the data type of the tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n&lt;/code&gt;: the number of batches&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt;: the number of channels&lt;/li&gt;
&lt;li&gt;&lt;code&gt;h&lt;/code&gt;: the height of the tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;w&lt;/code&gt;: the width of the tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nStride&lt;/code&gt;: the stride of the batch dimension&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cStride&lt;/code&gt;: the stride of the channel dimension&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hStride&lt;/code&gt;: the stride of the height dimension&lt;/li&gt;
&lt;li&gt;&lt;code&gt;wStride&lt;/code&gt;: the stride of the width dimension&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;dense-layers&#34;&gt;Dense Layers&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;dense layer&lt;/strong&gt; refers to a fully connected layer in a neural network. Each neuron in the layer is connected to every neuron in the previous layer. The weights of the connections are stored in a matrix, and the biases are stored in a vector. Implementing the forward and backward pass of a dense layer involves matrix multiplication and addition for which cuBLAS has optimized routines.&lt;/p&gt;
&lt;h3 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h3&gt;
&lt;p&gt;The forward pass of a dense layer is computed as follows:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{a} = W \mathbf{x} + \mathbf{b}
\]&lt;/p&gt;
&lt;p&gt;where \(\mathbf{W}\) is the weight matrix, \(\mathbf{x}\) is the input tensor, \(\mathbf{b}\) is the bias vector, and \(\mathbf{a}\) is the output tensor.&lt;/p&gt;
&lt;p&gt;This can be implemented in CUDA with a matrix multiply followed by a vector addition. The first operation we will use is &lt;a href=&#34;https://docs.nvidia.com/cuda/cublas/index.html?highlight=cublasSgemm#cublas-t-gemm&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&lt;code&gt;cublasSgemm&lt;/code&gt;&lt;/a&gt;. The function declaration is&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cublasStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cublasSgemm&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;cublasHandle_t&lt;/span&gt; handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;cublasOperation_t&lt;/span&gt; transa, &lt;span style=&#34;color:#66d9ef&#34;&gt;cublasOperation_t&lt;/span&gt; transb,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; m, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; k,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;           &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;           &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;A, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; lda,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;           &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;B, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; ldb,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;           &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                           &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;           &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;C, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; ldc);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This computes&lt;/p&gt;
&lt;p&gt;\[
C = \alpha \text{op}(A) \text{op}(B) + \beta C.
\]&lt;/p&gt;
&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;handle&lt;/code&gt;: the cuBLAS handle&lt;/li&gt;
&lt;li&gt;&lt;code&gt;transa&lt;/code&gt;: the operation to perform on matrix A (transpose or not)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;transb&lt;/code&gt;: the operation to perform on matrix B (transpose or not)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;m&lt;/code&gt;: the number of rows in matrix A and C&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n&lt;/code&gt;: the number of columns in matrix B and C&lt;/li&gt;
&lt;li&gt;&lt;code&gt;k&lt;/code&gt;: the number of columns in matrix A and rows in matrix B&lt;/li&gt;
&lt;li&gt;&lt;code&gt;alpha&lt;/code&gt;: scalar multiplier for the product of A and B&lt;/li&gt;
&lt;li&gt;&lt;code&gt;A&lt;/code&gt;: matrix A&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lda&lt;/code&gt;: leading dimension of matrix A&lt;/li&gt;
&lt;li&gt;&lt;code&gt;B&lt;/code&gt;: matrix B&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ldb&lt;/code&gt;: leading dimension of matrix B&lt;/li&gt;
&lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: scalar multiplier for matrix C&lt;/li&gt;
&lt;li&gt;&lt;code&gt;C&lt;/code&gt;: matrix C&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ldc&lt;/code&gt;: leading dimension of matrix C&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This function is called twice in the forward pass of a dense layer: once for the matrix multiplication and once for the vector addition.&lt;/p&gt;
&lt;h3 id=&#34;backward-pass&#34;&gt;Backward Pass&lt;/h3&gt;
&lt;p&gt;The backward pass of a dense layer computes the gradients of the weights and biases with respect to the loss.&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathbf{a}}{\partial W} = \frac{\partial}{\partial W} (W \mathbf{x} + \mathbf{b}) = \mathbf{x}
\]&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathbf{a}}{\partial \mathbf{b}} = \frac{\partial}{\partial \mathbf{b}} (W \mathbf{x} + \mathbf{b}) = 1
\]&lt;/p&gt;
&lt;p&gt;Additionally, the layer should propagate the gradients of the loss with respect to the input tensor.&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathbf{a}}{\partial \mathbf{x}} = \frac{\partial}{\partial \mathbf{x}} (W \mathbf{x} + \mathbf{b}) = W
\]&lt;/p&gt;
&lt;p&gt;These gradients are only the local gradients of the layer. During backpropagation, the gradients are multiplied by the gradients propagated from the subsequent layer, as shown below:&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial \mathbf{a}} \frac{\partial \mathbf{a}}{\partial W}
\]&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial L}{\partial \mathbf{b}} = \frac{\partial L}{\partial \mathbf{a}} \frac{\partial \mathbf{a}}{\partial \mathbf{b}}
\]&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial L}{\partial \mathbf{x}} = \frac{\partial L}{\partial \mathbf{a}} \frac{\partial \mathbf{a}}{\partial \mathbf{x}}
\]&lt;/p&gt;
&lt;p&gt;The first two gradients are used to update the weights and biases of the current layer. The last gradient is propagated to the previous layer.&lt;/p&gt;
&lt;p&gt;These can be implemented in CUDA with matrix multiplication and vector addition, similar to the forward pass.&lt;/p&gt;
&lt;h2 id=&#34;activation-functions&#34;&gt;Activation Functions&lt;/h2&gt;
&lt;p&gt;cuDNN supports a variety of activation functions, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sigmoid&lt;/li&gt;
&lt;li&gt;Hyperbolic Tangent&lt;/li&gt;
&lt;li&gt;Rectified Linear Unit (ReLU)&lt;/li&gt;
&lt;li&gt;Clipped Rectified Linear Unit (CLReLU)&lt;/li&gt;
&lt;li&gt;Exponential Linear Unit (ELU)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To use an activation function, we need to create an activation descriptor and set the activation function type.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnActivationDescriptor_t&lt;/span&gt; activation_desc;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnCreateActivationDescriptor&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;activation_desc);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSetActivationDescriptor&lt;/span&gt;(activation_desc, CUDNN_ACTIVATION_RELU, CUDNN_NOT_PROPAGATE_NAN, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The third enum &lt;code&gt;CUDNN_NOT_PROPAGATE_NAN&lt;/code&gt; indicates that NaN values should not be propagated through the activation function. The last parameter is a coefficient value, which is used by clipped ReLU and ELU.&lt;/p&gt;
&lt;p&gt;We can also query the activation descriptor to extract the properties.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnGetActivationDescriptor&lt;/span&gt;(activation_desc, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;mode, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;reluNanOpt, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;coef);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h3&gt;
&lt;p&gt;To process the forward pass of an activation function, we use the &lt;code&gt;cudnnActivationForward&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnActivationForward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt; handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnActivationDescriptor_t&lt;/span&gt; activationDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha, &lt;span style=&#34;color:#75715e&#34;&gt;// scalar multiplier
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta, &lt;span style=&#34;color:#75715e&#34;&gt;// scalar modifier
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; zDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;z);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This computes the following operation:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{z} = \alpha \cdot g(\mathbf{x}) + \beta \cdot \mathbf{z}
\]&lt;/p&gt;
&lt;p&gt;where \(\mathbf{x}\) is the input tensor, \(\mathbf{z}\) is the output tensor, \(\alpha\) is a scalar multiplier, and \(\beta\) is a scalar modifier.&lt;/p&gt;
&lt;h3 id=&#34;backward-pass&#34;&gt;Backward Pass&lt;/h3&gt;
&lt;p&gt;Likewise, the backward pass is done with &lt;code&gt;cudnnActivationBackward&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnActivationBackward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt; handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnActivationDescriptor_t&lt;/span&gt; activationDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha, &lt;span style=&#34;color:#75715e&#34;&gt;// gradient modifier
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; zDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;z,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; dzDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dz,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt; dxDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dx
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This computes the following operation:&lt;/p&gt;
&lt;p&gt;\[
d\mathbf{x} = \alpha \cdot \nabla_{\mathbf{x}} g(\mathbf{x}) d\mathbf{z} + \beta \cdot d\mathbf{x}
\]&lt;/p&gt;
&lt;p&gt;where \(d\mathbf{z}\) is the input tensor to the backward function. Under this same notation, \(\mathbf{z}\) was the output of the activation function. The input to the activation function \(d\mathbf{x}\) is the output tensor of the backward pass, since it is being propagated in the backwards direction.&lt;/p&gt;
&lt;h2 id=&#34;loss-functions&#34;&gt;Loss Functions&lt;/h2&gt;
&lt;p&gt;cuDNN also provides optimized implementations of loss functions such as cross-entropy. Since the related lab focuses on classification, we will limit our discussion to the cross-entropy loss combined with the softmax function.&lt;/p&gt;
&lt;h3 id=&#34;softmax&#34;&gt;Softmax&lt;/h3&gt;
&lt;p&gt;The softmax function is used to convert the output of a neural network into a probability distribution. It is defined as&lt;/p&gt;
&lt;p&gt;\[
\text{softmax}(\mathbf{x})_i = \frac{e^{x_i}}{\sum_j e^{x_j}}
\]&lt;/p&gt;
&lt;p&gt;where \(\mathbf{x}\) is the input tensor and \(i\) is the index of the output tensor.&lt;/p&gt;
&lt;h4 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h4&gt;
&lt;p&gt;Implementing the forward pass of the softmax function is straightforward. We use the &lt;code&gt;cudnnSoftmaxForward&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSoftmaxForward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                    handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnSoftmaxAlgorithm_t&lt;/span&gt;          algorithm,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnSoftmaxMode_t&lt;/span&gt;               mode,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    yDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;y)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Most of the parameters are similar to other cuDNN functions. The &lt;code&gt;algorithm&lt;/code&gt; parameter specifies the algorithm to use for the softmax function, and the &lt;code&gt;mode&lt;/code&gt; parameter specifies the mode of the softmax function.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;algorithm&lt;/code&gt;: &lt;code&gt;CUDNN_SOFTMAX_FAST&lt;/code&gt;, &lt;code&gt;CUDNN_SOFTMAX_ACCURATE&lt;/code&gt;, or &lt;code&gt;CUDNN_SOFTMAX_LOG&lt;/code&gt;. The most numerically stable is &lt;code&gt;CUDNN_SOFTMAX_ACCURATE&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mode&lt;/code&gt;: &lt;code&gt;CUDNN_SOFTMAX_MODE_INSTANCE&lt;/code&gt; or &lt;code&gt;CUDNN_SOFTMAX_MODE_CHANNEL&lt;/code&gt;. The former computes the softmax function for each instance in the batch, while the latter computes the softmax function for each channel in the tensor.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;backward-pass&#34;&gt;Backward Pass&lt;/h4&gt;
&lt;p&gt;The backward pass of the softmax function is implemented with &lt;code&gt;cudnnSoftmaxBackward&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSoftmaxBackward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                    handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnSoftmaxAlgorithm_t&lt;/span&gt;          algorithm,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnSoftmaxMode_t&lt;/span&gt;               mode,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    yDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;y,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    dyDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dy,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    dxDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dx)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that the softmax function is used in the forward pass of the loss function, so the gradients are propagated from the loss function to the softmax function. In practice, the two are combined into a much simpler gradient calculation. If the softmax function is followed by the cross-entropy loss, the gradients are computed as&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial L}{\partial \mathbf{x}} = \mathbf{x} - \mathbf{y}
\]&lt;/p&gt;
&lt;p&gt;where \(\mathbf{y}\) is the target tensor.&lt;/p&gt;
&lt;h2 id=&#34;convolutions&#34;&gt;Convolutions&lt;/h2&gt;
&lt;p&gt;For a background on convolutions, see &lt;a href=&#34;http://localhost:1313/notes/convolutional_neural_networks/&#34;&gt;these notes&lt;/a&gt;. The notes in this article refer to the cuDNN implementation of convolutions.&lt;/p&gt;
&lt;p&gt;When using convolution operations in cuDNN, we need to create a convolution descriptor &lt;code&gt;cudnnConvolutionDescriptor_t&lt;/code&gt; as well as a filter descriptor &lt;code&gt;cudnnFilterDescriptor_t&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;creating-a-filter&#34;&gt;Creating a filter&lt;/h3&gt;
&lt;p&gt;To create a filter descriptor, we use the &lt;code&gt;cudnnCreateFilterDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnFilterDescriptor_t&lt;/span&gt; filter_desc;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnCreateFilterDescriptor&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;filter_desc);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We then set the filter descriptor with the &lt;code&gt;cudnnSetFilter4dDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSetFilter4dDescriptor&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnFilterDescriptor_t&lt;/span&gt;    filterDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnDataType_t&lt;/span&gt;            dataType,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorFormat_t&lt;/span&gt;        format,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                        k,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                        c,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                        h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                        w)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;filterDesc&lt;/code&gt;: the filter descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dataType&lt;/code&gt;: the data type of the filter&lt;/li&gt;
&lt;li&gt;&lt;code&gt;format&lt;/code&gt;: the format of the filter (NCHW or NHWC). Use &lt;code&gt;CUDNN_TENSOR_NCHW&lt;/code&gt; for most cases.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;k&lt;/code&gt;: the number of output feature maps&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt;: the number of input feature maps&lt;/li&gt;
&lt;li&gt;&lt;code&gt;h&lt;/code&gt;: the height of the filter&lt;/li&gt;
&lt;li&gt;&lt;code&gt;w&lt;/code&gt;: the width of the filter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also query the filter descriptor to extract the properties.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnDataType_t&lt;/span&gt; data_type;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorFormat_t&lt;/span&gt; format;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; k, c, h, w;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnGetFilter4dDescriptor&lt;/span&gt;(filter_desc, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;data_type, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;format, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;k, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;c, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;h, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;w);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;creating-a-convolution&#34;&gt;Creating a convolution&lt;/h3&gt;
&lt;p&gt;To create a convolution descriptor, we use the &lt;code&gt;cudnnCreateConvolutionDescriptor&lt;/code&gt; function. Once we are done with it, we should destroy it with &lt;code&gt;cudnnDestroyConvolutionDescriptor&lt;/code&gt;. Since our convolution is 2D, we use the &lt;code&gt;cudnnSetConvolution2dDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSetConvolution2dDescriptor&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionDescriptor_t&lt;/span&gt;    convDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                             pad_h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                             pad_w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                             u,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                             v,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                             dilation_h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                             dilation_w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionMode_t&lt;/span&gt;          mode,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnDataType_t&lt;/span&gt;                 computeType)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;convDesc&lt;/code&gt;: the convolution descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pad_h&lt;/code&gt;: the height padding&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pad_w&lt;/code&gt;: the width padding&lt;/li&gt;
&lt;li&gt;&lt;code&gt;u&lt;/code&gt;: the vertical stride&lt;/li&gt;
&lt;li&gt;&lt;code&gt;v&lt;/code&gt;: the horizontal stride&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dilation_h&lt;/code&gt;: the height dilation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dilation_w&lt;/code&gt;: the width dilation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mode&lt;/code&gt;: the convolution mode (&lt;code&gt;CUDNN_CONVOLUTION&lt;/code&gt; or &lt;code&gt;CUDNN_CROSS_CORRELATION&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;computeType&lt;/code&gt;: the data type used for the convolution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although the library supports both convolution and cross-correlation, the difference is only in the order of the operands. In practice, the two are equivalent. Most deep learning frameworks use cross-correlation.&lt;/p&gt;
&lt;p&gt;To query the convolution descriptor, we can use the &lt;code&gt;cudnnGetConvolution2dDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnGetConvolution2dDescriptor&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionDescriptor_t&lt;/span&gt;    convDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;pad_h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;pad_w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;u,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;v,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dilation_h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dilation_w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionMode_t&lt;/span&gt;         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;mode,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnDataType_t&lt;/span&gt;                &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;computeType)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If we know all the parameters of the convolution, we can use the &lt;code&gt;cudnnGetConvolution2dForwardOutputDim&lt;/code&gt; function to calculate the output dimensions.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnGetConvolution2dForwardOutputDim&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionDescriptor_t&lt;/span&gt;    convDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;         inputTensorDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnFilterDescriptor_t&lt;/span&gt;         filterDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                                 &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;n,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                                 &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;c,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                                 &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;h,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                                 &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;cuDNN&lt;/code&gt; supports several methods for performing a convolution operation. An evaluation of the available algorithms can be found &lt;a href=&#34;https://core.ac.uk/download/pdf/224976536.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here.&lt;/a&gt; The algorithms provide tradeoffs in terms of speed and memory usage. Diving into these details is beyond the scope of this article, but it is important to be aware of the options.&lt;/p&gt;
&lt;p&gt;The forward pass of a convolution is implemented with the &lt;code&gt;cudnnConvolutionForward&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnConvolutionForward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                       handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;       xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnFilterDescriptor_t&lt;/span&gt;       wDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionDescriptor_t&lt;/span&gt;  convDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionFwdAlgo_t&lt;/span&gt;           algo,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                               &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;workSpace,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;size_t&lt;/span&gt;                              workSpaceSizeInBytes,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;       yDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                               &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;y)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;handle&lt;/code&gt;: the cuDNN handle&lt;/li&gt;
&lt;li&gt;&lt;code&gt;alpha&lt;/code&gt;: scalar multiplier for the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;xDesc&lt;/code&gt;: the input tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt;: the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;wDesc&lt;/code&gt;: the filter descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;w&lt;/code&gt;: the filter tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;convDesc&lt;/code&gt;: the convolution descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;algo&lt;/code&gt;: the algorithm to use for the convolution&lt;/li&gt;
&lt;li&gt;&lt;code&gt;workSpace&lt;/code&gt;: the workspace for the convolution&lt;/li&gt;
&lt;li&gt;&lt;code&gt;workSpaceSizeInBytes&lt;/code&gt;: the size of the workspace&lt;/li&gt;
&lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: scalar modifier for the output tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;yDesc&lt;/code&gt;: the output tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: the output tensor&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;backward-pass&#34;&gt;Backward Pass&lt;/h3&gt;
&lt;p&gt;There are three different backward passes for a convolutional layer: one for the weights, one for the input tensor, and one for the bias.&lt;/p&gt;
&lt;h4 id=&#34;weights&#34;&gt;Weights&lt;/h4&gt;
&lt;p&gt;The backward pass for the weights is implemented with the &lt;code&gt;cudnnConvolutionBackwardFilter&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnConvolutionBackwardFilter&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                       handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;       xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;       dyDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dy,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionDescriptor_t&lt;/span&gt;  convDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionBwdFilterAlgo_t&lt;/span&gt;     algo,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                               &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;workSpace,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;size_t&lt;/span&gt;                              workSpaceSizeInBytes,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnFilterDescriptor_t&lt;/span&gt;       dwDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                               &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dw)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;A detailed description of the parameters can be found &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-cnn-library.html#cudnnconvolutionbackwardfilter&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;bias&#34;&gt;Bias&lt;/h4&gt;
&lt;p&gt;The backward pass for the bias is implemented with the &lt;code&gt;cudnnConvolutionBackwardBias&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnConvolutionBackwardBias&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                    handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    dyDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dy,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    dbDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;db)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;A detailed description of the parameters can be found &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-cnn-library.html#cudnnconvolutionbackwardbias&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;input&#34;&gt;Input&lt;/h4&gt;
&lt;p&gt;The backward pass for the input tensor is implemented with the &lt;code&gt;cudnnConvolutionBackwardData&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnConvolutionBackwardData&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                       handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnFilterDescriptor_t&lt;/span&gt;       wDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;       dyDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dy,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionDescriptor_t&lt;/span&gt;  convDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnConvolutionBwdDataAlgo_t&lt;/span&gt;       algo,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                               &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;workSpace,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;size_t&lt;/span&gt;                              workSpaceSizeInBytes,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                         &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;       dxDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                               &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dx)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;A detailed description of the parameters can be found &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-cnn-library.html#cudnnconvolutionbackwarddata&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;pooling&#34;&gt;Pooling&lt;/h2&gt;
&lt;p&gt;Pooling is a common operation in convolutional neural networks. It reduces the spatial dimensions of the input tensor, which helps to reduce the number of parameters and computation in the network. Using pooling in cuDNN requires creating a descriptor. Make sure to destroy it when you&amp;rsquo;re done.&lt;/p&gt;
&lt;h3 id=&#34;creating-a-pooling-descriptor&#34;&gt;Creating a pooling descriptor&lt;/h3&gt;
&lt;p&gt;To create a pooling descriptor, we use the &lt;code&gt;cudnnCreatePoolingDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnPoolingDescriptor_t&lt;/span&gt; pooling_desc;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnCreatePoolingDescriptor&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;pooling_desc);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We then set the pooling descriptor with the &lt;code&gt;cudnnSetPooling2dDescriptor&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnSetPooling2dDescriptor&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnPoolingDescriptor_t&lt;/span&gt;    poolingDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnPoolingMode_t&lt;/span&gt;          mode,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnNanPropagation_t&lt;/span&gt;       maxpoolingNanOpt,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                         windowHeight,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                         windowWidth,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                         verticalPadding,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                         horizontalPadding,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                         verticalStride,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;                         horizontalStride)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;poolingDesc&lt;/code&gt;: the pooling descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mode&lt;/code&gt;: the pooling mode (&lt;code&gt;CUDNN_POOLING_MAX&lt;/code&gt; or &lt;code&gt;CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;maxpoolingNanOpt&lt;/code&gt;: the NaN propagation option for max pooling&lt;/li&gt;
&lt;li&gt;&lt;code&gt;windowHeight&lt;/code&gt;: the height of the pooling window&lt;/li&gt;
&lt;li&gt;&lt;code&gt;windowWidth&lt;/code&gt;: the width of the pooling window&lt;/li&gt;
&lt;li&gt;&lt;code&gt;verticalPadding&lt;/code&gt;: the vertical padding&lt;/li&gt;
&lt;li&gt;&lt;code&gt;horizontalPadding&lt;/code&gt;: the horizontal padding&lt;/li&gt;
&lt;li&gt;&lt;code&gt;verticalStride&lt;/code&gt;: the vertical stride&lt;/li&gt;
&lt;li&gt;&lt;code&gt;horizontalStride&lt;/code&gt;: the horizontal stride&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also query the pooling descriptor to extract the properties.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnPoolingMode_t&lt;/span&gt; mode;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnNanPropagation_t&lt;/span&gt; nan_opt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; window_h, window_w, pad_h, pad_w, stride_h, stride_w;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnGetPooling2dDescriptor&lt;/span&gt;(pooling_desc, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;mode, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;nan_opt, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;window_h, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;window_w, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;pad_h, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;pad_w, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;stride_h, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;stride_w);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h3&gt;
&lt;p&gt;The forward pass of a pooling operation is implemented with the &lt;code&gt;cudnnPoolingForward&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnPoolingForward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                    handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnPoolingDescriptor_t&lt;/span&gt;   poolingDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    yDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;y)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;handle&lt;/code&gt;: the cuDNN handle&lt;/li&gt;
&lt;li&gt;&lt;code&gt;poolingDesc&lt;/code&gt;: the pooling descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;alpha&lt;/code&gt;: scalar multiplier for the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;xDesc&lt;/code&gt;: the input tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt;: the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: scalar modifier for the output tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;yDesc&lt;/code&gt;: the output tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: the output tensor&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;backward-pass&#34;&gt;Backward Pass&lt;/h3&gt;
&lt;p&gt;The backward pass of a pooling operation is implemented with the &lt;code&gt;cudnnPoolingBackward&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnStatus_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cudnnPoolingBackward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnHandle_t&lt;/span&gt;                    handle,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnPoolingDescriptor_t&lt;/span&gt;   poolingDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;alpha,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    yDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;y,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    dyDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dy,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    xDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;x,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                      &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;beta,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;cudnnTensorDescriptor_t&lt;/span&gt;    dxDesc,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;                            &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dx)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The parameters are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;handle&lt;/code&gt;: the cuDNN handle&lt;/li&gt;
&lt;li&gt;&lt;code&gt;poolingDesc&lt;/code&gt;: the pooling descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;alpha&lt;/code&gt;: scalar multiplier for the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;yDesc&lt;/code&gt;: the output tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt;: the output tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dyDesc&lt;/code&gt;: the input tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dy&lt;/code&gt;: the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;xDesc&lt;/code&gt;: the input tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt;: the input tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: scalar modifier for the output tensor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dxDesc&lt;/code&gt;: the output tensor descriptor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dx&lt;/code&gt;: the output tensor&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Natural Language Processing</title>
      <link>http://localhost:1313/notes/natural_language_processing/</link>
      <pubDate>Sun, 23 Apr 2023 00:00:00 -0500</pubDate>
      
      <guid>http://localhost:1313/notes/natural_language_processing/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#text-preprocessing&#34;&gt;Text Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tasks&#34;&gt;Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#models&#34;&gt;Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#perplexity&#34;&gt;Perplexity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Text Preprocessing
&lt;ul&gt;
&lt;li&gt;Character-level tokenization&lt;/li&gt;
&lt;li&gt;Word-level tokenization&lt;/li&gt;
&lt;li&gt;Subword tokenization&lt;/li&gt;
&lt;li&gt;Stopwords&lt;/li&gt;
&lt;li&gt;Batching&lt;/li&gt;
&lt;li&gt;Padding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Unsupervised Pre-Training
&lt;ul&gt;
&lt;li&gt;Autoregression&lt;/li&gt;
&lt;li&gt;BERT loss&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tasks
&lt;ul&gt;
&lt;li&gt;Text Classification&lt;/li&gt;
&lt;li&gt;Named Entity Recognition&lt;/li&gt;
&lt;li&gt;Question Answering&lt;/li&gt;
&lt;li&gt;Summarization&lt;/li&gt;
&lt;li&gt;Translation&lt;/li&gt;
&lt;li&gt;Text Generation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;text-preprocessing&#34;&gt;Text Preprocessing&lt;/h2&gt;
&lt;p&gt;Text preprocessing is an essential step in NLP that involves cleaning and transforming unstructured text data to prepare it for analysis. Some common text preprocessing techniques include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Expanding contractions (e.g., &amp;ldquo;don&amp;rsquo;t&amp;rdquo; to &amp;ldquo;do not&amp;rdquo;) [7]&lt;/li&gt;
&lt;li&gt;Lowercasing text[7]&lt;/li&gt;
&lt;li&gt;Removing punctuations[7]&lt;/li&gt;
&lt;li&gt;Removing words and digits containing digits[7]&lt;/li&gt;
&lt;li&gt;Removing stopwords (common words that do not carry much meaning) [7]&lt;/li&gt;
&lt;li&gt;Rephrasing text[7]&lt;/li&gt;
&lt;li&gt;Stemming and Lemmatization (reducing words to their root forms) [7]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;common-tokenizers&#34;&gt;Common Tokenizers&lt;/h3&gt;
&lt;p&gt;Tokenization is the process of breaking a stream of textual data into words, terms, sentences, symbols, or other meaningful elements called tokens. Some common tokenizers used in NLP include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Whitespace Tokenizer&lt;/strong&gt;&lt;/strong&gt;: Splits text based on whitespace characters (e.g., spaces, tabs, and newlines) [2].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;NLTK Tokenizer&lt;/strong&gt;&lt;/strong&gt;: A popular Python library that provides various tokenization functions, including word and sentence tokenization[1].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;SpaCy Tokenizer&lt;/strong&gt;&lt;/strong&gt;: Another popular Python library for NLP that offers a fast and efficient tokenizer, which can handle large documents and is customizable[5].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;WordPiece Tokenizer&lt;/strong&gt;&lt;/strong&gt;: A subword tokenizer used in models like BERT, which breaks text into smaller subword units to handle out-of-vocabulary words more effectively[3].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Byte Pair Encoding (BPE) Tokenizer&lt;/strong&gt;&lt;/strong&gt;: A subword tokenizer that iteratively merges the most frequent character pairs in the text, resulting in a vocabulary of subword units[12].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;strong&gt;SentencePiece Tokenizer&lt;/strong&gt;&lt;/strong&gt;: A library that provides both BPE and unigram-based subword tokenization, which can handle multiple languages and does not rely on whitespace for tokenization[6].&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These tokenizers differ in the way they split text into tokens and handle language-specific considerations, such as handling out-of-vocabulary words, dealing with punctuation, and managing whitespace characters. The choice of tokenizer depends on the specific NLP task and the characteristics of the text data being processed.&lt;/p&gt;
&lt;p&gt;Citations:
[1] &lt;a href=&#34;https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/&lt;/a&gt;
[2] &lt;a href=&#34;https://neptune.ai/blog/tokenization-in-nlp&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://neptune.ai/blog/tokenization-in-nlp&lt;/a&gt;
[3] &lt;a href=&#34;https://towardsdatascience.com/comparing-transformer-tokenizers-686307856955&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://towardsdatascience.com/comparing-transformer-tokenizers-686307856955&lt;/a&gt;
[4] &lt;a href=&#34;https://www.analyticsvidhya.com/blog/2021/09/essential-text-pre-processing-techniques-for-nlp/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.analyticsvidhya.com/blog/2021/09/essential-text-pre-processing-techniques-for-nlp/&lt;/a&gt;
[5] &lt;a href=&#34;https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/&lt;/a&gt;
[6] &lt;a href=&#34;https://www.reddit.com/r/MachineLearning/comments/rprmq3/d_sentencepiece_wordpiece_bpe_which_tokenizer_is/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.reddit.com/r/MachineLearning/comments/rprmq3/d_sentencepiece_wordpiece_bpe_which_tokenizer_is/&lt;/a&gt;
[7] &lt;a href=&#34;https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/&lt;/a&gt;
[8] &lt;a href=&#34;https://towardsdatascience.com/top-5-word-tokenizers-that-every-nlp-data-scientist-should-know-45cc31f8e8b9&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://towardsdatascience.com/top-5-word-tokenizers-that-every-nlp-data-scientist-should-know-45cc31f8e8b9&lt;/a&gt;
[9] &lt;a href=&#34;https://www.projectpro.io/recipes/explain-difference-between-word-tokenizer&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.projectpro.io/recipes/explain-difference-between-word-tokenizer&lt;/a&gt;
[10] &lt;a href=&#34;https://www.telusinternational.com/insights/ai-data/article/what-is-text-mining&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.telusinternational.com/insights/ai-data/article/what-is-text-mining&lt;/a&gt;
[11] &lt;a href=&#34;https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4&lt;/a&gt;
[12] &lt;a href=&#34;https://towardsdatascience.com/a-comprehensive-guide-to-subword-tokenisers-4bbd3bad9a7c&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://towardsdatascience.com/a-comprehensive-guide-to-subword-tokenisers-4bbd3bad9a7c&lt;/a&gt;
[13] &lt;a href=&#34;https://towardsdatascience.com/text-preprocessing-in-natural-language-processing-using-python-6113ff5decd8&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://towardsdatascience.com/text-preprocessing-in-natural-language-processing-using-python-6113ff5decd8&lt;/a&gt;
[14] &lt;a href=&#34;https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/&lt;/a&gt;
[15] &lt;a href=&#34;https://docs.tamr.com/new/docs/tokenizers-and-similarity-functions&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://docs.tamr.com/new/docs/tokenizers-and-similarity-functions&lt;/a&gt;
[16] &lt;a href=&#34;https://pitt.libguides.com/textmining/preprocessing&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://pitt.libguides.com/textmining/preprocessing&lt;/a&gt;
[17] &lt;a href=&#34;https://medium.com/@ajay_khanna/tokenization-techniques-in-natural-language-processing-67bb22088c75&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://medium.com/@ajay_khanna/tokenization-techniques-in-natural-language-processing-67bb22088c75&lt;/a&gt;
[18] &lt;a href=&#34;https://datascience.stackexchange.com/questions/75304/bpe-vs-wordpiece-tokenization-when-to-use-which&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://datascience.stackexchange.com/questions/75304/bpe-vs-wordpiece-tokenization-when-to-use-which&lt;/a&gt;
[19] &lt;a href=&#34;https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing&lt;/a&gt;
[20] &lt;a href=&#34;https://www.tokenex.com/blog/ab-what-is-nlp-natural-language-processing-tokenization/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.tokenex.com/blog/ab-what-is-nlp-natural-language-processing-tokenization/&lt;/a&gt;
[21] &lt;a href=&#34;https://hungsblog.de/en/technology/learnings/difference-between-the-tokenizer-and-the-pretrainedtokenizer-class/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://hungsblog.de/en/technology/learnings/difference-between-the-tokenizer-and-the-pretrainedtokenizer-class/&lt;/a&gt;
[22] &lt;a href=&#34;https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html&lt;/a&gt;
[23] &lt;a href=&#34;https://medium.com/nlplanet/two-minutes-nlp-a-taxonomy-of-tokenization-methods-60e330aacad3&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://medium.com/nlplanet/two-minutes-nlp-a-taxonomy-of-tokenization-methods-60e330aacad3&lt;/a&gt;
[24] &lt;a href=&#34;https://www.geeksforgeeks.org/text-preprocessing-in-python-set-1/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.geeksforgeeks.org/text-preprocessing-in-python-set-1/&lt;/a&gt;
[25] &lt;a href=&#34;https://medium.com/@utkarsh.kant/tokenization-a-complete-guide-3f2dd56c0682&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://medium.com/@utkarsh.kant/tokenization-a-complete-guide-3f2dd56c0682&lt;/a&gt;
[26] &lt;a href=&#34;https://stackoverflow.com/questions/380455/looking-for-a-clear-definition-of-what-a-tokenizer-parser-and-lexers-are&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://stackoverflow.com/questions/380455/looking-for-a-clear-definition-of-what-a-tokenizer-parser-and-lexers-are&lt;/a&gt;
[27] &lt;a href=&#34;https://blog.floydhub.com/tokenization-nlp/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://blog.floydhub.com/tokenization-nlp/&lt;/a&gt;
[28] &lt;a href=&#34;https://medium.com/product-ai/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://medium.com/product-ai/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908&lt;/a&gt;
[29] &lt;a href=&#34;https://pub.towardsai.net/in-depth-tokenization-methods-of-14-nlp-libraries-with-python-example-297ecdd14c1&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://pub.towardsai.net/in-depth-tokenization-methods-of-14-nlp-libraries-with-python-example-297ecdd14c1&lt;/a&gt;
[30] &lt;a href=&#34;https://datascience.stackexchange.com/questions/88680/what-is-the-difference-between-countvectorizer-and-tokenizer-or-are-they-the&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://datascience.stackexchange.com/questions/88680/what-is-the-difference-between-countvectorizer-and-tokenizer-or-are-they-the&lt;/a&gt;
[31] &lt;a href=&#34;https://www.freecodecamp.org/news/train-algorithms-from-scratch-with-hugging-face/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.freecodecamp.org/news/train-algorithms-from-scratch-with-hugging-face/&lt;/a&gt;
[32] &lt;a href=&#34;https://exchange.scale.com/public/blogs/preprocessing-techniques-in-nlp-a-guide&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://exchange.scale.com/public/blogs/preprocessing-techniques-in-nlp-a-guide&lt;/a&gt;
[33] &lt;a href=&#34;https://huggingface.co/docs/transformers/tokenizer_summary&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://huggingface.co/docs/transformers/tokenizer_summary&lt;/a&gt;
[34] &lt;a href=&#34;https://blog.octanove.org/guide-to-subword-tokenization/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://blog.octanove.org/guide-to-subword-tokenization/&lt;/a&gt;
[35] &lt;a href=&#34;https://www.enjoyalgorithms.com/blog/text-data-pre-processing-techniques-in-ml/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.enjoyalgorithms.com/blog/text-data-pre-processing-techniques-in-ml/&lt;/a&gt;
[36] &lt;a href=&#34;https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;tasks&#34;&gt;Tasks&lt;/h2&gt;
&lt;h3 id=&#34;text-classification&#34;&gt;Text Classification&lt;/h3&gt;
&lt;p&gt;Commonly seen in the form of sentiment analysis, where the objective is to classify whether some input text is positive or negative. Document classification, in which documents are identified by their content, is also useful.&lt;/p&gt;
&lt;h3 id=&#34;named-entity-recognition&#34;&gt;Named Entity Recognition&lt;/h3&gt;
&lt;p&gt;Extract important nouns from a body of text.&lt;/p&gt;
&lt;h3 id=&#34;question-answering&#34;&gt;Question Answering&lt;/h3&gt;
&lt;h3 id=&#34;summarization&#34;&gt;Summarization&lt;/h3&gt;
&lt;h3 id=&#34;translation&#34;&gt;Translation&lt;/h3&gt;
&lt;h3 id=&#34;text-generation&#34;&gt;Text Generation&lt;/h3&gt;
&lt;p&gt;Generate text from a prompt. This could be in the form of a simple question or some initial dialog. This is also seen in tools like GitHub Co-Pilot to generate code based on contextual code in the same project.&lt;/p&gt;
&lt;h2 id=&#34;models&#34;&gt;Models&lt;/h2&gt;
&lt;p&gt;Discuss GPT2&lt;/p&gt;
&lt;h2 id=&#34;perplexity&#34;&gt;Perplexity&lt;/h2&gt;
&lt;p&gt;A measure of confidence of a language model. A naive model may predict a word by randomly selecting any of the \(N\) words in its vocabulary. As the model is optimized and the distribution of possible sequences is acquired, the perplexity decreases.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Transformers</title>
      <link>http://localhost:1313/notes/transformers/</link>
      <pubDate>Sun, 06 Nov 2022 00:00:00 -0500</pubDate>
      
      <guid>http://localhost:1313/notes/transformers/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#attention&#34;&gt;Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#key-value-store&#34;&gt;Key-value Store&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scaled-dot-product-attention&#34;&gt;Scaled Dot Product Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multi-head-attention&#34;&gt;Multi-Head Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#encoder-decoder-architecture&#34;&gt;Encoder-Decoder Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#encoder&#34;&gt;Encoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#decoder&#34;&gt;Decoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The story of Transformers begins with &amp;ldquo;Attention Is All You Need&amp;rdquo; (Vaswani et al., n.d.). In this seminal work, the authors describe the current landscape of sequential models, their shortcomings, and the novel ideas that result in their successful application.&lt;/p&gt;
&lt;p&gt;Their first point highlights a fundamental flaw in how &lt;a href=&#34;http://localhost:1313/notes/recurrent_neural_networks/&#34;&gt;Recurrent Neural Networks&lt;/a&gt; process sequential data: their output is a function of the previous time step. Given the hindsight of 2022, where large language models are crossing the &lt;a href=&#34;https://arxiv.org/pdf/2101.03961.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;trillion parameter milestone&lt;/a&gt;, a model requiring recurrent computation dependent on previous time steps without the possibility of parallelization would be virtually intractable.&lt;/p&gt;
&lt;p&gt;The second observation refers to attention mechanisms, a useful addition to sequential models that enable long-range dependencies focused on specific contextual information. When added to translation models, attention allows the model to focus on particular words (Bahdanau, Cho, and Bengio 2016).&lt;/p&gt;
&lt;p&gt;The Transformer architecture considers the entire sequence using only attention mechanisms.
There are no recurrence computations in the model, allowing for higher efficiency through parallelization.&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;The original architecture consists of an encoder and decoder, each containing one or more attention mechanisms.
Not every type of model uses both encoders and decoders. This is discussed later [TODO: discuss model types].
Before diving into the architecture itself, it is important to understand what an attention mechanism is and how it functions.&lt;/p&gt;
&lt;h2 id=&#34;attention&#34;&gt;Attention&lt;/h2&gt;
&lt;p&gt;Attention mechanisms produce relationships between sequences. When we look at an image of a dog running in a field with the intent of figuring out what the dog is doing in the picture, we pay greater attention to the dog and look at contextual cues in the image that might inform us of their task. This is an automatic process which allows us to efficiently process information.&lt;/p&gt;
&lt;p&gt;Attention mechanisms follow the same concept. Consider a machine translation task in which a sentence in English is translated to French. Certain words between the input and output will have stronger correlations than others.&lt;/p&gt;
&lt;h3 id=&#34;soft-attention&#34;&gt;Soft Attention&lt;/h3&gt;
&lt;p&gt;Use of context vector that is dependent on a sequence of annotations. These contain information about the input sequence with a focus on the parts surrounding the $i$-th word.&lt;/p&gt;
&lt;p&gt;\[
c_i = \sum_{j=1}^{T_x}\alpha_{ij}h_j
\]&lt;/p&gt;
&lt;p&gt;What is \(\alpha_{ij}\) and how is it computed? This comes from an alignment model which assigns a score reflecting how well the inputs around position \(j\) and output at position \(i\) match, given by&lt;/p&gt;
&lt;p&gt;\[
e_{ij} = a(s_{i-1}, h_j),
\]&lt;/p&gt;
&lt;p&gt;where \(a\) is a feed-forward neural network and \(h_j\) is an annotation produced by the hidden layer of a BRNN.
These scores are passed to the softmax function so that \(\alpha_{ij}\) represents the weight of annotation \(h_j\):&lt;/p&gt;
&lt;p&gt;\[
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp (e_{ik})}.
\]&lt;/p&gt;
&lt;p&gt;This weight reflects how important \(h_j\) is at deciding the next state \(s_i\) and generating \(y_i\).&lt;/p&gt;
&lt;h3 id=&#34;soft-vs-dot-hard-attention&#34;&gt;Soft vs. Hard Attention&lt;/h3&gt;
&lt;p&gt;This mechanism was also described in the context of visual attention as &amp;ldquo;soft&amp;rdquo; attention (Xu et al. 2016).
The authors also describe an alternative version they call &amp;ldquo;hard&amp;rdquo; attention.
Instead of providing a probability of where the model should look, hard attention provides a single location that is sampled from a multinoulli distribution parameterized by \(\alpha_i\).&lt;/p&gt;
&lt;p&gt;\[
p(s_{t,i} = 1 | s_{j&amp;lt;t}, \mathbf{a}) = \alpha_{t,i}
\]&lt;/p&gt;
&lt;p&gt;Here, \(s_{t,i}\) represents the location \(i\) at time \(t\), \(s_{j&amp;lt;t}\) are the location variables prior to \(t\), and \(\mathbf{a}\) is an image feature vector.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-11-10_12-07-42_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Hard attention for &amp;#34;A man and a woman playing frisbee in a field.&amp;#34; (Xu et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Hard attention for &amp;ldquo;A man and a woman playing frisbee in a field.&amp;rdquo; (Xu et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-11-10_12-08-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Soft attention for &amp;#34;A woman is throwing a frisbee in a park.&amp;#34; (Xu et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Soft attention for &amp;ldquo;A woman is throwing a frisbee in a park.&amp;rdquo; (Xu et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The two figures above show the difference between soft and hard attention.
Hard attention, while faster at inference time, is non-differentiable and requires more complex methods to train (TODO: cite Luong).&lt;/p&gt;
&lt;h3 id=&#34;self-attention&#34;&gt;Self-Attention&lt;/h3&gt;
&lt;p&gt;Self attention is particularly useful for determining the relationship between different parts of an input sequence. The figure below demonstrates self-attention given an input sentence (Cheng, Dong, and Lapata 2016).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-11-10_13-11-31_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Line thickness indicates stronger self-attention (Cheng et al.).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Line thickness indicates stronger self-attention (Cheng et al.).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;How aligned the two vectors are.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cross-attention&#34;&gt;Cross Attention&lt;/h3&gt;
&lt;p&gt;TODO&lt;/p&gt;
&lt;h2 id=&#34;key-value-store&#34;&gt;Key-value Store&lt;/h2&gt;
&lt;p&gt;Query, key, and value come from the same input (self-attention).&lt;/p&gt;
&lt;p&gt;Check query against all possible keys in the dictionary. They have the same size.
The value is the result stored there, not necessarily the same size.
Each item in the sequence will generate a query, key, and value.&lt;/p&gt;
&lt;p&gt;The attention vector is a function of they keys and the query.&lt;/p&gt;
&lt;p&gt;Hidden representation is a function of the values and the attention vector.&lt;/p&gt;
&lt;p&gt;The Transformer paper talks about queries, keys, and values. This idea comes from retrieval systems.
If you are searching for something (a video, book, song, etc.), you present a system your query. That system will compare your query against the keys in its database. If there is a key that matches your query, the value is returned.&lt;/p&gt;
&lt;p&gt;\[
att(q, \mathbf{k}, \mathbf{v}) = \sum_i v_i f(q, k_i),
\]
where \(f\) is a similarity function.&lt;/p&gt;
&lt;p&gt;This is an interesting and convenient representation of attention.
To implement this idea, we need some measure of similarity.
Why not orthogonality? Two vectors that are orthogonal produce a scalar value of 0.
The maximum value two vectors will produce as a result of the dot product occurs when the two vectors have the exact same direction.
This is convenient because the dot product is simple and efficient and we are already performing these calculations in our deep networks in the form of matrix multiplication.&lt;/p&gt;
&lt;h2 id=&#34;scaled-dot-product-attention&#34;&gt;Scaled Dot Product Attention&lt;/h2&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-11-21_18-39-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Scaled dot-product attention ((Vaswani et al., n.d.))&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Scaled dot-product attention ((Vaswani et al., n.d.))
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Each &lt;strong&gt;query&lt;/strong&gt; vector is multiplied with each &lt;strong&gt;key&lt;/strong&gt; using the dot product.
This is implemented more efficiently via matrix multiplication.
A few other things are added here to control the output.
The first is &lt;strong&gt;scaling&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;multi-head-attention&#34;&gt;Multi-Head Attention&lt;/h2&gt;
&lt;p&gt;A single attention head can transform the input into a single representation. Is this analagous to using a single convolutional filter? The benefit of having multiple filters is to create multiple possible representations from the same input.&lt;/p&gt;
&lt;h2 id=&#34;encoder-decoder-architecture&#34;&gt;Encoder-Decoder Architecture&lt;/h2&gt;
&lt;p&gt;The original architecture of a transformer was defined in the context of sequence transduction tasks, where both the input and output are sequences. The most common task of this type is machine translation.&lt;/p&gt;
&lt;h2 id=&#34;encoder&#34;&gt;Encoder&lt;/h2&gt;
&lt;p&gt;The encoder layer takes an input sequence \(\{\mathbf{x}_t\}_{t=0}^T\) and transforms it into another sequence \(\{\mathbf{z}_t\}_{t=0}^T\).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;What is \(\mathbf{z}_t\)?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How is it used?
Input as key and value into second multi-head attention layer of the &lt;strong&gt;decoder&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Could you create an encoder only model?
Yes. Suitable for classification tasks &amp;ndash; classify the representation produced by the encoder.
&lt;strong&gt;How does this representation relate to understanding?&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It&amp;rsquo;s a transformation to another representation.&lt;/p&gt;
&lt;p&gt;Generated representation also considers the context of other parts of the same sequence (bi-directional).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;decoder&#34;&gt;Decoder&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Generates an output sequence.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Decoder-only models?
Suitable for text generation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What does the input represent?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What does the output represent?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What if we don&amp;rsquo;t use an encoder, what information is added in lieu of the encoder output?&lt;/p&gt;
&lt;!-- This HTML table template is generated by emacs/table.el --&gt;
&lt;table border=&#34;1&#34;&gt;
  &lt;tr&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Model&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Examples&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Tasks&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Encoder&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;ALBERT,&amp;nbsp;BERT,&amp;nbsp;DistilBERT,&lt;br /&gt;
      ELECTRA,&amp;nbsp;RoBERTa&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Sentence&amp;nbsp;classification,&amp;nbsp;named&amp;nbsp;entity&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      recognition,&amp;nbsp;extractive&amp;nbsp;question&amp;nbsp;answering&amp;nbsp;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Decoder&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;CTRL,&amp;nbsp;GPT,&amp;nbsp;GPT-2,&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      Transformer&amp;nbsp;XL&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Text&amp;nbsp;generation&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Encoder-decoder&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;BART,&amp;nbsp;T5,&amp;nbsp;Marian,&amp;nbsp;mBART&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
    &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt;
      &amp;nbsp;Summarization,&amp;nbsp;translation,&amp;nbsp;generative&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
      question&amp;nbsp;answering&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;p&gt;TODO&lt;/p&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/labmlai/status/1543159412940242945?s=20&amp;amp;t=EDu5FzDWl92EqnJlWvfAxA&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://twitter.com/labmlai/status/1543159412940242945?s=20&amp;t=EDu5FzDWl92EqnJlWvfAxA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Transduction_%28machine_learning%29&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://en.wikipedia.org/wiki/Transduction_(machine_learning)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.apronus.com/math/transformer-language-model-definition&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.apronus.com/math/transformer-language-model-definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2018-06-24-attention/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://lilianweng.github.io/posts/2018-06-24-attention/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://nlp.seas.harvard.edu/annotated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;http://nlp.seas.harvard.edu/annotated-transformer/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Long Short-Term Memory</title>
      <link>http://localhost:1313/notes/long_short_term_memory/</link>
      <pubDate>Tue, 12 Apr 2022 00:00:00 -0500</pubDate>
      
      <guid>http://localhost:1313/notes/long_short_term_memory/</guid>
      <description>&lt;p&gt;The recurrent nature of RNNs means that gradients get smaller and smaller as the timesteps increase.
This is known as the &lt;strong&gt;vanishing gradient problem&lt;/strong&gt;.
One of the first popular solutions to this problem is called &lt;strong&gt;Long Short-Term Memory&lt;/strong&gt;, a recurrent network architecture by Hochreiter and Schmidhuber.&lt;/p&gt;
&lt;p&gt;An LSTM is made up of memory blocks as opposed to simple hidden units.
Each block is differentiable and contains a memory cell along with 3 gates: the input, output, and forget gates.
These components allow the blocks to maintain some history of information over longer range dependencies.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-14_13-36-14_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;LSTM memory block with a single cell (adapted from Andrew Ng&amp;#39;s diagram).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;LSTM memory block with a single cell (adapted from Andrew Ng&amp;rsquo;s diagram).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The original LSTM only had an input and output gate.
Forget gates were added in 2000 by Gers et al. to control the amount of context that could be reset, if the task called for it.
Peephole connections were proposed by Gers et al. in 2002.
These are weights that combine the previous cell state to the gates in order to learn tasks that require precise timing.&lt;/p&gt;
&lt;p&gt;By controlling when information can either enter or leave the memory cell, LSTM blocks are able to maintain more historical context than RNNs.&lt;/p&gt;
&lt;h2 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h2&gt;
&lt;p&gt;The equations listed here follow notation and description from Alex Graves&amp;rsquo; thesis.&lt;/p&gt;
&lt;p&gt;The weight from unit \(i\) to \(j\) is given as \(w_{ij}\).
The input to unit \(j\) at time \(t\) is \(a_j^t\) and the result of its activation function is \(b_j^t\).
Let \(\psi\), \(\phi\), and \(\omega\) be the input, forget, and output gates.
A memory cell is denoted by \(c \in C\), where \(C\) is the set of cells in the network.
The activation, or state, of a given cell \(c\) at time \(t\) is \(s_c^t\).
The output of each gate passes through an activation function \(f\), while the input and output activation functions of a memory block are given by \(g\) and \(h\).&lt;/p&gt;
&lt;p&gt;The forward pass for the input gates is&lt;/p&gt;
&lt;p&gt;\[
a_{\psi}^t = \sum_{i=1}^I w_{i\psi}x_i^t + \sum_{h=1}^H w_{h\psi}b_h^{t-1} + \sum_{c=1}^C w_{c\psi}s_c^{t-1}.
\]&lt;/p&gt;
&lt;p&gt;The output of the forget gates is&lt;/p&gt;
&lt;p&gt;\[
a_{\phi}^t = \sum_{i=1}^I w_{i\phi}x_i^t + \sum_{h=1}^H w_{h\phi}b_h^{t-1} + \sum_{c=1}^C w_{c\phi}s_c^{t-1}.
\]&lt;/p&gt;
&lt;p&gt;The output of the output gates is&lt;/p&gt;
&lt;p&gt;\[
a_{\omega}^t = \sum_{i=1}^I w_{i\omega}x_i^t + \sum_{h=1}^H w_{h\omega}b_h^{t-1} + \sum_{c=1}^C w_{c\omega}s_c^{t-1}.
\]&lt;/p&gt;
&lt;p&gt;Each of the outputs above is passed through an activation function \(f\).&lt;/p&gt;
&lt;p&gt;The output of each cell is computed as&lt;/p&gt;
&lt;p&gt;\[
a_c^t = \sum_{i=1}^I w_{ic}x_i^t + \sum_{i=1}^H w_{hc}b_h^{t-1}
\]&lt;/p&gt;
&lt;p&gt;and the internal state is updated via&lt;/p&gt;
&lt;p&gt;\[
s_c^t = b_{\phi}^t s_c^{t-1} + b_{\psi}^t g(a_c^t).
\]&lt;/p&gt;
&lt;p&gt;The state update considers the state at the previous timestep multiplied by the output of the forget gate.
That is, it controls how much of the current memory to keep.&lt;/p&gt;
&lt;p&gt;The final cell output is given as&lt;/p&gt;
&lt;p&gt;\[
b_c^t = b_{\omega}^t h(s_c^t).
\]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recurrent Neural Networks</title>
      <link>http://localhost:1313/notes/recurrent_neural_networks/</link>
      <pubDate>Sun, 10 Apr 2022 00:00:00 -0500</pubDate>
      
      <guid>http://localhost:1313/notes/recurrent_neural_networks/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bidirectional-recurrent-neural-networks&#34;&gt;Bidirectional Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Neural networks are an effective tool for regression and classification tasks, but they do not consider the dependencies of information over time.
Many tasks have implicit information that is dependent on input that may have already been processed or may not be seen until the future.&lt;/p&gt;
&lt;p&gt;Recurrent Neural Networks (RNN) consider the historical context of time-series data.
Bi-directional Recurrent Neural Networks (BRNN) consider both historical and future context. This is necessary for tasks like language tanslation.&lt;/p&gt;
&lt;p&gt;Parameter sharing across different parts of the model is key for sequence models.
Different instances of a particular feature may appear at different time steps.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I see Naomi there.&amp;rdquo; and &amp;ldquo;Naomi is right there&amp;rdquo; both convey that Naomi is present, but we would not require the model to have separate parameters just because the word position is different between the two.&lt;/p&gt;
&lt;p&gt;Recurrent connections provide a memory of sorts.
This enables important contextual information to be &amp;ldquo;remembered&amp;rdquo; throughout time.
These models are not without their limitations.
When trained with gradient descent, the gradient information passed throughout multiple time steps can become insignificant.
There are several ways to address the &lt;strong&gt;vanishing gradient&lt;/strong&gt; problem which are explored in alternative models such as &lt;a href=&#34;http://localhost:1313/notes/long_short_term_memory/&#34;&gt;Long Short-Term Memory&lt;/a&gt; and &lt;a href=&#34;http://localhost:1313/notes/transformers/&#34;&gt;Transformers&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;The definition of RNNs start with that of &lt;a href=&#34;http://localhost:1313/notes/neural_networks/&#34;&gt;Neural Networks&lt;/a&gt;.
One layer of an RNN has some number of hidden units that transforms the input into an intermediate representation.
In addition to transforming the input, another set of parameters is used to transform the hidden context over time.
The difference is that the hidden layer is shared over time, as seen in the equation below.&lt;/p&gt;
&lt;p&gt;\[
\mathbf{h}^{(t)} = f(\mathbf{h}^{(t-1)}, \mathbf{x}^{(t)}; \mathbf{\theta})
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-09_16-36-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Computation graph of an RNN (By fdeloche - Own work, CC BY-SA 4.0, &amp;lt;https://commons.wikimedia.org/w/index.php?curid=60109157&amp;gt;)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Computation graph of an RNN (By fdeloche - Own work, CC BY-SA 4.0, &lt;a href=&#34;https://commons.wikimedia.org/w/index.php?curid=60109157&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://commons.wikimedia.org/w/index.php?curid=60109157&lt;/a&gt;)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the computation graph above, a recurrent network has three weight matrices associated with its forward pass.
An input weight matrix \(U \in \mathbb{R}^{H \times D}\) processes the features for each &lt;em&gt;frame&lt;/em&gt; of the input sequence.
The hidden layer has a weight matrix \(V \in \mathbb{R}^{H \times H}\), where \(H\) is the number of hidden nodes.
The output layer will have a weight matrix \(W \in \mathbb{R}^{O \times H}\).&lt;/p&gt;
&lt;h3 id=&#34;forwards-pass&#34;&gt;Forwards Pass&lt;/h3&gt;
&lt;p&gt;To understand the computation graph of an RNN, consider an input of length \(T\) with \(D\) features. That is, each input sample is a sequence of features. This could be represented as encoded video data, text data, or any other sequence signals.
To compute the output of a hidden layer \(\mathbf{h}\) at time \(t\), take a linear combination of all input feature \(x_i^t\) at time \(t\) in addition to the output of the previous hidden layer and then add the linear combination of output activations for each node in the hidden layer:&lt;/p&gt;
&lt;p&gt;\[
a_h^t = \sum_{d=1}^D w_{dh} x_d^t + \sum_{h&amp;rsquo;=1}^H w_{h&amp;rsquo;h} b_{h&amp;rsquo;}^{t-1},
\]&lt;/p&gt;
&lt;p&gt;where \(b_h^t = \theta_h(a_h^t)\) and we assume the bias term is concatenated with the weights.&lt;/p&gt;
&lt;p&gt;Weights in the hidden layer are crucial for RNNs to adapt to contextual features based on their occurrence relative to time.
For example, a character-based language model based on a traditioinal network would produce similar output for consecutive letters that are the same.
In an RNN, the hidden weights would produce a different output for each consecutive character even if it were the same.&lt;/p&gt;
&lt;p&gt;The hidden layer outputs are used in both the subsequent computations through time as well as the output node for each instance \(t\). The inputs to the output node are computed from the hidden node at the same time as the output to the hidden activation:&lt;/p&gt;
&lt;p&gt;\[
a_k^t = \sum_{h=1}^H w_{hk}b_h^t.
\]&lt;/p&gt;
&lt;h3 id=&#34;backwards-pass&#34;&gt;Backwards Pass&lt;/h3&gt;
&lt;p&gt;The gradients of a recurrent network are computed using backpropagation, similar to neural networks.
Since the forward pass is over \(t\) time step, the backward pass must consider them as well.
This variant of backpropagation for recurrent models is calling backpropagation through time (BPTT).&lt;/p&gt;
&lt;p&gt;Like a feed forward network, the output is dependent on the activation of the hidden layer.
For a recurrent model, its dependence is through the output of the hidden layer as well as the pass to the next hidden time step.&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathcal{L}}{\partial a_h^t} = \frac{\partial \mathcal{L}}{\partial b_h^t} \frac{\partial b_h^t}{\partial a_h^t}
\]&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathcal{L}}{\partial b_h^t} = \sum_{k=1}^K \frac{\partial \mathcal{L}}{\partial a_k^t} \frac{\partial a_k^t}{\partial b_h^t} + \sum_{h&amp;rsquo;=1}^H \frac{\partial \mathcal{L}}{\partial a_{h&amp;rsquo;}^{t+1}} \frac{\partial a_{h&amp;rsquo;}^{t+1}}{\partial a_{h}^t}
\]&lt;/p&gt;
&lt;p&gt;The derivatives with respect to the weights are given as&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathcal{L}}{\partial w_{ij}} = \sum_{t=1}^T \frac{\partial \mathcal{L}}{\partial a_j^t} \frac{\partial a_j^t}{\partial w_{ij}}.
\]&lt;/p&gt;
&lt;h2 id=&#34;bidirectional-recurrent-neural-networks&#34;&gt;Bidirectional Recurrent Neural Networks&lt;/h2&gt;
&lt;p&gt;Standard RNNs work for many problems with sequential input.
Training such a model would consider the full input through time \(T\), but inference may only be able to consider the data up to time \(t &amp;lt; T\).
There are sequential tasks which could leverage from both past and future context, such as language translation.
For this case, BRNNs were proposed &amp;lt;&amp;amp;schusterBidirectionalRecurrentNeural1997&amp;gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-11-06_15-18-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Diagram of BRNN from Graves et al.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Diagram of BRNN from Graves et al.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://karpathy.github.io/2015/05/21/rnn-effectiveness/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;The Unreasonable Effectiveness of RNNs&lt;/a&gt; by Andrej Karpathy&lt;/li&gt;
&lt;li&gt;&amp;lt;&amp;amp;gravesSupervisedSequenceLabelling2012&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Understanding LSTM Networks&lt;/a&gt; by Christopher Colah&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Optimization for Deep Learning</title>
      <link>http://localhost:1313/notes/optimization_for_deep_learning/</link>
      <pubDate>Thu, 07 Apr 2022 00:00:00 -0500</pubDate>
      
      <guid>http://localhost:1313/notes/optimization_for_deep_learning/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gradient-descent-and-its-variants&#34;&gt;Gradient Descent and its Variants&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adaptive-learning-rate-methods&#34;&gt;Adaptive Learning Rate Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parameter-initialization&#34;&gt;Parameter Initialization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ruder.io/optimizing-gradient-descent/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://ruder.io/optimizing-gradient-descent/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.deeplearningbook.org/contents/optimization.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.deeplearningbook.org/contents/optimization.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;empirical risk minimization&lt;/strong&gt; - minimizing over an empirical distribution. Differs from risk minimization which is minimizing over the true distribution. We typically do not know the true distribution.&lt;/p&gt;
&lt;p&gt;Complex models are able to memorize the dataset.&lt;/p&gt;
&lt;p&gt;In many applications for training, what we want to optimize is different from what we actually optimize since we need to have useful derivatives for gradient descent. For example, the 0-1 loss&lt;/p&gt;
&lt;p&gt;\begin{equation*}
L(i, j) =
\begin{cases}
0 \qquad i = j \\
1 \qquad i \ne j
\end{cases}
\qquad i,j \in M
\end{equation*}&lt;/p&gt;
&lt;p&gt;is what we would really want to minimize for classification tasks.
In practice we use something like &lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Cross Entropy Loss&lt;/a&gt;.
As pointed out by (Goodfellow et al.), there are sometimes advantages with using &lt;strong&gt;surrogate loss functions&lt;/strong&gt;.
A 0-1 loss may eventually fit the training set with 100% accuracy.
At this point, no further optimization could take place as the error would be 0.
With losses like negative log likelihood, optimization could continue which may result in increasing the margin between classes.&lt;/p&gt;
&lt;p&gt;Larger batch sizes provide a more accurate estimate of the gradient.&lt;/p&gt;
&lt;p&gt;Randomly selecting samples is crucial for learning.
Datasets may be arranged in such a strong bias is present.
Shuffling once isn&amp;rsquo;t enough because the data is biased after the first iteration.
We could only get around this if we had the true distribution to generate new samples.&lt;/p&gt;
&lt;p&gt;If our training set is extremely large, we may converge to a solution without ever having gone through all samples.
Typically, models are able to train on multiple passes of the dataset to increase their generalization error.
Each subsequent pass may increase the bias, but not enough to decrease generalization performance.&lt;/p&gt;
&lt;p&gt;The gradient norm can be monitored while training to see if the issue is local minima or any other critical point (&lt;a href=&#34;#citeproc_bib_item_5&#34;&gt;Zhao, Zhang, and Hu, n.d.&lt;/a&gt;).
If the parameters were to get stuck at a critical point, the gradient norm should shrink over time.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2024-03-21_09-21-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;The gradient norm decreases as it settles into some minima (Zhao et al.).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;The gradient norm decreases as it settles into some minima (Zhao et al.).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;gradient-descent-and-its-variants&#34;&gt;Gradient Descent and its Variants&lt;/h2&gt;
&lt;p&gt;Original gradient descent update:&lt;/p&gt;
&lt;p&gt;\[
\theta = \theta - \eta \nabla_{\theta}J(\theta)
\]&lt;/p&gt;
&lt;p&gt;Having a constant value for \(\eta\) means that the network will usually be unable to converge to a local minimum.
As the parameters reach a minimum, the constant learning update means that it will jump around the true minimum point. This is usually remedied in part by setting up a decreasing learning rate schedule.
This necessarily requires more manual guess work as to what the best annealing schedule would be.&lt;/p&gt;
&lt;h3 id=&#34;momentum&#34;&gt;Momentum&lt;/h3&gt;
&lt;p&gt;When the loss surface is more steep in one dimension than others, SGD will move back and forth in the directions of greatest descent while only slowly moving in the direction with a smaller decline. The figure below gives an example.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-07_16-35-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;SGD moves slower towards covergence for non-uniform surfaces.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;SGD moves slower towards covergence for non-uniform surfaces.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;If the gradient had some &lt;em&gt;momentum&lt;/em&gt; which built up over time, it would take fewer iterations to converge.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-07_16-38-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;SGD with momentum converges in fewer iterations.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;SGD with momentum converges in fewer iterations.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In practice, this can be implemented by adding some fraction of the previous update to the current step:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{g}_t &amp;amp;= \alpha \mathbf{g}_{t-1} + \eta \nabla_{\theta}J(\theta)\\
\theta &amp;amp;= \theta - \mathbf{g}_t
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;nesterov-momentum&#34;&gt;Nesterov Momentum&lt;/h3&gt;
&lt;p&gt;If we allow the momentum to keep increasing, the steps become greater and greater. This could lead to the parameters &amp;ldquo;rolling&amp;rdquo; out of the minimum up a steep incline.
If our algorithm knew that it was coming up to an incline, it would be smarter to slow down.
This is essentially what Nesterov momentum does.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-07_17-01-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Nesterov momentum computes the gradient after applying momentum.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Nesterov momentum computes the gradient after applying momentum.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;\begin{align*}
\mathbf{g}_t &amp;amp;= \alpha \mathbf{g}_{t-1} + \eta \nabla_{\theta}J(\theta - \alpha \mathbf{g}_{t-1})\\
\theta &amp;amp;= \theta - \mathbf{g}_t
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;adaptive-learning-rate-methods&#34;&gt;Adaptive Learning Rate Methods&lt;/h2&gt;
&lt;p&gt;The rate at which a model converges to some solution is dependent on many factors. One that we can control is the learning rate.
If the learning rate is too large, the model may never converge because it jumps too far in each iteration.
If the learning rate is too small, it may take much longer to converge to any solution.&lt;/p&gt;
&lt;p&gt;It would be ideal if the optimization algorithm could adapt its learning rate to local changes in the loss landscape.
In that way, the algorithm would be less dependent on the initial learning rate.&lt;/p&gt;
&lt;h3 id=&#34;adagrad&#34;&gt;Adagrad&lt;/h3&gt;
&lt;p&gt;Adagrad adapts the learning rate to the parameters following the idea that parameters associated with salient features should be updated less frequently (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Duchi, Hazan, and Singer 2011&lt;/a&gt;). If they occur often, updating them with a larger step would result in a solution that is more dependent on them at the expense of other features.&lt;/p&gt;
&lt;p&gt;Adagrad uses a different learning rate for every parameter:&lt;/p&gt;
&lt;p&gt;\[
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}}g_t
\]&lt;/p&gt;
&lt;p&gt;Here, \(g_t = \frac{\partial J(\theta)}{\partial \theta}\). This provides a partial derivative for every parameter \(\theta_i\).
A history of gradient changes are accumulated in a matrix \(G_t \in \mathbb{R}^{d \times d}\) which is a diagonal matrix containing the sum of squares of the gradients with respect to each \(\theta_i\) up to the current step.&lt;/p&gt;
&lt;p&gt;In effect, the parameters with larger partial derivatives have a sharper decrease in learning rate.
The downside to this method is that, as squared gradients are accumulated in \(G_t\), the sum increases causing the learning rate to eventually be too small to learn.&lt;/p&gt;
&lt;h3 id=&#34;rmsprop&#34;&gt;RMSProp&lt;/h3&gt;
&lt;p&gt;To remedy the long term issues of Adagrad, Geoffrey Hinton proposed RMSProp.
There was no formal publication for this. It was discussed and taught in Coursera course on &lt;a href=&#34;https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Neural Networks&lt;/a&gt;.
Instead of accumulating gradients, RMSProp uses an exponentially weighted moving average:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{s}_t = \rho \mathbf{s} + (1 - \rho)\mathbf{g}_{t-1} \odot \mathbf{g}_t
\]&lt;/p&gt;
&lt;p&gt;A new parameter \(\rho\) controls how much of the historical gradient is used. The update is&lt;/p&gt;
&lt;p&gt;\[
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\mathbf{s}_t + \epsilon}}\mathbf{g}_t.
\]&lt;/p&gt;
&lt;p&gt;Hinton proposed that \(\rho=0.9\) in the original lectures.&lt;/p&gt;
&lt;h3 id=&#34;adam&#34;&gt;Adam&lt;/h3&gt;
&lt;p&gt;One of the most popular gradient descent variants in used today is Adam (&lt;a href=&#34;#citeproc_bib_item_4&#34;&gt;Kingma and Ba 2017&lt;/a&gt;).
Short for Adaptive Moment Estimation, Adam adapts the learning rate to each parameter.
Similar to RMSProp, it stores an exponentially moving average of past squared gradients.
Adam additionally stores first-order moments of the gradients.&lt;/p&gt;
&lt;p&gt;After calculating the gradients \(g_t\) at time \(t\) the first and second moment estimates are updated as&lt;/p&gt;
&lt;p&gt;\begin{align*}
m_t &amp;amp;= \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t\\
v_t &amp;amp;= \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2
\end{align*}&lt;/p&gt;
&lt;p&gt;The estimates \(m_t\) and \(v_t\) are initialized to zero leading to updated estimates that are biased to zero.
The authors counteract this by computing &lt;em&gt;bias-corrected estimates&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\hat{m}_t &amp;amp;= \frac{m_t}{1 - \beta_1^t}\\
\hat{v}_t &amp;amp;= \frac{v_t}{1 - \beta_2^t}
\end{align*}&lt;/p&gt;
&lt;p&gt;The final update rule step is&lt;/p&gt;
&lt;p&gt;\[
\theta_t = \theta_{t-1} - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}} + \epsilon}.
\]&lt;/p&gt;
&lt;p&gt;There are several other varients. A good overview of these can be found on &lt;a href=&#34;https://ruder.io/optimizing-gradient-descent/index.html#fn9&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Sebastian Ruder&amp;rsquo;s&lt;/a&gt; blog.
The figures below provide some visual intuition of the behavior of common gradient descent variants.
These visualizations were provided by &lt;a href=&#34;https://twitter.com/alecrad&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Alec Radford&lt;/a&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-07_22-14-25_opt1.gif&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Behavior of algorithms at a saddle point (Credit: Alec Radford).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Behavior of algorithms at a saddle point (Credit: Alec Radford).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-07_22-17-24_opt2.gif&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Behavior of each algorithm on a loss surface (Credit: Alec Radford).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Behavior of each algorithm on a loss surface (Credit: Alec Radford).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Additional visualizations can be found &lt;a href=&#34;https://github.com/Jaewan-Yun/optimizer-visualization&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;parameter-initialization&#34;&gt;Parameter Initialization&lt;/h2&gt;
&lt;p&gt;Due to the complexity of their loss landscapes, the choice of initialization can have a significant impact on the solution. This affects how quickly the model converges. Although &lt;a href=&#34;https://ai.googleblog.com/2022/04/reproducibility-in-deep-learning-and.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;recent work&lt;/a&gt; aims to smooth loss surfaces so that models are easier to train, deep learning models can be tricky to reproduce.&lt;/p&gt;
&lt;p&gt;There is not much known about what makes the most optimal initialization strategy, but one property is that of weight symmetry. If all weights are initialized to the same value, their update will also be uniform. If two nodes are connected to the same input, there update will be uniform as well. Understanding this, a reasonable initialization strategy would be to ensure that the weights to not permit any symmetry in nodes connected to the same input.&lt;/p&gt;
&lt;p&gt;Small weights during initialization may lead to vanishing gradients.
Large weights may lead to exploding gradients as successive multiplications are applied.
The parameter values should be large enough to propagate information effectively through the network.&lt;/p&gt;
&lt;h3 id=&#34;normalized-initialization--xavier&#34;&gt;Normalized Initialization (Xavier)&lt;/h3&gt;
&lt;p&gt;Normalized initialization chooses an initial scale of the weights of a fully connected layer based on the number input and output nodes:&lt;/p&gt;
&lt;p&gt;\[
W_{i,j} \sim U\Bigg(-\sqrt{\frac{6}{m + n}}, \sqrt{\frac{6}{m+n}}\Bigg),
\]&lt;/p&gt;
&lt;p&gt;where \(m\) and \(n\) are the number of input and output nodes, respectively.
This initialization was empirically validated by (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Glorot and Bengio, n.d.&lt;/a&gt;) with the goal that all layers have the same activation variance and back-propagated gradient variance.&lt;/p&gt;
&lt;h3 id=&#34;he-initialization&#34;&gt;He Initialization&lt;/h3&gt;
&lt;p&gt;Xavier initialization is based on successive matrix multiplications without any non-linearities.
Any deep learning model will surely break this assumption.
He et al. derive another initialization strategy while considering rectified linear units (ReLU) and parametric rectified linear units (PReLU) (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;He et al. 2015&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://spell.ml/blog/lr-schedulers-and-adaptive-optimizers-YHmwMhAAACYADm6F&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://spell.ml/blog/lr-schedulers-and-adaptive-optimizers-YHmwMhAAACYADm6F&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Duchi, John, Elad Hazan, and Yoram Singer. 2011. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.” &lt;i&gt;Journal of Machine Learning Research&lt;/i&gt; 12 (61): 2121–59. &lt;a href=&#34;http://jmlr.org/papers/v12/duchi11a.html&#34;&gt;http://jmlr.org/papers/v12/duchi11a.html&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Glorot, Xavier, and Yoshua Bengio. n.d. “Understanding the Difﬁculty of Training Deep Feedforward Neural Networks,” 8.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_3&#34;&gt;&lt;/a&gt;He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.” &lt;i&gt;Arxiv:1502.01852 [Cs]&lt;/i&gt;, February. &lt;a href=&#34;http://arxiv.org/abs/1502.01852&#34;&gt;http://arxiv.org/abs/1502.01852&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_4&#34;&gt;&lt;/a&gt;Kingma, Diederik P., and Jimmy Ba. 2017. “Adam: A Method for Stochastic Optimization.” &lt;i&gt;Arxiv:1412.6980 [Cs]&lt;/i&gt;, January. &lt;a href=&#34;http://arxiv.org/abs/1412.6980&#34;&gt;http://arxiv.org/abs/1412.6980&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_5&#34;&gt;&lt;/a&gt;Zhao, Yang, Hao Zhang, and Xiuyuan Hu. n.d. “Penalizing Gradient Norm for Efﬁciently Improving Generalization in Deep Learning,” 11.&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Convolutional Neural Networks</title>
      <link>http://localhost:1313/notes/convolutional_neural_networks/</link>
      <pubDate>Sat, 02 Apr 2022 00:00:00 -0500</pubDate>
      
      <guid>http://localhost:1313/notes/convolutional_neural_networks/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#convolution-operator&#34;&gt;Convolution Operator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#properties-of-convolutions&#34;&gt;Properties of Convolutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parameter-sharing&#34;&gt;Parameter Sharing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pooling&#34;&gt;Pooling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backwards-pass&#34;&gt;Backwards Pass&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example&#34;&gt;Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#neural-networks-for-image-classification&#34;&gt;Neural Networks for Image Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#useful-resources&#34;&gt;Useful Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;&lt;strong&gt;Key Concepts&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Invariance and Equivariance&lt;/li&gt;
&lt;li&gt;Definition&lt;/li&gt;
&lt;li&gt;Padding, Stride, Kernel size, dilation&lt;/li&gt;
&lt;li&gt;Purpose of multiple feature maps&lt;/li&gt;
&lt;li&gt;Receptive fields and hierarchies of features&lt;/li&gt;
&lt;li&gt;Downsampling, Upsampling, Examples in research&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Dense neural networks made up of linear layers and a chosen activation function are not practical for image data. Consider an image of size \(224\times224\times3\). The first layer of a dense network would require a \(150,528\times n\) parameter matrix, where \(n\) is the number of nodes in the first layer. It is common to build dense networks where the first layer has more nodes than input features. In this case, we would need a minimum of \(150,528^2\) parameters in the first layer. Even if we chose something much smaller like \(n=1024\), this would require \(154,140,672\) parameters for just the first layer. This is clearly impractical.&lt;/p&gt;
&lt;p&gt;Aside from requiring a large number of parameters, we might ask whether it is beneficial to feed raw pixel values into a dense network. The network itself would be learning pixel-wise features with no regard to their spatial relationship. This makes our network&amp;rsquo;s job much more difficult because the spatial arrangement of features tells us so much about what we see. In practice, this means that the network would have to learn the same features at every location in the image. We would instead prefer this network to learn features that are &lt;strong&gt;invariant&lt;/strong&gt; to translation. That is, the network should learn features that are the same regardless of where they appear in the image.&lt;/p&gt;
&lt;p&gt;Invariance to translation is very convenient and can save our network a lot of work in learning the same feature at every point in the input. It is also desirable that our network is invariant to other transformations such as rotation, scaling, skewing, and warping. Formally, a function \(f(\mathbf{x})\) of an image \(\mathbf{x}\) is invariant to a transformation \(t(\mathbf{x})\) if&lt;/p&gt;
&lt;p&gt;\[
f(t(\mathbf{x})) = f(\mathbf{x}).
\]&lt;/p&gt;
&lt;p&gt;Aside from invariance, some models should be &lt;strong&gt;equivariant&lt;/strong&gt; to certain transformations. That is, the output of the model should change in the same way as the input. Image segmentation models should be equivariant to translation. If we were to shift an image by a few pixels, the output segmentation mask should also shift by the same amount. Convolutional neural networks are equivariant to &lt;em&gt;translation&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;convolution-operator&#34;&gt;Convolution Operator&lt;/h2&gt;
&lt;p&gt;A convolution is a function that takes two functions as input and produces a third function as output. The first function is the input and the second function is the kernel. The output is called the feature map. The kernel is also sometimes called the filter.&lt;/p&gt;
&lt;p&gt;\[
(f * g)(t) = \int f(t-a)g(a)da
\]&lt;/p&gt;
&lt;p&gt;We can view them more concretely by considering the functions to be vectors. For example, let the function \(f\) be an input vector \(x\) and \(w\) be a kernel representing a filter. The convolution operator is then&lt;/p&gt;
&lt;p&gt;\[
(x * w)(t) = \int x(t-a)w(a)da.
\]&lt;/p&gt;
&lt;p&gt;The result the &lt;strong&gt;feature map&lt;/strong&gt; representing the response of the kernel at each location in the input.&lt;/p&gt;
&lt;p&gt;In the case of discrete values, the operator is written as&lt;/p&gt;
&lt;p&gt;\[
(x * w)(t) = \sum_{a}x(t-a)w(a).
\]&lt;/p&gt;
&lt;p&gt;In machine learning, the kernel \(w\) is usually represented by some set of parameters that is optimized.&lt;/p&gt;
&lt;p&gt;CNNs for images use a 2D convolution defined as&lt;/p&gt;
&lt;p&gt;\[
(I * K)(i, j) = \sum_m \sum_n I(i-m, j-n)K(m, n).
\]&lt;/p&gt;
&lt;p&gt;In this formulation, the kernel is effectively flipped across the vertical and horizontal axis.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-05_19-09-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;2D Convolution (Image Credit: Song Ho Ahn (linked above)).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;2D Convolution (Image Credit: Song Ho Ahn (linked above)).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In practice, most deep learning APIs implement &lt;strong&gt;cross-correlation&lt;/strong&gt;.
Whether the function is implemented as true convolution makes no difference when it comes to optimizing a deep model since filter weights that are produced with cross-correlation would be produced, albeit flipped, with convolution.&lt;/p&gt;
&lt;p&gt;\[
(K * I)(i, j) = \sum_m \sum_n I(i+m, j+n)K(m, n).
\]&lt;/p&gt;
&lt;h2 id=&#34;properties-of-convolutions&#34;&gt;Properties of Convolutions&lt;/h2&gt;
&lt;p&gt;Convolutional networks are commonly built on &lt;em&gt;full&lt;/em&gt; or &lt;em&gt;valid&lt;/em&gt; convolutions. Other variants have also been explored. Here, we will briefly discuss the different properties of this operator. A more detailed treatment can be found in (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;padding&#34;&gt;Padding&lt;/h3&gt;
&lt;p&gt;By definition, a convolution of an input with a filter of size \(n\times n\) will produce an output of size \((m-n+1)\times(m-n+1)\), where \(m\) is the size of the input. This means that the output will be smaller than the input. This is often referred to as a &lt;strong&gt;valid&lt;/strong&gt; convolution. The figure below shows a convolution between a \(3\times3\) kernel and a \(5\times5\) input.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2023-10-26_16-31-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A valid convolution (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A valid convolution (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The output of this convolution is a \(3\times3\) feature map. This is a problem if we want to build a deep network. Each convolution will reduce the size of the input. If we were to stack multiple convolutional layers, the output would eventually be too small to be useful. If we want our output to be same size as the input, we can add padding to the original input image before convolving it. This is often known as a &lt;strong&gt;full&lt;/strong&gt; convolution. An example is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2023-10-26_16-34-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;A full convolution (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;A full convolution (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;stride&#34;&gt;Stride&lt;/h3&gt;
&lt;p&gt;So far, we have only looked at convolutions which step by 1 unit as they shift over the image. We can control the size of this step, or &lt;strong&gt;stride&lt;/strong&gt;, to produce different outcomes. Picking a non-unit stride has a number of effects on the features that are learned in a convolutional neural network.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dimensionality reduction&lt;/strong&gt;: Skipping over pixels reduces the size of the output feature map. This provides another way of downsampling the input.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Less computation&lt;/strong&gt;: Fewer computations are required to produce the output feature map.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Increased field of view&lt;/strong&gt;: A larger stride increases the field of view of the kernel, leading to larger receptive fields in deeper layers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given an input of size \(m\times m\) and a kernel of size \(n\times n\), the output size of a convolution with stride \(s\) is given by&lt;/p&gt;
&lt;p&gt;\[
\left\lfloor\frac{m-n}{s}\right\rfloor + 1.
\]&lt;/p&gt;
&lt;p&gt;The figure below shows a convolution with stride 2 on a \(5\times5\) input.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2023-10-26_16-45-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;A convolution with stride 2 (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;A convolution with stride 2 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;kernel-size&#34;&gt;Kernel Size&lt;/h3&gt;
&lt;p&gt;The size of the kernel has a large impact on the features that are learned. A larger kernel will have a larger receptive field. This means that the kernel will be able to capture more information about the input. However, this comes at the cost of increased computation. Common kernel sizes in most CNNs are \(3\times3\), \(5\times5\), and \(7\times7\). It is also convenient to pick an odd kernel size so that the kernel has a center pixel.&lt;/p&gt;
&lt;h3 id=&#34;dilation&#34;&gt;Dilation&lt;/h3&gt;
&lt;p&gt;Around 2015, a research trend for CNNs was to find a way to increase the receptive field without adding more parameters. The result is a &lt;strong&gt;dilated&lt;/strong&gt; convolution. The output of a dilated convolution is computed by skipping over pixels in the input. The figure below shows a \(3\times3\) kernel with a dilation of 2.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2023-10-27_08-19-10_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;A dilated convolution (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;A dilated convolution (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The output size is computed as&lt;/p&gt;
&lt;p&gt;\[
\left\lfloor\frac{m + 2p - n - (n-1)(d-1)}{s}\right\rfloor + 1,
\]&lt;/p&gt;
&lt;p&gt;where \(p\) is the amount of padding and \(d\) is the dilation factor.&lt;/p&gt;
&lt;h2 id=&#34;parameter-sharing&#34;&gt;Parameter Sharing&lt;/h2&gt;
&lt;p&gt;In a densely connected layer, each input has a corresponding weight attached to it.
For example, we ran a few &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/tree/main/deep_learning&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;introductory experiments&lt;/a&gt; on the CIFAR10 dataset using a deep, densely connected network.
To reduce the amount of parameters in the first layer, we converted each image to grayscale.
The input also had to be vectorized in order to be processed.
With a processed image of size \(32 \times 32\), this resulted in a \(1024\) dimensional vector for each input.
Our first layer had \(512\) nodes resulting in a parameter matrix of size \(1024 \times 512\).&lt;/p&gt;
&lt;p&gt;Convolution layers have &lt;strong&gt;shared parameters&lt;/strong&gt;, meaning the same parameters are used for each region on the input.
A single channel 2D filter of size \(n \times n\) only requires \(n \times n\) parameters.
Each kernel is applied to every location in the original input using the same parameters.&lt;/p&gt;
&lt;p&gt;Kernels are &lt;strong&gt;equivariant&lt;/strong&gt; to translation because of their shared parameters.
That is, as the input changes, the output will change in the same way.
Formally, two functions \(f\) and \(g\) are equivarient if&lt;/p&gt;
&lt;p&gt;\[
f(g(x)) = g(f(x)).
\]&lt;/p&gt;
&lt;p&gt;In the context of image features, a kernel applied across an image will produce strong responses in regions that exhibit the same local features.
For example, a kernel that detects horizontal lines will produce strong responses across all parts of the image that show a large contrast between vertical pixels.&lt;/p&gt;
&lt;h2 id=&#34;pooling&#34;&gt;Pooling&lt;/h2&gt;
&lt;p&gt;When a convolution is applied to some input image, the resulting output feature map represents the responses of the kernel applied to each location in the image.
If this original image were to be shifted by a few pixels, the reponses would also be shifted.
In order to increase the robustness of a model to small perturbations such as translation, a pooling layer was historically employed after each non-linear activation following a convolutional layer.&lt;/p&gt;
&lt;p&gt;They effectively provide a summary statistic of a local region by selecting the average or maximum responses in a small window. This provides translation invariance since the maximum response will be the same for a region even if it is translated by a small amount.
It also acts as a quick way to downsample the image, leading to fewer parameters in the model.&lt;/p&gt;
&lt;p&gt;Modern works do not employ pooling operations as often. For example (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;He et al. 2016&lt;/a&gt;) perform dimensionality reduction with \(1 \times 1\) convolutions.
(&lt;a href=&#34;#citeproc_bib_item_6&#34;&gt;Springenberg et al. 2015&lt;/a&gt;) argue that fully convolutional networks can achieve the same performance without max pooling.&lt;/p&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster.&amp;rdquo; - Geoffrey Hinton&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;h2 id=&#34;backwards-pass&#34;&gt;Backwards Pass&lt;/h2&gt;
&lt;p&gt;The parameters of a convolutional layer are updated via backpropagation like any other layer with trainable parameters.
Given a kernel \(w\), it is necessary to compute \(\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}}\), where \(w_{m&amp;rsquo;, n&amp;rsquo;}\) is the \((m&amp;rsquo;, n&amp;rsquo;)th\) entry of the kernel.
This entry affects all entries in the feature map, so \(\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}}\) will sum over all such entries.&lt;/p&gt;
&lt;p&gt;To show the gradient calculation, we will assume a convolutional layer with zero padding and unit stride with a square \(2 \times 2\) kernel applied to a square \(3 \times 3\) input.
The output map is then \((3 - 2 + 1) \times (3 - 2 + 1) = 2 \times 2\).&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}} = \sum_{i=0}^2 \sum_{j=0}^2 \frac{\partial \mathcal{L}}{\partial x_{i, j}} \frac{\partial x_{i,j}}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}}
\]&lt;/p&gt;
&lt;p&gt;If \(\mathbf{z}^{(l-1)}\) is the output from the previous layer, then&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{\partial x_{i, j}}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}} &amp;amp;= \frac{\partial}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}} \sum_{m} \sum_{n} w_{m, n} z_{i+m, j+n}^{(l-1)} + b\\
&amp;amp;= \frac{\partial}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}} w_{m&amp;rsquo;, n&amp;rsquo;}z_{i+m&amp;rsquo;, j+n&amp;rsquo;}^{(l-1)}\\
&amp;amp;= z_{i+m&amp;rsquo;, j+n&amp;rsquo;}^{(l-1)}
\end{align*}&lt;/p&gt;
&lt;p&gt;Then \(\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}}\) becomes&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}} &amp;amp;= \sum_{i=0}^2 \sum_{j=0}^2 \frac{\partial \mathcal{L}}{\partial x_{i, j}} z_{i+m&amp;rsquo;, j+n&amp;rsquo;}^{(l-1)}\\
&amp;amp;= \frac{\partial \mathcal{L}}{\partial x_{i, j}} * z_{m&amp;rsquo;, n&amp;rsquo;}^{(l-1)}.
\end{align*}&lt;/p&gt;
&lt;p&gt;\(\frac{\partial \mathcal{L}}{\partial x_{i, j}}\) represent the gradients with respect to the feature maps. To match the flipped kernel used in the forward pass, they are flipped in an opposite manner.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s train and evaluate a convolutional neural network on the OG network: LeNet5 (&lt;a href=&#34;#citeproc_bib_item_4&#34;&gt;LeCun et al. 1989&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;neural-networks-for-image-classification&#34;&gt;Neural Networks for Image Classification&lt;/h2&gt;
&lt;h3 id=&#34;ilsvrc&#34;&gt;ILSVRC&lt;/h3&gt;
&lt;p&gt;The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is the most popular image classification and object detection challenge starting in 2010. It now exists as the ILSVRC 2012-2017 challenge on &lt;a href=&#34;https://www.kaggle.com/c/imagenet-object-localization-challenge/overview/description&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Kaggle&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;alexnet&#34;&gt;AlexNet&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://code.google.com/archive/p/cuda-convnet/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://code.google.com/archive/p/cuda-convnet/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The network that arguably popuarlized deep learning by achieving a 37.5% top-1 and 17% top-5 error rate on the ILSVRC-2010 test set. This model performed significantly better than leading competitors (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Krizhevsky, Sutskever, and Hinton 2017&lt;/a&gt;).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-12_18-25-11_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;ILSVRC-2010 results reported by Krizhevsky et al.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;ILSVRC-2010 results reported by Krizhevsky et al.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This performance was based on many different insights and techniques including ReLU activations and dropout.
The authors stated in their original publication that the large capacity of the model is necessary to fully describe the diversity of objects in ImageNet.&lt;/p&gt;
&lt;h4 id=&#34;architecture&#34;&gt;Architecture&lt;/h4&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-12_18-35-38_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;AlexNet architecture (from Krizhevsky et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;AlexNet architecture (from Krizhevsky et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;AlexNet is made up of 5 convolutional layers followed by 3 fully-connected layers.
The outputs of the last layer are used as input to the softmax function.&lt;/p&gt;
&lt;p&gt;Each layer uses the ReLU activation function.&lt;/p&gt;
&lt;p&gt;\[
f(x) = \max(0, x)
\]&lt;/p&gt;
&lt;p&gt;The justification for switching to ReLU as opposed to sigmoid or tanh is the faster training times.
Experiments on smaller CNNs show that networks with ReLU reach 25% training error on CIFAR-10 six times faster than those with tanh activations.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-12_18-49-42_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Training loss over time using ReLU (solid) versus tanh (dotted) (from Krizhevsky et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Training loss over time using ReLU (solid) versus tanh (dotted) (from Krizhevsky et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Another benefit of ReLU activations is that they are less reliant on input normalization.
In a saturating activation function like tanh, large absolute values in the inputs will be clamped to either -1 or 1.
ReLU is unbounded above 0.
Networks can still train as long as some input is positive.
Local response normalization (LRN) is used after the first and second convolutional layers.&lt;/p&gt;
&lt;p&gt;The motivation behind LRN is taken from &lt;em&gt;lateral inhibition&lt;/em&gt; in neurobiology.
An overly excited neuron (one with a high response) can subdue or dampen the responses from local neighbors.
If all responses in a local region are uniformly large, which can happend since ReLU is unbounded, it will dampen them all.&lt;/p&gt;
&lt;p&gt;In practice, they showed that applying LRNs to their model reduced the top-1 and top-5 error rates by 1.4% and 1.2%, respectively.&lt;/p&gt;
&lt;h4 id=&#34;regularization&#34;&gt;Regularization&lt;/h4&gt;
&lt;p&gt;The entire network has 60 million parameters.
Even with so many parameters and training on a dataset with over 8 million images, their model overfits the training data quickly without the aid of regularization.
They employ both image translations and horizontal reflections.&lt;/p&gt;
&lt;p&gt;The use of translations is where the popular \(224 \times 224\) training size originated.
The original size of the images in the dataset is \(256 \times 256\).
To work with random translations without worrying about padding, they crop the final output to \(224 \times 224\).
The final output of the network extracts 5 \(224 \times 224\) patches from the test input and averages the network prediction made on each patch.&lt;/p&gt;
&lt;p&gt;Additionally, they alter the RGB intensities so that the network is less reliant on specific intensities and illumination for each object.
The intuition is that the identity of an object is invariant to lighting conditions.&lt;/p&gt;
&lt;p&gt;As a last form of regularization, they employ dropout in the first two fully-connected layers.&lt;/p&gt;
&lt;h4 id=&#34;training&#34;&gt;Training&lt;/h4&gt;
&lt;p&gt;They trained their model on a training set of 1.2 million images using two NVIDIA GTX 580 3GB GPUs.
They had to write their own optimized CUDA code for this since deep learning frameworks such as Tensorflow and PyTorch did not exist yet.
The training took ~6 days to pass 90 epochs.&lt;/p&gt;
&lt;h3 id=&#34;vgg&#34;&gt;VGG&lt;/h3&gt;
&lt;p&gt;Published in 2015, (&lt;a href=&#34;#citeproc_bib_item_5&#34;&gt;Simonyan and Zisserman 2015&lt;/a&gt;) explore how depth plays a role in convolutional neural networks.
They systematically increase the depth of the network while keep other hyperparameters fixed.
The filter sizes are also kept at \(3 \times 3\).&lt;/p&gt;
&lt;p&gt;Similar to (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Krizhevsky, Sutskever, and Hinton 2017&lt;/a&gt;), they use ReLU activations and in only one of their models to they employ Local Response Normalization.
They found that adding LRN to their model did not increase performance.
Instead, it only increased computation time and memory consumption.
Their models are summarized in the table below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-13_07-48-12_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Model configurations used (Simonyan and Zisserman).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Model configurations used (Simonyan and Zisserman).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The number of parameters for each network is 133 million, 133 million, 134 million, 138 million, and 144 million starting from A to E.&lt;/p&gt;
&lt;h3 id=&#34;googlenet&#34;&gt;GoogLeNet&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-14_14-18-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;The network-in-network architecture pairs perfectly with the Inception meme.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;The network-in-network architecture pairs perfectly with the Inception meme.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Proposed a 22-layer network architecture that has \(12 \times\) fewer parameters than (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Krizhevsky, Sutskever, and Hinton 2017&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The authors were already thinking about applications in mobile computing, where hardware limitations would require smaller networks that still perform well.&lt;/p&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;In this paper, we will focus on an efficient deep neural network architecture for computer vision, codenamed Inception, which derives its name from the Network in network paper by Lin et al. in conjunction with the famous “we need to go deeper” internet meme.&amp;rdquo; - Szegedy et al.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;It was apparent at the time that building larger networks would generally lead to better performance.
Adding more parameters leads to easier overfitting.
Bigger networks also mean more computation. If the goal is to adapt high quality networks into mobile computing, solutions would have to include more sophistication than simply adding more components.&lt;/p&gt;
&lt;h4 id=&#34;hebbian-learning&#34;&gt;Hebbian Learning&lt;/h4&gt;
&lt;p&gt;A linear increase in filters leads to a quadratic increase in computation.
If most filter parameters end up being close to 0, then this increase in model capacity is wasted.
One solution is to include sparsity in the network instead of having dense connections.
Szegedy et al. were motivated by the work of Arora et al., which they summarized as follows.&lt;/p&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Their main result states that if the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs.&amp;rdquo; - Szegedy et al.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;This result relates with &lt;a href=&#34;https://en.wikipedia.org/wiki/Hebbian_theory&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Hebbian theory&lt;/a&gt; on synaptic plasticity which is summarized as &amp;ldquo;neurons that fire together, wire together.&amp;rdquo;&lt;/p&gt;
&lt;h4 id=&#34;from-theory-to-architecture&#34;&gt;From Theory to Architecture&lt;/h4&gt;
&lt;p&gt;Motivated by sparse connections, the architecture is designed to approximate sparsity given current dense components like convolutional layers.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-14_14-52-12_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 11: &amp;lt;/span&amp;gt;Naive version of the Inception module (Szegedy et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;Naive version of the Inception module (Szegedy et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The Inception module design as seen above is motivated as follows.
In layers closer to the raw input, filters would be grouped into local regions.
In this case, a \(1 \times 1\) convolution would summarize these groups.&lt;/p&gt;
&lt;p&gt;For clusters that are spread out, a larger filter would be needed to cover the larger regions.
This motivates the use of \(3 \times 3\) and \(5 \times 5\) filters.&lt;/p&gt;
&lt;p&gt;The choice to include a max pooling function in each module is based on previous successes of using max pooling.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-14_15-01-46_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 12: &amp;lt;/span&amp;gt;Description of layers from Szegedy et al.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 12: &lt;/span&gt;Description of layers from Szegedy et al.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;vanishing-gradients&#34;&gt;Vanishing Gradients&lt;/h4&gt;
&lt;p&gt;Creating a deeper network means that training is more susceptible to the vanishing gradient problem.
They noted that shallower networks that perform well on image classification would surely provide strong disciminative features.
They leverage this idea by computing 2 additional intermediate outputs: one in the middle of the network and an additional output 3 layers beyond that one.
This permits the gradients to be strengthened by intermediate losses when combined with the original gradients.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-14_15-07-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 13: &amp;lt;/span&amp;gt;GoogLeNet model (Szegedy et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 13: &lt;/span&gt;GoogLeNet model (Szegedy et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;results&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;GoogLeNet took 1st place in the 2014 ILSVRC with a 6.67% top-5 error rate.&lt;/p&gt;
&lt;h3 id=&#34;resnet&#34;&gt;ResNet&lt;/h3&gt;
&lt;p&gt;By 2016, it was clear that deeper models could build a richer hierarchy of features leading to better performance on a wide range of computer vision tasks.
However, with deeper networks comes the vanishing gradient problem.
Training them remained difficult for a time, but initialization and other normalization techniques found ways to resolve this issue.&lt;/p&gt;
&lt;p&gt;With deeper networks, a new problem appeared.
Adding more layers generally results in higher accuracy.
At a certain point, adding additional layers leads to a decrease in accuracy.
Many experiments ruled out the possibility of overfitting by observing that the training error was increasing as well.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-14_15-19-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 14: &amp;lt;/span&amp;gt;Result of experiments showing that decreased accuracy was not a result of overfitting.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 14: &lt;/span&gt;Result of experiments showing that decreased accuracy was not a result of overfitting.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;identity-mappings&#34;&gt;Identity Mappings&lt;/h4&gt;
&lt;p&gt;Consider a shallow network with some measure performance on a task.
If we were to add additional layers to make this network deeper, but those layers were simply identity mappings, then we should expect an error no greater than the original shallow network.
However, current solvers are unable to find such a solution in a reasonable amount of time on an equally deep network optimized from a random initialization.&lt;/p&gt;
&lt;h4 id=&#34;residual-functions&#34;&gt;Residual Functions&lt;/h4&gt;
&lt;p&gt;The main idea of this paper is to attempt to learn a residual function \(\mathcal{F}(\mathbf{x}) := \mathcal{H}(\mathbf{x}) - \mathbf{x}\) of the desired mapping \(\mathcal{H}(\mathbf{x})\) rather than attempting to learn the mapping directly.
The desired mapping then given by \(\mathcal{H}(\mathbf{x}) = \mathcal{F}(\mathbf{x}) + \mathbf{x}\).
If it were optimal to learn an identity mapping, the idea is that it would be simpler to learn by moving towards a 0 residual.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-14_15-45-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 15: &amp;lt;/span&amp;gt;Residual unit (He et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 15: &lt;/span&gt;Residual unit (He et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The function can be implemented into neural networks by using skip connections, as seen in the figure above.
Adding these identity mappings does not require any additional parameters, as the input is simply passed to the end of the stack.&lt;/p&gt;
&lt;h4 id=&#34;architecture-complexity&#34;&gt;Architecture Complexity&lt;/h4&gt;
&lt;p&gt;They compare a 34-layer plain network based on the VGG-19 architecture with a 34-layer residual network.
They note that VGG-19 has more filters and higher complexity than their residual network.
Specifically, VGG-19 requires 19.6 billion FLOPs compared to only 3.6 billion for their 34-layer residual network.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-14_15-49-39_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 16: &amp;lt;/span&amp;gt;Comparison of architectures and their complexity (He et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 16: &lt;/span&gt;Comparison of architectures and their complexity (He et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;results&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;They evaluate how well the residual networks generalize when adding more layers.
As mentioned in the introduction, typical models would see an increase in training error as the number of layers were increased.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-04-14_15-53-18_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 17: &amp;lt;/span&amp;gt;Training comparisons between plain and residual networks (He et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 17: &lt;/span&gt;Training comparisons between plain and residual networks (He et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Their ensemble of models achieved 3.57% top-5 error on the ImageNet test set, achieving 1st place in the ILSVRC 2015 classification challenge.
It additionally was adapted to other challenges and won first place on ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in both the ILSVRC and COCO 2015 competitions.&lt;/p&gt;
&lt;h2 id=&#34;useful-resources&#34;&gt;Useful Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.songho.ca/dsp/convolution/convolution.html#convolution_2d&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.songho.ca/dsp/convolution/convolution.html#convolution_2d&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vdumoulin/conv_arithmetic&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://github.com/vdumoulin/conv_arithmetic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cs231n.github.io/convolutional-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://cs231n.github.io/convolutional-networks/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://grzegorzgwardys.wordpress.com/2016/04/22/8/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://grzegorzgwardys.wordpress.com/2016/04/22/8/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Dumoulin, Vincent, and Francesco Visin. 2018. “A Guide to Convolution Arithmetic for Deep Learning.” &lt;i&gt;Arxiv:1603.07285 [Cs, Stat]&lt;/i&gt;, January. &lt;a href=&#34;http://arxiv.org/abs/1603.07285&#34;&gt;http://arxiv.org/abs/1603.07285&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “Deep Residual Learning for Image Recognition.” In &lt;i&gt;2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/i&gt;, 770–78. Las Vegas, NV, USA: IEEE. &lt;a href=&#34;https://doi.org/10.1109/CVPR.2016.90&#34;&gt;https://doi.org/10.1109/CVPR.2016.90&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_3&#34;&gt;&lt;/a&gt;Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. “ImageNet Classification with Deep Convolutional Neural Networks.” &lt;i&gt;Communications of the Acm&lt;/i&gt; 60 (6): 84–90. &lt;a href=&#34;https://doi.org/10.1145/3065386&#34;&gt;https://doi.org/10.1145/3065386&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_4&#34;&gt;&lt;/a&gt;LeCun, Yann, Bernhard Boser, John Denker, Donnie Henderson, R. Howard, Wayne Hubbard, and Lawrence Jackel. 1989. “Handwritten Digit Recognition with a Back-Propagation Network.” In &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;. Vol. 2. Morgan-Kaufmann. &lt;a href=&#34;https://papers.nips.cc/paper/1989/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html&#34;&gt;https://papers.nips.cc/paper/1989/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_5&#34;&gt;&lt;/a&gt;Simonyan, Karen, and Andrew Zisserman. 2015. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” &lt;i&gt;Arxiv:1409.1556 [Cs]&lt;/i&gt;, April. &lt;a href=&#34;http://arxiv.org/abs/1409.1556&#34;&gt;http://arxiv.org/abs/1409.1556&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_6&#34;&gt;&lt;/a&gt;Springenberg, Jost Tobias, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. 2015. “Striving for Simplicity: The All Convolutional Net.” &lt;i&gt;Arxiv:1412.6806 [Cs]&lt;/i&gt;, April. &lt;a href=&#34;http://arxiv.org/abs/1412.6806&#34;&gt;http://arxiv.org/abs/1412.6806&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning</title>
      <link>http://localhost:1313/notes/deep_learning/</link>
      <pubDate>Tue, 29 Mar 2022 00:00:00 -0500</pubDate>
      
      <guid>http://localhost:1313/notes/deep_learning/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-makes-a-model-deep&#34;&gt;What makes a model deep?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deep-networks&#34;&gt;Deep Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deep-vs-dot-shallow-networks&#34;&gt;Deep vs. Shallow Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#high-dimensional-structured-data&#34;&gt;High Dimensional Structured Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#activation-functions&#34;&gt;Activation Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#loss-functions&#34;&gt;Loss Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-typical-training-pipeline&#34;&gt;A Typical Training Pipeline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#useful-links&#34;&gt;Useful Links&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Deep learning is a term that you&amp;rsquo;ve probably heard of a million times by now in different contexts. It is an umbrella term that encompasses techniques for computer vision, bioinformatics, natural language processing, and much more. It almost always involves a neural network of some kind that was trained on a large corpus of data.&lt;/p&gt;
&lt;p&gt;The existence of the word &amp;ldquo;deep&amp;rdquo; implies a contrast to &amp;ldquo;shallow&amp;rdquo; learning. Some definition define a deep network as an artificial neural network with more than 1 layer. Another definition is that a deep model will include a hierarchy of features that are learned from the data. These features are learned as part of the optimization process as opposed to being manually engineered as is required in other machine learning techniques.&lt;/p&gt;
&lt;p&gt;If you are not yet familiar with &lt;a href=&#34;http://localhost:1313/notes/neural_networks/&#34;&gt;neural networks&lt;/a&gt;, follow the link to learn about their basics as they are the foundation of deep learning systems.&lt;/p&gt;
&lt;p&gt;We will cover how to implement an array of deep learning models for different tasks.
Different layers and activation functions will be explored as well as the effect of regularization.
There will also be a focus on best practices for organizing a machine learning project.&lt;/p&gt;
&lt;h2 id=&#34;what-makes-a-model-deep&#34;&gt;What makes a model deep?&lt;/h2&gt;
&lt;p&gt;We begin by comparing &lt;em&gt;shallow&lt;/em&gt; networks with &lt;em&gt;deep&lt;/em&gt; networks.
What defines a deep network? Is it as simple as crossing a threshold into \(n\) layers?
As evidenced by (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Zeiler and Fergus 2013&lt;/a&gt;) deeper networks allow for a more robust hierarchy of image features.&lt;/p&gt;
&lt;p&gt;There is work by (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Montúfar et al. 2014&lt;/a&gt;) which suggests that shallow networks require an exponential amount of nodes as compared to deeper networks.
Additionally, there are many individual results which suggest that deeper networks provide better task generalization.&lt;/p&gt;
&lt;p&gt;As we will later see when studying Convolutional Neural Networks, the optimization of such deep networks produces features that maximize the performance of a task. That is, the network is not only optimizing the overall performance of task, but it produces features from the data that may be useful in other contexts.
This is particularly useful for transfer learning, where large pre-trained models can be used as starting points for novel tasks.
The benefit being that a complete retraining of the model is not necessary.&lt;/p&gt;
&lt;h2 id=&#34;deep-networks&#34;&gt;Deep Networks&lt;/h2&gt;
&lt;p&gt;Like &lt;a href=&#34;http://localhost:1313/notes/neural_networks/&#34;&gt;neural networks&lt;/a&gt;, deep networks are defined by the number of layers, nodes per layer, activation functions, and loss functions.
We now review the forward and backward pass, providing more insight into the structure and usage of deep networks along the way.&lt;/p&gt;
&lt;p&gt;Consider a deep network with \(L\) layers. Layer \(l\) has \(n_{l-1}\) input connections and \(n_l\) output nodes and activation function \(g^{(l)}\).
The final output is evaluated with some ground truth using a loss function \(\mathcal{L}\).&lt;/p&gt;
&lt;h3 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h3&gt;
&lt;p&gt;\begin{align*}
\mathbf{a}^{(l)} &amp;amp;= W^{(l)}\mathbf{z}^{(l-1)} + \mathbf{b}^{(l)}\\
\mathbf{z}^{(l)} &amp;amp;= g^{(l)}(\mathbf{a}^{(l)})\\
\end{align*}&lt;/p&gt;
&lt;p&gt;This is repeated from the input to the last layer.
For the first layer \(l=1\), the input \(\mathbf{z}^{(0)} = \mathbf{x}\).
In practice, the output \(\mathbf{a}^{(l)}\) is cached since it is required for the backward pass.
This prevents the values from needing to be computed twice.&lt;/p&gt;
&lt;p&gt;It is also worth it to study the sizes of the matrices while performing a forward pass.
For a layer \(l\), \(W^{(l)} \in \mathbb{R}^{n_l \times n_{l-1}}\) and the input \(\mathbf{z}^{(l-1)} \in \mathbb{R}^{n_{l-1} \times 1}\).
When training, it is common to perform batch gradient descent with batches of input of size \(B\).
Then, \(\mathbf{z}^{(l-1)} \in \mathbb{R}^{n_{l-1}\times B}\) and \(\mathbf{a}^{(l)}, \mathbf{b}^{(l)} \in \mathbb{R}^{n_l \times B}\).&lt;/p&gt;
&lt;h3 id=&#34;backward-pass&#34;&gt;Backward Pass&lt;/h3&gt;
&lt;p&gt;During the backward pass, the gradient is propagated from the last layer to the first.
Each layer that contains trainable parameters must also compute the gradient of the network output with respect to the weights and biases.
This can be done in a modular way, as shown next.&lt;/p&gt;
&lt;p&gt;Consider the last layer. The gradients with respect to the weights and biases are&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{d\mathcal{L}}{dW^{(L)}} &amp;amp;= \frac{d\mathcal{L}}{d\mathbf{z}^{(L)}} \frac{d\mathbf{z}^{(L)}}{d\mathbf{a}^{(L)}} \frac{d\mathbf{a}^{(L)}}{dW^{(L)}}\\
\frac{d\mathcal{L}}{d\mathbf{b}^{(L)}} &amp;amp;= \frac{d\mathcal{L}}{d\mathbf{z}^{(L)}} \frac{d\mathbf{z}^{(L)}}{d\mathbf{a}^{(L)}} \frac{d\mathbf{a}^{(L)}}{d\mathbf{b}^{(L)}}.
\end{align*}&lt;/p&gt;
&lt;p&gt;To see how the gradient continues to be propagated backward, compute the same thing for layer \(L-1\)&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{d\mathcal{L}}{dW^{(L-1)}} &amp;amp;= \frac{d\mathcal{L}}{d\mathbf{z}^{(L)}} \frac{d\mathbf{z}^{(L)}}{d\mathbf{a}^{(L)}} \frac{d\mathbf{a}^{(L)}}{d\mathbf{z}^{(L-1)}} \frac{d\mathbf{z}^{(L-1)}}{d\mathbf{a}^{(L-1)}} \frac{d\mathbf{a}^{(L-1)}}{dW^{(L-1)}}\\
\frac{d\mathcal{L}}{d\mathbf{b}^{(L-1)}} &amp;amp;= \frac{d\mathcal{L}}{d\mathbf{z}^{(L)}} \frac{d\mathbf{z}^{(L)}}{d\mathbf{a}^{(L)}} \frac{d\mathbf{a}^{(L)}}{d\mathbf{z}^{(L-1)}} \frac{d\mathbf{z}^{(L-1)}}{d\mathbf{a}^{(L-1)}} \frac{d\mathbf{a}^{(L-1)}}{d\mathbf{b}^{(L-1)}}.
\end{align*}&lt;/p&gt;
&lt;p&gt;As seen above, to continue propagating the gradient backward, each layer \(l\) must also compute&lt;/p&gt;
&lt;p&gt;\[
\frac{d\mathbf{a}^{(l)}}{d\mathbf{z}^{(l-1)}}.
\]&lt;/p&gt;
&lt;p&gt;To summarize, every layer with trainable parameters will compute&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{d\mathcal{L}}{dW^{(l)}} = \frac{d\mathbf{a}^{(l+1)}}{d\mathbf{z}^{(l)}} \frac{d\mathbf{z}^{(l)}}{d\mathbf{a}^{(l)}} \frac{d\mathbf{a}^{(l)}}{dW^{(l)}}\\
\frac{d\mathcal{L}}{d\mathbf{b}^{(l)}} = \frac{d\mathbf{a}^{(l+1)}}{d\mathbf{z}^{(l)}} \frac{d\mathbf{z}^{(l)}}{d\mathbf{a}^{(l)}} \frac{d\mathbf{a}^{(l)}}{d\mathbf{b}^{(l)}}.
\end{align*}&lt;/p&gt;
&lt;p&gt;The term \(\frac{d\mathbf{a}^{(l+1)}}{d\mathbf{z}^{(l)}}\) is the gradient that is propagated from layer \(l+1\).&lt;/p&gt;
&lt;h2 id=&#34;deep-vs-dot-shallow-networks&#34;&gt;Deep vs. Shallow Networks&lt;/h2&gt;
&lt;p&gt;As mentioned above, a shallow network can approximate any continuous function to arbitrary precision. If a deep network can represent the composition of two shallow networks, then it can also approximate any continuous function to arbitrary precision. Then why are deep networks better than shallow networks when both can approximate any function? There are a few compelling reasons as to why, starting with the &lt;strong&gt;capacity&lt;/strong&gt; of the network and the number of linear regions it can represent per parameter.&lt;/p&gt;
&lt;p&gt;As discussed in &lt;em&gt;Understanding Deep Learning&lt;/em&gt; (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Prince 2023&lt;/a&gt;), a shallow network with 1 input, 1 output, and \(D &amp;gt; 2\) hidden units can create up to \(D + 1\) linear regions using \(3D+1\) parameters. The \(3D + 1\) comes from the fact that the hidden layer requires \(D\) parameters for the weights with an extra \(D\) parameters for the bias terms. To convert from the hidden layer to the output layer, there are \(D\) parameters for the weights and 1 parameter for the bias term. The figure below shows the maximum number of linear regions as a function of the number of parameters for networks that map one input to one output.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2023-10-22_21-30-14_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Maximum number of linear regions as a function of the number of parameters for networks that map one input to one output (&amp;lt;a href=&amp;#34;#citeproc_bib_item_2&amp;#34;&amp;gt;Prince 2023&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Maximum number of linear regions as a function of the number of parameters for networks that map one input to one output (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Prince 2023&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;high-dimensional-structured-data&#34;&gt;High Dimensional Structured Data&lt;/h2&gt;
&lt;p&gt;For high dimensional structured data, such as images, deep networks are able to learn a hierarchy of features that are useful for the task at hand while requiring a significantly smaller number of parameters than a shallow network. Consider a \(100\times100\) image used as input to a shallow network with 1 hidden layer. This would require \(10,001\) parameters to represent the weights and biases. If we instead use a deep network with with convolutional layers, we can use significantly fewer parameters. We will see this more closely when we study &lt;a href=&#34;http://localhost:1313/notes/convolutional_neural_networks/&#34;&gt;Convolutional Neural Networks&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;activation-functions&#34;&gt;Activation Functions&lt;/h2&gt;
&lt;h3 id=&#34;sigmoid&#34;&gt;Sigmoid&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\[
\sigma(\mathbf{x}) = \frac{1}{1 + \exp(-\mathbf{x})}
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Derivative&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\[
\sigma(\mathbf{x})(1 - \sigma(\mathbf{x}))
\]&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;http://localhost:1313/ox-hugo/2022-03-31_10-04-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Sigmoid non-linearity (Wikipedia)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Sigmoid non-linearity (Wikipedia)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;loss-functions&#34;&gt;Loss Functions&lt;/h2&gt;
&lt;p&gt;Loss functions are used to evaluate the performance of a model. In the context of gradient descent, their gradient with respect to the model parameters is used to update the parameters. Loss functions can be constructed using maximum likelihood estimation over a probability distribution or by using a distance metric between the model output and the ground truth. The table below from (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Prince 2023&lt;/a&gt;) shows some common loss functions and their use cases.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Data Type&lt;/th&gt;
&lt;th&gt;Domain&lt;/th&gt;
&lt;th&gt;Distribution&lt;/th&gt;
&lt;th&gt;Use&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Univariate, continuous, unbounded&lt;/td&gt;
&lt;td&gt;\(\mathbb{R}\)&lt;/td&gt;
&lt;td&gt;univariate normal&lt;/td&gt;
&lt;td&gt;Regression&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Univariate, discrete, binary&lt;/td&gt;
&lt;td&gt;\(\{0, 1\}\)&lt;/td&gt;
&lt;td&gt;Bernoulli&lt;/td&gt;
&lt;td&gt;Binary Classification&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Univariate, discrete, bounded&lt;/td&gt;
&lt;td&gt;\(\{0, 1\}^K\)&lt;/td&gt;
&lt;td&gt;Multinoulli&lt;/td&gt;
&lt;td&gt;Multiclass Classification&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;a-typical-training-pipeline&#34;&gt;A Typical Training Pipeline&lt;/h2&gt;
&lt;p&gt;When training and evaluating models, especially on benchmark datasets, it is important to properly test their generalization performance.
This test is crucial when comparing the efficacy of your ideas versus baseline evaluations or competing methods.&lt;/p&gt;
&lt;p&gt;To ensure that your model is evaluated in a fair way, it is common to set aside a set of test data that is only used during the final comparison.
This data is typically annotated so that some metric can be used.&lt;/p&gt;
&lt;p&gt;It is true that the training data drives the parameter tuning during optimization.
This is most commonly done with gradient descent.
However, we will also change the hyperparamers such as learning rate, batch size, and data augmentation.
In this case, we want to evaluate the relative performance of each change.&lt;/p&gt;
&lt;p&gt;If we use the test set to do this, then we are necessarily using the test set for training.
Our biases and intuitions about the model&amp;rsquo;s performance would be implicitly influenced by that set.
To track our relative changes without using the test set, we can take a portion of the original training set and label it as our &lt;strong&gt;validation set&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The split between training, validation, and test data is relatively small.
Most modern datasets are large, with millions of samples.
Consider &lt;a href=&#34;https://www.image-net.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;ImageNet&lt;/a&gt;, an image classification dataset with over 14 million samples.
Taking 10,000 samples to serve as a validation set is only \(~.07\%\) of the dataset.&lt;/p&gt;
&lt;p&gt;Most modern machine learning frameworks have an easy way to split the dataset.
We can do this in PyTorch using &lt;code&gt;torch.utils.data.random_split&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;train_dataset, val_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;utils&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random_split(dataset, [train_size, val_size])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;useful-links&#34;&gt;Useful Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=D_jt-xO_RmI&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Deep learning Bootcamp: Kaiming He (author of ResNet)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Montúfar, Guido, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. 2014. “On the Number of Linear Regions of Deep Neural Networks.” &lt;i&gt;Arxiv:1402.1869 [Cs, Stat]&lt;/i&gt;, June. &lt;a href=&#34;http://arxiv.org/abs/1402.1869&#34;&gt;http://arxiv.org/abs/1402.1869&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Prince, Simon J.D. 2023. &lt;i&gt;Understanding Deep Learning&lt;/i&gt;. MIT Press. &lt;a href=&#34;https://udlbook.github.io/udlbook/&#34;&gt;https://udlbook.github.io/udlbook/&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_3&#34;&gt;&lt;/a&gt;Zeiler, Matthew D., and Rob Fergus. 2013. “Visualizing and Understanding Convolutional Networks.” &lt;i&gt;Arxiv:1311.2901 [Cs]&lt;/i&gt;, November. &lt;a href=&#34;http://arxiv.org/abs/1311.2901&#34;&gt;http://arxiv.org/abs/1311.2901&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
