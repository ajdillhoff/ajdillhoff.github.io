<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>computer vision on Alex Dillhoff</title>
    <link>https://ajdillhoff.github.io/tags/computer-vision/</link>
    <description>Recent content in computer vision on Alex Dillhoff</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; {year}</copyright>
    <lastBuildDate>Sat, 02 Apr 2022 00:00:00 -0500</lastBuildDate>
    
	    <atom:link href="https://ajdillhoff.github.io/tags/computer-vision/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Convolutional Neural Networks</title>
      <link>https://ajdillhoff.github.io/notes/convolutional_neural_networks/</link>
      <pubDate>Sat, 02 Apr 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/convolutional_neural_networks/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#convolution-operator&#34;&gt;Convolution Operator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#properties-of-convolutions&#34;&gt;Properties of Convolutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parameter-sharing&#34;&gt;Parameter Sharing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pooling&#34;&gt;Pooling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backwards-pass&#34;&gt;Backwards Pass&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example&#34;&gt;Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#neural-networks-for-image-classification&#34;&gt;Neural Networks for Image Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#useful-resources&#34;&gt;Useful Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;&lt;strong&gt;Key Concepts&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Invariance and Equivariance&lt;/li&gt;
&lt;li&gt;Definition&lt;/li&gt;
&lt;li&gt;Padding, Stride, Kernel size, dilation&lt;/li&gt;
&lt;li&gt;Purpose of multiple feature maps&lt;/li&gt;
&lt;li&gt;Receptive fields and hierarchies of features&lt;/li&gt;
&lt;li&gt;Downsampling, Upsampling, Examples in research&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Dense neural networks made up of linear layers and a chosen activation function are not practical for image data. Consider an image of size \(224\times224\times3\). The first layer of a dense network would require a \(150,528\times n\) parameter matrix, where \(n\) is the number of nodes in the first layer. It is common to build dense networks where the first layer has more nodes than input features. In this case, we would need a minimum of \(150,528^2\) parameters in the first layer. Even if we chose something much smaller like \(n=1024\), this would require \(154,140,672\) parameters for just the first layer. This is clearly impractical.&lt;/p&gt;
&lt;p&gt;Aside from requiring a large number of parameters, we might ask whether it is beneficial to feed raw pixel values into a dense network. The network itself would be learning pixel-wise features with no regard to their spatial relationship. This makes our network&amp;rsquo;s job much more difficult because the spatial arrangement of features tells us so much about what we see. In practice, this means that the network would have to learn the same features at every location in the image. We would instead prefer this network to learn features that are &lt;strong&gt;invariant&lt;/strong&gt; to translation. That is, the network should learn features that are the same regardless of where they appear in the image.&lt;/p&gt;
&lt;p&gt;Invariance to translation is very convenient and can save our network a lot of work in learning the same feature at every point in the input. It is also desirable that our network is invariant to other transformations such as rotation, scaling, skewing, and warping. Formally, a function \(f(\mathbf{x})\) of an image \(\mathbf{x}\) is invariant to a transformation \(t(\mathbf{x})\) if&lt;/p&gt;
&lt;p&gt;\[
f(t(\mathbf{x})) = f(\mathbf{x}).
\]&lt;/p&gt;
&lt;p&gt;Aside from invariance, some models should be &lt;strong&gt;equivariant&lt;/strong&gt; to certain transformations. That is, the output of the model should change in the same way as the input. Image segmentation models should be equivariant to translation. If we were to shift an image by a few pixels, the output segmentation mask should also shift by the same amount. Convolutional neural networks are equivariant to &lt;em&gt;translation&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;convolution-operator&#34;&gt;Convolution Operator&lt;/h2&gt;
&lt;p&gt;A convolution is a function that takes two functions as input and produces a third function as output. The first function is the input and the second function is the kernel. The output is called the feature map. The kernel is also sometimes called the filter.&lt;/p&gt;
&lt;p&gt;\[
(f * g)(t) = \int f(t-a)g(a)da
\]&lt;/p&gt;
&lt;p&gt;We can view them more concretely by considering the functions to be vectors. For example, let the function \(f\) be an input vector \(x\) and \(w\) be a kernel representing a filter. The convolution operator is then&lt;/p&gt;
&lt;p&gt;\[
(x * w)(t) = \int x(t-a)w(a)da.
\]&lt;/p&gt;
&lt;p&gt;The result the &lt;strong&gt;feature map&lt;/strong&gt; representing the response of the kernel at each location in the input.&lt;/p&gt;
&lt;p&gt;In the case of discrete values, the operator is written as&lt;/p&gt;
&lt;p&gt;\[
(x * w)(t) = \sum_{a}x(t-a)w(a).
\]&lt;/p&gt;
&lt;p&gt;In machine learning, the kernel \(w\) is usually represented by some set of parameters that is optimized.&lt;/p&gt;
&lt;p&gt;CNNs for images use a 2D convolution defined as&lt;/p&gt;
&lt;p&gt;\[
(I * K)(i, j) = \sum_m \sum_n I(i-m, j-n)K(m, n).
\]&lt;/p&gt;
&lt;p&gt;In this formulation, the kernel is effectively flipped across the vertical and horizontal axis.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-05_19-09-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;2D Convolution (Image Credit: Song Ho Ahn (linked above)).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;2D Convolution (Image Credit: Song Ho Ahn (linked above)).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In practice, most deep learning APIs implement &lt;strong&gt;cross-correlation&lt;/strong&gt;.
Whether the function is implemented as true convolution makes no difference when it comes to optimizing a deep model since filter weights that are produced with cross-correlation would be produced, albeit flipped, with convolution.&lt;/p&gt;
&lt;p&gt;\[
(K * I)(i, j) = \sum_m \sum_n I(i+m, j+n)K(m, n).
\]&lt;/p&gt;
&lt;h2 id=&#34;properties-of-convolutions&#34;&gt;Properties of Convolutions&lt;/h2&gt;
&lt;p&gt;Convolutional networks are commonly built on &lt;em&gt;full&lt;/em&gt; or &lt;em&gt;valid&lt;/em&gt; convolutions. Other variants have also been explored. Here, we will briefly discuss the different properties of this operator. A more detailed treatment can be found in (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;padding&#34;&gt;Padding&lt;/h3&gt;
&lt;p&gt;By definition, a convolution of an input with a filter of size \(n\times n\) will produce an output of size \((m-n+1)\times(m-n+1)\), where \(m\) is the size of the input. This means that the output will be smaller than the input. This is often referred to as a &lt;strong&gt;valid&lt;/strong&gt; convolution. The figure below shows a convolution between a \(3\times3\) kernel and a \(5\times5\) input.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-26_16-31-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A valid convolution (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A valid convolution (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The output of this convolution is a \(3\times3\) feature map. This is a problem if we want to build a deep network. Each convolution will reduce the size of the input. If we were to stack multiple convolutional layers, the output would eventually be too small to be useful. If we want our output to be same size as the input, we can add padding to the original input image before convolving it. This is often known as a &lt;strong&gt;full&lt;/strong&gt; convolution. An example is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-26_16-34-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;A full convolution (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;A full convolution (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;stride&#34;&gt;Stride&lt;/h3&gt;
&lt;p&gt;So far, we have only looked at convolutions which step by 1 unit as they shift over the image. We can control the size of this step, or &lt;strong&gt;stride&lt;/strong&gt;, to produce different outcomes. Picking a non-unit stride has a number of effects on the features that are learned in a convolutional neural network.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dimensionality reduction&lt;/strong&gt;: Skipping over pixels reduces the size of the output feature map. This provides another way of downsampling the input.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Less computation&lt;/strong&gt;: Fewer computations are required to produce the output feature map.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Increased field of view&lt;/strong&gt;: A larger stride increases the field of view of the kernel, leading to larger receptive fields in deeper layers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given an input of size \(m\times m\) and a kernel of size \(n\times n\), the output size of a convolution with stride \(s\) is given by&lt;/p&gt;
&lt;p&gt;\[
\left\lfloor\frac{m-n}{s}\right\rfloor + 1.
\]&lt;/p&gt;
&lt;p&gt;The figure below shows a convolution with stride 2 on a \(5\times5\) input.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-26_16-45-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;A convolution with stride 2 (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;A convolution with stride 2 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;kernel-size&#34;&gt;Kernel Size&lt;/h3&gt;
&lt;p&gt;The size of the kernel has a large impact on the features that are learned. A larger kernel will have a larger receptive field. This means that the kernel will be able to capture more information about the input. However, this comes at the cost of increased computation. Common kernel sizes in most CNNs are \(3\times3\), \(5\times5\), and \(7\times7\). It is also convenient to pick an odd kernel size so that the kernel has a center pixel.&lt;/p&gt;
&lt;h3 id=&#34;dilation&#34;&gt;Dilation&lt;/h3&gt;
&lt;p&gt;Around 2015, a research trend for CNNs was to find a way to increase the receptive field without adding more parameters. The result is a &lt;strong&gt;dilated&lt;/strong&gt; convolution. The output of a dilated convolution is computed by skipping over pixels in the input. The figure below shows a \(3\times3\) kernel with a dilation of 2.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-27_08-19-10_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;A dilated convolution (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;A dilated convolution (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The output size is computed as&lt;/p&gt;
&lt;p&gt;\[
\left\lfloor\frac{m + 2p - n - (n-1)(d-1)}{s}\right\rfloor + 1,
\]&lt;/p&gt;
&lt;p&gt;where \(p\) is the amount of padding and \(d\) is the dilation factor.&lt;/p&gt;
&lt;h2 id=&#34;parameter-sharing&#34;&gt;Parameter Sharing&lt;/h2&gt;
&lt;p&gt;In a densely connected layer, each input has a corresponding weight attached to it.
For example, we ran a few &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/tree/main/deep_learning&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;introductory experiments&lt;/a&gt; on the CIFAR10 dataset using a deep, densely connected network.
To reduce the amount of parameters in the first layer, we converted each image to grayscale.
The input also had to be vectorized in order to be processed.
With a processed image of size \(32 \times 32\), this resulted in a \(1024\) dimensional vector for each input.
Our first layer had \(512\) nodes resulting in a parameter matrix of size \(1024 \times 512\).&lt;/p&gt;
&lt;p&gt;Convolution layers have &lt;strong&gt;shared parameters&lt;/strong&gt;, meaning the same parameters are used for each region on the input.
A single channel 2D filter of size \(n \times n\) only requires \(n \times n\) parameters.
Each kernel is applied to every location in the original input using the same parameters.&lt;/p&gt;
&lt;p&gt;Kernels are &lt;strong&gt;equivariant&lt;/strong&gt; to translation because of their shared parameters.
That is, as the input changes, the output will change in the same way.
Formally, two functions \(f\) and \(g\) are equivarient if&lt;/p&gt;
&lt;p&gt;\[
f(g(x)) = g(f(x)).
\]&lt;/p&gt;
&lt;p&gt;In the context of image features, a kernel applied across an image will produce strong responses in regions that exhibit the same local features.
For example, a kernel that detects horizontal lines will produce strong responses across all parts of the image that show a large contrast between vertical pixels.&lt;/p&gt;
&lt;h2 id=&#34;pooling&#34;&gt;Pooling&lt;/h2&gt;
&lt;p&gt;When a convolution is applied to some input image, the resulting output feature map represents the responses of the kernel applied to each location in the image.
If this original image were to be shifted by a few pixels, the reponses would also be shifted.
In order to increase the robustness of a model to small perturbations such as translation, a pooling layer was historically employed after each non-linear activation following a convolutional layer.&lt;/p&gt;
&lt;p&gt;They effectively provide a summary statistic of a local region by selecting the average or maximum responses in a small window. This provides translation invariance since the maximum response will be the same for a region even if it is translated by a small amount.
It also acts as a quick way to downsample the image, leading to fewer parameters in the model.&lt;/p&gt;
&lt;p&gt;Modern works do not employ pooling operations as often. For example (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;He et al. 2016&lt;/a&gt;) perform dimensionality reduction with \(1 \times 1\) convolutions.
(&lt;a href=&#34;#citeproc_bib_item_6&#34;&gt;Springenberg et al. 2015&lt;/a&gt;) argue that fully convolutional networks can achieve the same performance without max pooling.&lt;/p&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster.&amp;rdquo; - Geoffrey Hinton&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;h2 id=&#34;backwards-pass&#34;&gt;Backwards Pass&lt;/h2&gt;
&lt;p&gt;The parameters of a convolutional layer are updated via backpropagation like any other layer with trainable parameters.
Given a kernel \(w\), it is necessary to compute \(\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}}\), where \(w_{m&amp;rsquo;, n&amp;rsquo;}\) is the \((m&amp;rsquo;, n&amp;rsquo;)th\) entry of the kernel.
This entry affects all entries in the feature map, so \(\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}}\) will sum over all such entries.&lt;/p&gt;
&lt;p&gt;To show the gradient calculation, we will assume a convolutional layer with zero padding and unit stride with a square \(2 \times 2\) kernel applied to a square \(3 \times 3\) input.
The output map is then \((3 - 2 + 1) \times (3 - 2 + 1) = 2 \times 2\).&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}} = \sum_{i=0}^2 \sum_{j=0}^2 \frac{\partial \mathcal{L}}{\partial x_{i, j}} \frac{\partial x_{i,j}}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}}
\]&lt;/p&gt;
&lt;p&gt;If \(\mathbf{z}^{(l-1)}\) is the output from the previous layer, then&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{\partial x_{i, j}}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}} &amp;amp;= \frac{\partial}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}} \sum_{m} \sum_{n} w_{m, n} z_{i+m, j+n}^{(l-1)} + b\\
&amp;amp;= \frac{\partial}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}} w_{m&amp;rsquo;, n&amp;rsquo;}z_{i+m&amp;rsquo;, j+n&amp;rsquo;}^{(l-1)}\\
&amp;amp;= z_{i+m&amp;rsquo;, j+n&amp;rsquo;}^{(l-1)}
\end{align*}&lt;/p&gt;
&lt;p&gt;Then \(\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}}\) becomes&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}} &amp;amp;= \sum_{i=0}^2 \sum_{j=0}^2 \frac{\partial \mathcal{L}}{\partial x_{i, j}} z_{i+m&amp;rsquo;, j+n&amp;rsquo;}^{(l-1)}\\
&amp;amp;= \frac{\partial \mathcal{L}}{\partial x_{i, j}} * z_{m&amp;rsquo;, n&amp;rsquo;}^{(l-1)}.
\end{align*}&lt;/p&gt;
&lt;p&gt;\(\frac{\partial \mathcal{L}}{\partial x_{i, j}}\) represent the gradients with respect to the feature maps. To match the flipped kernel used in the forward pass, they are flipped in an opposite manner.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s train and evaluate a convolutional neural network on the OG network: LeNet5 (&lt;a href=&#34;#citeproc_bib_item_4&#34;&gt;LeCun et al. 1989&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;neural-networks-for-image-classification&#34;&gt;Neural Networks for Image Classification&lt;/h2&gt;
&lt;h3 id=&#34;ilsvrc&#34;&gt;ILSVRC&lt;/h3&gt;
&lt;p&gt;The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is the most popular image classification and object detection challenge starting in 2010. It now exists as the ILSVRC 2012-2017 challenge on &lt;a href=&#34;https://www.kaggle.com/c/imagenet-object-localization-challenge/overview/description&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Kaggle&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;alexnet&#34;&gt;AlexNet&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://code.google.com/archive/p/cuda-convnet/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://code.google.com/archive/p/cuda-convnet/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The network that arguably popuarlized deep learning by achieving a 37.5% top-1 and 17% top-5 error rate on the ILSVRC-2010 test set. This model performed significantly better than leading competitors (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Krizhevsky, Sutskever, and Hinton 2017&lt;/a&gt;).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-12_18-25-11_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;ILSVRC-2010 results reported by Krizhevsky et al.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;ILSVRC-2010 results reported by Krizhevsky et al.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This performance was based on many different insights and techniques including ReLU activations and dropout.
The authors stated in their original publication that the large capacity of the model is necessary to fully describe the diversity of objects in ImageNet.&lt;/p&gt;
&lt;h4 id=&#34;architecture&#34;&gt;Architecture&lt;/h4&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-12_18-35-38_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;AlexNet architecture (from Krizhevsky et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;AlexNet architecture (from Krizhevsky et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;AlexNet is made up of 5 convolutional layers followed by 3 fully-connected layers.
The outputs of the last layer are used as input to the softmax function.&lt;/p&gt;
&lt;p&gt;Each layer uses the ReLU activation function.&lt;/p&gt;
&lt;p&gt;\[
f(x) = \max(0, x)
\]&lt;/p&gt;
&lt;p&gt;The justification for switching to ReLU as opposed to sigmoid or tanh is the faster training times.
Experiments on smaller CNNs show that networks with ReLU reach 25% training error on CIFAR-10 six times faster than those with tanh activations.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-12_18-49-42_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Training loss over time using ReLU (solid) versus tanh (dotted) (from Krizhevsky et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Training loss over time using ReLU (solid) versus tanh (dotted) (from Krizhevsky et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Another benefit of ReLU activations is that they are less reliant on input normalization.
In a saturating activation function like tanh, large absolute values in the inputs will be clamped to either -1 or 1.
ReLU is unbounded above 0.
Networks can still train as long as some input is positive.
Local response normalization (LRN) is used after the first and second convolutional layers.&lt;/p&gt;
&lt;p&gt;The motivation behind LRN is taken from &lt;em&gt;lateral inhibition&lt;/em&gt; in neurobiology.
An overly excited neuron (one with a high response) can subdue or dampen the responses from local neighbors.
If all responses in a local region are uniformly large, which can happend since ReLU is unbounded, it will dampen them all.&lt;/p&gt;
&lt;p&gt;In practice, they showed that applying LRNs to their model reduced the top-1 and top-5 error rates by 1.4% and 1.2%, respectively.&lt;/p&gt;
&lt;h4 id=&#34;regularization&#34;&gt;Regularization&lt;/h4&gt;
&lt;p&gt;The entire network has 60 million parameters.
Even with so many parameters and training on a dataset with over 8 million images, their model overfits the training data quickly without the aid of regularization.
They employ both image translations and horizontal reflections.&lt;/p&gt;
&lt;p&gt;The use of translations is where the popular \(224 \times 224\) training size originated.
The original size of the images in the dataset is \(256 \times 256\).
To work with random translations without worrying about padding, they crop the final output to \(224 \times 224\).
The final output of the network extracts 5 \(224 \times 224\) patches from the test input and averages the network prediction made on each patch.&lt;/p&gt;
&lt;p&gt;Additionally, they alter the RGB intensities so that the network is less reliant on specific intensities and illumination for each object.
The intuition is that the identity of an object is invariant to lighting conditions.&lt;/p&gt;
&lt;p&gt;As a last form of regularization, they employ dropout in the first two fully-connected layers.&lt;/p&gt;
&lt;h4 id=&#34;training&#34;&gt;Training&lt;/h4&gt;
&lt;p&gt;They trained their model on a training set of 1.2 million images using two NVIDIA GTX 580 3GB GPUs.
They had to write their own optimized CUDA code for this since deep learning frameworks such as Tensorflow and PyTorch did not exist yet.
The training took ~6 days to pass 90 epochs.&lt;/p&gt;
&lt;h3 id=&#34;vgg&#34;&gt;VGG&lt;/h3&gt;
&lt;p&gt;Published in 2015, (&lt;a href=&#34;#citeproc_bib_item_5&#34;&gt;Simonyan and Zisserman 2015&lt;/a&gt;) explore how depth plays a role in convolutional neural networks.
They systematically increase the depth of the network while keep other hyperparameters fixed.
The filter sizes are also kept at \(3 \times 3\).&lt;/p&gt;
&lt;p&gt;Similar to (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Krizhevsky, Sutskever, and Hinton 2017&lt;/a&gt;), they use ReLU activations and in only one of their models to they employ Local Response Normalization.
They found that adding LRN to their model did not increase performance.
Instead, it only increased computation time and memory consumption.
Their models are summarized in the table below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-13_07-48-12_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Model configurations used (Simonyan and Zisserman).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Model configurations used (Simonyan and Zisserman).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The number of parameters for each network is 133 million, 133 million, 134 million, 138 million, and 144 million starting from A to E.&lt;/p&gt;
&lt;h3 id=&#34;googlenet&#34;&gt;GoogLeNet&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_14-18-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;The network-in-network architecture pairs perfectly with the Inception meme.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;The network-in-network architecture pairs perfectly with the Inception meme.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Proposed a 22-layer network architecture that has \(12 \times\) fewer parameters than (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Krizhevsky, Sutskever, and Hinton 2017&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The authors were already thinking about applications in mobile computing, where hardware limitations would require smaller networks that still perform well.&lt;/p&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;In this paper, we will focus on an efficient deep neural network architecture for computer vision, codenamed Inception, which derives its name from the Network in network paper by Lin et al. in conjunction with the famous “we need to go deeper” internet meme.&amp;rdquo; - Szegedy et al.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;It was apparent at the time that building larger networks would generally lead to better performance.
Adding more parameters leads to easier overfitting.
Bigger networks also mean more computation. If the goal is to adapt high quality networks into mobile computing, solutions would have to include more sophistication than simply adding more components.&lt;/p&gt;
&lt;h4 id=&#34;hebbian-learning&#34;&gt;Hebbian Learning&lt;/h4&gt;
&lt;p&gt;A linear increase in filters leads to a quadratic increase in computation.
If most filter parameters end up being close to 0, then this increase in model capacity is wasted.
One solution is to include sparsity in the network instead of having dense connections.
Szegedy et al. were motivated by the work of Arora et al., which they summarized as follows.&lt;/p&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Their main result states that if the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs.&amp;rdquo; - Szegedy et al.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;This result relates with &lt;a href=&#34;https://en.wikipedia.org/wiki/Hebbian_theory&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Hebbian theory&lt;/a&gt; on synaptic plasticity which is summarized as &amp;ldquo;neurons that fire together, wire together.&amp;rdquo;&lt;/p&gt;
&lt;h4 id=&#34;from-theory-to-architecture&#34;&gt;From Theory to Architecture&lt;/h4&gt;
&lt;p&gt;Motivated by sparse connections, the architecture is designed to approximate sparsity given current dense components like convolutional layers.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_14-52-12_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 11: &amp;lt;/span&amp;gt;Naive version of the Inception module (Szegedy et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;Naive version of the Inception module (Szegedy et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The Inception module design as seen above is motivated as follows.
In layers closer to the raw input, filters would be grouped into local regions.
In this case, a \(1 \times 1\) convolution would summarize these groups.&lt;/p&gt;
&lt;p&gt;For clusters that are spread out, a larger filter would be needed to cover the larger regions.
This motivates the use of \(3 \times 3\) and \(5 \times 5\) filters.&lt;/p&gt;
&lt;p&gt;The choice to include a max pooling function in each module is based on previous successes of using max pooling.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-01-46_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 12: &amp;lt;/span&amp;gt;Description of layers from Szegedy et al.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 12: &lt;/span&gt;Description of layers from Szegedy et al.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;vanishing-gradients&#34;&gt;Vanishing Gradients&lt;/h4&gt;
&lt;p&gt;Creating a deeper network means that training is more susceptible to the vanishing gradient problem.
They noted that shallower networks that perform well on image classification would surely provide strong disciminative features.
They leverage this idea by computing 2 additional intermediate outputs: one in the middle of the network and an additional output 3 layers beyond that one.
This permits the gradients to be strengthened by intermediate losses when combined with the original gradients.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-07-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 13: &amp;lt;/span&amp;gt;GoogLeNet model (Szegedy et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 13: &lt;/span&gt;GoogLeNet model (Szegedy et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;results&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;GoogLeNet took 1st place in the 2014 ILSVRC with a 6.67% top-5 error rate.&lt;/p&gt;
&lt;h3 id=&#34;resnet&#34;&gt;ResNet&lt;/h3&gt;
&lt;p&gt;By 2016, it was clear that deeper models could build a richer hierarchy of features leading to better performance on a wide range of computer vision tasks.
However, with deeper networks comes the vanishing gradient problem.
Training them remained difficult for a time, but initialization and other normalization techniques found ways to resolve this issue.&lt;/p&gt;
&lt;p&gt;With deeper networks, a new problem appeared.
Adding more layers generally results in higher accuracy.
At a certain point, adding additional layers leads to a decrease in accuracy.
Many experiments ruled out the possibility of overfitting by observing that the training error was increasing as well.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-19-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 14: &amp;lt;/span&amp;gt;Result of experiments showing that decreased accuracy was not a result of overfitting.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 14: &lt;/span&gt;Result of experiments showing that decreased accuracy was not a result of overfitting.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;identity-mappings&#34;&gt;Identity Mappings&lt;/h4&gt;
&lt;p&gt;Consider a shallow network with some measure performance on a task.
If we were to add additional layers to make this network deeper, but those layers were simply identity mappings, then we should expect an error no greater than the original shallow network.
However, current solvers are unable to find such a solution in a reasonable amount of time on an equally deep network optimized from a random initialization.&lt;/p&gt;
&lt;h4 id=&#34;residual-functions&#34;&gt;Residual Functions&lt;/h4&gt;
&lt;p&gt;The main idea of this paper is to attempt to learn a residual function \(\mathcal{F}(\mathbf{x}) := \mathcal{H}(\mathbf{x}) - \mathbf{x}\) of the desired mapping \(\mathcal{H}(\mathbf{x})\) rather than attempting to learn the mapping directly.
The desired mapping then given by \(\mathcal{H}(\mathbf{x}) = \mathcal{F}(\mathbf{x}) + \mathbf{x}\).
If it were optimal to learn an identity mapping, the idea is that it would be simpler to learn by moving towards a 0 residual.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-45-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 15: &amp;lt;/span&amp;gt;Residual unit (He et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 15: &lt;/span&gt;Residual unit (He et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The function can be implemented into neural networks by using skip connections, as seen in the figure above.
Adding these identity mappings does not require any additional parameters, as the input is simply passed to the end of the stack.&lt;/p&gt;
&lt;h4 id=&#34;architecture-complexity&#34;&gt;Architecture Complexity&lt;/h4&gt;
&lt;p&gt;They compare a 34-layer plain network based on the VGG-19 architecture with a 34-layer residual network.
They note that VGG-19 has more filters and higher complexity than their residual network.
Specifically, VGG-19 requires 19.6 billion FLOPs compared to only 3.6 billion for their 34-layer residual network.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-49-39_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 16: &amp;lt;/span&amp;gt;Comparison of architectures and their complexity (He et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 16: &lt;/span&gt;Comparison of architectures and their complexity (He et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;results&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;They evaluate how well the residual networks generalize when adding more layers.
As mentioned in the introduction, typical models would see an increase in training error as the number of layers were increased.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-53-18_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 17: &amp;lt;/span&amp;gt;Training comparisons between plain and residual networks (He et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 17: &lt;/span&gt;Training comparisons between plain and residual networks (He et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Their ensemble of models achieved 3.57% top-5 error on the ImageNet test set, achieving 1st place in the ILSVRC 2015 classification challenge.
It additionally was adapted to other challenges and won first place on ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in both the ILSVRC and COCO 2015 competitions.&lt;/p&gt;
&lt;h2 id=&#34;useful-resources&#34;&gt;Useful Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.songho.ca/dsp/convolution/convolution.html#convolution_2d&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.songho.ca/dsp/convolution/convolution.html#convolution_2d&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vdumoulin/conv_arithmetic&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://github.com/vdumoulin/conv_arithmetic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cs231n.github.io/convolutional-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://cs231n.github.io/convolutional-networks/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://grzegorzgwardys.wordpress.com/2016/04/22/8/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://grzegorzgwardys.wordpress.com/2016/04/22/8/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Dumoulin, Vincent, and Francesco Visin. 2018. “A Guide to Convolution Arithmetic for Deep Learning.” &lt;i&gt;Arxiv:1603.07285 [Cs, Stat]&lt;/i&gt;, January. &lt;a href=&#34;http://arxiv.org/abs/1603.07285&#34;&gt;http://arxiv.org/abs/1603.07285&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “Deep Residual Learning for Image Recognition.” In &lt;i&gt;2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/i&gt;, 770–78. Las Vegas, NV, USA: IEEE. &lt;a href=&#34;https://doi.org/10.1109/CVPR.2016.90&#34;&gt;https://doi.org/10.1109/CVPR.2016.90&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_3&#34;&gt;&lt;/a&gt;Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. “ImageNet Classification with Deep Convolutional Neural Networks.” &lt;i&gt;Communications of the Acm&lt;/i&gt; 60 (6): 84–90. &lt;a href=&#34;https://doi.org/10.1145/3065386&#34;&gt;https://doi.org/10.1145/3065386&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_4&#34;&gt;&lt;/a&gt;LeCun, Yann, Bernhard Boser, John Denker, Donnie Henderson, R. Howard, Wayne Hubbard, and Lawrence Jackel. 1989. “Handwritten Digit Recognition with a Back-Propagation Network.” In &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;. Vol. 2. Morgan-Kaufmann. &lt;a href=&#34;https://papers.nips.cc/paper/1989/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html&#34;&gt;https://papers.nips.cc/paper/1989/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_5&#34;&gt;&lt;/a&gt;Simonyan, Karen, and Andrew Zisserman. 2015. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” &lt;i&gt;Arxiv:1409.1556 [Cs]&lt;/i&gt;, April. &lt;a href=&#34;http://arxiv.org/abs/1409.1556&#34;&gt;http://arxiv.org/abs/1409.1556&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_6&#34;&gt;&lt;/a&gt;Springenberg, Jost Tobias, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. 2015. “Striving for Simplicity: The All Convolutional Net.” &lt;i&gt;Arxiv:1412.6806 [Cs]&lt;/i&gt;, April. &lt;a href=&#34;http://arxiv.org/abs/1412.6806&#34;&gt;http://arxiv.org/abs/1412.6806&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Sampling and Aliasing</title>
      <link>https://ajdillhoff.github.io/notes/sampling/</link>
      <pubDate>Sun, 30 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/sampling/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#resizing&#34;&gt;Resizing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sampling&#34;&gt;Sampling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;resizing&#34;&gt;Resizing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Aliasing arises through resampling an image&lt;/li&gt;
&lt;li&gt;How to resize - algorithm&lt;/li&gt;
&lt;li&gt;How to resolve aliasing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Resizing an image, whether increase or decreasing the size, is a common image operation. In Linear Algebra, &lt;strong&gt;scaling&lt;/strong&gt; is one of the transformations usually discussed, along with rotation and skew. Scaling is performed by creating a transformation matrix&lt;/p&gt;
&lt;p&gt;\begin{equation*}
M =
\begin{bmatrix}
s &amp;amp; 0\\
0 &amp;amp; s
\end{bmatrix},
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(s\) is the scaling factor. This matrix can then be used to transform the location of each point via matrix multiplication.&lt;/p&gt;
&lt;p&gt;Is is that simple for digital images? Can we simply transform each pixel location of the image using \(M\)? There are a couple of steps missing when it comes to scaling digital images. First, \(M\) simply creates a mapping between the location in the original image and the corresponding output location in the scaled image. If we were to implement this in code, we would need to take the pixel&amp;rsquo;s value from the original image.&lt;/p&gt;
&lt;h3 id=&#34;a-simple-example&#34;&gt;A Simple example&lt;/h3&gt;
&lt;p&gt;Take a \(2 \times 2\) image whose pixel values are all the same color.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-29_21-24-11_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;2 by 2 image whose values are the same.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;2 by 2 image whose values are the same.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;If we transform each pixel location of the image and copy that pixel&amp;rsquo;s value to the mapped location in the larger image, we would get something as seen in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-29_21-29-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;The resulting scaled image.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;The resulting scaled image.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This image is exactly what we would expect. The resulting image is two times as large as the first. What pixel values should the new ones take on? This is a question of &lt;strong&gt;sampling&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;sampling&#34;&gt;Sampling&lt;/h2&gt;
&lt;p&gt;Given \(s = 2\), the scaling matrix maps the original pixel locations to their new values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\((0, 0) \mapsto (0, 0)\)&lt;/li&gt;
&lt;li&gt;\((0, 1) \mapsto (0, 2)\)&lt;/li&gt;
&lt;li&gt;\((1,0) \mapsto (2, 0)\)&lt;/li&gt;
&lt;li&gt;\((1,1) \mapsto (2, 2)\)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What values should be given to the unmapped values of the new image? There are several sampling strategies used in practice. Two of the most common approaches are &lt;strong&gt;nearest neighbor&lt;/strong&gt; and &lt;strong&gt;bilinear&lt;/strong&gt; sampling. Let&amp;rsquo;s start with the nearest neighbor approach.&lt;/p&gt;
&lt;h3 id=&#34;nearest-neighbor&#34;&gt;Nearest Neighbor&lt;/h3&gt;
&lt;p&gt;First, we let the pixel location in an image be the center of that pixel, as depicted below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-29_22-50-00_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;A 2-by-2 image with pixel locations depicted as dots in the center.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;A 2-by-2 image with pixel locations depicted as dots in the center.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To establish a map between the pixel locations in the scaled image and that of the original image, we shrink the grid on the larger image and superimpose it over the smaller image.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-29_22-51-11_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Pixel grid of larger image superimposed on original image.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Pixel grid of larger image superimposed on original image.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;With nearest neighbor interpolation, the pixel value in the resized image corresponds to that of the nearest pixel in the original image. In the above figure, we can see that pixels \((0, 0), (0, 1), (1, 0), \text{ and } (1, 1)\) in the resized image are closest to pixel \((0, 0)\) in the original image. Thus, they will take on that pixel&amp;rsquo;s value.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s compare both of these approaches on a real image. The first figure below shows the original image \((16 \times 16\)). The following figure shows the image resized to \((32 \times 32)\) with no interpolation and nearest neighbor interpolation, respectively.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-29_23-59-17_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Original image.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Original image.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-29_23-59-52_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Image resized with no interpolation (left) and nearest neighbor (right).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Image resized with no interpolation (left) and nearest neighbor (right).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;bilinear-interpolation&#34;&gt;Bilinear Interpolation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Bilinear interpolation&lt;/strong&gt; is a slightly more sophisticated way of sampling which takes into account all neighboring pixels in the original image. The value of the pixel in the sampled image is a linear combination of the values of the neighbors of the corresponding pixel it is mapped to.&lt;/p&gt;
&lt;p&gt;Consider a \(3 \times 3\) image upsampled to a \(8 \times 8\) image. The figure below shows the original image with the coordinates of the upsampled image superimposed on it.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_11-36-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;3-by-3 grid with 8-by-8 coordinates overlaid.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;3-by-3 grid with 8-by-8 coordinates overlaid.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;How do we determine the coordinate map between the original and upscaled image?&lt;/strong&gt;
Solve a linear system.&lt;/p&gt;
&lt;p&gt;Note the extreme values of the image. That is, the smallest and largest coordinates. Since we stated previously that \((0, 0)\) refers to the coordinate in the middle of the pixel, the top-left of the image boundary for any image is \((-0.5, -0.5)\). The bottom-right corner for the smaller image is \((2.5, 2.5)\). The bottom-right corner for the resized image is \((7.5, 7.5)\). The equation that maps the top-left coordinates between the images is given by&lt;/p&gt;
&lt;p&gt;\[
-\frac{1}{2} = -\frac{1}{2}a + b.
\]&lt;/p&gt;
&lt;p&gt;The equation that maps the bottom-right coordinates between the images is given by&lt;/p&gt;
&lt;p&gt;\[
\frac{5}{2} = \frac{15}{2}a + b.
\]&lt;/p&gt;
&lt;p&gt;Thus, we have 2 equations 2 unknowns. Solving this yields \(a = \frac{3}{8}\) and \(b = -\frac{5}{16}\).&lt;/p&gt;
&lt;p&gt;With the mapping solved, let&amp;rsquo;s compute the color value for pixel \((3, 3)\) in the upsampled image. Here, \((3, 3) \mapsto (\frac{13}{16}, \frac{13}{16})\) in the original image. Our problem for this particular pixel is reduced to the following figure.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_18-08-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Determining the pixel value for the mapped pixel using bilinear interpolation.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Determining the pixel value for the mapped pixel using bilinear interpolation.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;We first interpolate between two pairs of pixels in the original image. That is, we find \(p_1\) and \(p_2\) in the following figure.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_18-15-27_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Step 1: Interpolate the pixel values between two pixels for (p_1) and (p_2).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Step 1: Interpolate the pixel values between two pixels for (p_1) and (p_2).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Here, \(p_2 = \frac{3}{16}(255, 255, 255) + \frac{13}{16}(128, 128, 128) \approx (152, 152, 152)\) and \(p_1 = \frac{3}{16}(255, 0, 0) + \frac{13}{16}(255, 255, 255) \approx (255, 207, 207)\). Note that the contribution of the pixel depends on the weight on the other side the intermediate value \(p_i\). For example, if you think of \(p_1\) as a slider from the red pixel to the white pixel. The value to the left of the slider reflects the contribution of the pixel to the right, and vice versa.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_18-23-43_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;Computed values of (p_1) and (p_2).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;Computed values of (p_1) and (p_2).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Finally, the value of the new pixel is a linear combination of \(p_1\) and \(p_2\). That is \(p = \frac{13}{16}(152, 152, 152) + \frac{3}{16}(255, 207, 207) \approx (171, 162, 162)\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_18-28-03_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 11: &amp;lt;/span&amp;gt;The final pixel value computed from (p_1) and (p_2).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;The final pixel value computed from (p_1) and (p_2).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The following figure compares the original image with both types of interpolation discussed in this section.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_18-34-13_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 12: &amp;lt;/span&amp;gt;Original image (left), upscaled 2x with NN interpolation (middle), upscaled with bilinear interpolation (right).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 12: &lt;/span&gt;Original image (left), upscaled 2x with NN interpolation (middle), upscaled with bilinear interpolation (right).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;aliasing&#34;&gt;Aliasing&lt;/h3&gt;
&lt;p&gt;Nearest neighbor interpolation often leads to images with &lt;strong&gt;aliasing&lt;/strong&gt;. In general, aliasing occurs when two signals are sampled at such a frequency that they become indistinguishable from each other. Usually, images are smoothed prior to upsampling or downsampling in an effort to alleviate the effects of aliasing. The figure below shows a downsampled image using nearest neighbor interpolation.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_10-31-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 13: &amp;lt;/span&amp;gt;Image downsampled by 4x. Notice the &amp;#34;jaggies&amp;#34;, especially along straight lines.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 13: &lt;/span&gt;Image downsampled by 4x. Notice the &amp;ldquo;jaggies&amp;rdquo;, especially along straight lines.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;By blurring the image and using bilinear interpolation, the same image looks much smoother when downsized.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-01-25_19-30-16_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 14: &amp;lt;/span&amp;gt;Image downsampled by 4x using bilinear interpolation.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 14: &lt;/span&gt;Image downsampled by 4x using bilinear interpolation.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Color</title>
      <link>https://ajdillhoff.github.io/notes/color/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/color/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#topics&#34;&gt;Topics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-human-eye&#34;&gt;The Human Eye&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#color-matching&#34;&gt;Color Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#color-physics&#34;&gt;Color Physics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#color-spaces&#34;&gt;Color Spaces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hsv-color-space&#34;&gt;HSV Color Space&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;What is color?&lt;/li&gt;
&lt;li&gt;How do we process color?&lt;/li&gt;
&lt;li&gt;What information does color contain?&lt;/li&gt;
&lt;li&gt;What can we infer from color?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-human-eye&#34;&gt;The Human Eye&lt;/h2&gt;
&lt;p&gt;The eye acts as a camera, including a lens which focuses light onto a receptive surface. The &lt;strong&gt;cornea&lt;/strong&gt; covers the &lt;strong&gt;lens&lt;/strong&gt; which combine to make a compound lens. The lens itself is flexible to allow the eye to focus on objects of variable distance. The lens is attached to &lt;strong&gt;ciliary muscles&lt;/strong&gt; which contract or expand to change the shape of the lens. This allows us to focus on near or far objects. As we age, the lens itself becomes hardened and does not transform back to a spherical shape when the ciliary muscles contract, resulting in farsightedness.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;pupil&lt;/strong&gt; is a diaphragm that adjusts the amount of light that enters the eye in response to varying intensities. However, this isn&amp;rsquo;t the only mechanism available for this task. Our perception of the amount of light, &lt;em&gt;luminance adaptation&lt;/em&gt;, occurs also in the retina and brain over a longer time period (usually several minutes).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-02_20-23-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The following image from Wikipedia shows the evolution of the eye from a simple region of photoreceptors to the current model we have today.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-04_10-00-18_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Source: &amp;lt;https://en.wikipedia.org/wiki/Eye&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Source: &lt;a href=&#34;https://en.wikipedia.org/wiki/Eye&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://en.wikipedia.org/wiki/Eye&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;retina&#34;&gt;Retina&lt;/h3&gt;
&lt;p&gt;The primary purpose of the optics of the eye is to focus light onto the &lt;strong&gt;retina&lt;/strong&gt;. This is a thin layer of nerve tissue that covers the inner surface of the eye. It consists of approximately 200 million layered cells. Half are &lt;strong&gt;photoreceptor&lt;/strong&gt; cells and the other half recode photoreceptor outputs before passing them on towards the brain.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fun fact:&lt;/strong&gt; The retina is woven with blood vessels whose purpose is to nourish the retinal tissue. These blood vessels produce the &lt;em&gt;red eye&lt;/em&gt; effect that is seen in flash photography.&lt;/p&gt;
&lt;h3 id=&#34;photoreceptor-cells&#34;&gt;Photoreceptor Cells&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-23_21-19-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Distribution of rods and cones. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Distribution of rods and cones. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Photorceptor cells can be classified into two types: rods and cones. There are about 100 million rods and 6 million cones. &lt;strong&gt;Rods&lt;/strong&gt; dominate our low-light vision. They do not have the variety of photopigments necessary for color vision. &lt;strong&gt;Cones&lt;/strong&gt; dominate our color vision. They can be separated into one of three classes of light receptors. These three classes contain a specific type of photopigment that is sensitive to different wavelengths of light.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2021-12-31_13-24-45_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Source: &amp;lt;https://handprint.com/HP/WCL/color1.html#3cones&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Source: &lt;a href=&#34;https://handprint.com/HP/WCL/color1.html#3cones&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://handprint.com/HP/WCL/color1.html#3cones&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Rods can pool input together before sending it to the brain. Low-light perception is better through peripheral vision.&lt;/p&gt;
&lt;p&gt;Most of the cones are in the &lt;strong&gt;fovea&lt;/strong&gt;. The high density of cones in this region means that the resolution is highest. Microsaccades - The eye is making tiny movements so that the eye is never fixated on a single point. This allows light to hit a greater number of photoreceptors.&lt;/p&gt;
&lt;p&gt;The other segment of a photoreceptor cell lies the photopigment molecules. These act as transducers which convert light energy into a biological response. These molecules are made up of a light sensitive molecule called &lt;strong&gt;chromophore&lt;/strong&gt; and a protein called &lt;strong&gt;opsin&lt;/strong&gt;. Together, they are usually referred to as &lt;strong&gt;rhodopsin&lt;/strong&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-02_20-47-55_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Photopigment Molecules. Source: &amp;lt;https://handprint.com/HP/WCL/color1.html&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Photopigment Molecules. Source: &lt;a href=&#34;https://handprint.com/HP/WCL/color1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://handprint.com/HP/WCL/color1.html&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;types-of-photopigments&#34;&gt;Types of Photopigments&lt;/h4&gt;
&lt;p&gt;Each photoreceptor can respond to light differently depending on the specific type of rhodopsin it is made up of.&lt;/p&gt;
&lt;p&gt;The response of each type of cone has been measured and can is separated into short wavelength (S), medium wavelength (M), and long wavelength (L). Some resources incorrectly label these as B, G, and R receptors. This is not entirely accurate as there is much overlap between the medium and long wavelength receptors.&lt;/p&gt;
&lt;h4 id=&#34;mathematical-model&#34;&gt;Mathematical Model&lt;/h4&gt;
&lt;p&gt;How would we model a photoreceptor, mathematically? The response is dependent on how sensitive the receptor is to a specific wavelength along with the light arriving at the receptor.&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p_k = \int_{\Lambda} \sigma_k(\lambda)E(\lambda)d\lambda
\end{equation*}&lt;/p&gt;
&lt;h2 id=&#34;color-matching&#34;&gt;Color Matching&lt;/h2&gt;
&lt;p&gt;The theory that the visible colors are visible from the three primaries is called &lt;strong&gt;trichomatic theory&lt;/strong&gt;. Trichromacy has been measured and observed.&lt;/p&gt;
&lt;p&gt;How is it known that these 3 (or 4) types of photoreceptors absorb a specific wavelength? In other words, how do we know that color vision break down? There has not been a way to measure cone responses in living humans, but there is a way to measure them indirectly.&lt;/p&gt;
&lt;p&gt;James Clerk Maxwell&amp;rsquo;s color matching experiments aim to do exactly that. In this experiment, participants are given a test color and a matching color. The goal is to add some amount of primary colors until the matching color is the same as the test color. There are some saturated test colors that cannot be matched in an additive manner (combining primaries). In these cases, the subjects are allowed to subtract some amount of a primary color from the test color until it matches the matching color.&lt;/p&gt;
&lt;p&gt;Over many different experiments involving many subjects, it was found that most subjects will match the test color with the same amount of primary weights. This provided confirmation that human vision is trichromatic. There were also carefully screen &lt;strong&gt;dichromats&lt;/strong&gt; who lacked one class of photoreceptor.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is color matching reliable?&lt;/strong&gt;
&lt;strong&gt;Do we see all wavelengths the same? That is, do we have an equal number of photoreceptors for each range?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The fact that many independent observers came up with similar distributions of primary colors to match test colors gave rise to Grassman&amp;rsquo;s laws.&lt;/p&gt;
&lt;p&gt;The result of these experiments reveals a function of cone sensitivities. Cones are unequally distributed across the retina. Visual angle and whether the light is detected in the central vs. peripheral region matters. The CIE 1931 RGB color matching functions show how much of each primary wavelength is required to match a particular target wavelength. The negative values mean that the primary was added to the test color in order to match.&lt;/p&gt;
&lt;p&gt;
  &lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-03_12-36-25_screenshot.png&#34; alt=&#34;&#34;&gt;

Source: &lt;a href=&#34;https://en.wikipedia.org/wiki/CIE_1931_color_space&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://en.wikipedia.org/wiki/CIE_1931_color_space&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Actual cone sensitivy depends on light sensitivy. One of the most popular graphs of cone response is from Stockman &amp;amp; Sharpe (2000):&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-03_13-27-23_screenshot.png&#34; &gt;


&lt;/figure&gt;

&lt;p&gt;This graph is normalized to peak response and is not representative of the distribution of cones in the retina. If we weight the responses by the proportion of each class of cone in the retina, the graph looks like:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-03_13-28-55_screenshot.png&#34; &gt;


&lt;/figure&gt;

&lt;p&gt;The fact that the 3 shapes are colored as blue, green, and red are misleading. There are far more photoreceptors that perceive green light than there are blue or red light. You can see this when comparing green text versus blue text. Reading the blue text may strain your eyes and appear blurry. This is because there are simply fewer receptors available for these wavelengths. Fewer such receptors also implies that the resolution is smaller.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-03_13-34-15_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Fewer blue cones results in blue light appearing more blurry.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Fewer blue cones results in blue light appearing more blurry.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;color-physics&#34;&gt;Color Physics&lt;/h2&gt;
&lt;p&gt;Light sources themselves can produce different colors at different wavelengths. The sky varies in color depending on the relative location of the sun. A surface is dependent on the color of the surface itself as well as the incident light shining towards it. A white surface with a green light shining on it will reflect green light. A green surface with white light will also reflect green. Further complexities such as atmosphere and dust can further complicate this.&lt;/p&gt;
&lt;p&gt;In our world, both the sky and the sun are important light sources. They are often referred to as skylight and sunlight, respectively. Imagine a clear day with the sun at the highest point in the sky. The sun is seen as a yellow object with the surrounding sky being a rich blue. Compared to the zenith of the sky, the horizon is typically brighter. This is because air scatters the incident light. This can be modeled by assuming the sky emits a constant amount of exitant light per volume.&lt;/p&gt;
&lt;p&gt;Why does the sun appear yellow and the sky appear blue? Longer wavelengths can travel farther before being scattered. This means that a ray of light travelling from the sun to the earth will scatter blue light before the other rays. Taking the blue light out of a ray of white light will leave a yellowish color. When the scattered blue light eventually hits the atmosphere of Earth, it is scattered once again and see as incident light to an observer. This gives the apperance of a blue sky.&lt;/p&gt;
&lt;p&gt;The perceived color of an object can be computed by multiplying the incident illumination with the reflectance of the object.&lt;/p&gt;
&lt;h3 id=&#34;color-temperature&#34;&gt;Color Temperature&lt;/h3&gt;
&lt;p&gt;One quantity that is commonly used to describe a light source is temperature (measured in Kelvins). This is derived from the concept of the &lt;strong&gt;black body&lt;/strong&gt;. That is, a body that does not reflect light. A heated black body emits radiation and the spectral power distribution of this radiation depends only on the temperature. The color temperature is the surface temperature of an ideal black body.&lt;/p&gt;
&lt;p&gt;At lower temperature, the color is a red. As it increases, the color becomes whiter until reaching a light blue. See the table below:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_11-49-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Colors corresponding to different temperatures. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Colors corresponding to different temperatures. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;color-spaces&#34;&gt;Color Spaces&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;How is color represented based on color matching?&lt;/strong&gt;
&lt;strong&gt;Can we accurately reproduce any color digitally?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.wnycstudios.org/podcasts/radiolab/episodes/211119-colors&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.wnycstudios.org/podcasts/radiolab/episodes/211119-colors&lt;/a&gt;
Interesting podcast on Color.&lt;/p&gt;
&lt;p&gt;There are several color spaces (or color models) available. The most common is RGB. In this section, we will explore the most common color spaces.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-03_17-51-15_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Comparison of several common color spaces. Source: &amp;lt;https://en.wikipedia.org/wiki/ProPhoto_RGB_color_space&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Comparison of several common color spaces. Source: &lt;a href=&#34;https://en.wikipedia.org/wiki/ProPhoto_RGB_color_space&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://en.wikipedia.org/wiki/ProPhoto_RGB_color_space&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;cie-xy-color-space&#34;&gt;CIE xy Color Space&lt;/h3&gt;
&lt;p&gt;Verifying trichomatic theory meant that one should attempt to reproduce monochromatic (single wavelength) colors as a weighted mixture of primary colors. The Commision Internationale d&amp;rsquo;Eclairage did just that in the 1930s. They performed their own color matching experiments using red, blue, and green wavelengths as primary colors. Another benefit of this process is to set a standard in which colors can be reproduced.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-03_17-30-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;CIE xy color space.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;CIE xy color space.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This shows the CIE xy space which was taken from CIE XYZ space. This 2D space is an intersection of the original XYZ space by the plane \(X + Y + Z = 1\). The coordinates are then&lt;/p&gt;
&lt;p&gt;\begin{equation*}
(x, y) = \Big(\frac{x}{x + y + z}, \frac{y}{x + y + z}\Big).
\end{equation*}&lt;/p&gt;
&lt;p&gt;The border of this shape represents the wavevelength of the pure color. The colors in the middle represent some linear combination of those wavelengths.&lt;/p&gt;
&lt;h3 id=&#34;rgb-color-spaces&#34;&gt;RGB Color Spaces&lt;/h3&gt;
&lt;p&gt;RGB color spaces are the most commonly used in computer graphics. CIE also has their own RGB color space. It is derived from color matching experiments using 3 monochromatic primary colors.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-24_20-49-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;CIE RGB gamut on the CIE xy color space.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;CIE RGB gamut on the CIE xy color space.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;RGB models are commonly represented as a cube, as seen in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-24_20-54-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 11: &amp;lt;/span&amp;gt;Cube representation of an RGB color space.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;Cube representation of an RGB color space.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;hsv-color-space&#34;&gt;HSV Color Space&lt;/h2&gt;
&lt;p&gt;Hue, Saturation, Value (HSV) provides an alternative representation of RGB. &lt;strong&gt;Hue&lt;/strong&gt; represents the color. Given a fixed value for saturation and value, colors in HSV should appear as if they are receiving the same level of light.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-24_21-05-11_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 12: &amp;lt;/span&amp;gt;Hue values when saturation and value are fixed. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 12: &lt;/span&gt;Hue values when saturation and value are fixed. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The CIE defines &lt;strong&gt;saturation&lt;/strong&gt; as &amp;ldquo;the colourfulness of an area judged in proportion to its brightness.&amp;rdquo; The &lt;strong&gt;value&lt;/strong&gt; of a pixel represents how bright the color is compared to black.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-24_21-10-29_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 13: &amp;lt;/span&amp;gt;HSV cylinder exemplifies the concepts of hue, saturation, and value. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 13: &lt;/span&gt;HSV cylinder exemplifies the concepts of hue, saturation, and value. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;conversion-from-rgb&#34;&gt;Conversion from RGB&lt;/h4&gt;
&lt;p&gt;We can convert images to HSV from RGB and vice versa. This article will only go through the steps of actually transforming it. To gain a better understanding of the conversion between HSV to RGB, check out the &lt;a href=&#34;https://en.wikipedia.org/wiki/HSL_and_HSV#General_approach&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Wikipedia page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We start by making sure that are input image is normalized such that the RGB values are in the range \([0, 1]\). If that&amp;rsquo;s the case, we can calculate the HSV value \(V\) as&lt;/p&gt;
&lt;p&gt;\[
V = \max(R, G, B).
\]&lt;/p&gt;
&lt;p&gt;We can do this in one command in Python using numpy:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;V &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max(img, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Saturation is computed based on another quantity called &lt;a href=&#34;https://en.wikipedia.org/wiki/Colorfulness&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Chroma&lt;/a&gt;. It is simply computed as&lt;/p&gt;
&lt;p&gt;\[
C = V - \min(R,G,B).
\]&lt;/p&gt;
&lt;p&gt;In Python, this is simply&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;C &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; V &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;min(img, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Saturation is then computed as \(S = \frac{C}{V}\). Note that this will be undefined if \(V = 0\). In practice, we can set \(S = 0\) if \(V\) is also 0.&lt;/p&gt;
&lt;p&gt;Hue is commonly measured in degrees between \([0, 360]\). As seen in the figure below, it is the angle past the previous edge of the hexagon.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_10-29-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 14: &amp;lt;/span&amp;gt;Hue is the angle of the projected point with respect to the hexagon. 0 degrees is marked by the red edge. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 14: &lt;/span&gt;Hue is the angle of the projected point with respect to the hexagon. 0 degrees is marked by the red edge. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The function for Hue can be written as a piecewise function, altered slightly to account for undefined values in practice:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
H&amp;rsquo; =
\begin{cases}
0, &amp;amp; C = 0\\
\frac{G - B}{C} \text{ mod } 6, &amp;amp; \text{if } V = R\\
\frac{B - R}{C} + 2, &amp;amp; \text{if } V = G\\
\frac{R - G}{C} + 4, &amp;amp; \text{if } V = B\\
\end{cases}
\end{equation*}&lt;/p&gt;
&lt;p&gt;Then, \(H = 60^{\circ} \times H&amp;rsquo;\).&lt;/p&gt;
&lt;h4 id=&#34;conversion-to-rgb&#34;&gt;Conversion to RGB&lt;/h4&gt;
&lt;p&gt;To go back to RGB, we take an HSV image with \(H \in [0^{\circ}, 360^{\circ}]\), and \(S, V \in [0, 1]\). The Chrome value is calcuated as&lt;/p&gt;
&lt;p&gt;\[
C = V \times S.
\]&lt;/p&gt;
&lt;p&gt;We then divide up the Hue into one of 6 values:&lt;/p&gt;
&lt;p&gt;\[
H&amp;rsquo; = \frac{H}{60^{\circ}}.
\]&lt;/p&gt;
&lt;p&gt;With these intermediate value, we can calculate the corresponding point within the RGB cube that has the same hue and chroma as the current pixel value, with \(X\) being the second largest component of the color:&lt;/p&gt;
&lt;p&gt;\[
X = C \times (1 - |H&amp;rsquo; \text{ mod } 2 - 1|)
\]&lt;/p&gt;
&lt;p&gt;and then&lt;/p&gt;
&lt;p&gt;\begin{equation*}
(R&amp;rsquo;, G&amp;rsquo;, B&amp;rsquo;) =
\begin{cases}
(C, X, 0) &amp;amp; \text{if } 0 \leq H&amp;rsquo; &amp;lt; 1\\
(X, C, 0) &amp;amp; \text{if } 1 \leq H&amp;rsquo; &amp;lt; 2\\
(0, C, X) &amp;amp; \text{if } 2 \leq H&amp;rsquo; &amp;lt; 3\\
(0, X, C) &amp;amp; \text{if } 3 \leq H&amp;rsquo; &amp;lt; 4\\
(X, 0, C) &amp;amp; \text{if } 4 \leq H&amp;rsquo; &amp;lt; 5\\
(C, 0, X) &amp;amp; \text{if } 5 \leq H&amp;rsquo; &amp;lt; 6\\
\end{cases}
\end{equation*}&lt;/p&gt;
&lt;p&gt;The final RGB value can be calculated by adding the difference between the value and chroma to each pixel:&lt;/p&gt;
&lt;p&gt;\[
m = V - C
\]&lt;/p&gt;
&lt;p&gt;\[
(R, G, B) = (R&amp;rsquo; + m, G&amp;rsquo; + m, B&amp;rsquo; + m).
\]&lt;/p&gt;
&lt;h4 id=&#34;some-examples&#34;&gt;Some Examples&lt;/h4&gt;
&lt;p&gt;Given an original image (below), we&amp;rsquo;ll view the output of changing the Hue, Saturation, and Value.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_10-53-17_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 15: &amp;lt;/span&amp;gt;Original image. Credit: The Expanse&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 15: &lt;/span&gt;Original image. Credit: The Expanse
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Reducing the Hue by 20 produces the following image:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_10-55-52_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 16: &amp;lt;/span&amp;gt;Image with Hue subtracted by 20 degrees. Credit: The Expanse&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 16: &lt;/span&gt;Image with Hue subtracted by 20 degrees. Credit: The Expanse
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Given our working knowledge of how hue is computed, this makes sense. The previous angle clearly pointed to lighter blue colors, reducing that by \(20^{\circ}\) moves us towards the green edge.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s take the original image and increase saturation by 0.3:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_10-59-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 17: &amp;lt;/span&amp;gt;Image with Saturation increased by 0.3. Credit: The Expanse&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 17: &lt;/span&gt;Image with Saturation increased by 0.3. Credit: The Expanse
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;All colors across the board look &amp;ldquo;richer&amp;rdquo; and &amp;ldquo;deeper&amp;rdquo;. This corresponds with the definition of the HSV cylinder.&lt;/p&gt;
&lt;p&gt;Finally, we&amp;rsquo;ll view an image in which the value was modified. Let&amp;rsquo;s increase the values by 0.2:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_11-03-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 18: &amp;lt;/span&amp;gt;Image with Values increased by 0.2. Credit: The Expanse&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 18: &lt;/span&gt;Image with Values increased by 0.2. Credit: The Expanse
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This looks washed out. All of the pixels with 0 values were increased uniformly. Perhaps we could clamp that by setting all pixels with the given value change back to their original values.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linear Filters</title>
      <link>https://ajdillhoff.github.io/notes/linear_filters/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/linear_filters/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#smoothing&#34;&gt;Smoothing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#convolution&#34;&gt;Convolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gaussian-filters&#34;&gt;Gaussian Filters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#image-derivatives&#34;&gt;Image Derivatives&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;How do we detect specific patterns in images (eyes, nose, spots, etc.)?&lt;/li&gt;
&lt;li&gt;Weighted sums of pixel values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;smoothing&#34;&gt;Smoothing&lt;/h2&gt;
&lt;p&gt;When discussing resizing and interpolation, we saw how the choice of scale factor and rotation can produce aliasing in images. Typically, this effect is hidden using some sort of smoothing.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s first look at the case of downsampling an image to 10\% of its original size. If we use nearest neighbor interpolation, the result is very blocky, as seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_20-49-14_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Rocinante cropped and scaled to 10% of the original size. Source: The Expanse&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Rocinante cropped and scaled to 10% of the original size. Source: The Expanse
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Effectively, an entire block of pixels in the original image is being mapped to the nearest neighbor. We are losing a lot of information from the original image. The figure below shows an overlay of the low resolution grid over a patch of the high resolution image.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_21-48-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;The center dots show the selected pixel value for the downsampled grid.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;The center dots show the selected pixel value for the downsampled grid.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;computing-the-average&#34;&gt;Computing the Average&lt;/h3&gt;
&lt;p&gt;Instead of naively selecting one pixel to represent an entire block, we could compute the average pixel value of all pixels within that block. This is a simple as taking an equal contribution from each pixel in the block and dividing by the total number of pixels in the block. An example of such a block is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_21-55-06_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;A (3 times 3) averaging filter.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;A (3 times 3) averaging filter.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;By varying the size of this filter, we can effectively change the factor for which we smooth the aliasing.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_21-32-36_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Original image smoothed with (9 times 9) average filter before downsampling to 10% of its original size.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Original image smoothed with (9 times 9) average filter before downsampling to 10% of its original size.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;convolution&#34;&gt;Convolution&lt;/h2&gt;
&lt;p&gt;How can we apply the averaging filter to an image effectively? We slide the filter across the image starting at the first pixel in the first row and move row by row until all pixels have been computed.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_22-20-56_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;The takes input from all pixels under it and computes the average.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;The takes input from all pixels under it and computes the average.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_23-13-19_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Final portion of image after kernel is applied.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Final portion of image after kernel is applied.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This is performed by the &lt;strong&gt;convolution&lt;/strong&gt; operation. It is defined as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
g(x, y) = \omega * f(x, y) = \sum_{dx = -a}^a \sum_{dy=-b}^b \omega(dw, dy)f(x + dx, y + dy).
\end{equation*}&lt;/p&gt;
&lt;p&gt;The range \((-a, a)\) represents the rows of the kernel and \((-b, b)\) the range of the columns of the kernel. The center of the kernel is at \((dx = 0, dy = 0)\).&lt;/p&gt;
&lt;p&gt;This operation is one of, if not the most, important operations in computer vision. It is used to apply filters, but we will later see it as one of the guiding operators for feature extraction and deep learning methods.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_15-40-19_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;An image (f) convolved with kernel (h). Source: Szeliski&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;An image (f) convolved with kernel (h). Source: Szeliski
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;properties&#34;&gt;Properties&lt;/h3&gt;
&lt;h4 id=&#34;commutativity&#34;&gt;Commutativity&lt;/h4&gt;
&lt;p&gt;\(f * g = g * f\)&lt;/p&gt;
&lt;h4 id=&#34;associativity&#34;&gt;Associativity&lt;/h4&gt;
&lt;p&gt;\(f * (g * h) = (f * g) * h\)&lt;/p&gt;
&lt;h4 id=&#34;distributivity&#34;&gt;Distributivity&lt;/h4&gt;
&lt;p&gt;\(f * (g + h) = (f * g) + (f * h)\)&lt;/p&gt;
&lt;h3 id=&#34;shift-invariant-linear-systems&#34;&gt;Shift Invariant Linear Systems&lt;/h3&gt;
&lt;p&gt;Convolution is a &lt;em&gt;linear shift-invariant&lt;/em&gt; operator. That is, it obeys the following properties.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Superposition:&lt;/strong&gt; The response of the sum of the input is the sum of the individual responses.&lt;/p&gt;
&lt;p&gt;\[
R(f + g) = R(f) + R(g)
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Scaling:&lt;/strong&gt; The response to a scaled input is equal to the scaled response of the same input.&lt;/p&gt;
&lt;p&gt;\[
R(kf) = kR(f)
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Shift Invariance:&lt;/strong&gt; The response to a translated input is equal to the translation of the response to the input.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;system&lt;/strong&gt; is linear if it satisfies both the superposition and scaling properties. Further, it is a shift-invariant linear system if it is linear and satisfies the shift-invariance property.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is the average box filter linear?&lt;/strong&gt; Yes, it is applied with convolution which behaves the same everywhere.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is thresholding a linear system?&lt;/strong&gt; No, it can be shown that \(f(n, m) + g(n, m) &amp;gt; T\), but \(f(n, m) &amp;lt; T\) and \(g(n, m) &amp;lt; T\).&lt;/p&gt;
&lt;h2 id=&#34;gaussian-filters&#34;&gt;Gaussian Filters&lt;/h2&gt;
&lt;p&gt;Blurring an image using a box filter does not simulate realistic blurring as well as Gaussian filters do. The following figure exemplifies the difference between the two.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_16-57-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;The left image is blurred using a uniform average box filter. The right image is blurred with a Gaussian filter.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;The left image is blurred using a uniform average box filter. The right image is blurred with a Gaussian filter.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The shortcomings of the average box filter can be seen when viewing the artifacting visible at edges, especially at corners.&lt;/p&gt;
&lt;p&gt;A Guassian kernel is defined as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
G(x, y;\sigma) = \frac{1}{2\pi \sigma^2}\exp\Big(-\frac{(x^2 + y^2)}{2 \sigma^2}\Big).
\end{equation*}&lt;/p&gt;
&lt;p&gt;In effect, it enforces a greater contribution from neighbors near the pixel and a smaller contribution from distant pixels.&lt;/p&gt;
&lt;h2 id=&#34;image-derivatives&#34;&gt;Image Derivatives&lt;/h2&gt;
&lt;p&gt;We can use convolution to approximate the partial derivative (or finite difference) of an image. Recall that&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{\partial f}{\partial x} = \lim\limits_{\epsilon \rightarrow 0} \frac{f(x + \epsilon, y) - f(x, y)}{\epsilon}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;We can estimate this as a finite difference&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{\partial h}{\partial x} \approx h_{i+1, j} - h_{i-1, j}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Which can be applied via convolution given a kernal&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathcal{H} =
\begin{bmatrix}
0 &amp;amp; 0 &amp;amp; 0\\
-1 &amp;amp; 0 &amp;amp; 1\\
0 &amp;amp; 0 &amp;amp; 0\\
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Applying both the horizontal and vertical derivative kernels to an image to a simple square shows the detection of horizontal and vertical edges.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_17-36-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Horizontal (right) and vertical (middle) derivative kernels applied to the original image (left).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Horizontal (right) and vertical (middle) derivative kernels applied to the original image (left).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The results of applying the derivative kernels are referred to as vertical edge and horizontal edge &lt;strong&gt;scores&lt;/strong&gt;. Consider the middle image in the figure above. The left edge has pixel values of 255; the right edge has -255. In either case, a high absolute score reveals that there is an edge. These scores report the direction of greatest change. The 255 on the left edge indicates the direction is to the right, while the -255 score indicates the direction is to the left. All other instances of the image return a rate of change of 0.
Let&amp;rsquo;s see how these filters perform on a more interesting image.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_22-45-59_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;Vertical derivative filter (left) and horizontal derivative filter (right).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;Vertical derivative filter (left) and horizontal derivative filter (right).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
  </channel>
</rss>
