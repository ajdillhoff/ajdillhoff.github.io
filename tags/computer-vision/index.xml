<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computer Vision on Alex Dillhoff</title>
    <link>https://ajdillhoff.github.io/tags/computer-vision/</link>
    <description>Recent content in Computer Vision on Alex Dillhoff</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; {year}</copyright>
    <lastBuildDate>Sun, 04 Feb 2024 18:54:00 -0600</lastBuildDate>
    
	    <atom:link href="https://ajdillhoff.github.io/tags/computer-vision/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Bag of Visual Words</title>
      <link>https://ajdillhoff.github.io/notes/bag_of_visual_words/</link>
      <pubDate>Sun, 04 Feb 2024 18:54:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/bag_of_visual_words/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#bag-of-visual-words&#34;&gt;Bag of Visual Words&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;&lt;strong&gt;Bag of Words&lt;/strong&gt; is a technique used in Natural Language Processing for document classification. It is a collection of word counts. To create a Bag of Words for a document, it necessary to create a dictionary first. Choosing the a dictionary is based on many factors including computational limitations. Next, the documents in a dataset are tokenized into words. The word counts are collected as part of a histogram and used as a feature vector for a machine learning model.&lt;/p&gt;
&lt;p&gt;The dictionary is the same for all documents in the original dataset. Ideally, the Bag of Word vectors for each document in the same class will be similar. This technique works well for problems in natural language processing, where each input document will have a varying number of words. By using a Bag of Words, the input data is transformed into a fixed length feature vector.&lt;/p&gt;
&lt;h2 id=&#34;bag-of-visual-words&#34;&gt;Bag of Visual Words&lt;/h2&gt;
&lt;p&gt;The Bag of Visual Words model adapts this technique to computer vision. Instead of words, distinct visual features are extracted from each image. Some images may have more features than others, similar to how some documents will have different word counts. The dictionary is created by clustering the visual features into a finite number of groups, determined as a hyperparameter. The visual features for each image are then counted and used as a feature vector for a machine learning model.&lt;/p&gt;
&lt;h3 id=&#34;extract-visual-features&#34;&gt;Extract Visual Features&lt;/h3&gt;
&lt;p&gt;The first step in creating a Bag of Visual Words is to extract visual features from each image. The visual features are typically extracted using a technique like SIFT, SURF, or ORB. These techniques are designed to extract features that are invariant to scaling, rotation, and translation. The visual features are then stored in a list for each image.&lt;/p&gt;
&lt;h3 id=&#34;create-visual-words&#34;&gt;Create Visual Words&lt;/h3&gt;
&lt;p&gt;Creating the dictionary requires clustering the features into a finite number of groups. The number of groups will vary depending on the complexity of the data. For a given dataset, this can be determined empirically. The most common clustering algorithm for this is K-Means, in which \(k\) different clusters are created and updated iteratively. The visual features are then assigned to the nearest cluster, and the cluster centers are updated. This process is repeated until the cluster centers converge.&lt;/p&gt;
&lt;h3 id=&#34;build-sparse-frequency-vectors&#34;&gt;Build Sparse Frequency Vectors&lt;/h3&gt;
&lt;p&gt;The next step is to create a histogram of the visual features for each image. The histogram is a sparse vector, where each element represents the count of a visual feature in the image. The histogram is then normalized to create a feature vector. Given an input image, the feature vector is extracted and assigned a label based on the cluster model. That label is one of the \(n\) chosen words in the vocabulary, which is incremented in the histogram.&lt;/p&gt;
&lt;h3 id=&#34;adjust-frequency-vectors&#34;&gt;Adjust Frequency Vectors&lt;/h3&gt;
&lt;p&gt;The feature vectors are then adjusted to account for the frequency of the visual features. This is done by applying a weighting scheme to them. The most common weighting scheme is called Term Frequency-Inverse Document Frequency (TF-IDF). TF-IDF scheme adjusts the frequency of a word in a document based on the frequency in the entire dataset. It is calculated as follows:&lt;/p&gt;
&lt;p&gt;\[
\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t),
\]&lt;/p&gt;
&lt;p&gt;where \(\text{TF}(t, d)\) is the term frequency of term \(t\) in document \(d\) and \(\text{IDF}(t)\) is the inverse document frequency of term \(t\) in the entire dataset.&lt;/p&gt;
&lt;p&gt;\(\text{TF}(t, d)\) is simply the number of times that visual feature \(t\) appears in the image \(d\). \(\text{IDF}(t)\) is calculated as follows:&lt;/p&gt;
&lt;p&gt;\[
\text{IDF}(t) = \log\left(\frac{N}{n_t}\right),
\]&lt;/p&gt;
&lt;p&gt;where \(N\) is the total number of images in the dataset and \(n_t\) is the number of images that contain the visual feature \(t\).&lt;/p&gt;
&lt;h3 id=&#34;compare-vectors&#34;&gt;Compare Vectors&lt;/h3&gt;
&lt;p&gt;The last step is to compare the feature vectors in service of some downstream task like classification. Since every feature vector is a fixed length, they can be used as input to a machine learning model.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Convolutional Neural Networks</title>
      <link>https://ajdillhoff.github.io/notes/convolutional_neural_networks/</link>
      <pubDate>Sat, 02 Apr 2022 00:00:00 -0500</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/convolutional_neural_networks/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#convolution-operator&#34;&gt;Convolution Operator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#properties-of-convolutions&#34;&gt;Properties of Convolutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parameter-sharing&#34;&gt;Parameter Sharing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pooling&#34;&gt;Pooling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backwards-pass&#34;&gt;Backwards Pass&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example&#34;&gt;Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#neural-networks-for-image-classification&#34;&gt;Neural Networks for Image Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#useful-resources&#34;&gt;Useful Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;p&gt;&lt;strong&gt;Key Concepts&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Invariance and Equivariance&lt;/li&gt;
&lt;li&gt;Definition&lt;/li&gt;
&lt;li&gt;Padding, Stride, Kernel size, dilation&lt;/li&gt;
&lt;li&gt;Purpose of multiple feature maps&lt;/li&gt;
&lt;li&gt;Receptive fields and hierarchies of features&lt;/li&gt;
&lt;li&gt;Downsampling, Upsampling, Examples in research&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Dense neural networks made up of linear layers and a chosen activation function are not practical for image data. Consider an image of size \(224\times224\times3\). The first layer of a dense network would require a \(150,528\times n\) parameter matrix, where \(n\) is the number of nodes in the first layer. It is common to build dense networks where the first layer has more nodes than input features. In this case, we would need a minimum of \(150,528^2\) parameters in the first layer. Even if we chose something much smaller like \(n=1024\), this would require \(154,140,672\) parameters for just the first layer. This is clearly impractical.&lt;/p&gt;
&lt;p&gt;Aside from requiring a large number of parameters, we might ask whether it is beneficial to feed raw pixel values into a dense network. The network itself would be learning pixel-wise features with no regard to their spatial relationship. This makes our network&amp;rsquo;s job much more difficult because the spatial arrangement of features tells us so much about what we see. In practice, this means that the network would have to learn the same features at every location in the image. We would instead prefer this network to learn features that are &lt;strong&gt;invariant&lt;/strong&gt; to translation. That is, the network should learn features that are the same regardless of where they appear in the image.&lt;/p&gt;
&lt;p&gt;Invariance to translation is very convenient and can save our network a lot of work in learning the same feature at every point in the input. It is also desirable that our network is invariant to other transformations such as rotation, scaling, skewing, and warping. Formally, a function \(f(\mathbf{x})\) of an image \(\mathbf{x}\) is invariant to a transformation \(t(\mathbf{x})\) if&lt;/p&gt;
&lt;p&gt;\[
f(t(\mathbf{x})) = f(\mathbf{x}).
\]&lt;/p&gt;
&lt;p&gt;Aside from invariance, some models should be &lt;strong&gt;equivariant&lt;/strong&gt; to certain transformations. That is, the output of the model should change in the same way as the input. Image segmentation models should be equivariant to translation. If we were to shift an image by a few pixels, the output segmentation mask should also shift by the same amount. Convolutional neural networks are equivariant to &lt;em&gt;translation&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;convolution-operator&#34;&gt;Convolution Operator&lt;/h2&gt;
&lt;p&gt;A convolution is a function that takes two functions as input and produces a third function as output. The first function is the input and the second function is the kernel. The output is called the feature map. The kernel is also sometimes called the filter.&lt;/p&gt;
&lt;p&gt;\[
(f * g)(t) = \int f(t-a)g(a)da
\]&lt;/p&gt;
&lt;p&gt;We can view them more concretely by considering the functions to be vectors. For example, let the function \(f\) be an input vector \(x\) and \(w\) be a kernel representing a filter. The convolution operator is then&lt;/p&gt;
&lt;p&gt;\[
(x * w)(t) = \int x(t-a)w(a)da.
\]&lt;/p&gt;
&lt;p&gt;The result the &lt;strong&gt;feature map&lt;/strong&gt; representing the response of the kernel at each location in the input.&lt;/p&gt;
&lt;p&gt;In the case of discrete values, the operator is written as&lt;/p&gt;
&lt;p&gt;\[
(x * w)(t) = \sum_{a}x(t-a)w(a).
\]&lt;/p&gt;
&lt;p&gt;In machine learning, the kernel \(w\) is usually represented by some set of parameters that is optimized.&lt;/p&gt;
&lt;p&gt;CNNs for images use a 2D convolution defined as&lt;/p&gt;
&lt;p&gt;\[
(I * K)(i, j) = \sum_m \sum_n I(i-m, j-n)K(m, n).
\]&lt;/p&gt;
&lt;p&gt;In this formulation, the kernel is effectively flipped across the vertical and horizontal axis.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-05_19-09-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;2D Convolution (Image Credit: Song Ho Ahn (linked above)).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;2D Convolution (Image Credit: Song Ho Ahn (linked above)).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In practice, most deep learning APIs implement &lt;strong&gt;cross-correlation&lt;/strong&gt;.
Whether the function is implemented as true convolution makes no difference when it comes to optimizing a deep model since filter weights that are produced with cross-correlation would be produced, albeit flipped, with convolution.&lt;/p&gt;
&lt;p&gt;\[
(K * I)(i, j) = \sum_m \sum_n I(i+m, j+n)K(m, n).
\]&lt;/p&gt;
&lt;h2 id=&#34;properties-of-convolutions&#34;&gt;Properties of Convolutions&lt;/h2&gt;
&lt;p&gt;Convolutional networks are commonly built on &lt;em&gt;full&lt;/em&gt; or &lt;em&gt;valid&lt;/em&gt; convolutions. Other variants have also been explored. Here, we will briefly discuss the different properties of this operator. A more detailed treatment can be found in (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;padding&#34;&gt;Padding&lt;/h3&gt;
&lt;p&gt;By definition, a convolution of an input with a filter of size \(n\times n\) will produce an output of size \((m-n+1)\times(m-n+1)\), where \(m\) is the size of the input. This means that the output will be smaller than the input. This is often referred to as a &lt;strong&gt;valid&lt;/strong&gt; convolution. The figure below shows a convolution between a \(3\times3\) kernel and a \(5\times5\) input.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-26_16-31-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;A valid convolution (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;A valid convolution (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The output of this convolution is a \(3\times3\) feature map. This is a problem if we want to build a deep network. Each convolution will reduce the size of the input. If we were to stack multiple convolutional layers, the output would eventually be too small to be useful. If we want our output to be same size as the input, we can add padding to the original input image before convolving it. This is often known as a &lt;strong&gt;full&lt;/strong&gt; convolution. An example is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-26_16-34-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;A full convolution (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;A full convolution (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;stride&#34;&gt;Stride&lt;/h3&gt;
&lt;p&gt;So far, we have only looked at convolutions which step by 1 unit as they shift over the image. We can control the size of this step, or &lt;strong&gt;stride&lt;/strong&gt;, to produce different outcomes. Picking a non-unit stride has a number of effects on the features that are learned in a convolutional neural network.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dimensionality reduction&lt;/strong&gt;: Skipping over pixels reduces the size of the output feature map. This provides another way of downsampling the input.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Less computation&lt;/strong&gt;: Fewer computations are required to produce the output feature map.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Increased field of view&lt;/strong&gt;: A larger stride increases the field of view of the kernel, leading to larger receptive fields in deeper layers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given an input of size \(m\times m\) and a kernel of size \(n\times n\), the output size of a convolution with stride \(s\) is given by&lt;/p&gt;
&lt;p&gt;\[
\left\lfloor\frac{m-n}{s}\right\rfloor + 1.
\]&lt;/p&gt;
&lt;p&gt;The figure below shows a convolution with stride 2 on a \(5\times5\) input.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-26_16-45-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;A convolution with stride 2 (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;A convolution with stride 2 (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;kernel-size&#34;&gt;Kernel Size&lt;/h3&gt;
&lt;p&gt;The size of the kernel has a large impact on the features that are learned. A larger kernel will have a larger receptive field. This means that the kernel will be able to capture more information about the input. However, this comes at the cost of increased computation. Common kernel sizes in most CNNs are \(3\times3\), \(5\times5\), and \(7\times7\). It is also convenient to pick an odd kernel size so that the kernel has a center pixel.&lt;/p&gt;
&lt;h3 id=&#34;dilation&#34;&gt;Dilation&lt;/h3&gt;
&lt;p&gt;Around 2015, a research trend for CNNs was to find a way to increase the receptive field without adding more parameters. The result is a &lt;strong&gt;dilated&lt;/strong&gt; convolution. The output of a dilated convolution is computed by skipping over pixels in the input. The figure below shows a \(3\times3\) kernel with a dilation of 2.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-10-27_08-19-10_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;A dilated convolution (&amp;lt;a href=&amp;#34;#citeproc_bib_item_1&amp;#34;&amp;gt;Dumoulin and Visin 2018&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;A dilated convolution (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dumoulin and Visin 2018&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The output size is computed as&lt;/p&gt;
&lt;p&gt;\[
\left\lfloor\frac{m + 2p - n - (n-1)(d-1)}{s}\right\rfloor + 1,
\]&lt;/p&gt;
&lt;p&gt;where \(p\) is the amount of padding and \(d\) is the dilation factor.&lt;/p&gt;
&lt;h2 id=&#34;parameter-sharing&#34;&gt;Parameter Sharing&lt;/h2&gt;
&lt;p&gt;In a densely connected layer, each input has a corresponding weight attached to it.
For example, we ran a few &lt;a href=&#34;https://github.com/ajdillhoff/CSE6363/tree/main/deep_learning&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;introductory experiments&lt;/a&gt; on the CIFAR10 dataset using a deep, densely connected network.
To reduce the amount of parameters in the first layer, we converted each image to grayscale.
The input also had to be vectorized in order to be processed.
With a processed image of size \(32 \times 32\), this resulted in a \(1024\) dimensional vector for each input.
Our first layer had \(512\) nodes resulting in a parameter matrix of size \(1024 \times 512\).&lt;/p&gt;
&lt;p&gt;Convolution layers have &lt;strong&gt;shared parameters&lt;/strong&gt;, meaning the same parameters are used for each region on the input.
A single channel 2D filter of size \(n \times n\) only requires \(n \times n\) parameters.
Each kernel is applied to every location in the original input using the same parameters.&lt;/p&gt;
&lt;p&gt;Kernels are &lt;strong&gt;equivariant&lt;/strong&gt; to translation because of their shared parameters.
That is, as the input changes, the output will change in the same way.
Formally, two functions \(f\) and \(g\) are equivarient if&lt;/p&gt;
&lt;p&gt;\[
f(g(x)) = g(f(x)).
\]&lt;/p&gt;
&lt;p&gt;In the context of image features, a kernel applied across an image will produce strong responses in regions that exhibit the same local features.
For example, a kernel that detects horizontal lines will produce strong responses across all parts of the image that show a large contrast between vertical pixels.&lt;/p&gt;
&lt;h2 id=&#34;pooling&#34;&gt;Pooling&lt;/h2&gt;
&lt;p&gt;When a convolution is applied to some input image, the resulting output feature map represents the responses of the kernel applied to each location in the image.
If this original image were to be shifted by a few pixels, the reponses would also be shifted.
In order to increase the robustness of a model to small perturbations such as translation, a pooling layer was historically employed after each non-linear activation following a convolutional layer.&lt;/p&gt;
&lt;p&gt;They effectively provide a summary statistic of a local region by selecting the average or maximum responses in a small window. This provides translation invariance since the maximum response will be the same for a region even if it is translated by a small amount.
It also acts as a quick way to downsample the image, leading to fewer parameters in the model.&lt;/p&gt;
&lt;p&gt;Modern works do not employ pooling operations as often. For example (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;He et al. 2016&lt;/a&gt;) perform dimensionality reduction with \(1 \times 1\) convolutions.
(&lt;a href=&#34;#citeproc_bib_item_6&#34;&gt;Springenberg et al. 2015&lt;/a&gt;) argue that fully convolutional networks can achieve the same performance without max pooling.&lt;/p&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster.&amp;rdquo; - Geoffrey Hinton&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;h2 id=&#34;backwards-pass&#34;&gt;Backwards Pass&lt;/h2&gt;
&lt;p&gt;The parameters of a convolutional layer are updated via backpropagation like any other layer with trainable parameters.
Given a kernel \(w\), it is necessary to compute \(\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}}\), where \(w_{m&amp;rsquo;, n&amp;rsquo;}\) is the \((m&amp;rsquo;, n&amp;rsquo;)th\) entry of the kernel.
This entry affects all entries in the feature map, so \(\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}}\) will sum over all such entries.&lt;/p&gt;
&lt;p&gt;To show the gradient calculation, we will assume a convolutional layer with zero padding and unit stride with a square \(2 \times 2\) kernel applied to a square \(3 \times 3\) input.
The output map is then \((3 - 2 + 1) \times (3 - 2 + 1) = 2 \times 2\).&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}} = \sum_{i=0}^2 \sum_{j=0}^2 \frac{\partial \mathcal{L}}{\partial x_{i, j}} \frac{\partial x_{i,j}}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}}
\]&lt;/p&gt;
&lt;p&gt;If \(\mathbf{z}^{(l-1)}\) is the output from the previous layer, then&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{\partial x_{i, j}}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}} &amp;amp;= \frac{\partial}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}} \sum_{m} \sum_{n} w_{m, n} z_{i+m, j+n}^{(l-1)} + b\\
&amp;amp;= \frac{\partial}{\partial w_{m&amp;rsquo;, n&amp;rsquo;}} w_{m&amp;rsquo;, n&amp;rsquo;}z_{i+m&amp;rsquo;, j+n&amp;rsquo;}^{(l-1)}\\
&amp;amp;= z_{i+m&amp;rsquo;, j+n&amp;rsquo;}^{(l-1)}
\end{align*}&lt;/p&gt;
&lt;p&gt;Then \(\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}}\) becomes&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{\partial \mathcal{L}}{\partial w_{m&amp;rsquo;,n&amp;rsquo;}} &amp;amp;= \sum_{i=0}^2 \sum_{j=0}^2 \frac{\partial \mathcal{L}}{\partial x_{i, j}} z_{i+m&amp;rsquo;, j+n&amp;rsquo;}^{(l-1)}\\
&amp;amp;= \frac{\partial \mathcal{L}}{\partial x_{i, j}} * z_{m&amp;rsquo;, n&amp;rsquo;}^{(l-1)}.
\end{align*}&lt;/p&gt;
&lt;p&gt;\(\frac{\partial \mathcal{L}}{\partial x_{i, j}}\) represent the gradients with respect to the feature maps. To match the flipped kernel used in the forward pass, they are flipped in an opposite manner.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s train and evaluate a convolutional neural network on the OG network: LeNet5 (&lt;a href=&#34;#citeproc_bib_item_4&#34;&gt;LeCun et al. 1989&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;neural-networks-for-image-classification&#34;&gt;Neural Networks for Image Classification&lt;/h2&gt;
&lt;h3 id=&#34;ilsvrc&#34;&gt;ILSVRC&lt;/h3&gt;
&lt;p&gt;The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is the most popular image classification and object detection challenge starting in 2010. It now exists as the ILSVRC 2012-2017 challenge on &lt;a href=&#34;https://www.kaggle.com/c/imagenet-object-localization-challenge/overview/description&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Kaggle&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;alexnet&#34;&gt;AlexNet&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://code.google.com/archive/p/cuda-convnet/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://code.google.com/archive/p/cuda-convnet/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The network that arguably popuarlized deep learning by achieving a 37.5% top-1 and 17% top-5 error rate on the ILSVRC-2010 test set. This model performed significantly better than leading competitors (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Krizhevsky, Sutskever, and Hinton 2017&lt;/a&gt;).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-12_18-25-11_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;ILSVRC-2010 results reported by Krizhevsky et al.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;ILSVRC-2010 results reported by Krizhevsky et al.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This performance was based on many different insights and techniques including ReLU activations and dropout.
The authors stated in their original publication that the large capacity of the model is necessary to fully describe the diversity of objects in ImageNet.&lt;/p&gt;
&lt;h4 id=&#34;architecture&#34;&gt;Architecture&lt;/h4&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-12_18-35-38_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;AlexNet architecture (from Krizhevsky et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;AlexNet architecture (from Krizhevsky et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;AlexNet is made up of 5 convolutional layers followed by 3 fully-connected layers.
The outputs of the last layer are used as input to the softmax function.&lt;/p&gt;
&lt;p&gt;Each layer uses the ReLU activation function.&lt;/p&gt;
&lt;p&gt;\[
f(x) = \max(0, x)
\]&lt;/p&gt;
&lt;p&gt;The justification for switching to ReLU as opposed to sigmoid or tanh is the faster training times.
Experiments on smaller CNNs show that networks with ReLU reach 25% training error on CIFAR-10 six times faster than those with tanh activations.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-12_18-49-42_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Training loss over time using ReLU (solid) versus tanh (dotted) (from Krizhevsky et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Training loss over time using ReLU (solid) versus tanh (dotted) (from Krizhevsky et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Another benefit of ReLU activations is that they are less reliant on input normalization.
In a saturating activation function like tanh, large absolute values in the inputs will be clamped to either -1 or 1.
ReLU is unbounded above 0.
Networks can still train as long as some input is positive.
Local response normalization (LRN) is used after the first and second convolutional layers.&lt;/p&gt;
&lt;p&gt;The motivation behind LRN is taken from &lt;em&gt;lateral inhibition&lt;/em&gt; in neurobiology.
An overly excited neuron (one with a high response) can subdue or dampen the responses from local neighbors.
If all responses in a local region are uniformly large, which can happend since ReLU is unbounded, it will dampen them all.&lt;/p&gt;
&lt;p&gt;In practice, they showed that applying LRNs to their model reduced the top-1 and top-5 error rates by 1.4% and 1.2%, respectively.&lt;/p&gt;
&lt;h4 id=&#34;regularization&#34;&gt;Regularization&lt;/h4&gt;
&lt;p&gt;The entire network has 60 million parameters.
Even with so many parameters and training on a dataset with over 8 million images, their model overfits the training data quickly without the aid of regularization.
They employ both image translations and horizontal reflections.&lt;/p&gt;
&lt;p&gt;The use of translations is where the popular \(224 \times 224\) training size originated.
The original size of the images in the dataset is \(256 \times 256\).
To work with random translations without worrying about padding, they crop the final output to \(224 \times 224\).
The final output of the network extracts 5 \(224 \times 224\) patches from the test input and averages the network prediction made on each patch.&lt;/p&gt;
&lt;p&gt;Additionally, they alter the RGB intensities so that the network is less reliant on specific intensities and illumination for each object.
The intuition is that the identity of an object is invariant to lighting conditions.&lt;/p&gt;
&lt;p&gt;As a last form of regularization, they employ dropout in the first two fully-connected layers.&lt;/p&gt;
&lt;h4 id=&#34;training&#34;&gt;Training&lt;/h4&gt;
&lt;p&gt;They trained their model on a training set of 1.2 million images using two NVIDIA GTX 580 3GB GPUs.
They had to write their own optimized CUDA code for this since deep learning frameworks such as Tensorflow and PyTorch did not exist yet.
The training took ~6 days to pass 90 epochs.&lt;/p&gt;
&lt;h3 id=&#34;vgg&#34;&gt;VGG&lt;/h3&gt;
&lt;p&gt;Published in 2015, (&lt;a href=&#34;#citeproc_bib_item_5&#34;&gt;Simonyan and Zisserman 2015&lt;/a&gt;) explore how depth plays a role in convolutional neural networks.
They systematically increase the depth of the network while keep other hyperparameters fixed.
The filter sizes are also kept at \(3 \times 3\).&lt;/p&gt;
&lt;p&gt;Similar to (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Krizhevsky, Sutskever, and Hinton 2017&lt;/a&gt;), they use ReLU activations and in only one of their models to they employ Local Response Normalization.
They found that adding LRN to their model did not increase performance.
Instead, it only increased computation time and memory consumption.
Their models are summarized in the table below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-13_07-48-12_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Model configurations used (Simonyan and Zisserman).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Model configurations used (Simonyan and Zisserman).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The number of parameters for each network is 133 million, 133 million, 134 million, 138 million, and 144 million starting from A to E.&lt;/p&gt;
&lt;h3 id=&#34;googlenet&#34;&gt;GoogLeNet&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_14-18-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;The network-in-network architecture pairs perfectly with the Inception meme.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;The network-in-network architecture pairs perfectly with the Inception meme.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Proposed a 22-layer network architecture that has \(12 \times\) fewer parameters than (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Krizhevsky, Sutskever, and Hinton 2017&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The authors were already thinking about applications in mobile computing, where hardware limitations would require smaller networks that still perform well.&lt;/p&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;In this paper, we will focus on an efficient deep neural network architecture for computer vision, codenamed Inception, which derives its name from the Network in network paper by Lin et al. in conjunction with the famous “we need to go deeper” internet meme.&amp;rdquo; - Szegedy et al.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;It was apparent at the time that building larger networks would generally lead to better performance.
Adding more parameters leads to easier overfitting.
Bigger networks also mean more computation. If the goal is to adapt high quality networks into mobile computing, solutions would have to include more sophistication than simply adding more components.&lt;/p&gt;
&lt;h4 id=&#34;hebbian-learning&#34;&gt;Hebbian Learning&lt;/h4&gt;
&lt;p&gt;A linear increase in filters leads to a quadratic increase in computation.
If most filter parameters end up being close to 0, then this increase in model capacity is wasted.
One solution is to include sparsity in the network instead of having dense connections.
Szegedy et al. were motivated by the work of Arora et al., which they summarized as follows.&lt;/p&gt;
&lt;div class=&#34;blockquote&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Their main result states that if the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs.&amp;rdquo; - Szegedy et al.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;This result relates with &lt;a href=&#34;https://en.wikipedia.org/wiki/Hebbian_theory&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Hebbian theory&lt;/a&gt; on synaptic plasticity which is summarized as &amp;ldquo;neurons that fire together, wire together.&amp;rdquo;&lt;/p&gt;
&lt;h4 id=&#34;from-theory-to-architecture&#34;&gt;From Theory to Architecture&lt;/h4&gt;
&lt;p&gt;Motivated by sparse connections, the architecture is designed to approximate sparsity given current dense components like convolutional layers.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_14-52-12_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 11: &amp;lt;/span&amp;gt;Naive version of the Inception module (Szegedy et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;Naive version of the Inception module (Szegedy et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The Inception module design as seen above is motivated as follows.
In layers closer to the raw input, filters would be grouped into local regions.
In this case, a \(1 \times 1\) convolution would summarize these groups.&lt;/p&gt;
&lt;p&gt;For clusters that are spread out, a larger filter would be needed to cover the larger regions.
This motivates the use of \(3 \times 3\) and \(5 \times 5\) filters.&lt;/p&gt;
&lt;p&gt;The choice to include a max pooling function in each module is based on previous successes of using max pooling.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-01-46_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 12: &amp;lt;/span&amp;gt;Description of layers from Szegedy et al.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 12: &lt;/span&gt;Description of layers from Szegedy et al.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;vanishing-gradients&#34;&gt;Vanishing Gradients&lt;/h4&gt;
&lt;p&gt;Creating a deeper network means that training is more susceptible to the vanishing gradient problem.
They noted that shallower networks that perform well on image classification would surely provide strong disciminative features.
They leverage this idea by computing 2 additional intermediate outputs: one in the middle of the network and an additional output 3 layers beyond that one.
This permits the gradients to be strengthened by intermediate losses when combined with the original gradients.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-07-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 13: &amp;lt;/span&amp;gt;GoogLeNet model (Szegedy et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 13: &lt;/span&gt;GoogLeNet model (Szegedy et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;results&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;GoogLeNet took 1st place in the 2014 ILSVRC with a 6.67% top-5 error rate.&lt;/p&gt;
&lt;h3 id=&#34;resnet&#34;&gt;ResNet&lt;/h3&gt;
&lt;p&gt;By 2016, it was clear that deeper models could build a richer hierarchy of features leading to better performance on a wide range of computer vision tasks.
However, with deeper networks comes the vanishing gradient problem.
Training them remained difficult for a time, but initialization and other normalization techniques found ways to resolve this issue.&lt;/p&gt;
&lt;p&gt;With deeper networks, a new problem appeared.
Adding more layers generally results in higher accuracy.
At a certain point, adding additional layers leads to a decrease in accuracy.
Many experiments ruled out the possibility of overfitting by observing that the training error was increasing as well.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-19-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 14: &amp;lt;/span&amp;gt;Result of experiments showing that decreased accuracy was not a result of overfitting.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 14: &lt;/span&gt;Result of experiments showing that decreased accuracy was not a result of overfitting.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;identity-mappings&#34;&gt;Identity Mappings&lt;/h4&gt;
&lt;p&gt;Consider a shallow network with some measure performance on a task.
If we were to add additional layers to make this network deeper, but those layers were simply identity mappings, then we should expect an error no greater than the original shallow network.
However, current solvers are unable to find such a solution in a reasonable amount of time on an equally deep network optimized from a random initialization.&lt;/p&gt;
&lt;h4 id=&#34;residual-functions&#34;&gt;Residual Functions&lt;/h4&gt;
&lt;p&gt;The main idea of this paper is to attempt to learn a residual function \(\mathcal{F}(\mathbf{x}) := \mathcal{H}(\mathbf{x}) - \mathbf{x}\) of the desired mapping \(\mathcal{H}(\mathbf{x})\) rather than attempting to learn the mapping directly.
The desired mapping then given by \(\mathcal{H}(\mathbf{x}) = \mathcal{F}(\mathbf{x}) + \mathbf{x}\).
If it were optimal to learn an identity mapping, the idea is that it would be simpler to learn by moving towards a 0 residual.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-45-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 15: &amp;lt;/span&amp;gt;Residual unit (He et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 15: &lt;/span&gt;Residual unit (He et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The function can be implemented into neural networks by using skip connections, as seen in the figure above.
Adding these identity mappings does not require any additional parameters, as the input is simply passed to the end of the stack.&lt;/p&gt;
&lt;h4 id=&#34;architecture-complexity&#34;&gt;Architecture Complexity&lt;/h4&gt;
&lt;p&gt;They compare a 34-layer plain network based on the VGG-19 architecture with a 34-layer residual network.
They note that VGG-19 has more filters and higher complexity than their residual network.
Specifically, VGG-19 requires 19.6 billion FLOPs compared to only 3.6 billion for their 34-layer residual network.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-49-39_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 16: &amp;lt;/span&amp;gt;Comparison of architectures and their complexity (He et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 16: &lt;/span&gt;Comparison of architectures and their complexity (He et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;results&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;They evaluate how well the residual networks generalize when adding more layers.
As mentioned in the introduction, typical models would see an increase in training error as the number of layers were increased.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-04-14_15-53-18_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 17: &amp;lt;/span&amp;gt;Training comparisons between plain and residual networks (He et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 17: &lt;/span&gt;Training comparisons between plain and residual networks (He et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Their ensemble of models achieved 3.57% top-5 error on the ImageNet test set, achieving 1st place in the ILSVRC 2015 classification challenge.
It additionally was adapted to other challenges and won first place on ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in both the ILSVRC and COCO 2015 competitions.&lt;/p&gt;
&lt;h2 id=&#34;useful-resources&#34;&gt;Useful Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.songho.ca/dsp/convolution/convolution.html#convolution_2d&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.songho.ca/dsp/convolution/convolution.html#convolution_2d&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vdumoulin/conv_arithmetic&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://github.com/vdumoulin/conv_arithmetic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cs231n.github.io/convolutional-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://cs231n.github.io/convolutional-networks/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://grzegorzgwardys.wordpress.com/2016/04/22/8/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://grzegorzgwardys.wordpress.com/2016/04/22/8/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Dumoulin, Vincent, and Francesco Visin. 2018. “A Guide to Convolution Arithmetic for Deep Learning.” &lt;i&gt;Arxiv:1603.07285 [Cs, Stat]&lt;/i&gt;, January. &lt;a href=&#34;http://arxiv.org/abs/1603.07285&#34;&gt;http://arxiv.org/abs/1603.07285&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “Deep Residual Learning for Image Recognition.” In &lt;i&gt;2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/i&gt;, 770–78. Las Vegas, NV, USA: IEEE. &lt;a href=&#34;https://doi.org/10.1109/CVPR.2016.90&#34;&gt;https://doi.org/10.1109/CVPR.2016.90&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_3&#34;&gt;&lt;/a&gt;Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. “ImageNet Classification with Deep Convolutional Neural Networks.” &lt;i&gt;Communications of the Acm&lt;/i&gt; 60 (6): 84–90. &lt;a href=&#34;https://doi.org/10.1145/3065386&#34;&gt;https://doi.org/10.1145/3065386&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_4&#34;&gt;&lt;/a&gt;LeCun, Yann, Bernhard Boser, John Denker, Donnie Henderson, R. Howard, Wayne Hubbard, and Lawrence Jackel. 1989. “Handwritten Digit Recognition with a Back-Propagation Network.” In &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;. Vol. 2. Morgan-Kaufmann. &lt;a href=&#34;https://papers.nips.cc/paper/1989/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html&#34;&gt;https://papers.nips.cc/paper/1989/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_5&#34;&gt;&lt;/a&gt;Simonyan, Karen, and Andrew Zisserman. 2015. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” &lt;i&gt;Arxiv:1409.1556 [Cs]&lt;/i&gt;, April. &lt;a href=&#34;http://arxiv.org/abs/1409.1556&#34;&gt;http://arxiv.org/abs/1409.1556&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_6&#34;&gt;&lt;/a&gt;Springenberg, Jost Tobias, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. 2015. “Striving for Simplicity: The All Convolutional Net.” &lt;i&gt;Arxiv:1412.6806 [Cs]&lt;/i&gt;, April. &lt;a href=&#34;http://arxiv.org/abs/1412.6806&#34;&gt;http://arxiv.org/abs/1412.6806&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tracking</title>
      <link>https://ajdillhoff.github.io/notes/tracking/</link>
      <pubDate>Mon, 07 Mar 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/tracking/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tracking-with-optical-flow&#34;&gt;Tracking with Optical Flow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kalman-filters&#34;&gt;Kalman Filters&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Tracking features and objects is required in many applications ranging from autonomous driving to security. Vision tracking systems are often used for live sports broadcasts to keep track of players, the ball, and other visual queues related to the game.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;Introduction/2022-03-08_21-16-34_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Source: &amp;lt;https://azbigmedia.com/lifestyle/ball-tracking-technology-changes-way-fans-consume-practice-sport-of-golf/&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Source: &lt;a href=&#34;https://azbigmedia.com/lifestyle/ball-tracking-technology-changes-way-fans-consume-practice-sport-of-golf/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://azbigmedia.com/lifestyle/ball-tracking-technology-changes-way-fans-consume-practice-sport-of-golf/&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Naive tracking will detect an object per frame without any regard for prior information.
More sophisticated trackers will consider the previous frame as a starting point to their search space.
However, even these trackers many need to initialize after a certain amount of time if their estimate drifts too far away from the object&amp;rsquo;s actual location.&lt;/p&gt;
&lt;p&gt;Example of the importance of reliable tracking for driving assistance:
&lt;a href=&#34;https://youtu.be/NSDTZQdo6H8?t=898&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://youtu.be/NSDTZQdo6H8?t=898&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;tracking-with-optical-flow&#34;&gt;Tracking with Optical Flow&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Image motion&lt;/strong&gt; can be described as complex changes in image intensity from one time to another,
displaced by \(\delta\).&lt;/p&gt;
&lt;p&gt;The displacement can be modeled as an &lt;em&gt;affine motion field&lt;/em&gt; where the point is warped and translated by \(d\):&lt;/p&gt;
&lt;p&gt;\[
\delta = D\mathbf{x} + \mathbf{x},
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
D = \begin{bmatrix}
d_{xx} &amp;amp; d_{xy}\\
d_{yx} &amp;amp; d_{yy}
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;When comparing features between an image at \(t\) and \(t-1\), the feature centered at \(\mathbf{x}\) is transformed by&lt;/p&gt;
&lt;p&gt;\[
\textbf{I}_{t}(A\mathbf{x} + \mathbf{d}) = \textbf{I}_{t-1}(\mathbf{x}).
\]&lt;/p&gt;
&lt;p&gt;Here, \(A = I_2 + D\), where \(I_2\) is the \(2 \times 2\) identity matrix.
This addition will be explained later.&lt;/p&gt;
&lt;p&gt;Smaller variations between frames are less reliable for parameter estimation, so a pure translational model is better in these cases. That is&lt;/p&gt;
&lt;p&gt;\[
\delta = \mathbf{d}.
\]&lt;/p&gt;
&lt;h3 id=&#34;computing-image-motion&#34;&gt;Computing Image Motion&lt;/h3&gt;
&lt;p&gt;Computing image motion then becomes a minimization problem.&lt;/p&gt;
&lt;p&gt;\[
\epsilon = \int \int_{W} \big[\mathbf{I}_{t}(A\mathbf{x} + \mathbf{d}) - \mathbf{I}_{t-1}(\mathbf{x})\big]^2 w(\mathbf{x}) d\mathbf{x}
\]&lt;/p&gt;
&lt;p&gt;This is used in a minimization problem which will minimize the dissimilarity between the tracked features between frames.
The point is weighted by a function \(w\) over a window \(W\).&lt;/p&gt;
&lt;p&gt;Minimization of this error involves taking the derivative of \(\epsilon\) with respect to the unknowns in \(D\) and displacement vector \(\mathbf{d}\):&lt;/p&gt;
&lt;p&gt;\begin{align*}
\frac{1}{2}\frac{\partial \epsilon}{\partial D} &amp;amp;= \int \int_W \Big[\mathbf{I}_t(A\mathbf{x}+\mathbf{d}) - \mathbf{I}_{t-1}(\mathbf{x})\Big]\mathbf{g}\mathbf{x}^T w d\mathbf{x} = 0\\
\frac{1}{2}\frac{\partial \epsilon}{\partial \mathbf{d}} &amp;amp;= \int \int_W \Big[\mathbf{I}_t(A\mathbf{x}+\mathbf{d}) - \mathbf{I}_{t-1}(\mathbf{x})\Big]\mathbf{g} w d\mathbf{x} = 0\\
\end{align*}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\mathbf{g} = \bigg(\frac{\partial \mathbf{I}_t}{\partial x}, \frac{\partial \mathbf{I}_t}{\partial y}\bigg)^T
\]&lt;/p&gt;
&lt;p&gt;is this spatial gradient of the image intensity.
We have computed these image gradients before!&lt;/p&gt;
&lt;p&gt;To linearlize \(\mathbf{I}_{t}\), a Taylor series expansion of it can be used, taking just the linear term:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{I}_t(A\mathbf{x}+\mathbf{d}) = \mathbf{I}_t(\mathbf{x})+\mathbf{g}^T(\mathbf{u}),
\]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\mathbf{u} = D\mathbf{x}+\mathbf{d}.
\]&lt;/p&gt;
&lt;p&gt;The authors argue that this approximation is reasonable assuming the motion in the images is small.&lt;/p&gt;
&lt;p&gt;Plugging this back into the derivatives above yields&lt;/p&gt;
&lt;p&gt;\begin{align*}
\int \int_W \mathbf{g}\mathbf{x}^T(\mathbf{g}^T\mathbf{u})w d\mathbf{x} &amp;amp;= \int \int_W \big[\mathbf{I}_{t-1}(\mathbf{x}) - \mathbf{I}_t\big]\mathbf{g}\mathbf{x}^T w d\mathbf{x}\\
\int \int_W \mathbf{g}(\mathbf{g}^T\mathbf{u})w d\mathbf{x} &amp;amp;= \int \int_W \big[\mathbf{I}_{t-1}(\mathbf{x}) - \mathbf{I}_t\big]\mathbf{g} w d\mathbf{x}\\
\end{align*}&lt;/p&gt;
&lt;p&gt;This is solved iteratively, following the Newton method, starting with the following values at \(t=0\):&lt;/p&gt;
&lt;p&gt;\begin{align*}
D_0 &amp;amp;= I\\
\mathbf{d}_0 &amp;amp;= \mathbf{0}\\
\mathbf{I}_0 &amp;amp;= \mathbf{I}(\mathbf{x}).
\end{align*}&lt;/p&gt;
&lt;p&gt;At step \(i\) the values are updated to&lt;/p&gt;
&lt;p&gt;\begin{align*}
D_i\\
\mathbf{d}_i\\
\mathbf{I}_i &amp;amp;= \mathbf{I}_{i-1}(A_i \mathbf{x} + \mathbf{d}_i).
\end{align*}&lt;/p&gt;
&lt;h3 id=&#34;a-more-compact-representation&#34;&gt;A More Compact Representation&lt;/h3&gt;
&lt;p&gt;At this point, the authors convert this representation into a more compact form in which the unknowns in \(D\) and the values of \(\mathbf{d}\) are separated from the function. To achieve this, we start with our current system of equations.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\int \int_W \mathbf{g}\mathbf{x}^T(\mathbf{g}^T\mathbf{u})w d\mathbf{x} &amp;amp;= \int \int_W \big[\mathbf{I}_{t-1}(\mathbf{x}) - \mathbf{I}_t(\mathbf{x})\big]\mathbf{g}\mathbf{x}^T w d\mathbf{x}\\
\int \int_W \mathbf{g}(\mathbf{g}^T\mathbf{u})w d\mathbf{x} &amp;amp;= \int \int_W \big[\mathbf{I}_{t-1}(\mathbf{x}) - \mathbf{I}_t(\mathbf{x})\big]\mathbf{g} w d\mathbf{x}\\
\end{align*}&lt;/p&gt;
&lt;p&gt;To achieve this, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Kronecker_product&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Kronecker product&lt;/a&gt; is used. This is a generalization of the outer product from vectors to matrices.
For two matrices \(A \in \mathbb{R}^{p \times q}\) and \(B \in \mathbf{R}^{m \times n}\), \(A \otimes B\) is a \(p \times q\) block matrix&lt;/p&gt;
&lt;p&gt;\[
A \otimes B = \begin{bmatrix}
a_{11}B &amp;amp; \cdots &amp;amp; a_{1q}\\
\vdots &amp;amp; \ddots &amp;amp; \vdots\\
a_{p1}B &amp;amp; \cdots &amp;amp; a_{pq}B
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;It has two particularly useful properties for this problem:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;\(A^T \otimes B^T = (A \otimes B)^T\)&lt;/li&gt;
&lt;li&gt;\(v(AXB) = (B^T \otimes A)v(X)\), where \(v\) is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Vectorization_%28mathematics%29&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;vectorization&lt;/a&gt; operator.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Using this product and the properties just listed, we can extract the unknowns \(D\) and \(\mathbf{d}\) from the equations above.&lt;/p&gt;
&lt;p&gt;First, note that \(\mathbf{g}^T \mathbf{u}\) appear in both of the equations. These can then be rewritten as follows.&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{g}^T \mathbf{u} &amp;amp;= \mathbf{g}^T(D\mathbf{x} + \mathbf{d})\\
&amp;amp;= \mathbf{g}^T D \mathbf{x} + \mathbf{g}^T \mathbf{d}\\
&amp;amp;= v(\mathbf{g}^T D \mathbf{x}) + \mathbf{g}^T \mathbf{d} &amp;amp;\text{ vectorization }\\
&amp;amp;= (\mathbf{x}^T \otimes \mathbf{g}^T)v(D) + \mathbf{g}^T \mathbf{d} &amp;amp;\text{ property 2 }\\
&amp;amp;= (\mathbf{x} \otimes \mathbf{g})^T v(D) + \mathbf{g}^T \mathbf{d} &amp;amp;\text{ property 1 }
\end{align*}&lt;/p&gt;
&lt;p&gt;Likewise, the term \(\mathbf{g}\mathbf{x}^T\) that appears on the right side of the first equation in our set to solve can be written as&lt;/p&gt;
&lt;p&gt;\begin{align*}
v(\mathbf{g}\mathbf{x}^T) &amp;amp;= v(\mathbf{g}1\mathbf{x}^T)\\
&amp;amp;= \mathbf{x} \otimes \mathbf{g}.
\end{align*}&lt;/p&gt;
&lt;p&gt;Plugging this into the first equation to solve from above yields&lt;/p&gt;
&lt;p&gt;\[
\int \int_W (\mathbf{x} \otimes \mathbf{g})((\mathbf{x} \otimes \mathbf{g})^T v(D) + \mathbf{g}^T\mathbf{d})w d\mathbf{x} = \int \int_W \big[\mathbf{I}_{t-1}(\mathbf{x}) - \mathbf{I}_t(\mathbf{x})\big](\mathbf{x} \otimes \mathbf{g}) w d\mathbf{x}.
\]&lt;/p&gt;
&lt;p&gt;Expanding these terms out produces&lt;/p&gt;
&lt;p&gt;\begin{align*}
\bigg(\int \int_W (\mathbf{x} \otimes \mathbf{g})(\mathbf{x} \otimes \mathbf{g})^T w d\mathbf{x}\bigg) v(D) + \bigg(\int \int_W (\mathbf{x} \otimes \mathbf{g}) \mathbf{g}^T w d\mathbf{x}\bigg) \mathbf{d}\\
= \int \int_W \big[\mathbf{I}_{t-1}(\mathbf{x}) - \mathbf{I}_t(\mathbf{x})\big](\mathbf{x} \otimes \mathbf{g}) w d\mathbf{x}.
\end{align*}&lt;/p&gt;
&lt;p&gt;The authors further simplify this equation using the following variables:&lt;/p&gt;
&lt;p&gt;\begin{align*}
U(\mathbf{x}) &amp;amp;= (\mathbf{x} \otimes \mathbf{g})(\mathbf{x} \otimes \mathbf{g})^T\\
V(\mathbf{x}) &amp;amp;= (\mathbf{x} \otimes \mathbf{g})\mathbf{g}^T\\
\mathbf{b}(\mathbf{x}) &amp;amp;= \big[\mathbf{I}_{t-1}(\mathbf{x}) - \mathbf{I}_{t}(\mathbf{x})\big]v(\mathbf{g}\mathbf{x}^T).
\end{align*}&lt;/p&gt;
&lt;p&gt;Then the above equation can be written as&lt;/p&gt;
&lt;p&gt;\[
\bigg(\int \int_W U(\mathbf{x}) w d\mathbf{x}\bigg) v(D) + \bigg(\int \int_W V(\mathbf{x}) w d\mathbf{x}\bigg) \mathbf{d} = \int \int_W \mathbf{b}(\mathbf{x}) w d\mathbf{x}.
\]&lt;/p&gt;
&lt;p&gt;To write the second equation in a similar way, the authors introduce two additional variables&lt;/p&gt;
&lt;p&gt;\begin{align*}
Z(\mathbf{x}) &amp;amp;= \mathbf{g}\mathbf{g}^T\\
\mathbf{x}(\mathbf{x}) &amp;amp;= \big[\mathbf{I}_{t-1}(\mathbf{x}) - \mathbf{I}_t(\mathbf{x})\big]\mathbf{g}.
\end{align*}&lt;/p&gt;
&lt;p&gt;Then,&lt;/p&gt;
&lt;p&gt;\[
\bigg(\int \int_W V^T(\mathbf{x}) w d\mathbf{x}\bigg) v(D) + \bigg(\int \int_W Z(\mathbf{x}) w d\mathbf{x}\bigg) \mathbf{d} = \int \int_W \mathbf{c}(\mathbf{x}) w d\mathbf{x}.
\]&lt;/p&gt;
&lt;p&gt;These equations can be written in a simple form: \(A\mathbf{x} + B\mathbf{y} = \mathbf{z}\).
A symmetric block matrix \(T \in \mathbb{R}^{6 \times 6}\) is then introduced:&lt;/p&gt;
&lt;p&gt;\begin{align*}
T &amp;amp;= \int \int_W \begin{bmatrix}
U &amp;amp; V\\
V^T &amp;amp; Z
\end{bmatrix} w d\mathbf{x}\\
&amp;amp;= \int \int_W \begin{bmatrix}
x^2g_x^2 &amp;amp; x^2 g_x g_y &amp;amp; x y g_x^2 &amp;amp; x y g_x g_y &amp;amp; x g_x^2 &amp;amp; x g_x g_y\\
x^2 g_x g_y &amp;amp; x^2 g_y^2 &amp;amp; x y g_x g_y &amp;amp; x y g_y^2 &amp;amp; x g_x g_y &amp;amp; x g_y^2\\
x y g_x^2 &amp;amp; x y g_x g_y &amp;amp; y^2 g_x^2 &amp;amp; y^2 g_x g_y &amp;amp; y g_x^2 &amp;amp; y g_x g_y\\
x y g_x g_y &amp;amp; x y g_y^2 &amp;amp; y^2 g_x g_y &amp;amp; y^2 g_y^2 &amp;amp; y g_x g_y &amp;amp; y g_y^2\\
x g_x^2 &amp;amp; x g_x g_y &amp;amp; y g_x^2 &amp;amp; y g_x g_y &amp;amp; g_x^2 &amp;amp; g_x g_y\\
x g_x g_y &amp;amp; x g_y^2 &amp;amp; y g_x g_y &amp;amp; y g_y^2 &amp;amp; g_x g_y &amp;amp; g_y^2
\end{bmatrix}w d \mathbf{x}.
\end{align*}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Look very closely at \(Z\)&lt;/strong&gt;. Does that remind you of anything?
Harris corner detection!&lt;/p&gt;
&lt;p&gt;The unknowns are vectorized as&lt;/p&gt;
&lt;p&gt;\[
\mathbf{z} = \begin{bmatrix}
v(D)\\
\mathbf{d}
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;The product vector is defined as&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{a} &amp;amp;= \int \int_W \begin{bmatrix}
\mathbf{b}\\
\mathbf{c}
\end{bmatrix} w d \mathbf{x}\\
&amp;amp;= \int \int_W \Big[\mathbf{I}_{t-1}(\mathbf{x}) - \mathbf{I}_t(\mathbf{x})\Big]\begin{bmatrix}
x g_x\\
x g_y\\
y g_x\\
y g_y\\
g_x\\
g_y
\end{bmatrix} w d \mathbf{x}.
\end{align*}&lt;/p&gt;
&lt;p&gt;Thus, the iterative solution requires solving the \(6 \times 6\) linear system&lt;/p&gt;
&lt;p&gt;\[
T \mathbf{z} = \mathbf{a}.
\]&lt;/p&gt;
&lt;h3 id=&#34;back-to-computing-image-motion&#34;&gt;Back to Computing Image Motion&lt;/h3&gt;
&lt;p&gt;With this more compact representation, the iterative solution is easier to achieve.
The authors conveniently note that the deformation of the feature window between frames will be relatively small, so \(D\) could be set to 0 for tracking.
This leads to the solution of a much smaller system for each time step:&lt;/p&gt;
&lt;p&gt;\[
Z \mathbf{d} = \begin{bmatrix}
g_x\\
g_y
\end{bmatrix}.
\]&lt;/p&gt;
&lt;p&gt;Note that this is only true for small steps between frames.
It is also important to measure the dissimilarity between the feature at the initial frame as it changes over time with the iterative estimates.
If it changes too much, the dissimilarity will be high, indicating that it is no longer a reliable feature to track.&lt;/p&gt;
&lt;h3 id=&#34;picking-the-best-feature&#34;&gt;Picking the Best Feature&lt;/h3&gt;
&lt;p&gt;Shi and Tomasi posit that the best feature is one that can be tracked well.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We can track a window from frame to frame if this system represents good measurements, and if it can be solved reliably.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;They analyze the basic equation \(Z \mathbf{d} = \mathbf{e}\), which is solved during tracking.
If both eigenvalues of \(Z\) are large and do not differ by several orders of magnitude, the feature can be tracked reliably.
That is, they accept a window if the eigenvalues of \(Z\) satisfy&lt;/p&gt;
&lt;p&gt;\[
\min(\lambda_1, \lambda_2) &amp;gt; \lambda.
\]&lt;/p&gt;
&lt;p&gt;In practice, \(\lambda\) is determined by selecting a lower bound based on a region of uniform brightness in the image as well as an upper bound based on features such as corners.
The selected value of \(\lambda\) is somewhere in between.&lt;/p&gt;
&lt;h3 id=&#34;measuring-dissimilarity&#34;&gt;Measuring dissimilarity&lt;/h3&gt;
&lt;p&gt;To determine if a feature is still reliable over a longer time period, a measure of dissimilarity is used to measure the original feature versus its warped version at the current frame.
Consider the sequence below over 21 frames.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-08_20-53-48_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Source: Shi and Tomasi.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Source: Shi and Tomasi.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Their method successfully tracks the speed limit sign, as seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-08_20-54-43_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Source: Shi and Tomasi.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Source: Shi and Tomasi.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-08_20-57-23_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Source: Shi and Tomasi.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Source: Shi and Tomasi.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;They note the importance of using an affine deformation to track reliable features.
The figure below plots the dissimilarity using translation versus the deformation matrix over time.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-08_20-56-09_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Dissimilarity over time using translation (dashed) versus affine (solid) (Shi and Tomasi).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Dissimilarity over time using translation (dashed) versus affine (solid) (Shi and Tomasi).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;They also present a case when the feature is lost to occlusion, thus the dissimilarity of both approaches increases greatly over time.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-08_20-59-27_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Source: Shi and Tomasi.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Source: Shi and Tomasi.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-08_20-59-09_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Source: Shi and Tomasi.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Source: Shi and Tomasi.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-08_20-59-52_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Sign tracking (plusses) versus window tracking (circles) (Shi and Tomasi).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Sign tracking (plusses) versus window tracking (circles) (Shi and Tomasi).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;kalman-filters&#34;&gt;Kalman Filters&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;Kalman filter&lt;/strong&gt; is a linear dynamic model that models conditional probabilities following normal distributions.
It is an simple and effective model for tracking motion even in the presence of noise from measurements.
Kalman filters keep an estimate of the state and can update their estimates based on the given observations.&lt;/p&gt;
&lt;p&gt;\(\mathbf{X}_i\) - State of object at step \(i\)&lt;/p&gt;
&lt;p&gt;\(\mathbf{Y}_i\) - Measurement at step \(i\)&lt;/p&gt;
&lt;p&gt;There are two primary tasks to deal with, the real-time tracking task and the offline smoothing task.
We are more interested in the tracking task, so we will focus on that.&lt;/p&gt;
&lt;p&gt;Tracking task: \(P(X_k|Y_0, \dots, Y_k)\)&lt;/p&gt;
&lt;p&gt;Smoothing task: \(P(X_k|X_0, \dots, Y_n)\)&lt;/p&gt;
&lt;p&gt;Assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(P(Y_k|X_0, \dots, X_N, Y_0, \dots, Y_N) = P(Y_k|X_k)\)&lt;/li&gt;
&lt;li&gt;\(P(X_k|X_0, \dots, X_{k-1}) = P(X_k|X_{k-1})\)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;prediction-task&#34;&gt;Prediction Task&lt;/h3&gt;
&lt;p&gt;When tracking an object, we use the model to first predict a state and then update its current parameters given some measurement.
We want to predict the state given measurements: \(P(\mathbf{X}_i|\mathbf{Y}_0 = \mathbf{y}_0, \dots, \mathbf{Y}_{k-1}=\mathbf{y}_{k-1})\).&lt;/p&gt;
&lt;p&gt;Given the previous observations up to \(k-1\), what is our model&amp;rsquo;s estimate of the current state?
This can help establish a search location if we were looking to narrow our detection algorithm.&lt;/p&gt;
&lt;h3 id=&#34;correction&#34;&gt;Correction&lt;/h3&gt;
&lt;p&gt;\(P(\mathbf{X}_i|\mathbf{Y}_0 = \mathbf{y}_0, \dots, \mathbf{Y}_{i}=\mathbf{y}_{i})\) is the cur  rent distribution.
This is the estimate given the actual observation at \(i\).
Note that this observation could be given from a noisy measurement.&lt;/p&gt;
&lt;h3 id=&#34;linear-dynamics&#34;&gt;Linear Dynamics&lt;/h3&gt;
&lt;p&gt;Assuming linear models, the problem becomes much simpler.
We can model the observations and state using normal distributions.&lt;/p&gt;
&lt;p&gt;\[
\mathbf{x} \sim \mathcal{N}(\mathbf{\mu}, \Sigma)
\]&lt;/p&gt;
&lt;p&gt;The measurements themselves can be modeled as&lt;/p&gt;
&lt;p&gt;\[
\mathbf{y}_k \sim \mathcal{N}(\mathcal{B}_k \mathbf{x}_k, \Sigma_k),
\]&lt;/p&gt;
&lt;p&gt;where \(k\) is the current step.&lt;/p&gt;
&lt;p&gt;For the future, the model will be represented as&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{x}_i &amp;amp;\sim \mathcal{N}(\mathcal{D}_i \mathbf{x}_{i-1}; \Sigma_{d_i})\\
\mathbf{y}_i &amp;amp;\sim \mathcal{N}(\mathcal{M}_i \mathbf{x}_i; \Sigma_{m_i}).
\end{align*}&lt;/p&gt;
&lt;p&gt;For covering the algorithm, we will use notation following Forsyth and Ponce.
\(\bar{\mathbf{x}}_i^-\) is the mean of \(P(\mathbf{x}_i|y_0, \dots, y_{i-1})\) and \(\bar{\mathbf{x}}_i^+\) is the mean of \(P(\mathbf{x}_i|y_0, \dots, y_{i})\).
\(\Sigma_i^-\) and \(\Sigma_i^+\) are the covariances of those distributions.&lt;/p&gt;
&lt;p&gt;However, by making a convenient assumption about the observation model, we will mainly need to focus on the state model.
That is, the matrix \(\mathcal{M}_i\) is defined so that the mean is simply the state position at step \(i\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What does our state represent?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In a simple model, \(\mathbf{x} = \begin{bmatrix}p_x\\p_y\\v_x\\v_y\end{bmatrix}\).
That is the 2D position and velocity of the object being tracked.
The corresponding covariance matrix is \(\mathbf{x}\mathbf{x}^T\).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do we predict the position and velocity of the next time step?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{p}_k &amp;amp;= \mathbf{p}_{k-1} + \Delta t \mathbf{v}_{k-1}\\
\mathbf{v}_k &amp;amp;= \mathbf{v}_{k-1}
\end{align*}&lt;/p&gt;
&lt;p&gt;This is making a simple, yet surprisingly effective, assumption that the velocity is constant.
We can write this as a matrix vector product:&lt;/p&gt;
&lt;p&gt;\[
\mathbf{x}_k = \begin{bmatrix}
1 &amp;amp; 0 &amp;amp; \Delta t &amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \Delta t\\
0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\mathbf{x}_{k-1}
\]&lt;/p&gt;
&lt;p&gt;Compactly, the prediction for step \(t\) is \(\bar{\mathbf{x}}_k^- = D_i \bar{\mathbf{x}}_{k-1}^+\).&lt;/p&gt;
&lt;p&gt;Since we updated every point \(\mathbf{x}_{k-1}\), we also need to make a prediction about the covariance matrix.
This is also achieved by multiplying every point by \(D_i\)&lt;/p&gt;
&lt;p&gt;\[
\Sigma_{i}^- = D_i\Sigma_{i-1}^+D_i^T
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if we want to add additional knowledge like acceleration?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Our position prediction then follows&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{p}_i &amp;amp;= \mathbf{p}_{i-1} + \Delta t \mathbf{v}_{i-1} + \frac{1}{2} \Delta t^2 \mathbf{a}_{i-1}\\
\mathbf{v}_i &amp;amp;= \mathbf{v}_{i-1} + \Delta t \mathbf{a}_{i-1}
\end{align*}&lt;/p&gt;
&lt;p&gt;We can simplify this and assume constant acceleration, then \(\mathbf{a}_i = \mathbf{a}_{i-1}\).
The resulting update equation for \(\mathbf{x}_k\) becomes&lt;/p&gt;
&lt;p&gt;\[
\mathbf{x}_k = \begin{bmatrix}
1 &amp;amp; 0 &amp;amp; \Delta t &amp;amp; 0 &amp;amp; \frac{1}{2}\Delta t^2 \mathbf{a} &amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \Delta t &amp;amp; 0 &amp;amp; \frac{1}{2}\Delta t^2 \mathbf{a}\\
0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \Delta t &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \Delta t\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1\\
\end{bmatrix}
\mathbf{x}_{k-1}
\]&lt;/p&gt;
&lt;p&gt;However, if something like acceleration is a known &lt;strong&gt;control&lt;/strong&gt; factor in our system, then it does not need to be part of the state.
In this case, we could separate the update vector into&lt;/p&gt;
&lt;p&gt;\[
\mathbf{x}_k = \mathcal{D}_i\mathbf{x}_{k-1} + B_k \mathbf{u}_{k},
\]&lt;/p&gt;
&lt;p&gt;where \(B_k\) is the &lt;strong&gt;control matrix&lt;/strong&gt; and \(\mathbf{u}_k\) is the &lt;strong&gt;control vector&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;One last consideration is that of uncertainty due to factors outside of our system.
This can also be modeled using a Gaussian with zero mean and covariance \(\Sigma_d\):&lt;/p&gt;
&lt;p&gt;\[
\xi_k \sim \mathcal{N}(\mathbf{0}, \Sigma_d).
\]&lt;/p&gt;
&lt;p&gt;With this added noise, the prediction step becomes&lt;/p&gt;
&lt;p&gt;\begin{align*}
\bar{\mathbf{x}}_k^- &amp;amp;= \mathcal{D}_k\bar{\mathbf{x}}_{k-1}^+ + B_k \mathbf{u}_k\\
\Sigma_k^- &amp;amp;= \mathcal{D}_k \Sigma_{k-1}^+\mathcal{D}_k^T + \xi_k.
\end{align*}&lt;/p&gt;
&lt;p&gt;In words, \(\bar{\mathbf{x}}_k^-\) is the prediction of our current state based on the previous best estimate with an added correction term based on known factors (acceleration).
\(\Sigma_k^-\) is the updated uncertainty based on the old uncertainty with added Gaussian noise to reflect unknown factors.&lt;/p&gt;
&lt;h3 id=&#34;making-corrections&#34;&gt;Making Corrections&lt;/h3&gt;
&lt;p&gt;As we are tracking an object, we may have some way of getting measurements.
This could be through an object detector or other physical sensor.
The new measurement can refine our current set of parameters.
Kalman filters work well even if the measurement is noisy.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-10_22-34-57_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;1D Kalman Filter. Source: Forsyth and Ponce&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;1D Kalman Filter. Source: Forsyth and Ponce
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the figure above, the initial prediction has a large amount of uncertainty.
After updating with the current measurement, the uncertainty is reduced.&lt;/p&gt;
&lt;p&gt;The goal here is to reconcile the uncertainty of our predicted state with the uncertainty of the measurement.
That is, we want to know the distribution over the union of these two distributions.
This is achieved by multiplying the Gaussians together.&lt;/p&gt;
&lt;p&gt;\[
\mathcal{N}(\bar{\mathbf{x}}_i^+, \Sigma_i^+) = \mathcal{N}(\bar{\mathbf{x}}_i^-, \Sigma_i^-) * \mathcal{N}(\mathbf{y}_i, \Sigma_{m_i})
\]&lt;/p&gt;
&lt;p&gt;Solving for \(\bar{\mathbf{x}}_i^+\) and \(\Sigma_i^+\) yields&lt;/p&gt;
&lt;p&gt;\begin{align*}
\bar{\mathbf{x}}_i^+ &amp;amp;= \bar{\mathbf{x}}_i^- + \mathcal{K}_i(\mathbf{y}_i - \bar{\mathbf{x}}_i^-)\\
\Sigma_i^+ &amp;amp;= \Sigma_i^- - \mathcal{K}_i \Sigma_i^-,
\end{align*}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\mathcal{K}_i = \Sigma_i^-(\Sigma_i^- + \Sigma_{m_i})^{-1}.
\]&lt;/p&gt;
&lt;p&gt;\(\mathcal{K}_i\) is called the &lt;strong&gt;Kalman gain&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Let&amp;rsquo;s combine the prediction and correction steps&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We have a state distribution with mean and variance&lt;/p&gt;
&lt;p&gt;\begin{align*}
\bar{\mathbf{x}}_k^- &amp;amp;= \mathcal{D}_k\bar{\mathbf{x}}_{k-1}^+ + B_k \mathbf{u}_k\\
\Sigma_k^- &amp;amp;= \mathcal{D}_k \Sigma_{k-1}^+\mathcal{D}_k^T + \xi_k
\end{align*}&lt;/p&gt;
&lt;p&gt;as well as an observation distribution with mean and variance \(\mathbf{y}_k\) and \(\Sigma_{m_k}\).&lt;/p&gt;
&lt;p&gt;Plugging these into the update equations yield&lt;/p&gt;
&lt;p&gt;\begin{align*}
\bar{\mathbf{x}}_i^+ &amp;amp;= \mathcal{D}_k\bar{\mathbf{x}}_{k-1}^+ + \mathcal{K}_i(\mathbf{y}_i - \mathcal{D}_k\bar{\mathbf{x}}_{k-1}^+)\\
\Sigma_i^+ &amp;amp;= \mathcal{D}_k \Sigma_{k-1}^+\mathcal{D}_k^T - \mathcal{K}_i \mathcal{D}_k \Sigma_{k-1}^+\mathcal{D}_k^T,
\end{align*}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\[
\mathcal{K}_i = \mathcal{D}_k \Sigma_{k-1}^+\mathcal{D}_k^T(\mathcal{D}_k \Sigma_{k-1}^+\mathcal{D}_k^T + \Sigma_{m_i})^{-1}.
\]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optical Flow</title>
      <link>https://ajdillhoff.github.io/notes/optical_flow/</link>
      <pubDate>Sun, 06 Mar 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/optical_flow/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#motion-features&#34;&gt;Motion Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#computing-optical-flow&#34;&gt;Computing Optical Flow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#assumptions-of-small-motion&#34;&gt;Assumptions of Small Motion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Optical flow refers to the apparent motion in a 2D image.
A motion field refers to the true motion of objects in 3D.
The idea of optical flow features is a simple concept.
If a fixed camera records a video of someone walking from the left side of the screen to the right, a difference of two consecutive frames reveals much about the apparent motion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TODO&lt;/strong&gt; Frame differencing figure&lt;/p&gt;
&lt;p&gt;Consider a sphere with Lambertian reflectance.&lt;/p&gt;
&lt;p&gt;If the sphere is rotated:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What does the motion field look like?&lt;/li&gt;
&lt;li&gt;What does the optical flow look like?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If, instead, a point light is rotated around the sphere:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What does the motion field look like?&lt;/li&gt;
&lt;li&gt;What does the optical flow look like?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;motion-features&#34;&gt;Motion Features&lt;/h2&gt;
&lt;p&gt;Motion features are used a wide variety of tasks from compression, image segmentation, tracking, detection, video de-noising, and more.
Consider a fixed camera with a sphere centered in the middle of the frame.
As the frame moves towards the camera, its apparent size will become larger.
If we were to analyze the optical flow of such a sequence, we would see that the flow is radial as the sphere, projected as a circle, grows.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-06_17-16-16_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;From &amp;#34;Computer Vision - A Modern Approach&amp;#34; by Forsyth and Ponce&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;From &amp;ldquo;Computer Vision - A Modern Approach&amp;rdquo; by Forsyth and Ponce
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;At \(t=1\), the radius of the circle is given as \(R\).
At \(t=2\), the radius is \(r = f\frac{R}{Z}\), where \(f\) is some function of the motion and \(Z\) is the distance between the sphere and the camera.
From this, we can also compute the speed at which the sphere is travelling towards the camera as \(V=\frac{dZ}{dt}\).
The apparent rate of growth as observed by the camera is \(\frac{dr}{dt} = -f\frac{RV}{Z^2}\).
We can also determine the time to contact with the camera as \(-\frac{Z}{V}\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-06_18-06-27_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Optical flow on different parts of the image as observed from a moving camera whose direction of focus is perpendicular to the white plane. Source: &amp;#34;Computer Vision - A Modern Approach&amp;#34; by Forsyth and Ponce&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Optical flow on different parts of the image as observed from a moving camera whose direction of focus is perpendicular to the white plane. Source: &amp;ldquo;Computer Vision - A Modern Approach&amp;rdquo; by Forsyth and Ponce
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the figure above, the observer is moving at a constant rate to the left.
The points in the image appear to translate towards the right edge of the frame.
Points that are close to the camera appear to move faster than those that are farther away.
This can be used to estimate the apparent depth between objects in a scene.&lt;/p&gt;
&lt;h2 id=&#34;computing-optical-flow&#34;&gt;Computing Optical Flow&lt;/h2&gt;
&lt;p&gt;A popular assumption for optical flow, as discussed in &amp;lt;&amp;amp;hornDeterminingOpticalFlow1980&amp;gt;, is that of brightness constancy.
That is, a local feature has the same image intensity in one frame as it does in the subsequent frame.&lt;/p&gt;
&lt;p&gt;\[
I(x + u, y + v, t + 1) = I(x, y, t)
\]&lt;/p&gt;
&lt;p&gt;To formulate the problem as a differentiable function, an additional spatial smoothness assumption is made.
It is assumed that a pixel&amp;rsquo;s neighbors will also have the same optical flow as the pixel in question.
This is a reasonable assumption that asserts that patches are locally rigid and will exhibit uniform motion.&lt;/p&gt;
&lt;p&gt;Given these two constraints, we can formulate an objective function.
First, the objective function assuming brightness constancy is given by&lt;/p&gt;
&lt;p&gt;\[
E_D(\mathbf{u}, \mathbf{v}) = \sum_{s}(I(x_s + u_s, y_s + v_s, t + 1) - I(x, y, t))^2.
\]&lt;/p&gt;
&lt;p&gt;Adding in the assumption of uniform motion in a local region yields the term&lt;/p&gt;
&lt;p&gt;\[
E_S(\mathbf{u}, \mathbf{v}) = \sum_{n\in G(s)}(u_s - u_n)^2 + \sum_{n \in G(s)}(v_s - v_n)^2.
\]&lt;/p&gt;
&lt;p&gt;Putting these together, with a weighting term, yields&lt;/p&gt;
&lt;p&gt;\[
E(\mathbf{u}, \mathbf{v}) = E_D + \lambda E_S.
\]&lt;/p&gt;
&lt;p&gt;This energy function \(E_D\) is minimized by differentiating and setting the equation to 0:&lt;/p&gt;
&lt;p&gt;\[
I_x u + I_y v + I_t = 0.
\]&lt;/p&gt;
&lt;p&gt;The entire energy to be minimized is then&lt;/p&gt;
&lt;p&gt;\[
E_D(\mathbf{u}, \mathbf{v}) = \sum_{s}(I_{x,s}u_s + I_{y,s}v_s + I_{t,s})^2 + \lambda \sum_{n\in G(s)}(u_s - u_n)^2 + \sum_{n \in G(s)}(v_s - v_n)^2.
\]&lt;/p&gt;
&lt;p&gt;Differentiating this and setting to 0 yields two equations in \(u\) and \(v\):&lt;/p&gt;
&lt;p&gt;\begin{align*}
\sum_s{(I_{x,s}^2 u_s + I_{x,s}I_{y,s}v_s + I_{x,s}I_{t,s}) + \lambda \sum_{n \in G(s)}(u_s - u_n)} &amp;amp;= 0\\
\sum_s{(I_{x,s}I_{y,s} u_s + I_{y,s}^2v_s + I_{y,s}I_{t,s}) + \lambda \sum_{n \in G(s)}(v_s - v_n)} &amp;amp;= 0\\
\end{align*}&lt;/p&gt;
&lt;p&gt;Note that this is computed for every pixel in the image.
This system is no longer underspecified because of the assumption that neighbors will exhibit the same flow.
We now have 5 equations per pixel.
In more recent works, larger neighborhood grids (\(5 \times 5\)) are used.
Then, we have 25 equations per pixel.
Since this is a system of linear equations, it could be computed directly using the normal equations.&lt;/p&gt;
&lt;p&gt;However, Horn and Schunck did not have very fast computers in 1981.
So, they introduced an iterative solution &amp;lt;&amp;amp;hornDeterminingOpticalFlow1980&amp;gt;.&lt;/p&gt;
&lt;h2 id=&#34;assumptions-of-small-motion&#34;&gt;Assumptions of Small Motion&lt;/h2&gt;
&lt;p&gt;One of the core assumptions in early formulations of optical flow is that motion is very small (&amp;lt;1 pixel).
In reality, some objects may move over 100 pixels within a single frame.
A simple solution to this problem was proposed by Bergen et al. in 1992 &amp;lt;&amp;amp;bergenHierarchicalModelbasedMotion&amp;gt;.
By creating an image pyramid over several resolutions, the assumption of small motion at each scale is still reasonable.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-06_19-37-48_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Hierarchical motion estimation (Bergen et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Hierarchical motion estimation (Bergen et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;At one scale, the warping parameters are estimated.
Next, they are used to warp the image to match the one at \(t-1\).
The warped image and true image at \(t-1\) are compared to refine the parameters.
The refined parameters are then sent to the next scale layer.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Segmentation via Clustering</title>
      <link>https://ajdillhoff.github.io/notes/segmentation_via_clustering/</link>
      <pubDate>Thu, 24 Feb 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/segmentation_via_clustering/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#agglomerative-clustering&#34;&gt;Agglomerative Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-means-clustering&#34;&gt;K-Means Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#simple-linear-iterative-clustering--slic&#34;&gt;Simple Linear Iterative Clustering (SLIC)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#superpixels-in-recent-work&#34;&gt;Superpixels in Recent Work&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The goal of segmentation is fairly broad: group visual elements together.
For any given task, the question is &lt;em&gt;how are elements grouped?&lt;/em&gt;
At the smallest level of an image, pixels can be grouped by color, intensity, or spatial proximity.
Without a model of higher level objects, the pixel-based approach will break down at a large enough scale.&lt;/p&gt;
&lt;p&gt;Segmentation by thresholding works in cases where the boundaries between features are clearly defined.
However, thresholding is not very robust to complex images with noise.
Consider a simple image and its intensity histogram as noise is added.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-01_10-27-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;From left to right, a noiseless image with increasing amounts of Gaussian noise added. Source: Pearson Education, Inc.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;From left to right, a noiseless image with increasing amounts of Gaussian noise added. Source: Pearson Education, Inc.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Even with some noise added, as seen in the middle image, thresholding is still relatively straightforward.
Once enough noise is added, thresholding via pixel intensities will not work.
A more sophisticated approach is needed in this case.&lt;/p&gt;
&lt;p&gt;Clustering is a fairly intuitive way to think about segmentation.
Instead of a fine-grained representation of an image as a collection of pixels, it is represented as groups or clusters that share some common features.
The general process of clustering is simple.
The image is represented as a collection of feature vectors (intensity, pixel color, etc.).
Feature vectors are assigned to a single cluster. These clusters represent some segment of the image.&lt;/p&gt;
&lt;p&gt;When it comes to clustering methods, there are two main approaches: agglomerative and divisive.
Simply, one is a bottom-up approach. The other is a top-down approach.
After briefly introductin agglomerative clustering, we will explore specific implementations of segmentation using k-means clustering as well as segmentation using superpixels (Achanta et al. 2012).&lt;/p&gt;
&lt;h2 id=&#34;agglomerative-clustering&#34;&gt;Agglomerative Clustering&lt;/h2&gt;
&lt;p&gt;Agglomerative clustering methods start by assuming every element is a separate cluster.
Elements are formed based on some local similarities.
As these methods iterate, the number of clusters decreases.
Deciding which elements to merge depends on &lt;strong&gt;inter-cluster distance&lt;/strong&gt;.
The exact choice of distance is dependent on the task. Some examples include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Single-link clustering&lt;/strong&gt;: The distance between the closest elements.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Complete-link clustering&lt;/strong&gt;: The maximum distance between an element of the first cluster and one of the second.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Group average clustering&lt;/strong&gt;: Average distance of elements in a cluster.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;How many clusters are or should be in a single image?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a difficult question to answer for many reasons. The answer will be largely dependent on the task at hand.
It is a problem of learning the underlying generative process of the visual elements in the image.
By defining the specific goal of segmentation (segment by color, shape, etc.), we are introducing a prior about the underlying generative processes which formed the image.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;figure--fig1&#34;&gt;&lt;/a&gt;&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-24_16-24-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;3D-PointCapsNet learns point segmentations on only 1% of the training data (Zhao et al.).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;3D-PointCapsNet learns point segmentations on only 1% of the training data (Zhao et al.).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;There are approaches which attempt to segment objects in semi-supervised settings.
As seen in &lt;a href=&#34;#figure--fig1&#34;&gt;Figure 1&lt;/a&gt;, Zhao et al. propose a part segmentation model for 3D objects which only utilizes 1-5% of the training part labels (Zhao et al. 2019).&lt;/p&gt;
&lt;p&gt;For example, if we divised an algorithm that would segment an image by color values, it might be able to segment the hand wearing a solid color glove relatively easily.
If we wanted to segment the hand into its individual joints, we would have to introduce a visual prior such as asking the subject to wear a multicolored glove.
We could also add prior information about the hand shape and joint configuration into the model itself.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;figure--joint-pc&#34;&gt;&lt;/a&gt;&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-01_22-08-18_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;An image-based joint regression model predicts joint locations (left) along with a point cloud generated from the joint estimates (right).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;An image-based joint regression model predicts joint locations (left) along with a point cloud generated from the joint estimates (right).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the &lt;a href=&#34;#figure--joint-pc&#34;&gt;figure above&lt;/a&gt;, the kinematic hand model could be used to segment the hand by assigning points in the point cloud to the nearest joint as estimated by the model.&lt;/p&gt;
&lt;p&gt;One way to visualize the cluster relationships is a &lt;em&gt;dendrogram&lt;/em&gt;.
Initially, each element is its own cluster. As the process evolves and clusters are merged based on some similarity,
the hierarchy is updated to show how the connections are formed.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-24_18-16-31_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Example output from scikit-image.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Example output from scikit-image.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;k-means-clustering&#34;&gt;K-Means Clustering&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/SLU-CSCI5750-SP2022/homework03_DigitClassificationKNN&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;K-Means Variant KNN Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html#sphx-glr-auto-examples-cluster-plot-color-quantization-py&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html#sphx-glr-auto-examples-cluster-plot-color-quantization-py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;K-Means clustering is a popular machine learning method used in both supervised and unsupervised settings.
It works by iteratively updating a set of &lt;em&gt;centroids&lt;/em&gt; or means until some stopping criteria is achieved.&lt;/p&gt;
&lt;p&gt;To use this with image segmentation, we start by treating our image features as vectors.
In the RGB case, each pixel is a vector of 3 values.
It starts out by initializing \(k\) clusters randomly with means \(\mathbf{m}_i\).
The next step is to compute the distance between the clusters and each point in the image.
Points are assigned to the cluster that is closest.&lt;/p&gt;
&lt;p&gt;\[
\text{arg}\min_{C} \sum_{i=1}^k \sum_{\mathbf{z}\in C_i}\|\mathbf{z} - \mathbf{m}_i\|^2,
\]&lt;/p&gt;
&lt;p&gt;where \(C = \{C_1, \dots, C_k\}\) is the cluster set.&lt;/p&gt;
&lt;p&gt;K-Means uses Expectation Maximization to update its parameters.
That is, it first computes the expected values given its current cluster centers before updating the cluster centers based on the new assignments.
The standard algorithm is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Initialize clusters&lt;/strong&gt; - Randomly select \(k\) points as cluster centers \(\mathbf{m}_i\).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assign samples to clusters&lt;/strong&gt; - Assign each sample to the closest cluster center based on some distance metric.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Update the means&lt;/strong&gt; - Compute a new value for the cluster centers based on the assignments in the previous step.
\[
\mathbf{m}_i = \frac{1}{|C_i|}\sum_{\mathbf{z} \in C_i}\mathbf{z}, \quad i = 1, \dots, k
\]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Test for convergence&lt;/strong&gt; - Compute the distances between the means at time \(t\) and time \(t - 1\) as \(E\). Stop if the difference is less than some threshold: \(E \leq T\).&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-01_21-41-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Image segmented using k-means with k=3. Source: Pearson Education, Inc.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Image segmented using k-means with k=3. Source: Pearson Education, Inc.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;simple-linear-iterative-clustering--slic&#34;&gt;Simple Linear Iterative Clustering (SLIC)&lt;/h2&gt;
&lt;p&gt;Simple Linear Iterative Clustering (SLIC) is widely used algorithm based on K-Means clustering for image segmentation (Achanta et al. 2012).&lt;/p&gt;
&lt;p&gt;As discussed in the original paper, the authors state that SLIC h   as two main advantages over traditional K-Means:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The search space for assigning points is reduced, leading to an increase in performance.&lt;/li&gt;
&lt;li&gt;By weighting the distance measure, color and spatial proximity are both considered when forming clusters.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The algorithm itself is simple to understand and implement, as seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-01_23-39-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;SLIC Algorithm (Achanta et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;SLIC Algorithm (Achanta et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;initialization&#34;&gt;Initialization&lt;/h3&gt;
&lt;p&gt;To keep the search space smaller, the individual search regions are spaced \(S = \sqrt{N/k}\) pixels apart, where \(N\) is the number of pixels and \(k\) is the number of cluster centers.&lt;/p&gt;
&lt;p&gt;The image itself is represented in &lt;a href=&#34;https://en.wikipedia.org/wiki/CIELAB_color_space&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;CIELAB color space&lt;/a&gt;.
This color space was chosen because it is &lt;em&gt;perceputally uniform&lt;/em&gt;.
That is, it is useful for detecting small differences in color.&lt;/p&gt;
&lt;p&gt;Each of the \(k\) pixel clusters is then defined as a superpixel consisting of the CIELAB color and position:&lt;/p&gt;
&lt;p&gt;\[
C_i = [l_i\ a_i\ b_i\ x_i\ y_i]^T.
\]&lt;/p&gt;
&lt;p&gt;For stability, the seed locations are moved to the lowest gradient position in a \(3 \times 3\) neighborhood.
If the superpixels are building locally distinct regions, it is better to avoid placing them on an edge (boundary) pixel.&lt;/p&gt;
&lt;h3 id=&#34;search-space-and-distance&#34;&gt;Search Space and Distance&lt;/h3&gt;
&lt;p&gt;The search space for a cluster center is a region \(2S \times 2S\) around the cluster.
Each pixel in this region is compared to the cluster center \(C_k\) using a distance measure \(D\).&lt;/p&gt;
&lt;p&gt;The distance measure should consider both the spatial and color distances:&lt;/p&gt;
&lt;p&gt;\begin{align*}
d_c &amp;amp;= \sqrt{(l_j - l_i)^2 + (a_j - a_i)^2 + (b_j - b_i)^2}\\
d_s &amp;amp;= \sqrt{(x_j - x_i)^2 + (y_j - y_i)^2}\\
D&amp;rsquo; &amp;amp;= \sqrt{\Big(\frac{d_c}{N_c}\Big)^2 + \Big(\frac{d_s}{N_s}\Big)^2}
\end{align*}&lt;/p&gt;
&lt;p&gt;The individual distances should be normalized by their respective maximums since the range of CIELAB values is different from the variable maximum of \(N_s\), which is based on the image size.
Here, \(N_s\) corresponds to the sampling size \(\sqrt{N/k}\).&lt;/p&gt;
&lt;p&gt;The authors found that normalizing this way was inconsistent since the color distances vary greatly from cluster to cluster.
They turn this normalization into a hyperparameter constant \(m\) so that the user can control the importance between spatial and color proximity.&lt;/p&gt;
&lt;p&gt;\[
D = \sqrt{d_c^2 + \Big(\frac{d_s}{S}\Big)^2 m^2}
\]&lt;/p&gt;
&lt;p&gt;A smaller \(m\) results in superpixels that adhere more to image boundaries, where a larger value promotes compact superpixels.&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_20-24-07_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Comparison of SLIC against other superpixel methods (Achanta et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Comparison of SLIC against other superpixel methods (Achanta et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_20-26-00_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Images segmented using a varying number of clusters (Achanta et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Images segmented using a varying number of clusters (Achanta et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;superpixels-in-recent-work&#34;&gt;Superpixels in Recent Work&lt;/h2&gt;
&lt;p&gt;Superpixels are useful for reducing the dimensionality of the feature space.
Their applications include tracking, segmentation, and object detection.
Methods that extract superpixels do not work out of the box with deep learning methods
due to their non-differentiable formulation.
Deep learning methods rely on gradient descent to optimize their parameters.
This requires that the functions used in a deep network be differentiable.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_20-47-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Superpixels optimized for semantic segmentation (Jampani et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Superpixels optimized for semantic segmentation (Jampani et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Superpixel Sampling Networks, proposed by Jampani et al., introduce the first attempt at integrating superpixel extraction methods with deep learning models (Jampani et al. 2018).
In this work, they adapt SLIC as a differentiable layer in a deep network which result in superpixels that are fine-tuned for specific tasks.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_21-45-09_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;Model diagram for SSN (Jampani et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;Model diagram for SSN (Jampani et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The train their model on a semantic segmentation task which fine tunes the learned superpixels such that they adhere more closely to segmentation boundaries.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_21-51-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 11: &amp;lt;/span&amp;gt;Results on semantic segmentation (Jampani et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;Results on semantic segmentation (Jampani et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In a more recent work, Yang et al. propose a deep network that directly produces the superpixels as opposed to using a soft K-Means layer (Yang et al. 2020).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_22-05-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 12: &amp;lt;/span&amp;gt;Model comparison between Jampani et al. and Yang et al. (Yang et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 12: &lt;/span&gt;Model comparison between Jampani et al. and Yang et al. (Yang et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Similar to SSN, they experiment on the Berkeley Image Segmentation Dataset.
Their results are competitive with other deep learning-based approaches.
The authors note that their method generalizes better in segmentation tasks by being robust to fine details and noise.
Additionally, their model runs at 50 fps using 4 NVIDIA Titan Xp GPUs.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-03-03_22-14-22_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 13: &amp;lt;/span&amp;gt;Comparison of results on competing methods (Yang et al.)&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 13: &lt;/span&gt;Comparison of results on competing methods (Yang et al.)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;#print_bibliography: t&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Edge Detection</title>
      <link>https://ajdillhoff.github.io/notes/edge_detection/</link>
      <pubDate>Tue, 01 Feb 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/edge_detection/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#computing-gradient-norms&#34;&gt;Computing Gradient Norms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nonmaxima-suppression&#34;&gt;Nonmaxima Suppression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#thresholding&#34;&gt;Thresholding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#connectivity-analysis&#34;&gt;Connectivity Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_22-45-59_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Vertical derivative filter (left) and horizontal derivative filter (right).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Vertical derivative filter (left) and horizontal derivative filter (right).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;When &lt;a href=&#34;https://ajdillhoff.github.io/notes/linear_filters/&#34;&gt;image gradient&lt;/a&gt; filters are applied to an image, we can observe that the sample responses are very sensitive to noise and detail. For example, look at the surface at the back of ship near the drive cone. To resolve this, the image should be smoothed before differentiating it. Recall that the Gaussian filter smooths the area so that neighboring pixels are more similar than distant pixels.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_22-49-44_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;dx kernel applied to image blurred with Gaussian (sigma=1).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;dx kernel applied to image blurred with Gaussian (sigma=1).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This is closer to what we want, but the end goal is to create an image that shows distinct edges. We need to be clear about what an edge is. For now, we consider the images produced by convolving the \(dx\) or \(dy\) kernels as edge score images. They are only intermediate; we still need to make a final decision.&lt;/p&gt;
&lt;p&gt;In this section, we will learn about the Canny Edge Detector. The general algorithm is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Smooth the image using Gaussian blurring.&lt;/li&gt;
&lt;li&gt;Compute the gradient image via filtering. Most commonly, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Sobel_operator&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Sobel operator&lt;/a&gt; is used.&lt;/li&gt;
&lt;li&gt;Filter out weaker edge score by selecting local pixels with the largest gradient change.&lt;/li&gt;
&lt;li&gt;Use double thresholding to separate strong, or definite, edge pixels from weak ones.&lt;/li&gt;
&lt;li&gt;Remove all weak pixels not connected to a strong pixel.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Canny edge detection follows 3 objective criteria:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Edges should be detected with a low error rate. The goal is to extract as many &lt;em&gt;actual&lt;/em&gt; edges as possible.&lt;/li&gt;
&lt;li&gt;A detected edge should correspond to the center pixel of the edge in the original image.&lt;/li&gt;
&lt;li&gt;It should be robust to noise and only mark an edge pixel once.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Smoothing the image can be done by applying a Gaussian blur. Next, we need to compute the gradient image.&lt;/p&gt;
&lt;h2 id=&#34;computing-gradient-norms&#34;&gt;Computing Gradient Norms&lt;/h2&gt;
&lt;p&gt;As we saw before, the derivative filters compute the direction of greatest change in the calculated direction. When combining the result of \(dx\) and \(dy\), we get the gradient of the pixel.&lt;/p&gt;
&lt;p&gt;\[
\nabla f(x, y) = \Bigg[\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\Bigg].
\]&lt;/p&gt;
&lt;p&gt;Canny edge detection works by selecting local pixels with the largest gradient change. In order to do this, we need to compute the &lt;strong&gt;norm&lt;/strong&gt; of the gradient. If we consider every pixel in the gradient image to be a vector indicating the direction of greatest change, the norm can be computed as&lt;/p&gt;
&lt;p&gt;\[
\|\nabla f(x, y)\| = \sqrt{\Big(\frac{\partial f}{\partial x}\Big)^2 + \Big(\frac{\partial f}{\partial y}\Big)^2}.
\]&lt;/p&gt;
&lt;p&gt;Additionally, we want the angle of direction of the gradient. This can be computed for each pixel as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\theta = \text{atan2}(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}).
\end{equation*}&lt;/p&gt;
&lt;p&gt;In practice, this can be computed at the same time. There are also efficient implementations of &lt;code&gt;atan2&lt;/code&gt; which can generate an array of the same size of the original image containing the computed angles for each pixel.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-06_12-47-47_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Gradient norm of image.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Gradient norm of image.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The figure above shows the result of computing the gradient norms for each pixel. This representation is intuitive to interpret. The largest values are on the edges of the violin. The image produced by this step is still too fuzzy. These do not represent the final edges.&lt;/p&gt;
&lt;h2 id=&#34;nonmaxima-suppression&#34;&gt;Nonmaxima Suppression&lt;/h2&gt;
&lt;p&gt;The gradient norm image is helpful in showing all edge scores, but the egdes are still too thick and there are many disconnected edge scores detected. We can thin the edges by evaluating neighboring pixels. We will select only the local pixels which have the highest absolute gradient and suppress the others.
This process is called &lt;strong&gt;nonmaxima suppression&lt;/strong&gt;.
There are two approaches to this problem. The first is approximates the closest gradient normal. The second uses interpolation to compute a more accurate value.&lt;/p&gt;
&lt;p&gt;In the first approach, the number of discrete orientations for the edge normal are split into horizontal, vertical, \(45^{\circ}\), and \(-45^{\circ}\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-06_13-13-55_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Discretizing angles into 4 regions.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Discretizing angles into 4 regions.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;For a given pixel, the gradient direction is discretized into one of the above four regions by selection the angle closest to the original angle given.
Next, the gradient norm of the given pixel is compared to that of the pixels on either side of it following the same discretized direction. If one of the neighboring pixels has a higher gradient norm, the current pixel&amp;rsquo;s value is set to 0.
The intuition here is that if it &lt;em&gt;were&lt;/em&gt; an edge pixel, it would have the largest gradient norm along its given direction.&lt;/p&gt;
&lt;p&gt;The result of applying this process on our gradient image is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-06_14-44-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Gradient norm image after nonmaxima suppression is applied.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Gradient norm image after nonmaxima suppression is applied.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;interpolation&#34;&gt;Interpolation&lt;/h3&gt;
&lt;p&gt;An alternative approach is to interpolate the gradient norm using the actual angle.
Instead of discretizing it into one of four regions above, the original angle is used to compute the neighboring pixels in continuous space.
This will, of course, produce invalid pixel locations. The gradient norm for the neighboring pixels follows the approach discussed in &lt;a href=&#34;https://ajdillhoff.github.io/notes/sampling/&#34;&gt;Sampling and Aliasing&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For example, if we are at pixel \((5, 5)\) with a gradient direction of \(55^{\circ}\), then the neighboring pixels along that angle can be computed by first finding the vector following that direction. That is&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathbf{p}_{\text{offset}} &amp;amp;=
\begin{bmatrix}
\cos (55^{\circ} \cdot \frac{\pi}{180^{\circ}})\\
\sin (55^{\circ} \cdot \frac{\pi}{180^{\circ}})
\end{bmatrix}\\
&amp;amp;=
\begin{bmatrix}
.5736\\
.8192
\end{bmatrix}
\end{align*}&lt;/p&gt;
&lt;p&gt;Then the two neighboring pixels along this direction are \(f(5 - .5373, 5 - .8192)\) and \(f(5 + .5373, 5 + .8192)\). These are clearly not valid pixel locations.
To compute the interpolated value, a weighted contribution from the closest 4 pixels are used for each of the two neighbors.
For \(f(4.4627, 4.1808)\), these pixels are \(\{(4, 4), (5, 4), (4, 5), (5, 5)\}\).
The interpolation weights for this pixel are computed as&lt;/p&gt;
&lt;p&gt;\begin{align*}
w_x &amp;amp;= 4.4627 - 4 = .4627\\
w_y &amp;amp;= 4.1808 - 4 = .1808
\end{align*}&lt;/p&gt;
&lt;p&gt;Then the resulting pixel value is computed via bilinear interpolation:&lt;/p&gt;
&lt;p&gt;\begin{align*}
f(4.4627, 4.1808) &amp;amp;=
(1 - w_x) \cdot (1 - w_y) \cdot f(4, 4)\\
&amp;amp;+ w_x \cdot (1 - w_y) \cdot f(5, 4)\\
&amp;amp;+ (1 - w_x) \cdot w_y \cdot f(4, 5)\\
&amp;amp;+ w_x \cdot w_y \cdot f(5, 5).
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;thresholding&#34;&gt;Thresholding&lt;/h2&gt;
&lt;p&gt;We now have an image of edge scores, but have not yet made a final determination on which pixels are actually edges. One approach to selecting the edge pixel is to use thresholding. That is, we suppress any pixel value that is lower than some parameter \(T\):&lt;/p&gt;
&lt;p&gt;\[
f_{T}(x, y) = f(x, y) \geq T.
\]&lt;/p&gt;
&lt;p&gt;However, this approach will still leave many false positives as well as edge segments that may be connected to strong edges.
This issue is partly resolved via &lt;strong&gt;hysteresis thresholding&lt;/strong&gt;.
For this, we choose 2 threshold values: one for weak edges and another for strong edge scores.
Using these scores, we can generate two images:&lt;/p&gt;
&lt;p&gt;\begin{align*}
f_{T_H}(x, y) &amp;amp;= f(x, y) \geq T_{H}\\
f_{T_L}(x, y) &amp;amp;= f(x, y) \geq T_{L}
\end{align*}&lt;/p&gt;
&lt;p&gt;We can then eliminate the duplicate pixels in \(f_{T_L}\) by subtracting \(f_{T_H}\):&lt;/p&gt;
&lt;p&gt;\[
f_{T_L} = f_{T_L} - f_{T_H}.
\]&lt;/p&gt;
&lt;p&gt;Using the image processed via nonmaxima suppression from before, this generates the following images:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-06_14-43-46_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Low threshold image (left) and high threshold image (right).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Low threshold image (left) and high threshold image (right).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;connectivity-analysis&#34;&gt;Connectivity Analysis&lt;/h2&gt;
&lt;p&gt;There must be a reason why we computed a lower threshold. There are weak edge pixels that may have been apart of a segment connected to strong pixels. In this case, we want to keep every weak pixel that is 8-connected to a strong pixel.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-06_15-00-57_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;The pixels surrounding the black pixel are 8-connected to it. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;The pixels surrounding the black pixel are 8-connected to it. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This can be accomplished with the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Locate an edge pixel in the high threshold image.&lt;/li&gt;
&lt;li&gt;Mark all pixels in the weak image that are 8-connected to the current strong pixel as strong pixels.&lt;/li&gt;
&lt;li&gt;Repeat steps 1 and 2 for all strong pixels in the original high threshold image.&lt;/li&gt;
&lt;li&gt;Set all pixels in the weak image that were not marked to 0.&lt;/li&gt;
&lt;li&gt;Add the marked weak pixels to the strong image.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Applying the procedure above given the weak and strong images from before yields the following result.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-06_15-04-45_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Final edge image after connectivity analysis.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Final edge image after connectivity analysis.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Sampling and Aliasing</title>
      <link>https://ajdillhoff.github.io/notes/sampling/</link>
      <pubDate>Sun, 30 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/sampling/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#resizing&#34;&gt;Resizing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sampling&#34;&gt;Sampling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;resizing&#34;&gt;Resizing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Aliasing arises through resampling an image&lt;/li&gt;
&lt;li&gt;How to resize - algorithm&lt;/li&gt;
&lt;li&gt;How to resolve aliasing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Resizing an image, whether increase or decreasing the size, is a common image operation. In Linear Algebra, &lt;strong&gt;scaling&lt;/strong&gt; is one of the transformations usually discussed, along with rotation and skew. Scaling is performed by creating a transformation matrix&lt;/p&gt;
&lt;p&gt;\begin{equation*}
M =
\begin{bmatrix}
s &amp;amp; 0\\
0 &amp;amp; s
\end{bmatrix},
\end{equation*}&lt;/p&gt;
&lt;p&gt;where \(s\) is the scaling factor. This matrix can then be used to transform the location of each point via matrix multiplication.&lt;/p&gt;
&lt;p&gt;Is is that simple for digital images? Can we simply transform each pixel location of the image using \(M\)? There are a couple of steps missing when it comes to scaling digital images. First, \(M\) simply creates a mapping between the location in the original image and the corresponding output location in the scaled image. If we were to implement this in code, we would need to take the pixel&amp;rsquo;s value from the original image.&lt;/p&gt;
&lt;h3 id=&#34;a-simple-example&#34;&gt;A Simple example&lt;/h3&gt;
&lt;p&gt;Take a \(2 \times 2\) image whose pixel values are all the same color.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-29_21-24-11_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;2 by 2 image whose values are the same.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;2 by 2 image whose values are the same.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;If we transform each pixel location of the image and copy that pixel&amp;rsquo;s value to the mapped location in the larger image, we would get something as seen in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-29_21-29-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;The resulting scaled image.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;The resulting scaled image.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This image is exactly what we would expect. The resulting image is two times as large as the first. What pixel values should the new ones take on? This is a question of &lt;strong&gt;sampling&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;sampling&#34;&gt;Sampling&lt;/h2&gt;
&lt;p&gt;Given \(s = 2\), the scaling matrix maps the original pixel locations to their new values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\((0, 0) \mapsto (0, 0)\)&lt;/li&gt;
&lt;li&gt;\((0, 1) \mapsto (0, 2)\)&lt;/li&gt;
&lt;li&gt;\((1,0) \mapsto (2, 0)\)&lt;/li&gt;
&lt;li&gt;\((1,1) \mapsto (2, 2)\)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What values should be given to the unmapped values of the new image? There are several sampling strategies used in practice. Two of the most common approaches are &lt;strong&gt;nearest neighbor&lt;/strong&gt; and &lt;strong&gt;bilinear&lt;/strong&gt; sampling. Let&amp;rsquo;s start with the nearest neighbor approach.&lt;/p&gt;
&lt;h3 id=&#34;nearest-neighbor&#34;&gt;Nearest Neighbor&lt;/h3&gt;
&lt;p&gt;First, we let the pixel location in an image be the center of that pixel, as depicted below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-29_22-50-00_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;A 2-by-2 image with pixel locations depicted as dots in the center.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;A 2-by-2 image with pixel locations depicted as dots in the center.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To establish a map between the pixel locations in the scaled image and that of the original image, we shrink the grid on the larger image and superimpose it over the smaller image.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-29_22-51-11_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Pixel grid of larger image superimposed on original image.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Pixel grid of larger image superimposed on original image.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;With nearest neighbor interpolation, the pixel value in the resized image corresponds to that of the nearest pixel in the original image. In the above figure, we can see that pixels \((0, 0), (0, 1), (1, 0), \text{ and } (1, 1)\) in the resized image are closest to pixel \((0, 0)\) in the original image. Thus, they will take on that pixel&amp;rsquo;s value.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s compare both of these approaches on a real image. The first figure below shows the original image \((16 \times 16\)). The following figure shows the image resized to \((32 \times 32)\) with no interpolation and nearest neighbor interpolation, respectively.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-29_23-59-17_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Original image.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Original image.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-29_23-59-52_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Image resized with no interpolation (left) and nearest neighbor (right).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Image resized with no interpolation (left) and nearest neighbor (right).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;bilinear-interpolation&#34;&gt;Bilinear Interpolation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Bilinear interpolation&lt;/strong&gt; is a slightly more sophisticated way of sampling which takes into account all neighboring pixels in the original image. The value of the pixel in the sampled image is a linear combination of the values of the neighbors of the corresponding pixel it is mapped to.&lt;/p&gt;
&lt;p&gt;Consider a \(3 \times 3\) image upsampled to a \(8 \times 8\) image. The figure below shows the original image with the coordinates of the upsampled image superimposed on it.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_11-36-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;3-by-3 grid with 8-by-8 coordinates overlaid.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;3-by-3 grid with 8-by-8 coordinates overlaid.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;How do we determine the coordinate map between the original and upscaled image?&lt;/strong&gt;
Solve a linear system.&lt;/p&gt;
&lt;p&gt;Note the extreme values of the image. That is, the smallest and largest coordinates. Since we stated previously that \((0, 0)\) refers to the coordinate in the middle of the pixel, the top-left of the image boundary for any image is \((-0.5, -0.5)\). The bottom-right corner for the smaller image is \((2.5, 2.5)\). The bottom-right corner for the resized image is \((7.5, 7.5)\). The equation that maps the top-left coordinates between the images is given by&lt;/p&gt;
&lt;p&gt;\[
-\frac{1}{2} = -\frac{1}{2}a + b.
\]&lt;/p&gt;
&lt;p&gt;The equation that maps the bottom-right coordinates between the images is given by&lt;/p&gt;
&lt;p&gt;\[
\frac{5}{2} = \frac{15}{2}a + b.
\]&lt;/p&gt;
&lt;p&gt;Thus, we have 2 equations 2 unknowns. Solving this yields \(a = \frac{3}{8}\) and \(b = -\frac{5}{16}\).&lt;/p&gt;
&lt;p&gt;With the mapping solved, let&amp;rsquo;s compute the color value for pixel \((3, 3)\) in the upsampled image. Here, \((3, 3) \mapsto (\frac{13}{16}, \frac{13}{16})\) in the original image. Our problem for this particular pixel is reduced to the following figure.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_18-08-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Determining the pixel value for the mapped pixel using bilinear interpolation.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Determining the pixel value for the mapped pixel using bilinear interpolation.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;We first interpolate between two pairs of pixels in the original image. That is, we find \(p_1\) and \(p_2\) in the following figure.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_18-15-27_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Step 1: Interpolate the pixel values between two pixels for (p_1) and (p_2).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Step 1: Interpolate the pixel values between two pixels for (p_1) and (p_2).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Here, \(p_2 = \frac{3}{16}(255, 255, 255) + \frac{13}{16}(128, 128, 128) \approx (152, 152, 152)\) and \(p_1 = \frac{3}{16}(255, 0, 0) + \frac{13}{16}(255, 255, 255) \approx (255, 207, 207)\). Note that the contribution of the pixel depends on the weight on the other side the intermediate value \(p_i\). For example, if you think of \(p_1\) as a slider from the red pixel to the white pixel. The value to the left of the slider reflects the contribution of the pixel to the right, and vice versa.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_18-23-43_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;Computed values of (p_1) and (p_2).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;Computed values of (p_1) and (p_2).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Finally, the value of the new pixel is a linear combination of \(p_1\) and \(p_2\). That is \(p = \frac{13}{16}(152, 152, 152) + \frac{3}{16}(255, 207, 207) \approx (171, 162, 162)\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_18-28-03_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 11: &amp;lt;/span&amp;gt;The final pixel value computed from (p_1) and (p_2).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;The final pixel value computed from (p_1) and (p_2).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The following figure compares the original image with both types of interpolation discussed in this section.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_18-34-13_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 12: &amp;lt;/span&amp;gt;Original image (left), upscaled 2x with NN interpolation (middle), upscaled with bilinear interpolation (right).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 12: &lt;/span&gt;Original image (left), upscaled 2x with NN interpolation (middle), upscaled with bilinear interpolation (right).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;aliasing&#34;&gt;Aliasing&lt;/h3&gt;
&lt;p&gt;Nearest neighbor interpolation often leads to images with &lt;strong&gt;aliasing&lt;/strong&gt;. In general, aliasing occurs when two signals are sampled at such a frequency that they become indistinguishable from each other. Usually, images are smoothed prior to upsampling or downsampling in an effort to alleviate the effects of aliasing. The figure below shows a downsampled image using nearest neighbor interpolation.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_10-31-40_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 13: &amp;lt;/span&amp;gt;Image downsampled by 4x. Notice the &amp;#34;jaggies&amp;#34;, especially along straight lines.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 13: &lt;/span&gt;Image downsampled by 4x. Notice the &amp;ldquo;jaggies&amp;rdquo;, especially along straight lines.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;By blurring the image and using bilinear interpolation, the same image looks much smoother when downsized.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2023-01-25_19-30-16_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 14: &amp;lt;/span&amp;gt;Image downsampled by 4x using bilinear interpolation.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 14: &lt;/span&gt;Image downsampled by 4x using bilinear interpolation.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Color</title>
      <link>https://ajdillhoff.github.io/notes/color/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/color/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#topics&#34;&gt;Topics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-human-eye&#34;&gt;The Human Eye&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#color-matching&#34;&gt;Color Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#color-physics&#34;&gt;Color Physics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#color-spaces&#34;&gt;Color Spaces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hsv-color-space&#34;&gt;HSV Color Space&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;What is color?&lt;/li&gt;
&lt;li&gt;How do we process color?&lt;/li&gt;
&lt;li&gt;What information does color contain?&lt;/li&gt;
&lt;li&gt;What can we infer from color?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-human-eye&#34;&gt;The Human Eye&lt;/h2&gt;
&lt;p&gt;The eye acts as a camera, including a lens which focuses light onto a receptive surface. The &lt;strong&gt;cornea&lt;/strong&gt; covers the &lt;strong&gt;lens&lt;/strong&gt; which combine to make a compound lens. The lens itself is flexible to allow the eye to focus on objects of variable distance. The lens is attached to &lt;strong&gt;ciliary muscles&lt;/strong&gt; which contract or expand to change the shape of the lens. This allows us to focus on near or far objects. As we age, the lens itself becomes hardened and does not transform back to a spherical shape when the ciliary muscles contract, resulting in farsightedness.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;pupil&lt;/strong&gt; is a diaphragm that adjusts the amount of light that enters the eye in response to varying intensities. However, this isn&amp;rsquo;t the only mechanism available for this task. Our perception of the amount of light, &lt;em&gt;luminance adaptation&lt;/em&gt;, occurs also in the retina and brain over a longer time period (usually several minutes).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-02_20-23-37_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The following image from Wikipedia shows the evolution of the eye from a simple region of photoreceptors to the current model we have today.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-04_10-00-18_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Source: &amp;lt;https://en.wikipedia.org/wiki/Eye&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Source: &lt;a href=&#34;https://en.wikipedia.org/wiki/Eye&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://en.wikipedia.org/wiki/Eye&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;retina&#34;&gt;Retina&lt;/h3&gt;
&lt;p&gt;The primary purpose of the optics of the eye is to focus light onto the &lt;strong&gt;retina&lt;/strong&gt;. This is a thin layer of nerve tissue that covers the inner surface of the eye. It consists of approximately 200 million layered cells. Half are &lt;strong&gt;photoreceptor&lt;/strong&gt; cells and the other half recode photoreceptor outputs before passing them on towards the brain.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fun fact:&lt;/strong&gt; The retina is woven with blood vessels whose purpose is to nourish the retinal tissue. These blood vessels produce the &lt;em&gt;red eye&lt;/em&gt; effect that is seen in flash photography.&lt;/p&gt;
&lt;h3 id=&#34;photoreceptor-cells&#34;&gt;Photoreceptor Cells&lt;/h3&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-23_21-19-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Distribution of rods and cones. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Distribution of rods and cones. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Photorceptor cells can be classified into two types: rods and cones. There are about 100 million rods and 6 million cones. &lt;strong&gt;Rods&lt;/strong&gt; dominate our low-light vision. They do not have the variety of photopigments necessary for color vision. &lt;strong&gt;Cones&lt;/strong&gt; dominate our color vision. They can be separated into one of three classes of light receptors. These three classes contain a specific type of photopigment that is sensitive to different wavelengths of light.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2021-12-31_13-24-45_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Source: &amp;lt;https://handprint.com/HP/WCL/color1.html#3cones&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Source: &lt;a href=&#34;https://handprint.com/HP/WCL/color1.html#3cones&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://handprint.com/HP/WCL/color1.html#3cones&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Rods can pool input together before sending it to the brain. Low-light perception is better through peripheral vision.&lt;/p&gt;
&lt;p&gt;Most of the cones are in the &lt;strong&gt;fovea&lt;/strong&gt;. The high density of cones in this region means that the resolution is highest. Microsaccades - The eye is making tiny movements so that the eye is never fixated on a single point. This allows light to hit a greater number of photoreceptors.&lt;/p&gt;
&lt;p&gt;The other segment of a photoreceptor cell lies the photopigment molecules. These act as transducers which convert light energy into a biological response. These molecules are made up of a light sensitive molecule called &lt;strong&gt;chromophore&lt;/strong&gt; and a protein called &lt;strong&gt;opsin&lt;/strong&gt;. Together, they are usually referred to as &lt;strong&gt;rhodopsin&lt;/strong&gt;.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-02_20-47-55_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;Photopigment Molecules. Source: &amp;lt;https://handprint.com/HP/WCL/color1.html&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;Photopigment Molecules. Source: &lt;a href=&#34;https://handprint.com/HP/WCL/color1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://handprint.com/HP/WCL/color1.html&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;types-of-photopigments&#34;&gt;Types of Photopigments&lt;/h4&gt;
&lt;p&gt;Each photoreceptor can respond to light differently depending on the specific type of rhodopsin it is made up of.&lt;/p&gt;
&lt;p&gt;The response of each type of cone has been measured and can is separated into short wavelength (S), medium wavelength (M), and long wavelength (L). Some resources incorrectly label these as B, G, and R receptors. This is not entirely accurate as there is much overlap between the medium and long wavelength receptors.&lt;/p&gt;
&lt;h4 id=&#34;mathematical-model&#34;&gt;Mathematical Model&lt;/h4&gt;
&lt;p&gt;How would we model a photoreceptor, mathematically? The response is dependent on how sensitive the receptor is to a specific wavelength along with the light arriving at the receptor.&lt;/p&gt;
&lt;p&gt;\begin{equation*}
p_k = \int_{\Lambda} \sigma_k(\lambda)E(\lambda)d\lambda
\end{equation*}&lt;/p&gt;
&lt;h2 id=&#34;color-matching&#34;&gt;Color Matching&lt;/h2&gt;
&lt;p&gt;The theory that the visible colors are visible from the three primaries is called &lt;strong&gt;trichomatic theory&lt;/strong&gt;. Trichromacy has been measured and observed.&lt;/p&gt;
&lt;p&gt;How is it known that these 3 (or 4) types of photoreceptors absorb a specific wavelength? In other words, how do we know that color vision break down? There has not been a way to measure cone responses in living humans, but there is a way to measure them indirectly.&lt;/p&gt;
&lt;p&gt;James Clerk Maxwell&amp;rsquo;s color matching experiments aim to do exactly that. In this experiment, participants are given a test color and a matching color. The goal is to add some amount of primary colors until the matching color is the same as the test color. There are some saturated test colors that cannot be matched in an additive manner (combining primaries). In these cases, the subjects are allowed to subtract some amount of a primary color from the test color until it matches the matching color.&lt;/p&gt;
&lt;p&gt;Over many different experiments involving many subjects, it was found that most subjects will match the test color with the same amount of primary weights. This provided confirmation that human vision is trichromatic. There were also carefully screen &lt;strong&gt;dichromats&lt;/strong&gt; who lacked one class of photoreceptor.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is color matching reliable?&lt;/strong&gt;
&lt;strong&gt;Do we see all wavelengths the same? That is, do we have an equal number of photoreceptors for each range?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The fact that many independent observers came up with similar distributions of primary colors to match test colors gave rise to Grassman&amp;rsquo;s laws.&lt;/p&gt;
&lt;p&gt;The result of these experiments reveals a function of cone sensitivities. Cones are unequally distributed across the retina. Visual angle and whether the light is detected in the central vs. peripheral region matters. The CIE 1931 RGB color matching functions show how much of each primary wavelength is required to match a particular target wavelength. The negative values mean that the primary was added to the test color in order to match.&lt;/p&gt;
&lt;p&gt;
  &lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-03_12-36-25_screenshot.png&#34; alt=&#34;&#34;&gt;

Source: &lt;a href=&#34;https://en.wikipedia.org/wiki/CIE_1931_color_space&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://en.wikipedia.org/wiki/CIE_1931_color_space&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Actual cone sensitivy depends on light sensitivy. One of the most popular graphs of cone response is from Stockman &amp;amp; Sharpe (2000):&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-03_13-27-23_screenshot.png&#34; &gt;


&lt;/figure&gt;

&lt;p&gt;This graph is normalized to peak response and is not representative of the distribution of cones in the retina. If we weight the responses by the proportion of each class of cone in the retina, the graph looks like:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-03_13-28-55_screenshot.png&#34; &gt;


&lt;/figure&gt;

&lt;p&gt;The fact that the 3 shapes are colored as blue, green, and red are misleading. There are far more photoreceptors that perceive green light than there are blue or red light. You can see this when comparing green text versus blue text. Reading the blue text may strain your eyes and appear blurry. This is because there are simply fewer receptors available for these wavelengths. Fewer such receptors also implies that the resolution is smaller.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-03_13-34-15_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Fewer blue cones results in blue light appearing more blurry.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Fewer blue cones results in blue light appearing more blurry.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;color-physics&#34;&gt;Color Physics&lt;/h2&gt;
&lt;p&gt;Light sources themselves can produce different colors at different wavelengths. The sky varies in color depending on the relative location of the sun. A surface is dependent on the color of the surface itself as well as the incident light shining towards it. A white surface with a green light shining on it will reflect green light. A green surface with white light will also reflect green. Further complexities such as atmosphere and dust can further complicate this.&lt;/p&gt;
&lt;p&gt;In our world, both the sky and the sun are important light sources. They are often referred to as skylight and sunlight, respectively. Imagine a clear day with the sun at the highest point in the sky. The sun is seen as a yellow object with the surrounding sky being a rich blue. Compared to the zenith of the sky, the horizon is typically brighter. This is because air scatters the incident light. This can be modeled by assuming the sky emits a constant amount of exitant light per volume.&lt;/p&gt;
&lt;p&gt;Why does the sun appear yellow and the sky appear blue? Longer wavelengths can travel farther before being scattered. This means that a ray of light travelling from the sun to the earth will scatter blue light before the other rays. Taking the blue light out of a ray of white light will leave a yellowish color. When the scattered blue light eventually hits the atmosphere of Earth, it is scattered once again and see as incident light to an observer. This gives the apperance of a blue sky.&lt;/p&gt;
&lt;p&gt;The perceived color of an object can be computed by multiplying the incident illumination with the reflectance of the object.&lt;/p&gt;
&lt;h3 id=&#34;color-temperature&#34;&gt;Color Temperature&lt;/h3&gt;
&lt;p&gt;One quantity that is commonly used to describe a light source is temperature (measured in Kelvins). This is derived from the concept of the &lt;strong&gt;black body&lt;/strong&gt;. That is, a body that does not reflect light. A heated black body emits radiation and the spectral power distribution of this radiation depends only on the temperature. The color temperature is the surface temperature of an ideal black body.&lt;/p&gt;
&lt;p&gt;At lower temperature, the color is a red. As it increases, the color becomes whiter until reaching a light blue. See the table below:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_11-49-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Colors corresponding to different temperatures. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Colors corresponding to different temperatures. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;color-spaces&#34;&gt;Color Spaces&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;How is color represented based on color matching?&lt;/strong&gt;
&lt;strong&gt;Can we accurately reproduce any color digitally?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.wnycstudios.org/podcasts/radiolab/episodes/211119-colors&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.wnycstudios.org/podcasts/radiolab/episodes/211119-colors&lt;/a&gt;
Interesting podcast on Color.&lt;/p&gt;
&lt;p&gt;There are several color spaces (or color models) available. The most common is RGB. In this section, we will explore the most common color spaces.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-03_17-51-15_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Comparison of several common color spaces. Source: &amp;lt;https://en.wikipedia.org/wiki/ProPhoto_RGB_color_space&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Comparison of several common color spaces. Source: &lt;a href=&#34;https://en.wikipedia.org/wiki/ProPhoto_RGB_color_space&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://en.wikipedia.org/wiki/ProPhoto_RGB_color_space&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;cie-xy-color-space&#34;&gt;CIE xy Color Space&lt;/h3&gt;
&lt;p&gt;Verifying trichomatic theory meant that one should attempt to reproduce monochromatic (single wavelength) colors as a weighted mixture of primary colors. The Commision Internationale d&amp;rsquo;Eclairage did just that in the 1930s. They performed their own color matching experiments using red, blue, and green wavelengths as primary colors. Another benefit of this process is to set a standard in which colors can be reproduced.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-03_17-30-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;CIE xy color space.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;CIE xy color space.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This shows the CIE xy space which was taken from CIE XYZ space. This 2D space is an intersection of the original XYZ space by the plane \(X + Y + Z = 1\). The coordinates are then&lt;/p&gt;
&lt;p&gt;\begin{equation*}
(x, y) = \Big(\frac{x}{x + y + z}, \frac{y}{x + y + z}\Big).
\end{equation*}&lt;/p&gt;
&lt;p&gt;The border of this shape represents the wavevelength of the pure color. The colors in the middle represent some linear combination of those wavelengths.&lt;/p&gt;
&lt;h3 id=&#34;rgb-color-spaces&#34;&gt;RGB Color Spaces&lt;/h3&gt;
&lt;p&gt;RGB color spaces are the most commonly used in computer graphics. CIE also has their own RGB color space. It is derived from color matching experiments using 3 monochromatic primary colors.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-24_20-49-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;CIE RGB gamut on the CIE xy color space.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;CIE RGB gamut on the CIE xy color space.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;RGB models are commonly represented as a cube, as seen in the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-24_20-54-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 11: &amp;lt;/span&amp;gt;Cube representation of an RGB color space.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;Cube representation of an RGB color space.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;hsv-color-space&#34;&gt;HSV Color Space&lt;/h2&gt;
&lt;p&gt;Hue, Saturation, Value (HSV) provides an alternative representation of RGB. &lt;strong&gt;Hue&lt;/strong&gt; represents the color. Given a fixed value for saturation and value, colors in HSV should appear as if they are receiving the same level of light.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-24_21-05-11_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 12: &amp;lt;/span&amp;gt;Hue values when saturation and value are fixed. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 12: &lt;/span&gt;Hue values when saturation and value are fixed. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The CIE defines &lt;strong&gt;saturation&lt;/strong&gt; as &amp;ldquo;the colourfulness of an area judged in proportion to its brightness.&amp;rdquo; The &lt;strong&gt;value&lt;/strong&gt; of a pixel represents how bright the color is compared to black.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-24_21-10-29_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 13: &amp;lt;/span&amp;gt;HSV cylinder exemplifies the concepts of hue, saturation, and value. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 13: &lt;/span&gt;HSV cylinder exemplifies the concepts of hue, saturation, and value. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;conversion-from-rgb&#34;&gt;Conversion from RGB&lt;/h4&gt;
&lt;p&gt;We can convert images to HSV from RGB and vice versa. This article will only go through the steps of actually transforming it. To gain a better understanding of the conversion between HSV to RGB, check out the &lt;a href=&#34;https://en.wikipedia.org/wiki/HSL_and_HSV#General_approach&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Wikipedia page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We start by making sure that are input image is normalized such that the RGB values are in the range \([0, 1]\). If that&amp;rsquo;s the case, we can calculate the HSV value \(V\) as&lt;/p&gt;
&lt;p&gt;\[
V = \max(R, G, B).
\]&lt;/p&gt;
&lt;p&gt;We can do this in one command in Python using numpy:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;V &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max(img, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Saturation is computed based on another quantity called &lt;a href=&#34;https://en.wikipedia.org/wiki/Colorfulness&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Chroma&lt;/a&gt;. It is simply computed as&lt;/p&gt;
&lt;p&gt;\[
C = V - \min(R,G,B).
\]&lt;/p&gt;
&lt;p&gt;In Python, this is simply&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;C &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; V &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;min(img, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Saturation is then computed as \(S = \frac{C}{V}\). Note that this will be undefined if \(V = 0\). In practice, we can set \(S = 0\) if \(V\) is also 0.&lt;/p&gt;
&lt;p&gt;Hue is commonly measured in degrees between \([0, 360]\). As seen in the figure below, it is the angle past the previous edge of the hexagon.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_10-29-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 14: &amp;lt;/span&amp;gt;Hue is the angle of the projected point with respect to the hexagon. 0 degrees is marked by the red edge. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 14: &lt;/span&gt;Hue is the angle of the projected point with respect to the hexagon. 0 degrees is marked by the red edge. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The function for Hue can be written as a piecewise function, altered slightly to account for undefined values in practice:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
H&amp;rsquo; =
\begin{cases}
0, &amp;amp; C = 0\\
\frac{G - B}{C} \text{ mod } 6, &amp;amp; \text{if } V = R\\
\frac{B - R}{C} + 2, &amp;amp; \text{if } V = G\\
\frac{R - G}{C} + 4, &amp;amp; \text{if } V = B\\
\end{cases}
\end{equation*}&lt;/p&gt;
&lt;p&gt;Then, \(H = 60^{\circ} \times H&amp;rsquo;\).&lt;/p&gt;
&lt;h4 id=&#34;conversion-to-rgb&#34;&gt;Conversion to RGB&lt;/h4&gt;
&lt;p&gt;To go back to RGB, we take an HSV image with \(H \in [0^{\circ}, 360^{\circ}]\), and \(S, V \in [0, 1]\). The Chrome value is calcuated as&lt;/p&gt;
&lt;p&gt;\[
C = V \times S.
\]&lt;/p&gt;
&lt;p&gt;We then divide up the Hue into one of 6 values:&lt;/p&gt;
&lt;p&gt;\[
H&amp;rsquo; = \frac{H}{60^{\circ}}.
\]&lt;/p&gt;
&lt;p&gt;With these intermediate value, we can calculate the corresponding point within the RGB cube that has the same hue and chroma as the current pixel value, with \(X\) being the second largest component of the color:&lt;/p&gt;
&lt;p&gt;\[
X = C \times (1 - |H&amp;rsquo; \text{ mod } 2 - 1|)
\]&lt;/p&gt;
&lt;p&gt;and then&lt;/p&gt;
&lt;p&gt;\begin{equation*}
(R&amp;rsquo;, G&amp;rsquo;, B&amp;rsquo;) =
\begin{cases}
(C, X, 0) &amp;amp; \text{if } 0 \leq H&amp;rsquo; &amp;lt; 1\\
(X, C, 0) &amp;amp; \text{if } 1 \leq H&amp;rsquo; &amp;lt; 2\\
(0, C, X) &amp;amp; \text{if } 2 \leq H&amp;rsquo; &amp;lt; 3\\
(0, X, C) &amp;amp; \text{if } 3 \leq H&amp;rsquo; &amp;lt; 4\\
(X, 0, C) &amp;amp; \text{if } 4 \leq H&amp;rsquo; &amp;lt; 5\\
(C, 0, X) &amp;amp; \text{if } 5 \leq H&amp;rsquo; &amp;lt; 6\\
\end{cases}
\end{equation*}&lt;/p&gt;
&lt;p&gt;The final RGB value can be calculated by adding the difference between the value and chroma to each pixel:&lt;/p&gt;
&lt;p&gt;\[
m = V - C
\]&lt;/p&gt;
&lt;p&gt;\[
(R, G, B) = (R&amp;rsquo; + m, G&amp;rsquo; + m, B&amp;rsquo; + m).
\]&lt;/p&gt;
&lt;h4 id=&#34;some-examples&#34;&gt;Some Examples&lt;/h4&gt;
&lt;p&gt;Given an original image (below), we&amp;rsquo;ll view the output of changing the Hue, Saturation, and Value.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_10-53-17_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 15: &amp;lt;/span&amp;gt;Original image. Credit: The Expanse&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 15: &lt;/span&gt;Original image. Credit: The Expanse
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Reducing the Hue by 20 produces the following image:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_10-55-52_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 16: &amp;lt;/span&amp;gt;Image with Hue subtracted by 20 degrees. Credit: The Expanse&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 16: &lt;/span&gt;Image with Hue subtracted by 20 degrees. Credit: The Expanse
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Given our working knowledge of how hue is computed, this makes sense. The previous angle clearly pointed to lighter blue colors, reducing that by \(20^{\circ}\) moves us towards the green edge.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s take the original image and increase saturation by 0.3:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_10-59-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 17: &amp;lt;/span&amp;gt;Image with Saturation increased by 0.3. Credit: The Expanse&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 17: &lt;/span&gt;Image with Saturation increased by 0.3. Credit: The Expanse
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;All colors across the board look &amp;ldquo;richer&amp;rdquo; and &amp;ldquo;deeper&amp;rdquo;. This corresponds with the definition of the HSV cylinder.&lt;/p&gt;
&lt;p&gt;Finally, we&amp;rsquo;ll view an image in which the value was modified. Let&amp;rsquo;s increase the values by 0.2:&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-25_11-03-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 18: &amp;lt;/span&amp;gt;Image with Values increased by 0.2. Credit: The Expanse&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 18: &lt;/span&gt;Image with Values increased by 0.2. Credit: The Expanse
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This looks washed out. All of the pixels with 0 values were increased uniformly. Perhaps we could clamp that by setting all pixels with the given value change back to their original values.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Histogram of Oriented Gradients</title>
      <link>https://ajdillhoff.github.io/notes/histogram_of_oriented_gradients/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/histogram_of_oriented_gradients/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#orientation-histograms&#34;&gt;Orientation Histograms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#histogram-of-oriented-gradients&#34;&gt;Histogram of Oriented Gradients&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Key Questions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why are these necessary?&lt;/li&gt;
&lt;li&gt;What limitations do they address that corner interest points cannot?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although the original &lt;a href=&#34;http://vision.stanford.edu/teaching/cs231b_spring1213/papers/CVPR05_DalalTriggs.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;HOG Paper&lt;/a&gt; came out after &lt;a href=&#34;https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;SIFT&lt;/a&gt;, it is much simpler to describe the process (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dalal and Triggs 2005&lt;/a&gt;).
Histograms of Oriented Gradients are feature vectors that are generated by evaluating gradients within a local neighborhood of interest points.&lt;/p&gt;
&lt;h2 id=&#34;orientation-histograms&#34;&gt;Orientation Histograms&lt;/h2&gt;
&lt;p&gt;This approach depends on building &lt;a href=&#34;https://john.cs.olemiss.edu/heroes/papers/hand_gesture.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;orientation histograms&lt;/a&gt;.
For each pixel in the original image, construct a histogram of gradient orientations of all pixels within a square window.
The gradient orientations of each pixel are easily calculated following the approach used for &lt;a href=&#34;https://ajdillhoff.github.io/notes/edge_detection/&#34;&gt;Edge Detection&lt;/a&gt;.
This transformation can be flattened into a single vector that is used to compare images via L2 distance or some other metric.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-12_19-48-07_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Orientation histograms of hand images.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Orientation histograms of hand images.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The pseudocode to generate orientation histograms is shown below.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-nil&#34; data-lang=&#34;nil&#34;&gt;let w be the window_size
let h be half the window_size
let norms be the gradient norms of the input image for each pixel
let angles be the computed orientations of the gradient vectors for each pixel
for each pixel (i, j):
    create a histogram of orientations with b bins
    weight the orientations of the bins based on the gradient norm
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The histograms of each local feature are translation invariant.
A histogram of gradient orientations for a feature in one image should be the same as one generated to a similar feature in another image.&lt;/p&gt;
&lt;p&gt;This approach is not scale invariant.&lt;/p&gt;
&lt;h2 id=&#34;histogram-of-oriented-gradients&#34;&gt;Histogram of Oriented Gradients&lt;/h2&gt;
&lt;p&gt;Dalal and Triggs propose a feature extraction method based on orientation histograms (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dalal and Triggs 2005&lt;/a&gt;). In their work published at CVPR, they evaluate their features by training a SVM for pedestrian detection on a standard (at that time) benchmark. They evaluate on a person detector using the metric &lt;strong&gt;False Positives Per Window (FPPW)&lt;/strong&gt;. This can be calculated as &lt;code&gt;num_fp / num_windows&lt;/code&gt;. This represents a tradeoff between the number of false positives and the number false negatives. Intuitively, lowering the threshold for detection will generate more false positives, but will also reduce the number of false negatives.&lt;/p&gt;
&lt;h3 id=&#34;computing-hog&#34;&gt;Computing HoG&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Normalize for color and gamma values.&lt;/li&gt;
&lt;li&gt;Compute the gradient image.&lt;/li&gt;
&lt;li&gt;Extract a window of some size.&lt;/li&gt;
&lt;li&gt;Divide window into sub-grid&lt;/li&gt;
&lt;li&gt;Compute orientation histogram of each cell.&lt;/li&gt;
&lt;li&gt;Concatentate the four histograms.&lt;/li&gt;
&lt;li&gt;Normalize the feature vector.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;During the binning step, each pixel provides a weighted vote for the histogram based on the orientation of the gradient element it centered on. This vote is weighted based on a function of the gradient magnitude.&lt;/p&gt;
&lt;p&gt;In the paper, they experiment with a wide range of different parameters. They show that optimal performance coincides with choosing 4 cells per window with 9 orientation bins.&lt;/p&gt;
&lt;h3 id=&#34;normalization-schemes&#34;&gt;Normalization Schemes&lt;/h3&gt;
&lt;p&gt;There were several normalizion schemes addressed in the paper.
The normalization scheme picked based on lowest FPPW is &lt;code&gt;L2-Hys&lt;/code&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Normalize the concatenated vector.&lt;/li&gt;
&lt;li&gt;Clip values to 0.2&lt;/li&gt;
&lt;li&gt;Normalize again.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;They also evaulated the features with L2, L1, and L1-sqrt.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-12_15-48-55_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Evaluation of normalization approaches (Dalal and Triggs, 2005).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Evaluation of normalization approaches (Dalal and Triggs, 2005).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Dalal, N., and B. Triggs. 2005. “Histograms of Oriented Gradients for Human Detection.” In &lt;i&gt;2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)&lt;/i&gt;, 1:886–93 vol. 1. &lt;a href=&#34;https://doi.org/10.1109/CVPR.2005.177&#34;&gt;https://doi.org/10.1109/CVPR.2005.177&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Image Features</title>
      <link>https://ajdillhoff.github.io/notes/image_features/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/image_features/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#detecting-corners&#34;&gt;Detecting Corners&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#describing-image-patches&#34;&gt;Describing Image Patches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scale-invariance&#34;&gt;Scale Invariance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Why do we care about image features? One of the main goals of computer vision is understanding of some environment through visual perception. In order to summarize a visual object, we need some description of it.
These descriptions can come in many forms, so we need to articulate some goals as to what we are ultimately looking for when describing an image.&lt;/p&gt;
&lt;p&gt;What makes an interesting feature in an image?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Something distinct&lt;/li&gt;
&lt;li&gt;Invariance properties (translation, rotation, scaling)&lt;/li&gt;
&lt;li&gt;Easy to compute&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Image features are the building blocks of many higher level applications such as&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Stereo correspondence&lt;/li&gt;
&lt;li&gt;Image stitching&lt;/li&gt;
&lt;li&gt;Object recognition and detection&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;Introduction/2022-02-08_09-02-34_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Patches taken from two images from different perspectives. Some patches are more descriptive than others. Source: Szeliski&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Patches taken from two images from different perspectives. Some patches are more descriptive than others. Source: Szeliski
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;Introduction/2022-02-08_09-06-10_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;Objects detected using YOLOv3. Source: Wikipedia.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;Objects detected using YOLOv3. Source: Wikipedia.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;Introduction/2022-02-08_09-07-29_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Image stitching result. The red lines show the seams at which the images are joined. Source: Wikipedia.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Image stitching result. The red lines show the seams at which the images are joined. Source: Wikipedia.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Topics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Corners&lt;/li&gt;
&lt;li&gt;HOG&lt;/li&gt;
&lt;li&gt;SIFT&lt;/li&gt;
&lt;li&gt;Correlation with template&lt;/li&gt;
&lt;li&gt;PCA&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We have talked about &lt;a href=&#34;https://ajdillhoff.github.io/notes/edge_detection/&#34;&gt;Edge Detection&lt;/a&gt;, which produces an image of edge pixels given some raw input. Edges are certainly useful features, but are they distinct enough to produce consistent image features?&lt;/p&gt;
&lt;p&gt;Consider an image patch detected from three different primitives:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Points&lt;/li&gt;
&lt;li&gt;Edges&lt;/li&gt;
&lt;li&gt;Corners&lt;/li&gt;
&lt;/ol&gt;






&lt;figure&gt;

&lt;img src=&#34;Introduction/2022-02-08_09-33-27_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Aperture problem for patches detected from different primitives. Source: Szeliski.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Aperture problem for patches detected from different primitives. Source: Szeliski.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The above figure illustrates the aperture problem.
Consider a flat surface without texture. If we generate a patch around any arbitrary point, it will have many correspondences with other patches.
It may be obvious that picking any arbitrary point on a flat, single-colored surface would not be descriptive enough to match with anything useful.&lt;/p&gt;
&lt;p&gt;What about an edge? An edge is distinct based on its orientation. The middle image in the figure above shows that, while some ambiguity has been resolved, there are still a wide range of possible locations that it could be matched to. There could be many such edges found between images.&lt;/p&gt;
&lt;p&gt;This brings us to a corner. A corner has two distinct gradient changes which make it a perfect candidate for interest point detection.&lt;/p&gt;
&lt;h2 id=&#34;detecting-corners&#34;&gt;Detecting Corners&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.434.4816&amp;amp;rep=rep1&amp;amp;type=pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Corners are a great choice for a feature. They are small, rotation and translation invariant, and can be computed simply from the gradient images we have computed before.&lt;/p&gt;
&lt;p&gt;There are a few distinct interest points of a violin. Maybe we can use a corner detector to come up with patches which can be reproduced across different images.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_09-51-14_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;A replica violin.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;A replica violin.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In a local window, a corner exhibits a large change in orientation.
A flat surface has no orientation response at all.
An edge only has an orientation in one direction.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How can we detect such changes in an image?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;sum of square differences (SSD)&lt;/strong&gt;. If we take some window and move it over some image, taking a the SSD at each point, we can produce an image of responses.
SSD is defined as&lt;/p&gt;
&lt;p&gt;\[
f(x, y) = \sum_{(u, v) \in W}\big(I(x + u, y + v) - I(u, v)\big)^2.
\]&lt;/p&gt;
&lt;p&gt;This difference was previously used to evaluate discrete steps. Harris et al. note this as a limitation and instead aim to evaluate all possible &lt;em&gt;small&lt;/em&gt; shifts about the origin of the shift. This is accomplished through analytic expansion of the term \(I(x + u, y + v)\).&lt;/p&gt;
&lt;p&gt;Through Taylor expansion, this can be approximated as&lt;/p&gt;
&lt;p&gt;\begin{align*}
I(x + u, y + v) &amp;amp;= I(u, v) + x \frac{\partial}{\partial x}I(x, y) + y \frac{\partial}{\partial y}I(x, y) + O(x^2, y^2) \\
&amp;amp;\approx I(u, v) + xI_x + yI_y
\end{align*}&lt;/p&gt;
&lt;p&gt;In the above approximation, \(O(x^2, y^2)\) describes the upper bound of the behavior of the function. Through Taylor expansion, we could write the higher order terms. However, we only care about small shifts about the shift origin, so only a first order, or linear, approximation is sufficient.&lt;/p&gt;
&lt;p&gt;Using this first order approximation, SSD can be written&lt;/p&gt;
&lt;p&gt;\begin{align*}
f(x, y) &amp;amp;\approx \sum_{(u, v) \in W} w(u, v) \big(I(u, v) + xI_x + yI_y - I(u,v)\big)^2\\
&amp;amp;= \sum_{(u, v) \in W} w(u, v) \big(xI_x + yI_y\big)^2\\
&amp;amp;= \sum_{(u, v) \in W} w(u, v) \big(x^2I_x^2 + 2xyI_xI_y + y^2I_y^2\big)
\end{align*}&lt;/p&gt;
&lt;p&gt;The term \(x^2I_x^2 + 2xyI_xI_y + y^2I_y^2\) is a linear combination and can be efficiently computed via matrix multiplication.&lt;/p&gt;
&lt;p&gt;\begin{equation*}
x^2I_x^2 + 2xyI_xI_y + y^2I_y^2 =
\begin{bmatrix}
x &amp;amp; y
\end{bmatrix}
\begin{bmatrix}
I_x^2 &amp;amp; I_x I_y \\
I_x I_y &amp;amp; I_y^2\\
\end{bmatrix}
\begin{bmatrix}
x\\
y
\end{bmatrix}
\end{equation*}&lt;/p&gt;
&lt;p&gt;We can now rewrite the original SSD as follows.&lt;/p&gt;
&lt;p&gt;\begin{equation*}
f(x, y) \approx
\begin{bmatrix}
x &amp;amp; y
\end{bmatrix}
M
\begin{bmatrix}
x\\
y
\end{bmatrix},
\end{equation*}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{equation*}
M = \sum_{(u, v) \in W} w(u, v) H.
\end{equation*}&lt;/p&gt;
&lt;p&gt;\begin{equation*}
H =
\begin{bmatrix}
I_x^2 &amp;amp; I_x I_y\\
I_x I_y &amp;amp; I_y^2 \\
\end{bmatrix}
\end{equation*}&lt;/p&gt;
&lt;p&gt;\(M\) is then an autocorrelation matrix. The benefit of this formulation is that \(M\) is a symmetric matrix. If we remember our studies from linear algebra, we remember that there are some very important properties and characteristics of symmetric matrices.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_16-54-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Gradient image (I_x^2).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Gradient image (I_x^2).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_16-55-10_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Gradient image (I_x I_y).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Gradient image (I_x I_y).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_16-55-38_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Gradient image (I_y^2).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Gradient image (I_y^2).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_10-07-09_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Gradient change in both (x) and (y). Credit: David Jacobs&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Gradient change in both (x) and (y). Credit: David Jacobs
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;First, lets consider a simple case of detecting the following corner. At this orientation, the changes in gradient are only in the vertical and horizontal directions. If we consider the matrix \(M\) from above, we would get the following result&lt;/p&gt;
&lt;p&gt;\begin{equation*}
M =
\begin{bmatrix}
\sum I_x^2 &amp;amp; \sum I_x I_y\\
\sum I_x I_y &amp;amp; \sum I_y^2
\end{bmatrix} =
\begin{bmatrix}
\lambda_1 &amp;amp; 0\\
0 &amp;amp; \lambda_2
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;The off-diagonal entries will be 0, by definition of the dot product and orthogonal vectors.
The entries on the main diagonal will represent the eigenvalues of \(M\).
If both entries on the main diagonal are large, this would indicate a large change in orientation within the window.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if the corner is not as ideal?&lt;/strong&gt;&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_10-13-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;Eigenvalue analysis of autocorrelation matrix. Source: Szeliski.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;Eigenvalue analysis of autocorrelation matrix. Source: Szeliski.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Relationship to Eigenvalues&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Recall the Spectral Theorem for Symmetric Matrices, which states:
A symmetric matrix \(A \in \mathbb{R}^{n\times n}\) has the following properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A has \(n\) real eigenvalues, counting multiplicities.&lt;/li&gt;
&lt;li&gt;The dimension of the eigenspace for each eigenvalue \(\lambda\) equals the multiplicity of \(\lambda\) as a root of the characteristic equation.&lt;/li&gt;
&lt;li&gt;The eigenspaces are mutually orthogonal, in the sense that eigenvectors corresponding to different eigenvalues are orthogonal.&lt;/li&gt;
&lt;li&gt;\(A\) is orthogonally diagonalizable.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Symmetric matrices are orthogonally diagonalizable. Thus, a symmetric matrix \(A\) can be written as \(A = PDP^{-1}\), where the columns of \(P\) are the eigenvectors and \(D\) are the corresponding eigenvalues.
Another perspective of this is that \(A\) is an ellipse with axis lengths determined by the eigenvalues (diagonal entries of \(D\)) rotated by \(P\).&lt;/p&gt;
&lt;p&gt;The eigenvalues of \(M\) can be classified into different regions depending on if they are indicative of a flat region, edge, or corner.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_10-15-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 11: &amp;lt;/span&amp;gt;Classification of responses. Source: Harris (1988).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 11: &lt;/span&gt;Classification of responses. Source: Harris (1988).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Performing eigendecomposition seems cumbersome in this case. There must be a simpler way we could compute these responses.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We can then approximate this response!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;\begin{align*}
R &amp;amp;= \det H - \alpha \cdot \textrm{tr}(H)^2\\
&amp;amp;= I_x^2 \cdot I_y^2 - (I_x I_y)^2 - \alpha\big(I_x^2 + I_y^2\big)^2
\end{align*}&lt;/p&gt;
&lt;p&gt;If there is a corner, the gradient values will depict orthogonality. That is, the middle term in the equation above will be smaller. This results in a larger response.&lt;/p&gt;
&lt;p&gt;The larger the value from the middle term, the less orthogonality is present. This results in a smaller response. In practice, we will see a negative response.&lt;/p&gt;
&lt;p&gt;In practice, \(\alpha \in [0.04, 0.06]\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_16-59-04_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 12: &amp;lt;/span&amp;gt;Response image (R).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 12: &lt;/span&gt;Response image (R).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;window-selection&#34;&gt;Window Selection&lt;/h3&gt;
&lt;p&gt;What is the best window to choose when computing responses across an image? Harris et al. considered this in their original formulation when comparing to Moravec&amp;rsquo;s corner detection function.
Using a flat window with uniform values produces a binary response when over interest points and 0 everywhere else. This can be written as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
M = \sum_{u, v} w(u, v)
\begin{bmatrix}
I_x^2 &amp;amp; I_x I_y\\
I_x I_y &amp;amp; I_y^2
\end{bmatrix}
\end{equation*}&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_10-23-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 13: &amp;lt;/span&amp;gt;Uniform response window. Results in 1 for interest points inside the window, 0 otherwise. Credit: Fei-Fei Li&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 13: &lt;/span&gt;Uniform response window. Results in 1 for interest points inside the window, 0 otherwise. Credit: Fei-Fei Li
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Instead, Harris et al. propose using a circular Gaussian window which can be computed as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
M = g(\sigma) *
\begin{bmatrix}
I_x^2 &amp;amp; I_x I_y\\
I_x I_y &amp;amp; I_y^2
\end{bmatrix}
\end{equation*}&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_10-25-42_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 14: &amp;lt;/span&amp;gt;Gaussian window response. Credit: Fei-Fei Li&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 14: &lt;/span&gt;Gaussian window response. Credit: Fei-Fei Li
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The Guassian window is easy enough to compute and has the added bonus of making the responses rotation invariant!&lt;/p&gt;
&lt;h3 id=&#34;nonmaxima-suppression--again&#34;&gt;Nonmaxima Suppression (Again)&lt;/h3&gt;
&lt;p&gt;We now have a response image in which each pixel gives an indication as to whether a corner has been detected.
To thin out these hypotheses, we will need to suppress neighborhood values that are not maximal.
Just like with &lt;a href=&#34;https://ajdillhoff.github.io/notes/edge_detection/&#34;&gt;Edge Detection&lt;/a&gt;, we will need to employ nonmaxima suppression.&lt;/p&gt;
&lt;p&gt;Before applying that, we may choose to threshold the image to filter out points that are obviously not candidates.&lt;/p&gt;
&lt;p&gt;The approach is quite simple here:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Slide a \(3 \times 3\) window across the image.&lt;/li&gt;
&lt;li&gt;If the center pixel is not the maximum value in the \(3 \times 3\) window, set it to 0.&lt;/li&gt;
&lt;li&gt;Continue until all pixels are evaluated.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Our final result is shown below. The detected corners are marked.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-08_17-05-13_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 15: &amp;lt;/span&amp;gt;Final output of corner detection.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 15: &lt;/span&gt;Final output of corner detection.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;describing-image-patches&#34;&gt;Describing Image Patches&lt;/h2&gt;
&lt;p&gt;Given a list of interest points, we can start to build a collection of regions or patches surrounding the point which are useful for feature matching.
The simple choice here is to take a fixed size patch surrounding an interest point and use it as a template.
We can then compare that to other interest points in different images to see how well they score.
There are many limitations to this naive approach that prevent it from working well in general.
Even if the perspective of the object is the same in multiple images, slight changes is brightness could affect the matching scores greatly.&lt;/p&gt;
&lt;p&gt;Another consideration is scale. If we have an interest point at one scale, will it be detected when that image is scaled by some factor \(k\)?&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-10_09-33-01_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 16: &amp;lt;/span&amp;gt;Patch surrounding a similar interest point at different scales. Credit: Kristen Grauman, B. Liebe.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 16: &lt;/span&gt;Patch surrounding a similar interest point at different scales. Credit: Kristen Grauman, B. Liebe.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;However we choose to represent the information surrounding an interest point need to be robust to translation, scale, and orientation changes.
It is important to note that, although the Harris corner detector is invariant to orientation, a naive image patch surrounding the interest point may not be.&lt;/p&gt;
&lt;h2 id=&#34;scale-invariance&#34;&gt;Scale Invariance&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Is the Harris corner detector scale-invariant?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;No! Consider the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-09_19-10-13_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 17: &amp;lt;/span&amp;gt;Harris corner detector is not scale invariant. Credit: Kristen Grauman&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 17: &lt;/span&gt;Harris corner detector is not scale invariant. Credit: Kristen Grauman
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The output from the detector will be different depending on scale.
One solution would be to resize the image over several different scales and consolidate the detections.
Doing this would produce many features and take much longer to compute.&lt;/p&gt;
&lt;p&gt;Given some feature template that is centered on an interest point, we would expect that size of the patch scales with the scale change of the image itself.
This property will drive the development of a scale-invariant method.
We select the size of the patch by placing some dark blob with a light background (or vice versa) over the interest point and then selecting the size which provides the greatest response.&lt;/p&gt;
&lt;h3 id=&#34;laplacian-filter&#34;&gt;Laplacian Filter&lt;/h3&gt;
&lt;p&gt;A good choice for this is the Laplacian filter. The Laplacian of a 2D function is&lt;/p&gt;
&lt;p&gt;\begin{equation*}
(\nabla^2 f)(x, y) = \frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;The Laplacian filter is created following the derivation in the two figures below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-10_09-55-08_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 18: &amp;lt;/span&amp;gt;Deriving second partial derivative filters for x and y. Source: &amp;lt;https://theailearner.com/2019/05/25/laplacian-of-gaussian-log/&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 18: &lt;/span&gt;Deriving second partial derivative filters for x and y. Source: &lt;a href=&#34;https://theailearner.com/2019/05/25/laplacian-of-gaussian-log/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://theailearner.com/2019/05/25/laplacian-of-gaussian-log/&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-10_09-55-53_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 19: &amp;lt;/span&amp;gt;Combining the x and y filters. Source: &amp;lt;https://theailearner.com/2019/05/25/laplacian-of-gaussian-log/&amp;gt;&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 19: &lt;/span&gt;Combining the x and y filters. Source: &lt;a href=&#34;https://theailearner.com/2019/05/25/laplacian-of-gaussian-log/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://theailearner.com/2019/05/25/laplacian-of-gaussian-log/&lt;/a&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;It is also common to smooth the operator before use.
This can be done by convolving with a Gaussian kernel:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
K_{\nabla^2} * G_{\sigma}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;This is referred as the &lt;strong&gt;Laplacian of Gaussian&lt;/strong&gt; filter.&lt;/p&gt;
&lt;h3 id=&#34;scale-space&#34;&gt;Scale Space&lt;/h3&gt;
&lt;p&gt;We can use the Laplacian of Gaussian to find the appropriate size of image patch for a given scale.
This is achieved by computing a &lt;strong&gt;scale-space&lt;/strong&gt; representation of an image.
When we resize an image to make it smaller, there is a loss of information.
Similarly, blurring the image causes a loss of information.
The larger the \(\sigma\) value for the Gaussian, the more information that is lost.
Thus, we can quickly compute different scale-image representations by applying Gaussian blurring with a range of \(\sigma\) values.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-09_19-27-45_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 20: &amp;lt;/span&amp;gt;Scale space representations. Source: Wikipedia&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 20: &lt;/span&gt;Scale space representations. Source: Wikipedia
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;As it turns out, blurring and resizing correspond with each other.
This is calculated by applying a Gaussian blur to the image following:&lt;/p&gt;
&lt;p&gt;\begin{equation*}
L_f(\sigma) = f(G_\sigma * I).
\end{equation*}&lt;/p&gt;
&lt;p&gt;This is the response of function \(f\) in scale-space \(\sigma\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-09_20-01-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 21: &amp;lt;/span&amp;gt;Selecting features at different scales. Credit: Kristen Grauman&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 21: &lt;/span&gt;Selecting features at different scales. Credit: Kristen Grauman
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The size of the patch can be found by iterating through different values of \(\sigma\), applying the Laplacian at each scale, and selecting the value of \(\sigma\) which produced the greatest result.&lt;/p&gt;
&lt;p&gt;Consider a simple rounded square as depicted below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-10_10-15-11_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 22: &amp;lt;/span&amp;gt;Simple rounded square.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 22: &lt;/span&gt;Simple rounded square.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;If we apply the LoG filter to select the scale which gives the greatest response for the region centered on the top left corner at the original scale, we produce the following graph.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-10_10-16-22_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 23: &amp;lt;/span&amp;gt;Responses of LoG at the top left corner from (sigma = 0) to (sigma = 8).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 23: &lt;/span&gt;Responses of LoG at the top left corner from (sigma = 0) to (sigma = 8).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;If we scale the original image by 2 and apply the same analysis again, we get the following graph.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-10_10-20-17_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 24: &amp;lt;/span&amp;gt;Responses of LoG at original image size at the top left corner from (sigma = 0) to (sigma = 8).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 24: &lt;/span&gt;Responses of LoG at original image size at the top left corner from (sigma = 0) to (sigma = 8).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To summarize, using this method will allow us to select the appropriate scale at which our interest point provides the strongest response.
However, the cost of this search is high. As we increase the size of the filters, the more work required for each convolution.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linear Filters</title>
      <link>https://ajdillhoff.github.io/notes/linear_filters/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/linear_filters/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#smoothing&#34;&gt;Smoothing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#convolution&#34;&gt;Convolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gaussian-filters&#34;&gt;Gaussian Filters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#image-derivatives&#34;&gt;Image Derivatives&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;How do we detect specific patterns in images (eyes, nose, spots, etc.)?&lt;/li&gt;
&lt;li&gt;Weighted sums of pixel values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;smoothing&#34;&gt;Smoothing&lt;/h2&gt;
&lt;p&gt;When discussing resizing and interpolation, we saw how the choice of scale factor and rotation can produce aliasing in images. Typically, this effect is hidden using some sort of smoothing.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s first look at the case of downsampling an image to 10\% of its original size. If we use nearest neighbor interpolation, the result is very blocky, as seen below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_20-49-14_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Rocinante cropped and scaled to 10% of the original size. Source: The Expanse&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Rocinante cropped and scaled to 10% of the original size. Source: The Expanse
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Effectively, an entire block of pixels in the original image is being mapped to the nearest neighbor. We are losing a lot of information from the original image. The figure below shows an overlay of the low resolution grid over a patch of the high resolution image.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_21-48-58_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;The center dots show the selected pixel value for the downsampled grid.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;The center dots show the selected pixel value for the downsampled grid.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;computing-the-average&#34;&gt;Computing the Average&lt;/h3&gt;
&lt;p&gt;Instead of naively selecting one pixel to represent an entire block, we could compute the average pixel value of all pixels within that block. This is a simple as taking an equal contribution from each pixel in the block and dividing by the total number of pixels in the block. An example of such a block is shown below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_21-55-06_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;A (3 times 3) averaging filter.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;A (3 times 3) averaging filter.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;By varying the size of this filter, we can effectively change the factor for which we smooth the aliasing.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_21-32-36_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Original image smoothed with (9 times 9) average filter before downsampling to 10% of its original size.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Original image smoothed with (9 times 9) average filter before downsampling to 10% of its original size.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;convolution&#34;&gt;Convolution&lt;/h2&gt;
&lt;p&gt;How can we apply the averaging filter to an image effectively? We slide the filter across the image starting at the first pixel in the first row and move row by row until all pixels have been computed.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_22-20-56_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;The takes input from all pixels under it and computes the average.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;The takes input from all pixels under it and computes the average.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-30_23-13-19_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Final portion of image after kernel is applied.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Final portion of image after kernel is applied.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;This is performed by the &lt;strong&gt;convolution&lt;/strong&gt; operation. It is defined as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
g(x, y) = \omega * f(x, y) = \sum_{dx = -a}^a \sum_{dy=-b}^b \omega(dw, dy)f(x + dx, y + dy).
\end{equation*}&lt;/p&gt;
&lt;p&gt;The range \((-a, a)\) represents the rows of the kernel and \((-b, b)\) the range of the columns of the kernel. The center of the kernel is at \((dx = 0, dy = 0)\).&lt;/p&gt;
&lt;p&gt;This operation is one of, if not the most, important operations in computer vision. It is used to apply filters, but we will later see it as one of the guiding operators for feature extraction and deep learning methods.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_15-40-19_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;An image (f) convolved with kernel (h). Source: Szeliski&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;An image (f) convolved with kernel (h). Source: Szeliski
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;properties&#34;&gt;Properties&lt;/h3&gt;
&lt;h4 id=&#34;commutativity&#34;&gt;Commutativity&lt;/h4&gt;
&lt;p&gt;\(f * g = g * f\)&lt;/p&gt;
&lt;h4 id=&#34;associativity&#34;&gt;Associativity&lt;/h4&gt;
&lt;p&gt;\(f * (g * h) = (f * g) * h\)&lt;/p&gt;
&lt;h4 id=&#34;distributivity&#34;&gt;Distributivity&lt;/h4&gt;
&lt;p&gt;\(f * (g + h) = (f * g) + (f * h)\)&lt;/p&gt;
&lt;h3 id=&#34;shift-invariant-linear-systems&#34;&gt;Shift Invariant Linear Systems&lt;/h3&gt;
&lt;p&gt;Convolution is a &lt;em&gt;linear shift-invariant&lt;/em&gt; operator. That is, it obeys the following properties.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Superposition:&lt;/strong&gt; The response of the sum of the input is the sum of the individual responses.&lt;/p&gt;
&lt;p&gt;\[
R(f + g) = R(f) + R(g)
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Scaling:&lt;/strong&gt; The response to a scaled input is equal to the scaled response of the same input.&lt;/p&gt;
&lt;p&gt;\[
R(kf) = kR(f)
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Shift Invariance:&lt;/strong&gt; The response to a translated input is equal to the translation of the response to the input.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;system&lt;/strong&gt; is linear if it satisfies both the superposition and scaling properties. Further, it is a shift-invariant linear system if it is linear and satisfies the shift-invariance property.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is the average box filter linear?&lt;/strong&gt; Yes, it is applied with convolution which behaves the same everywhere.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is thresholding a linear system?&lt;/strong&gt; No, it can be shown that \(f(n, m) + g(n, m) &amp;gt; T\), but \(f(n, m) &amp;lt; T\) and \(g(n, m) &amp;lt; T\).&lt;/p&gt;
&lt;h2 id=&#34;gaussian-filters&#34;&gt;Gaussian Filters&lt;/h2&gt;
&lt;p&gt;Blurring an image using a box filter does not simulate realistic blurring as well as Gaussian filters do. The following figure exemplifies the difference between the two.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_16-57-33_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;The left image is blurred using a uniform average box filter. The right image is blurred with a Gaussian filter.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;The left image is blurred using a uniform average box filter. The right image is blurred with a Gaussian filter.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The shortcomings of the average box filter can be seen when viewing the artifacting visible at edges, especially at corners.&lt;/p&gt;
&lt;p&gt;A Guassian kernel is defined as&lt;/p&gt;
&lt;p&gt;\begin{equation*}
G(x, y;\sigma) = \frac{1}{2\pi \sigma^2}\exp\Big(-\frac{(x^2 + y^2)}{2 \sigma^2}\Big).
\end{equation*}&lt;/p&gt;
&lt;p&gt;In effect, it enforces a greater contribution from neighbors near the pixel and a smaller contribution from distant pixels.&lt;/p&gt;
&lt;h2 id=&#34;image-derivatives&#34;&gt;Image Derivatives&lt;/h2&gt;
&lt;p&gt;We can use convolution to approximate the partial derivative (or finite difference) of an image. Recall that&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{\partial f}{\partial x} = \lim\limits_{\epsilon \rightarrow 0} \frac{f(x + \epsilon, y) - f(x, y)}{\epsilon}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;We can estimate this as a finite difference&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\frac{\partial h}{\partial x} \approx h_{i+1, j} - h_{i-1, j}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Which can be applied via convolution given a kernal&lt;/p&gt;
&lt;p&gt;\begin{equation*}
\mathcal{H} =
\begin{bmatrix}
0 &amp;amp; 0 &amp;amp; 0\\
-1 &amp;amp; 0 &amp;amp; 1\\
0 &amp;amp; 0 &amp;amp; 0\\
\end{bmatrix}.
\end{equation*}&lt;/p&gt;
&lt;p&gt;Applying both the horizontal and vertical derivative kernels to an image to a simple square shows the detection of horizontal and vertical edges.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_17-36-28_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 9: &amp;lt;/span&amp;gt;Horizontal (right) and vertical (middle) derivative kernels applied to the original image (left).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 9: &lt;/span&gt;Horizontal (right) and vertical (middle) derivative kernels applied to the original image (left).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The results of applying the derivative kernels are referred to as vertical edge and horizontal edge &lt;strong&gt;scores&lt;/strong&gt;. Consider the middle image in the figure above. The left edge has pixel values of 255; the right edge has -255. In either case, a high absolute score reveals that there is an edge. These scores report the direction of greatest change. The 255 on the left edge indicates the direction is to the right, while the -255 score indicates the direction is to the left. All other instances of the image return a rate of change of 0.
Let&amp;rsquo;s see how these filters perform on a more interesting image.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_22-45-59_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 10: &amp;lt;/span&amp;gt;Vertical derivative filter (left) and horizontal derivative filter (right).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 10: &lt;/span&gt;Vertical derivative filter (left) and horizontal derivative filter (right).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Scale Invariant Feature Transforms</title>
      <link>https://ajdillhoff.github.io/notes/scale_invariant_feature_transforms/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      
      <guid>https://ajdillhoff.github.io/notes/scale_invariant_feature_transforms/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;
&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#difference-of-gaussians&#34;&gt;Difference of Gaussians&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#keypoint-localization&#34;&gt;Keypoint Localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#orientation-assignment&#34;&gt;Orientation Assignment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#descriptor-formation&#34;&gt;Descriptor Formation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;!--endtoc--&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;General approach to computing SIFT features:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Scale-space Extrema Detection&lt;/li&gt;
&lt;li&gt;Keypoint localization&lt;/li&gt;
&lt;li&gt;Orientation Assignment&lt;/li&gt;
&lt;li&gt;Generate keypoint descriptors&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;difference-of-gaussians&#34;&gt;Difference of Gaussians&lt;/h2&gt;
&lt;p&gt;This same technique for detecting interesting points in a scale-invariant way can be approximated by taking the &lt;strong&gt;Difference of Gaussians&lt;/strong&gt;. Consider the figure below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-10_10-31-26_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Comparison of DoG and Laplacian. Credit: Fei-Fei Li.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Comparison of DoG and Laplacian. Credit: Fei-Fei Li.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;By taking the difference of images smoothed by a Gaussian with different values of \(\sigma\), the resulting pixel values correspond to areas with high gradient norms in the less blurry version.&lt;/p&gt;
&lt;p&gt;Let \(I_{\sigma_1}\) be the image blurred with a smaller value of \(\sigma\) and \(I_{\sigma_2}\) be the image blurred with a larger value.
Then \(D(I_{\sigma_1}, I_{\sigma_2}) = I_{\sigma_2} - I_{\sigma_1}\).
If a region in \(I_{\sigma_1}\) is locally flat, it will also be in flat in \(I_{\sigma_2}\).
The difference will be relatively small for that region.
If there are abrupt changes in a local region within \(I_{\sigma_1}\), they will be smoothed in \(I_{\sigma_2}\).
Therefore, the difference \(D(I_{\sigma_1}, I_{\sigma_2})\) will be higher for that region.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-13_20-29-08_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 2: &amp;lt;/span&amp;gt;The Royal Concertgebouw in Amsterdam.&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 2: &lt;/span&gt;The Royal Concertgebouw in Amsterdam.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;







&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-13_20-41-50_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 3: &amp;lt;/span&amp;gt;Difference of Gaussian between the original image blurred with (sigma = 0.5) and (sigma=1.5).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 3: &lt;/span&gt;Difference of Gaussian between the original image blurred with (sigma = 0.5) and (sigma=1.5).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;When building SIFT features, the extremum are selected by comparing 3 DoG images.
These are selected by evaluating each pixel to 26 of its neighbors in the current scale space and neighboring DoG spaces as visualized below.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-13_18-50-20_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 4: &amp;lt;/span&amp;gt;Finding extrema of pixel (i, j) in a neighborhood of 26 values (&amp;lt;a href=&amp;#34;#citeproc_bib_item_2&amp;#34;&amp;gt;Lowe 2004&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 4: &lt;/span&gt;Finding extrema of pixel (i, j) in a neighborhood of 26 values (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Lowe 2004&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To build the DoG pyramid, the authors propose that images are separated by a constant factor \(k\) in scale space.
Each octave of scale space is divided such that the scalespace doubles every \(s\) samples.&lt;/p&gt;
&lt;p&gt;Starting with \(\sigma = 0.5\), if we choose \(s=3\) then the fourth sample will be at \(\sigma = 1\), the seventh at \(\sigma=2\), and so on.
To make sure the DoG images cover the full range of an octave, \(s + 3\) images need to be created per octave.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why \(s + 3\)?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each octave should evaluate local extrema for \(s\) scales.
To evaluate this for scale \(\sigma_s\), we need the DoG for scales \(\sigma_{s-1}\) and \(\sigma_{s+1}\).
This would require 4 Gaussians images to compute.
The figure below represents the stack for \(s=2\).&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-10_17-35-51_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 5: &amp;lt;/span&amp;gt;DOG figure (&amp;lt;a href=&amp;#34;#citeproc_bib_item_2&amp;#34;&amp;gt;Lowe 2004&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 5: &lt;/span&gt;DOG figure (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Lowe 2004&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;How is the value of \(s\) determined?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the paper, the authors perform a repeatability test to determine if the keypoints would be localized even with random augmentations. The process is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Randomly augment an input image with noise, color jitter, scale, rotation, etc.&lt;/li&gt;
&lt;li&gt;Compute keypoints using using the extrema detection.&lt;/li&gt;
&lt;li&gt;Compare detected keypoints with known keypoints from original samples.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The authors found that using \(s = 3\) provided the highest percentage of repeatability in their experiments.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-13_19-25-02_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 6: &amp;lt;/span&amp;gt;Measuring repeatability of keypoint detections versus # of scales sampled per octave (&amp;lt;a href=&amp;#34;#citeproc_bib_item_2&amp;#34;&amp;gt;Lowe 2004&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 6: &lt;/span&gt;Measuring repeatability of keypoint detections versus # of scales sampled per octave (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Lowe 2004&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;keypoint-localization&#34;&gt;Keypoint Localization&lt;/h2&gt;
&lt;p&gt;Given the candidate keypoints selected by picking out local extrema, they pool of responses can further be refined
by removing points that are sensitive to noise or located along an edge. They borrow the same approach used in the Harris corner detector to select more robust interest points in corners.&lt;/p&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-15_20-36-13_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 7: &amp;lt;/span&amp;gt;Refinement of candidate keypoints by filtering those sensitive to noise (c) and those representing ambiguity along edges (d) (&amp;lt;a href=&amp;#34;#citeproc_bib_item_2&amp;#34;&amp;gt;Lowe 2004&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 7: &lt;/span&gt;Refinement of candidate keypoints by filtering those sensitive to noise (c) and those representing ambiguity along edges (d) (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Lowe 2004&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;orientation-assignment&#34;&gt;Orientation Assignment&lt;/h2&gt;
&lt;p&gt;Given a keypoint, an orientation histogram is generated. The authors use 36 bins to cover a 360 degree range for orientations.
Similar to &lt;a href=&#34;https://ajdillhoff.github.io/notes/histogram_of_oriented_gradients/&#34;&gt;Histogram of Oriented Gradients&lt;/a&gt;, the orientations are weighted by their gradient magnitudes (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Dalal and Triggs 2005&lt;/a&gt;).
Additionally, a Gaussian-weighted circular patch is applied, centered on the keypoint, to further weight the responses.
This means that points farther away from the center contribute less to the overall feature vector.&lt;/p&gt;
&lt;p&gt;In order to make the keypoint rotation invariant, the dominant orientation is determined.
If there are orientations that are within 80% of the highest orientation peak, multiple keypoints will be created using those orientations as well.&lt;/p&gt;
&lt;p&gt;Orientations in this window are rotated by the dominant gradient so that all directions are with respect to the dominant orientation.
This is a more efficient alternative to rotating the entire image by that orientation.&lt;/p&gt;
&lt;h2 id=&#34;descriptor-formation&#34;&gt;Descriptor Formation&lt;/h2&gt;






&lt;figure&gt;

&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-02-15_20-22-39_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 8: &amp;lt;/span&amp;gt;Keypoint descriptor generation (&amp;lt;a href=&amp;#34;#citeproc_bib_item_2&amp;#34;&amp;gt;Lowe 2004&amp;lt;/a&amp;gt;).&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;span class=&#34;figure-number&#34;&gt;Figure 8: &lt;/span&gt;Keypoint descriptor generation (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Lowe 2004&lt;/a&gt;).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In the paper, the authors generate keypoints using a \(16 \times 16\) window from which \(4 \times 4\) descriptors are generated following the descriptions above.
Through experimentation, each \(4 \times 4\) descriptor uses 8 orientations, resulting in a feature vector \(\mathbf{x} \in \mathbb{R}^{128}\).&lt;/p&gt;
&lt;p&gt;Different levels of contrast will product edges with higher gradient magnitudes.
To account for this, the final feature vector is normalized using the \(L2\) hysteresis approach used in Harris corner detection.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;style&gt;.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}&lt;/style&gt;&lt;div class=&#34;csl-bib-body&#34;&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_1&#34;&gt;&lt;/a&gt;Dalal, N., and B. Triggs. 2005. “Histograms of Oriented Gradients for Human Detection.” In &lt;i&gt;2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)&lt;/i&gt;, 1:886–93 vol. 1. &lt;a href=&#34;https://doi.org/10.1109/CVPR.2005.177&#34;&gt;https://doi.org/10.1109/CVPR.2005.177&lt;/a&gt;.&lt;/div&gt;
  &lt;div class=&#34;csl-entry&#34;&gt;&lt;a id=&#34;citeproc_bib_item_2&#34;&gt;&lt;/a&gt;Lowe, David G. 2004. “Distinctive Image Features from Scale-Invariant Keypoints.” &lt;i&gt;International Journal of Computer Vision&lt;/i&gt; 60 (2): 91–110. &lt;a href=&#34;https://doi.org/10.1023/B:VISI.0000029664.99615.94&#34;&gt;https://doi.org/10.1023/B:VISI.0000029664.99615.94&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
