<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computer Vision on Alex Dillhoff</title>
    <link>https://ajdillhoff.github.io/tags/computer-vision/</link>
    <description>Recent content in Computer Vision on Alex Dillhoff</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Tue, 29 Apr 2025 00:00:00 -0400</lastBuildDate>
    <atom:link href="https://ajdillhoff.github.io/tags/computer-vision/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Clustering</title>
      <link>https://ajdillhoff.github.io/notes/clustering/</link>
      <pubDate>Sun, 16 Feb 2025 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/clustering/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#k-means-clustering&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;K-Means Clustering&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#k-medoids-clustering&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;K-Medoids Clustering&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#mixtures-of-gaussians&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Mixtures of Gaussians&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;p&gt;In machine learning, the three most common forms of learning are&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bag of Visual Words</title>
      <link>https://ajdillhoff.github.io/notes/bag_of_visual_words/</link>
      <pubDate>Sun, 04 Feb 2024 18:54:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/bag_of_visual_words/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#bag-of-visual-words&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Bag of Visual Words&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;p&gt;&lt;strong&gt;Bag of Words&lt;/strong&gt; is a technique used in Natural Language Processing for document classification. It is a collection of word counts. To create a Bag of Words for a document, it necessary to create a dictionary first. Choosing the a dictionary is based on many factors including computational limitations. Next, the documents in a dataset are tokenized into words. The word counts are collected as part of a histogram and used as a feature vector for a machine learning model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Transformers for Computer Vision</title>
      <link>https://ajdillhoff.github.io/notes/transfomers_for_computer_vision/</link>
      <pubDate>Tue, 18 Apr 2023 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/notes/transfomers_for_computer_vision/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#vision-transformer--vit&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Vision Transformer (ViT) (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Dosovitskiy et al. 2021&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#swin-transformer&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Swin Transformer (&lt;a href=&#34;#citeproc_bib_item_4&#34;&gt;Liu et al. 2021&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;vision-transformer--vit&#34;&gt;Vision Transformer (ViT) (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Dosovitskiy et al. 2021&lt;/a&gt;)&lt;/h2&gt;&#xA;&lt;p&gt;The original Vision Transformer (ViT) was published by Google Brain with a simple objective: apply the Transformer architecture to images, adding as few modifications necessary. When trained on ImageNet, as was standard practice, the performance of ViT does not match models like ResNet. However, scaling up to hundreds of millions results in a better performing model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Instance Segmentation</title>
      <link>https://ajdillhoff.github.io/notes/instance_segmentation/</link>
      <pubDate>Mon, 18 Apr 2022 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/notes/instance_segmentation/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#mask-r-cnn&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Mask R-CNN (&lt;a href=&#34;#citeproc_bib_item_4&#34;&gt;He et al. 2018&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#centermask&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;CenterMask (&lt;a href=&#34;#citeproc_bib_item_6&#34;&gt;Lee and Park 2020&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#cascade-r-cnn&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Cascade R-CNN (&lt;a href=&#34;#citeproc_bib_item_1&#34;&gt;Cai and Vasconcelos 2019&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#maskformer&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;MaskFormer (&lt;a href=&#34;#citeproc_bib_item_3&#34;&gt;Cheng, Schwing, and Kirillov 2021&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#mask2former&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Mask2Former (&lt;a href=&#34;#citeproc_bib_item_2&#34;&gt;Cheng et al. 2022&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#mask-frozendetr&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Mask-FrozenDETR (&lt;a href=&#34;#citeproc_bib_item_7&#34;&gt;Liang and Yuan 2023&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#segment-anything&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Segment Anything (&lt;a href=&#34;#citeproc_bib_item_5&#34;&gt;Kirillov et al. 2023&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#segment-anything-2&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Segment Anything 2 (&lt;a href=&#34;#citeproc_bib_item_9&#34;&gt;Ravi et al. 2024&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Instance segmentation is the task of assigning a label to pixels based on which class they belong to. In a supervised setting, the results are more focused given that the domain objects is well defined. Remember that in &lt;a href=&#34;https://ajdillhoff.github.io/notes/image_segmentation/&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Image Segmentation&lt;/a&gt;, the pixels were grouped under a general critera such as proximity or color.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Object Detection</title>
      <link>https://ajdillhoff.github.io/notes/object_detection/</link>
      <pubDate>Mon, 18 Apr 2022 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/notes/object_detection/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#papers&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Papers&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#evaluating-object-detection-methods&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Evaluating Object Detection Methods&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#datasets&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Datasets&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#an-incomplete-history-of-deep-learning-based-object-detection&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;An Incomplete History of Deep-Learning-based Object Detection&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;papers&#34;&gt;Papers&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://awesomeopensource.com/projects/object-detection&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;https://awesomeopensource.com/projects/object-detection&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;evaluating-object-detection-methods&#34;&gt;Evaluating Object Detection Methods&lt;/h2&gt;&#xA;&lt;p&gt;Object detection algorithms are evaluated using the mean of Average Precision (mAP) across all classes in the dataset.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Convolutional Neural Networks</title>
      <link>https://ajdillhoff.github.io/notes/convolutional_neural_networks/</link>
      <pubDate>Sat, 02 Apr 2022 00:00:00 -0500</pubDate>
      <guid>https://ajdillhoff.github.io/notes/convolutional_neural_networks/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#convolution-operator&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Convolution Operator&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#properties-of-convolutions&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Properties of Convolutions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#parameter-sharing&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Parameter Sharing&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#pooling&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Pooling&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#backwards-pass&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Backwards Pass&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#example&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Example&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#neural-networks-for-image-classification&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Neural Networks for Image Classification&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#useful-resources&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Useful Resources&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;p&gt;&lt;strong&gt;Key Concepts&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tracking</title>
      <link>https://ajdillhoff.github.io/notes/tracking/</link>
      <pubDate>Mon, 07 Mar 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/tracking/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#tracking-with-optical-flow&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Tracking with Optical Flow&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#kalman-filters&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Kalman Filters&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Tracking features and objects is required in many applications ranging from autonomous driving to security. Vision tracking systems are often used for live sports broadcasts to keep track of players, the ball, and other visual queues related to the game.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Optical Flow</title>
      <link>https://ajdillhoff.github.io/notes/optical_flow/</link>
      <pubDate>Sun, 06 Mar 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/optical_flow/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#motion-features&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Motion Features&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#computing-optical-flow&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Computing Optical Flow&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#assumptions-of-small-motion&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Assumptions of Small Motion&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#applications&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Applications&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Optical flow refers to the apparent motion in a 2D image. Optical flow methods estimate a &lt;strong&gt;motion field&lt;/strong&gt;, which refers to the true motion of objects in 3D. If a fixed camera records a video of someone walking from the left side of the screen to the right, a difference of two consecutive frames reveals much about the apparent motion.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Segmentation via Clustering</title>
      <link>https://ajdillhoff.github.io/notes/segmentation_via_clustering/</link>
      <pubDate>Thu, 24 Feb 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/segmentation_via_clustering/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#agglomerative-clustering&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Agglomerative Clustering&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#k-means-clustering&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;K-Means Clustering&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#simple-linear-iterative-clustering--slic&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Simple Linear Iterative Clustering (SLIC)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#superpixels-in-recent-work&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Superpixels in Recent Work&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;The goal of segmentation is fairly broad: group visual elements together.&#xA;For any given task, the question is &lt;em&gt;how are elements grouped?&lt;/em&gt;&#xA;At the smallest level of an image, pixels can be grouped by color, intensity, or spatial proximity.&#xA;Without a model of higher level objects, the pixel-based approach will break down at a large enough scale.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Active Contours</title>
      <link>https://ajdillhoff.github.io/notes/active_contours/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/active_contours/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#resources&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Resources&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#parametric-representation&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Parametric Representation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#motivation-of-the-fundamental-snake-equation&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Motivation of the Fundamental Snake Equation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#external-force&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;External Force&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#energy-minimization&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Energy Minimization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#iterative-solution&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Iterative Solution&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#applications&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Applications&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;http://www.cs.ait.ac.th/~mdailey/cvreadings/Kass-Snakes.pdf&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;http://www.cs.ait.ac.th/~mdailey/cvreadings/Kass-Snakes.pdf&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Image Segmentation</title>
      <link>https://ajdillhoff.github.io/notes/image_segmentation/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/image_segmentation/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#resources&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Resources&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#gestalt-theory&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Gestalt Theory&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#grouping&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Grouping&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#segmentation-methods&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Segmentation Methods&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/&lt;/a&gt; (Berkeley Segmentation Database)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.15203v2&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;https://arxiv.org/abs/2105.15203v2&lt;/a&gt; (SegFormer)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1703.06870&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;https://arxiv.org/abs/1703.06870&lt;/a&gt; (Mask R-CNN)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/sithu31296/semantic-segmentation&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;https://github.com/sithu31296/semantic-segmentation&lt;/a&gt; (Collection of SOTA models)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Feature extraction methods such as &lt;a href=&#34;https://ajdillhoff.github.io/notes/scale_invariant_feature_transforms/&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;SIFT&lt;/a&gt; provide us with many distinct, low-level features that are useful for providing local descriptions images. We now &amp;ldquo;zoom out&amp;rdquo; and take a slightly higher level look at the next stage of image summarization.&#xA;Our goal here is to take these low-level features and group, or fit, them together such that they represent a higher level feature. For example, from small patches representing color changes or edges, we may wish to build higher-level feature representing an eye, mouth, and nose.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hough Transform</title>
      <link>https://ajdillhoff.github.io/notes/hough_transform/</link>
      <pubDate>Thu, 17 Feb 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/hough_transform/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#algorithm&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Algorithm&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#rectangle-detection-based-on-a-windowed-hough-transform&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Rectangle Detection based on a Windowed Hough Transform&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Fitting a model to a set of data by consensus, as in &lt;a href=&#34;https://ajdillhoff.github.io/notes/random_sample_consensus/&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;RANdom SAmple Consensus&lt;/a&gt;, produces a parameter estimate that is robust to outliers. A similar technique for detecting shapes in images is the &lt;strong&gt;Hough Transform&lt;/strong&gt;.&#xA;Originally it was designed for detecting simple lines, but it can be extended to detect &lt;a href=&#34;https://en.wikipedia.org/wiki/Generalised_Hough_transform&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;arbitrary shapes&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Edge Detection</title>
      <link>https://ajdillhoff.github.io/notes/edge_detection/</link>
      <pubDate>Tue, 01 Feb 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/edge_detection/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#computing-gradient-norms&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Computing Gradient Norms&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#nonmaxima-suppression&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Nonmaxima Suppression&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#thresholding&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Thresholding&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#connectivity-analysis&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Connectivity Analysis&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;outer-figure&#34;&gt;&#xA;&lt;figure class=&#34;custom-figure&#34;&gt;&#xA;&#xA;  &lt;a data-fancybox=&#34;&#34; href=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_22-45-59_screenshot.png&#34; data-caption=&#34;Figure 1: Vertical derivative filter (left) and horizontal derivative filter (right).&#34; class=&#34;glightbox&#34;&gt;&#xA;&#xA;&lt;img src=&#34;https://ajdillhoff.github.io/ox-hugo/2022-01-31_22-45-59_screenshot.png&#34; alt=&#34;&amp;lt;span class=&amp;#34;figure-number&amp;#34;&amp;gt;Figure 1: &amp;lt;/span&amp;gt;Vertical derivative filter (left) and horizontal derivative filter (right).&#34; &gt;&#xA;&lt;/a&gt;&#xA;&#xA;&#xA;&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;&#xA;  &#xA;  &lt;p&gt;&#xA;    &lt;span class=&#34;figure-number&#34;&gt;Figure 1: &lt;/span&gt;Vertical derivative filter (left) and horizontal derivative filter (right).&#xA;    &#xA;    &#xA;    &#xA;  &lt;/p&gt;</description>
    </item>
    <item>
      <title>Sampling and Aliasing</title>
      <link>https://ajdillhoff.github.io/notes/sampling/</link>
      <pubDate>Sun, 30 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/sampling/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#resizing&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Resizing&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#sampling&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Sampling&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;resizing&#34;&gt;Resizing&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Aliasing arises through resampling an image&lt;/li&gt;&#xA;&lt;li&gt;How to resize - algorithm&lt;/li&gt;&#xA;&lt;li&gt;How to resolve aliasing&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Resizing an image, whether increase or decreasing the size, is a common image operation. In Linear Algebra, &lt;strong&gt;scaling&lt;/strong&gt; is one of the transformations usually discussed, along with rotation and skew. Scaling is performed by creating a transformation matrix&lt;/p&gt;</description>
    </item>
    <item>
      <title>Color</title>
      <link>https://ajdillhoff.github.io/notes/color/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/color/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#agenda&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Agenda&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#light-and-color&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Light and Color&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#the-human-eye&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;The Human Eye&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#color-matching&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Color Matching&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#color-physics&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Color Physics&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#color-spaces&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Color Spaces&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#hsv-color-space&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;HSV Color Space&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;agenda&#34;&gt;Agenda&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What is color?&lt;/li&gt;&#xA;&lt;li&gt;How do we process color?&lt;/li&gt;&#xA;&lt;li&gt;How is color modeled?&lt;/li&gt;&#xA;&lt;li&gt;What information does color contain?&lt;/li&gt;&#xA;&lt;li&gt;What can we infer from color?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;light-and-color&#34;&gt;Light and Color&lt;/h2&gt;&#xA;&lt;p&gt;Light is electromagnetic radiation. It is generally described as waves in an electromagnetic field, but it also considered as photons. Electromagnetic radiation is typically measured based on its wavelength and intensity. Intensity refers to the amount of power that is carried by the photons.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Histogram of Oriented Gradients</title>
      <link>https://ajdillhoff.github.io/notes/histogram_of_oriented_gradients/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/histogram_of_oriented_gradients/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#orientation-histograms&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Orientation Histograms&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#histogram-of-oriented-gradients&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Histogram of Oriented Gradients&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Key Questions&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Image Features</title>
      <link>https://ajdillhoff.github.io/notes/image_features/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/image_features/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#detecting-corners&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Detecting Corners&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#describing-image-patches&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Describing Image Patches&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#scale-invariance&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Scale Invariance&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Why do we care about image features? One of the main goals of computer vision is understanding of some environment through visual perception. In order to summarize a visual object, we need some description of it.&#xA;These descriptions can come in many forms, so we need to articulate some goals as to what we are ultimately looking for when describing an image.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Filters</title>
      <link>https://ajdillhoff.github.io/notes/linear_filters/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/linear_filters/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#smoothing&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Smoothing&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#convolution&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Convolution&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#gaussian-filters&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Gaussian Filters&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#image-derivatives&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Image Derivatives&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;p&gt;Liner filters are a fundamental concept in computer vision. They are used to process images in a variety of ways, such as smoothing, sharpening, and edge detection. They are also used in convolutional neural networks to extract features from images. An essential building block of the computer vision pipeline, understanding linear filters is crucial for anyone working in the field.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Scale Invariant Feature Transforms</title>
      <link>https://ajdillhoff.github.io/notes/scale_invariant_feature_transforms/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 -0600</pubDate>
      <guid>https://ajdillhoff.github.io/notes/scale_invariant_feature_transforms/</guid>
      <description>&lt;div class=&#34;ox-hugo-toc toc&#34;&gt;&#xA;&lt;div class=&#34;heading&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#difference-of-gaussians&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Difference of Gaussians&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#keypoint-localization&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Keypoint Localization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#orientation-assignment&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Orientation Assignment&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#descriptor-formation&#34;&#xA;&#xA;&#xA;&#xA; &#xA;&#xA;&#xA;&gt;Descriptor Formation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;!--endtoc--&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf&#34;&#xA;&#xA;&#xA;&#xA;&#xA; target=&#34;_blank&#34;&#xA; &#xA;&#xA;&#xA;&gt;https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
